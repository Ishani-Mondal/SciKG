{"title": [{"text": "Lexical Paraphrasing for Document Retrieval and Node Identification", "labels": [], "entities": [{"text": "Document Retrieval", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.859677642583847}, {"text": "Node Identification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6935808062553406}]}], "abstractContent": [{"text": "We investigate lexical paraphrasing in the context of two distinct applications: document retrieval and node identification.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7608142793178558}, {"text": "node identification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7501712441444397}]}, {"text": "Document retrieval-the first step in question answering-retrieves documents that contain answers to user queries.", "labels": [], "entities": [{"text": "Document retrieval-the", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8815568685531616}, {"text": "question answering-retrieves documents that contain answers to user queries", "start_pos": 37, "end_pos": 112, "type": "TASK", "confidence": 0.8493289351463318}]}, {"text": "Node identification-performed in the context of a Bayesian argumentation system-matches users' Natural Language sentences to nodes in a Bayesian network.", "labels": [], "entities": [{"text": "Node identification-performed", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8654275834560394}]}, {"text": "Lexical paraphrases are generated using syntactic, semantic and corpus-based information.", "labels": [], "entities": []}, {"text": "Our evaluation shows that lexical paraphrasing improves retrieval performance for both applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the difficulties users face when accessing a knowledge repository is that of expressing themselves in away that will produce the desired outcome, e.g., retrieve relevant documents or make their dialogue contributions understood.", "labels": [], "entities": []}, {"text": "The use of appropriate terminology is one aspect of this problem: if a user's vocabulary differs from that within the resource being accessed, the system maybe unable to satisfy the user's requirements.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the application of lexical paraphrasing to two different information access applications: document retrieval and node identification.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7595332860946655}, {"text": "node identification", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7809030413627625}]}, {"text": "Document retrieval is performed in the context of the TREC Question Answering task, where the system retrieves documents that contain answers to users' queries.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9062745273113251}, {"text": "TREC Question Answering task", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.7481841966509819}]}, {"text": "Node identification is performed in the context of a Natural Language (NL) interface to a Bayesian argumentation system).", "labels": [], "entities": [{"text": "Node identification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9184942543506622}]}, {"text": "Here the system finds the nodes from a Bayesian network (BN) that best match a user's NL sentences.", "labels": [], "entities": []}, {"text": "Lexical paraphrases replace content words in a user's input with their synonyms.", "labels": [], "entities": []}, {"text": "We use the following information sources to perform this task: syntactic -obtained from Brill's part-of-speech tagger; semantic -obtained from WordNet () and the Webster-1913 online dictionary; and statistical -obtained from our document collection.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.9364734292030334}]}, {"text": "The statistical information is used to moderate the alternatives obtained from the semantic resources, by preferring query paraphrases that contain frequent word combinations.", "labels": [], "entities": []}, {"text": "Our evaluation assessed the effect of lexical paraphrasing on our two applications.", "labels": [], "entities": []}, {"text": "Its impact on document retrieval was evaluated using subsets of queries from the TREC8, TREC9 and TREC10 collections, and its effect on node identification was evaluated using paraphrases generated by people for some nodes in our BN.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7450501024723053}, {"text": "TREC9 and TREC10 collections", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.6526385694742203}, {"text": "node identification", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7520743906497955}]}, {"text": "In the next section we discuss related research.", "labels": [], "entities": []}, {"text": "Section 3 describes our two applications.", "labels": [], "entities": []}, {"text": "In Section 4, we consider the paraphrasing process, and in Section 5 the information retrieval procedures.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.8356778025627136}]}, {"text": "Section 6 presents the results of our evaluation, followed by concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our evaluation metric and data sets.", "labels": [], "entities": []}, {"text": "We then discuss our experiments, and analyze our results.", "labels": [], "entities": []}, {"text": "We use the number of matches measure to evaluate retrieval performance.", "labels": [], "entities": []}, {"text": "In the DocRet application, this measure returns the number of queries for which the system retrieved at least one document that contains the answer to a query.", "labels": [], "entities": []}, {"text": "In the NodeID application, this measure returns the number of sentences for which the intended node was among the returned nodes.", "labels": [], "entities": []}, {"text": "The following example illustrates why this measure was chosen instead of the standard precision measure.", "labels": [], "entities": [{"text": "precision measure", "start_pos": 86, "end_pos": 103, "type": "METRIC", "confidence": 0.9640373885631561}]}, {"text": "Consider a situation in the DocRet application where 10 correct documents are retrieved for each of 2 queries and 0 correct documents for each of 3 queries, compared to a situation where 2 correct documents are retrieved for each of 5 queries.", "labels": [], "entities": []}, {"text": "Average precision would yield a better score for the first situation, failing to address the question of interest for this application, namely how many queries have a chance of being answered, which is 2 in the first case and 5 in the second case.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9994791150093079}]}, {"text": "Our evaluation determines the effect of paraphrasing on retrieval performance, as well as the number of paraphrases that yields the best performance.", "labels": [], "entities": []}, {"text": "For both applications, we submitted to the retrieval engine increasing sets of paraphrases as follows: first the lemmatized query or sentence alone (Set 0), next we added to the query or sentence up to 2 paraphrases (Set 2), then up to 5 paraphrases (Set 5), up to 12 paraphrases (Set 12), and up to a maximum of 19 paraphrases (Set 19).", "labels": [], "entities": []}, {"text": "For the DocRet application, the number of retrieved documents was kept constant at 200, as suggested in).", "labels": [], "entities": []}, {"text": "Since for the NodeID application there is only one correct node, it is critical to return this node most of the time.", "labels": [], "entities": []}, {"text": "Hence, we considered 1 or 3 retrieved nodes.", "labels": [], "entities": []}, {"text": "We experimented with different combinations of the operating parameters of the paraphrasing process as follows: (P1) WordNet alone or WordNet+Webster; (P2) Pr Slem4 -baseline measure or the approximation in Eqn.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9197297096252441}, {"text": "WordNet+Webster", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.88614821434021}]}, {"text": "The combination of these parameters yielded the four configurations in (the remaining options are not viable).", "labels": [], "entities": []}, {"text": "In addition, we considered the following options for the retrieval process: (R1) Weighting -uniform or weighted; and (R2) func -maximum or average (only for the NodeID application, Section 5.2).", "labels": [], "entities": []}, {"text": "The uniform policy in R1 assigns the same weight (=1) to the paraphrases during retrieval, instead of using the weights obtained from the paraphrasing process (weighted policy).", "labels": [], "entities": []}, {"text": "The rationale for the uniform policy is that while a sorting criterion is required to rank the paraphrases, once a ranking is obtained, the top-K paraphrases maybe equally useful.", "labels": [], "entities": []}, {"text": "The policies in R1 yielded four additional configurations, one for each entry in.", "labels": [], "entities": [{"text": "R1", "start_pos": 16, "end_pos": 18, "type": "DATASET", "confidence": 0.8753113746643066}]}, {"text": "These configurations were used to generate and rank the paraphrases, but the top-K paraphrases were weighed uniformly during retrieval (we distinguish the weighted policy from the uniform policy by appending the suffix WGT to the weighed options and the suffix UNI to the uniform options).", "labels": [], "entities": []}, {"text": "Finally, the policies in R2 yielded another eight configurations for the NodeID application only (each of the above options with func=maximum or func=average).", "labels": [], "entities": []}, {"text": "These configurations are denoted by appending the suffix MAX or AVG to the above designations, e.g., WNWEB-SIMCTXT/UNI/AVG.", "labels": [], "entities": [{"text": "MAX", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.8897884488105774}, {"text": "AVG", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.8015632629394531}, {"text": "WNWEB-SIMCTXT/UNI/AVG", "start_pos": 101, "end_pos": 122, "type": "DATASET", "confidence": 0.8224440574645996}]}], "tableCaptions": []}