{"title": [{"text": "Semi-supervised Verb Class Discovery Using Noisy Features", "labels": [], "entities": [{"text": "Verb Class Discovery", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.6120352745056152}]}], "abstractContent": [{"text": "We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.", "labels": [], "entities": []}, {"text": "The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.", "labels": [], "entities": []}, {"text": "In moving to a scenario of verb class discovery, using clustering , we face the problem of having a large number of irrelevant features fora particular clustering task.", "labels": [], "entities": [{"text": "verb class discovery", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.650789608558019}]}, {"text": "We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7268482446670532}]}, {"text": "We find that the un-supervised method we tried cannot be consistently applied to our data.", "labels": [], "entities": []}, {"text": "However, the semi-supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies.", "labels": [], "entities": []}, {"text": "Learning the argument structure properties of verbs-the semantic roles they assign and their mapping to syntactic positions-is both particularly important and difficult.", "labels": [], "entities": []}, {"text": "A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (), selectional preferences, and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (.", "labels": [], "entities": [{"text": "lexical semantic classification", "start_pos": 165, "end_pos": 196, "type": "TASK", "confidence": 0.6603598892688751}]}, {"text": "Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (), on the handselection of features), or on the use of an extensive grammar (Schulte im).", "labels": [], "entities": []}, {"text": "We focus hereon extending the applicability of unsupervised methods, as in (Schulte im, to the lexical semantic classification of verbs.", "labels": [], "entities": [{"text": "lexical semantic classification of verbs", "start_pos": 95, "end_pos": 135, "type": "TASK", "confidence": 0.791444456577301}]}, {"text": "Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb.", "labels": [], "entities": []}, {"text": "As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon ().", "labels": [], "entities": []}, {"text": "However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.", "labels": [], "entities": []}, {"text": "Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.", "labels": [], "entities": []}, {"text": "It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features.", "labels": [], "entities": []}, {"text": "We have previously shown that abroad set of 220 noisy features performs well in supervised verb classification (.", "labels": [], "entities": [{"text": "supervised verb classification", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.6082150737444559}]}, {"text": "In contrast to, we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.).", "labels": [], "entities": []}, {"text": "On the other hand, in contrast to Schulte im, we demonstrated that accurate subcategorization statistics are unnecessary (see also).", "labels": [], "entities": []}, {"text": "By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7402708530426025}]}, {"text": "However, a general feature space means that most features will be irrelevant to any given verb discrimination task.", "labels": [], "entities": [{"text": "verb discrimination task", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.814356287320455}]}, {"text": "In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \"the curse of dimensionality\"?", "labels": [], "entities": [{"text": "verb class discovery", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.675543487071991}]}, {"text": "In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand.", "labels": [], "entities": []}, {"text": "In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner.", "labels": [], "entities": []}, {"text": "Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7491077482700348}, {"text": "verb class discovery", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7010857065518697}]}, {"text": "In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features,, and a semisupervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).", "labels": [], "entities": []}, {"text": "Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard.", "labels": [], "entities": [{"text": "verb class discovery", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7240826884905497}]}, {"text": "To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features.", "labels": [], "entities": []}, {"text": "The unsupervised feature selection method, on the other hand, was not usable for our data.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6633524149656296}]}, {"text": "In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs.", "labels": [], "entities": []}, {"text": "We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of related work, our contributions, and future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the same classes and example verbs as in the supervised experiments of to enable a comparison between the performance of the unsupervised and supervised methods.", "labels": [], "entities": []}, {"text": "Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.", "labels": [], "entities": []}, {"text": "We use three separate evaluation measures, that tap into very different properties of the clusterings.", "labels": [], "entities": []}, {"text": "We report here the results of a number of clustering experiments, using feature sets as follows: (1) the full feature space; (2) a manually selected subset of features; (3) unsupervised selection of features; and (4) semi-supervised selection, using a supervised learner applied to seed verbs to select the features.", "labels": [], "entities": []}, {"text": "For each type of feature set, we performed the same ten clustering tasks, shown in the first column of.", "labels": [], "entities": []}, {"text": "These are the same tasks performed in the supervised setting of.", "labels": [], "entities": []}, {"text": "The 2-and 3-way tasks, and their motivation, were described in Section 3.1.", "labels": [], "entities": []}, {"text": "Three multiway tasks explore performance over a larger number of classes: The 6-way task involves the Cheat, Steal-Remove, Wipe, Spray/Load, Fill, and \"Other Verbs of Putting\" classes, all of which undergo similar locative alternations.", "labels": [], "entities": []}, {"text": "To these 6, the 8-way task adds the Run and Sound Emission verbs, which also undergo locative alternations.", "labels": [], "entities": [{"text": "Run", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9323861002922058}]}, {"text": "The 13-way task includes all of our classes.", "labels": [], "entities": []}, {"text": "The second column of includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995569586753845}]}, {"text": "These are the results of a 10-fold crossvalidation (with boosting) repeated 50 times.", "labels": [], "entities": []}, {"text": "In our earlier work, we found that cross-validation performance averaged about .02, .04, and .11 higher than test performance on the 2-way, 3-way, and multiway tasks, respectively, and so should betaken as an upper bound on what can be achieved.", "labels": [], "entities": [{"text": "betaken", "start_pos": 195, "end_pos": 202, "type": "METRIC", "confidence": 0.9427562952041626}]}, {"text": "The third column of measures as described in Section 4.2, for each of the feature sets we explored in clustering, which we discuss in turn below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Verb classes (see Section 3.1), their Levin class  numbers, and the number of experimental verbs in each  (see Section 3.2).", "labels": [], "entities": []}, {"text": " Table 2: Experimental Results. C5.0 is supervised accuracy; Base", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9880927205085754}]}, {"text": " Table 3: Feature counts for Ling and Seed feature sets.", "labels": [], "entities": []}]}