{"title": [{"text": "Improved Automatic Keyword Extraction Given More Linguistic Knowledge", "labels": [], "entities": [{"text": "Improved Automatic Keyword Extraction", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8693966567516327}]}], "abstractContent": [{"text": "In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.", "labels": [], "entities": [{"text": "automatic extraction of keywords from abstracts", "start_pos": 30, "end_pos": 77, "type": "TASK", "confidence": 0.8300304015477499}]}, {"text": "The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers.", "labels": [], "entities": []}, {"text": "In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the POS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained , independent of the term selection approach applied.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9966880679130554}]}], "introductionContent": [{"text": "Automatic keyword assignment is a research topic that has received less attention than it deserves, considering keywords' potential usefulness.", "labels": [], "entities": [{"text": "Automatic keyword assignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7297723491986593}]}, {"text": "Keywords may, for example, serve as a dense summary fora document, lead to improved information retrieval, or be the entrance to a document collection.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.6875598132610321}]}, {"text": "However, relatively few documents have keywords assigned, and therefore finding methods to automate the assignment is desirable.", "labels": [], "entities": []}, {"text": "A related research area is that of terminology extraction (see e.g.,), where all terms describing a domain are to be extracted.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8992120325565338}]}, {"text": "The aim of keyword assignment is to find a small set of terms that describes a specific document, independently of the domain it belongs to.", "labels": [], "entities": [{"text": "keyword assignment", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8045815229415894}]}, {"text": "However, the latter may very well benefit from the results of the former, as appropriate keywords often are of a terminological character.", "labels": [], "entities": []}, {"text": "In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7433186620473862}]}, {"text": "Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm.", "labels": [], "entities": []}, {"text": "In this paper, experiments with three term selection approaches are presented: n-grams; noun phrase (NP) chunks; and terms matching any of a set of part-of-speech (POS) tag sequences.", "labels": [], "entities": []}, {"text": "Four different features are used: term frequency, collection frequency, relative position of the first occurrence, and the POS tag(s) assigned to the term.", "labels": [], "entities": [{"text": "POS tag(s)", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.8781443476676941}]}], "datasetContent": [{"text": "The feature values were calculated for each extracted unit in the training and the validation sets, that is for the n-grams, NP-chunks, stemmed NPchunks, patterns, and the stemmed patterns respectively.", "labels": [], "entities": []}, {"text": "In other words, the within-document frequency, the collection frequency, and the proportion of the document preceding the first appearance for each potential term were calculated.", "labels": [], "entities": [{"text": "collection frequency", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.7363688051700592}]}, {"text": "Also, the POS tag(s) for each term were extracted.", "labels": [], "entities": [{"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8519471287727356}]}, {"text": "In addition, as the machine learning approach is supervised, the class was added, i.e., whether the term is a manually assigned keyword or not.", "labels": [], "entities": []}, {"text": "For the stemmed terms, a unit was considered a keyword if it was equal to a stemmed manual keyword.", "labels": [], "entities": []}, {"text": "For the unstemmed terms, the term had to match exactly.", "labels": [], "entities": []}, {"text": "The measure used to evaluate the results on the validation set was the F-score, defined as The system is Rule Discovery System from Compumine AB.", "labels": [], "entities": [{"text": "F-score", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.997066080570221}]}, {"text": "combining the precision and the recall obtained.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996689558029175}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9992538094520569}]}, {"text": "In this study, the main concern is the precision and the recall for the examples that have been assigned the class positive, that is how many of the suggested keywords are correct (precision), and how many of the manually assigned keywords that are found (recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995988011360168}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9991642236709595}, {"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9977436065673828}, {"text": "recall", "start_pos": 256, "end_pos": 262, "type": "METRIC", "confidence": 0.9967873096466064}]}, {"text": "As the proportion of correctly suggested keywords is considered equally important as the amount of terms assigned by a professional indexer that was detected, \u00a7 was assigned the value 1, thus giving precision and recall equal weights.", "labels": [], "entities": [{"text": "precision", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9993763566017151}, {"text": "recall", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.9989553689956665}]}, {"text": "When calculating the recall, the value for the total number of manually assigned keywords present in the documents is used, independent of the number actually present in the different representations.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.996670663356781}]}, {"text": "This figure varies slightly for the unstemmed and the stemmed data, and for the two the corresponding value is used.", "labels": [], "entities": []}, {"text": "Several runs were made for each representation, with the goal to maximise the performance as evaluated on the validation set: first the weights of the positive examples were adjusted, as the data set is unbalanced.", "labels": [], "entities": []}, {"text": "A better performance was obtained when the positive examples in the training data outnumbered the negative ones.", "labels": [], "entities": []}, {"text": "Thereafter experiments with bagging were performed, and also, runs with and without the POS tag feature were made.", "labels": [], "entities": []}, {"text": "The results are presented next.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision, and the average number of  correct terms for Turney (2000)* and Frank et al.  (1999)**, for five and fifteen extracted terms.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9963186979293823}, {"text": "Turney (2000)* and Frank et al.  (1999)**", "start_pos": 66, "end_pos": 107, "type": "DATASET", "confidence": 0.8948082476854324}]}, {"text": " Table 2: For each representation is shown: the number of assigned (Assign.) terms in total and mean per  document; the number of correct (Corr.) terms in total and mean per document; precision; recall; and F- score. The highest value is shown in bold. The total number of manually assigned terms present in the  abstracts is 3 816, and the mean is 7.63 terms per document.", "labels": [], "entities": [{"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9995741248130798}, {"text": "recall", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9986691474914551}, {"text": "F- score", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9937120874722799}]}]}