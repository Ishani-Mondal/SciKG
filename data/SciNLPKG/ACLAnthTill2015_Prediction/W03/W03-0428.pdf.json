{"title": [{"text": "Named Entity Recognition with Character-Level Models", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7175277173519135}]}], "abstractContent": [{"text": "We discuss two named-entity recognition models which use characters and character \u00a4-grams either exclusively or as an important part of their data representation.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.7281810343265533}]}, {"text": "The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.", "labels": [], "entities": []}, {"text": "Our best model achieves an overall F\u00a5 of 86.07% on the English test data (92.31% on the development data).", "labels": [], "entities": [{"text": "F\u00a5", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9218167960643768}, {"text": "English test data", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9197384317715963}]}, {"text": "This number represents a 25% error reduction over the same model without word-internal (substring) features.", "labels": [], "entities": []}], "introductionContent": [{"text": "For most sequence-modeling tasks with word-level evaluation, including named-entity recognition and part-ofspeech tagging, it has seemed natural to use entire words as the basic input features.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.7360866814851761}, {"text": "part-ofspeech tagging", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7409544289112091}]}, {"text": "For example, the classic HMM view of these two tasks is one in which the observations are words and the hidden states encode class labels.", "labels": [], "entities": []}, {"text": "However, because of data sparsity, sophisticated unknown word models are generally required for good performance.", "labels": [], "entities": []}, {"text": "A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997,).", "labels": [], "entities": []}, {"text": "One then treats the unknown word as a collection of such features.", "labels": [], "entities": []}, {"text": "Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge.", "labels": [], "entities": []}, {"text": "Here, we examine the utility of taking character sequences as a primary representation.", "labels": [], "entities": []}, {"text": "We present two models in which the basic units are characters and character \u00a4 -grams, instead of words and word phrases.", "labels": [], "entities": []}, {"text": "Earlier papers have taken a character-level approach to named entity recognition (NER), notably, which used prefix and suffix tries, though to our knowledge incorporating all character \u00a4 -grams is new.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.7970744321743647}]}, {"text": "In section 2, we discuss a character-level HMM, while in section 3 we discuss a sequence-free maximum-entropy (maxent) classifier which uses \u00a4 -gram substring features.", "labels": [], "entities": []}, {"text": "Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tagging or earlier NER work).", "labels": [], "entities": []}, {"text": "shows a graphical model representation of our character-level HMM.", "labels": [], "entities": []}, {"text": "Characters are emitted one at a time, and there is one state per character.", "labels": [], "entities": []}, {"text": "Each state's identity depends only on the previous state.", "labels": [], "entities": []}, {"text": "Each character's identity depends on both the current state and on the previous", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: HMM F\u00a5 performance, English development set.", "labels": [], "entities": [{"text": "HMM F\u00a5 performance", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.708959236741066}, {"text": "English development set", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.7396034002304077}]}, {"text": " Table 2: CMM performance with incrementally added features on the English development set.", "labels": [], "entities": [{"text": "English development set", "start_pos": 67, "end_pos": 90, "type": "DATASET", "confidence": 0.6720695098241171}]}, {"text": " Table 3: Example of the features and weights at a local  decision point: deciding the classification of Grace.", "labels": [], "entities": [{"text": "classification of Grace", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.6801797946294149}]}]}