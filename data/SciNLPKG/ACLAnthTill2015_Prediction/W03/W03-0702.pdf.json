{"title": [{"text": "Conceptual Language Models for Dialog systems", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "The purpose of computer speech understanding is to find conceptual representations from signs coded into the speech signal.", "labels": [], "entities": [{"text": "computer speech understanding", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.616395910580953}]}, {"text": "Contrary to speech interpretation by humans in which the same discourse maybe interpreted differently by different subjects, for practical applications of computer understanding the result of interpretation should be unique fora given signal.", "labels": [], "entities": [{"text": "speech interpretation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7629357576370239}]}, {"text": "Usually it is represented by an object which is an instance of class corresponding to a semantic structure which can be fairly complex even if it is built with instances of conceptual constituents belonging to a small set of major ontological categories.", "labels": [], "entities": []}, {"text": "The mapping process that leads to a semantic interpretation can be derived manually because human interpretation of sentences can be completely explained with a logical formalism or it can be inferred by machine learning algorithms in order to ensure a large coverage of possible sentence patterns.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7461182475090027}]}, {"text": "Theories and practical implementations of these approaches are proposed in,.", "labels": [], "entities": []}, {"text": "Limitations of coverage in the manual approach and in precision of machine learning can be reduced by making manually a detailed analysis of a limited number of examples and generalizing each analysis with automatic methods.", "labels": [], "entities": [{"text": "coverage", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9724010229110718}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9985504746437073}]}, {"text": "In particular, a well structured lexicon can be very useful, in which the meaning of words is represented together with suggestions of possible syntactic and conceptual structures.", "labels": [], "entities": []}, {"text": "Word associations found with networks of word relations can also be useful for suggesting compositions of semantic constituents into conceptual structures.", "labels": [], "entities": []}, {"text": "Thus, given an observed example, other examples can be manually derived and generalized automatically.", "labels": [], "entities": []}, {"text": "Computer understanding of a spoken sentences is problem solving activity whose central engine is a search process involving various types of models.", "labels": [], "entities": [{"text": "Computer understanding of a spoken sentences", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8512254456679026}]}, {"text": "Searching for concepts can be combined with searching for words.", "labels": [], "entities": []}, {"text": "This suggests that statistical language models (LMs) could be adapted based on expectations of concepts predicted by a system belief.", "labels": [], "entities": []}, {"text": "With this perspective, it is important to notice that, while the observation of only certain words maybe sufficient for hypothesizing a conceptual structure, complete details of word phrases expressing a conceptual structure have to be known in order to adapt a generic LM to the expectation of such a structure.", "labels": [], "entities": []}, {"text": "This paper introduces a search method and a learning paradigm based on the just introduced considerations.", "labels": [], "entities": []}, {"text": "The search engine built with this method finds the best common path between the system knowledge represented by the composition of Stochastic Finite State Transducers (SFST) and a Stochastic Finite State Automaton (SFSA) representing the lattice of word hypotheses generated by an Automatic Speech Recognition System (ASR).", "labels": [], "entities": []}], "datasetContent": [{"text": "Leta dialogue system have a belief which generates expectations B about conceptual structures.", "labels": [], "entities": []}, {"text": "Expectation uncertainty is represented by a probability distribution P(B) which is non-zero fora set of conceptual structures expected at a given time.", "labels": [], "entities": [{"text": "Expectation uncertainty", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7947738468647003}, {"text": "probability distribution P(B)", "start_pos": 44, "end_pos": 73, "type": "METRIC", "confidence": 0.7144192854563395}]}, {"text": "Thus fora general concept structure \u0393 and a description Y of the speech signal, one gets: A general concept structure \u0393 can be represented as a string of parenthesized terminals and non-terminals.", "labels": [], "entities": []}, {"text": "These expressions can be decomposed into chunks.", "labels": [], "entities": []}, {"text": "A sentence may contain only one or more chunks of an incomplete structure, Thus, a system should be able to generate interpretation hypotheses about parts of a conceptual structure.", "labels": [], "entities": []}, {"text": "In this case, symbol \u0393 makes reference only to a set of components.", "labels": [], "entities": []}, {"text": "Probability P(\u0393|BW) can be simply set equal to 0 fora conceptual structure which cannot be inferred from W.", "labels": [], "entities": []}, {"text": "If the conceptual structure is part of the expectations of system beliefs and can be inferred unambiguously from W, then P(\u0393|BW) as in many practical applications including the one considered in this paper, then P(\u0393|BW).", "labels": [], "entities": []}, {"text": "Otherwise, let be the sequence of concept symbols corresponding to the preterminal symbols in \u0393.", "labels": [], "entities": []}, {"text": "Probability P(\u0393|BW) can be expressed as follows: At least, for some values of \u03b3 the probability is one fora class of applications.", "labels": [], "entities": []}, {"text": "Let \u03a6 be the set of conceptual components, chunks of them or conceptual structures known to the system.", "labels": [], "entities": []}, {"text": "Expectations derived from the system belief can be grouped into a set B1.", "labels": [], "entities": []}, {"text": "Let B2 the complement of B1 w.r.t.", "labels": [], "entities": []}, {"text": "\u03a6 and F be a filler structure representing all the conceptual structures not in the application or just ignored by ignorance of the system knowledge.", "labels": [], "entities": [{"text": "F", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.9802314639091492}]}, {"text": "B1, B2 and F are the possible values for B in the (1) and their probabilities P(B) can be established subjectively or by evaluating counts for user responses consistent with the belief, consistent with the application but not with the belief and inconsistent with the application knowledge.", "labels": [], "entities": [{"text": "F", "start_pos": 11, "end_pos": 12, "type": "METRIC", "confidence": 0.9859219193458557}]}, {"text": "Probability P(W|B) is that of an LM which is adapted to the system belief.", "labels": [], "entities": [{"text": "Probability P(W|B)", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.6969016705240522}]}, {"text": "It can be obtained with an LM builtin the following way.", "labels": [], "entities": []}, {"text": "Each conceptual structure or part of it \u0393 is represented by a finite-state network N(\u0393).", "labels": [], "entities": []}, {"text": "All the networks corresponding to structures in B1 are connected in parallel in a single structure with associated a probability P(B1).", "labels": [], "entities": []}, {"text": "A similar structure is built for the automata corresponding to structures in B2.", "labels": [], "entities": []}, {"text": "A filler F is also considered containing a network derived by a trigram LM.", "labels": [], "entities": []}, {"text": "A network N(\u0393) is obtained by the concatenation of finite-state automata C(\u0393) inferred with the procedure described in the next section representing chunks of knowledge with fillers F.", "labels": [], "entities": []}, {"text": "These automata output components of conceptual structures.", "labels": [], "entities": []}, {"text": "A search is performed by finding the most likely common path in the network and in the automaton derived from a lattice of word hypotheses generated by the speech recognizer with the generic trigram LM.", "labels": [], "entities": []}, {"text": "System belief make vary the topology of the network by dynamically changing the composition of sets B1 and B2.", "labels": [], "entities": []}, {"text": "Network recompilation can be avoided by just putting all the N(\u0393) in parallel and dynamically assigning each network of B1 a probability : where indicates the number of elements in B1.", "labels": [], "entities": [{"text": "Network recompilation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7255063951015472}]}, {"text": "Probabilities of networks in B2 are assigned in a similar way.", "labels": [], "entities": []}, {"text": "A word sequence W always corresponds to a path in F and may correspond to one or more conceptual structures represented bypaths in networks in B1 and B2.", "labels": [], "entities": []}, {"text": "In the second case, the likelihood of W in F will be much lower than the likelihood in B1 or B2 because phrases recognized by the chunk automata of the network are boosted as it will be shown later.", "labels": [], "entities": []}, {"text": "Thus the best path for W, in this case, will go through a network whose automata produce as output the components of a conceptual structure.", "labels": [], "entities": []}], "tableCaptions": []}