{"title": [{"text": "A Fast Algorithm for Feature Selection in Conditional Maximum Entropy Modeling", "labels": [], "entities": [{"text": "Feature Selection", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7591296434402466}, {"text": "Conditional Maximum Entropy Modeling", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.6718984842300415}]}], "abstractContent": [{"text": "This paper describes a fast algorithm that selects features for conditional maximum en-tropy modeling.", "labels": [], "entities": [{"text": "conditional maximum en-tropy modeling", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.5751651525497437}]}, {"text": "(1996) presents an incremental feature selection (IFS) algorithm , which computes the approximate gains for all candidate features at each selection stage, and is very time-consuming for any problems with large feature spaces.", "labels": [], "entities": [{"text": "incremental feature selection (IFS)", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.6403697629769644}]}, {"text": "In this new algorithm, instead, we only compute the approximate gains for the top-ranked features based on the models obtained from previous stages.", "labels": [], "entities": []}, {"text": "Experiments on WSJ data in Penn Treebank are conducted to show that the new algorithm greatly speeds up the feature selection process while maintaining the same quality of selected features.", "labels": [], "entities": [{"text": "WSJ data in Penn Treebank", "start_pos": 15, "end_pos": 40, "type": "DATASET", "confidence": 0.875585401058197}, {"text": "feature selection", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.6714024543762207}]}, {"text": "One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features.", "labels": [], "entities": []}, {"text": "The new algorithm is easy to implement, and given a feature space of size F, it only uses O(F) more space than the original IFS algorithm.", "labels": [], "entities": [{"text": "O(F)", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9223706722259521}]}], "introductionContent": [{"text": "Maximum Entropy (ME) modeling has received a lot of attention in language modeling and natural language processing for the past few years (e.g.,.", "labels": [], "entities": [{"text": "Maximum Entropy (ME) modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5306810190280279}, {"text": "language modeling", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7304043173789978}]}, {"text": "One of the main advantages using ME modeling is the ability to incorporate various features in the same framework with a sound mathematical foundation.", "labels": [], "entities": [{"text": "ME modeling", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9065663516521454}]}, {"text": "There are two main tasks in ME modeling: the feature selection process that chooses from a feature space a subset of good features to be included in the model; and the parameter estimation process that estimates the weighting factors for each selected feature in the exponential model.", "labels": [], "entities": [{"text": "ME modeling", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9721226096153259}]}, {"text": "This paper is primarily concerned with the feature selection process in ME modeling.", "labels": [], "entities": [{"text": "feature selection process", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7546097536881765}, {"text": "ME modeling", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9658119082450867}]}, {"text": "While the majority of the work in ME modeling has been focusing on parameter estimation, less effort has been made in feature selection.", "labels": [], "entities": [{"text": "ME modeling", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.975570023059845}, {"text": "parameter estimation", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.660128727555275}, {"text": "feature selection", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.6618009358644485}]}, {"text": "This is partly because feature selection may not be necessary for certain tasks when parameter estimate algorithms are fast.", "labels": [], "entities": []}, {"text": "However, when a feature space is large and complex, it is clearly advantageous to perform feature selection, which not only speeds up the probability computation and requires smaller memory size during its application, but also shortens the cycle of model selection during the training.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.6791333556175232}]}, {"text": "Feature selection is a very difficult optimization task when the feature space under investigation is large.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8872838318347931}]}, {"text": "This is because we essentially try to find a best subset from a collection of all the possible feature subsets, which has a size of 2 |W| , where |W| is the size of the feature space.", "labels": [], "entities": []}, {"text": "In the past, most researchers resorted to a simple count cutoff technique for selecting features, where only the features that occur in a corpus more than a predefined cutoff threshold get selected.", "labels": [], "entities": []}, {"text": "experimented on a feature selection technique that uses ac 2 test to see whether a feature should be included in the ME model, where the c 2 testis computed using the count from a prior distribution and the count from the real training data.", "labels": [], "entities": []}, {"text": "It is a simple and probably effective technique for language modeling tasks.", "labels": [], "entities": [{"text": "language modeling tasks", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.8610317707061768}]}, {"text": "Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship between c 2 test score and the likelihood gain, which, however, is absent.", "labels": [], "entities": []}, {"text": "presented an incremental feature selection (IFS) algorithm where only one feature is added at each selection and the estimated parameter values are kept for the features selected in the previous stages.", "labels": [], "entities": [{"text": "incremental feature selection (IFS)", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.6350846240917841}]}, {"text": "While this greedy search assumption is reasonable, the speed of the IFS algorithm is still an issue for complex tasks.", "labels": [], "entities": []}, {"text": "For better understanding its performance, we reimplemented the algorithm.", "labels": [], "entities": []}, {"text": "Given a task of 600,000 training instances, it takes nearly four days to select 1000 features from a feature space with a little more than 190,000 features.", "labels": [], "entities": []}, {"text": "proposed an f-orthogonal condition for selecting k features at the same time without affecting much the quality of the selected features.", "labels": [], "entities": []}, {"text": "While this technique is applicable for certain feature sets, such as word link features reported in their paper, the f-orthogonal condition usually does not hold if part-of-speech tags are dominantly present in a feature subset.", "labels": [], "entities": []}, {"text": "Past work, including and, has shown that the IFS algorithm utilizes much fewer features than the count cutoff method, while maintaining the similar precision and recall on tasks, such as prepositional phrase attachment, text categorization and base NP chunking.", "labels": [], "entities": [{"text": "IFS", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9241358041763306}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9992263317108154}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.997963547706604}, {"text": "prepositional phrase attachment", "start_pos": 187, "end_pos": 218, "type": "TASK", "confidence": 0.6142220000425974}, {"text": "base NP chunking", "start_pos": 244, "end_pos": 260, "type": "TASK", "confidence": 0.6204746067523956}]}, {"text": "This leads us to further explore the possible improvement on the IFS algorithm.", "labels": [], "entities": [{"text": "IFS", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9106524586677551}]}, {"text": "In section 2, we briefly review the IFS algorithm.", "labels": [], "entities": [{"text": "IFS", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9028241634368896}]}, {"text": "Then, a fast feature selection algorithm is described in section 3.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7130560129880905}]}, {"text": "Section 4 presents a number of experiments, which show a massive speed-up and quality feature selection of the new algorithm.", "labels": [], "entities": []}, {"text": "Finally, we conclude our discussion in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "A number of experiments have been conducted to verify the rationale behind the algorithm.", "labels": [], "entities": []}, {"text": "In particular, we would like to have a good understanding of the quality of the selected features using the SGC algorithm, as well as the amount of speedups, in comparison with the IFS algorithm.", "labels": [], "entities": []}, {"text": "The first sets of experiments use a dataset {(x, y)}, derived from the Penn Treebank, where x is a 10 dimension vector including word, POS tag and grammatical relation tag information from two adjacent regions, and y is the grammatical relation tag between the two regions.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.995201826095581}]}, {"text": "Examples of the grammatical relation tags are subject and object with either the right region or the left region as the head.", "labels": [], "entities": []}, {"text": "The total number of different grammatical tags, i.e., the size of the output space, is 86.", "labels": [], "entities": []}, {"text": "There area little more than 600,000 training instances generated from section 02-22 of WSJ in Penn Treebank, and the test corpus is generated from section 23.", "labels": [], "entities": [{"text": "WSJ in Penn Treebank", "start_pos": 87, "end_pos": 107, "type": "DATASET", "confidence": 0.8192255198955536}]}, {"text": "In our experiments, the feature space is partitioned into sub-spaces, called feature templates, where only certain dimensions are included.", "labels": [], "entities": []}, {"text": "Considering all the possible combinations in the 10-dimensional space would lead to 2 10 feature templates.", "labels": [], "entities": []}, {"text": "To perform a feasible and fair comparison, we use linguistic knowledge to filter out implausible subspaces so that only 24 feature templates are actually used.", "labels": [], "entities": []}, {"text": "With this amount of feature templates, we get more than 1,900,000 candidate features from the training data.", "labels": [], "entities": []}, {"text": "To speedup the experiments, which is necessary for the IFS algorithm, we use a cutoff of 5 to reduce the feature space down to 191,098 features.", "labels": [], "entities": [{"text": "IFS", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9408003687858582}]}, {"text": "On average, each candidate feature covers about 485 instances, which accounts for 0.083% over the whole training instance set and is computed through: The first experiment is to compare the speed of the IFS algorithm with that of SGC algorithm.", "labels": [], "entities": []}, {"text": "Theoretically speaking, the IFS algorithm computes the gains for all the features at every stage.", "labels": [], "entities": []}, {"text": "This means that it requires O(NF) time to select a feature subset of size N from a candidate feature set of size F.", "labels": [], "entities": [{"text": "O(NF) time", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9083574771881103}]}, {"text": "On the other hand, the SGC algorithm considers much fewer features, only 24.1 features on average at each stage, when selecting a feature from the large feature space in this experiment.", "labels": [], "entities": [{"text": "SGC", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9047580361366272}]}, {"text": "shows the average number of features computed at the selected points for the SGC algorithm, SGC with 500 look-ahead, as well as the IFS algorithm.", "labels": [], "entities": []}, {"text": "The averaged number of features is taken over an interval from the initial stage to the current feature selection point, which is to smooth out the fluctuation of the numbers of features each selection stage considers.", "labels": [], "entities": []}, {"text": "The second algorithm looks at an additional fixed number of features, 500 in this experiment, beyond the ones considered by the basic SGC algorithm.", "labels": [], "entities": []}, {"text": "The last algorithm has a linear decreasing number of features to select, because the selected features will not be considered again.", "labels": [], "entities": []}, {"text": "In, the IFS algorithm stops after 1000 features are selected.", "labels": [], "entities": [{"text": "IFS", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.7244991660118103}]}, {"text": "This is because it takes too long for this algorithm to complete the entire selection process.", "labels": [], "entities": []}, {"text": "The same thing happens in, which is to be explained below.", "labels": [], "entities": []}, {"text": "To seethe actual amount of time taken by the SGC algorithms and the IFS algorithm with the currently available computing power, we use a Linux workstation with 1.6Ghz dual Xeon CPUs and 1 GB memory to run the two experiments simultaneously.", "labels": [], "entities": []}, {"text": "As it can be expected, excluding the beginning common part of the code from the two algorithms, the speedup from using the SGC algorithm is many orders of magnitude, from more than 100 times to thousands, depending on the number of features selected.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "To verify the quality of the selected features using our SGC algorithm, we conduct four experiments: one uses all the features to build a conditional ME model, the second uses the IFS algorithm to select 1,000 features, the third uses our SGC algorithm, the fourth uses the SGC algorithm with 500 look-ahead, and the fifth takes the top n most frequent features in the training data.", "labels": [], "entities": []}, {"text": "The precisions are computed on section 23 of the WSJ data set in Penn Treebank.", "labels": [], "entities": [{"text": "precisions", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9988545179367065}, {"text": "WSJ data set", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9895269672075907}, {"text": "Penn Treebank", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8207097053527832}]}, {"text": "The results are listed in.", "labels": [], "entities": []}, {"text": "Three factors can be learned from this figure.", "labels": [], "entities": []}, {"text": "First, the three IFS and SGC algorithms perform similarly.", "labels": [], "entities": []}, {"text": "Second, 3000 seems to be a dividing line: when the models include fewer than 3000 selected features, the IFS and SGC algorithms do not perform as well as the model with all the features; when the models include more than 3000 selected features, their performance significantly surpass the model with all the features.", "labels": [], "entities": []}, {"text": "The inferior performance of the model with all the features at the right side of the chart is likely due to the data over-fitting problem.", "labels": [], "entities": []}, {"text": "Third, the simple count cutoff algorithm significantly underperforms the other feature selection algorithms when feature subsets with no more than 10,000 features are considered.", "labels": [], "entities": []}, {"text": "To further confirm the findings regarding precision, we conducted another experiment with Base NP recognition as the task.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9992254972457886}, {"text": "Base NP recognition", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7398712237675985}]}, {"text": "The experiment uses section 15-18 of WSJ as the training data, and section 20 as the test data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9565705060958862}]}, {"text": "When we select 1,160 features from a simple feature space using our SGC algorithm, we obtain a precision/recall of 92.75%/93.25%.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9996618032455444}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8929097652435303}]}, {"text": "The best reported ME work on this task includes that has the precision/recall of 92.84%/93.18% with a cutoff of 5, and has reached the performance of 93.04%/93.31% with cutoff of 7 and reached a performance of 92.46%/92.74% with 615 features using the IFS algorithm.", "labels": [], "entities": [{"text": "ME", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9766544103622437}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9995716214179993}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9644307494163513}]}, {"text": "While the results are not directly comparable due to different feature spaces used in the above experiments, our result is competitive to these best numbers.", "labels": [], "entities": []}, {"text": "This shows that our new algorithm is both very effective in selecting high quality features and very efficient in performing the task.", "labels": [], "entities": []}], "tableCaptions": []}