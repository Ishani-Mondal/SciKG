{"title": [{"text": "Question Answering on a Case Insensitive Corpus", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7985625267028809}]}], "abstractContent": [{"text": "Most question answering (QA) systems rely on both keyword index and Named Entity (NE) tagging.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 5, "end_pos": 28, "type": "TASK", "confidence": 0.8806688070297242}]}, {"text": "The corpus from which the QA systems attempt to retrieve answers is usually mixed case text.", "labels": [], "entities": []}, {"text": "However, there are numerous corpora that consist of case insensitive documents, e.g. speech recognition results.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8093080818653107}]}, {"text": "This paper presents a successful approach to QA on a case insensitive corpus, whereby a preprocessing module is designed to restore the case-sensitive form.", "labels": [], "entities": [{"text": "QA", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9891183376312256}]}, {"text": "The document pool with the restored case then feeds the QA system, which remains unchanged.", "labels": [], "entities": []}, {"text": "The case restoration preprocessing is implemented as a Hidden Markov Model trained on a large raw corpus of case sensitive documents.", "labels": [], "entities": [{"text": "case restoration preprocessing", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.9001932740211487}]}, {"text": "It is demonstrated that this approach leads to very limited degradation in QA benchmarking (2.8%), mainly due to the limited degradation in the underlying information extraction support.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.7437709867954254}]}], "introductionContent": [{"text": "Natural language Question Answering (QA) is recognized as a capability with great potential.", "labels": [], "entities": [{"text": "Natural language Question Answering (QA)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7822189245905194}]}, {"text": "The NIST-sponsored Text Retrieval Conference (TREC) has been the driving force for developing this technology through its QA track since TREC-8].", "labels": [], "entities": [{"text": "NIST-sponsored Text Retrieval Conference (TREC)", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.7311452116285052}]}, {"text": "There has been significant progress and interest in QA research in recent years [.", "labels": [], "entities": [{"text": "QA research", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9247812926769257}]}, {"text": "In real-life QA applications, a system should be robust enough to handle diverse textual media degraded to different degrees.", "labels": [], "entities": []}, {"text": "One of the challenges from degraded text is the treatment of case insensitive documents such as speech recognition results, broadcast transcripts, and the Foreign Broadcast Information Service (FBIS) sources.", "labels": [], "entities": []}, {"text": "In the intelligence domain, the majority of archives consist of documents in all uppercase.", "labels": [], "entities": []}, {"text": "The orthographic case information for written text is an important information source.", "labels": [], "entities": []}, {"text": "In particular, the basic information extraction (IE) support for QA, namely Named Entity (NE) tagging, relies heavily on the case information for recognizing proper names.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8605276465415954}, {"text": "Named Entity (NE) tagging", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6672073602676392}]}, {"text": "Almost all NE systems (e.g.,]) utilize case-related features.", "labels": [], "entities": []}, {"text": "When this information is not available, if the system is not retrained or adapted, serious performance degradation will occur.", "labels": [], "entities": []}, {"text": "In the case of the statistical NE tagger, without adaptation the system simply does notwork.", "labels": [], "entities": [{"text": "NE tagger", "start_pos": 31, "end_pos": 40, "type": "TASK", "confidence": 0.7125894725322723}]}, {"text": "The degradation for proper name NE tagging is more than 70% based on our testing.", "labels": [], "entities": [{"text": "NE tagging", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.8329162895679474}]}, {"text": "The key issue here is how to minimize the performance degradation by adopting some strategy for the system adaptation.", "labels": [], "entities": []}, {"text": "For search engines, the case information is often ignored in keyword indexing and retrieval for the sake of efficiency and robustness/recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9956476092338562}]}, {"text": "However, QA requires fine-grained text processing beyond keyword indexing since, instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level is expected to be returned in response to a query.", "labels": [], "entities": [{"text": "keyword indexing", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7432349920272827}]}, {"text": "Typically QA is supported by Natural Language Processing (NLP) and IE] [.", "labels": [], "entities": []}, {"text": "Examples of using NLP and IE in Question Answering include shallow parsing], deep parsing [ [, and IE [.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7628247439861298}, {"text": "deep parsing", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.6041091233491898}, {"text": "IE", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.8899278044700623}]}, {"text": "Almost all state-of-the-art QA systems rely on NE in searching for candidate answers.", "labels": [], "entities": []}, {"text": "For a system based on language models, a feature exclusion approach is used to re-train the models, excluding features related to the case information]].", "labels": [], "entities": []}, {"text": "In particular, the DARPA HUB-4 program evaluates NE systems on speech recognizer output in SNOR (Standard Normalized Orthographic Representation) that is case insensitive and has no punctuations].", "labels": [], "entities": []}, {"text": "Research on case insensitive text has so far been restricted to NE and the feature exclusion approach [].", "labels": [], "entities": [{"text": "NE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.7027716636657715}]}, {"text": "When we examine a system beyond the shallow processing of NE, the traditional feature exclusion approach may not be feasible.", "labels": [], "entities": []}, {"text": "A sophisticated QA system usually involves several components with multiple modules, involving NLP/IE processing at various levels.", "labels": [], "entities": [{"text": "QA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9568051099777222}]}, {"text": "Each processing module may involve some sort of case information as constraints.", "labels": [], "entities": []}, {"text": "It is too costly and sometimes impossible to maintain two versions of a multi-module QA system for the purpose of handling two types of documents, with or without case.", "labels": [], "entities": []}, {"text": "This paper presents a case restoration approach to this problem, as applied to QA.", "labels": [], "entities": [{"text": "case restoration", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.6817911565303802}]}, {"text": "The focus is to study the feasibility of QA on a case insensitive corpus using the presented case restoration approach.", "labels": [], "entities": [{"text": "QA", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9906432032585144}, {"text": "case restoration", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.7179786711931229}]}, {"text": "For this purpose, we use an existing QA system as the baseline in experiments; we are not concerned with enhancing the QA system itself.", "labels": [], "entities": []}, {"text": "A preprocessing module is designed to restore the case-sensitive form to feed to this QA system.", "labels": [], "entities": []}, {"text": "The case restoration module is based on a Hidden Markov Model (HMM) trained on a large raw corpus of case sensitive documents, which are drawn from a given domain with no need for human annotation.", "labels": [], "entities": [{"text": "case restoration", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9208556115627289}]}, {"text": "With the plug-in of this preprocessing module, the entire QA system with its underlying NLP/IE components needs no change or adaptation in handling the case insensitive corpus.", "labels": [], "entities": []}, {"text": "Using the TREC corpus with the case information artificially removed, this approach has been benchmarked with very good results, leading to only 2.8% degradation in QA performance.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8169557750225067}]}, {"text": "In the literature, this is the first time a QA system is applied to case insensitive corpora.", "labels": [], "entities": []}, {"text": "Although the artificially-made case insensitive corpus is an easier case than some real life corpora from speech recognition, the insight and techniques gained in this research are helpful in further exploring solutions of spoken language QA.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7378089725971222}]}, {"text": "In addition, by using the TREC corpus and the TREC benchmarking standards, the QA degradation benchmarking is easy to interpret and to compare with other QA systems in the community.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9363040328025818}, {"text": "TREC benchmarking", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9213988482952118}]}, {"text": "The remaining text is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the QA system.", "labels": [], "entities": [{"text": "QA", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.8320389986038208}]}, {"text": "Section 3 describes the language model for case restoration.", "labels": [], "entities": [{"text": "case restoration", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.9025276303291321}]}, {"text": "Section 4 benchmarks the IE engine and Section 5 benchmarks the IE-supported QA application.", "labels": [], "entities": []}, {"text": "In both benchmarking sections, we compare the performance degradation from case sensitive input to case insensitive input.", "labels": [], "entities": []}, {"text": "Section 5 is the Conclusion.", "labels": [], "entities": [{"text": "Conclusion", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.940652072429657}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Case Restoration Performance", "labels": [], "entities": [{"text": "Case Restoration", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8095331788063049}]}, {"text": " Table 2: NE Degradation Benchmarking", "labels": [], "entities": [{"text": "NE Degradation Benchmarking", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.5863759517669678}]}, {"text": " Table 3: SVO/CE Degradation Benchmarking", "labels": [], "entities": [{"text": "SVO/CE Degradation Benchmarking", "start_pos": 10, "end_pos": 41, "type": "DATASET", "confidence": 0.5458136975765229}]}, {"text": " Table 4: QA Degradation Benchmarking-1", "labels": [], "entities": [{"text": "QA Degradation Benchmarking-1", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5543388724327087}]}, {"text": " Table 5: QA Degradation Benchmarking-2", "labels": [], "entities": [{"text": "QA Degradation Benchmarking-2", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5422581235567728}]}]}