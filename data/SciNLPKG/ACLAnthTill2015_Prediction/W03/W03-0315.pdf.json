{"title": [{"text": "Efficient Optimization for Bilingual Sentence Alignment Based on Linear Regression", "labels": [], "entities": [{"text": "Bilingual Sentence Alignment", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8430885871251425}]}], "abstractContent": [{"text": "This paper presents a study on optimizing sentence pair alignment scores of a bilingual sentence alignment module.", "labels": [], "entities": [{"text": "sentence pair alignment", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.6073764661947886}]}, {"text": "Five candidate scores based on perplexity and sentence length are introduced and tested.", "labels": [], "entities": []}, {"text": "Then a linear regression model based on those candidates is proposed and trained to predict sentence pairs' alignment quality scores solicited from human subjects.", "labels": [], "entities": []}, {"text": "Experiments are carried out on data automatically collected from Internet.", "labels": [], "entities": []}, {"text": "The correlation between the scores generated by the linear regression model and the scores from human subjects is in the range of the inter subject agreement score correlations.", "labels": [], "entities": []}, {"text": "Pear-son's correlation ranges from 0.53 up to 0.72 in our experiments.", "labels": [], "entities": [{"text": "correlation", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.8648597598075867}]}], "introductionContent": [{"text": "In many instances, multilingual natural language systems like machine translation systems are developed and trained on parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7198519855737686}]}, {"text": "When faced with a different, unseen text genre, however, translation performance usually drops noticeably.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9760172963142395}]}, {"text": "One way to remedy this situation is to adapt and retrain the system parameters based on bilingual data from the same source or at least a closely related source.", "labels": [], "entities": []}, {"text": "A bilingual sentence alignment program () is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Internet, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other.", "labels": [], "entities": [{"text": "bilingual sentence alignment", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.6158959170182546}]}, {"text": "The set of identified bilingual parallel sentence pairs is then added to the training set for parameter reestimation.", "labels": [], "entities": []}, {"text": "As is well known, text mined from the Internet is very noisy.", "labels": [], "entities": []}, {"text": "Even after careful html parsing and filtering for text size and language, the text from comparable html-page pairs still contains mismatches of content or non-parallel junk text, and the sentence order can be too different to be aligned.", "labels": [], "entities": [{"text": "html parsing", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7188805192708969}]}, {"text": "Together with a large mismatch of vocabulary, the aligned sentence pairs, which are extracted from these collected comparable html-page pairs, contain a number of low translation quality alignments.", "labels": [], "entities": []}, {"text": "These need to be removed before the retraining of the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9702457785606384}]}, {"text": "In this paper, we present an approach to automatically optimizing the alignment scores of such a bilingual sentence alignment program.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7348886728286743}]}, {"text": "The alignment score is a combination (by linear regression) of two word translation lexicon scores and three sentence length scores and predicts the translation quality scores from a set of human annotators.", "labels": [], "entities": []}, {"text": "We also present experiments analyzing how many different human scorers are needed for good prediction and also how many sentence pairs should be scored per human annotator.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: in section 2, the text mining system is briefly described.", "labels": [], "entities": [{"text": "text mining", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.805492103099823}]}, {"text": "In section 3, five sentence alignment models based on lexical information and sentence length are explained.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7172237932682037}]}, {"text": "In section 4, a regression model is proposed to combine the five models to get further improvement in predicting alignment quality.", "labels": [], "entities": [{"text": "predicting alignment", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8967187106609344}]}, {"text": "We describe alignment experiments in section 5, focusing on the correlation between the alignment scores predicted by the sentence alignment models and by humans.", "labels": [], "entities": []}, {"text": "Conclusions are given in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "1500 pairs of comparable html document pairs were obtained from bilingual web pages crawled from Internet.", "labels": [], "entities": []}, {"text": "After preprocessing, filtering, and sentence alignment, the alignment types were distributed as shown in.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7568044662475586}]}, {"text": "Ignoring the alignment type of insertion (0:1) and deletion (1:0), we extracted around 5941 parallel sentences., we seethe data is very noisy, containing a large portion of insertions (23.7%) and deletions (41.9%).", "labels": [], "entities": []}, {"text": "This is very different from the LDC XinHua pre-aligned collection provided by LDC, which is relatively clean.", "labels": [], "entities": [{"text": "LDC XinHua pre-aligned collection", "start_pos": 32, "end_pos": 65, "type": "DATASET", "confidence": 0.8780797868967056}, {"text": "LDC", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9545642137527466}]}, {"text": "For this set of English-Chinese bilingual sentences, we randomly selected 200 sentence pairs, focusing on Viterbi alignment scores below 12.0 from sentence alignment, which was an empirically determined threshold (The alignment scores here were purely reflecting the Model-1 parameters using equation).", "labels": [], "entities": [{"text": "Viterbi alignment scores", "start_pos": 106, "end_pos": 130, "type": "METRIC", "confidence": 0.81466144323349}]}, {"text": "Three human subjects then had to score the 'translation quality' of every sentence pair, using a 6 point scale described in section 4.2.", "labels": [], "entities": []}, {"text": "We further excluded very short sentences from consideration and evaluated 168 remaining sentences.", "labels": [], "entities": []}, {"text": "Pearson R correlation is applied to calculate the magnitude of the association between two variables (humanhuman or human-machine in our case) that are on an interval or ratio scale.", "labels": [], "entities": [{"text": "Pearson R correlation", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.6596861581007639}]}, {"text": "The correlation coefficients (Pearson R) between human subjects were in (all are statistically significant): ----0.568 Overall, more than 2/3 of the human scores are identical or differ by only 1 (between subjects).", "labels": [], "entities": [{"text": "Pearson R)", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9478680690129598}]}, {"text": "For the automatic score prediction, the five component scores described in section 4.1 are used, which are then combined using a standard Linear Regression as described in section 4.2.", "labels": [], "entities": [{"text": "automatic score prediction", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.5087566375732422}]}, {"text": "shows the correlation between alignment scores based on Model X and human subjects' predicted quality scores: The data we used in our training of the lexicon is Hong Kong news parallel data from LDC.", "labels": [], "entities": [{"text": "Hong Kong news parallel data from LDC", "start_pos": 161, "end_pos": 198, "type": "DATASET", "confidence": 0.9098286543573652}]}, {"text": "There are 290K parallel sentence pairs, with 7 million words of English and 7.3 million Chinese words after segmentation.", "labels": [], "entities": []}, {"text": "The IBM Model-1 for PP-1 and PP-2 are both trained using 5 EM iterations.", "labels": [], "entities": []}, {"text": "The other three length models are also calculated from the same 290K sentence pairs.", "labels": [], "entities": []}, {"text": "Punctuation is removed before the calculation of all automatic score prediction models.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9911208748817444}]}, {"text": "The regression model here is the standard linear regression using the observations from three human subjects as described in section 4.1.", "labels": [], "entities": []}, {"text": "The average performance of the regression model is shown in the bottom line of the above.", "labels": [], "entities": []}, {"text": "The average correla- Average of 3-human judgement 5-scale score (5-best, 0-non) Pearson r Correlation tion varies from 0.53 upto 0.72, which shows that the regression model has a very strong positive correlation with the human judgment.", "labels": [], "entities": [{"text": "Pearson r Correlation tion", "start_pos": 80, "end_pos": 106, "type": "METRIC", "confidence": 0.9739279448986053}]}, {"text": "Also from, we see both lexicon based models: PP-1 and PP-2 are better than the length models in term of correlation with human scorer.", "labels": [], "entities": []}, {"text": "Model PP-2 has the largest correlation, and is slightly better than PP-1.", "labels": [], "entities": []}, {"text": "PP-2 is based on the conditional probability of p(e|f), which models the generation of an English word from a Chinese word.", "labels": [], "entities": []}, {"text": "The vocabulary size of Chinese is usually smaller than English vocabulary size, so this model can be more reliably estimated than the reverse direction of p(f|e).", "labels": [], "entities": []}, {"text": "This explains why PP-2 is slightly better than PP-1.", "labels": [], "entities": []}, {"text": "For sentence length models, we see L-2, for which the lengths of both the English sentence and the Chinese sentence are measured in words, has the best performance among the three settings of a sentence length model.", "labels": [], "entities": []}, {"text": "This indicates that the length model measured in words is more reliable.", "labels": [], "entities": []}, {"text": "Also shown in, the na\u00efve interpolation of these different models, i.e. just using each model with equal weight, resulted in lower correlation than the best single alignment model.", "labels": [], "entities": [{"text": "correlation", "start_pos": 130, "end_pos": 141, "type": "METRIC", "confidence": 0.9670011401176453}]}, {"text": "We also performed correlation experiments with varied numbers of training sentences from either Human-1/Human-2/Human-3 or from all of the three human subjects.", "labels": [], "entities": []}, {"text": "We picked the first 30/60/90/120 labeled sentence pairs for training and saved the last 48 sentence pairs for testing.", "labels": [], "entities": []}, {"text": "The average performance of the regression model is as follows: The average correlation of the regression models showed here increased noticeably when the training set was increased from 30 sentence pairs to 90 sentence pairs.", "labels": [], "entities": []}, {"text": "More sentence pairs caused no or only marginal improvements (esp. for the third human subject).", "labels": [], "entities": []}, {"text": "shows a scatter plot, which illustrates a good correlation (here: Pearson R=0.74) between our regression model predictors and the human scorers.", "labels": [], "entities": [{"text": "Pearson R", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9750847816467285}]}], "tableCaptions": [{"text": " Table 2. Sentence length ratio statistics  L-1  L-2  L-3:  Mean  1.59  1.01  0.33  Var  3.82  0.79  0.71", "labels": [], "entities": [{"text": "Sentence length ratio", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.6873768866062164}, {"text": "Mean  1.59  1.01  0.33  Var  3.82  0.79  0.71", "start_pos": 60, "end_pos": 105, "type": "METRIC", "confidence": 0.8832703158259392}]}, {"text": " Table 3. Ignoring the alignment type of insertion (0:1)  and deletion (1:0), we extracted around 5941 parallel  sentences.", "labels": [], "entities": []}, {"text": " Table 3. Alignment types' distribution of mined  data from noisy web data crawled  1:0  0:1  1:1  2:1  1:2  2:2  3:1  % 23.7 41.9 29.4 1.99 0.01 0.02 2.79", "labels": [], "entities": []}, {"text": " Table 4. Correlation between Human Subjects  H2  H3  H1  0.786  0.615  H2", "labels": [], "entities": []}, {"text": " Table 5. Correlation between optimization models  and human subjects  Model  human-1  human -2  human -3  PP-1  .57  .53  .32  PP-2  .60  .58  .46  L-1  .42  .41  .30  L-2  .46  .41  .40  L-3  .40  .38  .29  Na\u00efve  .58  .56  .38  Regression .72  .68  .53", "labels": [], "entities": [{"text": "Regression", "start_pos": 231, "end_pos": 241, "type": "METRIC", "confidence": 0.9964182376861572}]}, {"text": " Table 6. Correlation between different training set  sizes and human scorers.  Training  set size", "labels": [], "entities": []}]}