{"title": [{"text": "Named Entity Recognition with Long Short-Term Memory", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7791094183921814}]}], "abstractContent": [{"text": "In this approach to named entity recognition, a recurrent neural network, known as Long Short-Term Memory, is applied.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6314506232738495}]}, {"text": "The network is trained to perform 2 passes on each sentence, outputting its decisions on the second pass.", "labels": [], "entities": []}, {"text": "The first pass is used to acquire information for dis-ambiguation during the second pass.", "labels": [], "entities": []}, {"text": "SARD-NET, a self-organising map for sequences is used to generate representations for the lexical items presented to the LSTM network, whilst orthogonal representations are used to represent the part of speech and chunk tags.", "labels": [], "entities": [{"text": "SARD-NET", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5703482627868652}]}], "introductionContent": [{"text": "In this paper, Long Short-Term Memory (LSTM)) is applied to named entity recognition, using data from the Reuters Corpus, English Language, Volume 1, and the European Corpus Initiative Multilingual Corpus 1.", "labels": [], "entities": [{"text": "Long Short-Term Memory (LSTM))", "start_pos": 15, "end_pos": 45, "type": "METRIC", "confidence": 0.7446896930535635}, {"text": "named entity recognition", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.6625936230023702}, {"text": "Reuters Corpus", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.9733526706695557}, {"text": "English Language, Volume 1", "start_pos": 122, "end_pos": 148, "type": "DATASET", "confidence": 0.8753803253173829}, {"text": "European Corpus Initiative Multilingual Corpus 1", "start_pos": 158, "end_pos": 206, "type": "DATASET", "confidence": 0.9474784235159556}]}, {"text": "LSTM is an architecture and training algorithm for recurrent neural networks (RNNs), capable of remembering information overlong time periods during the processing of a sequence.", "labels": [], "entities": []}, {"text": "LSTM was applied to an earlier CoNLL shared task, namely clause identification) although the performance was significantly below the performance of other methods, e.g. LSTM achieved an fscore of 50.42 on the test data where other systems' fscores ranged from 62.77 to 80.44.", "labels": [], "entities": [{"text": "clause identification", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8023107647895813}, {"text": "fscore", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9972658157348633}]}, {"text": "However, not all training data was used in training the LSTM networks.", "labels": [], "entities": []}, {"text": "Better performance has since been obtained where the complete training set was used (Hammerton, unpublished), yielding an fscore of 64.66 on the test data.", "labels": [], "entities": [{"text": "Hammerton, unpublished)", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.897177591919899}, {"text": "fscore", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.99930739402771}]}], "datasetContent": [{"text": "The LSTM networks used here were trained as follows: Each sentence is presented word byword in two passes.", "labels": [], "entities": []}, {"text": "The first pass is used to accumulate information for disambiguation in the second pass.", "labels": [], "entities": []}, {"text": "In the second pass the network is trained to output a vector representation (see) of the relevant output tag.", "labels": [], "entities": []}, {"text": "During the first pass the network is just trained to produce \"0.1\"s at all its outputs.", "labels": [], "entities": []}, {"text": "Note that the binary patterns listed in are converted to \"0.1\"s and \"0.9\"s when used as target patterns.", "labels": [], "entities": []}, {"text": "This technique has been found to improve performance.", "labels": [], "entities": []}, {"text": "The inputs to the networks are as follows:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Vector representations used for output tags", "labels": [], "entities": []}, {"text": " Table 2: Networks used in the experiments here.", "labels": [], "entities": []}, {"text": " Table 4: Performance of best network from", "labels": [], "entities": []}]}