{"title": [{"text": "Text Summarization Challenge 2 Text summarization evaluation at NTCIR Workshop 3", "labels": [], "entities": [{"text": "Text Summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7620510458946228}, {"text": "Text summarization", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7794353067874908}, {"text": "NTCIR Workshop", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9388362467288971}]}], "abstractContent": [{"text": "We describe the outline of Text Summarization Challenge 2 (TSC2 hereafter), a sequel text summarization evaluation conducted as one of the tasks at the NTCIR Workshop 3.", "labels": [], "entities": [{"text": "Text Summarization Challenge", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8470039367675781}, {"text": "text summarization evaluation", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.7123185197512308}, {"text": "NTCIR Workshop 3", "start_pos": 152, "end_pos": 168, "type": "DATASET", "confidence": 0.9221221009890238}]}, {"text": "First, we describe briefly the previous evaluation, Text Summarization Challenge (TSC1) as introduction to TSC2.", "labels": [], "entities": [{"text": "Text Summarization Challenge (TSC1)", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.8431606888771057}]}, {"text": "Then we explain TSC2 including the participants, the two tasks in TSC2, data used, evaluation methods for each task, and brief report on the results.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We use summaries prepared by human as key data for evaluation.", "labels": [], "entities": []}, {"text": "The same two intrinsic evaluation methods are used for both tasks.", "labels": [], "entities": []}, {"text": "They are evaluation by ranking summaries and by measuring the degree of revisions.", "labels": [], "entities": []}, {"text": "Here are the details of the two methods.", "labels": [], "entities": []}, {"text": "We use 30 articles for task A and 30 sets of documents (30 topics) for task B at formal run evaluation.", "labels": [], "entities": []}, {"text": "Unfortunately, due to the limitation of the budget, only an evaluator evaluates a system's result for an article(or a set).", "labels": [], "entities": []}, {"text": "This is basically the same as the evaluation method used for TSC1 task A-2 (subjective evaluation).", "labels": [], "entities": [{"text": "TSC1 task A-2", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7627538045247396}]}, {"text": "We ask human judges, who are experienced in producing summaries, to evaluate and rank the system summaries in terms of two points of views.", "labels": [], "entities": [{"text": "summaries", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.9559476375579834}]}, {"text": "1. Content: How much the system summary covers the important content of the original article.", "labels": [], "entities": []}, {"text": "2. Readability: How readable the system summary is.", "labels": [], "entities": []}, {"text": "The judges are given 4 types of summaries to be evaluated and rank them in 1 to 4 scale (1 is the best, 2 for the second, 3 for the third best, and 4 for the worst).", "labels": [], "entities": []}, {"text": "For task A, the first two types are human-produced abstract-type type1 and type2 summaries.", "labels": [], "entities": []}, {"text": "The third is system results, and the fourth is summaries produced by lead method.", "labels": [], "entities": []}, {"text": "For task B, the first is human-produced free summaries of the given set of documents, and the second is system results.", "labels": [], "entities": []}, {"text": "The third is the results of the baseline system based on lead method where the first sentence of each document is used.", "labels": [], "entities": []}, {"text": "The fourth is the results of the benchmark system using Stein method () whose procedure is as follows: 1.", "labels": [], "entities": []}, {"text": "Produce a summary for each document.", "labels": [], "entities": []}, {"text": "2. Group the summaries into several clusters.", "labels": [], "entities": []}, {"text": "The number of clusters is adjusted to be less than the half of the number of the documents.", "labels": [], "entities": []}, {"text": "3. Choose the most representative summary as the summary of the cluster.", "labels": [], "entities": []}, {"text": "4. Compute the similarity among the clusters and output the representative summaries in such order that the similarity of neighboring summaries is high.", "labels": [], "entities": [{"text": "similarity", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9551331996917725}]}, {"text": "It is a newly introduced evaluation method in TSC2 to evaluate the summaries by measuring the degree of revision to system results.", "labels": [], "entities": [{"text": "TSC2", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.5186899900436401}]}, {"text": "The judges read the original documents and revise the system summaries in terms of the content and readability.", "labels": [], "entities": []}, {"text": "The revisions are made by one of three editing operations (insertion, deletion, replacement).", "labels": [], "entities": []}, {"text": "The degree of the revision is computed based on the number of the operations and the number of revised characters.", "labels": [], "entities": [{"text": "degree", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9608079791069031}]}, {"text": "The revisers could be completely free in what they did, though they were instructed to do minimum revision.", "labels": [], "entities": []}, {"text": "As baseline for task A, lead-method results are used.", "labels": [], "entities": []}, {"text": "As reference for task A, human produced summaries (abstract type1 and abstract type 2) are used.", "labels": [], "entities": []}, {"text": "And as baseline, reference, and benchmark for task B, leadmethod results, human produced summaries that are different from the key data, and the results based on the Stein method are used respectively.", "labels": [], "entities": []}, {"text": "When more than half of the document needs to be revised, the judges can 'give up' revising the document.", "labels": [], "entities": []}, {"text": "Please note that UIM stands for unimportant, RD for readability, IM for important, C for content in Tables 6 to 9.", "labels": [], "entities": [{"text": "UIM", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9241624474525452}, {"text": "IM", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9907915592193604}]}, {"text": "They mean the reason for the operations, e.g. 'unimportant' is for deletion operation due to the part judged to be unimportant, and 'content' is for replacement operation due to excess and deficiency of content.", "labels": [], "entities": []}, {"text": "In, 'ld' means a baseline system using lead method, 'free' is free summaries produced by human (abstract type 2), and 'part' is human-produced (abstract type1) summaries, and these three are baseline and reference scores for task A.", "labels": [], "entities": []}, {"text": "In, 'human' means humanproduced summaries which are different from the key data, and 'ld' means a baseline system using lead method, 'stein' means a benchmark system using Stein method, and these three are baseline, reference, and benchmark scores for task B.", "labels": [], "entities": []}, {"text": "To determine the plausibility of the judges' revision, the revised summaries were again evaluated with the evaluation methods in section 5.", "labels": [], "entities": [{"text": "judges' revision", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.48539768159389496}]}, {"text": "In, `edit' means the evaluation results for the revised summaries.", "labels": [], "entities": [{"text": "edit", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9615074396133423}]}, {"text": "We also measure as degree of revision the number of revised characters for the three editing operations, and the number of documents that are given up revising by the judges.", "labels": [], "entities": []}, {"text": "Please look at the detailed data at NTCIR Workshop 3 data booklet.", "labels": [], "entities": [{"text": "NTCIR Workshop 3 data booklet", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.9817862391471863}]}, {"text": "indicates how much the scores for content and readability vary for the summaries of the same summarization rate.", "labels": [], "entities": []}, {"text": "It shows that the readability scores tend to be higher than those for content, and it is especially clearer for 40% summarization.", "labels": [], "entities": []}, {"text": "shows the differences in scores for the different summarization rates, i.e. 20% and 40% of task A.", "labels": [], "entities": []}, {"text": "'C20-C40' means the score for content 20% minus the score for content 40%.", "labels": [], "entities": []}, {"text": "'R20-R40' 'means the score for readability 20% minus the score for readability 40%.", "labels": [], "entities": [{"text": "R20-R40", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.8827810883522034}]}, {"text": "Evaluation by revision (task B short)  Second, consider task B. shows the differences in scores for content and readability for each system for task B.", "labels": [], "entities": []}, {"text": "'CS-RS' means the score for content short summaries minus the score for readability short summaries.", "labels": [], "entities": []}, {"text": "'CL-RL' is computed in the same way for long summaries.", "labels": [], "entities": []}, {"text": "We here further look into how the participating systems perform by analysing the ranking results in terms of differences in scores for content and those for readability.", "labels": [], "entities": []}, {"text": "First, consider task A. shows the differences in scores for content and readability for each system.", "labels": [], "entities": []}, {"text": "'C20-R20' means the score for content 20% minus the score for readability 20%.", "labels": [], "entities": []}, {"text": "'C40-R40' means the score for content 40% minus the score for readability 40%., that the scores for readability tend to be higher, thence, the differences are in minus values, than those for content for both short and long summaries.", "labels": [], "entities": []}, {"text": "In addition, the differences are larger than the differences we saw for task A, i.e. in. shows the differences in scores for the different summarization lengths, i.e. short and long summaries of task B.", "labels": [], "entities": []}, {"text": "'CS-CL' means the score for content short summaries minus the score for content long summaries.", "labels": [], "entities": []}, {"text": "'RS-RL' means the score for readability short summaries minus the score for readability long summaries.", "labels": [], "entities": [{"text": "RS-RL", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9846881031990051}]}, {"text": "tells us, unlike Figure2, the scores for short summaries tend to be lower than those for long summaries.", "labels": [], "entities": []}, {"text": "This tendency is very clear for the readability ranking scores.", "labels": [], "entities": []}, {"text": "show that when we compare the ranking scores for content and readability summaries, the readability scores tend to be higher than those for content, which means that the evaluation for readability is worse than that for content. and 4 shows contradicting tendencies.", "labels": [], "entities": []}, {"text": "indicates that short (20%) summaries are higher in ranking scores, i.e. worse in evaluation.", "labels": [], "entities": []}, {"text": "However, indicates the other way round.", "labels": [], "entities": []}, {"text": "Intuitively longer summaries can have better readability since they have more words to deal with, and it is shown in Figure2.", "labels": [], "entities": []}, {"text": "However, it is not the case with task B ranking results.", "labels": [], "entities": []}, {"text": "Longer summaries had worse scores, especially in readability evaluation.", "labels": [], "entities": []}, {"text": "To determine the plausibility of the judges' revision, the revised summaries were again evaluated with the evaluation methods in section 5.", "labels": [], "entities": [{"text": "judges' revision", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.48539768159389496}]}, {"text": "show, the degree of the revisions for the revised summaries is rather smaller than that for the original ones and is almost same as that for human summaries.", "labels": [], "entities": []}, {"text": "show the results of evaluation by ranking for the revised summaries at task A and B respectively.", "labels": [], "entities": []}, {"text": "Compared with show that the scores for the revised summaries are rather smaller than those for the original ones and are almost same as those for human summaries.", "labels": [], "entities": []}, {"text": "From these results, the quality of the revised summaries is considered as same as that of human summaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2 Ranking evaluation (task A)", "labels": [], "entities": []}, {"text": " Table 3 Ranking evaluation (task B)", "labels": [], "entities": []}, {"text": " Table 4 Ranking evaluation (task A, human and  baseline)", "labels": [], "entities": []}, {"text": " Table 5 Ranking evaluation (task B, human,  baseline, and benchmark)", "labels": [], "entities": []}, {"text": " Table 6 Evaluation by revision (task A 40%)", "labels": [], "entities": [{"text": "A", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.6583937406539917}]}, {"text": " Table 7 Evaluation by revision (task A 20%)", "labels": [], "entities": [{"text": "A", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.7342687845230103}]}, {"text": " Table 8 Evaluation by revision (task B long)", "labels": [], "entities": []}, {"text": " Table 9 Evaluation by revision (task B short)", "labels": [], "entities": []}, {"text": " Table 10 Ranking evaluation (task A)", "labels": [], "entities": []}, {"text": " Table 11 Ranking evaluation (task B)", "labels": [], "entities": []}]}