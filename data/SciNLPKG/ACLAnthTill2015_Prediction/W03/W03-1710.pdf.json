{"title": [{"text": "Modeling of Long Distance Context Dependency in Chinese", "labels": [], "entities": [{"text": "Modeling of Long Distance Context Dependency", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7100623349348704}]}], "abstractContent": [{"text": "Ngram modeling is simple in language modeling and has been widely used in many applications.", "labels": [], "entities": [{"text": "Ngram modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.718366876244545}, {"text": "language modeling", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.754316121339798}]}, {"text": "However, it can only capture the short distance context dependency within an N-word window where the largest practical N for natural language is three.", "labels": [], "entities": []}, {"text": "In the meantime, much of context dependency in natural language occurs beyond a three-word window.", "labels": [], "entities": []}, {"text": "In order to incorporate this kind of long distance context dependency, this paper proposes anew MI-Ngram modeling approach.", "labels": [], "entities": []}, {"text": "The MI-Ngram model consists of two components: an ngram model and an MI model.", "labels": [], "entities": []}, {"text": "The ngram model captures the short distance context dependency within an N-word window while the MI model captures the long distance context dependency between the word pairs beyond the N-word window by using the concept of mutual information.", "labels": [], "entities": [{"text": "MI", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.841858983039856}]}, {"text": "It is found that MI-Ngram modeling has much better performance than ngram modeling.", "labels": [], "entities": []}, {"text": "Evaluation on the XINHUA new corpus of 29 million words shows that inclusion of the best 1,600,000 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model.", "labels": [], "entities": [{"text": "XINHUA new corpus", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.9074063102404276}]}, {"text": "In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.5793046156565348}]}], "introductionContent": [{"text": "Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6981040835380554}]}, {"text": "Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation; Bai et al 1998;.", "labels": [], "entities": [{"text": "ngram modeling", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7160555869340897}, {"text": "speech recognition", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8063704669475555}, {"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8025460243225098}]}, {"text": "Although ngram modeling is simple in nature and easy to use, it has obvious deficiencies.", "labels": [], "entities": [{"text": "ngram modeling", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.6863337457180023}]}, {"text": "For instance, ngram modeling can only capture the short distance context dependency within an N-word window where currently the largest practical N for natural language is three.", "labels": [], "entities": []}, {"text": "In the meantime, it is found that there always exist many preferred relationships between words.", "labels": [], "entities": []}, {"text": "Two highly associated word pairs are \u4e0d\u4ec5/\u800c\u4e14 (\"not only/but also\") and \u533b \u751f / \u62a4 \u58eb (\"doctor/nurse\").", "labels": [], "entities": []}, {"text": "Psychological experiments in indicated that the human's reaction to a highly associated word pair was stronger and faster than that to a poorly associated word pair.", "labels": [], "entities": []}, {"text": "Such preference information is very useful for natural language processing (.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6478601197401682}]}, {"text": "Obviously, the preference relationships between words can expand from a short to long distance.", "labels": [], "entities": []}, {"text": "While we can use traditional ngram modeling to capture the short distance context dependency, the long distance context dependency should also be exploited properly.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to propose anew MI-Ngram modeling approach to capture the context dependency over both a short distance and along distance.", "labels": [], "entities": [{"text": "MI-Ngram modeling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7481601238250732}]}, {"text": "Experimentation shows that this new MI-Ngram modeling approach can significantly decrease the perplexity of the new MI-Ngram model compared with traditional ngram model.", "labels": [], "entities": []}, {"text": "In the meantime, evaluation on Chinese word segmentation shows that this new approach can significantly reduce the error rate.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.580474982659022}, {"text": "error rate", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9745814800262451}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the traditional ngram modeling approach and discuss its main property.", "labels": [], "entities": [{"text": "ngram modeling", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.6850776821374893}]}, {"text": "In section 3, we propose the new MI-Ngram modeling approach to capture context dependency over both a short distance and along distance.", "labels": [], "entities": []}, {"text": "In section 4, we measure the MI-Ngram modeling approach and evaluate its application in Chinese word segmentation.", "labels": [], "entities": [{"text": "MI-Ngram modeling", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7209010869264603}, {"text": "Chinese word segmentation", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.58549831310908}]}, {"text": "Finally we give a summary of this paper in section 5.", "labels": [], "entities": []}, {"text": "And the probability P can be estimated by using maximum likelihood estimation (MLE) principle: represents the number of times the sequence occurs in the training data.", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE)", "start_pos": 48, "end_pos": 83, "type": "METRIC", "confidence": 0.7945496886968613}]}, {"text": "In practice, due to the data sparseness problem, some smoothing techniques, such as linear interpolation) and back-off modeling), are applied.", "labels": [], "entities": []}], "datasetContent": [{"text": "As , it is conventional to use the relation when computing entropy.", "labels": [], "entities": []}], "tableCaptions": []}