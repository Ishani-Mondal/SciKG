{"title": [{"text": "Variation of Entropy and Parse Trees of Sentences as a Function of the Sentence Number", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we explore the variation of sentences as a function of the sentence number.", "labels": [], "entities": []}, {"text": "We demonstrate that while the entropy of the sentence increases with the sentence number, it decreases at the paragraph boundaries in accordance with the Entropy Rate Constancy principle (intro-duced in related work).", "labels": [], "entities": []}, {"text": "We also demonstrate that the principle holds for different genres and languages and explore the role of genre informativeness.", "labels": [], "entities": []}, {"text": "We investigate potential causes of entropy variation by looking at the tree depth, the branching factor, the size of constituents, and the occurrence of gapping.", "labels": [], "entities": []}], "introductionContent": [{"text": "All the work on this problem so far has focused on the Wall Street Journal articles.", "labels": [], "entities": [{"text": "Wall Street Journal articles", "start_pos": 55, "end_pos": 83, "type": "DATASET", "confidence": 0.9666491448879242}]}, {"text": "The results are thus naturally suspect; perhaps the observed effect is simply an artifact of the journalistic writing style.", "labels": [], "entities": []}, {"text": "To address this criticism, we need to perform comparable experiments on another genre.", "labels": [], "entities": []}, {"text": "Wall Street Journal is a fairly prototypical example of a news article, or, more generally, a writing with a primarily informative purpose.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9794271985689799}]}, {"text": "One obvious counterpart of such a genre is fiction 4 . Another alternative might be to use transcripts of spoken dialogue.", "labels": [], "entities": []}, {"text": "Unfortunately, works of fiction, are either nonhomogeneous (collections of works) or relatively short with relatively long subdivisions (chapters).", "labels": [], "entities": []}, {"text": "This is crucial, since in the sentence number experiments we obtain one data point per article, therefore it is impossible to use book chapters in place of articles.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the whole Penn Treebank corpus as our data set.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.9956845243771871}]}, {"text": "This corpus contains about 50000 parsed sentences.", "labels": [], "entities": []}, {"text": "Many of the statistics we wish to compute are very sensitive to the length of the sentence.", "labels": [], "entities": []}, {"text": "For example, the depth of the tree is almost linearly related to the sentence length.", "labels": [], "entities": []}, {"text": "This is important because the average length of the sentence varies with the sentence number.", "labels": [], "entities": []}, {"text": "To make sure we exclude the effect of the sentence length, we need to normalize for it.", "labels": [], "entities": []}, {"text": "We proceed in the following way.", "labels": [], "entities": []}, {"text": "Let T be the set of trees, and f : T \u2192 R be some statistic of a tree.", "labels": [], "entities": []}, {"text": "Let l(t) be the length of the underlying sentence for Let L(n) = {t|l(t) = n} be the set of trees of size n.", "labels": [], "entities": []}, {"text": "Let L f (n) be defined as 1 , the average value of the statistic f on all sentences of length n.", "labels": [], "entities": []}, {"text": "We then define the sentence-lengthadjusted statistic, for all t, as The average value of the adjusted statistic is now equal to 1, and it is independent of the sentence length.", "labels": [], "entities": []}, {"text": "We can now report the average value of each statistic for each sentence number, as we have done before, but instead we will group the sentence numbers into a small number of \"buckets\" of exponentially increasing length 8 . We do so to capture the behavior for all the sentence numbers, and not just for the first ten (as before), as well as to lump together sentences with similar sentence numbers, for which we do not expect much variation.", "labels": [], "entities": []}, {"text": "For our experiments we use War and Peace (Tolstoy, 1869), since it is rather large and publicly available.", "labels": [], "entities": [{"text": "War and Peace (Tolstoy, 1869)", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.46093838289380074}]}, {"text": "It contains only about 365 rather long chapters . Unlike WSJ articles, each chapter is not written on a single topic, but usually has multiple topic shifts.", "labels": [], "entities": []}, {"text": "These shifts, however, are marked only as paragraph breaks.", "labels": [], "entities": []}, {"text": "We, therefore, have to assume that each paragraph break represents a topic shift, The experimental setup is very similar to the one used in Section 2.2.", "labels": [], "entities": []}, {"text": "We use roughly half of the data for training purposes and split the rest into testing sets, one per each sentence number, counted from the beginning of a paragraph.", "labels": [], "entities": []}, {"text": "We then evaluate the results using the same method as in Section 2.2.", "labels": [], "entities": []}, {"text": "We expect that the entropy would increase with the sentence number, just as in the case of the sentences numbered from the article boundary.", "labels": [], "entities": [{"text": "entropy", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9373800158500671}]}, {"text": "This effect is present, but is not very pronounced.", "labels": [], "entities": []}, {"text": "To make sure that it is statistically significant, we also do 1000 control runs for comparison, with paragraph breaks inserted randomly at the appropriate rate.", "labels": [], "entities": []}, {"text": "The results (including 3 random runs) can be seen in.", "labels": [], "entities": []}, {"text": "To make sure our results are significant we compare the correlation coefficient between entropy and sentence number to ones from simulated runs, and find them to be significant (P=0.016).", "labels": [], "entities": []}, {"text": "It is fairly clear that the variation, especially between the first and the later sentences, is greater than it would be expected fora purely random occurrence.", "labels": [], "entities": [{"text": "variation", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9791027903556824}]}, {"text": "We will see further evidence for this in the next section.", "labels": [], "entities": []}, {"text": "To further verify that this effect is significant and universal, it is necessary to do similar experiments in other languages.", "labels": [], "entities": []}, {"text": "Luckily, War and Peace is also digitally available in other languages, of which we pick Russian and Spanish for our experiments.", "labels": [], "entities": [{"text": "War and Peace", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.5932569404443105}]}, {"text": "We follow the same experimental procedure as in Section 3.1.2 and obtain the results for Russian) and Spanish).", "labels": [], "entities": []}, {"text": "We see that results are very similar to the ones we obtained for English.", "labels": [], "entities": []}, {"text": "The results are again significant for both Russian (P=0.004) and Spanish (P=0.028).", "labels": [], "entities": []}], "tableCaptions": []}