{"title": [{"text": "Virtual Examples for Text Classification with Support Vector Machines", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7777164280414581}]}], "abstractContent": [{"text": "We explore how virtual examples (artifi-cially created examples) improve performance of text classification with Support Vector Machines (SVMs).", "labels": [], "entities": [{"text": "text classification", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7543914914131165}]}, {"text": "We propose techniques to create virtual examples for text classification based on the assumption that the category of a document is unchanged even if a small number of words are added or deleted.", "labels": [], "entities": [{"text": "text classification", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.756382942199707}]}, {"text": "We evaluate the proposed methods by Reuters-21758 test set collection.", "labels": [], "entities": [{"text": "Reuters-21758 test set collection", "start_pos": 36, "end_pos": 69, "type": "DATASET", "confidence": 0.9677578806877136}]}, {"text": "Experimental results show virtual examples improve the performance of text classification with SVMs, especially for small training sets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8114223778247833}]}], "introductionContent": [{"text": "Corpus-based supervised learning is now a standard approach to achieve high-performance in natural language processing.", "labels": [], "entities": []}, {"text": "However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large.", "labels": [], "entities": []}, {"text": "Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus.", "labels": [], "entities": []}, {"text": "The problem is that corpus annotation is labor intensive and very expensive.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7900383770465851}]}, {"text": "In order to overcome this, several methods are proposed, including minimally-supervised learning methods (e.g.,), and active learning methods (e.g.,).", "labels": [], "entities": []}, {"text": "The spirit behind these methods is to utilize precious labeled examples maximally.", "labels": [], "entities": []}, {"text": "Another method following the same spirit is one using virtual examples (artificially created examples) generated from labeled examples.", "labels": [], "entities": []}, {"text": "This method has been rarely discussed in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6410235067208608}]}, {"text": "In terms of active learning, mentioned the use of virtual examples in text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7997056543827057}]}, {"text": "They did not, however, take forward this approach because it did not seem to be possible that a classifier created virtual examples of documents in natural language and then requested a human teacher to label them.", "labels": [], "entities": []}, {"text": "In the field of pattern recognition, some kind of virtual examples has been studied.", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8598676919937134}]}, {"text": "The first report of methods using virtual examples with Support Vector Machines (SVMs) is that of, who demonstrated significant improvement of the accuracy in hand-written digit recognition (Section 3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9992430210113525}, {"text": "hand-written digit recognition", "start_pos": 159, "end_pos": 189, "type": "TASK", "confidence": 0.5985087951024374}]}, {"text": "They created virtual examples from labeled examples based on prior knowledge of the task: slightly translated (e.g., 1 pixel shifted to the right) images have the same label (class) of the original image.", "labels": [], "entities": []}, {"text": "also discussed the use of prior knowledge by creating virtual examples and thereby expanding the effective training set size.", "labels": [], "entities": []}, {"text": "The purpose of this study is to explore the effectiveness of virtual examples in NLP, motivated by the results of.", "labels": [], "entities": []}, {"text": "To our knowledge, use of virtual examples in corpus-based NLP has never been studied so far.", "labels": [], "entities": []}, {"text": "It is, however, important to investigate this approach by which it is expected that we can alleviate the cost of corpus annotation.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.6931245625019073}]}, {"text": "In particular, we focus on virtual examples with Support Vector Machines, introduced by.", "labels": [], "entities": []}, {"text": "The reason for this is that SVM is one of most successful machine learning methods in NLP.", "labels": [], "entities": []}, {"text": "For example, NL tasks to which SVMs have been applied are text classification), chunking (), dependency analysis () and so forth.", "labels": [], "entities": [{"text": "text classification", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8002159595489502}, {"text": "dependency analysis", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8322000503540039}]}, {"text": "In this study, we choose text classification as a first case of the study of virtual examples in NLP because text classification in real world requires minimizing annotation cost, and it is not too complicated to perform some non-trivial experiments.", "labels": [], "entities": [{"text": "text classification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7609523832798004}, {"text": "text classification", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7367424517869949}]}, {"text": "Moreover, there are simple methods, which we propose, to generate virtual examples from labeled examples (Section 4).", "labels": [], "entities": []}, {"text": "We show how virtual examples can improve the performance of a classifier with SVM in text classification, especially for small training sets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7565170526504517}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Number of Training and Test Examples", "labels": [], "entities": []}, {"text": " Table 3: Comparison of Micro-Average F-measure of Different Methods. \"VSV\" means virtual SV.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.5839086174964905}]}, {"text": " Table 4: F-Measures for the Reuters Categories with the Original SVM. The hyphen '-' denotes the case  where F-measure cannot be computed because the classifier always says 'negative' and therefore its preci- sion is undefined. The scores in bold means that the score of the original SVM is better than that of SVM  with 4 Virtual SVs per SV (shown in", "labels": [], "entities": []}, {"text": " Table 5: F-Measures for the Reuters Categories with SVM with 4 Virtual SVs per SV. The scores in bold  means that the score of SVM with 4 Virtual SVs per SV is better than that of the original SVM (shown in", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9923179149627686}]}]}