{"title": [{"text": "Statistical Translation Alignment with Compositionality Constraints", "labels": [], "entities": [{"text": "Statistical Translation Alignment with Compositionality Constraints", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8048432866732279}]}], "abstractContent": [{"text": "This article presents a method for aligning words between translations, that imposes a compositionality constraint on alignments produced with statistical translation models.", "labels": [], "entities": []}, {"text": "Experiments conducted within the WPT-03 shared task on word alignment demonstrate the effectiveness of the proposed approach.", "labels": [], "entities": [{"text": "WPT-03 shared task", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.7463237245877584}, {"text": "word alignment", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7406428903341293}]}], "introductionContent": [{"text": "Since the pioneering work of the IBM machine translation team almost 15 years ago (, statistical methods have proven to be valuable tools in approaching the automation of translation.", "labels": [], "entities": [{"text": "IBM machine translation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.5622954467932383}]}, {"text": "Word alignments (WA) play a central role in the statistical modeling process, and reliable WA techniques are crucial in acquiring the parameters of the models).", "labels": [], "entities": [{"text": "Word alignments (WA)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8019834995269776}, {"text": "statistical modeling process", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.829798678557078}]}, {"text": "Yet, the very nature of these alignments, as defined in the IBM modeling approach (, lead to descriptions of the correspondences between sourcelanguage (SL) and target-language (TL) words of a translation that are often unsatisfactory, at least from a human perspective.", "labels": [], "entities": []}, {"text": "One notion that is typically evacuated in the statistical modeling process is that of compositionality: a fundamental assumption in statistical machine translation is that, ultimately, all the words of a SL segment S contribute to produce all the words of its TL translation T , at least to some degree.", "labels": [], "entities": [{"text": "statistical modeling", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7988998591899872}, {"text": "statistical machine translation", "start_pos": 132, "end_pos": 163, "type": "TASK", "confidence": 0.6406737665335337}]}, {"text": "While this makes perfect sense from a stochastic point of view, it contrasts with the hypothesis at the basis of most (if not all) other MT approaches, as well as with our natural intuitions about translation: that individual portions of the SL text produce individual TL portions autonomously, and that the final translation T is obtained by somehow piecing together these TL portions.", "labels": [], "entities": [{"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9514544010162354}]}, {"text": "In what follows, we show how re-integrating compositionality into the statistical translation word alignment process leads to better alignments.", "labels": [], "entities": [{"text": "statistical translation word alignment", "start_pos": 70, "end_pos": 108, "type": "TASK", "confidence": 0.6975376158952713}]}, {"text": "We first take a closer look at the \"standard\" statistical WA techniques in section 2, and then propose away of imposing a compositionality constraint on these techniques in section 3.", "labels": [], "entities": []}, {"text": "In section 4, we discuss various implementation issues, and finally present the experimental results of this approach on the WPT-03 shared task on WA in section 5.", "labels": [], "entities": [{"text": "WPT-03 shared task on WA", "start_pos": 125, "end_pos": 149, "type": "DATASET", "confidence": 0.7774077534675599}]}, {"text": "define a word alignment as a vector a = a 1 ...a m that connects each word of a sourcelanguage text S = s 1 ...s m to a target-language word in its translation T = t 1 ...t n , with the interpretation that word t aj is the translation of word s j in S (a j = 0 is used to denote words of s that do not produce anything in T ).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.6906259059906006}]}], "datasetContent": [{"text": "The different word-alignment methods described in sections 2 and 3 were run on the test corpora of the WPT-03 shared task on alignment.", "labels": [], "entities": [{"text": "WPT-03 shared task on alignment", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.7000776052474975}]}, {"text": "Results were evaluated in terms of alignment precision (P), recall (R), F-measure and alignment error rate (AER)).", "labels": [], "entities": [{"text": "alignment precision (P)", "start_pos": 35, "end_pos": 58, "type": "METRIC", "confidence": 0.9210537910461426}, {"text": "recall (R)", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.968728318810463}, {"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9976551532745361}, {"text": "alignment error rate (AER))", "start_pos": 86, "end_pos": 113, "type": "METRIC", "confidence": 0.9626521666844686}]}, {"text": "As specified in the shared task description, all of these metrics were computed taking null-alignments into account (i.e. tokens left unconnected in an alignment were actually counted as aligned to virtual word token \"0\").", "labels": [], "entities": []}, {"text": "The results of our experiments are reproduced in table 2.", "labels": [], "entities": []}, {"text": "We observe that imposing a \"contiguous compositionality\" constraint (C and RC methods) allows for substantial gains with regard to plain Viterbi alignments (V and RV respectively), especially in terms of precision and AER (a slight decline in recall can be observed between the V and C methods on the ro-en corpus, but it is not clear whether this is significant).", "labels": [], "entities": [{"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9994912147521973}, {"text": "AER", "start_pos": 218, "end_pos": 221, "type": "METRIC", "confidence": 0.9994276165962219}, {"text": "recall", "start_pos": 243, "end_pos": 249, "type": "METRIC", "confidence": 0.9980095028877258}, {"text": "ro-en corpus", "start_pos": 301, "end_pos": 313, "type": "DATASET", "confidence": 0.7296531349420547}]}, {"text": "These gains are even more interesting when one considers that all pairs of alignments (V and C, RV and RC) are obtained using exactly the same data.", "labels": [], "entities": []}, {"text": "This highlights both the deficiencies of IBM Model-2 and the importance of compositionality.", "labels": [], "entities": []}, {"text": "Using both the forward and reverse models (CC) yields yet more gains with regard to all metrics.", "labels": [], "entities": []}, {"text": "This result is interesting, because it shows the potential of the compositional alignment method for integrating various sources of information.", "labels": [], "entities": [{"text": "compositional alignment", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8643329739570618}]}, {"text": "With regard to language pairs, it is interesting to note that all alignment methods produce figures that are substantially better in recall and worse in precision on the roen data, compared to en-fr.", "labels": [], "entities": [{"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9980736970901489}, {"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9991260170936584}]}, {"text": "Overall, ro-en alignments display significantly higher F-measures.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9971714615821838}]}, {"text": "This is surprising, considering that the provided en-fr corpus contained 20 times more training material.", "labels": [], "entities": []}, {"text": "This phenomenon is likely due to the fact that the en-fr test reference contains much more alignments per word (1.98 per target word) than the ro-en (1.12).", "labels": [], "entities": []}, {"text": "All alignment methods described here produce roughly between 1 and 1.25 alignments per target words.", "labels": [], "entities": []}, {"text": "This fact affects recall and F-measure figures positively on the ro-en test, while precision and AER (which correlates strongly with precision in practice) are affected inversely.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9993880987167358}, {"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9954068660736084}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9997320771217346}, {"text": "AER", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9994150400161743}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9987475872039795}]}], "tableCaptions": [{"text": " Table 1: WPT-03 shared task resources", "labels": [], "entities": [{"text": "WPT-03 shared task", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.5387977361679077}]}]}