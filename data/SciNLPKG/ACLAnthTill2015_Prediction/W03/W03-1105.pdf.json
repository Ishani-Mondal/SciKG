{"title": [{"text": "Poisson Naive Bayes for Text Classification with Feature Weighting", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8415042161941528}]}], "abstractContent": [{"text": "In this paper, we investigate the use of multivariate Poisson model and feature weighting to learn naive Bayes text clas-sifier.", "labels": [], "entities": []}, {"text": "Our new naive Bayes text classification model assumes that a document is generated by a multivariate Poisson model while the previous works consider a document as a vector of binary term features based on the presence or absence of each term.", "labels": [], "entities": [{"text": "text classification", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7285750508308411}]}, {"text": "We also explore the use of feature weighting for the naive Bayes text classification rather than feature selection, which is a quite costly process when a small number of the new training documents are continuously provided.", "labels": [], "entities": [{"text": "Bayes text classification", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.545265773932139}, {"text": "feature selection", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.6459325402975082}]}, {"text": "Experimental results on the two test collections indicate that our new model with the proposed parameter estimation and the feature weighting technique leads to substantial improvements compared to the unigram language model classifiers that are known to outperform the original pure naive Bayes text classifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The naive Bayes classifier has been one of the core frameworks in the information retrieval research for many years.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.8084795773029327}]}, {"text": "Recently, naive Bayes is emerged as a research topic itself because it sometimes achieves good performances on various tasks, compared to more complex learning algorithms, in spite of the wrong independence assumptions on naive Bayes.", "labels": [], "entities": []}, {"text": "Similarly, naive Bayes is also an attractive approach in the text classification task because it is simple enough to be practically implemented even with a great number of features.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.876075029373169}]}, {"text": "This simplicity enables us to integrate the text classification and filtering modules with the existing information retrieval systems easily.", "labels": [], "entities": [{"text": "text classification and filtering", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7582725584506989}]}, {"text": "It is because that the frequency related information stored in the general text retrieval systems is all the required information in naive Bayes learning.", "labels": [], "entities": []}, {"text": "No further complex generalization processes are required unlike the other machine learning methods such as SVM or boosting.", "labels": [], "entities": []}, {"text": "Moreover, incremental adaptation using a small number of new training documents can be performed by just adding or updating frequencies.", "labels": [], "entities": []}, {"text": "Several earlier works have extensively studied the naive Bayes text classification).", "labels": [], "entities": [{"text": "Bayes text classification", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.5733910799026489}]}, {"text": "However, their pure naive Bayes classifiers considered a document as a binary feature vector, and so they can't utilize the term frequencies in a document, resulting the poor performances.", "labels": [], "entities": []}, {"text": "For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers).", "labels": [], "entities": []}, {"text": "Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers, support vector machines, and boosting), etc.", "labels": [], "entities": []}, {"text": "In the real world, an operational text classification system is usually placed in the environment where the amount of human-annotated training documents is small in spite of the hundreds of thousands classes.", "labels": [], "entities": [{"text": "text classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7020402401685715}]}, {"text": "Moreover, re-training of the text classifiers is required since a small number of new training documents are continuously provided.", "labels": [], "entities": []}, {"text": "In this environment, naive Bayes is probably the most appropriate model for the practical systems rather than other complex learning models.", "labels": [], "entities": []}, {"text": "Therefore, more intensive studies about the naive Bayes text classification model are required.", "labels": [], "entities": [{"text": "Bayes text classification", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.5776681303977966}]}, {"text": "In this paper, we revisit the naive Bayes framework, and propose a Poisson naive Bayes model for text classification with a statistical feature weighting method.", "labels": [], "entities": [{"text": "text classification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8129364848136902}]}, {"text": "Feature weighting has many advantages compared to the previous feature selection approaches, especially when the new training examples are continuously provided.", "labels": [], "entities": [{"text": "Feature weighting", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8237296640872955}]}, {"text": "Our new model assumes that a document is generated by a multivariate Poisson model, and their parameters are estimated by weighted averaging of the normalized and smoothed term frequencies overall the training documents.", "labels": [], "entities": []}, {"text": "Under the assumption, we have tested the feature weighting approach with three measures: information gain, \ud97b\udf59 \u00be -statistic, and newly introduced probability ratio.", "labels": [], "entities": [{"text": "information gain", "start_pos": 89, "end_pos": 105, "type": "METRIC", "confidence": 0.7914531528949738}, {"text": "\ud97b\udf59 \u00be -statistic", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.8678271323442459}]}, {"text": "With the proposed model and feature weighting techniques, we can get much better performance without losing the simplicity and efficiency of the naive Bayes model.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9652750492095947}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section presents a naive Bayes framework for the text classification briefly.", "labels": [], "entities": [{"text": "text classification", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7137195765972137}]}, {"text": "Section 3 describes our new naive Bayes model and the proposed technique, and the experimental results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper by suggesting possible directions for future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were performed on the two datasets: Reuters21578 and KoreanNews2002 collection.", "labels": [], "entities": [{"text": "Reuters21578", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9802380800247192}, {"text": "KoreanNews2002 collection", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.9861557483673096}]}, {"text": "Reuters21578 collection is the most widely used benchmark dataset for the text categorization research.", "labels": [], "entities": [{"text": "Reuters21578 collection", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.981345534324646}, {"text": "text categorization", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7683693766593933}]}, {"text": "We have used \"ModApte\" split version, which consists of 9603 training documents and 3299 test documents.", "labels": [], "entities": []}, {"text": "There are 90 categories, and each document has one or more of the categories.", "labels": [], "entities": []}, {"text": "We have built another benchmark collection -KoreanNews2002 collection.", "labels": [], "entities": [{"text": "KoreanNews2002 collection", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.9887236654758453}]}, {"text": "KoreanNews2002 collection is composed of 15,000 news articles published during the year of 2002.", "labels": [], "entities": [{"text": "KoreanNews2002 collection", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.9906652569770813}]}, {"text": "The articles are collected from a number of Korean news portal websites, and each article is labeled with exactly one of the 46 classes.", "labels": [], "entities": []}, {"text": "All the documents have date stamps attached and have been ordered according to their date stamps.", "labels": [], "entities": [{"text": "date", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9341721534729004}]}, {"text": "With this date order, we divided them into the former 10,000 documents for training and the latter 5,000 documents for testing.", "labels": [], "entities": []}, {"text": "The performances are evaluated using popular F1 measure, and the F1 values for each class are microaveraged(MicroF1) and macro-averaged(MacroF1) to examine the general classification performances.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9705242216587067}, {"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9982823133468628}]}, {"text": "shows the performances of our new model named Poisson naive Bayes(PNB) classifiers ac-  is a unigram model classifier (UM) which is also referred to multinomial naive Bayes classifier described in.", "labels": [], "entities": []}, {"text": "From this result, we can assume that there is no global optimal value of \u00ab, but each class has its own optimal \u00ab.", "labels": [], "entities": []}, {"text": "In our experiments, many of the classes have the highest F1 value when \u00ab is about 0.8 or 0.9 except some classes such as corn class which shows the highest F1 value at \u00ab \ud97b\udf59 \u00bc\ud97b\udf59\u00bf.", "labels": [], "entities": [{"text": "F1 value", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9800044298171997}, {"text": "F1", "start_pos": 156, "end_pos": 158, "type": "METRIC", "confidence": 0.9969214200973511}]}, {"text": "Similar results are obtained in the KoreanNews2002 collections.", "labels": [], "entities": [{"text": "KoreanNews2002 collections", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.9926562309265137}]}, {"text": "shows the MicroF1 and MacroF1 values of the unigram model classifiers and our PNB on the two collections, where PNB(min) and PNB(max) are the highest and lowest values at different \u00ab.", "labels": [], "entities": [{"text": "MicroF1", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.9099195003509521}, {"text": "PNB", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9415380954742432}, {"text": "PNB(max)", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.894846498966217}]}, {"text": "In any cases, PNB is superior to UM.", "labels": [], "entities": [{"text": "PNB", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.46476271748542786}]}], "tableCaptions": [{"text": " Table 2: Performances of UM and PNB on the  Reuters21578 collection  UM  PNB(min) PNB(max)  MicroF1 0.7212  0.7644  0.7706  MacroF1 0.3214  0.4227  0.4358", "labels": [], "entities": [{"text": "Reuters21578 collection  UM  PNB", "start_pos": 45, "end_pos": 77, "type": "DATASET", "confidence": 0.939210057258606}, {"text": "MicroF1 0.7212  0.7644  0.7706  MacroF1 0.3214  0.4227  0.4358", "start_pos": 93, "end_pos": 155, "type": "DATASET", "confidence": 0.9170591458678246}]}, {"text": " Table 3: Performances of UM and PNB on the Ko- reanNews2002 collection  UM  PNB(min) PNB(max)  MicroF1 0.6502  0.7031  0.7094  MacroF1 0.5208  0.5859  0.5949", "labels": [], "entities": [{"text": "Ko- reanNews2002 collection  UM  PNB", "start_pos": 44, "end_pos": 80, "type": "DATASET", "confidence": 0.895675520102183}, {"text": "MicroF1 0.6502  0.7031  0.7094  MacroF1 0.5208  0.5859  0.5949", "start_pos": 96, "end_pos": 158, "type": "DATASET", "confidence": 0.8964418843388557}]}, {"text": " Table 4: Summary of the performances on the Reuters21578 collection  UM  PNB  PNB-IG  PNB-CHI  PNB-PrR  PNB \u00a3  MicroF1 0.7212 0.7690 0.7971  0.8167  0.8190(+13.56%) 0.8341  MacroF1 0.3414 0.4307 0.5800  0.6601(+93.35%) 0.5899  0.6645", "labels": [], "entities": [{"text": "Reuters21578 collection  UM  PNB  PNB-IG  PNB-CHI  PNB-PrR  PNB \u00a3  MicroF1 0.7212 0.7690", "start_pos": 45, "end_pos": 133, "type": "DATASET", "confidence": 0.9400801112254461}]}, {"text": " Table 5: Summary of the performances on the KoreanNews2002 collection  UM  PNB  PNB-IG  PNB-CHI  PNB-PrR  PNB \u00a3  MicroF1 0.6502 0.7056 0.7114  0.7122  0.7409(+13.95%) 0.7438  MacroF1 0.5208 0.5906 0.6305(+21.06%) 0.5748  0.6119  0.6662", "labels": [], "entities": [{"text": "KoreanNews2002 collection  UM  PNB  PNB-IG  PNB-CHI  PNB-PrR  PNB \u00a3  MicroF1 0.6502", "start_pos": 45, "end_pos": 128, "type": "DATASET", "confidence": 0.9413877292112871}]}]}