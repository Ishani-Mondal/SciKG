{"title": [{"text": "Unsupervised learning of word sense disambiguation rules by estimating an optimum iteration number in the EM algorithm", "labels": [], "entities": [{"text": "Unsupervised learning of word sense disambiguation", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.5674526790777842}]}], "abstractContent": [{"text": "In this paper, we improve an unsuper-vised learning method using the Expectation-Maximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambigua-tion (WSD) problems.", "labels": [], "entities": [{"text": "text classification", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8183524906635284}, {"text": "word sense disambigua-tion (WSD)", "start_pos": 191, "end_pos": 223, "type": "TASK", "confidence": 0.7801572034756342}]}, {"text": "The improved method stops the EM algorithm at the optimum iteration number.", "labels": [], "entities": [{"text": "EM", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8678836822509766}]}, {"text": "To estimate that number, we propose two methods.", "labels": [], "entities": []}, {"text": "In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.7600324153900146}]}, {"text": "The score of our method is a match for the best public score of this task.", "labels": [], "entities": []}, {"text": "Furthermore, our methods were confirmed to be effective also for verb WSD problems .", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.7586855888366699}]}], "introductionContent": [{"text": "In this paper, we improve an unsupervised learning method using the Expectation-Maximization (EM) algorithm proposed by ( ) for text classification problems in order to apply it to word sense disambiguation (WSD) problems.", "labels": [], "entities": [{"text": "Expectation-Maximization (EM) algorithm", "start_pos": 68, "end_pos": 107, "type": "METRIC", "confidence": 0.8730369925498962}, {"text": "text classification", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7959848940372467}, {"text": "word sense disambiguation (WSD)", "start_pos": 181, "end_pos": 212, "type": "TASK", "confidence": 0.798484742641449}]}, {"text": "The original method works well, but often causes worse classification for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.8875197172164917}]}, {"text": "To avoid this, we propose two methods to estimate the optimum iteration number in the EM algorithm.", "labels": [], "entities": []}, {"text": "Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method.", "labels": [], "entities": []}, {"text": "This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually.", "labels": [], "entities": []}, {"text": "To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently()()().", "labels": [], "entities": []}, {"text": "Among these methods, the method using the EM algorithm proposed by the paper( ), which is referred to as the EM method in this paper, is the state of the art.", "labels": [], "entities": []}, {"text": "However, the target of the EM method is text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8391489088535309}]}, {"text": "It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8846654295921326}]}, {"text": "The EM method works well in text classification, but often causes worse classification in WSD.", "labels": [], "entities": [{"text": "text classification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8529408574104309}, {"text": "WSD", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.7679338455200195}]}, {"text": "The EM method is expected to improve the accuracy of learned rules step by step in proportion to the iteration number in the EM algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9989814162254333}]}, {"text": "However, this rarely happens in practice, and in many cases, the accuracy falls after a certain iteration number in the EM algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9995166063308716}]}, {"text": "In the worst case, the accuracy of the rule learned through only labeled data is degraded by using unlabeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993688464164734}]}, {"text": "To overcome this problem, we estimate an optimum iteration number in the EM algorithm, and in actual learning, we stop the iteration of the EM algorithm at the estimated number.", "labels": [], "entities": []}, {"text": "If the estimated number is 0, it means that the EM method is not used.", "labels": [], "entities": []}, {"text": "To estimate the optimum iteration number, we propose two methods: one uses cross validation and the other uses two heuristics besides cross validation.", "labels": [], "entities": []}, {"text": "In this paper, we refer to the former method as CV-EM and the latter method as CV-EM2.", "labels": [], "entities": []}, {"text": "In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2().", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.7703447341918945}]}, {"text": "The original EM method failed to boost the precision (76.78%) of the rule learned through only labeled data.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.999600350856781}]}, {"text": "On the other hand, CV-EM and CV-EM2 boosted the precision to 77.88% and 78.56%.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9997803568840027}]}, {"text": "The score of CV-EM2 is a match for the best public score of this task.", "labels": [], "entities": []}, {"text": "Furthermore, these methods were confirmed to be effective also for verb WSD problems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.6765041351318359}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of experiments (Noun)", "labels": [], "entities": [{"text": "Noun", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9381494522094727}]}, {"text": " Table 2: Results of experiments (Verb)", "labels": [], "entities": [{"text": "Verb", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8020354509353638}]}, {"text": " Table 3: Effects of the distribution of meanings  d > 0 d < 0  improvement  6  7  deterioration  2  8", "labels": [], "entities": []}]}