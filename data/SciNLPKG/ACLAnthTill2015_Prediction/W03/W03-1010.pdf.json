{"title": [{"text": "A Plethora of Methods for Learning English Countability", "labels": [], "entities": [{"text": "Learning English Countability", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6412553588549296}]}], "abstractContent": [{"text": "This paper compares a range of methods for classifying words based on linguistic diagnostics, focusing on the task of learning countabilities for English nouns.", "labels": [], "entities": []}, {"text": "We propose two basic approaches to feature representation: distribution-based representation, which simply looks at the distribution of features in the corpus data, and agreement-based representation which analyses the level of token-wise agreement between multiple pre-processor systems.", "labels": [], "entities": [{"text": "feature representation", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7521848380565643}]}, {"text": "We additionally compare a single multiclass classifier architecture with a suite of binary classifiers, and combine analyses from multiple pre-processors.", "labels": [], "entities": []}, {"text": "Finally, we present and evaluate a feature selection method.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6984677165746689}]}], "introductionContent": [{"text": "Lexical acquisition can be described as the process of populating a grammar skeleton with lexical items, through a process of mapping word lemmata onto lexical types described in the grammar.", "labels": [], "entities": [{"text": "Lexical acquisition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.905825138092041}]}, {"text": "Depending on the linguistic precision of the base grammar, lexical acquisition can range in complexity from simple part-of-speech tagging (shallow lexical acquisition) to the acquisition of selectionally-constrained subcategorisation frame clusters or constructional compatibilities (deep lexical acquisition).", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7836198508739471}]}, {"text": "Our particular interest is in the latter task of deep lexical acquisition with respect to English nouns.", "labels": [], "entities": [{"text": "deep lexical acquisition", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.6621768673261007}]}, {"text": "We are interested in developing learning techniques for deep lexical acquisition which take a fixed set of linguistic diagnostics, and classify words according to corpus data.", "labels": [], "entities": [{"text": "deep lexical acquisition", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6846226652463278}]}, {"text": "We propose a range of general techniques for this task, as exemplified over the task of English countability acquisition.", "labels": [], "entities": [{"text": "English countability acquisition", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.6840632756551107}]}, {"text": "Countability is the syntactic property that determines whether a noun can take singular and plural forms, and affects the range of permissible modifiers.", "labels": [], "entities": []}, {"text": "Many nouns have both countable and uncountable lemmas, with differences in meaning: I submitted two papers \"documents\" (countable) vs. Please use white paper \"substance to be written on\" (uncountable).", "labels": [], "entities": []}, {"text": "This research complements that described in, where we present the linguistic foundations and features drawn upon in the countability classification task, and motivate the claim that countability preferences can be learned from corpus evidence.", "labels": [], "entities": [{"text": "countability classification task", "start_pos": 120, "end_pos": 152, "type": "TASK", "confidence": 0.8324086666107178}]}, {"text": "In this paper, we focus on the methods used to tackle the task of countability classification based on this fixed feature set.", "labels": [], "entities": [{"text": "countability classification", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.7666929364204407}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 outlines the countability classes, resources and pre-processors.", "labels": [], "entities": []}, {"text": "Section 3 presents two methods of representing the feature space.", "labels": [], "entities": []}, {"text": "Section 4 details the different classifier designs and the dataset, which are then evaluated in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper with a discussion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Below, we outline the different classifiers tested and describe the process used to generate the goldstandard data.", "labels": [], "entities": []}, {"text": "Evaluation of the supervised classifiers was carried out based on 10-fold stratified cross-validation over the relevant dataset, and results presented here are averaged over the 10 iterations.", "labels": [], "entities": []}, {"text": "Classifier performance is rated according to classification accuracy (the proportion of instances classified correctly) and F-score (\u03b2 = 1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9188405275344849}, {"text": "F-score", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9995713829994202}]}, {"text": "In the case of the SINGLE classifier, the class-wise F-score is calculated by decomposing the multiclass labels into their components.", "labels": [], "entities": [{"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9620313048362732}]}, {"text": "A countable+uncountable instance misclassified as countable, for example, would count as a misclassification in terms of classification accuracy, a correct classification in the calculation of the countable F-score, and a misclassification in the calculation of the uncountable F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9438353776931763}, {"text": "F-score", "start_pos": 207, "end_pos": 214, "type": "METRIC", "confidence": 0.9106545448303223}]}, {"text": "Note that the SINGLE classifier is run over a different dataset to each member of the SUITE classifier, and cross-comparison of the classification accuracies is not representative of the relative system performance (classification accuracies for the SIN-GLE classifier are given in parentheses to reinforce this point).", "labels": [], "entities": []}, {"text": "Classification accuracies are thus simply used for classifier comparison within a basic classifier architecture (SINGLE or SUITE), and F-score is .939 .960 Dist (.857) .944 Dist .937 .959 Agree .902 .936 Agree .911 .941  We present the results for two baseline systems for each countability class: a majority-class classifier and the unsupervised method.", "labels": [], "entities": [{"text": "F-score", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.9967543482780457}, {"text": "Dist", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9717414975166321}, {"text": "Agree", "start_pos": 204, "end_pos": 209, "type": "METRIC", "confidence": 0.9627173542976379}]}, {"text": "The Majority class system is run over the binary data used by the SUITE classifier for the given class, and simply classifies all instances according to the most commonly-attested class in that dataset.", "labels": [], "entities": []}, {"text": "Irrespective of the majority class, we calculate the F-score based on a positive-class classifier, i.e. a classifier which naively classifies each instance as belonging to the given class; in the case that the positive class is not the majority class, the F-score is given in parentheses.", "labels": [], "entities": [{"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9973179697990417}, {"text": "F-score", "start_pos": 256, "end_pos": 263, "type": "METRIC", "confidence": 0.9945396780967712}]}, {"text": "The results for the different system configurations over the four countability classes are presented in, in which the highest classification accuracy and F-score values for each class are presented in boldface.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9642696380615234}, {"text": "F-score", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9936195015907288}]}, {"text": "The classifier Dist(All CON ,SUITE), for example, applies the distribution-based feature representation in a SUITE classifier configuration (i.e. it tests for binary membership in each countability class), using the concatenated feature vectors from each of the tagger, chunker and RASP.", "labels": [], "entities": []}, {"text": "We targeted the Dist(All CON ,SUITE) system for evaluation (3852 features), and ran it over both the countable and uncountable classes.", "labels": [], "entities": []}, {"text": "We additionally carried out random feature selection as a baseline to compare the feature selection results against.", "labels": [], "entities": []}, {"text": "Note that the x-axis (N ) and right y-axis (instances/sec) are both logarithmic, such that the linear right-decreasing time curves are indicative of the direct proportionality between the number of features and processing time.", "labels": [], "entities": []}, {"text": "The differential in F-score for the best-N configuration as compared to the full feature set is statistically insignificant for N > 100 for countable nouns and N > 50 for uncountable nouns.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9961322546005249}]}, {"text": "That is, feature selection facilitates a relative speed-up of around 30\u00d7 without a significant drop in F-score.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.8309705853462219}, {"text": "F-score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.99699866771698}]}, {"text": "Comparing the results for the best-N and rand-N features, the difference in F-score was statistically significant for all values of N < 1000.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9983471632003784}]}, {"text": "The proposed method of feature selection thus allows us to maintain the full classification potential of the feature set while enabling No significant performance difference was observed for: Dist(ChunkMEAN,SUITE) vs. Dist(All * ,SUITE) for countable nouns, and Dist(POSCON,SUITE) vs. Dist(AllCON,SUITE) for uncountable nouns.", "labels": [], "entities": []}, {"text": "As evaluated on an AMD Athlon 2100+ CPU with 3GB of memory.", "labels": [], "entities": []}, {"text": "We focus exclusively on countable and uncountable nouns here and in the remainder of supplementary evaluation as these are by far the most populous countability classes.: Results for restricted feature sets a speedup greater than an order of magnitude, potentially making the difference in practical utility for the proposed method.", "labels": [], "entities": []}, {"text": "To determine the relative impact of the component feature values on the performance of the distribution-based feature representation, we used the Dist(All MEAN ,SUITE) configuration to build: (a) a classifier using a single binary value for each unit feature, based on simple corpus occurrence (Binary); and (b) 3 separate classifiers based on each of the corpfreq, wordfreq and featfreq features values only (without the 2D feature cluster totals).", "labels": [], "entities": []}, {"text": "In each case, the total number of feature values is 206.", "labels": [], "entities": []}, {"text": "The results for each of these classifiers over countable and uncountable nouns are presented in, as compared to the basic Dist(All MEAN ,SUITE) classifier with all 1,284 features (All features) and also the best-200 features.", "labels": [], "entities": [{"text": "Dist(All MEAN", "start_pos": 122, "end_pos": 135, "type": "METRIC", "confidence": 0.6326462924480438}]}, {"text": "Results which differ from those for All features to a level of statistical significance are asterisked.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 63, "end_pos": 87, "type": "METRIC", "confidence": 0.9411779046058655}]}, {"text": "The binary classifiers performed significantly worse than All features for both countable and uncountable nouns, underlining the utility of the distribution-based feature representation.", "labels": [], "entities": []}, {"text": "wordfreq is marginally superior to corpfreq as a standalone feature representation, and both of these were on the whole slightly below the full feature set in performance (although no significant difference was observed).", "labels": [], "entities": [{"text": "wordfreq", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8498092889785767}]}, {"text": "featfreq performed slightly worse again, significantly below the level of the full feature set.", "labels": [], "entities": [{"text": "featfreq", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9106993079185486}]}, {"text": "Results for the best-200 classifier were marginally higher than those for each of the individual feature representations in the case of the countable class, but marginally below the results for corpfreq and wordfreq in the case of the uncountable class.", "labels": [], "entities": []}, {"text": "The differences here are not statistically significant, and additional evaluation is required to determine the relative success of feature selection over simply using wordfreq values, for example.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Basic results for countable nouns", "labels": [], "entities": []}, {"text": " Table 2: Basic results for uncountable nouns", "labels": [], "entities": []}, {"text": " Table 3: Basic results for plural only nouns", "labels": [], "entities": []}, {"text": " Table 4: Basic results for bipartite nouns", "labels": [], "entities": []}, {"text": " Table 5: Results for restricted feature sets", "labels": [], "entities": []}]}