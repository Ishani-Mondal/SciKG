{"title": [{"text": "Statistical QA -Classifier vs. Re-ranker: What's the difference?", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we show that we can obtain a good baseline performance for Question Answering (QA) by using only 4 simple features.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.8481817245483398}]}, {"text": "Using these features , we contrast two approaches used fora Maximum Entropy based QA system.", "labels": [], "entities": []}, {"text": "We view the QA problem as a classification problem and as a re-ranking problem.", "labels": [], "entities": [{"text": "QA problem", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.8032442629337311}]}, {"text": "Our results indicate that the QA system viewed as a re-ranker clearly outperforms the QA system used as a classifier.", "labels": [], "entities": []}, {"text": "Both systems are trained using the same data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open-Domain factoid Question Answering (QA) is defined as the task of answering fact-based questions phrased in Natural Language.", "labels": [], "entities": [{"text": "Open-Domain factoid Question Answering (QA)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.721420556306839}, {"text": "answering fact-based questions phrased in Natural Language", "start_pos": 70, "end_pos": 128, "type": "TASK", "confidence": 0.7329345600945609}]}, {"text": "Examples of some question and answers that fall in the fact-based category are: 1.", "labels": [], "entities": []}, {"text": "What is the capital of Japan?", "labels": [], "entities": []}, {"text": "Where is the Eiffel Tower?", "labels": [], "entities": []}, {"text": "-Paris The architecture of most of QA systems consists of two basic modules: the information retrieval (IR) module and the answer pinpointing module.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.7301181197166443}]}, {"text": "These two modules are used in atypical pipeline architecture.", "labels": [], "entities": []}, {"text": "For a given question, the IR module finds a set of relevant segments.", "labels": [], "entities": [{"text": "IR", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9194243550300598}]}, {"text": "Each segment typically consists of at most R sentences . The answer pinpointing module processes each of these segments and finds the appropriate answer phrase.", "labels": [], "entities": []}, {"text": "In our experiments we use R=1 phrase.", "labels": [], "entities": []}, {"text": "Evaluation of a QA system is judged on the basis on the final output answer and the corresponding evidence provided by the segment.", "labels": [], "entities": []}, {"text": "This paper focuses on the answer pinpointing module.", "labels": [], "entities": []}, {"text": "Typical QA systems perform re-ranking of candidate answers as an important step in pinpointing.", "labels": [], "entities": []}, {"text": "The goal is to rank the most likely answer first by using either symbolic or statistical methods.", "labels": [], "entities": []}, {"text": "Some QA systems make use of statistical answer pinpointing by treating it as a classification problem.", "labels": [], "entities": [{"text": "statistical answer pinpointing", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.7098743716875712}]}, {"text": "In this paper, we cast the pinpointing problem in a statistical framework and compare two approaches, classification and re-ranking.", "labels": [], "entities": [{"text": "classification", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.9507557153701782}]}], "datasetContent": [{"text": "The performance of the QA system is highly dependent on the performance of the two individual modules IR and answer-pinpointing.", "labels": [], "entities": []}, {"text": "It is almost impossible to measure recall because the IR collection is typically large and involves several hundreds of thousands of documents.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9944251775741577}]}, {"text": "Hence, we evaluate our IR by only the precision measure at top N segments.", "labels": [], "entities": [{"text": "IR", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9746048450469971}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9993510842323303}]}, {"text": "This method is actually a rather sloppy approximation to the original recall and precision measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9984667897224426}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9451581239700317}]}, {"text": "Questions with fewer correct answers in the collection would have a lower precision score as compared to questions with many answers.", "labels": [], "entities": [{"text": "precision score", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.9873362183570862}]}, {"text": "Similarly, it is unclear how one would evaluate answer questions with No Answer (NIL) in the collection using this metric.", "labels": [], "entities": [{"text": "No Answer (NIL)", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.8237906694412231}]}, {"text": "All these questions would have zero precision from the IR collection.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9985626339912415}, {"text": "IR collection", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.7963775992393494}]}, {"text": "The answer-pinpointing module is evaluated by checking if the answer returned by the system as the top ranked (#1) answer is correct/incorrect with respect to the IR collection and the true answer.", "labels": [], "entities": []}, {"text": "Hence, if the IR system fails to return even a single sentence that contains the correct answer for the given question, we do not penalize the answer-pinpointing module.", "labels": [], "entities": [{"text": "IR", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9264614582061768}]}, {"text": "It is again unclear how to evaluate questions with No answer (NIL).", "labels": [], "entities": [{"text": "No answer (NIL)", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.8519950985908509}]}, {"text": "(Here, for our experiments we attribute the error to the IR module.)", "labels": [], "entities": []}, {"text": "Finally, the combined system is evaluated by using the standard technique, wherein the Answer (ranked #1) returned by the system is judged to be either corrector incorrect and then the average is taken.", "labels": [], "entities": [{"text": "Answer (ranked #1) returned", "start_pos": 87, "end_pos": 114, "type": "METRIC", "confidence": 0.9129385096686227}]}], "tableCaptions": []}