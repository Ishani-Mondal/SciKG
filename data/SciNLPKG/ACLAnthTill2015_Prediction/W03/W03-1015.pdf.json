{"title": [{"text": "Bootstrapping Coreference Classifiers with Multiple Machine Learning Algorithms", "labels": [], "entities": []}], "abstractContent": [{"text": "Successful application of multi-view co-training algorithms relies on the ability to factor the available features into views that are compatible and uncorrelated.", "labels": [], "entities": []}, {"text": "This can potentially preclude their use on problems such as coreference resolution that lack an obvious feature split.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9674793183803558}]}, {"text": "To bootstrap coref-erence classifiers, we propose and evaluate a single-view weakly supervised algorithm that relies on two different learning algorithms in lieu of the two different views required by co-training.", "labels": [], "entities": []}, {"text": "In addition, we investigate a method for ranking un-labeled instances to be fed back into the bootstrapping loop as labeled data, aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping.", "labels": [], "entities": []}], "introductionContent": [{"text": "Co-training) is a weakly supervised paradigm that learns a task from a small set of labeled data and a large pool of unlabeled data using separate, but redundant views of the data (i.e. using disjoint feature subsets to represent the data).", "labels": [], "entities": []}, {"text": "To ensure provable performance guarantees, the co-training algorithm assumes as input a set of views that satisfies two fairly strict conditions.", "labels": [], "entities": []}, {"text": "First, each view must be sufficient for learning the target concept.", "labels": [], "entities": []}, {"text": "Second, the views must be conditionally independent of each other given the class.", "labels": [], "entities": []}, {"text": "Empirical results on artificial data sets by and confirm that co-training is sensitive to these assumptions.", "labels": [], "entities": []}, {"text": "Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification) and named entity classification), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution).", "labels": [], "entities": [{"text": "web page classification", "start_pos": 149, "end_pos": 172, "type": "TASK", "confidence": 0.6983233292897543}, {"text": "named entity classification", "start_pos": 178, "end_pos": 205, "type": "TASK", "confidence": 0.6561331748962402}]}, {"text": "As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. and use two different learning algorithms in lieu of the multiple views required by standard co-training.", "labels": [], "entities": []}, {"text": "The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data.", "labels": [], "entities": []}, {"text": "Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different.", "labels": [], "entities": []}, {"text": "In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled data.", "labels": [], "entities": []}, {"text": "On the other hand, Steedman et al. use two learning algorithms that correspond to coarsely different features, thus retaining in spirit the advantages provided by conditionally independent feature splits in the Blum and Mitchell algorithm.", "labels": [], "entities": []}, {"text": "The goal of this paper is two-fold.", "labels": [], "entities": []}, {"text": "First, we propose a single-view algorithm for bootstrapping coreference classifiers.", "labels": [], "entities": []}, {"text": "Like anaphora resolution, noun phrase coreference resolution is a problem for which a natural feature split is not readily available.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7685586214065552}, {"text": "noun phrase coreference resolution", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.6932694911956787}]}, {"text": "In related work, we compare the performance of the Blum and Mitchell cotraining algorithm with that of two existing singleview bootstrapping algorithms -self-training with bagging () and EM ( ) -on coreference resolution, and show that single-view weakly supervised learners area viable alternative to co-training for the task.", "labels": [], "entities": [{"text": "EM", "start_pos": 187, "end_pos": 189, "type": "METRIC", "confidence": 0.9609804749488831}, {"text": "coreference resolution", "start_pos": 198, "end_pos": 220, "type": "TASK", "confidence": 0.9424779713153839}]}, {"text": "This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms.", "labels": [], "entities": []}, {"text": "Second, we investigate anew method that, inspired by, ranks unlabeled instances to be added to the labeled data in an attempt to alleviate a problem commonly observed in bootstrapping experiments -performance deterioration due to the degradation in the quality of the labeled data as bootstrapping progresses.", "labels": [], "entities": []}, {"text": "Ina set of baseline experiments, we first demonstrate that multi-view co-training fails to boost the performance of the coreference system under various parameter settings.", "labels": [], "entities": []}, {"text": "We then show that our single-view weakly supervised algorithm successfully bootstraps the coreference classifiers, boosting the F-measure score by 9-12% on two standard coreference data sets.", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 128, "end_pos": 143, "type": "METRIC", "confidence": 0.968242734670639}]}, {"text": "Finally, we present experimental results that suggest that our method for ranking instances is more resistant to performance deterioration in the bootstrapping process than Blum and Mitchell's \"rank-by-confidence\" method.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the goals of the experiments is to enable a fair comparison of the multi-view algorithm with our single-view bootstrapping algorithm.", "labels": [], "entities": []}, {"text": "Since the B&M co-training algorithm is sensitive not only to the views employed but also to other input parame-  ters such as the pool size and the growth size), we evaluate the algorithm under different parameter settings, as described below.", "labels": [], "entities": []}, {"text": "We use the MUC-6 (1995) and MUC-7 (1998) coreference data sets for evaluation.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9381407499313354}, {"text": "MUC-7 (1998) coreference data sets", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.8182836856160846}]}, {"text": "The training set is composed of 30 \"dry run\" texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.9730457663536072}, {"text": "MUC-7 data sets", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.9355690677960714}]}, {"text": "Unlike Ng and Cardie where we choose one of the dryrun texts (contributing approximately 3500-3700 instances) form the labeled data set, however, here we randomly select 1000 instances.", "labels": [], "entities": [{"text": "dryrun texts", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8791103959083557}]}, {"text": "The remaining instances are used as unlabeled data.", "labels": [], "entities": []}, {"text": "Testing is performed by applying the bootstrapped coreference classifier and the clustering algorithm described in section 2 on the 20-30 \"formal evaluation\" texts for each of the MUC-6 and MUC-7 data sets.", "labels": [], "entities": [{"text": "MUC-6 and MUC-7 data sets", "start_pos": 180, "end_pos": 205, "type": "DATASET", "confidence": 0.8148237109184265}]}, {"text": "Two sets of experiments are conducted, one using naive Bayes as the underlying supervised learning algorithm and the other the decision list learner.", "labels": [], "entities": []}, {"text": "All results reported are averages across five runs.", "labels": [], "entities": []}, {"text": "The co-training parameters are set as follows.", "labels": [], "entities": []}, {"text": "We used three methods to generate the views from the 25 features used by the coreference system: greedy method, random splitting of features into views, and splitting of features according to the feature type (i.e. lexicosyntactic vs. non-lexico-syntactic features).", "labels": [], "entities": []}, {"text": "We tested values of 500, 1000, 5000.", "labels": [], "entities": []}, {"text": "We tested values of 10, 50, 100, 200.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and", "labels": [], "entities": [{"text": "Recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.912165641784668}, {"text": "Precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9929344654083252}]}, {"text": " Table 3: Summary of the major similarities and differences among four bootstrapping schemes: Blum and  Mitchell, Goldman and Zhou, Steedman et al., and ours. Only the relevant dimensions are discussed here.", "labels": [], "entities": []}]}