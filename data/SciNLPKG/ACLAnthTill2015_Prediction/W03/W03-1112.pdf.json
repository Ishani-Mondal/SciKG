{"title": [{"text": "AnyQ: Answer Set based Information Retrieval System", "labels": [], "entities": [{"text": "AnyQ", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9336850047111511}, {"text": "Answer Set based Information Retrieval", "start_pos": 6, "end_pos": 44, "type": "TASK", "confidence": 0.5877879619598388}]}], "abstractContent": [{"text": "The accuracy of IR result continues to grow on importance as exponential growth of WWW, and it is therefore increasingly important that appropriate retrieval technologies be developed for the web.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993607401847839}, {"text": "IR", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9806033968925476}, {"text": "WWW", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.8575020432472229}]}, {"text": "We explore anew type of IR, \"answer set based IR\", and its operational experience.", "labels": [], "entities": [{"text": "IR", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9801140427589417}]}, {"text": "Our proposed approach attempts to provide high quality answer documents to user by maintaining a knowledge base with expected queries and corresponding answer document.", "labels": [], "entities": []}, {"text": "We will elaborate on our architecture and the experimental results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of Information Retrieval (IR) is finding answer suited to user question from massive document collections with satisfied response time.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.8634836554527283}]}, {"text": "With the exponential growth of information on the Web, user is expecting to find answer more fast with less effort.", "labels": [], "entities": []}, {"text": "Current IR systems especially focus on improving precision the result rather than recall.", "labels": [], "entities": [{"text": "IR", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9863335490226746}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9973491430282593}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9978237152099609}]}, {"text": "A notable trend in IR is to provide more accurate, immediately usable information as in Question Answering systems(Q/A) or in some systems using pre-constructed question/answer document pairs, known \"answer set driven\" system.", "labels": [], "entities": [{"text": "IR", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.992268979549408}, {"text": "Question Answering systems(Q/A)", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.7296248972415924}]}, {"text": "While traditional search engine uses term indexing, i.e. tf*idf, answer approaches use syntactic, semantic and pragmatic knowledge provided expert, i.e. WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.9661425352096558}]}, {"text": "Another difference comes from the fact that answer approach returns \"answer set\" distilled information need of user as retrieval result, not just document appeared query terms.", "labels": [], "entities": []}, {"text": "The TREC Q/A track which has motivated much of the recent work in the field focuses on fact-based, short-answer question type, e.g. \"Who is Barbara Jordan?\" or \"What is Mardi Gras?.", "labels": [], "entities": [{"text": "TREC Q/A", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.6948610842227936}, {"text": "What is Mardi Gras?", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.5793043196201324}]}, {"text": "The Q/A runs find an actual answer in TREC collection, rather than a ranked list of documents, in response to a question.", "labels": [], "entities": [{"text": "TREC collection", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8386130034923553}]}, {"text": "On the other hand, user queries in answer set driven system, like AskJeeves, are more implicit and conceptual.", "labels": [], "entities": []}, {"text": "These system was developed targeting the Web, is larger than the TREC Q/A document collection.", "labels": [], "entities": [{"text": "TREC Q/A document collection", "start_pos": 65, "end_pos": 93, "type": "DATASET", "confidence": 0.7216835916042328}]}, {"text": "Whereas the user gives incomplete query to system, they need not only answers but related information.", "labels": [], "entities": []}, {"text": "Sometimes the user even has uncertainty what exactly they need.", "labels": [], "entities": []}, {"text": "For example, the user query just \"Paris\" is answered by gathering information including Paris city guide, photographs of Paris, and soon.", "labels": [], "entities": []}, {"text": "To catch information need of user, these system have pre-defined query pattern and prepared correct answers belonging to each question.", "labels": [], "entities": []}, {"text": "Since it is still considered difficult, if not impossible, to capture semantics and pragmatics of sentences in user queries and documents, such systems require knowledge bases built manually so that a certain level of quality can be guaranteed.", "labels": [], "entities": []}, {"text": "Needless to say, this knowledge base construction process is laborintensive, typically requiring significant and continuous human efforts.", "labels": [], "entities": []}, {"text": "This paper rests on the both directions: anew type of IR and its operational experience.", "labels": [], "entities": [{"text": "IR", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9866923689842224}]}, {"text": "Our system, named \"AnyQ\" 1 , attempts to provide high quality answer documents to user queries by maintaining a knowledge base consisting of expected queries and corresponding answer document.", "labels": [], "entities": []}, {"text": "We defined the semantic category of the answer as attributes and the 1 http://anyq.etri.re.kr in korean", "labels": [], "entities": []}], "datasetContent": [{"text": "Whereas traditional Q/A and IR system have competition conference, like TREC, so that they can start with standard retrieval test collection, to explore how useful the proposed approach, we evaluate performance of answer document and candidate answer sentence.", "labels": [], "entities": []}, {"text": "Another difference comes from the fact that result units for these systems are different.", "labels": [], "entities": []}, {"text": "That is Q/A system returns exactly relevant answer (50 byte or 250 byte), while IR system returns document scored by ranking mechanism.", "labels": [], "entities": []}, {"text": "Our system returns \"answer set\" distilled semantic knowledge as retrieval result", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Result of Answer Set Construction", "labels": [], "entities": [{"text": "Result of Answer Set Construction", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8291378021240234}]}, {"text": " Table 3. Result of Answer Set Retrieval  AS based IR  Web IR  Total  Top 5  Top 5  Top 10  Precision  0.584  0.769  0.291  0.2864  Recall  0.391  0.655  0.291  0.315  F-score  0.468  0.797  0.291  0.3  Highlighting  MRR  0.78", "labels": [], "entities": [{"text": "Precision  0.584  0.769  0.291  0.2864  Recall  0.391  0.655  0.291  0.315  F-score  0.468  0.797  0.291  0.3  Highlighting  MRR  0.78", "start_pos": 92, "end_pos": 226, "type": "METRIC", "confidence": 0.675932365987036}]}]}