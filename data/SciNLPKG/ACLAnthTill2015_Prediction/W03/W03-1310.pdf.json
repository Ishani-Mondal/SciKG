{"title": [{"text": "Answering Clinical Questions with Role Identification", "labels": [], "entities": [{"text": "Answering Clinical Questions", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9020531972249349}, {"text": "Role Identification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6855520755052567}]}], "abstractContent": [{"text": "We describe our work in progress on natural language analysis in medical question-answering in the context of a broader medical text-retrieval project.", "labels": [], "entities": [{"text": "natural language analysis", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7361402114232382}]}, {"text": "We analyze the limitations in the medical domain of the technologies that have been developed for general question-answering systems, and describe an alternative approach whose organizing principle is the identification of semantic roles in both question and answer texts that correspond to the fields of PICO format.", "labels": [], "entities": [{"text": "PICO format", "start_pos": 305, "end_pos": 316, "type": "TASK", "confidence": 0.7401273250579834}]}], "introductionContent": [], "datasetContent": [{"text": "Evaluation of QA systems in the medical area is different from current evaluation methods for general QA systems.", "labels": [], "entities": [{"text": "QA", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9278661608695984}]}, {"text": "The Text Retrieval Conference uses the Mean Reciprocal Rank (MRR) as an evaluation metric.", "labels": [], "entities": [{"text": "Text Retrieval Conference", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7761990229288737}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 39, "end_pos": 65, "type": "METRIC", "confidence": 0.95259756843249}]}, {"text": "In this method, a system may return an ordered list of up to five different candidate answers to a question, and the score received is 1=n, where n is the position in the list of the correct answer (if it appears at all); for example, if the correct answer is fourth in the list, the system receives a score of 0.25 for that test item.", "labels": [], "entities": []}, {"text": "This metric cannot be applied here, since returning a list of alternative candidate answers to a question, each of which must then be further verified, is not acceptable fora clinical question that is posed onsite.", "labels": [], "entities": []}, {"text": "Different answer formats should be evaluated separately.", "labels": [], "entities": []}, {"text": "The short answer has to be concise.", "labels": [], "entities": []}, {"text": "So what a concise answer is must be defined (at least for the wh-questions).", "labels": [], "entities": []}, {"text": "A long answer needs to provide detailed information that explains the short answer.", "labels": [], "entities": []}, {"text": "For no-answer questions, relevant information (if there is some) should be returned.", "labels": [], "entities": []}, {"text": "For these two types of answers, it has to be clear what information can be viewed as \"detail\" or \"relevant\"; (2) what the difference between the two is; and how much information should be included.", "labels": [], "entities": []}, {"text": "Partial answers should be considered in the evaluation.", "labels": [], "entities": []}, {"text": "If part of the correct answer is included in the system output, it should be evaluated according to the importance of the correct information.", "labels": [], "entities": []}, {"text": "A partial answer that contains more crucial information should obtain a higher score.", "labels": [], "entities": []}, {"text": "Similarly, if an answer helps make a wrong decision, it should be punished in the evaluation.", "labels": [], "entities": []}], "tableCaptions": []}