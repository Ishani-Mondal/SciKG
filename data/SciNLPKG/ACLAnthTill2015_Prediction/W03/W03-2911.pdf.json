{"title": [{"text": "Morpho-syntactic Clues for Terminological Processing in Serbian", "labels": [], "entities": [{"text": "Terminological Processing", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.8262669444084167}]}], "abstractContent": [{"text": "In this paper we discuss morpho-syntactic clues that can be used to facilitate termi-nological processing in Serbian.", "labels": [], "entities": [{"text": "termi-nological processing", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.7516675889492035}]}, {"text": "A method (called SRCE) for automatic extraction of multiword terms is presented.", "labels": [], "entities": [{"text": "SRCE", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.8351296782493591}, {"text": "automatic extraction of multiword terms", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.7653957188129425}]}, {"text": "The approach incorporates a set of generic morpho-syntactic filters for recognition of term candidates, a method for conflation of morphological variants and a module for foreign word recognition.", "labels": [], "entities": [{"text": "recognition of term candidates", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.7869294881820679}, {"text": "foreign word recognition", "start_pos": 171, "end_pos": 195, "type": "TASK", "confidence": 0.6653759280840555}]}, {"text": "Morpho-syntactic filters describe general term formation patterns, and are implemented as generic regular expressions.", "labels": [], "entities": [{"text": "term formation patterns", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.8299151062965393}]}, {"text": "The inner structure together with the agreements within term candidates are used as clues to discover the boundaries of nested terms.", "labels": [], "entities": []}, {"text": "The results of the termi-nological processing of a textbook corpus in the domains of mathematics and computer science are presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "An overwhelming amount of textual information presented in newswire, scientific literature, legal texts, etc., makes it difficult fora human to efficiently localise the information of interest.", "labels": [], "entities": []}, {"text": "In particular, it is doubtful that anybody could process such huge amount of information without an automated help, especially when the information content spans across domains.", "labels": [], "entities": []}, {"text": "The amount of edocuments and their fuzzy structure require effective tools that can help users to systematically gather and make use of the information encoded in text documents.", "labels": [], "entities": []}, {"text": "For these reasons, different text and/or literature mining techniques have been developed recently (e.g. (;) in order to facilitate efficient discovery of knowlcient discovery of knowledge contained in large scientific or legal text collections.", "labels": [], "entities": [{"text": "text and/or literature mining", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.6860343168179194}, {"text": "knowlcient discovery of knowledge contained in large scientific or legal text collections", "start_pos": 155, "end_pos": 244, "type": "TASK", "confidence": 0.6720068007707596}]}, {"text": "The main goal is to retrieve the knowledge \"buried\" in a text and to present it to users in a digested form.", "labels": [], "entities": []}, {"text": "The discovery (and transfer) of knowledge relies heavily on the identification of relevant concepts, which are linguistically represented by domain specific terms.", "labels": [], "entities": [{"text": "discovery (and transfer) of knowledge", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.7345777026244572}]}, {"text": "Terms represent the most important notions in a domain and characterise documents semantically, and thus should be used as a basis for sophisticated knowledge acquisition.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 149, "end_pos": 170, "type": "TASK", "confidence": 0.7213712781667709}]}, {"text": "Still, few text-mining systems incorporate deep and dynamic terminology processing, although there is an increasing amount of new terms that represent newly created concepts in rapidly developing fields.", "labels": [], "entities": []}, {"text": "Existing term dictionaries and standardised terminologies offer only a partial solution, as they are almost never up-todate.", "labels": [], "entities": []}, {"text": "Although naming conventions do exist for some types of concepts (e.g. gene and protein names in biomedicine), these are only guidelines and as such do not impose restrictions to domain experts, who frequently introduce ad-hoc terms.", "labels": [], "entities": []}, {"text": "Thus, the lack of clear naming conventions makes the automatic term recognition (ATR) task difficult even for languages that are not morphologically and derivationally rich.", "labels": [], "entities": [{"text": "automatic term recognition (ATR)", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7601989408334097}]}, {"text": "ATR tools have been developed for English (), French, Japanese), etc.", "labels": [], "entities": []}, {"text": "Some methods rely purely on linguistic information, namely morpho-syntactic features of term candidates.", "labels": [], "entities": []}, {"text": "Hybrid approaches combining linguistic patterns and statistical measures (e.g. () and machine-learning techniques (e.g. () have been also used.", "labels": [], "entities": []}, {"text": "However, few studies have been done for morphologically rich Slavic languages.", "labels": [], "entities": []}, {"text": "For example, Vintar (2000) presented two methods for extraction of terminological collocations in order to assist the translation process in Slovene.", "labels": [], "entities": [{"text": "translation process", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.911231279373169}]}, {"text": "The statistical approach was based on the mutual expectation and LocalMax measures, and involved collocation extraction from raw text.", "labels": [], "entities": []}, {"text": "The extracted collocations were filtered with a stopword list, and only collocations containing single-word terms (devised previously by bilingual alignment) were accepted as relevant.", "labels": [], "entities": []}, {"text": "In another approach, she used regular expression patterns to extract term collocations from a morphosyntactically tagged corpus.", "labels": [], "entities": []}, {"text": "However, these patterns are too general, and consequently not all extracted phrases were terminologically relevant.", "labels": [], "entities": []}, {"text": "In this paper we discuss automatic terminology recognition in Serbian, in particular, the extraction of multiword terms, which are very frequent 1 in certain domains (e.g. natural sciences, mathematics, etc.).", "labels": [], "entities": [{"text": "terminology recognition", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8079185485839844}]}, {"text": "Since Serbian is a highly inflective and morphologically and derivationally rich language, morpho-syntactic clues are indispensable in the ATR process.", "labels": [], "entities": [{"text": "ATR", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9321932792663574}]}, {"text": "Our hybrid approach (called SRCE -Serbian C-value) combines morphosyntactic features of term candidates and statistical analysis of their occurrences in text.", "labels": [], "entities": [{"text": "SRCE -Serbian C-value", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.578206941485405}]}, {"text": "In addition, since terms appear in texts in many different forms due to their morphological and derivational variations, the necessity of taking these variations into account becomes particularly apparent.", "labels": [], "entities": []}, {"text": "Therefore, the SRCE method incorporates generic morpho-syntactic patterns, a term normalisation approach and a foreign word detection method.", "labels": [], "entities": [{"text": "SRCE", "start_pos": 15, "end_pos": 19, "type": "TASK", "confidence": 0.9825849533081055}, {"text": "term normalisation", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.6986013501882553}, {"text": "foreign word detection", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.6175950765609741}]}, {"text": "The paper is organised as follows: in Section 2 we present an overview of the core term extraction method, called the C-value method.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7545698583126068}]}, {"text": "In Section 3 we discuss morpho-syntactic clues, the normalisation approach and the foreign word recognition that are used for singling out terms in Serbian.", "labels": [], "entities": [{"text": "foreign word recognition", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6508030196030935}]}, {"text": "The experiments and evaluation are described in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The preliminary ATR experiments were conducted using the SRCE system on a corpus containing samples from university textbooks in mathematics and computer science 10 (altogether 120k words).", "labels": [], "entities": [{"text": "ATR", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.8771806359291077}]}, {"text": "Texts were pre-processed, i.e. initially tagged, by a system of electronic dictionaries (edictionaries) containing simple nominal words for Serbian.", "labels": [], "entities": []}, {"text": "E-dictionaries contain exhaustive description of morpho-syntactic characteristics and are used for lexical recognition and initial lemmatisation of words that occur in a text.", "labels": [], "entities": [{"text": "lexical recognition", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7044360339641571}]}, {"text": "This process is realised by edictionary look-up, which results in an initially tagged text: each textual word is associated with its lemma(s) and corresponding morpho-syntactic categories (tags) retrieved from the e-dictionary.", "labels": [], "entities": []}, {"text": "In general, e-dictionaries cannot resolve lexical ambiguities that result from the fact that there is no one-to-one correspondence between word forms and their morpho-syntactic features.", "labels": [], "entities": []}, {"text": "There are different methods to resolve ambiguities (e.g. cache-dictionaries or local grammars), but in our experiments no disambiguation techniques were applied.", "labels": [], "entities": []}, {"text": "In order to extract a list of term candidates, the set of morpho-syntactic filters described in 3.1 was applied to the initially tagged corpus.", "labels": [], "entities": []}, {"text": "We performed two sets of experiments.", "labels": [], "entities": []}, {"text": "In the first experiment, we did not use any stoplist to discard unwanted constituents of term candidates.", "labels": [], "entities": []}, {"text": "For each term candidate, we generated a canonical form (nominative, singular), a morphologically normalised form (list of normalised words comprising the term candidate) and a list of nested term candidates (see for examples).", "labels": [], "entities": []}, {"text": "In the next step, C-values for term candidates were calculated using statistics based on occurrences of normalised forms, and all term candidates with C-values above an empirically chosen threshold were selected as terms.", "labels": [], "entities": []}, {"text": "gives some examples of the recognised terms.", "labels": [], "entities": []}, {"text": "In order to calculate the precision, we ex-amined separately interval precisions in subcorpora in mathematical analysis and computer science (see).", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.999381422996521}, {"text": "mathematical analysis", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7396837770938873}]}, {"text": "Intervals are sets of recognised terms that are placed at certain positions within the list.", "labels": [], "entities": []}, {"text": "For example, interval 1-50 contains top 50 terms, while the interval over 150 contains all terms whose positions in the list are above 150.", "labels": [], "entities": []}, {"text": "Terms have been inspected by the first two authors, who are Serbian native speakers and are specialists in both computer science and mathematics.", "labels": [], "entities": []}, {"text": "In the first 50 terms for the domain of mathematical analysis, there was only one false term candidate (specijalna klasa neprekidnih preslikavanja), which contained an \"unwanted\" adjective specijalna (Engl. special).", "labels": [], "entities": [{"text": "mathematical analysis", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.8018339276313782}]}, {"text": "The reason for the significant drop in the precision in the second and third intervals is mainly the same: apart from few true negatives 11 , the majority of false term candidates contained common \"unwanted\" constituents, which are sampled in.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.999422550201416}]}, {"text": "The results for the computer science sub-corpus were slightly worse since the mathematical language seems to be more consistent and restricted.", "labels": [], "entities": []}, {"text": "Such as: toplo\u0161ka ta\u010dka gledi\u0161ta, kompletnost prostora igra, kod preslikavnja.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Top ranked terms in the domain of  mathematical analysis", "labels": [], "entities": [{"text": "mathematical analysis", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8339091539382935}]}, {"text": " Table 7: A sample of normalised stop-words", "labels": [], "entities": []}, {"text": " Table 8: Precision of the ATR method  (with the usage of a stoplist)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9877983927726746}, {"text": "ATR", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.5152970552444458}]}]}