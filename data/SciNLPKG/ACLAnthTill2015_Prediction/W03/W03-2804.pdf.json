{"title": [{"text": "A Quantitative Method for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8994898200035095}]}], "abstractContent": [{"text": "Accurate evaluation of machine translation (MT) is an open problem.", "labels": [], "entities": [{"text": "Accurate evaluation of machine translation (MT)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7404111437499523}]}, {"text": "A brief survey of the current approach to tackle this problem is presented and anew proposal is introduced.", "labels": [], "entities": []}, {"text": "This proposal attempts to measure the percentage of words, which should be modified at the output of an automatic translator in order to obtain a correct translation.", "labels": [], "entities": []}, {"text": "To show the feasibility of the method we have assessed the most important Spanish-Catalan translators in comparing the results obtained by the various methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research in automatic translation lacks an appropriate, consistent and easy to use criterion for evaluating the results ().", "labels": [], "entities": [{"text": "automatic translation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7246343791484833}]}, {"text": "However, it turns out to be indispensable to have some tool that may allow us to compare two translation systems or to elicit how any variation of our system may affect the quality of the translations.", "labels": [], "entities": []}, {"text": "This is important in the field of research as well as when a user has to choose between two or more translators.", "labels": [], "entities": []}, {"text": "The evaluation of a translation system shows a number of inherent difficulties.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9774599075317383}]}, {"text": "First of all we are dealing with a subjective process, which is even difficult to define.", "labels": [], "entities": []}, {"text": "This paper is circumscribed to the project SISHITRA (SIStemas H\u00cdbridos para la TRAducci\u00f3n valenciano-castellano supported by the Spanish Government), whose aim is the construction of an automatic translator between Spanish and Catalan texts using hybrid methods (both deductive and inductive).", "labels": [], "entities": []}, {"text": "In the following section we discuss some of the most important translation quality metrics.", "labels": [], "entities": []}, {"text": "After that, we introduce a semiautomatic methodology for MT evaluation and we show a tool to facilitate this kind of evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.983814001083374}]}, {"text": "Finally, we present the results obtained on the evaluation of several Spanish-Catalan translators.", "labels": [], "entities": []}], "datasetContent": [{"text": "Within the scope of inductive translation, the use of objective metrics, which can be evaluated automatically, is quite frequent.", "labels": [], "entities": [{"text": "inductive translation", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7063420414924622}]}, {"text": "These metrics take as their starting point a possible reference translation for each of the sentences we want to translate.", "labels": [], "entities": []}, {"text": "This reference will be compared with the proposed sentences by the translation system.", "labels": [], "entities": []}, {"text": "The most important metric systems are: Word Error Rate (WER): WER is the percentage of words, which are to be inserted, deleted or replaced in the translation in order to obtain the sentence of reference (Vidal, 1997;).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.8574323952198029}, {"text": "WER", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9563606977462769}]}, {"text": "WER can be obtained automatically by using the editing distance between both sentences.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8408833742141724}]}, {"text": "This metric is computed efficiently and is reproducible (successive applications to the same data produce the same results).", "labels": [], "entities": []}, {"text": "However, the main drawback is its dependency on the sentences of reference.", "labels": [], "entities": []}, {"text": "There is an almost unlimited number of correct translations for one and the same sentence and, however, this metric considers only one to be correct.", "labels": [], "entities": []}, {"text": "Other kinds of metrics have been developed, which require human intervention in order to obtain an evaluation.", "labels": [], "entities": []}, {"text": "Among the most widely used we could stand out: Subjective Sentence Error Rate (SSER) Each sentence is scored from 0 to 10, according to its translation quality ().", "labels": [], "entities": [{"text": "Subjective Sentence Error Rate (SSER)", "start_pos": 47, "end_pos": 84, "type": "METRIC", "confidence": 0.8016646504402161}]}, {"text": "An example of these categories is: 0 -nonsensical...", "labels": [], "entities": []}, {"text": "1 -some aspects of the content are conveyed ...", "labels": [], "entities": []}, {"text": "5 -comprehensible, but with important syntactic errors ...", "labels": [], "entities": []}, {"text": "The biggest problem shown by this technique is its subjective nature.", "labels": [], "entities": []}, {"text": "Two people who may evaluate the same experiment could obtain quite different results.", "labels": [], "entities": []}, {"text": "To solve this problem several evaluations can be performed.", "labels": [], "entities": []}, {"text": "Another drawback is that the different sentence lengths have not been taken into account.", "labels": [], "entities": []}, {"text": "The score of a 100 word-long sentence has the same impact on the total score as that of a word-long sentence.", "labels": [], "entities": []}, {"text": "Automatic metrics are especially useful, since their cost is practically null.", "labels": [], "entities": []}, {"text": "However, they are very dependent on the used references.", "labels": [], "entities": []}, {"text": "In some cases they can yield misleading results, for instance, if we want to compare an inductive translation system with some deductive one which, in principle, should produce translations of a similar quality.", "labels": [], "entities": []}, {"text": "If we extract the references from the same source as the training material of the inductive translator, the inductive translator will have an advantage over the deductive translator, since it has learned to translate by using a vocabulary and structures that are similar to those appearing in the references.", "labels": [], "entities": []}, {"text": "The non-automatic evaluation metrics described above presents various constraints: When an SSER is used, it maybe very difficult to decide the score to be assigned to one sentence.", "labels": [], "entities": []}, {"text": "For example, if in one sentence a small syntactic error appears, we can assign an 8.", "labels": [], "entities": []}, {"text": "If in the following sentence two similar errors appear, what score should we assign?", "labels": [], "entities": []}, {"text": "The same or half the score?", "labels": [], "entities": []}, {"text": "To solve these kinds of matters, IER introduces the concept of \"information item\".", "labels": [], "entities": []}, {"text": "This proposal has the drawback of being quite costly, both during the initial stage of deciding the word segments which form each item as well as when classifying the correction for each item.", "labels": [], "entities": []}, {"text": "After having seen the previous drawbacks the following metric has been introduced:  In order to facilitate the evaluation of automatic translators a graphic user interface has been implemented.", "labels": [], "entities": []}, {"text": "The metrics provided by the program are: WER, mWER, aWER, SER, SSER and aSER.", "labels": [], "entities": [{"text": "WER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.965858519077301}, {"text": "aWER", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9644763469696045}, {"text": "SER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9949811100959778}, {"text": "SSER", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.975716769695282}, {"text": "aSER", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9900357723236084}]}, {"text": "shows how it is displayed.", "labels": [], "entities": []}, {"text": "Next, the way the program works is described: On the editing window from top to bottom the following items are displayed: the source sentence, the sentence to be evaluated, the new sentences proposed by the user, the four most similar references to the sentence under evaluation (according to editing distance).", "labels": [], "entities": []}, {"text": "The new sentence proposed by the user will be in principle the same as that of the most similar reference.", "labels": [], "entities": []}, {"text": "In the sentence being evaluated using different colours, depending on whether they are considered insertions, replacements or deletions, the words that maybe wrong are highlighted.", "labels": [], "entities": []}, {"text": "The user can click with the mouse on those words that maybe considered correct.", "labels": [], "entities": []}, {"text": "As a result, this action will modify the new reference.", "labels": [], "entities": []}, {"text": "In the example (), if the user clicks on the highlighted words \"-\", \"Diagram\" and \"locate\", he will obtain the new reference \"Diagram shows the scan procedure to locate the archives.\".", "labels": [], "entities": []}, {"text": "This new reference reduces the editing distance from 5 to 2.", "labels": [], "entities": [{"text": "editing distance", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.8760717213153839}]}, {"text": "The user will also be able to click directly on some word of new reference to modify it.", "labels": [], "entities": []}, {"text": "The aim of this is to allow the evaluator the introduction of any new reference which maybe a correct translation of the source sentence and which, furthermore, may resemble most closely the sentence being evaluated.", "labels": [], "entities": []}, {"text": "This tool can be obtained for free on (http://ttt.gan.upv.es/~jtomas/eval), both in the Linux version as wells as in Windows.", "labels": [], "entities": []}, {"text": "A format in XML has been defined to store the reference files.", "labels": [], "entities": []}, {"text": "For each evaluation sentence we store: the source sentence, the target reference sentences and the target sentences proposed by the different MT with their subjective evaluations.", "labels": [], "entities": []}, {"text": "Should during an aWER evaluation anew reference be proposed, this one is also stored.", "labels": [], "entities": []}, {"text": "An example of a file with a sentence under evaluation is shown as follows: <evalTrans> <sentence> <source> La figura muestra el m\u00e9todo.", "labels": [], "entities": []}, {"text": "</source> <eval translator=\"first reference\"> <target> This figure shows the procedure.", "labels": [], "entities": []}, {"text": "</target> </eval> <eval translator=\"multi reference\"> <target> This figure shows the method.", "labels": [], "entities": []}, {"text": "</target> </eval> <eval translator=\"Statistical\" evaluator=\"JM\" sser=\"8\" awer=\"1/5\"> <target> Chart represent the method.", "labels": [], "entities": []}, {"text": "</target> <newRef> Chart represents the method.", "labels": [], "entities": []}, {"text": "</newRef> </eval> </sentence> ...", "labels": [], "entities": []}, {"text": "In order to carryout our evaluation, we have translated 120 sentences (2456 words) with the different MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.7249243855476379}]}, {"text": "These sentences have been taken from different media: a newspaper, a technical manual, legal text...", "labels": [], "entities": []}, {"text": "The references used by the WER were also taken from the Catalan version of the same documents.", "labels": [], "entities": [{"text": "WER", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7101663947105408}]}, {"text": "In mWER and in BLEU we used three additional references.", "labels": [], "entities": [{"text": "mWER", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9598320722579956}, {"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9966011047363281}]}, {"text": "These new references have been introduced by a human translator modifying the initial reference.", "labels": [], "entities": []}, {"text": "Before applying the metrics shown in point 2, a human expert carries out a detailed analysis in order to establish the quality of the translations.", "labels": [], "entities": []}, {"text": "The experiment consists of sorting out the four outputs obtained by each translator for each test sentence, according to its quality.", "labels": [], "entities": []}, {"text": "If the expert does no find any quality difference between the sentences proposed by two translators, he assigns the same rank to them.", "labels": [], "entities": []}, {"text": "After this sentence by sentence analysis, the expert concludes that Salt is the better translator, followed closely by Incyta.", "labels": [], "entities": []}, {"text": "Statistical is in an intermediate position and the worst is Internostrum.", "labels": [], "entities": [{"text": "Internostrum", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.7372077703475952}]}], "tableCaptions": [{"text": " Table 2. Comparative classification sentence by sentence.", "labels": [], "entities": [{"text": "Comparative classification sentence", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.940426230430603}]}, {"text": " Table 3. Comparative evaluation time (minutes) of the 120 sentences using  the different metrics. *Time spent to introduce the proposed references.", "labels": [], "entities": []}]}