{"title": [{"text": "Learning to \"Read Between the Lines\" using Bayesian Logic Programs", "labels": [], "entities": []}], "abstractContent": [{"text": "Most information extraction (IE) systems identify facts that are explicitly stated in text.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.8660309314727783}]}, {"text": "However, in natural language, some facts are implicit, and identifying them requires \"read-ing between the lines\".", "labels": [], "entities": []}, {"text": "Human readers naturally use commonsense knowledge to infer such implicit information from the explicitly stated facts.", "labels": [], "entities": []}, {"text": "We propose an approach that uses Bayesian Logic Programs (BLPs), a statistical relational model combining first-order logic and Bayesian networks, to infer additional implicit information from extracted facts.", "labels": [], "entities": []}, {"text": "It involves learning uncertain common-sense knowledge (in the form of probabilis-tic first-order rules) from natural language text by mining a large corpus of automatically extracted facts.", "labels": [], "entities": []}, {"text": "These rules are then used to derive additional facts from extracted information using BLP inference.", "labels": [], "entities": [{"text": "BLP", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.727257251739502}]}, {"text": "Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach .", "labels": [], "entities": [{"text": "machine reading", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7713221311569214}]}], "introductionContent": [{"text": "The task of information extraction (IE) involves automatic extraction of typed entities and relations from unstructured text.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8797311663627625}, {"text": "automatic extraction of typed entities and relations from unstructured text", "start_pos": 49, "end_pos": 124, "type": "TASK", "confidence": 0.7937682569026947}]}, {"text": "IE systems are trained to extract facts that are stated explicitly in text.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9470320343971252}]}, {"text": "However, some facts are implicit, and human readers naturally \"read between the lines\" and infer them from the stated facts using commonsense knowledge.", "labels": [], "entities": []}, {"text": "Answering many queries can require inferring such implicitly stated facts.", "labels": [], "entities": [{"text": "Answering", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9493783116340637}]}, {"text": "Consider the text \"Barack Obama is the president of the United States of America.\"", "labels": [], "entities": []}, {"text": "Given the query \"Barack Obama is a citizen of what country?\", standard IE systems cannot identify the answer since citizenship is not explicitly stated in the text.", "labels": [], "entities": []}, {"text": "However, a human reader possesses the commonsense knowledge that the president of a country is almost always a citizen of that country, and easily infers the correct answer.", "labels": [], "entities": []}, {"text": "The standard approach to inferring implicit information involves using commonsense knowledge in the form of logical rules to deduce additional information from the extracted facts.", "labels": [], "entities": []}, {"text": "Since manually developing such a knowledge base is difficult and arduous, an effective alternative is to automatically learn such rules by mining a substantial database of facts that an IE system has already automatically extracted from a large corpus of text).", "labels": [], "entities": []}, {"text": "Most existing rule learners assume that the training data is largely accurate and complete.", "labels": [], "entities": []}, {"text": "However, the facts extracted by an IE system are always quite noisy and incomplete.", "labels": [], "entities": [{"text": "IE", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9122246503829956}]}, {"text": "Consequently, a purely logical approach to learning and inference is unlikely to be effective.", "labels": [], "entities": []}, {"text": "Consequently, we propose using statistical relational learning (SRL), specifically, Bayesian Logic Programs (BLPs) (, to learn probabilistic rules in first-order logic from a large corpus of extracted facts and then use the resulting BLP to make effective probabilistic inferences when interpreting new documents.", "labels": [], "entities": [{"text": "statistical relational learning (SRL)", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7029656569163004}, {"text": "interpreting new documents", "start_pos": 286, "end_pos": 312, "type": "TASK", "confidence": 0.8700720071792603}]}, {"text": "We have implemented this approach by using an off-the-shelf IE system and developing novel adaptations of existing learning methods to efficiently construct fast and effective BLPs for \"reading be-tween the lines.\"", "labels": [], "entities": []}, {"text": "We present an experimental evaluation of our resulting system on a realistic test corpus from DARPA's Machine Reading project, and demonstrate improved performance compared to a purely logical approach based on Inductive Logic Programming (ILP), and an alternative SRL approach based on Markov Logic Networks (MLNs).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first paper that employs BLPs for inferring implicit information from natural language text.", "labels": [], "entities": [{"text": "BLPs", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9444568157196045}]}, {"text": "We demonstrate that it is possible to learn the structure and the parameters of BLPs automatically using only noisy extractions from natural language text, which we then use to infer additional facts from text.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work and highlights key differences between our approach and existing work.", "labels": [], "entities": []}, {"text": "Section 3 provides a brief background on BLPs.", "labels": [], "entities": [{"text": "BLPs", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.5487874746322632}]}, {"text": "Section 4 describes our BLP-based approach to learning to infer implicit facts.", "labels": [], "entities": [{"text": "BLP-based", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.7379865050315857}]}, {"text": "Section 5 describes our experimental methodology and discusses the results of our evaluation.", "labels": [], "entities": []}, {"text": "Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The lack of ground truth annotation for inferred facts prevents an automated evaluation, so we resorted to a manual evaluation.", "labels": [], "entities": []}, {"text": "We randomly sampled 40 documents (4 from each test fold), judged the accuracy of the inferences for those documents, and computed precision, the fraction of inferences that were deemed correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.998702883720398}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9990212917327881}]}, {"text": "For probabilistic methods like BLPs and MLNs that provide certainties for their inferences, we also computed precision at top n, which measures the precision of then inferences with the highest marginal probability across the 40 test documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9984015822410583}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9928500652313232}]}, {"text": "Measuring recall for making inferences is very difficult since it would require labeling a reasonable-sized corpus of documents with all of the correct inferences fora given set of target relations, which would be extremely time consuming.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9970464110374451}]}, {"text": "Our evaluation is similar to that used in previous related work.", "labels": [], "entities": []}, {"text": "SIRE frequently makes incorrect extractions, and therefore inferences made from these extractions are also inaccurate.", "labels": [], "entities": []}, {"text": "To account for the mistakes made by the extractor, we report two different precision scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9988499879837036}]}, {"text": "The \"unadjusted\" (UA) score, does not correct for errors made by the extractor.", "labels": [], "entities": [{"text": "unadjusted\" (UA) score", "start_pos": 5, "end_pos": 27, "type": "METRIC", "confidence": 0.9094703495502472}]}, {"text": "The \"adjusted\" (AD) score does not count mistakes due to extraction errors.", "labels": [], "entities": [{"text": "adjusted\" (AD) score", "start_pos": 5, "end_pos": 25, "type": "METRIC", "confidence": 0.9723880986372629}]}, {"text": "That is, if an inference is incorrect because it was based on incorrect extracted facts, we remove it from the set of inferences and calculate precision for the remaining inferences.", "labels": [], "entities": [{"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9995705485343933}]}], "tableCaptions": []}