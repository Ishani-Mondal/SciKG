{"title": [{"text": "Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining", "labels": [], "entities": [{"text": "Syntactic Analysis", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.7318500429391861}, {"text": "Statistical Language Modeling", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8641436497370402}, {"text": "Substructure Sharing", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.8615894019603729}]}], "abstractContent": [{"text": "Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7269681543111801}, {"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7902413308620453}]}, {"text": "However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set.", "labels": [], "entities": []}, {"text": "In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures.", "labels": [], "entities": [{"text": "substructure sharing", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.823403537273407}]}, {"text": "We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training.", "labels": [], "entities": [{"text": "speech tagger", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7003681808710098}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9986881613731384}]}, {"text": "When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.796172708272934}, {"text": "WER reduction", "start_pos": 198, "end_pos": 211, "type": "METRIC", "confidence": 0.8744918406009674}]}], "introductionContent": [{"text": "Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 124, "end_pos": 158, "type": "TASK", "confidence": 0.8104156951109568}, {"text": "machine translation (MT)", "start_pos": 163, "end_pos": 187, "type": "TASK", "confidence": 0.8424840092658996}]}, {"text": "While traditional LMs use word n-grams, where then \u2212 1 previous words predict the next word, newer models integrate long-span information in making decisions.", "labels": [], "entities": []}, {"text": "For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams).", "labels": [], "entities": []}, {"text": "The long-distance dependencies can be modeled in either a generative or a discriminative framework.", "labels": [], "entities": []}, {"text": "Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (); these models with syntactic information have obtained state of the art results.", "labels": [], "entities": []}, {"text": "However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N -best lists).", "labels": [], "entities": []}, {"text": "For discriminative models, this limitation applies to training as well.", "labels": [], "entities": []}, {"text": "Moreover, the non-local features used in rescoring are usually extracted via auxiliary toolswhich in the case of syntactic features include part of speech taggers and parsers -from a set of ASR system hypotheses.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.9637764692306519}]}, {"text": "Separately applying auxiliary tools to each N -best list hypothesis leads to major inefficiencies as many hypotheses differ only slightly.", "labels": [], "entities": []}, {"text": "Recent work on hill climbing algorithms for ASR lattice rescoring iteratively searches fora higherscoring hypothesis in a local neighborhood of the current-best hypothesis, leading to a much more efficient algorithm in terms of the number, N , of hypotheses evaluated (; the idea also leads to a discriminative hill climbing training algorithm ().", "labels": [], "entities": [{"text": "ASR lattice rescoring", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.9322661757469177}]}, {"text": "Even so, the reliance on auxiliary tools slow LM application to the point of being impractical for real time systems.", "labels": [], "entities": [{"text": "LM application", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.8664115965366364}]}, {"text": "While faster auxiliary tools are an option, they are usually less accurate.", "labels": [], "entities": []}, {"text": "In this paper, we propose a general modifica-tion to the decoders used in auxiliary tools to utilize the commonalities among the set of generated hypotheses.", "labels": [], "entities": []}, {"text": "The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions.", "labels": [], "entities": []}, {"text": "We demonstrate our approach on a local Perceptron based part of speech tagger) and a shift reduce dependency parser, yielding significantly faster tagging and parsing of ASR hypotheses.", "labels": [], "entities": [{"text": "ASR hypotheses", "start_pos": 170, "end_pos": 184, "type": "TASK", "confidence": 0.7690975069999695}]}, {"text": "While these simpler structured prediction models are faster, we compensate for the model's simplicity through uptraining ( ), yielding auxiliary tools that are both fast and accurate.", "labels": [], "entities": []}, {"text": "The result is significant speed improvements and a reduction in word error rate (WER) for both N -best list and the already fast hill climbing rescoring.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 64, "end_pos": 85, "type": "METRIC", "confidence": 0.9113451739152273}]}, {"text": "The net result is arguably the first syntactic LM fast enough to be used in areal time ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8150485157966614}]}], "datasetContent": [{"text": "Our ASR system is based on the 2007 IBM Speech transcription system for the GALE Distillation Go/No-go Evaluation () with state of the art discriminative acoustic models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9676878452301025}, {"text": "GALE Distillation Go/No-go Evaluation", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.5706539154052734}]}, {"text": "We use a modified Kneser-Ney (KN) backoff 4-gram baseline LM.", "labels": [], "entities": []}, {"text": "Word-lattices for discriminative training and rescoring come from this baseline ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9772112369537354}]}, {"text": "The longspan discriminative LM's baseline feature weight (\u03b1 0 ) is tuned on dev data and hill climbing) is used for training and rescoring.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 129, "end_pos": 138, "type": "TASK", "confidence": 0.9464255571365356}]}, {"text": "The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of , a state of the art broadcast news (BN) parser, with phrase structures converted to labeled dependencies by the Stanford converter.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7367675304412842}, {"text": "POS tagger", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.7008984535932541}, {"text": "CKY-style bottom-up constituent parser", "start_pos": 106, "end_pos": 144, "type": "TASK", "confidence": 0.7406450361013412}]}, {"text": "While accurate, the parser has a huge grammar (32GB) from using products of latent variable grammars and requires O(l 3 ) time to parse a sentence of length l.", "labels": [], "entities": [{"text": "O(l 3 ) time", "start_pos": 114, "end_pos": 126, "type": "METRIC", "confidence": 0.9126643538475037}]}, {"text": "Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long, although the shorter up-training text data was not a problem.", "labels": [], "entities": [{"text": "ASR rescoring", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9758197665214539}]}, {"text": "We evaluate both unlabeled (UAS) and labeled dependency accuracy (LAS).", "labels": [], "entities": [{"text": "UAS", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.7966873645782471}, {"text": "labeled dependency accuracy (LAS)", "start_pos": 37, "end_pos": 70, "type": "METRIC", "confidence": 0.7857484519481659}]}], "tableCaptions": [{"text": " Table 2: A summary of the data for training and evaluation. The Ontonotes corpus is from Weischedel et al. (2008).", "labels": [], "entities": [{"text": "Ontonotes corpus", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9497347772121429}]}, {"text": " Table 3: Speedups and WER for hill climbing rescor- ing. Substructure sharing yields a 5.3 times speedup. The  times for with and without up-training are nearly identi- cal, so we include only one set for clarity. Time spent  is dominated by the parser, so the faster parser accounts  for much of the overall speedup. Timing information in- cludes neighborhood generation and LM rescoring, so it  is more than the sum of the times in", "labels": [], "entities": [{"text": "WER", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9992007613182068}, {"text": "hill climbing rescor- ing", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7018620491027832}, {"text": "clarity", "start_pos": 206, "end_pos": 213, "type": "METRIC", "confidence": 0.9777040481567383}, {"text": "LM rescoring", "start_pos": 377, "end_pos": 389, "type": "TASK", "confidence": 0.6173218786716461}]}, {"text": " Table 4: Time in seconds for the parser and POS tagger  to process hypotheses during hill climbing rescoring.", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9712358713150024}, {"text": "hill climbing rescoring", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.5735191901524862}]}]}