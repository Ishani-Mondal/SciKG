{"title": [], "abstractContent": [{"text": "Our research aims at building computational models of word meaning that are perceptually grounded.", "labels": [], "entities": []}, {"text": "Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models.", "labels": [], "entities": []}, {"text": "Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic re-latedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words.", "labels": [], "entities": []}, {"text": "Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in mul-timodal models often improves performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional semantic space models represent meaning on the basis of word co-occurrence statistics in large text corpora).", "labels": [], "entities": []}, {"text": "These models (as well as virtually all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation), crucially on the information gathered through perception.", "labels": [], "entities": []}, {"text": "Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision.", "labels": [], "entities": []}, {"text": "A few studies have begun to use visual information extracted from images as part of distributional semantic models;).", "labels": [], "entities": []}, {"text": "These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 172, "end_pos": 182, "type": "DATASET", "confidence": 0.9479889273643494}]}, {"text": "This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carryout a systematic comparison of models using textual, visual, and both types of information.", "labels": [], "entities": []}, {"text": "(2) We evaluate the models on general semantic relatedness tasks and on two specific tasks where visual information is highly relevant, as they focus on color terms.", "labels": [], "entities": []}, {"text": "(3) Unlike previous work, we study the impact of using different kinds of visual information for these semantic tasks.", "labels": [], "entities": []}, {"text": "Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general semantic tasks, they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words.", "labels": [], "entities": []}, {"text": "Moreover, we show that visual and textual information are tapping on different aspects of meaning, such that they are complementary sources of information, and indeed combining them in multimodal models often improves performance.", "labels": [], "entities": []}, {"text": "We also show that \"hybrid\" models exploiting the patterns of co-occurrence of words as tags of the same images can be a powerful surrogate of visual information under certain circumstances.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the textual, visual, multimodal, and hybrid models we use for our experiments.", "labels": [], "entities": []}, {"text": "We present our experiments in sections 3 to 5.", "labels": [], "entities": []}, {"text": "Section 6 reviews related work, and section 7 finishes with conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In Experiment 1, we test the hypothesis that the relation between words denoting concrete things and words denoting their typical color is reflected by the distance of the corresponding vectors better when the models are sensitive to visual information.", "labels": [], "entities": []}, {"text": "Experiment 2 requires more sophisticated information than Experiment 1, as it involves distinguishing between literal and nonliteral uses of color terms.", "labels": [], "entities": []}, {"text": "We also experimented with a model based on direct cooccurrence of adjectives and nouns, obtaining promising results in a preliminary version of Exp.", "labels": [], "entities": [{"text": "Exp.", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.9130718111991882}]}, {"text": "1. We abandoned this approach because such a model inherently lacks scalability, as it will not generalize behind cases where the training data contain direct examples of co-occurrences of the target pairs.", "labels": [], "entities": []}], "tableCaptions": []}