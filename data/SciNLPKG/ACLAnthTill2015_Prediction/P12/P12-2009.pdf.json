{"title": [{"text": "A Novel Burst-based Text Representation Model for Scalable Event Detection", "labels": [], "entities": [{"text": "Scalable Event Detection", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7016877233982086}]}], "abstractContent": [{"text": "Mining retrospective events from text streams has been an important research topic.", "labels": [], "entities": [{"text": "Mining retrospective events", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8159970243771871}]}, {"text": "Classic text representation model (i.e., vector space model) cannot model temporal aspects of documents.", "labels": [], "entities": []}, {"text": "To address it, we proposed a novel burst-based text representation model, denoted as BurstVSM.", "labels": [], "entities": [{"text": "BurstVSM", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.8931109309196472}]}, {"text": "BurstVSM corresponds dimensions to bursty features instead of terms, which can capture semantic and temporal information.", "labels": [], "entities": [{"text": "BurstVSM", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8780314326286316}]}, {"text": "Meanwhile, it significantly reduces the number of non-zero entries in the representation.", "labels": [], "entities": []}, {"text": "We test it via scalable event detection , and experiments in a 10-year news archive show that our methods are both effective and efficient.", "labels": [], "entities": [{"text": "event detection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7116824388504028}]}], "introductionContent": [{"text": "Mining retrospective events () has been quite an important research topic in text mining.", "labels": [], "entities": [{"text": "Mining retrospective events", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.84713747104009}, {"text": "text mining", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.8998763561248779}]}, {"text": "One standard way for that is to cluster news articles as events by following a two-step approach (): 1) represent document as vectors and calculate similarities between documents; 2) run the clustering algorithm to obtain document clusters as events.", "labels": [], "entities": []}, {"text": "Underlying text representation often plays a critical role in this approach, especially for long text streams.", "labels": [], "entities": []}, {"text": "In this paper, our focus is to study how to represent temporal documents effectively for event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.7793508470058441}]}, {"text": "Classical text representation methods, i.e., Vector Space Model (VSM), have a few shortcomings when dealing with temporal documents.", "labels": [], "entities": []}, {"text": "The major one is that it maps one dimension to one term, which completely ignores temporal information, and therefore VSM can never capture the evolving trends in text streams.", "labels": [], "entities": []}, {"text": "See the example in may have a high similarity based on VSM due to the presence of some general terms (e.g., \"election\") related to U.S. presidential election, although general terms correspond to events in different periods (i.e.,.", "labels": [], "entities": [{"text": "similarity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9733717441558838}, {"text": "VSM", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.5752695202827454}]}, {"text": "Temporal information has to betaken into consideration for event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.8157901167869568}]}, {"text": "Another important issue is scalability, with the increasing of the number in the text stream, the size of the vocabulary, i.e., the number of dimensions in VSM, can be very large, which requires a considerable amount of space for storage and time for downstream processing.", "labels": [], "entities": []}, {"text": "To address these difficulties, in this paper, we propose a burst based text representation method for scalable event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 111, "end_pos": 126, "type": "TASK", "confidence": 0.7418872714042664}]}, {"text": "The major novelty is to naturally incorporate temporal information into dimensions themselves instead of using external time decaying functions ().", "labels": [], "entities": []}, {"text": "We instantiate this idea by using bursty features as basic representation units of documents.", "labels": [], "entities": []}, {"text": "In this paper, bursty feature refers to a sudden surge of the frequency of a single term in a text stream, and it is represented as the term itself together with the time interval during which the burst takes place.", "labels": [], "entities": []}, {"text": "For example, can be regarded as a bursty feature.", "labels": [], "entities": []}, {"text": "We also call the term in a bursty feature its bursty term.", "labels": [], "entities": []}, {"text": "In our model, each dimension corresponds to a bursty feature, which contains both temporal and semantic information.", "labels": [], "entities": []}, {"text": "Bursty features capture and reflect the evolving topic trends, which can be learnt by searching surge patterns in stream data.", "labels": [], "entities": [{"text": "Bursty", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8191961646080017}]}, {"text": "Built on bursty features, our representation model can well adapt to text streams with complex trends, and therefore provides a more reasonable temporal document representation.", "labels": [], "entities": []}, {"text": "We further propose a split-cluster-merge algorithm to generate clusters as events.", "labels": [], "entities": []}, {"text": "This algorithm can run a mutli-thread mode to speedup processing.", "labels": [], "entities": []}, {"text": "Our contribution can be summarized as two aspects: 1) we propose a novel burst-based text representation model, to our best knowledge, it is the first work which explicitly incorporates temporal information into dimensions themselves; 2) we test this representation model via scalable event detection task on a very large news corpus, and extensive experiments show the proposed methods are both effective and efficient.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a subset of 68 millon deduplicated timestamped web pages generated from this archive ().", "labels": [], "entities": []}, {"text": "Since our major focus is to detect events from news articles, we only keep the web pages with keyword \"news\" in URL field.", "labels": [], "entities": []}, {"text": "The final collection contains 11, 218, 581 articles with total 1, 730, 984, 304 tokens ranging from 2000 to 2009.", "labels": [], "entities": []}, {"text": "We run all the experiments on a 64-bit linux server with four Quad-Core AMD Opteron(tm) Processors and 64GB of RAM.", "labels": [], "entities": []}, {"text": "For split-cluster-merge algorithm, we implement the cluster step in a multithread mode, so that different parts can be processed in parallel.", "labels": [], "entities": []}, {"text": "Similar to the evaluation in information retrieval , given a target event, we evaluate the quality of the returned \"relevant\" documents by systems.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7438809275627136}]}, {"text": "We use average precision, average recall and mean average precision(MAP) as evaluation metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.7478628158569336}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9111781120300293}, {"text": "mean average precision(MAP)", "start_pos": 45, "end_pos": 72, "type": "METRIC", "confidence": 0.9683050711949667}]}, {"text": "A difference is that we do not have queries, and the output of a system is a set of document clusters.", "labels": [], "entities": []}, {"text": "So fora system, given an event in golden standard, we first select the cluster (the system generates) which has the  most relevant documents, then sort the documents in the descending order of similarities with the cluster centroid and finally compute P, R ,F and MAP in this cluster.", "labels": [], "entities": [{"text": "F", "start_pos": 258, "end_pos": 259, "type": "METRIC", "confidence": 0.9586411714553833}, {"text": "MAP", "start_pos": 264, "end_pos": 267, "type": "METRIC", "confidence": 0.9720511436462402}]}, {"text": "We perform Wilcoxon signed-rank test for significance testing.", "labels": [], "entities": []}, {"text": "We used the event detection method in) as baseline, denoted as timemines-\u03c7 2 . As) suggested, we tried two versions: 1) using all nouns and 2) using all named entities.", "labels": [], "entities": [{"text": "event detection", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7039736807346344}]}, {"text": "Recall that BurstVSM relies on bursty features as dimensions, we tested different burst detection algorithms in our proposed BurstVSM model, including swan (Swan and Allan, 2000), kleinberg and our proposed TVBurst algorithm.", "labels": [], "entities": []}, {"text": "In, we can see that 1) BurstVSM with any of these three burst detection algorithms is significantly better than timemines-\u03c7 2 , suggesting our event detection method is very effective; 2) TVBurst with BurstVSM gives the best performance, which suggests using moving average base probability will improve the performance of burst detection.", "labels": [], "entities": [{"text": "burst detection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.721622571349144}, {"text": "event detection", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.7313877791166306}, {"text": "burst detection", "start_pos": 323, "end_pos": 338, "type": "TASK", "confidence": 0.8057314455509186}]}, {"text": "We use TVBurst as the default burst detection algorithm in later experiments.", "labels": [], "entities": [{"text": "TVBurst", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9731805324554443}, {"text": "burst detection", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7214157581329346}]}, {"text": "Then we compare the performance of different text representation models for event detection, namely).", "labels": [], "entities": [{"text": "event detection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7411353439092636}]}, {"text": "For different representation models, we use split-cluster-merge as clustering algorithm.", "labels": [], "entities": []}, {"text": "shows that BurstVSM is much effecitve than boostVSM for event detection.", "labels": [], "entities": [{"text": "BurstVSM", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9895119071006775}, {"text": "boostVSM", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.978477954864502}, {"text": "event detection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7799317240715027}]}, {"text": "In fact, we empirically find boostVSM is appropriate for We use the same parameter settings in the original paper.", "labels": [], "entities": []}, {"text": "clustering documents in a coarse grain (e.g., in topic level) but not for event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.8069683015346527}]}, {"text": "In our methods, event detection is treated as document clustering.", "labels": [], "entities": [{"text": "event detection", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.8316908776760101}, {"text": "document clustering", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6927336901426315}]}, {"text": "It is very important to study how similarities affect the performance of clustering.", "labels": [], "entities": []}, {"text": "To see why our proposed representation methods are better than boostVSM, we present the average intra-class similarity and inter-class similarity for different events in.", "labels": [], "entities": []}, {"text": "We can see BurstVSM results in a larger intra-class similarity and a smaller inter-class similarity than boostVSM.", "labels": [], "entities": [{"text": "BurstVSM", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9528144598007202}]}, {"text": "Analysis of the space/time complexity.", "labels": [], "entities": []}, {"text": "We further analyze the space/time complexity of different representation models.", "labels": [], "entities": []}, {"text": "We can see that BurstVSM has much smaller space/time cost compared with boostVSM, and meanwhile it has a better performance for event detection (See).", "labels": [], "entities": [{"text": "BurstVSM", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9431931376457214}, {"text": "event detection", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.7802006900310516}]}, {"text": "In burst-based representation, one document has fewer non-zero entries.", "labels": [], "entities": []}, {"text": "The core idea of this work is initialized and developped by Kai Fan.", "labels": [], "entities": []}, {"text": "This work is partially supported by HGJ 2010 Grant 2011ZX01042-001-001, NSFC Grant 61073082 and 60933004.", "labels": [], "entities": [{"text": "HGJ 2010 Grant 2011ZX01042-001-001", "start_pos": 36, "end_pos": 70, "type": "DATASET", "confidence": 0.8923188000917435}, {"text": "NSFC Grant 61073082", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.9150185386339823}]}, {"text": "Xin Zhao is supported by Google PhD Fellowship (China).", "labels": [], "entities": [{"text": "Google PhD Fellowship", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.9270575245221456}]}, {"text": "We thank the insightful comments from Junjie Yao, Jing Liu and the anonymous reviewers.", "labels": [], "entities": []}, {"text": "We have developped an online Chinese large-scale event search engine based on this work, visit http://sewm.pku.edu.cn/eventsearch for more details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of event detection. Our proposed method is better than all the other baselines at confidence level 0.9.", "labels": [], "entities": [{"text": "event detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8541608452796936}]}, {"text": " Table 3: Comparisons of average intra-class and inter- class similarity.", "labels": [], "entities": []}, {"text": " Table 4: Comparisons of observed runtime and storage.", "labels": [], "entities": []}]}