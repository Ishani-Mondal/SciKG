{"title": [{"text": "Cross-Lingual Mixture Model for Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.9843473136425018}]}], "abstractContent": [{"text": "The amount of labeled sentiment data in En-glish is much larger than that in other languages.", "labels": [], "entities": []}, {"text": "Such a disproportion arouse interest in cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) using labeled data in the source language (e.g. English).", "labels": [], "entities": [{"text": "cross-lingual sentiment classification", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.7742897272109985}, {"text": "sentiment classification", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.8663063943386078}]}, {"text": "Most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7136874496936798}]}, {"text": "This approach suffers from the limited coverage of vocabulary in the machine translation results.", "labels": [], "entities": []}, {"text": "In this paper, we propose a gen-erative cross-lingual mixture model (CLMM) to leverage unlabeled bilingual parallel data.", "labels": [], "entities": []}, {"text": "By fitting parameters to maximize the likelihood of the bilingual parallel data, the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly.", "labels": [], "entities": []}, {"text": "Experiments on multiple data sets show that CLMM is consistently effective in two settings: (1) labeled data in the target language are unavailable; and (2) labeled data in the target language are also available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment Analysis (also known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9260211288928986}, {"text": "opinion mining)", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8246604800224304}]}, {"text": "Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9564832746982574}, {"text": "sentiment analysis", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.9404720366001129}]}, {"text": "There is * Contribution during internship at Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.90188201268514}]}, {"text": "already a large amount of work on sentiment classification of text in various genres and in many languages.", "labels": [], "entities": [{"text": "sentiment classification of text", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.9366454929113388}]}, {"text": "For example, focus on sentiment classification of movie reviews in English, and study the problem of classifying product reviews in Chinese.", "labels": [], "entities": [{"text": "sentiment classification of movie reviews", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.9458431482315064}]}, {"text": "During the past few years, NTCIR 1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (.", "labels": [], "entities": [{"text": "NTCIR 1", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.7720046043395996}, {"text": "sentiment classification of news articles written in English", "start_pos": 69, "end_pos": 129, "type": "TASK", "confidence": 0.9265908971428871}]}, {"text": "For English sentiment classification, there are several labeled corpora available ().", "labels": [], "entities": [{"text": "English sentiment classification", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8018437226613363}]}, {"text": "However, labeled resources in other languages are often insufficient or even unavailable.", "labels": [], "entities": []}, {"text": "Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages.", "labels": [], "entities": [{"text": "sentiment classification of documents", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.9171247035264969}]}, {"text": "One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language.", "labels": [], "entities": []}, {"text": "Although the machine-translation-based methods are intuitive, they have certain limitations.", "labels": [], "entities": []}, {"text": "First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words cannot be learned from the translated labeled data.", "labels": [], "entities": []}, {"text": "report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of show that vocabulary coverage has a strong correlation with sentiment classification accuracy.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 223, "end_pos": 247, "type": "TASK", "confidence": 0.8264114856719971}, {"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.7679908275604248}]}, {"text": "Second, machine translation may change the sentiment polarity of the original text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7402148842811584}]}, {"text": "For example, the negative English sentence \"It is too good to be true\" is translated to a positive sentence in Chinese \"\u8fd9 \u662f \u597d \u5f97 \u662f \u771f \u5b9e \u7684\" by Google Translate (http://translate.google.com/), which literally means \"It is good and true\".", "labels": [], "entities": []}, {"text": "In this paper we propose a cross-lingual mixture model (CLMM) for cross-lingual sentiment classification.", "labels": [], "entities": [{"text": "cross-lingual sentiment classification", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.7748618523279825}]}, {"text": "Instead of relying on the unreliable machine translated labeled data, CLMM leverages bilingual parallel data to bridge the language gap between the source language and the target language.", "labels": [], "entities": []}, {"text": "CLMM is a generative model that treats the source language and target language words in parallel data as generated simultaneously by a set of mixture components.", "labels": [], "entities": []}, {"text": "By \"synchronizing\" the generation of words in the source language and the target language in a parallel corpus, the proposed model can (1) improve vocabulary coverage by learning sentiment words from the unlabeled parallel corpus; (2) transfer polarity label information between the source language and target language using a parallel corpus.", "labels": [], "entities": [{"text": "vocabulary coverage", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.6762123554944992}]}, {"text": "Besides, CLMM can improve the accuracy of cross-lingual sentiment classification consistently regardless of whether labeled data in the target language are present or not.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992069602012634}, {"text": "cross-lingual sentiment classification", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.7957637906074524}]}, {"text": "We evaluate the model on sentiment classification of Chinese using English labeled data.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.958752453327179}]}, {"text": "The experiment results show that CLMM yields 71% inaccuracy when no Chinese labeled data are used, which significantly improves Chinese sentiment classification and is superior to the SVM and co-training based methods.", "labels": [], "entities": [{"text": "Chinese sentiment classification", "start_pos": 128, "end_pos": 160, "type": "TASK", "confidence": 0.7790029644966125}]}, {"text": "When Chinese labeled data are employed, CLMM yields 83% inaccuracy, which is remarkably better than the SVM and achieve state-of-the-art performance.", "labels": [], "entities": []}, {"text": "This paper makes two contributions: (1) we propose a model to effectively leverage large bilingual parallel data for improving vocabulary coverage; and (2) the proposed model is applicable in both settings of cross-lingual sentiment classification, irrespective of the availability of labeled data in the target language.", "labels": [], "entities": [{"text": "cross-lingual sentiment classification", "start_pos": 209, "end_pos": 247, "type": "TASK", "confidence": 0.7626094023386637}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We review related work in Section 2, and present the cross-lingual mixture model in Section 3.", "labels": [], "entities": []}, {"text": "Then we present the experimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiment setup: We conduct experiments on two common cross-lingual sentiment classification settings.", "labels": [], "entities": [{"text": "cross-lingual sentiment classification", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.7493561108907064}]}, {"text": "In the first setting, no labeled data in the target language are available.", "labels": [], "entities": []}, {"text": "This setting has realistic significance, since in some situations we need to quickly develop a sentiment classifier for languages that we do not have labeled data in hand.", "labels": [], "entities": []}, {"text": "In this case, we classify text in the target language using only labeled data in the source language.", "labels": [], "entities": []}, {"text": "In the second setting, labeled data in the target language are also available.", "labels": [], "entities": []}, {"text": "In this case, a more reasonable strategy is to make full use of both labeled data in the source language and target language to develop the sentiment classifier for the target language.", "labels": [], "entities": []}, {"text": "In our experiments, we consider English as the source language and Chinese as the target language.", "labels": [], "entities": []}, {"text": "Data sets: For Chinese sentiment classification, we use the same data set described in ().", "labels": [], "entities": [{"text": "Chinese sentiment classification", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.8267220258712769}]}, {"text": "The labeled data sets consist of two English data sets and one Chinese data set.", "labels": [], "entities": [{"text": "Chinese data set", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.7995705107847849}]}, {"text": "The English data set is from the Multi-Perspective Question Answering (MPQA) corpus) and the NT-CIR Opinion Analysis Pilot Task data set ().", "labels": [], "entities": [{"text": "English data set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8436973690986633}, {"text": "Multi-Perspective Question Answering (MPQA)", "start_pos": 33, "end_pos": 76, "type": "TASK", "confidence": 0.7523494362831116}, {"text": "NT-CIR Opinion Analysis Pilot Task data set", "start_pos": 93, "end_pos": 136, "type": "DATASET", "confidence": 0.8508310147694179}]}, {"text": "The Chinese data set also comes from the NTCIR Opinion Analysis Pilot Task data set.", "labels": [], "entities": [{"text": "Chinese data set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9561238487561544}, {"text": "NTCIR Opinion Analysis Pilot Task data set", "start_pos": 41, "end_pos": 83, "type": "DATASET", "confidence": 0.8978328279086522}]}, {"text": "The unlabeled parallel sentences are selected from ISI Chinese-English parallel corpus (.", "labels": [], "entities": [{"text": "ISI Chinese-English parallel corpus", "start_pos": 51, "end_pos": 86, "type": "DATASET", "confidence": 0.794368177652359}]}, {"text": "Following the description in (), we remove neutral sentences and keep only high confident positive and negative sentences as predicted by a maximum entropy classifier trained on the labeled data.", "labels": [], "entities": []}, {"text": "shows the statistics for the data sets used in the experiments.", "labels": [], "entities": []}, {"text": "We conduct experiments on two data settings: (1) MPQA + NTCIR-CH and  When Chinese labeled data are unavailable, we set \u03bb t to 1 and \u03bb s to 0.1, since no Chinese labeled data are used and the contribution of target language to the source language is limited.", "labels": [], "entities": []}, {"text": "When Chinese labeled data are available, we set \u03bb sand \u03bb t to 0.2.", "labels": [], "entities": []}, {"text": "To prevent long sentences from dominating the parameter estimation, we preprocess the data set by normalizing the length of all sentences to the same constant (), the average length of the sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the Data", "labels": [], "entities": []}, {"text": " Table 2: Classification Accuracy Using Only  English Labeled Data", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9443512558937073}, {"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.7334834337234497}]}, {"text": " Table 3: Classification Accuracy Using English and  Chinese Labeled Data", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9578131437301636}, {"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.7581917643547058}]}, {"text": " Table 4: Training Speed Comparison", "labels": [], "entities": [{"text": "Comparison", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.402249813079834}]}]}