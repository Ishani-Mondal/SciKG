{"title": [{"text": "Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation", "labels": [], "entities": [{"text": "Bayesian Word Segmentation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.5564822256565094}]}], "abstractContent": [{"text": "We present a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introduced in Goldwater (2006).", "labels": [], "entities": [{"text": "word segmentation problem", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.8297777573267618}]}, {"text": "By adding rejuve-nation to a particle filter, we are able to considerably improve its performance, both in terms of finding higher probability and higher accuracy solutions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9914727210998535}]}], "introductionContent": [{"text": "The goal of word segmentation is to segment a stream of segments, e.g. characters or phonemes, into words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7407207787036896}]}, {"text": "For example, given the sequence \"youwanttoseethebook\", the goal is to recover the segmented string \"you want to seethe book\".", "labels": [], "entities": []}, {"text": "The models introduced in solve this problem in a fully unsupervised way by defining a generative process for word sequences, making use of the Dirichlet Process (DP) prior.", "labels": [], "entities": []}, {"text": "Until recently, the only inference algorithm applied to these models were batch Markov Chain Monte Carlo (MCMC) sampling algorithms.", "labels": [], "entities": []}, {"text": "B\u00f6rschinger and Johnson (2011) proposed a strictly incremental particle filter algorithm that, however, performed considerably worse than the standard batch algorithms, in particular for the Bigram model.", "labels": [], "entities": []}, {"text": "We extend that algorithm by adding rejuvenation steps and show that this leads to considerable improvements, thus strengthening the case for particle filters as another tool for Bayesian inference in computational linguistics.", "labels": [], "entities": [{"text": "particle filters", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.7741171717643738}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Sections 2 and 3 provide the relevant background about word segmentation and previous work.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7848179340362549}]}, {"text": "Section 4 describes our algorithm.", "labels": [], "entities": []}, {"text": "Section 5 reports on an experimental evaluation of our algorithm, and section 6 concludes and suggests possible directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the performance of a batch MetropolisHastings sampler for the Unigram and Bigram model with that of particle filter learners both with and without rejuvenation, as described in the previous section.", "labels": [], "entities": []}, {"text": "For the batch samplers, we use simulated annealing to facilitate the finding of high probability solutions, and for the particle filters, we compare the performance of a 'degenerate' 1-particle learner with a 16-particle learner in the rejuvenation setting.", "labels": [], "entities": []}, {"text": "To get an impression of the contribution of particle number and rejuvenation steps, we compare: Results for both the Unigram and the Bigram model.", "labels": [], "entities": [{"text": "Unigram", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9122980833053589}]}, {"text": "MHS is a Metropolis-Hastings batch sampler.", "labels": [], "entities": [{"text": "MHS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9194324016571045}]}, {"text": "PF x is a particle filter with x particles and no rejuvenation.", "labels": [], "entities": []}, {"text": "PF x,s is a particle filter with x particles and s rejuvenation steps.", "labels": [], "entities": []}, {"text": "TF is token f-score, logProb is the log-probability (\u00d710 3 ) of the training-data at the end of learning.", "labels": [], "entities": [{"text": "TF", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9652247428894043}]}, {"text": "Less negative logProb indicates a better solution according to the model, higher TF indicates a better quality segmentation.", "labels": [], "entities": [{"text": "TF", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9965999722480774}]}, {"text": "All results are averaged across 4 runs.", "labels": [], "entities": []}, {"text": "Results for the 1000 particle setting are taken from B\u00f6rschinger and Johnson (2011).", "labels": [], "entities": []}, {"text": "the 16-particle learner with rejuvenation with a 1-particle learner that performs 16 times as many rejuvenation samples.", "labels": [], "entities": []}, {"text": "For comparison, we also cite previous results for the 1000-particle learners without rejuvenation reported in B\u00f6rschinger and Johnson (2011), using their choice of parameters to allow fora direct comparison: \u03b1 = 20 for the Unigram model, \u03b1 0 = 3000, \u03b1 1 = 100 for the Bigram model, and we use their base-distribution which differs from the one described in  in that it doesn't assume a uniform distribution over segments in the base-distribution but puts a Dirichlet Prior on it.", "labels": [], "entities": []}, {"text": "We apply each learner to the Bernstein-Ratner corpus) that is standardly used in the word segmentation literature, which consists of 9790 unsegmented and phonemically transcribed child-directed speech utterances.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7127766907215118}]}, {"text": "We evaluate each algorithm in two ways: inference performance, for which the final log-probability of the training data is the criterion, and segmentation performance, for which we consider token f-score to be the best measure, since it indicates how well the actual word tokens in the data are recovered.Note that these two measures can diverge, as previously documented for the Unigram model) and, less so, for the Bigram model (.", "labels": [], "entities": []}, {"text": "gives the results for our experiments.", "labels": [], "entities": []}, {"text": "For both models, adding rejuvenation always improves performance markedly as compared to the corresponding run without rejuvenation both in terms of log-probability and segmentation f-score.", "labels": [], "entities": []}, {"text": "Note in particular that for the Bigram model, using 16 particles with 100 rejuvenation steps leads to an improvement in token f-score of more than 10% points over 1000 particles without rejuvenation.", "labels": [], "entities": []}, {"text": "Comparing the 1-particle learner with 1600 rejuvenation steps to the 16-particle learner with 100 rejuvenation steps, for both models the former outperforms the latter in both log-probability and token fscore.", "labels": [], "entities": []}, {"text": "This suggests that if one has to trade-off particle number against rejuvenation steps, one maybe better off favouring the latter.", "labels": [], "entities": []}, {"text": "Despite the dramatic improvement over not using rejuvenation, there is still a considerable gap between all the incremental learners and the batch sampling algorithm in terms of log-probability.", "labels": [], "entities": []}, {"text": "A similar observation was made by Johnson and Goldwater (2009) for incremental initialisation in word segmentation using adaptor grammars.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.721841424703598}]}, {"text": "Their batch sampler converged on higher token f-score but lower probability solutions in some settings when initialized in an incremental fashion as opposed to randomly.", "labels": [], "entities": []}, {"text": "We agree with their suggestion that this maybe due to the \"greedy\" character of an incremental learner.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for both the Unigram and the Bigram  model. MHS is a Metropolis-Hastings batch sampler.  PF x is a particle filter with x particles and no rejuve- nation. PF x,s is a particle filter with x particles and s  rejuvenation steps. TF is token f-score, logProb is the  log-probability (\u00d710 3 ) of the training-data at the end of  learning. Less negative logProb indicates a better solu- tion according to the model, higher TF indicates a better  quality segmentation. All results are averaged across 4  runs. Results for the 1000 particle setting are taken from  B\u00f6rschinger and Johnson (2011).", "labels": [], "entities": [{"text": "TF", "start_pos": 245, "end_pos": 247, "type": "METRIC", "confidence": 0.9503695368766785}]}]}