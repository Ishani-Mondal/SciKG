{"title": [{"text": "Assessing the Effect of Inconsistent Assessors on Summarization Evaluation", "labels": [], "entities": [{"text": "Assessing the Effect of Inconsistent Assessors", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7509794135888418}, {"text": "Summarization Evaluation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.9601158499717712}]}], "abstractContent": [{"text": "We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.975952684879303}]}, {"text": "Using Text Analysis Conference data, we measure anno-tator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring.", "labels": [], "entities": [{"text": "Text Analysis Conference data", "start_pos": 6, "end_pos": 35, "type": "DATASET", "confidence": 0.7037300243973732}, {"text": "consistency", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.6494736075401306}]}, {"text": "We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.9221029281616211}]}, {"text": "Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9867506623268127}, {"text": "CLASSY", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9273701310157776}]}], "introductionContent": [{"text": "Automatic summarization of documents is a research area that unfortunately depends on human feedback.", "labels": [], "entities": [{"text": "Automatic summarization of documents", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8516744375228882}]}, {"text": "Although attempts have been made at automating the evaluation of summaries, none is so good as to remove the need for human assessors.", "labels": [], "entities": [{"text": "evaluation of summaries", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7324502468109131}]}, {"text": "Human judgment of summaries, however, is not perfect either.", "labels": [], "entities": [{"text": "judgment of summaries", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7121487061182658}]}, {"text": "We investigate two ways of measuring evaluation consistency in order to see what effect it has on summarization evaluation and training of automatic evaluation metrics.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.9718216359615326}]}], "datasetContent": [{"text": "Since human assessment is used to rank participating summarizers in the TAC Summarization track,  we should examine the potential impact of inconsistent assessors on the overall evaluation.", "labels": [], "entities": [{"text": "TAC Summarization", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7058390378952026}]}, {"text": "Because the final summarizer score is the average over many topics, and the topics are fairly evenly distributed among assessors for annotation, excluding noisy topics/assessors has very little impact on summarizer ranking.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 204, "end_pos": 214, "type": "TASK", "confidence": 0.9764068126678467}]}, {"text": "As an example, consider the 2011 assessor consistency data in and.", "labels": [], "entities": [{"text": "consistency", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.7841370701789856}]}, {"text": "If we exclude topics by the worst performing assessor from each of these categories, recalculate the summarizer rankings, and then check the correlation between the original and newly created rankings, we obtain results in.", "labels": [], "entities": []}, {"text": "Although the impact on evaluating automatic summarizers is small, it could be argued that excluding topics with inconsistent human scoring will have an impact on the performance of automatic evaluation metrics, which might be unfairly penalized by their inability to emulate random human mistakes.", "labels": [], "entities": []}, {"text": "shows ROUGE-2), one of the state-of-the-art automatic metrics used in TAC, and its correlations with human metrics, before and after exclusion of noisy topics from 2011 data.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.99639892578125}, {"text": "TAC", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.4592124819755554}]}, {"text": "The results are fairly inconclusive: it seems that inmost cases, removing topics does more harm than good, suggesting that the signal-to-noise ratio is still tipped in favor of signal.", "labels": [], "entities": []}, {"text": "The only exception is Readability, where ROUGE records a slight increase in correlation; this is unsurprising, given that consistency values for Readability are the lowest of all categories, and perhaps here removing noise has more impact.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9735548496246338}, {"text": "correlation", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9594225883483887}]}, {"text": "In the case of Pyramid, there is a small gain when we exclude the single worst assessor, but excluding two assessors results in a decreased correlation, perhaps because we remove too much valid information at the same time.", "labels": [], "entities": []}, {"text": "A different picture emerges when we examine how well ROUGE-2 can predict human scores on the summary level.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.6252963542938232}]}, {"text": "We pooled together all sum-: Correlation between ROUGE-2 and human metrics on a summary level before and after excluding topics by one or two worst assessors in that category.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.965785801410675}]}, {"text": "maries annotated by each particular assessor and calculated the correlation between ROUGE-2 and this assessor's manual scores for individual summaries.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9965360164642334}]}, {"text": "Then we calculated the mean correlation overall assessors.", "labels": [], "entities": []}, {"text": "Unsurprisingly, inconsistent assessors tend to correlate poorly with automatic (and therefore always consistent) metrics, so excluding one or two worst assessors from each category increases ROUGE's average per-assessor summary-level correlation, as can be seen in.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 191, "end_pos": 196, "type": "METRIC", "confidence": 0.764957845211029}]}, {"text": "The only exception here is when we exclude assessors based on their autoPyramid performance: again, because inconsistent SCU selection doesn't necessarily translate into inconsistent final Pyramid scores, excluding those assessors doesn't do much for ROUGE-2.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 251, "end_pos": 258, "type": "METRIC", "confidence": 0.9199516177177429}]}], "tableCaptions": [{"text": " Table 1: Annotator consistency in assigning Readability  and Responsiveness scores and in Pyramid evaluation, as  represented by Krippendorff's alpha for interval values,  on 2011 data.", "labels": [], "entities": [{"text": "Annotator", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9647555947303772}, {"text": "consistency", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.5569390654563904}, {"text": "Pyramid evaluation", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8600310683250427}]}, {"text": " Table 2: Correlation between the original summarizer  ranking and the ranking after excluding topics by one or  two worst assessors in each category.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9317601919174194}]}, {"text": " Table 3: Correlation between the summarizer rankings  according to ROUGE-2 and human metrics, before and  after excluding topics by one or two worst assessors in  that category.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.8874341249465942}]}, {"text": " Table 4: Correlation between ROUGE-2 and human met- rics on a summary level before and after excluding topics  by one or two worst assessors in that category.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9092124700546265}]}, {"text": " Table 5: Correlations between CLASSY and human met- rics on 2011 data (main and update summaries), before  and after excluding most inconsistent topic from 2009- 2010 training data for CLASSY.", "labels": [], "entities": []}]}