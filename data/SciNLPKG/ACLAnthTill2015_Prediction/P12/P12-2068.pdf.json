{"title": [{"text": "Sentence Compression with Semantic Role Constraints", "labels": [], "entities": [{"text": "Semantic Role Constraints", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6094589134057363}]}], "abstractContent": [{"text": "For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8145056366920471}]}, {"text": "These constraints are based on semantic roles and superior to the constraints of syntactic dependencies.", "labels": [], "entities": []}, {"text": "Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques.", "labels": [], "entities": [{"text": "Written News Compression Corpus (Clarke and Lapata, 2008)", "start_pos": 32, "end_pos": 89, "type": "DATASET", "confidence": 0.7618148191408678}]}], "introductionContent": [{"text": "Recent work in document summarization do not only extract sentences but also compress sentences.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.5400000214576721}]}, {"text": "Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems).", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8775968849658966}, {"text": "summarizers", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9671857357025146}]}, {"text": "Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.8166668713092804}]}, {"text": "In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies.", "labels": [], "entities": []}, {"text": "Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb.", "labels": [], "entities": []}, {"text": "shows an example with a coordination structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental setting follows previous work.", "labels": [], "entities": []}, {"text": "As stated in Section 2, we employed the WNC Corpus.", "labels": [], "entities": [{"text": "WNC Corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9588495194911957}]}, {"text": "For preprocessing, we performed POS tagging by stanford-tagger. and dependency parsing by MST-parser ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7411571145057678}, {"text": "dependency parsing", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8704267740249634}, {"text": "MST-parser", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.8374508619308472}]}, {"text": "In addition, LTH 6 was exploited to perform both dependency parsing and SR labeling.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8913434147834778}, {"text": "SR labeling", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9497939348220825}]}, {"text": "We implemented our model by Markov Thebeast with Gurobi optimizer.", "labels": [], "entities": []}, {"text": "Our evaluation consists of two types of automatic evaluations.", "labels": [], "entities": []}, {"text": "The first evaluation is dependency based evaluation same as.", "labels": [], "entities": []}, {"text": "We performed dependency parsing on gold data and system outputs by RASP.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7531788647174835}, {"text": "RASP", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.752730667591095}]}, {"text": "8 Then we calculated precision, recall, and F1 for the set of label(head, modi f ier).", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9997712969779968}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.999663233757019}, {"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9998282194137573}]}, {"text": "In order to demonstrate how well our SR constraints keep correct predicate-argument structures in compression, we propose SRL based evaluation.", "labels": [], "entities": []}, {"text": "We performed SR labeling on gold data: Results of Sentence Compression and system outputs by LTH.", "labels": [], "entities": [{"text": "SR labeling", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9724756479263306}, {"text": "LTH", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.6817002296447754}]}, {"text": "Then we calculated precision, recall, and F1 value for the set of role(predicate, argument).", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996569156646729}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9995284080505371}, {"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9998394250869751}]}, {"text": "The training time of our MLN model are approximately 8 minutes on all training data, with 3.1GHz Intel Core i3 CPU and 4G memory.", "labels": [], "entities": []}, {"text": "While the prediction can be done within 20 seconds on the test data.", "labels": [], "entities": []}, {"text": "shows the results of our compression models by compression rate (CompR), dependencybased F1 (F1-Dep), and SRL-based F1 (F1-SRL).", "labels": [], "entities": [{"text": "compression rate (CompR)", "start_pos": 47, "end_pos": 71, "type": "METRIC", "confidence": 0.8753500342369079}, {"text": "dependencybased F1 (F1-Dep)", "start_pos": 73, "end_pos": 100, "type": "METRIC", "confidence": 0.7390683770179749}, {"text": "SRL-based F1", "start_pos": 106, "end_pos": 118, "type": "METRIC", "confidence": 0.9437527358531952}]}, {"text": "In our experiment, we have three models.", "labels": [], "entities": []}, {"text": "McDonald is a re-implementation of. also re-implemented McDonald's model with an ILP solver and experimented it on the WNC Corpus.", "labels": [], "entities": [{"text": "WNC Corpus", "start_pos": 119, "end_pos": 129, "type": "DATASET", "confidence": 0.9847538769245148}]}, {"text": "MLN with SRL and MLN w/o SRL are our Markov Logic models with and without SR Constraints, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of Arguments in Compression  tions in the Written News Compression (WNC) Cor- pus. It has 82 documents (1,629 sentences). We di- vided them into three: 55 documents are used for  training (1106 sentences); 10 for development (184  sentences); 17 for testing (339 sentences).", "labels": [], "entities": [{"text": "Written News Compression (WNC) Cor- pus", "start_pos": 63, "end_pos": 102, "type": "DATASET", "confidence": 0.6150592333740659}]}, {"text": " Table 4: Analysis of Errors", "labels": [], "entities": [{"text": "Analysis of Errors", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7866121331850687}]}]}