{"title": [{"text": "Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia", "labels": [], "entities": [{"text": "Multilingual Named Entity Recognition", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6366514936089516}]}], "abstractContent": [{"text": "In this paper we propose a method to automatically label multilingual data with named entity tags.", "labels": [], "entities": []}, {"text": "We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences.", "labels": [], "entities": []}, {"text": "The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence.", "labels": [], "entities": [{"text": "foreign sentence tagging", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6201900939146677}]}, {"text": "The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity Recognition (NER) is a frequently needed technology in NLP applications.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7758141656716665}]}, {"text": "State-ofthe-art statistical models for NER typically require a large amount of training data and linguistic expertise to be sufficiently accurate, which makes it nearly impossible to build high-accuracy models fora large number of languages.", "labels": [], "entities": [{"text": "NER", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9739635586738586}]}, {"text": "Recently, there have been two lines of work which have offered hope for creating NER analyzers in many languages.", "labels": [], "entities": [{"text": "NER analyzers", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9185032546520233}]}, {"text": "The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (.", "labels": [], "entities": []}, {"text": "The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language ().", "labels": [], "entities": []}, {"text": "Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages ().", "labels": [], "entities": []}, {"text": "* This research was conducted during the author's internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages.", "labels": [], "entities": []}, {"text": "Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia.", "labels": [], "entities": []}, {"text": "We propose a statistical model which can combine the two types of information.", "labels": [], "entities": []}, {"text": "Similarly to the joint model of, our model can incorporate both monolingual and bilingual features in a log-linear framework.", "labels": [], "entities": []}, {"text": "The advantage of our model is that it is much more efficient as it does not require summing over matchings of source and target entities.", "labels": [], "entities": [{"text": "summing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9636815786361694}]}, {"text": "It is a conditional model for target sentence annotation given an aligned English source sentence, where the English sentence is used only as a source of features.", "labels": [], "entities": [{"text": "target sentence annotation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6424870689709982}]}, {"text": "Exact inference is performed using standard semi-markov CRF model inference techniques ( ).", "labels": [], "entities": []}, {"text": "Our results show that the semi-CRF model improves on the performance of projection models by more than 10 points in F-measure, and that we can achieve tagging F-measure of over 91 using a very small number of annotated sentence pairs.", "labels": [], "entities": [{"text": "tagging F-measure", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7138085663318634}]}, {"text": "The paper is organized as follows: We first describe the datasets and task setting in Section 2.", "labels": [], "entities": []}, {"text": "Next, we present our two baseline methods: A Wikipedia metadata-based tagger and a crosslingual projection tagger in Sections 3 and 4, respectively.", "labels": [], "entities": []}, {"text": "We present our direct semi-CRF tagging model in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the tagging F-measure for projection models on the English-Bulgarian and EnglishKorean datasets.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8632505536079407}, {"text": "EnglishKorean datasets", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.911571204662323}]}, {"text": "10-fold cross-validation was used to estimate model performance.", "labels": [], "entities": []}, {"text": "The foreign language NE F-measure is reported in.", "labels": [], "entities": [{"text": "NE F-measure", "start_pos": 21, "end_pos": 33, "type": "METRIC", "confidence": 0.7598129212856293}]}, {"text": "The best Wiki-based tagger performance is shown on the last line as a baseline (repeated from).", "labels": [], "entities": [{"text": "Wiki-based tagger", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.6021911054849625}]}, {"text": "We present a detailed evaluation of the model to gain understanding of the strengths and limitations of the projection approach and to motivate our direct semi-CRF model.", "labels": [], "entities": []}, {"text": "To give an estimate of the upper bound on performance for the projection model, we first present two oracles.", "labels": [], "entities": []}, {"text": "The goal of the oracles it to estimate the impact of two sources of error for the projection model: the first is the error in detecting English entities, and the second is the error in determining the corresponding foreign entity fora given English entity.", "labels": [], "entities": []}, {"text": "The first oracle ORACLE1 has access to the goldstandard English entities and gold-standard word alignments among English and foreign words.", "labels": [], "entities": [{"text": "ORACLE1", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9835995435714722}]}, {"text": "For each source entity, ORACLE1 selects the longest foreign language sequence of words that could be extracted in a phrase pair coupled with the source entity word sequence (according the standard phrase extraction heuristic (), and labels it with the label of the source entity.", "labels": [], "entities": []}, {"text": "Note that the word alignments do not uniquely identify the corresponding foreign phrase for each English phrase and some error is possible due to this.", "labels": [], "entities": []}, {"text": "The performance of this oracle is closely related to the percentage of linked source-target entities reported in.", "labels": [], "entities": []}, {"text": "The second oracle ORACLE2 provides the performance of the projection model when gold-standard source entities are known, but the corresponding target entities still have to be determined by the projection model (gold-standard alignments are not known).", "labels": [], "entities": [{"text": "ORACLE2", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9853873252868652}]}, {"text": "In other words, ORACLE2 is the projection model with all features, wherein the test set we provide the gold standard English entities as input.", "labels": [], "entities": [{"text": "ORACLE2", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9772040843963623}]}, {"text": "The performance of ORACLE2 is determined by the error in automatic word alignment and in determining phonetic correspondence.", "labels": [], "entities": [{"text": "ORACLE2", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9258798956871033}, {"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7119320034980774}]}, {"text": "As we can seethe drop due to this error is very large, especially on Korean, where performance drops from 90.0 to 81.9 F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9716443419456482}]}, {"text": "The next section in the  mance of non-oracle projection models, which do not have access to any manually labeled information.", "labels": [], "entities": []}, {"text": "The local+global Wiki-based tagger is used to define English entities, and only automatically derived alignment information is used.", "labels": [], "entities": []}, {"text": "PM+WF is the projection model using all features.", "labels": [], "entities": [{"text": "PM+WF", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7933763066927592}]}, {"text": "The line above, PM-WF represents the projection model without the Wiki-tagger derived features, and is included to show that the gain from using these features is substantial.", "labels": [], "entities": []}, {"text": "The difference inaccuracy between the projection model and ORACLE2 is very large, and is due to the error of the Wiki-based English taggers.", "labels": [], "entities": [{"text": "ORACLE2", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.6990797519683838}]}, {"text": "The drop for Bulgarian is so large that the best projection model PM+WF does not reach the performance of 83.2 achieved by the baseline Wiki-based tagger.", "labels": [], "entities": [{"text": "Bulgarian", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.8498144745826721}, {"text": "PM+WF", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.7817202011744181}]}, {"text": "When source entities are assigned with error for this language pair, projecting entity annotations from the source is not better than using the target Wiki-based annotations directly.", "labels": [], "entities": []}, {"text": "For Korean while the trend in model performance is similar as oracle information is removed, the projection model achieves substantially better performance (80.8 vs 69.9) due to the much larger difference in performance between the English and Korean Wiki-based taggers.", "labels": [], "entities": []}, {"text": "The drawback of the projection model is that it determines target entities only by assigning the best candidate for each source entity.", "labels": [], "entities": []}, {"text": "It cannot create target entities that do not correspond to source entities, it is notable to take into account multiple conflicting source NE taggers as sources of information, and it does not make use of target sentence context and entity consistency constraints.", "labels": [], "entities": []}, {"text": "To address these shortcomings we propose a direct semi-CRF model, described in the next section.", "labels": [], "entities": []}, {"text": "Our main results are listed in.", "labels": [], "entities": []}, {"text": "We perform 10-fold cross-validation as in the projection experiments.", "labels": [], "entities": []}, {"text": "The best Wiki-based and projection models are listed as baselines at the bottom of the    We look at performance using four sets of features: (i) Monolingual Wiki-tagger based, using only the features in Group 1 (MONO); (ii) Bilingual label match and Wiki-tagger based, using features in Groups 1 and 3 (BI); (iii) Monolingual all, using features in Groups 1 and 2 (MONO-ALL), and (iv) Bilingual all, using all features (BI-ALL).", "labels": [], "entities": []}, {"text": "Additionally, we report performance of the full bilingual model with all features, but when English candidate entities are generated only according to the local+global Wiki-taggger (BI-ALL-WT).", "labels": [], "entities": [{"text": "BI-ALL-WT", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9457144141197205}]}, {"text": "The main results show that the full semi-CRF model greatly outperforms the baseline projection and Wiki-taggers.", "labels": [], "entities": []}, {"text": "For Bulgarian, the F-measure of the full model is 92.8 compared to the best baseline result of 83.2.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9997501969337463}]}, {"text": "For Korean, the F-measure of the semi-CRF is 91.2, more than 10 points higher than the performance of the projection model.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9995137453079224}]}, {"text": "Within the semi-CRF model, the contribution of English sentence context was substantial, leading to 2.5 point increase in F-measure for Bulgarian (92.8 versus 90.3 F-measure), and 4.0 point increase for Korean (91.2 versus 87.2).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9980369210243225}]}, {"text": "The additional gain due to considering candidate source entities generated from all English taggers was 1.3 F-measure points for both language pairs (comparing models BI-ALL and BI-ALL-WT).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9989118576049805}]}, {"text": "If we restrict the semi-CRF to use only features similar to the ones used by the projection model, we still obtain performance much better than that of the projection model: comparing BI to the projection model, we see gains of 9.4 points for Bulgarian, and 4 points for Korean.", "labels": [], "entities": [{"text": "BI", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.8630216717720032}]}, {"text": "This is due to the fact that the semi-CRF is able to relax the assumption of one-toone correspondence between source and target entities, and can effectively combine information from multiple source and target taggers.", "labels": [], "entities": []}, {"text": "We should note that the proposed method can only tag foreign sentences in English-foreign sentence pairs.", "labels": [], "entities": []}, {"text": "The next step for this work is to train monolingual NE taggers for the foreign languages, which can work on text within or outside of Wikipedia.", "labels": [], "entities": [{"text": "NE taggers", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.8173069357872009}]}, {"text": "Preliminary results show performance of over 80 Fmeasure for such monolingual models.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9984500408172607}]}], "tableCaptions": [{"text": " Table 1: English-Bulgarian and English-Korean data  characteristics.", "labels": [], "entities": []}, {"text": " Table 2: English-Bulgarian and English-Korean Wiki-based tagger performance.", "labels": [], "entities": [{"text": "Wiki-based tagger performance", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.5919379591941833}]}, {"text": " Table 3: English-Bulgarian and English-Korean Projection tagger performance.", "labels": [], "entities": [{"text": "Projection tagger", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7293180525302887}]}, {"text": " Table 5: English-Bulgarian and English-Korean semi-CRF tagger performance.", "labels": [], "entities": []}]}