{"title": [{"text": "Efficient Tree-based Approximation for Entailment Graph Learning", "labels": [], "entities": [{"text": "Approximation", "start_pos": 21, "end_pos": 34, "type": "METRIC", "confidence": 0.8999512791633606}, {"text": "Entailment Graph Learning", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.8132994771003723}]}], "abstractContent": [{"text": "Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years.", "labels": [], "entities": []}, {"text": "In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs).", "labels": [], "entities": []}, {"text": "We first identify that entailment graphs exhibit a \"tree-like\" property and are very similar to a novel type of graph termed forest-reducible graph.", "labels": [], "entities": []}, {"text": "We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time.", "labels": [], "entities": []}, {"text": "We compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efficient and scalable both theoretically and empirically, while its output quality is close to that given by the optimal solution of the exact algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Performing textual inference is in the heart of many semantic inference applications such as Question Answering (QA) and Information Extraction (IE).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.8485535383224487}, {"text": "Information Extraction (IE)", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.849735152721405}]}, {"text": "A prominent generic paradigm for textual inference is Textual Entailment (TUE) ( ).", "labels": [], "entities": [{"text": "Textual Entailment (TUE)", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.776248836517334}]}, {"text": "In TUE, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text.", "labels": [], "entities": []}, {"text": "For example, the text \"Cyprus was invaded by the Ottoman Empire in 1571\" implies the hypothesis \"The Ottomans attacked Cyprus\".", "labels": [], "entities": []}, {"text": "Semantic inference applications such as QA and IE crucially rely on entailment rules) or equivalently inference rules, that is, rules that describe a directional inference relation between two fragments of text.", "labels": [], "entities": []}, {"text": "An important type of entailment rule specifies the entailment relation between natural language predicates, e.g., the entailment rule 'X invade Y \u2192 X attack Y' can be helpful in inferring the aforementioned hypothesis.", "labels": [], "entities": []}, {"text": "Consequently, substantial effort has been made to learn such rules ().", "labels": [], "entities": []}, {"text": "Textual entailment is inherently a transitive relation , that is, the rules 'x \u2192 y' and 'y \u2192 z' imply the rule 'x \u2192 z'.", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7287695407867432}]}, {"text": "Accordingly, formulated the problem of learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivity.", "labels": [], "entities": []}, {"text": "Since finding the optimal set of edges respecting transitivity is NP-hard, they employed Integer Linear Programming (ILP) to find the exact solution.", "labels": [], "entities": []}, {"text": "Indeed, they showed that applying global transitivity constraints improves rule learning comparing to methods that ignore graph structure.", "labels": [], "entities": [{"text": "rule learning", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9023626744747162}]}, {"text": "More recently,) introduced a more efficient exact algorithm, which decomposes the graph into connected components and then applies an ILP solver over each component.", "labels": [], "entities": []}, {"text": "Despite this progress, finding the exact solution remains NP-hard -the authors themselves report they were unable to solve some graphs of rather moderate size and that the coverage of their method is limited.", "labels": [], "entities": []}, {"text": "Thus, scaling their algorithm to data sets with tens of thousands of predicates (e.g., the extractions of) is unlikely.", "labels": [], "entities": []}, {"text": "In this paper we present a novel method for learning the edges of entailment graphs.", "labels": [], "entities": []}, {"text": "Our method computes much more efficiently an approximate solution that is empirically almost as good as the exact solution.", "labels": [], "entities": []}, {"text": "To that end, we first (Section 3) conjecture and empirically show that entailment graphs exhibit a \"tree-like\" property, i.e., that they can be reduced into a structure similar to a directed forest.", "labels": [], "entities": []}, {"text": "Then, we present in Section 4 our iterative approximation algorithm, wherein each iteration anode is removed and re-attached back to the graph in a locally-optimal way.", "labels": [], "entities": []}, {"text": "Combining this scheme with our conjecture about the graph structure enables a linear algorithm for node re-attachment.", "labels": [], "entities": []}, {"text": "Section 5 shows empirically that this algorithm is by orders of magnitude faster than the state-of-the-art exact algorithm, and that though an optimal solution is not guaranteed, the area under the precision-recall curve drops by merely a point.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 198, "end_pos": 214, "type": "METRIC", "confidence": 0.9786493182182312}]}, {"text": "To conclude, the contribution of this paper is twofold: First, we define a novel modeling assumption about the tree-like structure of entailment graphs and demonstrate its validity.", "labels": [], "entities": []}, {"text": "Second, we exploit this assumption to develop a polynomial approximation algorithm for learning entailment graphs that can scale to much larger graphs than in the past.", "labels": [], "entities": []}, {"text": "Finally, we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction () and Ontology induction, and thus our approach may improve scalability in these fields as well.", "labels": [], "entities": [{"text": "Taxonomy Induction", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.6777560412883759}, {"text": "Ontology induction", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.801062822341919}]}], "datasetContent": [{"text": "In this section we empirically demonstrate that TNF is more efficient than other baselines and its output quality is close to that given by the optimal solution.", "labels": [], "entities": []}, {"text": "In our experiments we utilize the data set released by.", "labels": [], "entities": []}, {"text": "The data set contains 10 entailment graphs, where graph nodes are typed predicates.", "labels": [], "entities": []}, {"text": "A typed predicate (e.g., 'X disease occur in Y country ') includes a predicate and two typed variables that specify the semantic type of the arguments.", "labels": [], "entities": []}, {"text": "For instance, the typed variable X disease can be instantiated by arguments such as 'flu' or 'diabetes'.", "labels": [], "entities": []}, {"text": "The data set contains 39,012 potential edges, of which 3,427 are annotated as edges (valid entailment rules) and 35,585 are annotated as non-edges.", "labels": [], "entities": []}, {"text": "The data set also contains, for every pair of predicates i, j in every graph, a local score s ij , which is the output of a classifier trained over distributional similarity features.", "labels": [], "entities": []}, {"text": "A positive s ij indicates that the classifier believes i \u2192 j.", "labels": [], "entities": []}, {"text": "The weighting function for the graph edges w is defined as w ij = s ij \u2212\u03bb, where \u03bb is a single parameter controlling graph sparseness: as \u03bb increases, w ij decreases and becomes negative for more pairs of predicates, rendering the graph more sparse.", "labels": [], "entities": []}, {"text": "In addition, the data set contains a set of local constraints (see Section 4.3).", "labels": [], "entities": []}, {"text": "We implemented the following algorithms for learning graph edges, wherein all of them the graph is first decomposed into components according to Berant et al's method, as explained in Section 2.", "labels": [], "entities": []}, {"text": "No-trans Local scores are used without transitivity constraints -an edge (i, j) is inserted iff w ij > 0.", "labels": [], "entities": []}, {"text": "Exact-graph Berant et al.'s exact method (2011) for Max-Trans-Graph, which utilizes an ILP solver 1 . Exact-forest Solving Max-Trans-Forest exactly by applying an ILP solver (see Eq. 2).", "labels": [], "entities": []}, {"text": "LP-relax Solving Max-Trans-Graph approximately by applying LP-relaxation (see Section 2) on each graph component.", "labels": [], "entities": []}, {"text": "We apply the LP solver within the same cutting-plane procedure as Exactgraph to allow fora direct comparison.", "labels": [], "entities": [{"text": "LP solver", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.4217766970396042}]}, {"text": "This also keeps memory consumption manageable, as otherwise all |V | 3 constraints must be explicitly encoded into the LP.", "labels": [], "entities": []}, {"text": "As mentioned, our goal is to present a method for learning transitive graphs, while LPrelax produces solutions that violate transitivity.", "labels": [], "entities": [{"text": "LPrelax", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.8878069519996643}]}, {"text": "However, we run it on our data set to obtain empirical results, and to compare run-times against TNF.", "labels": [], "entities": []}, {"text": "Graph-Node-Fix (GNF) Initialization of each component is performed in the following way: if the graph is very sparse, i.e. \u03bb \u2265 C for some constant C (set to 1 in our experiments), then solving the graph exactly is not an issue and we use Exact-graph.", "labels": [], "entities": [{"text": "Exact-graph", "start_pos": 238, "end_pos": 249, "type": "DATASET", "confidence": 0.880772054195404}]}, {"text": "Otherwise, we initialize by applying Exact-graph in a sparse configuration, i.e., \u03bb = C.", "labels": [], "entities": []}, {"text": "Tree-Node-Fix (TNF) Initialization is done as in GNF, except that if it generates a graph that is not an FRG, it is corrected by a simple heuristic: for every node in the reduced graph G red that has more than We use the Gurobi optimization package in all experiments.", "labels": [], "entities": []}, {"text": "one parent, we choose from its current parents the single one whose SCC is composed of the largest number of nodes in G.", "labels": [], "entities": []}, {"text": "We evaluate algorithms by comparing the set of gold standard edges with the set of edges learned by each algorithm.", "labels": [], "entities": []}, {"text": "We measure recall, precision and F 1 for various values of the sparseness parameter \u03bb, and compute the area under the precision-recall Curve (AUC) generated.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9993403553962708}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9994593262672424}, {"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.99473837018013}, {"text": "precision-recall Curve (AUC)", "start_pos": 118, "end_pos": 146, "type": "METRIC", "confidence": 0.911562192440033}]}, {"text": "Efficiency is evaluated by comparing run-times.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8788770437240601}]}], "tableCaptions": []}