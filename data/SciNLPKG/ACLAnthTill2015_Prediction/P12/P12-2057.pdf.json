{"title": [{"text": "Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Translation Model Size Reduction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7980493903160095}, {"text": "Hierarchical Phrase-based Statistical Machine Translation", "start_pos": 37, "end_pos": 94, "type": "TASK", "confidence": 0.592662101984024}]}], "abstractContent": [{"text": "In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems.", "labels": [], "entities": [{"text": "hierarchical phrase-based machine translation", "start_pos": 87, "end_pos": 132, "type": "TASK", "confidence": 0.5790154784917831}]}, {"text": "Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage.", "labels": [], "entities": []}, {"text": "On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 215, "end_pos": 227, "type": "TASK", "confidence": 0.923277348279953}]}, {"text": "Experimental results on Chinese-to-English machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.", "labels": [], "entities": [{"text": "Chinese-to-English machine translation tasks", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.678295686841011}]}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) has gained considerable attention during last decades.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8705703616142273}]}, {"text": "From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9797724485397339}]}, {"text": "Phrasebased model () and hierarchical phrase-based model show state-of-the-art performance in various language pairs.", "labels": [], "entities": []}, {"text": "This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus.", "labels": [], "entities": []}, {"text": "However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7096303254365921}, {"text": "translation rule acquisition", "start_pos": 147, "end_pos": 175, "type": "TASK", "confidence": 0.8794908126195272}]}, {"text": "According to and our own preliminary experiments, the size of phrase table and hierarchical rule table consistently increases linearly with the growth of training size, while the translation performance tends to gain minor improvement after a certain point.", "labels": [], "entities": []}, {"text": "Consequently, the model size reduction is necessary and meaningful for SMT systems if it can be performed without significant performance degradation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9957432150840759}]}, {"text": "The smaller the model size is, the faster the SMT decoding speed is, because there are fewer hypotheses to be investigated during decoding.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8554535210132599}]}, {"text": "Especially, in a limited environment, such as mobile device, and fora time-urgent task, such as speech-to-speech translation, the compact size of translation rules is required.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.7188076674938202}]}, {"text": "In this case, the model reduction would be the one of the main techniques we have to consider.", "labels": [], "entities": [{"text": "model reduction", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.757889598608017}]}, {"text": "Previous methods of reducing the size of SMT model try to identify infrequent entries (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9933223128318787}]}, {"text": "Several statistical significance testing methods are also examined to detect unreliable noisy entries (.", "labels": [], "entities": []}, {"text": "These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9684935808181763}]}, {"text": "The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules.", "labels": [], "entities": []}, {"text": "In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents.", "labels": [], "entities": []}, {"text": "Suppose that the source phrase s 1 s 2 is always translated into t 1 t 2 with phrase entry <s 1 s 2 \u2192t 1 t 2 > where s i and ti are correspond-ing translations.", "labels": [], "entities": []}, {"text": "Similarly, source phrases s 1 and s 2 are always translated into t 1 and t 2 , with phrase entries, <s 1 \u2192t 1 > and <s 2 \u2192t 2 >, respectively.", "labels": [], "entities": []}, {"text": "In this case, it is intuitive that <s 1 s 2 \u2192t 1 t 2 > could be unnecessary and redundant since its substituent always produces the same result.", "labels": [], "entities": []}, {"text": "This paper presents statistical analysis of this redundancy measurement.", "labels": [], "entities": []}, {"text": "The redundancy-based reduction can be performed to prune the phrase table, the hierarchical rule table, and both.", "labels": [], "entities": []}, {"text": "Since the similar translation knowledge is accumulated at both of tables during the training stage, our reduction method performs effectively and safely.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9610971808433533}]}, {"text": "Unlike previous studies solely focus on either phrase table or hierarchical rule table, this work is the first attempt to reduce phrases and hierarchical rules simultaneously.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate the effectiveness of our reduction method by conducting Chinese-to-English translation task.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.6486348658800125}]}, {"text": "The training data, as same as, consists of about 500K parallel sentence pairs which is a mixture of several datasets published by LDC.", "labels": [], "entities": [{"text": "LDC", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.9506649971008301}]}, {"text": "NIST 2003 set is used as a development set., and 2008 sets are used for evaluation purpose.", "labels": [], "entities": [{"text": "NIST 2003 set", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9834905862808228}]}, {"text": "For word alignment, we use GIZA++ 1 , an implementation of IBM models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8601451218128204}]}, {"text": "We have implemented a hierarchical phrase-based SMT model similar to.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9052214026451111}]}, {"text": "The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.868268609046936}]}, {"text": "Sampled 10,000 sentences from Chinese Gigaword corpus was used for source-side development dataset to measure consistency.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.8443406621615092}, {"text": "consistency", "start_pos": 110, "end_pos": 121, "type": "METRIC", "confidence": 0.9607484340667725}]}, {"text": "Our main metric for translation performance evaluation is case- insensitive BLEU-4 scores ().", "labels": [], "entities": [{"text": "translation performance evaluation", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.8329814473787943}, {"text": "BLEU-4", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9638357758522034}]}, {"text": "As a baseline system, we chose the frequencybased cutoff method, which is one of the most widely used filtering methods.", "labels": [], "entities": []}, {"text": "As shown in, almost half of the phrases and hierarchical rules are pruned when cutoff=2, while the BLEU score is also deteriorated significantly.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9843257367610931}]}, {"text": "We introduced two methods for selecting the N pruning entries considering dependency relationships.", "labels": [], "entities": []}, {"text": "The non-dependency method does not consider dependency relationships, while the dependency method prunes independent entries first.", "labels": [], "entities": []}, {"text": "Each method can be combined with cross reduction.", "labels": [], "entities": [{"text": "cross reduction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.753206193447113}]}, {"text": "The performance is measured in three different reduction tasks: phrase reduction, hierarchical rule reduction, and joint reduction.", "labels": [], "entities": [{"text": "phrase reduction", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.8444662690162659}, {"text": "hierarchical rule reduction", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.5772628287474314}, {"text": "joint reduction", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.6703619360923767}]}, {"text": "As the reduction ratio becomes higher, the model size, i.e., the number of entries, is reduced while BLEU scores and coverage are decreased.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9776075482368469}, {"text": "coverage", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9942255616188049}]}, {"text": "The results show that the translation performance is highly co-related with the consistency.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9403128623962402}, {"text": "consistency", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9929893016815186}]}, {"text": "The co-relation scores measured between them on the phrase reduction and the hierarchical rule reduction tasks are 0.99 and 0.95, respectively, which indicates very strong positive relationship.", "labels": [], "entities": [{"text": "phrase reduction", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7700430154800415}, {"text": "hierarchical rule reduction tasks", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.6900493204593658}]}, {"text": "For the phrase reduction task, the dependency method outperforms the non-dependency method in terms of BLEU score.", "labels": [], "entities": [{"text": "phrase reduction task", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.8809738556543986}, {"text": "BLEU score", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9800989329814911}]}, {"text": "When the cross reduction technique was used for the phrase reduction task, BLEU score is not deteriorated even when more than half of phrase entries are pruned.", "labels": [], "entities": [{"text": "cross reduction", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.739979475736618}, {"text": "phrase reduction task", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.8876862128575643}, {"text": "BLEU score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9815203845500946}]}, {"text": "This result implies that there is much redundant information stored in the hierarchical rule table.", "labels": [], "entities": []}, {"text": "On the other hand, for the hierarchical rule reduction task, the non-dependency method shows the better performance.", "labels": [], "entities": [{"text": "rule reduction task", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7793939212958018}]}, {"text": "The dependency method sometimes performs worse than the baseline method.", "labels": [], "entities": []}, {"text": "We expect that this is caused by the unreliable estimation of dependency among hierarchical rules since the most of them are automatically generated from the phrases.", "labels": [], "entities": []}, {"text": "The excessive dependency of these rules would cause overestimation of hierarchical rule redundancy score.", "labels": [], "entities": []}], "tableCaptions": []}