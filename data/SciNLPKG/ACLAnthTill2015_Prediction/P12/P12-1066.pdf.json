{"title": [{"text": "Selective Sharing for Multilingual Dependency Parsing", "labels": [], "entities": [{"text": "Selective Sharing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7256732881069183}, {"text": "Multilingual Dependency Parsing", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.5824426809946696}]}], "abstractContent": [{"text": "We present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse anew unannotated language.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6138549546400706}]}, {"text": "Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource-rich languages.", "labels": [], "entities": []}, {"text": "The algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly.", "labels": [], "entities": []}, {"text": "The model factorizes the process of generating a dependency tree into two steps: selection of syntactic dependents and their ordering.", "labels": [], "entities": []}, {"text": "Being largely language-universal, the selection component is learned in a supervised fashion from all the training languages.", "labels": [], "entities": []}, {"text": "In contrast, the ordering decisions are only influenced by languages with similar properties.", "labels": [], "entities": []}, {"text": "We systematically model this cross-lingual sharing using typological features.", "labels": [], "entities": []}, {"text": "In our experiments, the model consistently outperforms a state-of-the-art multilingual parser.", "labels": [], "entities": []}, {"text": "The largest improvement is achieved on the non Indo-European languages yielding again of 14.4%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current top performing parsing algorithms rely on the availability of annotated data for learning the syntactic structure of a language.", "labels": [], "entities": []}, {"text": "Standard approaches for extending these techniques to resourcelean languages either use parallel corpora or rely on The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/unidep/ annotated trees from other source languages.", "labels": [], "entities": []}, {"text": "These techniques have been shown to work well for language families with many annotated resources (such as Indo-European languages).", "labels": [], "entities": []}, {"text": "Unfortunately, for many languages there are no available parallel corpora or annotated resources in related languages.", "labels": [], "entities": []}, {"text": "For such languages the only remaining option is to resort to unsupervised approaches, which are known to produce highly inaccurate results.", "labels": [], "entities": []}, {"text": "In this paper, we present anew multilingual algorithm for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8934155702590942}]}, {"text": "In contrast to previous approaches, this algorithm can learn dependency structures using annotations from a diverse set of source languages, even if this set is not related to the target language.", "labels": [], "entities": []}, {"text": "In our selective sharing approach, the algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly.", "labels": [], "entities": []}, {"text": "This approach is rooted in linguistic theory that characterizes the connection between languages at various levels of sharing.", "labels": [], "entities": []}, {"text": "Some syntactic properties are universal across languages.", "labels": [], "entities": []}, {"text": "For instance, nouns take adjectives and determiners as dependents, but not adverbs.", "labels": [], "entities": []}, {"text": "However, the order of these dependents with respect to the parent is influenced by the typological features of each language.", "labels": [], "entities": []}, {"text": "To implement this intuition, we factorize generation of a dependency tree into two processes: selection of syntactic dependents and their ordering.", "labels": [], "entities": []}, {"text": "The first component models the distribution of dependents for each part-of-speech tag, abstracting over their order.", "labels": [], "entities": []}, {"text": "Being largely language-universal, this distribution can be learned in a supervised fashion from all the training languages.", "labels": [], "entities": []}, {"text": "On the other hand, ordering of dependents varies greatly across languages and therefore should only be influenced by languages with similar properties.", "labels": [], "entities": []}, {"text": "Furthermore, this similarity has to be expressed at the level of dependency types -i.e., two languages may share nounadposition ordering, but differ in noun-determiner ordering.", "labels": [], "entities": []}, {"text": "To systematically model this cross-lingual sharing, we rely on typological features that reflect ordering preferences of a given language.", "labels": [], "entities": []}, {"text": "In addition to the known typological features, our parsing model embeds latent features that can capture crosslingual structural similarities.", "labels": [], "entities": []}, {"text": "While the approach described so far supports a seamless transfer of shared information, it does not account for syntactic properties of the target language unseen in the training languages.", "labels": [], "entities": []}, {"text": "For instance, in the CoNLL data, Arabic is the only language with the VSO ordering.", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9620235562324524}]}, {"text": "To handle such cases, our approach augments cross-lingual sharing with unsupervised learning on the target languages.", "labels": [], "entities": []}, {"text": "We evaluated our selective sharing model on 17 languages from 10 language families.", "labels": [], "entities": []}, {"text": "On this diverse set, our model consistently outperforms stateof-the-art multilingual dependency parsers.", "labels": [], "entities": []}, {"text": "Performance gain, averaged overall the languages, is 5.9% when compared to the highest baseline.", "labels": [], "entities": []}, {"text": "Our model achieves the most significant gains on nonIndo-European languages, where we see a 14.4% improvement.", "labels": [], "entities": []}, {"text": "We also demonstrate that in the absence of observed typological information, a set of automatically induced latent features can effectively work as a proxy for typology.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets and Evaluation We test the effectiveness of our approach on 17 languages: Arabic, Basque, Bulgarian, Catalan, Chinese, Czech, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Spanish, Swedish and Turkish.", "labels": [], "entities": []}, {"text": "We used datasets distributed for the 2006 and 2007 CoNLL Shared Tasks (.", "labels": [], "entities": [{"text": "CoNLL Shared Tasks", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.7418361703554789}]}, {"text": "Each dataset provides manually annotated dependency trees and POS tags.", "labels": [], "entities": []}, {"text": "To enable crosslingual sharing, we map the gold partof-speech tags in each corpus to a common coarse tagset).", "labels": [], "entities": []}, {"text": "The coarse tagset consists of 11 tags: noun, verb, adjective, adverb, pronoun, determiner, adposition, numeral, conjunction, particle, punctuation mark, and X (a catch-all tag).", "labels": [], "entities": [{"text": "X", "start_pos": 157, "end_pos": 158, "type": "METRIC", "confidence": 0.9748775959014893}]}, {"text": "Among several available fineto-coarse mapping schemes, we employ the one of that yields consistently better performance for our method and the baselines than the mapping proposed by . As the evaluation metric, we use directed dependency accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.671826958656311}]}, {"text": "Following standard evaluation practices, we do not evaluate on punctuation.", "labels": [], "entities": []}, {"text": "For both the baselines and our model we evaluate on all sentences of length 50 or less ignoring punctuation.", "labels": [], "entities": []}, {"text": "Training Regime Our model typically converges quickly and does not require more than 50 iterations of EM.", "labels": [], "entities": []}, {"text": "When the model involves latent typological variables, the initialization of these variables can impact the final performance.", "labels": [], "entities": []}, {"text": "As a selection criterion for initialization, we consider the performance of the final model averaged over the supervised source languages.", "labels": [], "entities": [{"text": "initialization", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9717440605163574}]}, {"text": "We perform ten random restarts and select the best according to this criterion.", "labels": [], "entities": []}, {"text": "Likewise, the threshold value b for the PR constraint on the dependency length is tuned on the source languages, using average test set accuracy as the selection criterion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8842348456382751}]}, {"text": "Baselines We compare against the state-of-the-art multilingual dependency parsers that do not use parallel corpora for training.", "labels": [], "entities": []}, {"text": "All the systems were eval-uated using the same fine-to-coarse tagset mapping.", "labels": [], "entities": []}, {"text": "The first baseline, Transfer, uses direct transfer of a discriminative parser trained on all the source languages ).", "labels": [], "entities": []}, {"text": "This simple baseline achieves surprisingly good results, within less than 3% difference from a parser trained using parallel data.", "labels": [], "entities": []}, {"text": "In the second baseline (Mixture), parameters of the target language are estimated as a weighted mixture of the parameters learned from annotated source languages.", "labels": [], "entities": []}, {"text": "The underlying parsing model is the dependency model with valance (DMV) ().", "labels": [], "entities": []}, {"text": "Originally, the baseline methods were evaluated on different sets of languages using a different tag mapping.", "labels": [], "entities": []}, {"text": "Therefore, we obtained new results for these methods in our setup.", "labels": [], "entities": []}, {"text": "For the Transfer baseline, for each target language we trained the model on all other languages in our dataset.", "labels": [], "entities": []}, {"text": "For the Mixture baseline, we trained the model on the same four languages used in the original paper -English, German, Czech and Italian.", "labels": [], "entities": []}, {"text": "When measuring the performance on these languages, we selected another set of four languages with a similar level of diversity.", "labels": [], "entities": []}], "tableCaptions": []}