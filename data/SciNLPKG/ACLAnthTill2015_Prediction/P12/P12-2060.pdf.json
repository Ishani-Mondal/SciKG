{"title": [{"text": "Improving the IBM Alignment Models Using Variational Bayes", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9780932664871216}, {"text": "IBM Alignment", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.5172910094261169}]}], "abstractContent": [{"text": "Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters.", "labels": [], "entities": []}, {"text": "We apply one such Bayesian technique, vari-ational Bayes, to the IBM models of word alignment for statistical machine translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.7504850924015045}, {"text": "statistical machine translation", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.7018179694811503}]}, {"text": "We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.6578205823898315}, {"text": "BLEU score", "start_pos": 195, "end_pos": 205, "type": "METRIC", "confidence": 0.9862721860408783}]}], "introductionContent": [{"text": "The IBM Models of word alignment, along with the Hidden Markov Model (HMM) (, serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7051622867584229}, {"text": "machine translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7432668209075928}]}, {"text": "Both the IBM Models and the HMM are trained using the EM algorithm.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.9455025792121887}]}, {"text": "Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting.", "labels": [], "entities": [{"text": "EM", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9538971781730652}]}, {"text": "For instance, showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.672084629535675}]}, {"text": "In machine translation, and use Bayesian techniques to learn bilingual phrase pairs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7790882587432861}]}, {"text": "In this setting, which involves finding a segmentation of the input sentences into phrasal units, it is particularly important to control the tendency of EM to choose longer phrases, which explain the training data well but are unlikely to generalize.", "labels": [], "entities": []}, {"text": "However, most state-of-the-art machine translation systems today are built on the basis of wordlevel alignments of the type generated by GIZA++ from the IBM Models and the HMM.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7155406177043915}, {"text": "IBM Models", "start_pos": 153, "end_pos": 163, "type": "DATASET", "confidence": 0.936839371919632}]}, {"text": "Overfitting is also a problem in this context, and improving these word alignment systems could be of broad utility in machine translation research.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7406518310308456}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.793353945016861}]}, {"text": "Moore (2004) discusses details of how EM overfits the data when training IBM Model 1.", "labels": [], "entities": []}, {"text": "He discovers that the EM algorithm is particularly susceptible to overfitting in the case of rare words, due to the \"garbage collection\" phenomenon.", "labels": [], "entities": []}, {"text": "Suppose a sentence contains an English word e 1 that occurs nowhere else in the data, and its French translation f 1 . Suppose that same sentence also contains a word e 2 which occurs frequently in the overall data but whose translation in this sentence, f 2 , co-occurs with it infrequently.", "labels": [], "entities": []}, {"text": "If the translation t(f 2 |e 2 ) occurs with probability 0.1, then the sentence will have a higher probability if EM assigns the rare word and its actual translation a probability of t(f 1 |e 1 ) = 0.5, and assigns the rare word's translation to f 2 a probability of t(f 2 |e 1 ) = 0.5, than if it assigns a probability of 1 to the correct translation t(f 1 |e 1 ).", "labels": [], "entities": []}, {"text": "Moore suggests a number of solutions to this issue, including add-n smoothing and initializing the probabilities based on a heuristic rather than choosing uniform probabilities.", "labels": [], "entities": []}, {"text": "When combined, his solutions cause a significant decrease in alignment error rate (AER).", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 61, "end_pos": 87, "type": "METRIC", "confidence": 0.9104582071304321}]}, {"text": "More recently, have added a Bayesian prior to IBM Model 1 using Gibbs sampling for inference, showing improvements in BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9985427856445312}]}, {"text": "In this paper, we describe the results of incorpo-rating variational Bayes (VB) into the widely used GIZA++ software for word alignment.", "labels": [], "entities": [{"text": "incorpo-rating variational Bayes (VB)", "start_pos": 42, "end_pos": 79, "type": "METRIC", "confidence": 0.817171906431516}, {"text": "word alignment", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.8195837438106537}]}, {"text": "We use VB both because it converges more quickly than Gibbs sampling, and because it can be applied in a fairly straightforward manner to all of the models implemented by GIZA++.", "labels": [], "entities": []}, {"text": "In Section 2, we describe VB in more detail.", "labels": [], "entities": [{"text": "VB", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.6831359267234802}]}, {"text": "In Section 3, we present results for VB for the various models, in terms of perplexity of held-out test data, alignment error rate (AER), and the BLEU scores which result from using our version of GIZA++ in the end-to-end phrase-based machine translation system Moses.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 110, "end_pos": 136, "type": "METRIC", "confidence": 0.9598569869995117}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9994526505470276}, {"text": "phrase-based machine translation", "start_pos": 222, "end_pos": 254, "type": "TASK", "confidence": 0.6380521158377329}]}, {"text": "2 Variational Bayes and GIZA++ gives a detailed derivation of a variational Bayesian algorithm for HMMs.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9310015439987183}]}, {"text": "The result is a very slight change to the M step of the original EM algorithm.", "labels": [], "entities": [{"text": "M step", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9497369229793549}]}, {"text": "During the M step of the original algorithm, the expected counts collected in the E step are normalized to give the new values of the parameters: The variational Bayesian M step performs an inexact normalization, where the resulting parameters will add up to less than one.", "labels": [], "entities": []}, {"text": "It does this by passing the expected counts collected in the E step through the function f (v) = exp(\u03c8(v)), where \u03c8 is the digamma function, and \u03b1 is the hyperparameter of the Dirichlet prior: This modified M step can be applied to any model which uses a multinomial distribution; for this reason, it works for the IBM Models as well as HMMs, and is thus what we use for GIZA++.", "labels": [], "entities": []}, {"text": "In practice, the digamma function has the effect of subtracting 0.5 from its argument.", "labels": [], "entities": []}, {"text": "When \u03b1 is set to a low value, this results in \"anti-smoothing\".", "labels": [], "entities": []}, {"text": "For the translation probabilities, because about 0.5 is subtracted from the expected counts, small counts corresponding to rare co-occurrences of words will be penalized heavily, while larger counts will not be affected very much.", "labels": [], "entities": []}, {"text": "Thus, low values of \u03b1 cause the algorithm to favor words which co-occur frequently and to distrust words that co-occur rarely.: An example of data with rare words.", "labels": [], "entities": []}, {"text": "In this way, VB controls the overfitting that would otherwise occur with rare words.", "labels": [], "entities": []}, {"text": "On the other hand, higher values of \u03b1 can be chosen if smoothing is desired, for instance in the case of the alignment probabilities, which state how likely a word in position i of the English sentence is to align to a word in position j of the French sentence.", "labels": [], "entities": []}, {"text": "For these probabilities, smoothing is important because we do not want to rule out any alignment altogether, no matter how infrequently it occurs in the data.", "labels": [], "entities": []}, {"text": "We implemented VB for the translation probabilities as well as for the position alignment probabilities of IBM Model 2.", "labels": [], "entities": [{"text": "VB", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.988239049911499}, {"text": "IBM Model 2", "start_pos": 107, "end_pos": 118, "type": "DATASET", "confidence": 0.9142183462778727}]}, {"text": "We discovered that adding VB for the translation probabilities improved the performance of the system.", "labels": [], "entities": [{"text": "VB", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9718429446220398}]}, {"text": "However, including VB for the alignment probabilities had relatively little effect, because the alignment table in its original form does some smoothing during normalization by interpolating the counts with a uniform distribution.", "labels": [], "entities": [{"text": "VB", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9823519587516785}]}, {"text": "Because VB can itself be a form of smoothing, the two versions of the code behave similarly.", "labels": [], "entities": [{"text": "VB", "start_pos": 8, "end_pos": 10, "type": "DATASET", "confidence": 0.8149511814117432}]}, {"text": "We did not experiment with VB for the distortion probabilities of the HMM or Models 3 and 4, as these distributions have fewer parameters and are likely to have reliable counts during EM.", "labels": [], "entities": [{"text": "VB", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9863677620887756}]}, {"text": "Thus, in Section 3, we present the results of using VB for the translation probabilities only.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Effect of Adding Variational Bayes to Specific  Models", "labels": [], "entities": []}]}