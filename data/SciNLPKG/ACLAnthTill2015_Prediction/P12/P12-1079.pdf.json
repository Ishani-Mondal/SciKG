{"title": [{"text": "A Topic Similarity Model for Hierarchical Phrase-based Translation", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.7519153157869974}]}], "abstractContent": [{"text": "Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level.", "labels": [], "entities": [{"text": "statistical machine translation (SMT", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.7364684998989105}]}, {"text": "However , SMT has been advanced from word-based paradigm to phrase/rule-based paradigm.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9934502840042114}]}, {"text": "We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.6517562419176102}]}, {"text": "We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents.", "labels": [], "entities": []}, {"text": "We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.957480788230896}, {"text": "NIST Chinese-to-English translation", "start_pos": 95, "end_pos": 130, "type": "TASK", "confidence": 0.745201031366984}]}, {"text": "Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.", "labels": [], "entities": [{"text": "speed", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9836693406105042}]}], "introductionContent": [{"text": "Topic model) is a popular technique for discovering the underlying topic structure of documents.", "labels": [], "entities": []}, {"text": "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models () to improve translation quality.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.7968381742636362}, {"text": "topic-specific lexicon translation", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.761553148428599}]}, {"text": "Topic-specific lexicon translation models focus on word-level translations.", "labels": [], "entities": [{"text": "Topic-specific lexicon translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7166390816370646}, {"text": "word-level translations", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7075873166322708}]}, {"text": "Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases * Corresponding author by these probabilities.", "labels": [], "entities": [{"text": "word translation probabilities", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.7714337507883707}]}, {"text": "However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word byword.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9466512203216553}]}, {"text": "Since asynchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level.", "labels": [], "entities": []}, {"text": "Consequently, we propose a topic similarity model for hierarchical phrase-based translation, where each synchronous rule is associated with a topic distribution.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.6056578854719797}]}, {"text": "In particular, \u2022 Given a document to be translated, we calculate the topic similarity between a rule and the document based on their topic distributions.", "labels": [], "entities": []}, {"text": "We augment the hierarchical phrase-based system by integrating the proposed topic similarity model as anew feature (Section 3.1).", "labels": [], "entities": []}, {"text": "\u2022 As we will discuss in Section 3.2, the similarity between a generic rule and a given source document computed by our topic similarity model is often very low.", "labels": [], "entities": [{"text": "similarity", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.952277421951294}]}, {"text": "We don't want to penalize these generic rules.", "labels": [], "entities": []}, {"text": "Therefore we further propose a topic sensitivity model which rewards generic rules so as to complement the topic similarity model.", "labels": [], "entities": []}, {"text": "\u2022 We estimate the topic distribution fora rule based on both the source and target side topic models (Section 4.1).", "labels": [], "entities": []}, {"text": "In order to calculate similarities between target-side topic distributions of rules and source-side topic distributions of given documents during decoding, we project (d) X1 \u4e3e \u884c \u4f1a \u8c08 X2 \u21d2 held talks X1 X2 Figure 1: Four synchronous rules with topic distributions.", "labels": [], "entities": []}, {"text": "Each sub-graph shows a rule with its topic distribution, where the X-axis means topic index and the Y-axis means the topic probability.", "labels": [], "entities": []}, {"text": "Notably, the rule (b) and rule (c) shares the same source Chinese string, but they have different topic distributions due to the different English translations.", "labels": [], "entities": []}, {"text": "the target-side topic distributions of rules into the space of source-side topic model by one-tomany projection (Section 4.2).", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English translation tasks (Section 6) show that, our method outperforms the baseline hierarchial phrase-based system by +0.9 BLEU points.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.7121166388193766}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9987900853157043}]}, {"text": "This result is also +0.5 points higher and 3 times faster than the previous topic-specific lexicon translation method.", "labels": [], "entities": [{"text": "topic-specific lexicon translation", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.6538140773773193}]}, {"text": "We further show that both the source-side and target-side topic distributions improve translation quality and their improvements are complementary to each other.", "labels": [], "entities": [{"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9603551626205444}]}], "datasetContent": [{"text": "We try to answer the following questions by experiments: 1.", "labels": [], "entities": []}, {"text": "Is our topic similarity model able to improve translation quality in terms of BLEU?", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9600804448127747}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9978547692298889}]}, {"text": "Furthermore, are source-side and target-side rule-topic distributions complementary to each other?: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (\"Baseline\") and the topic-specific lexicon translation method (\"TopicLex\").", "labels": [], "entities": [{"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9994426369667053}, {"text": "speed", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.9518656134605408}, {"text": "topic-specific lexicon translation", "start_pos": 256, "end_pos": 290, "type": "TASK", "confidence": 0.6775661905606588}]}, {"text": "\"SimSrc\" and \"SimTgt\" denote similarity by source-side and target-side rule-distribution respectively, while \"Sim+Sen\" activates the two similarity and two sensitivity features.", "labels": [], "entities": []}, {"text": "\"Avg\" is the average BLEU score on the two test sets.", "labels": [], "entities": [{"text": "Avg", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9994679093360901}, {"text": "BLEU score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.98066246509552}]}, {"text": "Scores marked in bold mean significantly) better than Baseline (p < 0.01).", "labels": [], "entities": []}, {"text": "2. Is it helpful to introduce the topic sensitivity model to distinguish topic-insensitive and topic-sensitive rules?", "labels": [], "entities": []}, {"text": "3. Is it necessary to project topics by one-to-many correspondence instead of one-to-one correspondence?", "labels": [], "entities": []}, {"text": "4. What is the effect of our method on various types of rules, such as phrase rules and rules with non-terminals?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of topic-to-topic correspondence. The  last line shows the correspondence probability. Each col- umn means a topic represented by its top-10 topical word- s. The first column is a target-side topic, while the rest  three columns are source-side topics.", "labels": [], "entities": []}, {"text": " Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the  traditional hierarchical system (\"Baseline\") and the topic-specific lexicon translation method (\"TopicLex\"). \"SimSrc\"  and \"SimTgt\" denote similarity by source-side and target-side rule-distribution respectively, while \"Sim+Sen\" acti- vates the two similarity and two sensitivity features. \"Avg\" is the average BLEU score on the two test sets. Scores  marked in bold mean significantly", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9991835951805115}, {"text": "speed", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9764450192451477}, {"text": "topic-specific lexicon translation", "start_pos": 167, "end_pos": 201, "type": "TASK", "confidence": 0.7063729166984558}, {"text": "Avg", "start_pos": 405, "end_pos": 408, "type": "METRIC", "confidence": 0.9957977533340454}, {"text": "BLEU", "start_pos": 425, "end_pos": 429, "type": "METRIC", "confidence": 0.9987169504165649}]}, {"text": " Table 3: Percentage of topic-sensitive rules of various  types of rule according to source-side (\"Src\") and target- side (\"Tgt\") topic distributions. Phrase rules are fully  lexicalized, while monotone and reordering rules contain  nonterminals (Section 6.5).", "labels": [], "entities": []}]}