{"title": [{"text": "Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations", "labels": [], "entities": [{"text": "Approximation", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9784182906150818}]}], "abstractContent": [{"text": "We address the issue of consuming heterogeneous annotation data for Chinese word seg-mentation and part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.7171689569950104}]}, {"text": "We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible.", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 78, "end_pos": 105, "type": "DATASET", "confidence": 0.9709483087062836}, {"text": "PKU's People's Daily (PPD)", "start_pos": 110, "end_pos": 136, "type": "DATASET", "confidence": 0.9536914229393005}]}, {"text": "The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) retraining models with high quality automatically converted data to reduce the estimation error.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9884599447250366}, {"text": "approximation error", "start_pos": 165, "end_pos": 184, "type": "METRIC", "confidence": 0.9179437160491943}]}, {"text": "Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.", "labels": [], "entities": [{"text": "CTB", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8765268325805664}, {"text": "error reduction", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.8646814227104187}]}], "introductionContent": [{"text": "A majority of data-driven NLP systems rely on large-scale, manually annotated corpora that are important to train statistical models but very expensive to build.", "labels": [], "entities": []}, {"text": "Nowadays, for many tasks, multiple heterogeneous annotated corpora have been built and publicly available.", "labels": [], "entities": []}, {"text": "For example, the Penn Treebank is popular to train PCFG-based parsers, while the Redwoods Treebank is well known for HPSG research; the Propbank is favored to build general semantic role labeling systems, while the FrameNet is attractive for predicate-specific labeling.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9956521093845367}, {"text": "Redwoods Treebank", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.8435781002044678}, {"text": "Propbank", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.9446994662284851}]}, {"text": "The anno- * This work is mainly finished when the first author was in Saarland University and DFKI.", "labels": [], "entities": [{"text": "DFKI", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.9168341755867004}]}, {"text": "Both authors are the corresponding authors.", "labels": [], "entities": []}, {"text": "tation schemes in different projects are usually different, since the underlying linguistic theories vary and have different ways to explain the same language phenomena.", "labels": [], "entities": [{"text": "tation", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9406768679618835}]}, {"text": "Though statistical NLP systems usually are not bound to specific annotation standards, almost all of them assume homogeneous annotation in the training corpus.", "labels": [], "entities": []}, {"text": "The co-existence of heterogeneous annotation data therefore presents anew challenge to the consumers of such resources.", "labels": [], "entities": []}, {"text": "There are two essential characteristics of heterogeneous annotations that can be utilized to reduce two main types of errors in statistical NLP, i.e. the approximation error that is due to the intrinsic suboptimality of a model and the estimation error that is due to having only finite training data.", "labels": [], "entities": [{"text": "approximation error", "start_pos": 154, "end_pos": 173, "type": "METRIC", "confidence": 0.94135981798172}]}, {"text": "First, heterogeneous annotations are (similar but) different as a result of different annotation schemata.", "labels": [], "entities": []}, {"text": "Systems respectively trained on heterogeneous annotation data can produce different but relevant linguistic analysis.", "labels": [], "entities": []}, {"text": "This suggests that complementary features from heterogeneous analysis can be derived for disambiguation, and therefore the approximation error can be reduced.", "labels": [], "entities": [{"text": "approximation", "start_pos": 123, "end_pos": 136, "type": "METRIC", "confidence": 0.9469363689422607}]}, {"text": "Second, heterogeneous annotations are (different but) similar because their linguistic analysis is highly correlated.", "labels": [], "entities": []}, {"text": "This implies that appropriate conversions between heterogeneous corpora could be reasonably accurate, and therefore the estimation error can be reduced by reason of the increase of reliable training data.", "labels": [], "entities": [{"text": "estimation error", "start_pos": 120, "end_pos": 136, "type": "METRIC", "confidence": 0.9676370620727539}]}, {"text": "This paper explores heterogeneous annotations to reduce both approximation and estimation errors for Chinese word segmentation and part-of-speech (POS) tagging, which are fundamental steps for more advanced Chinese language processing tasks.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.5959405899047852}, {"text": "part-of-speech (POS) tagging", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.6684039831161499}, {"text": "Chinese language processing tasks", "start_pos": 207, "end_pos": 240, "type": "TASK", "confidence": 0.6794408187270164}]}, {"text": "We empirically analyze the diversity between two representative popular heterogeneous corpora, i.e. Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD).", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 100, "end_pos": 127, "type": "DATASET", "confidence": 0.9693863093852997}, {"text": "PKU's People's Daily (PPD)", "start_pos": 132, "end_pos": 158, "type": "DATASET", "confidence": 0.936951257288456}]}, {"text": "To that end, we manually label 200 sentences from CTB with PPD-style annotations.", "labels": [], "entities": []}, {"text": "Our analysis confirms the aforementioned two properties of heterogeneous annotations.", "labels": [], "entities": []}, {"text": "Inspired by the sub-word tagging method introduced in, we propose a structure-based stacking model to fully utilize heterogeneous word structures to reduce the approximation error.", "labels": [], "entities": [{"text": "sub-word tagging", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7533802092075348}]}, {"text": "In particular, joint word segmentation and POS tagging is addressed as a two step process.", "labels": [], "entities": [{"text": "joint word segmentation", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.6053083737691244}, {"text": "POS tagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.8426963090896606}]}, {"text": "First, character-based taggers are respectively trained on heterogeneous annotations to produce multiple analysis.", "labels": [], "entities": [{"text": "character-based taggers", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.7040064334869385}]}, {"text": "The outputs of these taggers are then merged into sub-word sequences, which are further re-segmented and tagged by a sub-word tagger.", "labels": [], "entities": []}, {"text": "The sub-word tagger is designed to refine the tagging result with the help of heterogeneous annotations.", "labels": [], "entities": []}, {"text": "To reduce the estimation error, we employ a learning-based approach to convert complementary heterogeneous data to increase labeled training data for the target task.", "labels": [], "entities": []}, {"text": "Both the character-based tagger and the sub-word tagger can be refined by re-training with automatically converted data.", "labels": [], "entities": []}, {"text": "We conduct experiments on the CTB and PPD data, and compare our system with state-of-theart systems.", "labels": [], "entities": [{"text": "CTB and PPD data", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.6734628975391388}]}, {"text": "Our structure-based stacking model achieves an f-score of 94.36, which is superior to a feature-based stacking model introduced in).", "labels": [], "entities": [{"text": "f-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9923276305198669}]}, {"text": "The converted data can also enhance the baseline model.", "labels": [], "entities": []}, {"text": "A simple character-based model can be improved from 93.41 to 94.11.", "labels": [], "entities": []}, {"text": "Since the two treatments are concerned with reducing different types of errors and thus not fully overlapping, the combination of them gives a further improvement.", "labels": [], "entities": []}, {"text": "Our final system achieves an f-score of 94.68, which yields a relative error reduction of 11% over the best published result (94.02).", "labels": [], "entities": [{"text": "f-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9984672665596008}, {"text": "error", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.7308040857315063}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: F-scores relative to sizes of training data. Sizes  (shown in column #CTB and #PPD) are numbers of sen- tences in each training corpus.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9914458990097046}]}, {"text": " Table 4: F-scores with gold PPD-style tagging on the  manually converted data.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9832866191864014}]}, {"text": " Table 5: Performance of re-trained models on the devel- opment data.", "labels": [], "entities": []}, {"text": " Table 6: Performance of different systems on the test  data.", "labels": [], "entities": []}]}