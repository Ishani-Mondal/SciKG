{"title": [{"text": "Learning to Translate with Multiple Objectives", "labels": [], "entities": [{"text": "Learning to Translate with Multiple Objectives", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6958575248718262}]}], "abstractContent": [{"text": "We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8519759058952332}]}, {"text": "Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9953658580780029}, {"text": "TER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9744687676429749}, {"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9638022184371948}]}, {"text": "Our approach is based on the theory of Pareto Optimality.", "labels": [], "entities": []}, {"text": "It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics.", "labels": [], "entities": []}, {"text": "We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.9316253960132599}, {"text": "MT optimization", "start_pos": 151, "end_pos": 166, "type": "TASK", "confidence": 0.9890158772468567}]}], "introductionContent": [{"text": "Weight optimization is an important step in building machine translation (MT) systems.", "labels": [], "entities": [{"text": "Weight optimization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.806684285402298}, {"text": "machine translation (MT)", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8565167188644409}]}, {"text": "Discriminative optimization methods such as MERT, MIRA (), PRO, and have been influential in improving MT systems in recent years.", "labels": [], "entities": [{"text": "MERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.7729985117912292}, {"text": "MIRA", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.8660094141960144}, {"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9957245588302612}]}, {"text": "These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9984351992607117}]}, {"text": "However, we know that a single metric such as BLEU is not enough.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9983660578727722}]}, {"text": "Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality.", "labels": [], "entities": []}, {"text": "* *Now at Nara Institute of While many alternatives have been proposed, such a perfect evaluation metric remains elusive.", "labels": [], "entities": [{"text": "Nara Institute", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9453787505626678}]}, {"text": "As a result, many MT evaluation campaigns now report multiple evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9194925427436829}]}, {"text": "Different evaluation metrics focus on different aspects of translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8757962882518768}]}, {"text": "For example, while BLEU () focuses on word-based n-gram precision, METEOR allows for stem/synonym matching and incorporates recall.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9952651262283325}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.8406359553337097}, {"text": "METEOR", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9641189575195312}, {"text": "stem/synonym matching", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.6488235965371132}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9951620697975159}]}, {"text": "TER) allows arbitrary chunk movements, while permutation metrics like RIBES () measure deviation in word order. and semantics () also help.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8871899247169495}, {"text": "RIBES", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.8323059678077698}]}, {"text": "Arguably, all these metrics correspond to our intuitions on what is a good translation.", "labels": [], "entities": []}, {"text": "The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9392564296722412}]}, {"text": "Can we really claim that a system is good if it has high BLEU, but very low METEOR?", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9995105266571045}, {"text": "METEOR", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.997683048248291}]}, {"text": "Similarly, is a high-METEOR low-BLEU system desirable?", "labels": [], "entities": []}, {"text": "Our goal is to propose a multi-objective optimization method that avoids \"overfitting to a single metric\".", "labels": [], "entities": []}, {"text": "We want to build a MT system that does well with respect to many aspects of translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9827621579170227}, {"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9588596820831299}]}, {"text": "In general, we cannot expect to improve multiple metrics jointly if there are some inherent tradeoffs.", "labels": [], "entities": []}, {"text": "We therefore need to define the notion of Pareto Optimality, which characterizes this tradeoff in a rigorous way and distinguishes the set of equally good solutions.", "labels": [], "entities": []}, {"text": "We will describe Pareto Optimality in detail later, but roughly speaking, a hypothesis is pareto-optimal if there exist no other hypothesis better in all metrics.", "labels": [], "entities": []}, {"text": "The contribution of this paper is two-fold: \u2022 We introduce PMO (Pareto-based Multiobjective Optimization), a general approach for learning with multiple metrics.", "labels": [], "entities": []}, {"text": "Existing singleobjective methods can be easily extended to multi-objective using PMO.", "labels": [], "entities": []}, {"text": "\u2022 We show that PMO outperforms the alternative (single-objective optimization of linearlycombined metrics) in multi-objective space, and especially obtains stronger results for metrics that maybe difficult to tune individually.", "labels": [], "entities": [{"text": "PMO", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7778745889663696}]}, {"text": "In the following, we first explain the theory of Pareto Optimality (Section 2), and then use it to buildup our proposed PMO approach (Section 3).", "labels": [], "entities": []}, {"text": "Experiments on NIST Chinese-English and PubMed English-Japanese translation using BLEU, TER, and RIBES are presented in Section 4.", "labels": [], "entities": [{"text": "NIST Chinese-English", "start_pos": 15, "end_pos": 35, "type": "DATASET", "confidence": 0.8759316205978394}, {"text": "PubMed English-Japanese translation", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.6396447718143463}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9981978535652161}, {"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9948558807373047}, {"text": "RIBES", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9947670698165894}]}, {"text": "We conclude by discussing related work (Section 5) and opportunities/limitations (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific abstracts.", "labels": [], "entities": [{"text": "English-to-Japanese translation of scientific abstracts", "start_pos": 56, "end_pos": 111, "type": "TASK", "confidence": 0.7371924757957459}]}, {"text": "As metrics we use BLEU and RIBES (which demonstrated good human correlation in this language pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9987840056419373}, {"text": "RIBES", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9874367713928223}]}, {"text": "The NIST task is Chinese-to-English translation with OpenMT08 training data and MT06 as devset.", "labels": [], "entities": [{"text": "NIST", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8396300077438354}, {"text": "OpenMT08 training data", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.8812268376350403}, {"text": "MT06", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.7643054723739624}]}, {"text": "As metrics we use BLEU and NTER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9992917776107788}, {"text": "NTER", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.5939440727233887}]}, {"text": "\u2022 BLEU = BP \u00d7 (\u03a0prec n ) 1/4 . BP is brevity penality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9993894100189209}, {"text": "BP", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9922217726707458}, {"text": "BP", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.996465802192688}]}, {"text": "prec n is precision of n-gram matches.", "labels": [], "entities": [{"text": "prec", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9527141451835632}, {"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9960429668426514}]}, {"text": "\u2022 RIBES = (\u03c4 + 1)/2 \u00d7 prec 1/4 1 , with Kendall's \u03c4 computed by measuring permutation between matching words in reference and hypothesis 5 . \u2022 NTER=max(1\u2212TER, 0), which normalizes Translation Edit Rate 6 so that NTER=1 is best.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9947302341461182}, {"text": "NTER", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9148109555244446}, {"text": "TER", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.8644872307777405}, {"text": "NTER", "start_pos": 212, "end_pos": 216, "type": "METRIC", "confidence": 0.9397194981575012}]}, {"text": "We compare two multi-objective approaches: 1.", "labels": [], "entities": []}, {"text": "Linear-Combination of metrics (Eq. 2), optimized with PRO.", "labels": [], "entities": []}, {"text": "We search a range of combination settings: (p 1 , p 2 ) = {(0, 1), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3), (1, 0)}.", "labels": [], "entities": []}, {"text": "Note (1, 0) reduces to standard single-metric optimization of e.g. BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9855060577392578}]}, {"text": "2. Proposed Pareto approach (PMO-PRO).", "labels": [], "entities": []}, {"text": "Evaluation of multi-objective problems can be tricky because there is no single figure-of-merit.", "labels": [], "entities": []}, {"text": "We thus adopted the following methodology: We run both methods 5 times (i.e. using the 5 different (p 1 , p 2 ) setting each time) and I = 20 iterations each.", "labels": [], "entities": [{"text": "I", "start_pos": 135, "end_pos": 136, "type": "METRIC", "confidence": 0.9898348450660706}]}, {"text": "For each method, this generates 5x20=100 results, and we plot the Pareto Frontier of these points in a 2-dimensional metric space (e.g. see).", "labels": [], "entities": [{"text": "Pareto Frontier", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.9376491904258728}]}, {"text": "A method is deemed better if its final Pareto Frontier curve is strictly dominating the other.", "labels": [], "entities": []}, {"text": "We report devset results here; testset trends are similar but not included due to space constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Training time usage in PMO-PRO (Algo 2).", "labels": [], "entities": [{"text": "Algo 2)", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.8928532600402832}]}]}