{"title": [{"text": "Joint Learning of a Dual SMT System for Paraphrase Generation", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9493776559829712}]}], "abstractContent": [{"text": "SMT has been used in paraphrase generation by translating a source sentence into another (pivot) language and then back into the source.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9783506989479065}, {"text": "paraphrase generation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.9706156253814697}]}, {"text": "The resulting sentences can be used as candidate paraphrases of the source sentence.", "labels": [], "entities": []}, {"text": "Existing work that uses two independently trained SMT systems cannot directly optimize the paraphrase results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9813871383666992}]}, {"text": "Paraphrase criteria especially the paraphrase rate is notable to be ensured in that way.", "labels": [], "entities": [{"text": "paraphrase rate", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.8020229637622833}]}, {"text": "In this paper, we propose a joint learning method of two SMT systems to optimize the process of paraphrase generation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9902336597442627}, {"text": "paraphrase generation", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.8837180733680725}]}, {"text": "In addition, a revised BLEU score (called iBLEU) which measures the adequacy and diversity of the generated paraphrase sentence is proposed for tuning parameters in SMT systems.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9765859842300415}, {"text": "SMT", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.9913138747215271}]}, {"text": "Our experiments on NIST 2008 testing data with automatic evaluation as well as human judgments suggest that the proposed method is able to enhance the paraphrase quality by adjusting between semantic equivalency and surface dissimilarity.", "labels": [], "entities": [{"text": "NIST 2008 testing data", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.9549693018198013}]}], "introductionContent": [{"text": "Paraphrasing (at word, phrase, and sentence levels) is a procedure for generating alternative expressions with an identical or similar meaning to the original text.", "labels": [], "entities": []}, {"text": "Paraphrasing technology has been applied in many NLP applications, such as machine translation (MT), question answering (QA), and natural language generation (NLG).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.8438425421714782}, {"text": "question answering (QA)", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.8979191184043884}, {"text": "natural language generation (NLG)", "start_pos": 130, "end_pos": 163, "type": "TASK", "confidence": 0.8059258162975311}]}, {"text": "As paraphrasing can be viewed as a translation process between the original expression (as input) and the paraphrase results (as output), both in the same language, statistical machine translation (SMT) has been used for this task.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 165, "end_pos": 202, "type": "TASK", "confidence": 0.7718590547641119}]}, {"text": "build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events.", "labels": [], "entities": []}, {"text": "enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications ().", "labels": [], "entities": []}, {"text": "Performance of the monolingual MT-based method in paraphrase generation is limited by the large-scale paraphrase corpus it relies on as the corpus is not readily available ( ).", "labels": [], "entities": [{"text": "MT-based", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.9434553980827332}, {"text": "paraphrase generation", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.9542918801307678}]}, {"text": "In contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase).", "labels": [], "entities": []}, {"text": "Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the original language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9744438529014587}]}, {"text": "For question expansion, Dubou\u00e9 and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc.", "labels": [], "entities": [{"text": "question expansion", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8367679119110107}]}, {"text": "Max (2009) generates paraphrase fora given segment by forcing the segment being translated independently in both of the translation processes.", "labels": [], "entities": []}, {"text": "Context features are added into the SMT system to improve translation correctness against polysemous.", "labels": [], "entities": [{"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9937024116516113}, {"text": "translation correctness", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.9120067656040192}]}, {"text": "To reduce the noise introduced by machine translation,  propose combining the results of multiple machine translation engines' by performing MBR (Minimum Bayes Risk) () decoding on the N-best translation candidates.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7764084041118622}, {"text": "MBR (Minimum Bayes Risk)", "start_pos": 141, "end_pos": 165, "type": "METRIC", "confidence": 0.8494171102841696}]}, {"text": "The work presented in this paper belongs to the pivot language method for paraphrase generation.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.9434164464473724}]}, {"text": "Previous work employs two separately trained SMT systems the parameters of which are tuned for SMT scheme and therefore cannot directly optimize the paraphrase purposes, for example, optimize the diversity against the input.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9845957159996033}, {"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9661068320274353}]}, {"text": "Another problem comes from the contradiction between two criteria in paraphrase generation: adequacy measuring the semantic equivalency and paraphrase rate measuring the surface dissimilarity.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.889584869146347}]}, {"text": "As they are incompatible ( , the question arises how to adapt between them to fit different application scenarios.", "labels": [], "entities": []}, {"text": "To address these issues, in this paper, we propose a joint learning method of two SMT systems for paraphrase generation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9879814982414246}, {"text": "paraphrase generation", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.8759406507015228}]}, {"text": "The jointly-learned dual SMT system: (1) Adapts the SMT systems so that they are tuned specifically for paraphrase generation purposes, e.g., to increase the dissimilarity; (2) Employs a revised BLEU score (named iBLEU , as it's an input-aware BLEU metric) that measures adequacy and dissimilarity of the paraphrase results at the same time.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9263348579406738}, {"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9494630694389343}, {"text": "paraphrase generation", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.7412685751914978}, {"text": "BLEU score", "start_pos": 195, "end_pos": 205, "type": "METRIC", "confidence": 0.980635017156601}]}, {"text": "We test our method on NIST 2008 testing data.", "labels": [], "entities": [{"text": "NIST 2008 testing data", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.9762052744626999}]}, {"text": "With both automatic and human evaluations, the results show that the proposed method effectively balance between adequacy and dissimilarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "The joint inference method with MERT enables the dual SMT system to be optimized towards the quality of paraphrasing results.", "labels": [], "entities": [{"text": "MERT", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.6417601704597473}, {"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9718682765960693}]}, {"text": "Different application scenarios of paraphrase have different demands on the paraphrasing results and up to now, the widely mentioned criteria include (): Semantic adequacy, fluency and dissimilarity.", "labels": [], "entities": []}, {"text": "However, as pointed out by, there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.8976967930793762}]}, {"text": "Two issues are also raised in ( ) about using automatic metrics: paraphrase changes less gets larger BLEU score and the evaluations of paraphrase quality and rate tend to be incompatible.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9834987819194794}]}, {"text": "To address the above problems, we propose a metric for tuning parameters and evaluating the quality of each candidate paraphrase c : where sis the input sentence, r s represents the reference paraphrases.", "labels": [], "entities": []}, {"text": "BLEU (c, r s ) captures the semantic equivalency between the candidates and the references ( have shown the capability for measuring semantic equivalency using BLEU score); BLEU (c, s) is the BLEU score computed between the candidate and the source sentence to measure the dissimilarity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9840602874755859}, {"text": "BLEU score", "start_pos": 160, "end_pos": 170, "type": "METRIC", "confidence": 0.9804609715938568}, {"text": "BLEU", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.9987340569496155}, {"text": "BLEU score", "start_pos": 192, "end_pos": 202, "type": "METRIC", "confidence": 0.9764675199985504}]}, {"text": "\u03b1 is a parameter taking balance between adequacy and dissimilarity, smaller \u03b1 value indicates larger punishment on selfparaphrase.", "labels": [], "entities": []}, {"text": "Fluency is not explicitly presented because there is high correlation between fluency and adequacy (  and SMT has already taken this into consideration.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9598739147186279}, {"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9674639701843262}]}, {"text": "By using iBLEU , we aim at adapting paraphrasing performance to different application needs by adjusting \u03b1 value..", "labels": [], "entities": []}, {"text": "10-best lists are used in both of the translation processes.", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.973548948764801}]}, {"text": "The results of paraphrasing are illustrated in.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.953300952911377}]}, {"text": "We show the BLEU score (computed against references) to measure the adequacy and self-BLEU (computed against source sentence) to evaluate the dissimilarity (lower is better).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9819023609161377}]}, {"text": "By \"No Joint\", it means two independently trained SMT systems are employed in translating sentences from English to Chinese and then back into English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9842696189880371}]}, {"text": "This result is listed to indicate the performance when we do not involve joint learning to control the quality of paraphrase results.", "labels": [], "entities": []}, {"text": "For joint learning, results of \u03b1 from 0.7 to 1 are listed.", "labels": [], "entities": [{"text": "joint learning", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7547852993011475}]}, {"text": "From the results we can see that, when the value of \u03b1 decreases to address more penalty on selfparaphrase, the self-BLEU score rapidly decays while the consequence effect is that BLEU score computed against references also drops seriously.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 179, "end_pos": 189, "type": "METRIC", "confidence": 0.9794346690177917}]}, {"text": "When \u03b1 drops under 0.6 we observe the sentences become completely incomprehensible (this is the reason why we leave out showing the results of \u03b1 under 0.7).", "labels": [], "entities": []}, {"text": "The best balance is achieved when \u03b1 is between 0.7 and 0.9, where both of the sentence quality and variety are relatively preserved.", "labels": [], "entities": [{"text": "variety", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9638242721557617}]}, {"text": "As \u03b1 value is manually defined and not specially tuned, the exper-  iments only achieve comparable results with no joint learning when \u03b1 equals 0.8.", "labels": [], "entities": []}, {"text": "However, the results show that our method is able to effectively control the self-paraphrase rate and lower down the score of self-BLEU, this is done by both of the process of joint learning and introducing the metric of iBLEU to avoid trivial self-paraphrase.", "labels": [], "entities": []}, {"text": "It is not capable with no joint learning or with the traditional BLEU score does not take self-paraphrase into consideration.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9971076846122742}]}, {"text": "Human evaluation results are shown in.", "labels": [], "entities": []}, {"text": "We randomly choose 100 sentences from testing data.", "labels": [], "entities": []}, {"text": "For each setting, two annotators are asked to give scores about semantic adequacy, fluency, variety and overall quality.", "labels": [], "entities": []}, {"text": "The scales are 0 (meaning changed; incomprehensible; almost same; cannot be used), 1 (almost same meaning; little flaws; containing different words; maybe useful) and 2 (same meaning; good sentence; different sentential form; could be used).", "labels": [], "entities": []}, {"text": "The agreements between the annotators on these scores are 0.87, 0.74, 0.79 and 0.69 respectively.", "labels": [], "entities": []}, {"text": "From the results we can see that human evaluations are quite consistent with the automatic evaluation, where higher BLEU scores correspond to larger number of good adequacy and fluency labels, and higher self-BLEU results tend to get lower human evaluations over dissimilarity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9984070658683777}]}, {"text": "In our observation, we found that adequacy and fluency are relatively easy to be kept especially for short sentences.", "labels": [], "entities": []}, {"text": "In contrast, dissimilarity is not easy to achieve.", "labels": [], "entities": []}, {"text": "This is because the translation tables are used bi-directionally so lots of source sentences' fragments present in the paraphrasing results.", "labels": [], "entities": []}, {"text": "We show an example of the paraphrase results under different settings.", "labels": [], "entities": []}, {"text": "All the results' sentential forms are not changed comparing with the input sentence and also well-formed.", "labels": [], "entities": []}, {"text": "This is due to the short length of the source sentence.", "labels": [], "entities": []}, {"text": "Also, with smaller value of \u03b1, more variations show up in the paraphrase results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: iBLEU Score Results(NIST 2008)", "labels": [], "entities": [{"text": "iBLEU Score Results(NIST 2008)", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.8329064164842878}]}]}