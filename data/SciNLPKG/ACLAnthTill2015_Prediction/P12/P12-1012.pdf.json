{"title": [], "abstractContent": [{"text": "Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.9087984561920166}]}, {"text": "This paper proposes a methodologi-cal approach to temporally anchored relation extraction.", "labels": [], "entities": [{"text": "temporally anchored relation extraction", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.5772567093372345}]}, {"text": "Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity , aggregating evidence from documents supporting the relation.", "labels": [], "entities": []}, {"text": "We use a rich graph-based document-level representation to generate novel features for this task.", "labels": [], "entities": []}, {"text": "Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step.", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.773970901966095}, {"text": "relation extraction", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8301340341567993}]}, {"text": "Compared to the state of the art, the overall system achieves the highest precision reported.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994345307350159}]}], "introductionContent": [{"text": "A question that arises when extracting a relation is how to capture its temporal validity: Can we assign a period of time when the obtained relation held?", "labels": [], "entities": []}, {"text": "As pointed out in (, while much research in automatic relation extraction has focused on distilling static facts from text, many of the target relations are in fact fluents, dynamic relations whose truth value is dependent on time.", "labels": [], "entities": [{"text": "automatic relation extraction", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.6131266454855601}]}, {"text": "The Temporally anchored relation extraction problem consists in, given a natural language text document corpus, C, a target entity, e, and a target relation, r, extracting from the corpus the value of that relation for the entity, and a temporal interval for which the relation was valid.", "labels": [], "entities": [{"text": "Temporally anchored relation extraction", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.9085698276758194}]}, {"text": "In this paper, we introduce a methodological approach to temporal anchoring of relations automatically extracted from unrestricted text.", "labels": [], "entities": [{"text": "temporal anchoring of relations automatically extracted from unrestricted text", "start_pos": 57, "end_pos": 135, "type": "TASK", "confidence": 0.8702269328965081}]}, {"text": "Our system (see) extracts relational facts from text using distant supervision () and then anchors the relation to an interval of temporal validity.", "labels": [], "entities": []}, {"text": "The intuition is that a distant supervised system can effectively extract relations from the source text collection, and a straightforward date aggregation can then be applied to anchor them.", "labels": [], "entities": []}, {"text": "We propose a four step process for temporal anchoring: (1) represent temporal evidence; (2) select temporal information relevant to the relation; (3) decide how a relational fact and its relevant temporal information are themselves related; and (4) aggregate imprecise temporal intervals across multiple documents.", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7580370008945465}]}, {"text": "In contrast with previous approaches that aim at intra-document temporal information extraction (, we focus on mining a corpus aggregating temporal evidences across the supporting documents.", "labels": [], "entities": [{"text": "intra-document temporal information extraction", "start_pos": 49, "end_pos": 95, "type": "TASK", "confidence": 0.7031624913215637}]}, {"text": "We address the following research questions: (1) Validate whether distant supervised learning is suitable for the task, and evaluate its shortcomings.  performance.", "labels": [], "entities": []}, {"text": "The representation we use for temporal information is detailed in section 2; the rich document-level representation we exploit is described in section 3.", "labels": [], "entities": []}, {"text": "For a query entity and target relation, the system first performs relation extraction (section 4); then, we find and aggregate time constraint evidence for the same relation across different documents, to establish a temporal validity anchor interval (section 5).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7221656441688538}, {"text": "temporal validity anchor interval", "start_pos": 217, "end_pos": 250, "type": "METRIC", "confidence": 0.6593662053346634}]}, {"text": "Empirical comparative evaluation of our approach is introduced in section 6; while some related work is shown in section 7 and conclusions in section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have used for our evaluation the dataset compiled within the TAC-KBP 2011 Temporal Slot Filling Task).", "labels": [], "entities": [{"text": "TAC-KBP 2011 Temporal Slot Filling Task", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.7756864627202352}]}, {"text": "We employed as initial KB the one distributed to participants in the task, which has been compiled from Wikipedia infoboxes.", "labels": [], "entities": []}, {"text": "It contains 898 triples entity, slot type, value for 100 different entities and up to 8 different slots (relations) per entity . This gold standard contains the correct responses pooled from the participant systems plus a set of responses manually found by annotators.", "labels": [], "entities": []}, {"text": "Each triple has associated a temporal anchor.", "labels": [], "entities": []}, {"text": "The relations had to be extracted from a domain-general collection of 1.7 million documents.", "labels": [], "entities": []}, {"text": "Our system was one of the five that took part in the task.We have evaluated the overall system and the two main components of the architecture: Relation Extraction, and Temporal Anchoring of the relations.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.8530016243457794}]}, {"text": "Due to space limitations, the description of our implementation is very concise; refer to () for further details.", "labels": [], "entities": []}, {"text": "System response in the relation extraction step consists in a set of triples entity, slot type, value.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7732053995132446}]}, {"text": "Performance is measured using precision, recall and F-measure (harmonic mean) with respect to the 898 triples in the key.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9996721744537354}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9996908903121948}, {"text": "F-measure", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9988237023353577}, {"text": "harmonic mean)", "start_pos": 63, "end_pos": 77, "type": "METRIC", "confidence": 0.8205427130063375}]}, {"text": "Target relations (slots) are potentially list-valued, that is, more than one value can be valid fora relation (possibly at different points in time).", "labels": [], "entities": []}, {"text": "Only correct values yield any score, and redundant triples are ignored.", "labels": [], "entities": []}, {"text": "We run two different system settings for the relation extraction step.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.870801717042923}]}, {"text": "They differ in the document representation used (detailed in section3), in order to empirically assess whether clustering of discourse referents into single nodes benefits the extraction.", "labels": [], "entities": []}, {"text": "In SETTING 1, each document is represented as a document graph, G D , while in SETTING 2 collapsed document graph representation, G C , is employed.", "labels": [], "entities": [{"text": "SETTING 1", "start_pos": 3, "end_pos": 12, "type": "TASK", "confidence": 0.745495617389679}]}, {"text": "Results are shown in in the column Relation Extraction.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7459528148174286}]}, {"text": "Both settings have a similar performance with a slight increase in the case of graphs with clustered referents.", "labels": [], "entities": []}, {"text": "Although precision is close to 0.5, recall is lower than 0.1.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9997110962867737}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9998037219047546}]}, {"text": "We have studied the limits of the assumptions our approach is based on.", "labels": [], "entities": []}, {"text": "First, our standard retrieval component performance limits the overall system's.", "labels": [], "entities": []}, {"text": "As a matter of example, if we retrieve the first 100 documents per entity, we find relevant documents only for 62% of the triples in the key.", "labels": [], "entities": []}, {"text": "This number means that no matter how good relation extraction method is, 38% of relations will not be found.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8162478804588318}]}, {"text": "Second, the distant supervision assumption underlying our approach is that fora seed relation instance entity, relation, value, any textual mention of entity and value expresses the relation.", "labels": [], "entities": []}, {"text": "It has been shown that this assumption is more often violated when training knowledge base and document collection are of different type, e.g. Wikipedia and news-wire (.", "labels": [], "entities": []}, {"text": "We have realized that a more determinant factor is the relation itself and the type of arguments it takes.", "labels": [], "entities": []}, {"text": "We randomly sampled 100 training examples per relation, and manually inspected them to assess if they were indeed mentions of the relation.", "labels": [], "entities": []}, {"text": "While for the relation cities of residence only 30% of the training examples are expressing the relation, for spouse the number goes up to 59%.", "labels": [], "entities": []}, {"text": "For title, up to 90% of the examples are correct.", "labels": [], "entities": [{"text": "title", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.5683552622795105}]}, {"text": "This fact explains, at least partially, the zeros we obtain for some relations.", "labels": [], "entities": []}, {"text": "Under the evaluation metrics proposed by TAC-KBP 2011, if the value of the relation instance is judged as correct, the score for temporal anchoring depends on how well the returned interval matches the one provided in the key.", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.6513834297657013}]}, {"text": "More precisely, let the correct imprecise anchor interval in the gold standard key be S k = (k 1 , k 2 , k 3 , k 4 ) and the system response be S = (r 1 , r 2 , r 3 , r 4 ).", "labels": [], "entities": [{"text": "correct imprecise anchor interval", "start_pos": 24, "end_pos": 57, "type": "METRIC", "confidence": 0.703317366540432}]}, {"text": "The absence of a constraint int 1 or t 3 is treated as a value of \u2212\u221e; the absence of a constraint int 2 or t 4 is treated as a value of +\u221e.", "labels": [], "entities": []}, {"text": "Then, let d i = |k i \u2212 r i |, for i \u2208 1, . .", "labels": [], "entities": []}, {"text": ", 4, be the difference, areal number measured in years.", "labels": [], "entities": [{"text": "areal number measured", "start_pos": 24, "end_pos": 45, "type": "METRIC", "confidence": 0.9681825836499532}]}, {"text": "The score for the system response is: The score fora target relation Q(r) is computed by summing Q(S) overall unique instances of the relation whose value is correct.", "labels": [], "entities": []}, {"text": "If the gold standard contains N responses, and the system output M responses, then precision is: P = Q(r)/M , and recall: R = Q(r)/N ; F 1 is the harmonic mean of P and R.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9997331500053406}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9996320009231567}]}, {"text": "We evaluated two different settings for the temporal anchoring step; both use the collapsed document graph representation, G C (SETTING 2).", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7032003402709961}, {"text": "SETTING", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.7686195969581604}]}, {"text": "The goal of the experiment is twofold.", "labels": [], "entities": []}, {"text": "First, test the strength of the document creation time as evidence for temporal anchoring.", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7268552333116531}]}, {"text": "Second, test how hard this metadata-level baseline is to beat using contextual temporal expressions.", "labels": [], "entities": []}, {"text": "The SETTING 2-I assumes a within temporal link between the document creation time and any relation expressed inside the document, and aggregates this information across the documents that we have identified as supporting the relation.", "labels": [], "entities": [{"text": "SETTING", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.749841570854187}]}, {"text": "The SETTING 2-II considers documents content in order to extract temporal links from the context of the text that expresses the relation.", "labels": [], "entities": [{"text": "SETTING 2-II", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.764406681060791}]}, {"text": "If no temporal expression is found, the date of the document is used as default.", "labels": [], "entities": [{"text": "date", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.968471884727478}]}, {"text": "Temporal links from all supporting documents are mapped into intervals and aggregated as detailed in section 5.", "labels": [], "entities": []}, {"text": "The performance on relation extraction is an upper bound for temporal anchoring, attainable if temporal anchoring is perfect.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.9033802151679993}, {"text": "temporal anchoring", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.6692869365215302}]}, {"text": "Thus, we also evaluate the temporal anchoring performance as the percentage the final system achieves with respect to the relation extraction upper bound.", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6067246943712234}, {"text": "relation extraction", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7562382519245148}]}, {"text": "Results are shown in under column Temporal Anchoring.", "labels": [], "entities": [{"text": "Temporal", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7437185645103455}, {"text": "Anchoring", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.7549902200698853}]}, {"text": "They are low, due to the upper bound that error propagation in candidate retrieval and relation extraction imposes upon this step: temporally anchoring alone achives 69% of its upper bound.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.816521555185318}]}, {"text": "This value corresponds to the baseline SET-TING 2-I, showing its strength.", "labels": [], "entities": [{"text": "SET-TING 2-I", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9060288071632385}]}, {"text": "The difference with SETTING 2-II shows that this baseline is difficult to beat by considering temporal evidence inside the document content.", "labels": [], "entities": [{"text": "SETTING 2-II", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.689882218837738}]}, {"text": "There is a reason for this.", "labels": [], "entities": []}, {"text": "The temporal link mapping into time intervals does not depend only on the type of link, but also on the semantics of the text that expresses the relation as we pointed out above.", "labels": [], "entities": []}, {"text": "We have to decide how to transform the link between relation and temporal expression into a temporal interval.", "labels": [], "entities": []}, {"text": "Learning a model for this is a hard open research problem that has a strong adversary in the baseline proposed.: System ID, number of filled responses of the system, precision, recall and F measure.", "labels": [], "entities": [{"text": "ID", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.7247490882873535}, {"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9996746778488159}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9995954632759094}, {"text": "F measure", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9797073602676392}]}, {"text": "Our approach was compared with the other four participants at the KBP Temporal Slot Filling Task 2011.", "labels": [], "entities": [{"text": "KBP Temporal Slot Filling Task 2011", "start_pos": 66, "end_pos": 101, "type": "TASK", "confidence": 0.7890607118606567}]}, {"text": "shows results sorted by F-measure in comparison to our two settings (described above).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9961667656898499}]}, {"text": "These official results correspond to a previous dataset containing 712 triples 4 . As shown in column Filled our approach returns less triples than other systems, explaining low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9995005130767822}]}, {"text": "However, our system achieves the highest precision for the complete task of temporally anchored relation extraction.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9993021488189697}, {"text": "temporally anchored relation extraction", "start_pos": 76, "end_pos": 115, "type": "TASK", "confidence": 0.6051386222243309}]}, {"text": "Despite low recall, our system obtains the third best F 1 value.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9995977282524109}, {"text": "F 1 value", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.981114129225413}]}, {"text": "This is a very promising result, since several directions can be explored to consider more candidates and increase recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.99810791015625}]}], "tableCaptions": [{"text": " Table 3: Results of experiments for each relation: (1) per:stateorprovinces of residence; (2) per:employee of; (3)  per:countries of residence; (4) per:member of; (5) per:title; (6) org:top members/employees; (7) per:spouse; (8)  per:cities of residence; (9) overall results (calculated as a micro-average).", "labels": [], "entities": []}, {"text": " Table 4: System ID, number of filled responses of the  system, precision, recall and F measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9997764229774475}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9996588230133057}, {"text": "F measure", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9602589905261993}]}]}