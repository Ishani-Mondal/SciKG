{"title": [{"text": "Semi-supervised Dependency Parsing using Lexical Affinities", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7571209073066711}]}], "abstractContent": [{"text": "Treebanks are not large enough to reliably model precise lexical phenomena.", "labels": [], "entities": []}, {"text": "This deficiency provokes attachment errors in the parsers trained on such data.", "labels": [], "entities": []}, {"text": "We propose in this paper to compute lexical affinities, on large corpora, for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser.", "labels": [], "entities": []}, {"text": "Experiments on the French Treebank showed a relative decrease of the error rate of 7.1% Labeled Accuracy Score yielding the best parsing results on this treebank.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.9950917661190033}, {"text": "error rate", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9911629855632782}, {"text": "Labeled Accuracy Score", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.8423215746879578}]}], "introductionContent": [{"text": "Probabilistic parsers are usually trained on treebanks composed of few thousands sentences.", "labels": [], "entities": []}, {"text": "While this amount of data seems reasonable for learning syntactic phenomena and, to some extent, very frequent lexical phenomena involving closed parts of speech (POS), it proves inadequate when modeling lexical dependencies between open POS, such as nouns, verbs and adjectives.", "labels": [], "entities": []}, {"text": "This fact was first recognized by) who showed that bilexical dependencies were barely used in Michael Collins' parser.", "labels": [], "entities": []}, {"text": "The work reported in this paper aims at a better modeling of such phenomena by using a raw corpus that is several orders of magnitude larger than the treebank used for training the parser.", "labels": [], "entities": []}, {"text": "The raw corpus is first parsed and the computed lexical affinities between lemmas, in specific lexico-syntactic configurations, are then injected back in the parser.", "labels": [], "entities": []}, {"text": "Two outcomes are expected from this procedure, the first is, as mentioned above, a better modeling of bilexical dependencies and the second is a method to adapt a parser to new domains.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews some work on the same topic and highlights their differences with ours.", "labels": [], "entities": []}, {"text": "In section 3, we describe the parser that we use in our experiments and give a detailed description of the frequent attachment errors.", "labels": [], "entities": []}, {"text": "Section 4 describes how lexical affinities between lemmas are calculated and their impact is then evaluated with respect to the attachment errors made by the parser.", "labels": [], "entities": []}, {"text": "Section 5 describes three ways to integrate the lexical affinities in the parser and reports the results obtained with the three methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "The three methods described above have been evaluated on FTB TEST.", "labels": [], "entities": [{"text": "FTB TEST", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.7465014457702637}]}, {"text": "Results are reported in table 11.", "labels": [], "entities": []}, {"text": "The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method).", "labels": [], "entities": []}, {"text": "The best performances were obtained by the Double Parsing method that achieved a labeled relative error reduction of 7, 1% on predicted POS tags, yielding the best parsing results on the French Treebank.", "labels": [], "entities": [{"text": "labeled relative error reduction", "start_pos": 81, "end_pos": 113, "type": "METRIC", "confidence": 0.7715249732136726}, {"text": "French Treebank", "start_pos": 187, "end_pos": 202, "type": "DATASET", "confidence": 0.99415123462677}]}, {"text": "It performs better than the Post Processing method, which means that the second parsing stage corrects some inconsistencies introduced in the Post Processing method.: Parser accuracy on FTB TEST using the standard parser (BL) the post processing method (PP), the double parsing method (DP) and the feature based method.", "labels": [], "entities": [{"text": "Parser", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9800297617912292}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.939908504486084}, {"text": "FTB TEST", "start_pos": 186, "end_pos": 194, "type": "DATASET", "confidence": 0.8788444399833679}]}], "tableCaptions": [{"text": " Table 1: Size and decomposition of the French Treebank", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9689810872077942}, {"text": "French Treebank", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.986372321844101}]}, {"text": " Table 2: Labeled and unlabeled accuracy score for auto- matically predicted and gold POS tags with and without  taking into account punctuation on FTB TEST.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9705580472946167}, {"text": "FTB TEST", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.7411587238311768}]}, {"text": " Table 3: The 13 most common error types", "labels": [], "entities": []}, {"text": " Table 5: sizes of the corpora used to gather lexical counts", "labels": [], "entities": [{"text": "gather lexical counts", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.8440854946772257}]}, {"text": " Table 6: Coverage of the lexical resource over FTB DEV.", "labels": [], "entities": [{"text": "FTB DEV", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9430278837680817}]}, {"text": " Table 8: Correction Rate of the lexical resource with re- spect to FTB DEV.", "labels": [], "entities": [{"text": "Correction Rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9528014659881592}, {"text": "FTB DEV", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.8222606480121613}]}, {"text": " Table 9: Coverage and Correction Rate on FTB DEV for  several values of ambiguity threshold.", "labels": [], "entities": [{"text": "Correction Rate", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.9397349655628204}, {"text": "FTB DEV", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9461113512516022}]}, {"text": " Table 10: Correction Rate on FTB DEV when taking into  account parser confidence.", "labels": [], "entities": [{"text": "Correction Rate", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.9801861047744751}, {"text": "FTB DEV", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9120057225227356}]}, {"text": " Table 11: Parser accuracy on FTB TEST using the  standard parser (BL) the post processing method (PP),  the double parsing method (DP) and the feature based  method.", "labels": [], "entities": [{"text": "Parser", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.6536712646484375}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9174464344978333}, {"text": "FTB TEST", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.8457039594650269}]}]}