{"title": [{"text": "Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages", "labels": [], "entities": [{"text": "Machine Translation Between Closely-Related Languages", "start_pos": 52, "end_pos": 105, "type": "TASK", "confidence": 0.8594415068626404}]}], "abstractContent": [{"text": "We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.7360506653785706}]}, {"text": "We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based translit-eration at the word level and combine with a word-level translation model.", "labels": [], "entities": [{"text": "character-level translation", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.7048837840557098}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.96030193567276}]}, {"text": "The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9992884397506714}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems, require parallel corpora of sentences and their translations, called bitexts, which are often not sufficiently large.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7817756235599518}]}, {"text": "However, for many closely-related languages, SMT can be carried out even with small bitexts by exploring relations below the word level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9946604371070862}]}, {"text": "Closely-related languages such as Macedonian and Bulgarian exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities.", "labels": [], "entities": []}, {"text": "Spelling conventions in such related languages can still be different, and they may diverge more substantially at the level of morphology.", "labels": [], "entities": []}, {"text": "However, the differences often constitute consistent regularities that can be generalized when translating.", "labels": [], "entities": []}, {"text": "The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation) and transliteration.", "labels": [], "entities": [{"text": "translation", "start_pos": 165, "end_pos": 176, "type": "TASK", "confidence": 0.9664266109466553}]}, {"text": "Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9698581695556641}]}, {"text": "However, the strength of phrasebased SMT ( is that it can support rather large sequences (phrases) that capture translations of entire chunks.", "labels": [], "entities": [{"text": "phrasebased SMT", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.5466466844081879}]}, {"text": "This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration.", "labels": [], "entities": []}, {"text": "shows how character-level phrase tables can cover mappings spanning over multi-word units.", "labels": [], "entities": []}, {"text": "Thus, character-level phrase-based SMT models combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, as well as to various combinations thereof.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8273476958274841}]}], "datasetContent": [{"text": "For our experiments, we used translated movie subtitles from the OPUS corpus.", "labels": [], "entities": [{"text": "OPUS corpus", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.9634643495082855}]}, {"text": "For Macedonian-Bulgarian there were only about 102,000 aligned sentences containing approximately 1.3 million tokens altogether.", "labels": [], "entities": []}, {"text": "There was substantially more monolingual data available for Bulgarian: about 16 million sentences containing ca.", "labels": [], "entities": []}, {"text": "However, this data was noisy.", "labels": [], "entities": []}, {"text": "Thus, we realigned the corpus using hunalign and we removed some Bulgarian files that were misclassified as Macedonian and vice versa, using a BLEU-filter.", "labels": [], "entities": [{"text": "BLEU-filter", "start_pos": 143, "end_pos": 154, "type": "METRIC", "confidence": 0.9943665862083435}]}, {"text": "Furthermore, we also removed sentence pairs containing language-specific characters on the wrong side.", "labels": [], "entities": []}, {"text": "From the remaining data we selected 10,000 sentence pairs (roughly 128,000 words) for development and another 10,000 (ca.", "labels": [], "entities": []}, {"text": "125,000 words) for testing; we used the rest for training.", "labels": [], "entities": []}, {"text": "The evaluation results are summarized in: Macedonian-Bulgarian translation and transliteration.", "labels": [], "entities": [{"text": "Macedonian-Bulgarian translation", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.4961097240447998}]}, {"text": "Superscripts show the absolute improvement in BLEU compared to the word-level baseline (w1).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9991554021835327}]}, {"text": "The top rows of show the results for Macedonian-Bulgarian transliteration.", "labels": [], "entities": []}, {"text": "First, we can see that the BLEU score for the original Macedonian testset evaluated against the Bulgarian reference is 10.74, which is quite high and reflects the similarity between the two languages.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9776087999343872}]}, {"text": "The next line (t1) shows that many differences between Macedonian and Bulgarian stem from mere differences in orthography: we mapped the six letters in the Macedonian alphabet that do not exist in the Bulgarian alphabet to corresponding Bulgarian letters and letter sequences, gaining over 1.3 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 294, "end_pos": 298, "type": "METRIC", "confidence": 0.9993748068809509}]}, {"text": "The following line (t2) shows the results using the sophisticated transliteration described in Section 3, which takes two kinds of context into account: (1) wordinternal letter context, and (2) sentence-level word context.", "labels": [], "entities": []}, {"text": "We generated a lattice for each Macedonian test sentence, which included the original Macedonian words and the 1-best 2 Bulgarian transliteration option from the character-level transliteration model.", "labels": [], "entities": []}, {"text": "We then decoded the lattice using a Bulgarian language model; this increased BLEU to 22.74.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9996083378791809}]}, {"text": "Naturally, lattice-based transliteration cannot really compete against standard word-level translation (w1), which is better by 8 BLEU points.", "labels": [], "entities": [{"text": "word-level translation", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.6651447862386703}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9990338087081909}]}, {"text": "Still, as line (w2) shows, using the 1-best transliteration lattice as an input to (w1) yields 3 consistent improvement over (w1) for four evaluation metrics: BLEU (), NIST v.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9934699535369873}]}, {"text": "13, TER) v.", "labels": [], "entities": [{"text": "TER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6403132081031799}]}, {"text": "The baseline system is also significantly better than the on-line version of Apertium (http://www.apertium.org/), a shallow transfer-rulebased MT system that is optimized for closelyrelated languages (accessed on 2012/05/02).", "labels": [], "entities": [{"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.927518904209137}]}, {"text": "Here, Apertium suffers badly from a large number of unknown words in our testset (ca. 15%).", "labels": [], "entities": [{"text": "Apertium", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.8222348093986511}]}, {"text": "Moving down to the next group of experiments in, we can see that standard character-level SMT (c1), i.e., simply treating characters as separate words, performs significantly better than word-level SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.7879369854927063}, {"text": "SMT", "start_pos": 198, "end_pos": 201, "type": "TASK", "confidence": 0.6324812173843384}]}, {"text": "Using bigram-based character alignments yields further improvement of +0.43 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9954166412353516}]}, {"text": "Since word-level and character-level models have different strengths and weaknesses, we further tried to combine them.", "labels": [], "entities": []}, {"text": "We used MEMT, a state-of-the-art Multi-Engine Machine Translation system, to combine the outputs of (c3) with the output of (w1) and of (w2).", "labels": [], "entities": [{"text": "Multi-Engine Machine Translation", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6226025323073069}]}, {"text": "Both combinations improved over the individual systems, but (w1)+(c2) performed better, by +0.6 BLEU points over (c2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9993416666984558}]}, {"text": "Combining word-level and phrase-level SMT.", "labels": [], "entities": [{"text": "phrase-level SMT", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.498489111661911}]}, {"text": "Finally, we also combined (w1) with (c3) in a more direct way: by merging their phrase tables.", "labels": [], "entities": []}, {"text": "First, we split the phrases in the word-level phrase tables of (w1) to characters as in character-level models.", "labels": [], "entities": []}, {"text": "Then, we generated four versions of each phrase pair: with/without \" \" at the beginning/end of the phrase.", "labels": [], "entities": []}, {"text": "Finally, we merged these phrase pairs with those in the phrase table of (c3), adding two extra features indicating each phrase pair's origin: the first/second feature is 1 if the pair came from the first/second table, and 0.5 otherwise.", "labels": [], "entities": []}, {"text": "This combination outperformed MEMT, probably because it expands the search space of the SMT system more directly.", "labels": [], "entities": [{"text": "MEMT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.48883557319641113}, {"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9812017679214478}]}, {"text": "We further tried scoring with two language models in the process of translation, character-based and word-based, but we did not get consistent improvements.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9665606617927551}]}, {"text": "Finally, we experimented with a 1-best character-level lattice input that encodes the same options and weights as for (.", "labels": [], "entities": []}, {"text": "This yielded our best overall BLEU score of 33.94, which is +2.84 BLEU points of absolute improvement over the (w1) baseline, and +1.23 BLEU points over (c2).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9842254817485809}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9986494183540344}, {"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9977964162826538}]}], "tableCaptions": [{"text": " Table 2: Vocabulary size of character-level alignment  models and the corresponding word-level model.", "labels": [], "entities": []}, {"text": " Table 3: Macedonian-Bulgarian translation and  transliteration. Superscripts show the absolute improve- ment in BLEU compared to the word-level baseline (w1).", "labels": [], "entities": [{"text": "Macedonian-Bulgarian translation", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.5336885452270508}, {"text": "absolute improve- ment", "start_pos": 87, "end_pos": 109, "type": "METRIC", "confidence": 0.7847364246845245}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.714202344417572}]}]}