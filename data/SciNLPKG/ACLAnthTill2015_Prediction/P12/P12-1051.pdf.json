{"title": [{"text": "Semantic Parsing with Bayesian Tree Transducers", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7956869900226593}]}], "abstractContent": [{"text": "Many semantic parsing models use tree transformations to map between natural language and meaning representation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.7411142289638519}]}, {"text": "However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata.", "labels": [], "entities": []}, {"text": "This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7345700263977051}]}, {"text": "In particular , this paper further introduces a varia-tional Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 173, "end_pos": 189, "type": "TASK", "confidence": 0.7624346911907196}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping natural language sentences to a formal representation of meaning.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8263192772865295}]}, {"text": "Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences.", "labels": [], "entities": []}, {"text": "Most semantic parsing models rely on an assumption of structural similarity between MR and NL.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.793511837720871}]}, {"text": "Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations.", "labels": [], "entities": []}, {"text": "Several approaches assume a tree structure to the NL, MR, or both (Ge and), and often in- volve tree transformations either between two trees or a tree and a string.", "labels": [], "entities": [{"text": "MR", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.7880174517631531}]}, {"text": "The tree transducer, a formalism from automata theory which has seen interest in machine translation ( and has potential applications in many other areas, is well suited to formalizing such tree transformation based models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7862986624240875}]}, {"text": "Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments in one line of inquiry relate to others.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.715075209736824}]}, {"text": "We argue fora unifying theory of tree transformation based semantic parsing by presenting a tree transducer model and drawing connections to other similar systems.", "labels": [], "entities": [{"text": "tree transformation based semantic parsing", "start_pos": 33, "end_pos": 75, "type": "TASK", "confidence": 0.7423802971839905}]}, {"text": "We make a further contribution by bringing to tree transducers the benefits of the Bayesian framework for principled handling of data sparsity and prior knowledge.", "labels": [], "entities": []}, {"text": "present an EM training procedure for top down tree transducers, but while there are Bayesian approaches to string transducers ( and PCFGs (), there has yet to be a proposal for Bayesian inference in tree transducers.", "labels": [], "entities": []}, {"text": "Our variational algorithm produces better semantic parses than EM while remaining general to abroad class of transducers appropriate for other domains.", "labels": [], "entities": [{"text": "semantic parses", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.716618537902832}]}, {"text": "In short, our contributions are three-fold: we present anew state-of-the-art semantic parsing model, propose a broader theory for tree transformation based semantic parsing, and present a general inference algorithm for the tree transducer framework.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.7499681115150452}, {"text": "tree transformation based semantic parsing", "start_pos": 130, "end_pos": 172, "type": "TASK", "confidence": 0.6055311620235443}]}, {"text": "We recommend the last of these as just one benefit of working within a general theory: contributions are more broadly applicable.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the system on GeoQuery (), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish.", "labels": [], "entities": []}, {"text": "We present here additional translations of the full 880 sentences into German, Greek, and Thai.", "labels": [], "entities": []}, {"text": "For evaluation, following from, we reserve 280 sentences for test and train on the remaining 600.", "labels": [], "entities": []}, {"text": "During development, we use cross-validation on the 600 sentence training set.", "labels": [], "entities": []}, {"text": "At test, we run once on the remaining 280 and perform 10 fold cross-validation on the 250 sentence sets.", "labels": [], "entities": []}, {"text": "To judge correctness, we follow standard practice and submit each parse as a GeoQuery database query, and say the parse is correct only if the answer matches the gold standard.", "labels": [], "entities": []}, {"text": "We report raw accuracy (the percentage of sentences with correct answers), as well as F1: the harmonic mean of precision (the proportion of correct answers out of sentences with a parse) and recall (the proportion of correct answers out of all sentences).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8925002217292786}, {"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9997981190681458}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9961726069450378}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.999550998210907}]}, {"text": "We run three other state-of-the-art systems for comparison.", "labels": [], "entities": []}, {"text": "WASP () and the hybrid tree () are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S () as a nontree based top-performing system.", "labels": [], "entities": []}, {"text": "The hybrid tree is notable as the only other system based on a generative model, and uni-hybrid, aversion that uses a unigram distribution over words, is very similar to our own model.", "labels": [], "entities": []}, {"text": "We also report the best performing version, re-hybrid, which incorporates a discriminative re-ranking step.", "labels": [], "entities": []}, {"text": "We report transducer performance under three different training conditions: tsEM using EM, tsVBauto using VB with empirical Bayes, and tsVB-hand using hyper-parameters manually tuned on the German training data (\u03b1 of 0.3, 0.8, and 0.25 for MR rule, NL pattern, and word choices, respectively).", "labels": [], "entities": [{"text": "German training data", "start_pos": 190, "end_pos": 210, "type": "DATASET", "confidence": 0.6942977706591288}, {"text": "MR", "start_pos": 240, "end_pos": 242, "type": "METRIC", "confidence": 0.8492725491523743}]}, {"text": "shows results for 10 fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "The results highlight the benefit of the Dirichlet prior, whether manually or automatically set.", "labels": [], "entities": []}, {"text": "VB improves over EM considerably, most likely because (1) the handling of unknown words and MR entities allows it to return an analysis for all sentences, and (2) the sparse Dirichlet prior favors fewer rules, reasonable in this setting where only a few words are likely to share the same meaning.: Accuracy and F1 score comparisons on the geo600 training set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 299, "end_pos": 307, "type": "METRIC", "confidence": 0.9946104288101196}, {"text": "F1 score", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.9690811038017273}, {"text": "geo600 training set", "start_pos": 340, "end_pos": 359, "type": "DATASET", "confidence": 0.7651424805323283}]}, {"text": "Highest scores are in bold, while the highest among the tree based models are marked with a bullet.", "labels": [], "entities": []}, {"text": "The dotted line separates the tree based from non-tree based models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy and F1 score comparisons on the  geo600 training set. Highest scores are in bold, while  the highest among the tree based models are marked with  a bullet. The dotted line separates the tree based from  non-tree based models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983291029930115}, {"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9727331399917603}, {"text": "geo600 training set", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.7947275439898173}]}, {"text": " Table 2: Accuracy and F1 score comparisons on the  geo880 and geo250 test sets. Highest scores are in  bold, while the highest among the tree based models are  marked with a bullet. The dotted line separates the tree  based from non-tree based models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988468885421753}, {"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9759856164455414}, {"text": "geo250 test sets", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9145694573720297}]}]}