{"title": [{"text": "Deciphering Foreign Language by Combining Language Models and Context Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we show how to train statistical machine translation systems on real-life tasks using only non-parallel monolingual data from two languages.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6361272136370341}]}, {"text": "We present a modification of the method shown in (Ravi and Knight, 2011) that is scalable to vocabulary sizes of several thousand words.", "labels": [], "entities": []}, {"text": "On the task shown in (Ravi and Knight, 2011) we obtain better results with only 5% of the computational effort when running our method with an n-gram language model.", "labels": [], "entities": []}, {"text": "The efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of the VERBMOBIL corpus.", "labels": [], "entities": [{"text": "VERBMOBIL corpus", "start_pos": 153, "end_pos": 169, "type": "DATASET", "confidence": 0.9324510395526886}]}, {"text": "We also report results using data from the monolingual French and English GIGAWORD corpora.", "labels": [], "entities": [{"text": "GIGAWORD corpora", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.8570756018161774}]}], "introductionContent": [{"text": "It has long been a vision of science fiction writers and scientists to be able to universally communicate in all languages.", "labels": [], "entities": []}, {"text": "In these visions, even previously unknown languages can be learned automatically from analyzing foreign language input.", "labels": [], "entities": []}, {"text": "In this work, we attempt to learn statistical translation models from only monolingual data in the source and target language.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.5852266848087311}]}, {"text": "The reasoning behind this idea is that the elements of languages share statistical similarities that can be automatically identified and matched with other languages.", "labels": [], "entities": []}, {"text": "This work is a big step towards large-scale and large-vocabulary unsupervised training of statistical translation models.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.6176638305187225}]}, {"text": "Previous approaches have faced constraints in vocabulary or data size.", "labels": [], "entities": []}, {"text": "We show how * Author now at Google Inc., amauser@google.com.", "labels": [], "entities": []}, {"text": "to scale unsupervised training to real-life translation tasks and how large-scale experiments can be done.", "labels": [], "entities": []}, {"text": "Monolingual data is more readily available, if not abundant compared to true parallel or even just translated data.", "labels": [], "entities": []}, {"text": "Learning from only monolingual data in real-life translation tasks could improve especially low resource language pairs where few or no parallel texts are available.", "labels": [], "entities": []}, {"text": "In addition to that, this approach offers the opportunity to decipher new or unknown languages and derive translations based solely on the available monolingual data.", "labels": [], "entities": []}, {"text": "While we do tackle the full unsupervised learning task for MT, we make some very basic assumptions about the languages we are dealing with: 1.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9877815246582031}]}, {"text": "We have large amounts of data available in source and target language.", "labels": [], "entities": []}, {"text": "This is not a very strong assumption as books and text on the internet are readily available for almost all languages.", "labels": [], "entities": []}, {"text": "2. We can divide the given text in tokens and sentence-like units.", "labels": [], "entities": []}, {"text": "This implies that we know enough about the language to tokenize and sentence-split a given text.", "labels": [], "entities": [{"text": "tokenize", "start_pos": 55, "end_pos": 63, "type": "TASK", "confidence": 0.9644105434417725}]}, {"text": "Again, for the vast majority of languages, this is not a strong restriction.", "labels": [], "entities": []}, {"text": "3. The writing system is one-dimensional left-toright.", "labels": [], "entities": []}, {"text": "It has been shown () that the writing direction can be determined separately and therefore this assumption does not pose areal restriction.", "labels": [], "entities": []}, {"text": "Previous approaches to unsupervised training for SMT prove feasible only for vocabulary sizes up to around 500 words () and data sets of roughly 15,000 sentences containing only about 4 tokens per sentence on average.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.996541440486908}]}, {"text": "Real data as it occurs in texts such as web pages or news texts does not meet any of these characteristics.", "labels": [], "entities": []}, {"text": "In this work, we will develop, describe, and evaluate methods for large vocabulary unsupervised learning of machine translation models suitable for real-world tasks.", "labels": [], "entities": [{"text": "large vocabulary unsupervised learning of machine translation", "start_pos": 66, "end_pos": 127, "type": "TASK", "confidence": 0.6152096944195884}]}, {"text": "The remainder of this paper is structured as follows: In Section 2, we will review the related work and describe how our approach extends existing work.", "labels": [], "entities": []}, {"text": "Section 3 describes the model and training criterion used in this work.", "labels": [], "entities": []}, {"text": "The implementation and the training of this model is then described in Section 5 and experimentally evaluated in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on three different corpora.", "labels": [], "entities": []}, {"text": "At first we apply our method to non-parallel Spanish/English data that is based on the OPUS corpus ( and that was also used in ().", "labels": [], "entities": [{"text": "OPUS corpus", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.9623818397521973}]}, {"text": "We show that our method performs better by 1.6 BLEU than the best performing method described in ( being approximately 15 to 20 times faster than their n-gram based approach.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9990426898002625}]}, {"text": "After that we apply our method to a non-parallel version of the German/English VERBMOBIL corpus, which has a vocabulary size of 6,000 words on the German side, and 3,500 words on the target side and which thereby is approximately one order of magnitude larger than the previous OPUS experiment.", "labels": [], "entities": [{"text": "German/English VERBMOBIL corpus", "start_pos": 64, "end_pos": 95, "type": "DATASET", "confidence": 0.7361544609069824}]}, {"text": "We finally run our system on a subset of the nonparallel French/English GIGAWORD corpus, which has a vocabulary size of 60,000 words for both French and English.", "labels": [], "entities": [{"text": "French/English GIGAWORD corpus", "start_pos": 57, "end_pos": 87, "type": "DATASET", "confidence": 0.6000509142875672}]}, {"text": "We show first interesting results on such a big task.", "labels": [], "entities": []}, {"text": "In case of the OPUS and VERBMOBIL corpus, we evaluate the results using BLEU () and TER () to reference translations.", "labels": [], "entities": [{"text": "VERBMOBIL corpus", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.8202735185623169}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9989451766014099}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9949640035629272}]}, {"text": "We report all scores in percent.", "labels": [], "entities": []}, {"text": "For BLEU higher values are better, for TER lower values are better.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9972599744796753}, {"text": "TER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.995641827583313}]}, {"text": "We also compare the results on these corpora to a system trained on parallel data.", "labels": [], "entities": []}, {"text": "In case of the GIGAWORD corpus we show lexicon entries obtained during training.", "labels": [], "entities": [{"text": "GIGAWORD corpus", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.9136842787265778}]}, {"text": "This setup is based on a subset of the monolingual GIGAWORD corpus.", "labels": [], "entities": [{"text": "GIGAWORD corpus", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9087486565113068}]}, {"text": "We selected 100,000 French sentences from the news agency AFP and 100,000 sentences from the news agency Xinhua.", "labels": [], "entities": [{"text": "AFP", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9432387351989746}]}, {"text": "To have a more reliable set of training instances, we selected only sentences with more than 7 tokens.", "labels": [], "entities": []}, {"text": "Note that these corpora form true non-parallel data which, besides the length filtering, were not specifically preselected or pre-processed.", "labels": [], "entities": []}, {"text": "More details on these non-parallel corpora are summarized in.", "labels": [], "entities": []}, {"text": "The vocabularies have a size of approximately 60,000 words which is more than 100 times larger than the vocabularies of the OPUS corpus.", "labels": [], "entities": [{"text": "OPUS corpus", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.9539967477321625}]}, {"text": "Also it incorporates more than 25 times as many tokens as the OPUS corpus.", "labels": [], "entities": [{"text": "OPUS corpus", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.9366137385368347}]}, {"text": "After initialization, we run our method with NC = 150 candidates per source word for 20 EM iterations using a 2-gram LM.", "labels": [], "entities": [{"text": "NC", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9509085416793823}]}, {"text": "After the first context vector step with NC = 50 we run another 4 \u00d7 20 iterations with NC = 50 with a 2-gram LM.", "labels": [], "entities": []}, {"text": "shows example lexicon entries we obtained.", "labels": [], "entities": []}, {"text": "Note that we obtained these results by using purely non-parallel data, and that we neither used a seed lexicon, nor orthographic features to assign e.g. numbers or proper names: All results are obtained using 2-gram statistics and the context of words only.", "labels": [], "entities": []}, {"text": "We find the results encouraging and think that they show the potential of large-scale unsupervised techniques for MT in the future.", "labels": [], "entities": [{"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.99741131067276}]}], "tableCaptions": [{"text": " Table 1: Statistics of the corpora used in this paper.", "labels": [], "entities": []}, {"text": " Table 2: Results obtained on the OPUS corpus.", "labels": [], "entities": [{"text": "OPUS corpus", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9487802982330322}]}, {"text": " Table 3. Even on this more complex  task our method achieves encouraging results: The", "labels": [], "entities": []}, {"text": " Table 3: Results obtained on the VERBMOBIL corpus.", "labels": [], "entities": [{"text": "VERBMOBIL corpus", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9455754458904266}]}, {"text": " Table 4: Lexicon entries obtained by running our method on the non-parallel GIGAWORD corpus. The first column  shows in which iteration the algorithm found the first correct translations f (compared to a parallely trained lexicon)  among the top 5 candidates", "labels": [], "entities": [{"text": "GIGAWORD corpus", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.905062347650528}]}]}