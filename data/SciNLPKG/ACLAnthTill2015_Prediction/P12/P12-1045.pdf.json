{"title": [{"text": "Fast Online Lexicon Learning for Grounded Language Acquisition", "labels": [], "entities": [{"text": "Grounded Language Acquisition", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7366456190745035}]}], "abstractContent": [{"text": "Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language.", "labels": [], "entities": []}, {"text": "It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context.", "labels": [], "entities": [{"text": "language grounding", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.755222886800766}]}, {"text": "Recent work by Chen and Mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs.", "labels": [], "entities": []}, {"text": "While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets.", "labels": [], "entities": [{"text": "learning to interpret navigation instructions", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.6145838081836701}]}, {"text": "In this paper we introduce anew online algorithm that is an order of magnitude faster and surpasses the state-of-the-art results.", "labels": [], "entities": []}, {"text": "We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon's Mechanical Turk we can further improve the results.", "labels": [], "entities": []}, {"text": "We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to understand the semantics of human languages has been one of the ultimate goals of natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.822415272394816}]}, {"text": "Traditional learning approaches have relied on access to parallel corpora of natural language sentences paired with their meanings.", "labels": [], "entities": []}, {"text": "However, constructing such semantic annotations can be difficult and time-consuming.", "labels": [], "entities": []}, {"text": "More recently, there has been work on learning from ambiguous supervision where a set of potential sentence meanings are given, only one (or a small subset) of which are correct.", "labels": [], "entities": []}, {"text": "Given the training data, the system needs to infer the correcting meaning for each training sentence.", "labels": [], "entities": []}, {"text": "Building a lexicon of the formal meaning representations of words and phrases, either implicitly or explicitly, is usually an important step in inferring the meanings of entire sentences.", "labels": [], "entities": []}, {"text": "In particular, first learned a lexicon to help them resolve ambiguous supervision of relational data in which the number of choices is exponential.", "labels": [], "entities": []}, {"text": "They represent the perceptual context as a graph and allow each sentence in the training data to align to any connected subgraph.", "labels": [], "entities": []}, {"text": "Their lexicon learning algorithm finds the common connected subgraph that occurs with a word by taking intersections of the graphs that represent the different contexts in which the word appears.", "labels": [], "entities": []}, {"text": "While the algorithm produced a good lexicon for their application of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets.", "labels": [], "entities": [{"text": "interpret navigation instructions", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.7384888927141825}]}, {"text": "In this paper we introduce a novel online algorithm that is an order of magnitude faster and also produces better results on their navigation task.", "labels": [], "entities": []}, {"text": "In addition to the new lexicon learning algorithm, we also look at modifying the meaning representation grammar (MRG) for their formal semantic language.", "labels": [], "entities": [{"text": "meaning representation grammar (MRG)", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.7500718335310618}]}, {"text": "By using a MRG that correlates better to the structure of natural language, we further improve the performance on the navigation task.", "labels": [], "entities": [{"text": "navigation task", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.9168076515197754}]}, {"text": "Since our al-gorithm can scale to larger datasets, we present results on collecting and training on additional data from Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 121, "end_pos": 145, "type": "DATASET", "confidence": 0.9444468021392822}]}, {"text": "Finally, we show the generality of our approach by demonstrating our system's ability to learn from a Chinese translation of the training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our new lexicon learning algorithm as well as the other modifications to the navigation system using the same three tasks as.", "labels": [], "entities": []}, {"text": "The first task is disambiguating the training data by inferring the correct navigation plans associated with each training sentence.", "labels": [], "entities": []}, {"text": "The second task is evaluating the performance of the semantic parsers trained on the disambiguated data.", "labels": [], "entities": []}, {"text": "We measure the performance of both of these tasks by comparing to gold-standard data using the same partial correctness metric used by Chen and Mooney which gives credit to a parse for producing the correct action type and additional credit if the arguments were also correct.", "labels": [], "entities": []}, {"text": "Finally, the third task is to complete the end-to-end navigation task.", "labels": [], "entities": [{"text": "end-to-end navigation task", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6303823093573252}]}, {"text": "There are two versions of this task, the complete task uses the original instructions which are several sentences long and the other version uses instructions that have been manually split into single sentences.", "labels": [], "entities": []}, {"text": "Task completion is measured by the percentage of trials in which the system reached the correct destination (and orientation in the single-sentence version).", "labels": [], "entities": []}, {"text": "We follow the same evaluation scheme as Chen and Mooney and perform leave-one-map-out experiments.", "labels": [], "entities": []}, {"text": "For the first task, we build a lexicon using ambiguous training data from two maps, and then use the lexicon to produce the best disambiguated semantic meanings for those same data.", "labels": [], "entities": []}, {"text": "For the second and third tasks, we train a semantic parser on the automatically disambiguated data, and test on sentences from the third, unseen map.", "labels": [], "entities": []}, {"text": "For all comparisons to the Chen and Mooney results, we use the performance of their refined landmarks plans system which performed the best overall.", "labels": [], "entities": []}, {"text": "Moreover, it provides the most direct comparison to our approach since both use a lexicon to refine the landmarks plans.", "labels": [], "entities": []}, {"text": "Other than the modifications discussed, we use the same components as their system including using KRISP to train the semantic parsers and using the execution module from to carryout the navigation plans.", "labels": [], "entities": []}, {"text": "In addition to evaluating the system on English data, we also translated the corpus used by Chen and Mooney into Mandarin Chinese.", "labels": [], "entities": []}, {"text": "1 To run our sys-tem, we first segmented the sentences using the Stanford Chinese Word Segmenter (.", "labels": [], "entities": [{"text": "Stanford Chinese Word Segmenter", "start_pos": 65, "end_pos": 96, "type": "DATASET", "confidence": 0.8616340905427933}]}, {"text": "We evaluated using the same three tasks as before.", "labels": [], "entities": []}, {"text": "This resulted in a precision, recall, and F1 of 87.07, 71.67, and 78.61, respectively for the inferred plans.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9998629093170166}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.999601423740387}, {"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9999024868011475}]}, {"text": "The trained semantic parser's precision, recall, and F1 were 88.87, 58.76, and 70.74, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9998072981834412}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9996147155761719}, {"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9998753070831299}]}, {"text": "Finally, the system completed 58.70% of the single-sentence task and 20.13% of the complete task.", "labels": [], "entities": []}, {"text": "All of these numbers are very similar to the English results, showing the generality of the system in its ability to learn other languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the navigation instruction cor- pora. The average statistics for each instruction are  shown with standard deviations in parentheses.", "labels": [], "entities": [{"text": "navigation instruction cor- pora", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.7953617811203003}]}, {"text": " Table 1. Overall, the new corpus has a  slightly smaller vocabulary, and each instruction is  slightly shorter both in terms of the number of words  and the number of actions.", "labels": [], "entities": []}, {"text": " Table 2: Partial parse accuracy of how well each algo- rithm can infer the gold-standard navigation plans.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.993452787399292}]}, {"text": " Table 3: Partial parse accuracy of the semantic parsers  trained on the disambiguated navigation plans.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9937469959259033}]}, {"text": " Table 4: End-to-end navigation task completion rates.", "labels": [], "entities": [{"text": "navigation task completion", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8001829783121744}]}, {"text": " Table 5: The time (in seconds) it took to build the lexicon.", "labels": [], "entities": []}]}