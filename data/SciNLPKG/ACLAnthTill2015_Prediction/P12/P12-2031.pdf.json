{"title": [], "abstractContent": [{"text": "The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources.", "labels": [], "entities": []}, {"text": "However , evaluating such resources has turned out to be a non-trivial task, slowing progress in the field.", "labels": [], "entities": []}, {"text": "In this paper, we suggest a framework for evaluating inference-rule resources.", "labels": [], "entities": []}, {"text": "Our framework simplifies a previously proposed \"instance-based evaluation\" method that involved substantial annotator training, making it suitable for crowdsourcing.", "labels": [], "entities": []}, {"text": "We show that our method produces a large amount of annotations with high inter-annotator agreement fora low cost at a short period of time, without requiring training expert annotators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inference rules are an important component in semantic applications, such as Question Answering (QA)) and Information Extraction (IE)), describing a directional inference relation between two text patterns with variables.", "labels": [], "entities": [{"text": "Question Answering (QA))", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.8291648924350739}, {"text": "Information Extraction (IE))", "start_pos": 106, "end_pos": 134, "type": "TASK", "confidence": 0.8378068208694458}]}, {"text": "For example, to answer the question 'Where was Reagan raised?'", "labels": [], "entities": [{"text": "answer the question 'Where was Reagan raised?'", "start_pos": 16, "end_pos": 62, "type": "TASK", "confidence": 0.659448891878128}]}, {"text": "a QA system can use the rule 'X brought up in Y\u2192X raised in Y' to extract the answer from 'Reagan was brought up in Dixon'.", "labels": [], "entities": [{"text": "Reagan was brought up in Dixon", "start_pos": 91, "end_pos": 121, "type": "DATASET", "confidence": 0.8920016984144846}]}, {"text": "Similarly, an IE system can use the rule 'X work as Y\u2192X hired as Y' to extract the PERSON and ROLE entities in the \"hiring\" event from 'Bob worked as an analyst for Dell'.", "labels": [], "entities": [{"text": "ROLE", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.830322265625}]}, {"text": "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (, and generate knowledge resources for inference systems.", "labels": [], "entities": []}, {"text": "However, despite their potential, utilization of inference rule resources is currently somewhat limited.", "labels": [], "entities": []}, {"text": "This is largely due to the fact that these algorithms often produce invalid rules.", "labels": [], "entities": []}, {"text": "Thus, evaluation is necessary both for resource developers as well as for inference system developers who want to asses the quality of each resource.", "labels": [], "entities": []}, {"text": "Unfortunately, as evaluating inference rules is hard and costly, there is no clear evaluation standard, and this has become a slowing factor for progress in the field.", "labels": [], "entities": []}, {"text": "One option for evaluating inference rule resources is to measure their impact on an end task, as that is what ultimately interests an inference system developer.", "labels": [], "entities": []}, {"text": "However, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource.", "labels": [], "entities": []}, {"text": "An example is the Recognizing Textual Entailment (RTE) framework, in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7180696030457815}]}, {"text": "This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources' impact can vary considerably from one system to another.", "labels": [], "entities": [{"text": "RTE challenges", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.9109027087688446}]}, {"text": "These issues have also been noted by and.", "labels": [], "entities": []}, {"text": "A complementary application-independent evaluation method is hence necessary.", "labels": [], "entities": []}, {"text": "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule ().", "labels": [], "entities": []}, {"text": "However, observed that directly judging rules out of context often results in low inter-annotator agreement.", "labels": [], "entities": []}, {"text": "To remedy that, and proposed \"instance-based evaluation\", in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference.", "labels": [], "entities": []}, {"text": "This simulates the utility of rules in an application and yields high inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly.", "labels": [], "entities": []}, {"text": "Thus, a simple, robust and replicable evaluation method is needed.", "labels": [], "entities": []}, {"text": "Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF) have been employed for semantic inference annotation ().", "labels": [], "entities": [{"text": "semantic inference annotation", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.759006122748057}]}, {"text": "These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules.", "labels": [], "entities": [{"text": "RTE text-hypothesis pairs", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.8294382095336914}]}, {"text": "In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing.", "labels": [], "entities": []}, {"text": "Our method substantially simplifies annotation of rule applications and avoids annotator training completely.", "labels": [], "entities": []}, {"text": "The novelty in our framework is two-fold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions.", "labels": [], "entities": []}, {"text": "We apply methodological principles that efficiently communicate the definition of the \"inference\" relation to untrained crowdsourcing workers (Turkers).", "labels": [], "entities": []}, {"text": "As a case study, we applied our method to evaluate algorithms for learning inference rules between predicates.", "labels": [], "entities": []}, {"text": "We show that we can produce many annotations cheaply, quickly, at good quality, while achieving high inter-annotator agreement.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}