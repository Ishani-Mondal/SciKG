{"title": [{"text": "PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning", "labels": [], "entities": [{"text": "PORT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8606563806533813}, {"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9135704040527344}, {"text": "Tuning", "start_pos": 56, "end_pos": 62, "type": "TASK", "confidence": 0.8972954750061035}]}], "abstractContent": [{"text": "Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 5, "end_pos": 40, "type": "TASK", "confidence": 0.8522702306509018}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9959638118743896}]}, {"text": "In principle, tuning on these metrics should yield better systems than tuning on BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.984546422958374}]}, {"text": "However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning.", "labels": [], "entities": []}, {"text": "This paper presents PORT 1 , anew MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems.", "labels": [], "entities": [{"text": "PORT 1", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9492607116699219}, {"text": "MT evaluation", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9025980830192566}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9990620017051697}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9984068274497986}, {"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.8986495733261108}]}, {"text": "PORT does not require external resources and is quick to compute.", "labels": [], "entities": [{"text": "PORT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5475727319717407}]}, {"text": "It has a better correlation with human judgment than BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9957448840141296}]}, {"text": "We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.7315727472305298}, {"text": "BLEU-tuned", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9878755211830139}]}, {"text": "PORT tuning achieves consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9787877202033997}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9986850619316101}, {"text": "BLEU", "start_pos": 276, "end_pos": 280, "type": "METRIC", "confidence": 0.9888211488723755}]}], "introductionContent": [{"text": "Automatic evaluation metrics for machine translation (MT) quality area key part of building statistical MT (SMT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8420213758945465}, {"text": "MT (SMT)", "start_pos": 104, "end_pos": 112, "type": "TASK", "confidence": 0.8165310174226761}]}, {"text": "They play two PORT: Precision-Order-Recall Tunable metric.", "labels": [], "entities": [{"text": "PORT", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9789907932281494}]}, {"text": "roles: to allow rapid (though sometimes inaccurate) comparisons between different systems or between different versions of the same system, and to perform tuning of parameter values during system training.", "labels": [], "entities": []}, {"text": "The latter has become important since the invention of minimum error rate training (MERT) and related tuning methods.", "labels": [], "entities": [{"text": "minimum error rate training (MERT", "start_pos": 55, "end_pos": 88, "type": "METRIC", "confidence": 0.7708837489287058}]}, {"text": "These methods perform repeated decoding runs with different system parameter values, which are tuned to optimize the value of the evaluation metric over a development set with reference translations.", "labels": [], "entities": []}, {"text": "MT evaluation metrics fall into three groups: \u2022 BLEU (), NIST), WER, PER, TER), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9035543203353882}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9987848401069641}, {"text": "WER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9922502636909485}, {"text": "PER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8442028164863586}, {"text": "TER", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.756926953792572}, {"text": "LRscore", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9093074798583984}]}, {"text": "\u2022 METEOR (), METEOR-NEXT (, TER-Plus (,), TESLA (, AMBER (Chen and Kuhn, 2011) and MTeRater) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.8514187335968018}, {"text": "AMBER", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9016401171684265}, {"text": "MTeRater", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.8411325216293335}, {"text": "part-of-speech tagging", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.7008877247571945}]}, {"text": "\u2022 More sophisticated metrics such as RTE (, DCU-LFG ( and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations.", "labels": [], "entities": [{"text": "RTE", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.5733509063720703}, {"text": "DCU-LFG", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9121323823928833}, {"text": "MEANT", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9753909707069397}]}, {"text": "Among these metrics, BLEU is the most widely used for both evaluation and tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.998126208782196}]}, {"text": "Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9975032210350037}, {"text": "WMT Evaluation Task", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.6910483042399088}]}, {"text": "However, BLEU remains the de facto standard tuning metric, for two reasons.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9983078241348267}]}, {"text": "First, there is no evidence that any other tuning metric yields better MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.989680826663971}]}, {"text": "showed that BLEU tuning is more robust than tuning with other metrics (METEOR, TER, etc.), as gauged by both automatic and human evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.997620165348053}, {"text": "METEOR", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9734829664230347}, {"text": "TER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9942048192024231}]}, {"text": "Second, though a tuning metric should correlate strongly with human judgment, MERT (and similar algorithms) invoke the chosen metric so often that it must be computed quickly.", "labels": [], "entities": []}, {"text": "claimed that TESLA tuning performed better than BLEU tuning according to human judgment.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.8470360636711121}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9975816011428833}]}, {"text": "However, in the WMT 2011 \"tunable metrics\" shared pilot task, this did not hold).", "labels": [], "entities": [{"text": "WMT 2011 \"tunable metrics\" shared pilot task", "start_pos": 16, "end_pos": 60, "type": "DATASET", "confidence": 0.7917738291952345}]}, {"text": "In (Birch and Osborne, 2011), humans preferred the output from LRscore-tuned systems 52.5% of the time, versus BLEU-tuned system outputs 43.9% of the time.", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9841289520263672}]}, {"text": "In this work, our goal is to devise a metric that, like BLEU, is computationally cheap and language-independent, but that yields better MT systems than BLEU when used for tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9560856819152832}, {"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.9689898490905762}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9431834816932678}]}, {"text": "We tried out different combinations of statistics before settling on the final definition of our metric.", "labels": [], "entities": []}, {"text": "The final version, PORT, combines precision, recall, strict brevity penalty and strict redundancy penalty () in a quadratic mean expression.", "labels": [], "entities": [{"text": "PORT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9836940169334412}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9994441866874695}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9991797804832458}]}, {"text": "This expression is then further combined with anew measure of word ordering, v, designed to reflect long-distance as well as short-distance word reordering (BLEU only reflects short-distance reordering).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.996755063533783}]}, {"text": "Ina later section, 3.3, we describe experiments that vary parts of the definition of PORT.", "labels": [], "entities": [{"text": "PORT", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.5678274631500244}]}, {"text": "Results given below show that PORT correlates better with human judgments of translation quality than BLEU does, and sometimes outperforms METEOR in this respect, based on data from WMT.", "labels": [], "entities": [{"text": "PORT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9837769269943237}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9969621300697327}, {"text": "METEOR", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.8388729095458984}, {"text": "WMT", "start_pos": 182, "end_pos": 185, "type": "DATASET", "confidence": 0.9498404264450073}]}, {"text": "However, since PORT is designed for tuning, the most important results are those showing that PORT tuning yields systems with better translations than those produced by BLEU tuning -both as determined by automatic metrics (including BLEU), and according to human judgment, as applied to five data conditions involving four language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9597761631011963}, {"text": "BLEU", "start_pos": 233, "end_pos": 237, "type": "METRIC", "confidence": 0.9906816482543945}]}], "datasetContent": [{"text": "We studied PORT as an evaluation metric on WMT data; test sets include English-to-all submissions.", "labels": [], "entities": [{"text": "PORT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.7922745943069458}, {"text": "WMT data", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8261369466781616}]}, {"text": "The languages \"all\" (\"xx\" in) include French, Spanish, German and Czech.", "labels": [], "entities": []}, {"text": "summarizes the test set statistics.", "labels": [], "entities": []}, {"text": "In order to compute the v part of PORT, we require source-target word alignments for the references and MT outputs.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 104, "end_pos": 114, "type": "TASK", "confidence": 0.8079881072044373}]}, {"text": "These aren't included in WMT data, so we compute them with GIZA++.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.7891713380813599}]}, {"text": "We used Spearman's rank correlation coefficient \u03c1 to measure correlation of the metric with systemlevel human judgments of translation.", "labels": [], "entities": [{"text": "rank correlation coefficient \u03c1", "start_pos": 19, "end_pos": 49, "type": "METRIC", "confidence": 0.7775166258215904}]}, {"text": "The human judgment score is based on the \"Rank\" only, i.e., how often the translations of the system were rated as better than those from other systems).", "labels": [], "entities": []}, {"text": "Thus, BLEU, METEOR, and PORT were evaluated on how well their rankings correlated with the human ones.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.999022364616394}, {"text": "METEOR", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9727034568786621}, {"text": "PORT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.996126115322113}]}, {"text": "For the segment level, we follow) in using Kendall's rank correlation coefficient \u03c4.", "labels": [], "entities": [{"text": "Kendall's rank correlation coefficient \u03c4", "start_pos": 43, "end_pos": 83, "type": "METRIC", "confidence": 0.6670469840367635}]}, {"text": "As shown in, we compared PORT with smoothed BLEU (mteval-v13a), and METEOR v1.0.", "labels": [], "entities": [{"text": "PORT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9409565925598145}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9909350275993347}, {"text": "METEOR", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9624967575073242}]}, {"text": "Both BLEU and PORT perform matching of n-grams up ton = 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9986900687217712}, {"text": "PORT", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9913604855537415}]}, {"text": "The first set of experiments to study PORT as a tuning metric involved Chinese-to-English (zh-en); there were two data conditions.", "labels": [], "entities": [{"text": "PORT", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.8169273734092712}]}, {"text": "The first is the small data condition where FBIS 2 is used to train the translation and reordering models.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.5411379337310791}, {"text": "translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9681955575942993}]}, {"text": "It contains 10.5M target word tokens.", "labels": [], "entities": []}, {"text": "We trained two language models (LMs), which were combined loglinearly.", "labels": [], "entities": []}, {"text": "The first is a 4-gram LM which is estimated on the target side of the texts used in the large data condition (below).", "labels": [], "entities": []}, {"text": "The second is a 5-gram LM estimated on English Gigaword.", "labels": [], "entities": [{"text": "LM", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9371094107627869}, {"text": "English Gigaword", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.8475171327590942}]}, {"text": "The large data condition uses training data from NIST 3 2009 (Chinese-English track).", "labels": [], "entities": [{"text": "NIST 3 2009", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9544600248336792}]}, {"text": "All allowed bilingual corpora except UN, Hong Kong Laws and Hong Kong Hansard were used to train the translation model and reordering models.", "labels": [], "entities": [{"text": "UN", "start_pos": 37, "end_pos": 39, "type": "DATASET", "confidence": 0.9412367939949036}, {"text": "Hong Kong Laws", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.8508308331171671}, {"text": "Hong Kong Hansard", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.8845143715540568}, {"text": "translation", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.9658221006393433}]}, {"text": "There are about 62.6M target word tokens.", "labels": [], "entities": []}, {"text": "The same two LMs are used for large data as for small data, and the same development (\"dev\") and test sets are also used.", "labels": [], "entities": []}, {"text": "The dev set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST.", "labels": [], "entities": [{"text": "NIST 2005 test set", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.9790158867835999}, {"text": "NIST", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.9721388220787048}]}, {"text": "Evaluation was performed on NIST 2006 and 2008.", "labels": [], "entities": [{"text": "NIST 2006 and 2008", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9748300462961197}]}, {"text": "Four references were provided for all dev and test sets.", "labels": [], "entities": []}, {"text": "The third data condition is a French-to-English (fr-en).", "labels": [], "entities": []}, {"text": "The parallel training data is from Canadian Hansard data, containing 59.3M word tokens.", "labels": [], "entities": [{"text": "Canadian Hansard data", "start_pos": 35, "end_pos": 56, "type": "DATASET", "confidence": 0.8078484733899435}]}, {"text": "We used two LMs in loglinear combination: a 4-gram LM trained on the target side of the parallel training data, and the English Gigaword 5-gram LM.", "labels": [], "entities": []}, {"text": "The dev set has 1992 sentences; the two test sets have 2140 and 2164 sentences respectively.", "labels": [], "entities": []}, {"text": "There is one reference for all dev and test sets.", "labels": [], "entities": []}, {"text": "The fourth and fifth conditions involve German--English Europarl data.", "labels": [], "entities": [{"text": "German--English Europarl data", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.5667964339256286}]}, {"text": "This parallel corpus contains 48.5M German tokens and 50.8M English tokens.", "labels": [], "entities": []}, {"text": "We translate both German-to-English (deen) and English-to-German (en-de).", "labels": [], "entities": []}, {"text": "The two conditions both use an LM trained on the target side of the parallel training data, and de-en also uses the English Gigaword 5-gram LM.", "labels": [], "entities": []}, {"text": "News test 2008 set is used as dev set; News test  All experiments were carried outwith \u03b1 in Eq.", "labels": [], "entities": [{"text": "News test 2008 set", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9586897641420364}, {"text": "News", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8754307627677917}]}, {"text": "(17) set to 0.25, and involved only lowercase European-language text.", "labels": [], "entities": []}, {"text": "They were performed with MOSES (, whose decoder includes lexicalized reordering, translation models, language models, and word and phrase penalties.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.7245326042175293}]}, {"text": "Tuning was done with n-best MERT, which is available in MOSES.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9585628509521484}, {"text": "MERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9585666060447693}, {"text": "MOSES", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9453620910644531}]}, {"text": "In all tuning experiments, both BLEU and PORT performed lowercase matching of n-grams up ton = 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9976229071617126}, {"text": "PORT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9698187112808228}]}, {"text": "We also conducted experiments with tuning on aversion of BLEU that incorporates SBP as a baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9908429384231567}]}, {"text": "The results of original IBM BLEU and BLEU with SBP were tied; to save space, we only report results for original IBM BLEU here.", "labels": [], "entities": [{"text": "IBM", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.6715261936187744}, {"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.842549741268158}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.995435893535614}]}, {"text": "We conducted a human evaluation on outputs from BLEU-and PORT-tuned systems.", "labels": [], "entities": [{"text": "BLEU-and", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9908269047737122}]}, {"text": "The examples are randomly picked from all \"to-English\" conditions shown ine., all conditions except English-to-German).", "labels": [], "entities": []}, {"text": "We performed pairwise comparison of the translations produced by the system types as in).", "labels": [], "entities": []}, {"text": "First, we eliminated examples where the reference had fewer than 10 words or more than 50 words, or where outputs of the BLEU-tuned and PORT-tuned systems were identical.", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9924396276473999}]}, {"text": "The evaluators (colleagues not involved with this paper) objected to comparing two bad translations, so we then selected for human evaluation only translations that had high sentence-level (1-TER) scores.", "labels": [], "entities": []}, {"text": "To be fair to both metrics, for each condition, we took the union of examples whose BLEU-tuned output was in the top n% of BLEU outputs and those whose PORT-tuned output was in the top n% of PORT outputs (based on (1-TER)).", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9878859519958496}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9038605690002441}]}, {"text": "The value of n varied by condition: we chose the top 20% of zh-en small, top 20% of ende, top 50% of fr-en and top 40% of zh-en large.", "labels": [], "entities": []}, {"text": "We then randomly picked 450 of these examples to form the manual evaluation set.", "labels": [], "entities": []}, {"text": "This set was split into 15 subsets, each containing 30 sentences.", "labels": [], "entities": []}, {"text": "The first subset was used as a common set; each of the other 14 subsets was put in a separate file, to which the common set is added.", "labels": [], "entities": []}, {"text": "Each of the 14 evaluators received one of these files, containing 60 examples (30 unique examples and 30 examples shared with the other evaluators).", "labels": [], "entities": []}, {"text": "Within each example, BLEU-tuned and PORT-tuned outputs were presented in random order.", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9982829093933105}, {"text": "PORT-tuned", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9790656566619873}]}, {"text": "After receiving the 14 annotated files, we computed Fleiss's Kappa on the common set to measure inter-annotator agreement, all \u03ba . Then, we excluded annotators one at a time to compute i \u03ba (Kappa score without i-th annotator, i.e., from the other 13).", "labels": [], "entities": []}, {"text": "Finally, we filtered out the files from the 4 annotators whose answers were most different from everybody else's: i.e., annotators with the biggest This left 10 files from 10 evaluators.", "labels": [], "entities": []}, {"text": "We threw away the common set in each file, leaving 300 pairwise comparisons.", "labels": [], "entities": []}, {"text": "shows that the evaluators preferred the output from the PORTtuned system 136 times, the output from the BLEU-tuned one 98 times, and had no preference the other 66 times.", "labels": [], "entities": [{"text": "PORTtuned", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.6667455434799194}, {"text": "BLEU-tuned", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9849298596382141}]}, {"text": "This indicates that there is a human preference for outputs from the PORTtuned system over those from the BLEU-tuned system at the p<0.01 significance level (in cases where people prefer one of them).", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9939479827880859}]}, {"text": "PORT tuning seems to have a bigger advantage over BLEU tuning when the translation task is hard.", "labels": [], "entities": [{"text": "PORT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8406075239181519}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9861511588096619}, {"text": "translation task", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.8980436623096466}]}, {"text": "Of the language pairs, the one where PORT tuning helps most has the lowest BLEU in (German-English); the one where it helps least in has the highest BLEU in (French-English).", "labels": [], "entities": [{"text": "PORT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9840996265411377}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9984570741653442}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9979483485221863}]}, {"text": "does not prove BLEU is superior to PORT for French-English tuning: statistically, the difference between 14 and 17 here is a tie).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9993789196014404}, {"text": "PORT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9968827962875366}]}, {"text": "Maybe by picking examples for each condition that were the easiest for the system to translate (to make human evaluation easier), we mildly biased the results in against PORT tuning.", "labels": [], "entities": [{"text": "PORT", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.8458787202835083}]}, {"text": "Another possible factor is reordering.", "labels": [], "entities": []}, {"text": "PORT differs from BLEU partly in modeling longdistance reordering more accurately; English and French have similar word order, but the other two language pairs don't.", "labels": [], "entities": [{"text": "PORT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9534252285957336}, {"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9929721355438232}]}, {"text": "The results in section 3.3 (below) for Qmean, aversion of PORT without word ordering factor v, suggest v maybe defined suboptimally for French-English.", "labels": [], "entities": [{"text": "PORT", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9793750643730164}]}, {"text": "PORT win BLEU win equal total zh-en small", "labels": [], "entities": [{"text": "PORT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9809972047805786}, {"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9898468852043152}]}], "tableCaptions": [{"text": " Table 1: Statistics of the WMT dev and test sets.", "labels": [], "entities": [{"text": "WMT dev and test sets", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.7837066888809204}]}, {"text": " Table 2: Correlations with human judgment on WMT", "labels": [], "entities": [{"text": "WMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8843810558319092}]}, {"text": " Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.", "labels": [], "entities": [{"text": "BLEU-tuned", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9976576566696167}, {"text": "PORT-tuned", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9587578773498535}]}, {"text": " Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).", "labels": [], "entities": []}, {"text": " Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.", "labels": [], "entities": [{"text": "PORT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9283159971237183}, {"text": "BLEU-tuned", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9929779171943665}]}, {"text": " Table 4 BLEU baseline because  the dev sets differ).", "labels": [], "entities": [{"text": "BLEU baseline", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9530666172504425}]}, {"text": " Table 7: PORT tuning -human & GIZA++ alignment", "labels": [], "entities": [{"text": "PORT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9385191202163696}, {"text": "GIZA", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7222580909729004}]}, {"text": " Table 9: Comparison of the ordering measure: replacing  \u03bd with \u03c1 or \u03c4 in PORT.", "labels": [], "entities": [{"text": "PORT", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7160441279411316}]}, {"text": " Table 10: Ordering scores (\u03c1, \u03c4 and v) for test sets NIST  2006, 2008 and CTB.", "labels": [], "entities": [{"text": "NIST  2006, 2008", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.950943112373352}, {"text": "CTB", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.5153882503509521}]}, {"text": " Table 11: #matching-ngram/#total-ngram and BP score", "labels": [], "entities": [{"text": "matching-ngram", "start_pos": 12, "end_pos": 26, "type": "METRIC", "confidence": 0.9798959493637085}, {"text": "BP score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9731007218360901}]}]}