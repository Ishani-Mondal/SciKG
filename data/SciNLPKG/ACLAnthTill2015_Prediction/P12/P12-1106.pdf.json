{"title": [{"text": "Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation Metrics", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.8107104748487473}, {"text": "Summarization Evaluation", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.9808225631713867}]}], "abstractContent": [{"text": "An ideal summarization system should produce summaries that have high content coverage and linguistic quality.", "labels": [], "entities": [{"text": "summarization", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9810435175895691}]}, {"text": "Many state-of-the-art summarization systems focus on content coverage by extracting content-dense sentences from source articles.", "labels": [], "entities": []}, {"text": "A current research focus is to process these sentences so that they read fluently as a whole.", "labels": [], "entities": []}, {"text": "The current AE-SOP task encourages research on evaluating summaries on content, readability, and overall responsiveness.", "labels": [], "entities": [{"text": "AE-SOP", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.5985490679740906}]}, {"text": "In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary read-ability, and combine both in a trained regression model to evaluate overall responsiveness.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6852068454027176}]}, {"text": "The results show significantly improved performance over AESOP 2011 submitted met-rics.", "labels": [], "entities": [{"text": "AESOP 2011 submitted met-rics", "start_pos": 57, "end_pos": 86, "type": "DATASET", "confidence": 0.9121194034814835}]}], "introductionContent": [{"text": "Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (; ).", "labels": [], "entities": [{"text": "summarization", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.8715188503265381}, {"text": "content coverage", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.6584710031747818}]}, {"text": "However, users may still find it difficult to read such high-content coverage summaries as they lack fluency.", "labels": [], "entities": []}, {"text": "To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC)) introduced anew subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC))", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.7181137750546137}, {"text": "Automatically Evaluating Summaries of Peers (AESOP) task", "start_pos": 150, "end_pos": 206, "type": "TASK", "confidence": 0.782492631011539}]}, {"text": "Most of the state-of-the-art summarization systems) are extraction-based.", "labels": [], "entities": []}, {"text": "They extract the most content-dense sentences from source articles.", "labels": [], "entities": []}, {"text": "If no post-processing is performed to the generated summaries, the presentation of the extracted sentences may confuse readers.", "labels": [], "entities": []}, {"text": "argued that when the sentences of a text are randomly ordered, the text becomes difficult to understand, as its discourse structure is disturbed.", "labels": [], "entities": []}, {"text": "validated this argument by using a trained model to differentiate an original text from a randomlyordered permutation of its sentences by looking at their discourse structures.", "labels": [], "entities": []}, {"text": "This prior work leads us to believe that we can apply such discourse models to evaluate the readability of extract-based summaries.", "labels": [], "entities": []}, {"text": "We will discuss the application of Lin et al.'s discourse coherence model to evaluate readability of machine generated summaries.", "labels": [], "entities": []}, {"text": "We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the original model.", "labels": [], "entities": []}, {"text": "There are parallels between evaluations of machine translation (MT) and summarization with respect to textual content.", "labels": [], "entities": [{"text": "evaluations of machine translation (MT)", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.7663014658859798}, {"text": "summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.9838585257530212}]}, {"text": "For instance, the widely used ROUGE () metrics are influenced by BLEU (): both look at surface n-gram overlap for content coverage.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9486165046691895}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9982529282569885}]}, {"text": "Motivated by this, we will adapt a state-of-theart, linear programming-based MT evaluation metric, TESLA (, to evaluate the content coverage of summaries.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9740962386131287}]}, {"text": "TAC's overall responsiveness metric evaluates the quality of a summary with regard to both its content and readability.", "labels": [], "entities": [{"text": "TAC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7138394117355347}]}, {"text": "Given this, we combine our two component coherence and content models into an SVM-trained regression model as our surrogate to overall responsiveness.", "labels": [], "entities": []}, {"text": "Our experiments show that the coherence model significantly outperforms all AESOP 2011 submissions on both initial and update tasks, while the adapted MT evaluation metric and the combined model significantly outperform all submissions on the initial task.", "labels": [], "entities": [{"text": "AESOP 2011 submissions", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.7968504031499227}, {"text": "MT evaluation", "start_pos": 151, "end_pos": 164, "type": "TASK", "confidence": 0.7049172818660736}]}, {"text": "To the best of our knowledge, this is the first work that applies a discourse coherence model to measure the readability of summaries in the AESOP task.", "labels": [], "entities": [{"text": "AESOP task", "start_pos": 141, "end_pos": 151, "type": "TASK", "confidence": 0.4700217694044113}]}], "datasetContent": [{"text": "The last three rows in show the correlation scores of our regression model trained with SVM linear function (LF), polynomial function (PF), and radial basis function (RBF).", "labels": [], "entities": [{"text": "correlation", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9557360410690308}, {"text": "polynomial function (PF)", "start_pos": 114, "end_pos": 138, "type": "METRIC", "confidence": 0.8732522368431092}, {"text": "radial basis function (RBF)", "start_pos": 144, "end_pos": 171, "type": "METRIC", "confidence": 0.8673646549383799}]}, {"text": "PF performs better than LF, suggesting that content and readability scores should not be linearly combined.", "labels": [], "entities": [{"text": "PF", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7036068439483643}]}, {"text": "RBF gives better performances than both LF and PF, suggesting that RBF better models the way humans combine content and readability.", "labels": [], "entities": [{"text": "RBF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.548966109752655}]}, {"text": "On the initial task, the model trained with RBF outperforms all submitted metrics.", "labels": [], "entities": []}, {"text": "It outperforms the best correlation scores by 1.71%, 3.86%, and 4.60% on Pearson, Spearman, and Kendall, respectively.", "labels": [], "entities": [{"text": "correlation", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9942784309387207}]}, {"text": "All three regression models do not perform as well on the update task.", "labels": [], "entities": []}, {"text": "Koehn's significance test shows that when trained with RBF, CREMER outperforms ROUGE-2 and ROUGE-SU4 on the initial task at a significance level of 99% for all three correlation measures.", "labels": [], "entities": [{"text": "CREMER", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9341102242469788}, {"text": "ROUGE-2", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9602928161621094}, {"text": "ROUGE-SU4", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9487587809562683}]}], "tableCaptions": [{"text": " Table 1: Content correlation with human judgment  on summarizer level. Top three scores among AE- SOP metrics are underlined. The TESLA-S score is  bolded when it outperforms all others. ROUGE-2 is  shortened to R-2 and ROUGE-SU4 to R-SU4.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.9603940844535828}, {"text": "AE- SOP metrics", "start_pos": 95, "end_pos": 110, "type": "METRIC", "confidence": 0.8862434476613998}, {"text": "TESLA-S score", "start_pos": 131, "end_pos": 144, "type": "METRIC", "confidence": 0.9805766940116882}, {"text": "ROUGE-2", "start_pos": 188, "end_pos": 195, "type": "METRIC", "confidence": 0.9885850548744202}, {"text": "ROUGE-SU4", "start_pos": 221, "end_pos": 230, "type": "METRIC", "confidence": 0.9283609390258789}]}, {"text": " Table 3: Readability correlation with human judg- ment on summarizer level. Top three scores among  AESOP metrics are underlined. Our score is bolded  when it outperforms all AESOP metrics.", "labels": [], "entities": []}, {"text": " Table 5: Responsiveness correlation with human  judgment on summarizer level. Top three scores  among AESOP metrics are underlined. CREMER  score is bolded when it outperforms all AESOP met- rics.", "labels": [], "entities": [{"text": "CREMER  score", "start_pos": 133, "end_pos": 146, "type": "METRIC", "confidence": 0.9849114716053009}, {"text": "AESOP met- rics", "start_pos": 181, "end_pos": 196, "type": "DATASET", "confidence": 0.7225576341152191}]}]}