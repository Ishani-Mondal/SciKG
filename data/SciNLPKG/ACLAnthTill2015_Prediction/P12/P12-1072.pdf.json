{"title": [{"text": "A Probabilistic Model for Canonicalizing Named Entity Mentions", "labels": [], "entities": [{"text": "Canonicalizing Named Entity Mentions", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.9177672863006592}]}], "abstractContent": [{"text": "We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes).", "labels": [], "entities": [{"text": "canonicalizing named entity mentions", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.8620963990688324}]}, {"text": "The model is novel in that it incorporates entity context, surface features, first-order dependencies among attribute-parts, and a notion of noise.", "labels": [], "entities": []}, {"text": "Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation.", "labels": [], "entities": []}, {"text": "We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglom-erative clustering approach and previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Proper handling of mentions in text of real-world entities-identifying and resolving them-is a central part of many NLP applications.", "labels": [], "entities": [{"text": "handling of mentions in text of real-world entities-identifying and resolving them-is", "start_pos": 7, "end_pos": 92, "type": "TASK", "confidence": 0.8000918626785278}]}, {"text": "We seek an algorithm that infers a set of real-world entities from mentions in a text, mapping each entity mention token to an entity, and discovers general categories of words used in names (e.g., titles and last names).", "labels": [], "entities": []}, {"text": "Here, we use a probabilistic model to infer a structured representation of canonical forms of entity attributes through transductive learning from named entity mentions with a small number of seeds (see).", "labels": [], "entities": []}, {"text": "The input is a collection of mentions found by a named entity recognizer, along with their contexts, and, following, the output is a table in which entities are rows (the number of which is not pre-specified) and attribute words are organized into columns.", "labels": [], "entities": []}, {"text": "This paper contributes a model that builds on the approach of, but also: \u2022 incorporates context of the mention to help with disambiguation and to allow mentions that do not share words to be merged liberally; \u2022 conditions against shape features, which improve the assignment of words to columns; \u2022 is designed to explicitly handle some noise; and \u2022 is learned using elements of Bayesian inference with conditional estimation (see \u00a72).", "labels": [], "entities": []}, {"text": "We experiment with variations of our model, comparing it to a baseline clustering method and the model of, on two datasets, demonstrating improved performance over both at recovering a gold standard table.", "labels": [], "entities": []}, {"text": "Ina political blogs dataset, the mentions refer to political figures in the United States (e.g., Mrs. Obama and Michelle Obama).", "labels": [], "entities": []}, {"text": "As a result, the model discovers parts of names-Mrs., Michelle, Obamawhile simultaneously performing coreference resolution for named entity mentions.", "labels": [], "entities": [{"text": "coreference resolution for named entity mentions", "start_pos": 101, "end_pos": 149, "type": "TASK", "confidence": 0.8340181907018026}]}, {"text": "In the sports news dataset, the model is provided with named entity mentions of heterogenous types, and success here consists of identifying the correct team for every player (e.g., Kobe Bryant and Los Angeles Lakers).", "labels": [], "entities": []}, {"text": "In this scenario, given a few seed examples, the model begins to identify simple relations among named entities (in addition to discovering attribute structures), since attributes are expressed as named entities across multiple mentions.", "labels": [], "entities": []}, {"text": "We believe this adaptability is important, as the salience of different kinds of names and their usages vary considerably across domains.", "labels": [], "entities": []}, {"text": "C is the number of attributes/columns, the number of rows is infinite, \u03b1 is a vector of concentration parameters, \u03c6 is a multinomial distribution over strings, and x is a word in a table cell.", "labels": [], "entities": []}, {"text": "Lower left, for choosing entities to be mentioned: \u03c4 determines the stick lengths and \u03b7 is the distribution over entities to be selected for mention.", "labels": [], "entities": []}, {"text": "Middle right, for choosing attributes to use in a mention: f is the feature vector, and \u03b2 is the weight vector drawn from a Laplace distribution with mean zero and variance \u00b5.", "labels": [], "entities": []}, {"text": "Center, for generating mentions: M is the number of mentions in the data, w is a word token set from an entity/row rand attribute/column c.", "labels": [], "entities": []}, {"text": "Lower right, for generating contexts: sis a context word, drawn from a multinomial distribution \u03b8 with a Dirichlet prior \u03bb.", "labels": [], "entities": []}, {"text": "Variables that are known or fixed are shaded; variables that are optimized are double circled.", "labels": [], "entities": []}, {"text": "Others are latent; dashed lines imply collapsing.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare several variations of our model to Eisenstein et al.", "labels": [], "entities": []}, {"text": "(2011) (the authors provided their implementation to us) and a clustering baseline.", "labels": [], "entities": []}, {"text": "We collected named entity mentions from two corpora: political blogs and sports news.", "labels": [], "entities": []}, {"text": "The political blogs corpus is a collection of blog posts about politics in the United States, and the sports news corpus contains news summaries of major league sports games (National Basketball On our moderate-sized datasets (see \u00a74.1), each iteration takes approximately three minutes on a 2.2GHz CPU.", "labels": [], "entities": []}, {"text": "For the politics dataset, we set C = 6, \u03b1 = 1.0, 1.0, 10 \u221212 , 10 \u221215 , 10 \u221212 , 10 \u22128 , initialized \u03c4 = 1, and used a Dirichlet prior on transition counts such that before observing any data: N0,1 = 10, N0,5 = 5, N2,0 = 10, N2,1 = 10, N2,3 = 10, N2,4 = 5, N3,0 = 10, N3,1 = 10, N5,1 = 15 (others are set to zero).", "labels": [], "entities": [{"text": "politics dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.7382389307022095}]}, {"text": "For the sports dataset, we set C = 5, \u03b1 = 1.0, 1.0, 10 \u221215 , 10 \u22126 , 10 \u22126 , initialized \u03c4 = 1, and used a Dirichlet prior on transition counts N0,1 = 10, N2,3 = 20, N3,4 = 10 (others are set to zero).", "labels": [], "entities": [{"text": "sports dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.7127486765384674}]}, {"text": "We also manually initialized the weights of some features \u03b2 for both datasets.", "labels": [], "entities": []}, {"text": "These values were obtained from preliminary experiments on a smaller sample of the datasets, and updated on the first EM iteration.", "labels": [], "entities": []}, {"text": "shows the seeds for both datasets.", "labels": [], "entities": []}, {"text": "We propose both a row evaluation to determine how well a model disambiguates entities and merges mentions of the same entity and a column evaluation to measure how well the model relates words used in different mentions.", "labels": [], "entities": []}, {"text": "Both scores are new for this task.", "labels": [], "entities": []}, {"text": "The first step in evaluation is to find a maximum score bipartite matching between rows in the response and reference table.", "labels": [], "entities": []}, {"text": "Given the response and reference tables, x res and xref , we can compute: where i and j denote rows, Match(i, j) is one if i and j are matched to each other in the optimal matching or zero otherwise.", "labels": [], "entities": [{"text": "Match", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.997464656829834}]}, {"text": "S res is a precision-like score, and S ref is a recall-like score.", "labels": [], "entities": [{"text": "precision-like score", "start_pos": 11, "end_pos": 31, "type": "METRIC", "confidence": 0.9804527163505554}, {"text": "recall-like score", "start_pos": 48, "end_pos": 65, "type": "METRIC", "confidence": 0.9751244783401489}]}, {"text": "Column evaluation is the same, but compares columns instead.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Descriptive statistics about the datasets.", "labels": [], "entities": []}]}