{"title": [], "abstractContent": [{"text": "We present a novel approach to the task of word lemmatisation.", "labels": [], "entities": [{"text": "word lemmatisation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7082346081733704}]}, {"text": "We formalise lemmati-sation as a category tagging task, by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred fora specific language.", "labels": [], "entities": [{"text": "category tagging task", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7837231755256653}]}, {"text": "In this way, a lemmatisation system can be trained and tested using any supervised tagging model.", "labels": [], "entities": []}, {"text": "In contrast to previous approaches, the proposed technique allows us to easily integrate relevant contextual information.", "labels": [], "entities": []}, {"text": "We test our approach on eight languages reaching anew state-of-the-art level for the lemmatisation task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lemmatisation and part-of-speech (POS) tagging are necessary steps in automatic processing of language corpora.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6506009221076965}]}, {"text": "This annotation is a prerequisite for developing systems for more sophisticated automatic processing such as information retrieval, as well as for using language corpora in linguistic research and in the humanities.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.7926540970802307}]}, {"text": "Lemmatisation is especially important for processing morphologically rich languages, where the number of different word forms is too large to be included in the part-ofspeech tag set.", "labels": [], "entities": []}, {"text": "The work on morphologically rich languages suggests that using comprehensive morphological dictionaries is necessary for achieving good results).", "labels": [], "entities": []}, {"text": "However, such dictionaries are constructed manually and they cannot be expected to be developed quickly for many languages.", "labels": [], "entities": []}, {"text": "In this paper, we present anew general approach to the task of lemmatisation which can be used to overcome the shortage of comprehensive dictionaries for languages for which they have not been developed.", "labels": [], "entities": []}, {"text": "Our approach is based on redefining the task of lemmatisation as a category tagging task.", "labels": [], "entities": [{"text": "category tagging task", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.7969420552253723}]}, {"text": "Formulating lemmatisation as a tagging task allows the use of advanced tagging techniques, and the efficient integration of contextual information.", "labels": [], "entities": []}, {"text": "We show that this approach gives the highest accuracy known on eight European languages having different morphological complexity, including agglutinative (Hungarian, Estonian) and fusional (Slavic) languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9992722868919373}]}], "datasetContent": [{"text": "The advantage of structuring the lemmatisation task as a tagging task is that it allows us to apply successful tagging techniques and use the context information in assigning transformation labels to the words in a text.", "labels": [], "entities": []}, {"text": "For the experimental evaluations we use the Bidirectional Tagger with Guided Learning presented in.", "labels": [], "entities": []}, {"text": "We chose this model since it has been shown to be easily adaptable for solving a wide set of tagging and chunking tasks obtaining state-of-the-art performances with short execution time.", "labels": [], "entities": [{"text": "tagging and chunking tasks", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.7307089269161224}]}, {"text": "Furthermore, this model has consistently shown good generalisation behaviour reaching significantly higher accuracy in tagging unknown words than other systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9983412027359009}, {"text": "tagging unknown words", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.8883086442947388}]}, {"text": "We train and test the tagger on manually annotated G.", "labels": [], "entities": []}, {"text": "Orwell's \"1984\" and its translations to seven European languages (see, column 1), included in the Multext-East corpora . The words in the corpus are annotated with both lemmas and detailed morphosyntactic descriptions including the POS labels.", "labels": [], "entities": [{"text": "Orwell's \"1984\"", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9481120109558105}, {"text": "Multext-East corpora", "start_pos": 98, "end_pos": 118, "type": "DATASET", "confidence": 0.9680289030075073}]}, {"text": "The corpus contains 6737 sentences (approximatively 110k tokens) for each language.", "labels": [], "entities": []}, {"text": "We use 90% of the sentences for training and 10% for testing.", "labels": [], "entities": []}, {"text": "We compare lemmatisation performance in different settings.", "labels": [], "entities": []}, {"text": "Each setting is defined by the set of features that are used for training and prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 78, "end_pos": 88, "type": "TASK", "confidence": 0.9113871455192566}]}, {"text": "the second experiment are reported in the third column of.", "labels": [], "entities": []}, {"text": "The consistent improvements over the BL scores for all the languages, varying from the lowest relative error reduction (RER) for Czech (5.8%) to the highest for Romanian (31.6%), confirm the significance of the context information.", "labels": [], "entities": [{"text": "BL", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9980154037475586}, {"text": "relative error reduction (RER)", "start_pos": 94, "end_pos": 124, "type": "METRIC", "confidence": 0.9216296474138895}]}, {"text": "In the third experiment, we use a feature set in which the BL set is expanded with the predicted POS tag of the current word, [pos 0 ].", "labels": [], "entities": [{"text": "BL", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9846934080123901}, {"text": "POS tag", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.959122896194458}]}, {"text": "The accuracy measured in the third experiment, column 4) shows consistent improvement over the BL (the best RER is 34.2% for Romanian).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995600581169128}, {"text": "BL", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.998641312122345}, {"text": "RER", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9984093308448792}]}, {"text": "Furthermore, we observe that the accuracy scores in the third experiment are close to those in the second experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9996402263641357}]}, {"text": "This allows us to state that it is possible to design high quality lemmatisation systems which are independent of the POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.5828910768032074}]}, {"text": "Instead of using the POS information, which is currently standard practice for lemmatisation, the task can be performed in a context-wise setting using only the information about surrounding words and lemmas.", "labels": [], "entities": []}, {"text": "In the fourth experiment we use a feature set consisting of contextual features of words, predicted lemmas and predicted POS tags.", "labels": [], "entities": []}, {"text": "This setting com-bines the use of the context with the use of the predicted POS tags.", "labels": [], "entities": []}, {"text": "The scores obtained in the fourth experiment are considerably higher than those in the previous experiments, column 5).", "labels": [], "entities": []}, {"text": "The RER computed against the BL varies between 28.1% for Hungarian and 66.7% for English.", "labels": [], "entities": [{"text": "RER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9990752935409546}, {"text": "BL", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9963545799255371}]}, {"text": "For this setting, we also report accuracies on unseen words only (UWA, column 6 in) to show the generalisation capacities of the lemmatizer.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9628476500511169}, {"text": "UWA", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.8844479918479919}]}, {"text": "The UWA scores 85% or higher for all the languages except Estonian (78.5%).", "labels": [], "entities": [{"text": "UWA", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7915607690811157}]}, {"text": "The results of the fourth experiment show that interesting improvements in the performance are obtained by combining the POS and context information.", "labels": [], "entities": [{"text": "POS", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.8902653455734253}]}, {"text": "This option has not been explored before.", "labels": [], "entities": []}, {"text": "Current systems typically use only the information on the POS of the target word together with lemmatisation rules acquired separately from a dictionary, which roughly corresponds to the setting of our third experiment.", "labels": [], "entities": [{"text": "POS", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9446365833282471}]}, {"text": "The improvement in the fourth experiment compared to the third experiment (RER varying between 12.5% for Czech and 50% for English) shows the advantage of our context-sensitive approach over the currently used techniques.", "labels": [], "entities": [{"text": "RER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9988980293273926}]}, {"text": "All the scores reported in represent performance with raw text as input.", "labels": [], "entities": []}, {"text": "It is important to stress that the results are achieved using a general tagging system trained only a small manually annotated corpus, with no language specific external sources of data such as independent morphological dictionaries, which have been considered necessary for efficient processing of morphologically rich languages.", "labels": [], "entities": []}, {"text": "propose a general multilingual lemmatisation tool, LemGen, which is tested on the same corpora that we used in our evaluation.", "labels": [], "entities": []}, {"text": "LemGen learns word transformations in the form of ripple-down rules.", "labels": [], "entities": [{"text": "LemGen", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9086318016052246}, {"text": "word transformations", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.6823711842298508}]}, {"text": "Disambiguition between multiple possible lemmas fora word form is based on the gold-standard morphosyntactic label of the word.", "labels": [], "entities": []}, {"text": "Our system outperforms LemGen on all the languages.", "labels": [], "entities": [{"text": "LemGen", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9215933680534363}]}, {"text": "We measure a Relative Error Reduction varying between 81% for Serbian and 86% for English.", "labels": [], "entities": [{"text": "Relative Error Reduction", "start_pos": 13, "end_pos": 37, "type": "METRIC", "confidence": 0.9302135904630026}]}, {"text": "It is worth noting that we do not use manually constructed dictionaries for training, while use additional dictionaries for languages for which they are available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table  1 reports the four feature sets used. Table 2 reports  the accuracy scores achieved in each setting. We es- tablish the Base Line (BL) setting and performance  in the first experiment. This setting involves only  features of the current word, [w 0 ], such as the word  form, suffixes and prefixes and features that flag the  presence of special characters (digits, hyphen, caps).  The BL accuracy is reported in the second column of  Table 2).  In the second experiment, the BL feature set is  expanded with features of the surrounding words  ([w \u22121 ], [w 1 ]) and surrounding predicted lemmas  ([lem \u22121 ], [lem 1 ]). The accuracy scores obtained in", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9987163543701172}, {"text": "Base Line (BL) setting", "start_pos": 128, "end_pos": 150, "type": "METRIC", "confidence": 0.9407112995783488}, {"text": "BL accuracy", "start_pos": 393, "end_pos": 404, "type": "METRIC", "confidence": 0.7722646296024323}, {"text": "accuracy", "start_pos": 630, "end_pos": 638, "type": "METRIC", "confidence": 0.9977229237556458}]}, {"text": " Table 2: Accuracy of the lemmatizer in the four settings.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983397722244263}]}]}