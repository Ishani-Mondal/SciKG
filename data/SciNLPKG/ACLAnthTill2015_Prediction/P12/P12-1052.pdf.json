{"title": [{"text": "Dependency Hashing for n-best CCG Parsing", "labels": [], "entities": [{"text": "Dependency Hashing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8360122442245483}, {"text": "Parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.526032567024231}]}], "abstractContent": [{"text": "Optimising for one grammatical representation , but evaluating over a different one is a particular challenge for parsers and n-best CCG parsing.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.5752926617860794}]}, {"text": "We find that this mismatch causes many n-best CCG parses to be semantically equivalent, and describe a hashing technique that eliminates this problem, improving oracle n-best F-score by 0.7% and reranking accuracy by 0.4%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 175, "end_pos": 182, "type": "METRIC", "confidence": 0.9511165022850037}, {"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9391779899597168}]}, {"text": "We also present a comprehensive analysis of errors made by the C&C CCG parser, providing the first breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy.", "labels": [], "entities": [{"text": "C&C CCG parser", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.930009913444519}, {"text": "parsing", "start_pos": 177, "end_pos": 184, "type": "TASK", "confidence": 0.9742917418479919}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.8510283827781677}]}], "introductionContent": [{"text": "Reranking techniques are commonly used for improving the accuracy of parsing).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9986431002616882}, {"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9355791211128235}]}, {"text": "Efficient decoding of a parse forest is infeasible without dynamic programming, but this restricts features to local tree contexts.", "labels": [], "entities": []}, {"text": "Reranking operates over a list of n-best parses according to the original model, allowing poor local parse decisions to be identified using arbitrary rich parse features.", "labels": [], "entities": []}, {"text": "The performance of reranking depends on the quality of the underlying n-best parses.'s n-best algorithms are used in a wide variety of parsers, including an n-best version of the C&C CCG parser.", "labels": [], "entities": [{"text": "C&C CCG parser", "start_pos": 179, "end_pos": 193, "type": "DATASET", "confidence": 0.7516965508460999}]}, {"text": "The oracle F-score of this parser (calculated by selecting the most optimal parse in the n-best list) is 92.60% with n = 50 over a baseline 1-best Fscore of 86.84%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9546684622764587}]}, {"text": "In contrast, the Charniak parser records an oracle F-score of 96.80% in 50-best mode over a baseline of 91.00%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9120751619338989}]}, {"text": "The 4.2% oracle score difference suggests that further optimisations maybe possible for CCG.", "labels": [], "entities": [{"text": "CCG", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.6022190451622009}]}, {"text": "We describe how n-best parsing algorithms that operate over derivations do not account for absorption ambiguities in parsing, causing semantically identical parses to exist in the CCG n-best list.", "labels": [], "entities": [{"text": "CCG n-best list", "start_pos": 180, "end_pos": 195, "type": "DATASET", "confidence": 0.8542474309603373}]}, {"text": "This is caused by the mismatch between the optimisation target (different derivations) and the evaluation target (CCG dependencies).", "labels": [], "entities": []}, {"text": "We develop a hashing technique over dependencies that removes duplicates and improves the oracle F-score by 0.7% to 93.32% and reranking accuracy by 0.4%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9688480496406555}, {"text": "reranking", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9290103912353516}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9005324840545654}]}, {"text": "proposed a similar idea where strings generated by a syntax-based MT rescoring system were hashed to prevent duplicate translations.", "labels": [], "entities": []}, {"text": "Despite this improvement, there is still a substantial gap between the C&C and Charniak oracle Fscores.", "labels": [], "entities": [{"text": "C&C and Charniak oracle Fscores", "start_pos": 71, "end_pos": 102, "type": "DATASET", "confidence": 0.8073155879974365}]}, {"text": "We perform a comprehensive subtractive analysis of the C&C parsing pipeline, identifying the relative contribution of each error class and why the gap exists.", "labels": [], "entities": [{"text": "C&C parsing pipeline", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.6262100815773011}]}, {"text": "The parser scores 99.49% F-score with gold-standard categories on section 00 of CCGbank, and 94.32% F-score when returning the best parse in the chart using the supertagger on standard settings.", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9995224475860596}, {"text": "CCGbank", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9804893732070923}, {"text": "F-score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9990843534469604}]}, {"text": "Thus the supertagger contributes roughly 5% of parser error, and the parser model the remaining 7.5%.", "labels": [], "entities": [{"text": "error", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.4883868098258972}]}, {"text": "Various other speed optimisations also detrimentally affect accuracy to a smaller degree.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.998790442943573}]}, {"text": "Several subtle trade-offs are made in parsers between speed and accuracy, but their actual impact is often unclear.", "labels": [], "entities": [{"text": "speed", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9962453246116638}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9980452060699463}]}, {"text": "Our work investigates these and the general issue of how different optimisation and evaluation targets can affect parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 114, "end_pos": 121, "type": "TASK", "confidence": 0.9780595898628235}]}, {"text": "Jack swims across the river: A CCG derivation with a PP adjunct, demonstrating forward and backward combinator application.", "labels": [], "entities": []}, {"text": "Adapted from Villavicencio (2002).", "labels": [], "entities": [{"text": "Adapted from Villavicencio (2002)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7849133312702179}]}], "datasetContent": [{"text": "CCGbank) is a transformation of the Penn Treebank (PTB) data into CCG derivations, and it is the standard corpus for English CCG parsing.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) data", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9753098785877228}, {"text": "English CCG parsing", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.5263301233450571}]}, {"text": "Other CCG corpora have been induced in a similar way for German) and Chinese ().", "labels": [], "entities": []}, {"text": "CCGbank contains 99.44% of the sentences from the PTB, and several non-standard rules were necessary to achieve this coverage.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9559215307235718}, {"text": "PTB", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.5066684484481812}]}, {"text": "These include punctuation absorption rules and unary type-changing rules for clausal adjuncts that are otherwise difficult to represent.", "labels": [], "entities": [{"text": "punctuation absorption", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7036053985357285}]}, {"text": "The standard CCG parsing evaluation calculates labeled precision, recall, and F-score over the dependencies recovered by a parser as compared to).", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.5235061794519424}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.8658096194267273}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9991133809089661}, {"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9993830919265747}]}, {"text": "All components of a dependency must match the gold standard for it to be scored as correct, and this makes the procedure much harsher than the PARSEVAL labeled brackets metric.", "labels": [], "entities": []}, {"text": "In, the PP across the river has been interpreted as an argument rather than an adjunct as in.", "labels": [], "entities": []}, {"text": "Both parses would score identically under PARSEVAL as their bracketing is unchanged.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.7699100971221924}]}, {"text": "However, the adjunct to argument change results in different categories for swims and across; nearly every CCG dependency in the sentence is headed by one of these two words and thus each one changes as a result.", "labels": [], "entities": []}, {"text": "An incorrect argument/adjunct distinction in this sentence produces a score close to 0.", "labels": [], "entities": []}, {"text": "All experiments in this paper use the normal-form C&C parser model over CCGbank 00.", "labels": [], "entities": []}, {"text": "Scores are reported for sentences which the parser could analyse; we observed similar conclusions when repeating our experiments over the subset of sentences that were parsable under all configurations described in this paper.", "labels": [], "entities": []}, {"text": "We develop an oracle methodology to distinguish between grammar, supertagger, and model errors.", "labels": [], "entities": []}, {"text": "This is the most comprehensive error analysis of a parsing pipeline in the literature.", "labels": [], "entities": [{"text": "parsing pipeline", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.9254180788993835}]}, {"text": "First, we supplied gold-standard categories for each word in the sentence.", "labels": [], "entities": []}, {"text": "In this experiment the parser only needs to combine the categories correctly to form the gold parse.", "labels": [], "entities": []}, {"text": "In our testing over CCGbank 00, the parser scores 99.49% Fscore given perfect categories, with 95.61% coverage.", "labels": [], "entities": [{"text": "CCGbank 00", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9741545021533966}, {"text": "Fscore given perfect categories", "start_pos": 57, "end_pos": 88, "type": "METRIC", "confidence": 0.7703855037689209}, {"text": "coverage", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9819728136062622}]}, {"text": "Thus, grammar error accounts for about 0.5% of overall parser errors as well as a 4.4% drop in coverage 2 . All results in this section will be compared against this 99.49% result as it removes the grammar error from consideration.", "labels": [], "entities": [{"text": "grammar error", "start_pos": 6, "end_pos": 19, "type": "METRIC", "confidence": 0.8583232760429382}, {"text": "coverage", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9929560422897339}]}, {"text": "We obtain an overall maximum possible F-score for the parser using this scoring formula.", "labels": [], "entities": [{"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9984133243560791}]}, {"text": "The difference between this maximum F-score and the oracle result of 99.49% represents supertagger error (where the supertagger has not provided the correct categories), and the difference to the baseline performance indicates model error (where the parser model has not selected the optimal parse given the current categories).", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9957160353660583}]}, {"text": "We also try disabling the seen rules constraint to determine its impact on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9967838525772095}]}, {"text": "The impact of tag dictionary errors must be neutralised in order to distinguish between the types of supertagger error.", "labels": [], "entities": []}, {"text": "To do this, we added the gold category fora word to the set of possible tags considered for that word by the supertagger.", "labels": [], "entities": []}, {"text": "This was done for categories that the supertagger could use; categories that were not in the permissible set of 425 categories were not considered.", "labels": [], "entities": []}, {"text": "This is an optimistic experiment; removing the tag dictionary entirely would greatly increase the number of categories considered by the supertagger and may dramatically change the tagging results.", "labels": [], "entities": []}, {"text": "shows the results of our experiments.", "labels": [], "entities": []}, {"text": "The delta columns indicate the difference in labeled Fscore to the oracle result, which discounts the grammar error in the parser.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.8488459587097168}]}, {"text": "We ran the experiment in four configurations: disabling the tag dictionary, disabling the seen rules constraint, and disabling both.", "labels": [], "entities": []}, {"text": "There are coverage differences of less than 0.5% that will have a small impact on these results.", "labels": [], "entities": [{"text": "coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9913763403892517}]}, {"text": "The \"best in chart\" experiment produces a result of 94.32% with gold POS tags and 92.60% with auto POS tags.", "labels": [], "entities": []}, {"text": "These numbers are the upper bound of the parser with the supertagger on standard settings.", "labels": [], "entities": []}, {"text": "Our result with gold POS tags is statistically identical to the oracle experiment conducted by, which exchanged brackets for dependencies in the forest oracle algorithm of.", "labels": [], "entities": []}, {"text": "This illustrates the validity of our technique.", "labels": [], "entities": []}, {"text": "A perfect tag dictionary that always contains the gold standard category if it is available results in an upper bound accuracy of 95.42%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9802261590957642}]}, {"text": "This shows that overall supertagger error in the parser is around 5.2%, with roughly 1% attributable to the use of the tag dictionary and the remainder to the supertagger model.", "labels": [], "entities": [{"text": "error", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.6236108541488647}]}, {"text": "The baseline parser is 12.5% worse than the oracle categories result due to model error and supertagger error, so model error accounts for roughly 7.3% of the loss.", "labels": [], "entities": []}, {"text": "Eliminating the seen rules constraint contributes to a 0.5% accuracy improvement over both the standard parser configuration and the -tagdict configuration, at the cost of roughly 0.3% coverage to both.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9990965127944946}, {"text": "coverage", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9574180841445923}]}, {"text": "This is of similar magnitude to grammar error; but  here accuracy is traded off against coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9991970658302307}, {"text": "coverage", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.956644594669342}]}, {"text": "The results also show that model and supertagger error largely accounts for the remaining oracle accuracy difference between the C&C n-best parser and the Charniak/Collins n-best parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.986909806728363}]}, {"text": "The absolute upper bound of the C&C parser is only 1% higher than the oracle 50-best score in, placing the n-best parser close to its theoretical limit.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average and distinct parses per sentence over  CCGbank 00 with respect to CCG dependencies. # indi- cates the inclusion of dependency hashing", "labels": [], "entities": []}, {"text": " Table 2: Average and distinct parses per sentence over  CCGbank 00 with respect to GRs. # indicates the inclu- sion of dependency hashing", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9198495149612427}]}, {"text": " Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. # denotes the  inclusion of dependency hashing.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9915784597396851}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9994364380836487}, {"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9993821382522583}]}, {"text": " Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing", "labels": [], "entities": [{"text": "Reranked", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9052889943122864}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9318698048591614}, {"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9826117753982544}]}, {"text": " Table 6: Oracle labeled precision, recall, F-score, F-score with auto POS, and coverage over CCGbank 00. -tagdict  indicates disabling the tag dictionary, -seen rules indicates disabling the seen rules constraint", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9897508025169373}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9993571639060974}, {"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9894562363624573}, {"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9935192465782166}, {"text": "coverage", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9984869956970215}]}, {"text": " Table 7: Category ambiguity, speed, labeled P, R, F-score on gold and auto POS, and coverage over CCGbank 00 for  the standard supertagger parameters selecting the best scoring parse against the gold parse in the chart.", "labels": [], "entities": [{"text": "speed", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9980317950248718}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9700818061828613}, {"text": "coverage", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9990450739860535}, {"text": "CCGbank 00", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.7909647226333618}]}, {"text": " Table 8: Labeled F-score, coverage, and deltas over  CCGbank 00 for combinations of gold and auto POS tags.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9142643809318542}, {"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9909275770187378}, {"text": "CCGbank 00", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.9305674433708191}]}]}