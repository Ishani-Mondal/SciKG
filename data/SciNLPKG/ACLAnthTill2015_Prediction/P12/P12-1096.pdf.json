{"title": [{"text": "A Ranking-based Approach to Word Reordering for Statistical Machine Translation *", "labels": [], "entities": [{"text": "Word Reordering", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7649798691272736}, {"text": "Statistical Machine Translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.8060349027315775}]}], "abstractContent": [{"text": "Long distance word reordering is a major challenge in statistical machine translation research.", "labels": [], "entities": [{"text": "Long distance word reordering", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7358655780553818}, {"text": "statistical machine translation research", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.7489503175020218}]}, {"text": "Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference.", "labels": [], "entities": []}, {"text": "In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to repo-sition nodes in the syntactic parse tree of a source sentence.", "labels": [], "entities": []}, {"text": "The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features.", "labels": [], "entities": []}, {"text": "We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase-based SMT system.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.7885602513949076}, {"text": "SMT", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.8701923489570618}]}], "introductionContent": [{"text": "Modeling word reordering between source and target sentences has been a research focus since the emerging of statistical machine translation.", "labels": [], "entities": [{"text": "Modeling word reordering between source and target sentences", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.8798688054084778}, {"text": "statistical machine translation", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.6618468562761942}]}, {"text": "In phrase-based models, phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation.", "labels": [], "entities": [{"text": "language translation", "start_pos": 249, "end_pos": 269, "type": "TASK", "confidence": 0.7265788465738297}]}, {"text": "Further work in this direction employed lexi- * This work has been done while the first author was visiting Microsoft Research Asia.", "labels": [], "entities": []}, {"text": "calized distortion models, including both generative ( ) and discriminative () variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation.", "labels": [], "entities": []}, {"text": "Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling.", "labels": [], "entities": []}, {"text": "The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.931327760219574}]}, {"text": "There are two major categories in this line of research.", "labels": [], "entities": []}, {"text": "One is tree-to-string model) which directly uses source parse trees to derive a large set of translation rules and associated model parameters.", "labels": [], "entities": []}, {"text": "The other is called syntax pre-reordering -an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees.", "labels": [], "entities": []}, {"text": "This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9143140912055969}]}, {"text": "In this paper, we continue this line of work and address the problem of word reordering based on source syntactic parse trees for SMT.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7367578744888306}, {"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.9841618537902832}]}, {"text": "Similar to most previous work, our approach tries to rearrange the source tree nodes sharing a common parent to mimic the word order in target language.", "labels": [], "entities": []}, {"text": "To this end, we propose a simple but effective ranking-based approach to word reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.8143623173236847}]}, {"text": "The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked.", "labels": [], "entities": []}, {"text": "The ranks of tree nodes are determined by their relative positions in the target language -the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank.", "labels": [], "entities": []}, {"text": "The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (, and does not require any special tweaking in model training.", "labels": [], "entities": []}, {"text": "The ranking model cannot only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9816376566886902}]}, {"text": "We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.7901287575562795}, {"text": "SMT", "start_pos": 218, "end_pos": 221, "type": "TASK", "confidence": 0.8686016201972961}]}, {"text": "In the rest of the paper, we will first formally present our ranking-based word reordering model, then followed by detailed steps of modeling training and integration into a phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.8871805667877197}]}, {"text": "Experimental results are shown in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 consists of more discussions on related work, and Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test our ranking reorder model, we carryout experiments on large scale English-To-Japanese, and Japanese-To-English translation tasks.", "labels": [], "entities": [{"text": "Japanese-To-English translation tasks", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.7370244065920512}]}, {"text": "We collect 3,500 Japanese sentences and 3,500 English sentences from the web.", "labels": [], "entities": []}, {"text": "They come from a wide range of domains, such as technical documents, web forum data, travel logs etc.", "labels": [], "entities": []}, {"text": "They are manually translated into the other language to produce 7,000 sentence pairs, which are split into two parts: 2,000 pairs as development set (dev) and the other 5,000 pairs as test set (web test).", "labels": [], "entities": []}, {"text": "Beside that, we collect another 999 English sentences from newswire domain which are translated into Japanese to form an out-of-domain test data set (news test).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU(%) score on dev and test data for  both E-J and J-E experiment. All settings signifi- cantly improve over the baseline at 95% confidence  level. Baseline is the BTG phrase system system;  ManR-PR is pre-reorder with manual rule; Rank-PR  is pre-reorder with ranking reorder model; Rank-IT  is system with integrated ranking reorder model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9996445178985596}]}, {"text": " Table 3: Reorder performance measured by  crossing-link number per sentence. None means the  original sentences without reordering; Oracle means  the best permutation allowed by the source parse  tree; ManR refers to manual reorder rules; Rank  means ranking reordering model.", "labels": [], "entities": [{"text": "Oracle", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.8190816044807434}]}, {"text": " Table 4: Effect of ranking features. Acc. is Rank- ingSVM accuracy in percentage on the training data;", "labels": [], "entities": [{"text": "Acc.", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9963110089302063}, {"text": "Rank- ingSVM", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.9342155853907267}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7639961242675781}]}]}