{"title": [{"text": "A Joint Model for Discovery of Aspects in Utterances", "labels": [], "entities": [{"text": "Discovery of Aspects in Utterances", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.8593781113624572}]}], "abstractContent": [{"text": "We describe a joint model for understanding user actions in natural language utterances.", "labels": [], "entities": [{"text": "understanding user actions in natural language utterances", "start_pos": 30, "end_pos": 87, "type": "TASK", "confidence": 0.6504123721803937}]}, {"text": "Our multi-layer generative approach uses both labeled and unlabeled utterances to jointly learn aspects regarding utterance's target domain (e.g. movies), intention (e.g., finding a movie) along with other semantic units (e.g., movie name).", "labels": [], "entities": []}, {"text": "We inject information extracted from unstructured web search query logs as prior information to enhance the generative process of the natural language utterance understanding model.", "labels": [], "entities": [{"text": "natural language utterance understanding", "start_pos": 134, "end_pos": 174, "type": "TASK", "confidence": 0.7221979349851608}]}, {"text": "Using utterances from five domains, our approach shows up to 4.5% improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model (which requires fully labeled data).", "labels": [], "entities": []}], "introductionContent": [{"text": "Virtual personal assistance (VPA) is a human to machine dialog system, which is designed to perform tasks such as making reservations at restaurants, checking flight statuses, or planning weekend activities.", "labels": [], "entities": [{"text": "Virtual personal assistance (VPA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6928051163752874}]}, {"text": "A typical spoken language understanding (SLU) module of a VPA) defines a structured representation for utterances, in which the constituents correspond to meaning representations in terms of slot/value pairs (see).", "labels": [], "entities": []}, {"text": "While target domain corresponds to the context of an utterance in a dialog, the dialog act represents overall intent of an utterance.", "labels": [], "entities": []}, {"text": "The slots are entities, which are semantic constituents at the word or phrase level.", "labels": [], "entities": []}, {"text": "Learning each component Sample utterances on 'plan a night out' scenario (I) Show me theaters in   is a challenging task not only because there are no a priori constraints on what a user might say, but also systems must generalize from a tractably small amount of labeled training data.", "labels": [], "entities": []}, {"text": "In this paper, we argue that each of these components are interdependent and should be modeled simultaneously.", "labels": [], "entities": []}, {"text": "We build a joint understanding framework and introduce a multi-layer context model for semantic representation of utterances of multiple domains.", "labels": [], "entities": []}, {"text": "Although different strategies can be applied, typically a cascaded approach is used where each semantic component is modeled separately/sequentially (), focusing lesson interrelated aspects, i.e., dialog's domain, user's intentions, and semantic tags that can be shared across domains.", "labels": [], "entities": []}, {"text": "Recent work on SLU ( presents joint modeling of two components, i.e., the domain and slot or dialog act and slot components together.", "labels": [], "entities": []}, {"text": "Furthermore, most of these systems rely on labeled training utterances, focusing little on issues such as information sharing between the discourse and word level components across different domains, or variations in use of language.", "labels": [], "entities": []}, {"text": "To deal with de-pendency and language variability issues, a model that considers dependencies between semantic components and utilizes information from large bodies of unlabeled text can be beneficial for SLU.", "labels": [], "entities": [{"text": "SLU", "start_pos": 205, "end_pos": 208, "type": "TASK", "confidence": 0.9343544244766235}]}, {"text": "In this paper, we present a novel generative Bayesian model that learns domain/dialog-act/slot semantic components as latent aspects of text utterances.", "labels": [], "entities": []}, {"text": "Our approach can identify these semantic components simultaneously in a hierarchical framework that enables the learning of dependencies.", "labels": [], "entities": []}, {"text": "We incorporate prior knowledge that we observe in web search query logs as constraints on these latent aspects.", "labels": [], "entities": []}, {"text": "Our model can discover associations between words within a multi-layered aspect model, in which some words are indicative of higher layer (meta) aspects (domain or dialog act components), while others are indicative of lower layer specific entities.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: (i) construction of a novel Bayesian framework for semantic parsing of natural language (NL) utterances in a unifying framework in \u00a74, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in \u00a73 & \u00a74, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in \u00a75.", "labels": [], "entities": [{"text": "semantic parsing of natural language (NL) utterances", "start_pos": 99, "end_pos": 151, "type": "TASK", "confidence": 0.8871877259678311}]}, {"text": "We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce.", "labels": [], "entities": [{"text": "generative", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9759811758995056}]}], "datasetContent": [{"text": "We performed several experiments to evaluate our proposed approach.", "labels": [], "entities": []}, {"text": "Before presenting our results, we describe our datasets as well as two baselines.", "labels": [], "entities": []}, {"text": "Our dataset contains utterances obtained from dialogs between human users and our personal assistant system.", "labels": [], "entities": []}, {"text": "We use the transcribed text forms of  the utterances obtained from (acoustic modeling engine) to train our models 4 . Thus, our dataset contains 18084 NL utterances, 5034 of which are used for measuring the performance of our models.", "labels": [], "entities": []}, {"text": "The dataset consists of five domain classes, i.e, movie, restaurant, hotel, event, other, 42 unique dialog acts and 41 slot tags.", "labels": [], "entities": []}, {"text": "Each utterance is labeled with a domain, dialog act and a sequence of slot tags corresponding to segments in utterance (see examples in).", "labels": [], "entities": []}, {"text": "shows sample dialog act and slot labels.", "labels": [], "entities": []}, {"text": "Annotation agreement, Kappa measure, was around 85%.", "labels": [], "entities": [{"text": "Annotation agreement", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9460989534854889}]}, {"text": "We pulled a month of web query logs and extracted over 2 million search queries from the movie, hotel, event, and restaurant domains.", "labels": [], "entities": []}, {"text": "We also used generic web queries to compile a set of 'other' domain queries.", "labels": [], "entities": []}, {"text": "Our vocabulary consists of n-grams and segments (phrases) in utterances that are extracted using web n-grams and entity lists of \u00a73.", "labels": [], "entities": []}, {"text": "We extract distributions of n-grams and entities to inject as prior weights for entity list base (\u03c8 j E ) and web n-gram context base measures (\u03c8 j G ) (see \u00a74).", "labels": [], "entities": []}, {"text": "We evaluated two baselines and two variants of our joint SLU approach as follows: Sequence-SLU: A traditional approach to SLU extracts domain, dialog act and slots as semantic components of utterances using three sequential models.", "labels": [], "entities": []}, {"text": "Typically, domain and dialog act detection models are taken as query classification, where a given NL query is assigned domain and act labels.", "labels": [], "entities": [{"text": "dialog act detection", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6201179126898447}, {"text": "query classification", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7357591688632965}]}, {"text": "Among supervised query classification meth-  ods, we used the Adaboost, utterance classification method that starts from a set of weak classifiers and builds a strong classifier by boosting the weak classifiers.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.6576080620288849}]}, {"text": "Slot discovery is taken as a sequence labeling task in which segments in utterances are labeled.", "labels": [], "entities": [{"text": "Slot discovery", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9625382721424103}]}, {"text": "For segment labeling we use SemiMarkov Conditional Random Fields (Semi-CRF) () method as a benchmark in evaluating semantic tagging performance.", "labels": [], "entities": [{"text": "segment labeling", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8441901803016663}, {"text": "semantic tagging", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.7192621529102325}]}, {"text": "Tri-CRF: We used Triangular Chain CRF (Jeong and as our supervised joint model baseline.", "labels": [], "entities": []}, {"text": "It is a state-of-the art method that learns the sequence labels and utterance class (domain or dialog act) as meta-sequence in a joint framework.", "labels": [], "entities": []}, {"text": "It encodes the inter-dependence between the slot sequence sand meta-sequence label (d or a) using a triangular chain (dual-layer) structure.", "labels": [], "entities": []}, {"text": "Base-MCM: Our first version injects an informative prior for domain, dialog act and slot topic distributions using information extracted from only labeled training utterances and inject as prior constraints (corpus n-gram base measure \u03c8 j C ) during topic assignments.", "labels": [], "entities": []}, {"text": "WebPrior-MCM: Our full model encodes distributions extracted from labeled training data as well as structured web logs as asymmetric Dirichlet priors.", "labels": [], "entities": []}, {"text": "We analyze performance gain by the information from web sources (\u03c8 j G and \u03c8 j E ) when injected into our approach compared to Base-MCM.", "labels": [], "entities": []}, {"text": "We inject dictionary constraints as features to train supervised discriminative methods, i.e., boosting and Semi-CRF in Sequence-SLU, and Tri-CRF models.", "labels": [], "entities": []}, {"text": "For semantic tagging, dictionary constraints apply to the features between individual segments and their labels, and for utterance classification (to predict domain and dialog acts) they apply to the features between utterance and its label.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7185267210006714}, {"text": "utterance classification", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.8940651714801788}]}, {"text": "Given a list of dictionaries, these constraints specify which label is more likely.", "labels": [], "entities": []}, {"text": "For discriminative methods, we use several named entities, e.g., Movie-Name, Restaurant-Name, Hotel-Name, etc., non-named entities, e.g., Genre, Cuisine, etc., and domain independent dictionaries, e.g., Time, Location, etc.", "labels": [], "entities": []}, {"text": "We train domain and dialog act classifiers via Icsiboost () with 10K iterations using lexical features (up to 3-n-grams) and constraining dictionary features (all dictionaries).", "labels": [], "entities": []}, {"text": "For feature templates of sequence learners, i.e., Semi-CRF and Tri-CRF, we use current word, bi-gram and dictionary features.", "labels": [], "entities": []}, {"text": "For Base-MCM and WebPrior-MCM, we run Gibbs sampler for 2000 iterations with the first 500 samples as burn-in.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our joint model on two experiments using two metrics.", "labels": [], "entities": []}, {"text": "For domain and dialog act detection performance we present results inaccuracy, and for slot detection we use the F1 pairwise measure.", "labels": [], "entities": [{"text": "dialog act detection", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.6194702883561453}, {"text": "slot detection", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.8563652336597443}, {"text": "F1 pairwise measure", "start_pos": 113, "end_pos": 132, "type": "METRIC", "confidence": 0.9259601831436157}]}, {"text": "Encoding Prior Knowledge: A common evaluation method in SLU tasks is to measure the performance of each individual semantic model, i.e., domain, dialog act and semantic tagging (slot filling).", "labels": [], "entities": [{"text": "SLU tasks", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8938031196594238}, {"text": "semantic tagging", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.7056427299976349}, {"text": "slot filling", "start_pos": 178, "end_pos": 190, "type": "TASK", "confidence": 0.703184187412262}]}, {"text": "Here, we not only want to demonstrate the performance of each component of MCM but also their performance under limited amount of labeled data.", "labels": [], "entities": []}, {"text": "We randomly select subsets of labeled training data U i L \u2282 UL with different samples sizes, n i L ={\u03b3 * n L }, where n L represents the sample size of UL and \u03b3={10%,25%,..} is the subset percentage.", "labels": [], "entities": []}, {"text": "At each random selection, the rest of the utterances are used as unlabeled data to boost the performance of MCM.", "labels": [], "entities": [{"text": "MCM", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.8711833357810974}]}, {"text": "The supervised baselines do not leverage the unlabeled utterances.", "labels": [], "entities": []}, {"text": "The results reported in reveal both the strengths and some shortcomings of our approach.", "labels": [], "entities": []}, {"text": "When the number of labeled data is small (n i L \u226425%*n L ), our WebPrior-MCM has a better performance on domain and act predictions compared to the two baselines.", "labels": [], "entities": []}, {"text": "Compared to Sequence-SLU, we observe 4.5% and 3% performance improvement on the domain and dialog act models, whereas our gain is 2.6% and 1.7% over Tri-CRF models.", "labels": [], "entities": []}, {"text": "As the percentage of labeled utterances in training data increase, Tri-CRF performance increases, however WebPrior-MCM is still comparable with Sequence-SLU.", "labels": [], "entities": []}, {"text": "This is because we utilize domain priors obtained from the web sources as supervision during generative process as well as unlabeled utterances that enable handling language variability.", "labels": [], "entities": [{"text": "generative process", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8953213095664978}]}, {"text": "Adding labeled data improves the performance of all models however supervised models benefit more compared to MCM models.", "labels": [], "entities": []}, {"text": "Although WebPrior-MCM's domain and dialog act performances are comparable (if not better than) the other baselines, it falls short on the semantic tagging model.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.684985414147377}]}, {"text": "This is partially due to the HMM assumption compared to the supervised conditional model's used in the other baselines, i.e.,.", "labels": [], "entities": []}, {"text": "Our work can be extended by replacing HMM assumption with CRF based sequence learner to enhance the capability of the sequence tagging component of MCM.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.6714834123849869}]}, {"text": "Being Bayesian, our model can incorporate unlabeled data at training time.", "labels": [], "entities": []}, {"text": "Here, we evaluate the performance gain on domain, act and slot predictions as more unlabeled data is introduced at learning time.", "labels": [], "entities": []}, {"text": "We use only 10% of the utterances as labeled data in this experiment and incrementally add unlabeled data (90% of labeled data are treated as unlabeled).", "labels": [], "entities": []}, {"text": "The results are shown in. n% (n=10,25,..) unlabeled data indicates that the WebPrior-MCM is trained using n% of unlabeled utterances along with training utterances.", "labels": [], "entities": []}, {"text": "Adding unlabeled data has a positive impact on the performance of all three se- mantic components when WebPrior-MCM is used.", "labels": [], "entities": []}, {"text": "The results show that our joint modeling approach has an advantage over the other joint models (i.e., Tri-CRF) in that it can leverage unlabeled NL utterances.", "labels": [], "entities": []}, {"text": "Our approach might be usefully extended into the area of understanding search queries, where an abundance of unlabeled queries is observed.", "labels": [], "entities": [{"text": "understanding search queries", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8405118981997172}]}], "tableCaptions": [{"text": " Table 3:  Performance evaluation results of  WebPrior-MCM using different sizes of unlabeled  utterances at learning time.  Unlabeled Domain Dialog Act  Slot  %  Accuracy Accuracy F-Measure  10%  94.69  84.17  52.61  25%  94.89  84.29  54.22  50%  95.08  84.39  56.58  75%  95.19  84.44  57.45  100%  95.28  84.52  58.18", "labels": [], "entities": [{"text": "Accuracy Accuracy F-Measure", "start_pos": 163, "end_pos": 190, "type": "METRIC", "confidence": 0.8259884715080261}]}]}