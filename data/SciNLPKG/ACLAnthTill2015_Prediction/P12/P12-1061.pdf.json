{"title": [{"text": "Community Answer Summarization for Multi-Sentence Question with Group L 1 Regularization", "labels": [], "entities": [{"text": "Community Answer Summarization", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7118669748306274}, {"text": "Regularization", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.8355674147605896}]}], "abstractContent": [{"text": "We present a novel answer summarization method for community Question Answering services (cQAs) to address the problem of \"in-complete answer\", i.e., the \"best answer\" of a complex multi-sentence question misses valuable information that is contained in other answers.", "labels": [], "entities": [{"text": "answer summarization", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7139626145362854}, {"text": "Question Answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.664010688662529}]}, {"text": "In order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field (CRF) based answer summary method with group L 1 regularization.", "labels": [], "entities": []}, {"text": "Various textual and non-textual QA features are explored.", "labels": [], "entities": []}, {"text": "Specifically, we explore four different types of contextual factors, namely, the information novelty and non-redundancy mod-eling for local and non-local sentence interactions under question segmentation.", "labels": [], "entities": [{"text": "question segmentation", "start_pos": 182, "end_pos": 203, "type": "TASK", "confidence": 0.7246166318655014}]}, {"text": "To further unleash the potential of the abundant cQA features, we introduce the group L 1 regu-larization for feature learning.", "labels": [], "entities": []}, {"text": "Experimental results on a Yahoo!", "labels": [], "entities": []}, {"text": "Answers dataset show that our proposed method significantly outper-forms state-of-the-art methods on cQA sum-marization task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community Question and Answering services (cQAs) have become valuable resources for users to pose questions of their interests and share their knowledge by providing answers to questions.", "labels": [], "entities": [{"text": "Community Question and Answering", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5251755565404892}]}, {"text": "They perform much better than the traditional frequently asked questions (FAQ) systems (Jijkoun and) which are just based on natural language processing and information retrieving technologies due to the need for human intelligence in user generated contents(.", "labels": [], "entities": [{"text": "Jijkoun", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.9297392964363098}, {"text": "information retrieving", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.7038522362709045}]}, {"text": "In cQAs such as Yahoo!", "labels": [], "entities": []}, {"text": "Answers, a resolved question often gets more than one answers and a \"best answer\" will be chosen by the asker or voted by other community participants.", "labels": [], "entities": []}, {"text": "This {question, best answer} pair is then stored and indexed for further uses such as question retrieval.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7936326861381531}]}, {"text": "It performs very well in simple factoid QA settings, where the answers to factoid questions often relate to a single named entity like a person, time or location.", "labels": [], "entities": []}, {"text": "However, when it comes to the more sophisticated multisentence questions, it would suffer from the problem of \"incomplete answer\".", "labels": [], "entities": []}, {"text": "That is, such question often comprises several sub questions in specific contexts and the asker wishes to get elaborated answers for as many aspects of the question as possible.", "labels": [], "entities": []}, {"text": "In which case, the single best answer that covers just one or few aspects may not be a good choice (.", "labels": [], "entities": []}, {"text": "Since \"everyone knows something\" (, the use of a single best answer often misses valuable human generated information contained in other answers.", "labels": [], "entities": []}, {"text": "In an early literature,  reported that no more than 48% of the 400 best answers were indeed the unique best answers in 4 most popular Yahoo!", "labels": [], "entities": []}, {"text": "shows an example of the \"incomplete answer\" problem from Yahoo!", "labels": [], "entities": []}, {"text": "Answers . The asker wishes to know why his teeth bloods and how to prevent it.", "labels": [], "entities": []}, {"text": "However, the best answer only gives information on the reason of teeth blooding.", "labels": [], "entities": []}, {"text": "It is clear that some valuable information about the reasons of gums blooding and some solutions are presented in other answers.", "labels": [], "entities": [{"text": "gums blooding", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.7609449326992035}]}], "datasetContent": [{"text": "To evaluate the performance of our CRF based answer summarization model, we conduct experiments on the Yahoo!", "labels": [], "entities": [{"text": "CRF based answer summarization", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.5898407101631165}]}, {"text": "W ebscope TM Program 4 opens up a number of Yahoo!", "labels": [], "entities": [{"text": "W ebscope TM Program 4", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.9152792811393737}]}, {"text": "Answers datasets for interested academics in different categories.", "labels": [], "entities": []}, {"text": "Our original dataset contains 1,300,559 questions and 2,770,896 answers in ten taxonomies from Yahoo!", "labels": [], "entities": []}, {"text": "After filtering the questions which have less than 5 answers and some trivial factoid questions using the features by , we reduce the dataset to 55,132 questions.", "labels": [], "entities": []}, {"text": "From this sub-set, we next select the questions with incomplete answers as defined in Section 2.1.", "labels": [], "entities": []}, {"text": "Specifically, we select the questions where the average similarity between the best answer and all sub questions is less than 0.6 or when the star rating of the best answer is less than 4.", "labels": [], "entities": []}, {"text": "We obtain 7,784 questions after this step.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of this method, we randomly choose 400 questions in the filtered dataset and invite 10 graduate candidate students (not in NLP research field) to verify whether a question suffers from the incomplete answer problem.", "labels": [], "entities": []}, {"text": "We divide the students into five groups of two each.", "labels": [], "entities": []}, {"text": "We consider the questions as the \"incomplete answer questions\" only when they are judged by both members in a group to be the case.", "labels": [], "entities": []}, {"text": "As a result, we find that 360 (90%) of these questions indeed suffer from the incomplete answer problem, which indicates that our automatic detection method is efficient.", "labels": [], "entities": []}, {"text": "This randomly selected 400 questions along with their 2559 answers are then further manually summarized for evaluation of automatically generated answer summaries by our model in experiments.", "labels": [], "entities": []}, {"text": "When taking the summarization as a sequential biclassification problem, we can make use of the usual precision, recall and F1 measures for classification accuracy evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9827526807785034}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9994732737541199}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9959656000137329}, {"text": "F1", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9948391318321228}, {"text": "classification", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.9516772031784058}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.7266491651535034}]}, {"text": "In our experiments, we also compare the precision, recall and F1 score in the ROUGE-1, ROUGE-2 and ROUGE-L measures) for answer summarization performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9997175335884094}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.998794674873352}, {"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9837810695171356}, {"text": "ROUGE-1", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9315263628959656}, {"text": "ROUGE-2", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.6953045129776001}, {"text": "answer summarization", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.8107912540435791}]}, {"text": "For group L 1 regularization term, we set the \u03b5 = 10 \u22124 in Equation 6.", "labels": [], "entities": [{"text": "Equation", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8892738223075867}]}, {"text": "To see how much the different textual and non-textual features contribute to community answer summarization, the accumulated weight of each group of sentence-level features 5 is presented in.", "labels": [], "entities": [{"text": "community answer summarization", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.5924588640530905}]}, {"text": "It shows that the textual features such as 1 (Sentence Length), 2 (Position) 3 (Answer Length), 6 (Has Link) and non-textual features such as 8 (Best Answer Star) , 12 (Total Answer Number) as well as 13 (Total Points) have larger weights, which play a significant role in the summarization task as we intuitively considered; features 4 (Stopwords Rate), 5 (Uppercase Rate) and 9 (Thumbs Up) have medium weights relatively; and the other features like 7 (Similarity to Question), 10 (Author Level) and 11 (Best Answer Rate) have the smallest accumulated weights.", "labels": [], "entities": [{"text": "Total Answer Number)", "start_pos": 169, "end_pos": 189, "type": "METRIC", "confidence": 0.7883495092391968}, {"text": "summarization", "start_pos": 277, "end_pos": 290, "type": "TASK", "confidence": 0.990699827671051}]}, {"text": "The main reasons that the feature 7 (Similarity to Question) has low contribution is that we have utilized the similarity to question in the contextual factors, and this similarity feature in the single site becomes redundant.", "labels": [], "entities": []}, {"text": "Similarly, the features Author Level and Best Answer Number are likely to be redundant when other non-textual features are presented together.", "labels": [], "entities": [{"text": "Best Answer Number", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.6341866552829742}]}, {"text": "The experimental results demonstrate that with the use of group L 1 -regularization we have learnt better combination of these features.", "labels": [], "entities": []}, {"text": "Note that we have already evaluated the contribution of the contextual factors in Section 5.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The Precision, Recall and F1 of ROUGE-1, ROUGE-2, ROUGE-L in the baselines SVM,LR, LCRF and our  general CRF based models (gCRF, gCRF-QS, gCRF-QS-l1). The down-arrow means performance degradation with  statistical significance.", "labels": [], "entities": [{"text": "Precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9993002414703369}, {"text": "Recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9978526830673218}, {"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9976605176925659}]}, {"text": " Table 2: The Precision, Recall and F1 measures of the  baselines SVM,LR, LCRF and our general CRF based  models (gCRF, gCRF-QS, gCRF-QS-l1). The up-arrow  denotes the performance improvement compared to the  precious method (above) with statistical significance un- der p value of 0.05, the short line '-' denotes there is no  difference in statistical significance.", "labels": [], "entities": [{"text": "Precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9990135431289673}, {"text": "Recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9964136481285095}, {"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9980643391609192}]}]}