{"title": [{"text": "Modeling Sentences in the Latent Space", "labels": [], "entities": [{"text": "Modeling Sentences in the Latent Space", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8254016836484274}]}], "abstractContent": [{"text": "Sentence Similarity is the process of computing a similarity score between two sentences.", "labels": [], "entities": [{"text": "Sentence Similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8776658773422241}]}, {"text": "Previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7330993413925171}]}, {"text": "In this paper, we show that by carefully handling words that are not in the sentences (missing words), we can train a reliable latent variable model on sentences.", "labels": [], "entities": []}, {"text": "In the process, we propose anew evaluation framework for sentence similarity: Concept Definition Retrieval.", "labels": [], "entities": [{"text": "Concept Definition Retrieval", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.6233219703038534}]}, {"text": "The new framework allows for large scale tuning and testing of Sentence Similarity models.", "labels": [], "entities": [{"text": "Sentence Similarity", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8827396333217621}]}, {"text": "Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models.", "labels": [], "entities": []}, {"text": "Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7061820030212402}]}], "introductionContent": [{"text": "Identifying the degree of semantic similarity between two sentences is at the core of many NLP applications that focus on sentence level semantics such as Machine Translation), Summarization (), Text Coherence Detection (Lapata and), etc.To date, almost all Sentence Similarity approaches work in the high-dimensional word space and rely mainly on word similarity.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.8078874945640564}, {"text": "Summarization", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.9785464406013489}, {"text": "Text Coherence Detection", "start_pos": 195, "end_pos": 219, "type": "TASK", "confidence": 0.6297548512617747}, {"text": "Sentence Similarity", "start_pos": 258, "end_pos": 277, "type": "TASK", "confidence": 0.914077877998352}]}, {"text": "There are two main (not unrelated) disadvantages to word similarity based approaches: 1.", "labels": [], "entities": []}, {"text": "lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; 2.", "labels": [], "entities": []}, {"text": "word co-occurrence information is not sufficiently exploited.", "labels": [], "entities": []}, {"text": "Latent variable models, such as Latent Semantic Analysis (), Probabilistic Latent Semantic Analysis (, Latent Dirichlet Allocation [LDA] ( can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6061219473679861}, {"text": "Probabilistic Latent Semantic Analysis", "start_pos": 61, "end_pos": 99, "type": "TASK", "confidence": 0.5499597117304802}]}, {"text": "However, attempts at addressing SS using LSA perform significantly below high dimensional word similarity based models (.", "labels": [], "entities": [{"text": "addressing SS", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.8083130121231079}]}, {"text": "We believe that the latent semantics approaches applied to date to the SS problem have not yielded positive results due to the deficient modeling of the sparsity in the semantic space.", "labels": [], "entities": [{"text": "SS problem", "start_pos": 71, "end_pos": 81, "type": "TASK", "confidence": 0.9148814082145691}]}, {"text": "SS operates in a very limited contextual setting where the sentences are typically very short to derive robust latent semantics.", "labels": [], "entities": [{"text": "SS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9545499682426453}]}, {"text": "Apart from the SS setting, robust modeling of the latent semantics of short sentences/texts is becoming a pressing need due to the pervasive presence of more bursty data sets such as Twitter feeds and SMS where short contexts are an inherent characteristic of the data.", "labels": [], "entities": []}, {"text": "In this paper, we propose to model the missing words (words that are not in the sentence), a feature that is typically overlooked in the text modeling literature, to address the sparseness issue for the SS task.", "labels": [], "entities": [{"text": "SS task", "start_pos": 203, "end_pos": 210, "type": "TASK", "confidence": 0.9319334626197815}]}, {"text": "We define the missing words of a sentence as the whole vocabulary in a corpus minus the observed words in the sentence.", "labels": [], "entities": []}, {"text": "Our intuition is since observed words in a sentence are too few to tell us what the sentence is about, missing words can be used to tell us what the sentence is not about.", "labels": [], "entities": []}, {"text": "We assume that the semantic space of both the observed and missing words makeup the complete semantics profile of a sentence.", "labels": [], "entities": []}, {"text": "After analyzing the way traditional latent variable models (LSA, PLSA/LDA) handle missing words, we decide to model sentences using a weighted matrix factorization approach, which allows us to treat observed words and missing words differently.", "labels": [], "entities": []}, {"text": "We handle missing words using a weighting scheme that distinguishes missing words from observed words yielding robust latent vectors for sentences.", "labels": [], "entities": []}, {"text": "Since we use a feature that is already implied by the text itself, our approach is very general (similar to LSA/LDA) in that it can be applied to any format of short texts.", "labels": [], "entities": []}, {"text": "In contrast, existing work on modeling short texts focuses on exploiting additional data, e.g., model tweets using their metadata (author, hashtag, etc.).", "labels": [], "entities": []}, {"text": "Moreover in this paper, we introduce anew evaluation framework for SS: Concept Definition Retrieval (CDR).", "labels": [], "entities": [{"text": "Concept Definition Retrieval (CDR)", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.7116473962863287}]}, {"text": "Compared to existing data sets, the CDR data set allows for large scale tuning and testing of SS modules without further human annotation.", "labels": [], "entities": [{"text": "CDR data set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9389627973238627}]}], "datasetContent": [{"text": "We need to show the impact of our proposed model WTMF on the SS task.", "labels": [], "entities": [{"text": "SS task", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9317561089992523}]}, {"text": "However we are faced with a problem, the lack of a suitable large evaluation set from which we can derive robust observations.", "labels": [], "entities": []}, {"text": "The two data sets we know of for SS are: 1.", "labels": [], "entities": [{"text": "SS", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9811424612998962}]}, {"text": "human-rated sentence pair similarity data set (); 2.", "labels": [], "entities": [{"text": "human-rated sentence pair similarity data set", "start_pos": 0, "end_pos": 45, "type": "DATASET", "confidence": 0.6291045993566513}]}, {"text": "the Microsoft Research Paraphrase Corpus ().", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus", "start_pos": 4, "end_pos": 40, "type": "DATASET", "confidence": 0.9263546764850616}]}, {"text": "The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary.", "labels": [], "entities": [{"text": "LI06 data set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9540634552637736}, {"text": "Collin Cobuild Dictionary", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.9779849648475647}]}, {"text": "A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed.", "labels": [], "entities": [{"text": "LI06", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8485826849937439}, {"text": "similarity", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.972652792930603}]}, {"text": "While this is the ideal data set for SS, the small size makes it impossible for tuning SS algorithms or deriving significant performance conclusions.", "labels": [], "entities": [{"text": "SS", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9931665062904358}]}, {"text": "On the other hand, the MSR04 data set comprises a much larger set of sentence pairs: 4,076 training and 1,725 test pairs.", "labels": [], "entities": [{"text": "MSR04 data set", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9705253839492798}]}, {"text": "The ratings on the pairs are binary labels: similar/not similar.", "labels": [], "entities": []}, {"text": "This is not a problem per se, however the issue is that it is very strict in its assignment of a positive label, for example the following sentence pair as cited in) is rated not semantically similar: Ballmer has been vocal in the past warning that Linux is a threat to Microsoft.", "labels": [], "entities": []}, {"text": "In the memo, Ballmer reiterated the open-source threat to Microsoft.", "labels": [], "entities": [{"text": "Microsoft", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9313579797744751}]}, {"text": "We believe that the ratings on a data set for SS should accommodate variable degrees of similarity with various ratings, however such a large scale set does not exist yet.", "labels": [], "entities": [{"text": "SS", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9837189316749573}]}, {"text": "Therefore for purposes of evaluating our proposed approach we devise anew framework inspired by the LI06 data set in that it comprises concept definitions but on a large scale.", "labels": [], "entities": [{"text": "LI06 data set", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9839798410733541}]}, {"text": "We evaluate WTMF on three data sets: 1.", "labels": [], "entities": [{"text": "WTMF", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.5710813403129578}]}, {"text": "CDR data set using ATOP metric; 2.", "labels": [], "entities": [{"text": "CDR data set", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9261478583017985}, {"text": "ATOP", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9094305038452148}]}, {"text": "Human-rated Sentence Similarity data set [LI06] using Pearson and Spearman Correlation; 3.", "labels": [], "entities": [{"text": "Sentence Similarity data set [LI06]", "start_pos": 12, "end_pos": 47, "type": "DATASET", "confidence": 0.8449288180896214}, {"text": "Pearson", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.8559109568595886}]}, {"text": "MSR Paraphrase corpus [MSR04] using accuracy.", "labels": [], "entities": [{"text": "MSR Paraphrase corpus [MSR04", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8657660961151123}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9989948868751526}]}, {"text": "The performance of WTMF on CDR is compared with (a) an Information Retrieval model (IR) that is based on surface word matching, (b) an ngram model (N-gram) that captures phrase overlaps by returning the number of overlapping ngrams as the similarity score of two sentences, (c) LSA that uses svds() function in Matlab, and (d) LDA that uses Gibbs Sampling for inference ().", "labels": [], "entities": []}, {"text": "WTMF is also compared with all existing reported SS results on LI06 and MSR04 data sets, as well as LDA that is trained on the same data as WTMF.", "labels": [], "entities": [{"text": "WTMF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8483269214630127}, {"text": "LI06 and MSR04 data sets", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.8134151339530945}, {"text": "WTMF", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.9331177473068237}]}, {"text": "The similarity of two sentences is computed by cosine similarity (except N-gram).", "labels": [], "entities": []}, {"text": "More details on each task will be explained in the subsections.", "labels": [], "entities": []}, {"text": "To eliminate randomness in statistical models (WTMF and LDA), all the reported results are averaged over 10 runs.", "labels": [], "entities": [{"text": "WTMF", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.840022623538971}]}, {"text": "We run 20 iterations for WTMF.", "labels": [], "entities": [{"text": "WTMF", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8560629487037659}]}, {"text": "And we run 5000 iterations for LDA; each LDA model is averaged over the last 10 Gibbs Sampling iterations to get more robust predictions.", "labels": [], "entities": []}, {"text": "The latent vector of a sentence is computed by: (1) using equation 4 in WTMF, or (2) summing up the latent vectors of all the constituent words weighted by X ij in LSA and LDA, similar to the work reported in ().", "labels": [], "entities": [{"text": "WTMF", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9378586411476135}]}, {"text": "For LDA the latent vector of a word is computed by P (z|w).", "labels": [], "entities": []}, {"text": "It is worth noting that we could directly use the estimated topic distribution \u03b8 j to represent a sentence, however, as discussed the topic distribution has only non-zero values on one or two topics, leading to a low ATOP value around 0.8.", "labels": [], "entities": [{"text": "ATOP", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9983944296836853}]}], "tableCaptions": [{"text": " Table 1: Three possible latent vectors hypotheses for the definition of bank#n#1", "labels": [], "entities": []}, {"text": " Table 2: ATOP Values of Models (K = 100 for LSA/LDA/WTMF)", "labels": [], "entities": [{"text": "ATOP", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7977772355079651}]}, {"text": " Table 3. In addition, to demonstrate that w m does  not overfit the 30 data points, we also evaluate on", "labels": [], "entities": []}, {"text": " Table 4: Pearson's correlation r and Spearman's corre-", "labels": [], "entities": [{"text": "Pearson's correlation r", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.7710099369287491}, {"text": "Spearman's corre", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.6283732950687408}]}, {"text": " Table 5: Performance on MSR04 test set", "labels": [], "entities": [{"text": "MSR04 test set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9680390357971191}]}]}