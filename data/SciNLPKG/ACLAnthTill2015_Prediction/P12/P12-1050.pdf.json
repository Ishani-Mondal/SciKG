{"title": [{"text": "Modified Distortion Matrices for Phrase-Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.7992626130580902}]}], "abstractContent": [{"text": "This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.8742782771587372}]}, {"text": "We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena.", "labels": [], "entities": []}, {"text": "Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.9657008647918701}]}, {"text": "Finally we encode these reorder-ings by modifying selected entries of the distortion cost matrix, on a per-sentence basis.", "labels": [], "entities": []}, {"text": "In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit.", "labels": [], "entities": []}, {"text": "The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.990043044090271}]}], "introductionContent": [{"text": "Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.715172603726387}, {"text": "SMT", "start_pos": 148, "end_pos": 151, "type": "TASK", "confidence": 0.9763126373291016}]}, {"text": "On one hand, the phrase-based approach (PSMT), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency.", "labels": [], "entities": []}, {"text": "On the other, tree-based approaches gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders.", "labels": [], "entities": []}, {"text": "Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules.", "labels": [], "entities": []}, {"text": "Notable examples are the family-unrelated Arabic-English and the related German-English language pairs.", "labels": [], "entities": []}, {"text": "Interestingly, on these pairs, PSMT generally prevails over tree-based SMT 1 , producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning.", "labels": [], "entities": [{"text": "PSMT", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.8106839656829834}, {"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9466233849525452}]}, {"text": "Previous works on this type of language pairs have mostly focused on source reordering prior to translation (), or on sophisticated reordering models integrated into decoding (), achieving mixed results.", "labels": [], "entities": []}, {"text": "To merge the best of both approaches -namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter -we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed fora given input sentence.", "labels": [], "entities": []}, {"text": "Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informed definition of the search space that simplifies the task of the in-decoder reordering model, besides decreasing its complexity.", "labels": [], "entities": [{"text": "low distortion limit (DL)", "start_pos": 60, "end_pos": 85, "type": "METRIC", "confidence": 0.8580918610095978}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "After reviewing a selection of relevant works, we analyze salient reordering patterns in Arabic-English and GermanEnglish, and describe the corresponding chunkbased reordering rule sets.", "labels": [], "entities": []}, {"text": "In the following sections we present a reordering selection technique based on reordered n-gram LMs and, finally, explain the notion of modified distortion matrices.", "labels": [], "entities": []}, {"text": "In the last part of the paper, we evaluate the proposed techniques on two popular MT tasks.", "labels": [], "entities": [{"text": "MT tasks", "start_pos": 82, "end_pos": 90, "type": "TASK", "confidence": 0.9484155178070068}]}], "datasetContent": [{"text": "In this section we evaluate the impact of modified distortion matrices on two news translation tasks.", "labels": [], "entities": [{"text": "news translation", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.6846283823251724}]}, {"text": "Matrices were integrated into the Moses toolkit () using a sentencelevel XML markup.", "labels": [], "entities": []}, {"text": "The list of word shortcuts for each sentence is provided as an XML tag that is parsed by the decoder to modify the distortion matrix just before starting the search.", "labels": [], "entities": []}, {"text": "As usual, the distortion matrix is queried by the distortion penalty generator and by the hypothesis expander 9 .  For Arabic-English, we use the union of all indomain parallel corpora provided for the NIST-MT09 evaluation 10 fora total of 986K sentences, 31M English words.", "labels": [], "entities": []}, {"text": "The target LM is trained on the English side of all available NIST-MT09 parallel data, UN included (147M words).", "labels": [], "entities": [{"text": "NIST-MT09 parallel data", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.8780916929244995}]}, {"text": "For development and test, we use the newswire sections of the NIST benchmarks, hereby called dev06-NW, eval08-NW and eval09-NW: 1033, 813 and 586 sentences, respectively, each provided with four reference translations.", "labels": [], "entities": [{"text": "NIST benchmarks", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.968526691198349}]}, {"text": "The German-English system is instead trained on WMT10 data: namely Europarl (v.5) plus Newscommentary-2010 fora total of 1.6M parallel sentences, 43M English words.", "labels": [], "entities": [{"text": "WMT10 data", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.948177695274353}, {"text": "Europarl", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9711577296257019}, {"text": "Newscommentary-2010", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.8911069631576538}]}, {"text": "The target LM is trained on the monolingual news data provided for the constrained track (1133M words).", "labels": [], "entities": []}, {"text": "For development and test, we use the WMT10 news benchmarks test08, test09 and test10: 2051, 2525 and 2489 sentences, respectively, with one reference translation.", "labels": [], "entities": [{"text": "WMT10 news benchmarks test08", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.93319171667099}]}, {"text": "Concerning pre-processing, we apply standard tokenization to the English data, while for Arabic we use our in-house tokenizer that removes diacritics and normalizes special characters.", "labels": [], "entities": []}, {"text": "Arabic text is then segmented with AMIRA () according to the ATB scheme . German tokenization and compound splitting is performed with Tree Tagger ( and the Gertwol morphological analyser ( . Using Moses we build competitive baselines on the training data described above.", "labels": [], "entities": [{"text": "AMIRA", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.945213794708252}, {"text": "tokenization", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.6514701843261719}, {"text": "compound splitting", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7058303356170654}]}, {"text": "Word alignment is produced by the Berkeley Aligner ().", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.707144096493721}]}, {"text": "The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7601369917392731}]}, {"text": "The reordering model is a hierarchical phrase orientation model) trained on all the available parallel data.", "labels": [], "entities": []}, {"text": "We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task.", "labels": [], "entities": []}, {"text": "Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks.", "labels": [], "entities": []}, {"text": "The DL is initially set to 5 words for Arabic-English and to 10 for German-English.", "labels": [], "entities": []}, {"text": "According to our experience, these are the optimal settings for the evaluated tasks.", "labels": [], "entities": []}, {"text": "Feature weights are optimized by minimum error training on the development sets (dev06-NW and test08).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Impact of modified distortion matrices on translation quality, measured with BLEU, METEOR and KRS  (all in percentage form, higher scores mean higher quality). The settings used for weight tuning are marked with  \u2020.  Statistically significant differences wrt the baseline are marked with \u2022 at the p \u2264 .05 level and \u2022 at the p \u2264 .10 level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9990033507347107}, {"text": "METEOR", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9932796359062195}, {"text": "KRS", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9458641409873962}, {"text": "weight tuning", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.7779603600502014}]}]}