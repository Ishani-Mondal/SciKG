{"title": [{"text": "Utilizing Dependency Language Models for Graph-based Dependency Parsing Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach to enriching high-order feature representations for graph-based dependency parsing models using a dependency language model and beam search.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7255209237337112}]}, {"text": "The dependency language model is built on a large-amount of additional auto-parsed data that is processed by a baseline parser.", "labels": [], "entities": []}, {"text": "Based on the dependency language model, we represent a set of features for the parsing model.", "labels": [], "entities": []}, {"text": "Finally, the features are efficiently integrated into the parsing model during decoding using beam search.", "labels": [], "entities": []}, {"text": "Our approach has two advantages.", "labels": [], "entities": []}, {"text": "Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus.", "labels": [], "entities": []}, {"text": "Secondly our approach does not increase the decoding complexity.", "labels": [], "entities": []}, {"text": "We evaluate the proposed approach on English and Chinese data.", "labels": [], "entities": []}, {"text": "The experimental results show that our new parser achieves the best accuracy on the Chi-nese data and comparable accuracy with the best known systems on the English data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9990769624710083}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9993095397949219}, {"text": "English data", "start_pos": 157, "end_pos": 169, "type": "DATASET", "confidence": 0.8702794015407562}]}], "introductionContent": [{"text": "In recent years, there are many data-driven models that have been proposed for dependency parsing . Among them, graphbased dependency parsing models have achieved state-of-the-art performance fora wide range of languages as shown in recent CoNLL shared tasks * Corresponding author).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8530697226524353}, {"text": "dependency parsing", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.6868240684270859}]}, {"text": "In the graph-based models, dependency parsing is treated as a structured prediction problem in which the graphs are usually represented as factored structures.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8591713905334473}]}, {"text": "The information of the factored structures decides the features that the models can utilize.", "labels": [], "entities": []}, {"text": "There are several previous studies that exploit high-order features that lead to significant improvements. and develop models that represent first-order features over a single arc in graphs.", "labels": [], "entities": []}, {"text": "By extending the firstorder model, and exploit second-order features over two adjacent arcs in second-order models.", "labels": [], "entities": []}, {"text": "further propose a third-order model that uses third-order features.", "labels": [], "entities": []}, {"text": "These models utilize higher-order feature representations and achieve better performance than the first-order models.", "labels": [], "entities": []}, {"text": "But this achievement is at the cost of the higher decoding complexity, from O(n 2 ) to O(n 4 ), where n is the length of the input sentence.", "labels": [], "entities": []}, {"text": "Thus, it is very hard to develop higher-order models further in this way.", "labels": [], "entities": []}, {"text": "How to enrich high-order feature representations without increasing the decoding complexity for graph-based models becomes a very challenging problem in the dependency parsing task.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.8376980423927307}]}, {"text": "In this paper, we solve this issue by enriching the feature representations fora graph-based model using a dependency language model (DLM)).", "labels": [], "entities": []}, {"text": "The N-gram DLM has the ability to predict the next child based on the N-1 immediate previous children and their head).", "labels": [], "entities": []}, {"text": "The basic idea behind is that we use the DLM to evaluate whether a valid dependency tree  is well-formed from a view of large scope.", "labels": [], "entities": []}, {"text": "The parsing model searches for the final dependency trees by considering the original scores and the scores of DLM.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9660675525665283}]}, {"text": "In our approach, the DLM is built on a large amount of auto-parsed data, which is processed by an original first-order parser).", "labels": [], "entities": []}, {"text": "We represent the features based on the DLM.", "labels": [], "entities": []}, {"text": "The DLM-based features can capture the N-gram information of the parent-children structures for the parsing model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.9701858162879944}]}, {"text": "Then, they are integrated directly in the decoding algorithms using beam-search.", "labels": [], "entities": []}, {"text": "Our new parsing model can utilize rich high-order feature representations but without increasing the complexity.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of the proposed approach, we conduct experiments on English and Chinese data.", "labels": [], "entities": []}, {"text": "The results indicate that the approach greatly improves the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9987490177154541}]}, {"text": "In summary, we make the following contributions: \u2022 We utilize the dependency language model to enhance the graph-based parsing model.", "labels": [], "entities": []}, {"text": "The DLM-based features are integrated directly into the beam-search decoder.", "labels": [], "entities": []}, {"text": "\u2022 The new parsing model uses the rich high-order features defined over a view of large scope and and additional large raw corpus, but without increasing the decoding complexity.", "labels": [], "entities": []}, {"text": "\u2022 Our parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9989622831344604}, {"text": "Chinese data", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.7821878790855408}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9994478821754456}, {"text": "English data", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.7692243158817291}]}], "datasetContent": [{"text": "We conducted experiments on English and Chinese data.", "labels": [], "entities": []}, {"text": "Since the setting of K (for beam search) affects our parsers, we studied its influence on the development set for English.", "labels": [], "entities": []}, {"text": "We added the DLM-based features to MST1.", "labels": [], "entities": [{"text": "MST1", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9280490875244141}]}, {"text": "shows the UAS curves on the development set, where K is beam size for Intersect and K-best for Rescoring, the X-axis represents K, and the Y-axis represents the UAS scores.", "labels": [], "entities": [{"text": "Rescoring", "start_pos": 95, "end_pos": 104, "type": "TASK", "confidence": 0.8632833361625671}, {"text": "UAS", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.8055108189582825}]}, {"text": "The parsing performance generally increased as the K increased.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9691779017448425}, {"text": "K", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9939196109771729}]}, {"text": "The parser with Intersect always outperformed the one with Rescoring.: The parsing times on the development set (seconds for all the sentences) shows the parsing times of Intersect on the development set for English.", "labels": [], "entities": [{"text": "Rescoring.", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.8245609402656555}]}, {"text": "By comparing the curves of, we can see that, while using larger K reduced the parsing speed, it improved the performance of our parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9505357146263123}]}, {"text": "In the rest of the experiments, we set K=8 in order to obtain the high accuracy with reasonable speed and used Intersect to add the DLM-based features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9988934397697449}]}, {"text": "Then, we studied the effect of adding different Ngram DLMs to MST1.", "labels": [], "entities": [{"text": "MST1", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.7387394905090332}]}, {"text": "From the table, we found that the parsing performance roughly increased as the N increased.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.974152684211731}]}, {"text": "When N=3 and N=4, the parsers obtained the same scores for English.", "labels": [], "entities": []}, {"text": "For Chinese, the parser obtained the best score when N=4.", "labels": [], "entities": []}, {"text": "Note that the size of the Chinese unannotated data was larger than that of English.", "labels": [], "entities": [{"text": "Chinese unannotated data", "start_pos": 26, "end_pos": 50, "type": "DATASET", "confidence": 0.6681287189324697}]}, {"text": "In the rest of the experiments, we used 3-gram for English and 4-gram for Chinese.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The parsing times on the development set (sec- onds for all the sentences)", "labels": [], "entities": []}, {"text": " Table 4: Effect of different N-gram DLMs", "labels": [], "entities": []}, {"text": " Table 5: Main results for English", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9453728199005127}]}, {"text": " Table 5. As in  the English experiments, the parsers using the DLM- based features consistently outperformed the Base- lines. For the basic models (MST1/2), we obtained  absolute improvements of 4.28 and 3.51 points re- spectively. For the enhanced models (MSTB1/2),  we got 3.00 and 2.93 points improvements respec- tively. We obtained large improvements on the Chi- nese data. The reasons may be that we use the very  large amount of data and 4-gram DLM that captures  high-order information. The improvements were  significant in", "labels": [], "entities": []}, {"text": " Table 6: Main results for Chinese", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9130603075027466}]}, {"text": " Table 7: Relevant results for English. G denotes the su- pervised graph-based parsers, S denotes the graph-based  parsers with semi-supervised methods, D denotes our  new parsers", "labels": [], "entities": []}, {"text": " Table 8: Relevant results for Chinese", "labels": [], "entities": [{"text": "Relevant", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9863224029541016}]}]}