{"title": [{"text": "Enhancing Statistical Machine Translation with Character Alignment", "labels": [], "entities": [{"text": "Enhancing Statistical Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.771830826997757}, {"text": "Character Alignment", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6983382999897003}]}], "abstractContent": [{"text": "The dominant practice of statistical machine translation (SMT) uses the same Chinese word segmentation specification in both alignment and translation rule induction steps in building Chinese-English SMT system, which may suffer from a suboptimal problem that word seg-mentation better for alignment is not necessarily better for translation.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.7816370526949564}, {"text": "translation rule induction", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.7267974615097046}, {"text": "SMT", "start_pos": 200, "end_pos": 203, "type": "TASK", "confidence": 0.8667665123939514}]}, {"text": "To tackle this, we propose a framework that uses two different segmenta-tion specifications for alignment and translation respectively: we use Chinese character as the basic unit for alignment, and then convert this alignment to conventional word alignment for translation rule induction.", "labels": [], "entities": [{"text": "translation", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.9263242483139038}, {"text": "translation rule induction", "start_pos": 261, "end_pos": 287, "type": "TASK", "confidence": 0.9002112547556559}]}, {"text": "Experimentally, our approach outperformed two baselines: fully word-based system (using word for both alignment and translation) and fully character based system, in terms of alignment quality and translation performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Chinese Word segmentation is a necessary step in Chinese-English statistical machine translation (SMT) because Chinese sentences do not delimit words by spaces.", "labels": [], "entities": [{"text": "Chinese Word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5353811780611674}, {"text": "Chinese-English statistical machine translation (SMT)", "start_pos": 49, "end_pos": 102, "type": "TASK", "confidence": 0.7637817008154733}]}, {"text": "The key characteristic of a Chinese word segmenter is the segmentation specification . As depicted in, the dominant practice of SMT uses the same word segmentation for both word alignment and translation rule induction.", "labels": [], "entities": [{"text": "Chinese word segmenter", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.6559609572092692}, {"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9909055233001709}, {"text": "word alignment", "start_pos": 173, "end_pos": 187, "type": "TASK", "confidence": 0.7811317443847656}, {"text": "translation rule induction", "start_pos": 192, "end_pos": 218, "type": "TASK", "confidence": 0.909283717473348}]}, {"text": "For brevity, we will refer to the word segmentation of the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation rules 2 , which also determines how the translation rules would be matched by the source sentences.", "labels": [], "entities": []}, {"text": "It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7879577875137329}, {"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9973180890083313}]}, {"text": "Therefore, many approaches have been proposed to learn word segmentation suitable for SMT.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.6828379780054092}, {"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9948821067810059}]}, {"text": "These approaches were either complicated (, or of high computational complexity.", "labels": [], "entities": []}, {"text": "Moreover, they implicitly assumed that WSA and WSR should be equal.", "labels": [], "entities": [{"text": "WSA", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.4196770489215851}, {"text": "WSR", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.47257372736930847}]}, {"text": "This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7451263070106506}]}, {"text": "To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.703721359372139}, {"text": "WSA", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.7495356202125549}, {"text": "WSR", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.6757550239562988}]}, {"text": "We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz.", "labels": [], "entities": [{"text": "alignment", "start_pos": 99, "end_pos": 108, "type": "TASK", "confidence": 0.9746156930923462}]}, {"text": "character alignment; second, we use a simple method to convert the character alignment to conventional word alignment for translation rule induction.", "labels": [], "entities": [{"text": "character alignment", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7641326487064362}, {"text": "word alignment", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7127579599618912}, {"text": "translation rule induction", "start_pos": 122, "end_pos": 148, "type": "TASK", "confidence": 0.9303057591120402}]}, {"text": "In the 2 Interestingly, word is also a basic token in syntax-based rules.", "labels": [], "entities": []}, {"text": "experiment, our approach consistently outperformed two baselines with three different word segmenters: fully word-based system (using word for both alignment and translation) and fully character-based system, in terms of alignment quality and translation performance.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows: Section 2 analyzes the influences of WSA and WSR on SMT respectively; Section 3 discusses how to convert character alignment to word alignment; Section 4 presents experimental results, followed by conclusions and future work in section 5.", "labels": [], "entities": [{"text": "WSA", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.8284497261047363}, {"text": "WSR", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.7382465600967407}, {"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9869699478149414}, {"text": "character alignment", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.6860840022563934}, {"text": "word alignment", "start_pos": 182, "end_pos": 196, "type": "TASK", "confidence": 0.7489644587039948}]}], "datasetContent": [{"text": "We first evaluate the alignment quality.", "labels": [], "entities": [{"text": "alignment", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9468811750411987}]}, {"text": "The method discussed in section 3 was used to compare character and word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.686014860868454}]}, {"text": "As can be seen from, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations.", "labels": [], "entities": []}, {"text": "This gain can be attributed to the small vocabulary size (sparsity) for character alignment.", "labels": [], "entities": [{"text": "character alignment", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8610082566738129}]}, {"text": "The observation is consistent with which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly distinguishing WSA and WSR.", "labels": [], "entities": [{"text": "WSA", "start_pos": 176, "end_pos": 179, "type": "DATASET", "confidence": 0.7443854808807373}, {"text": "WSR", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.7610599398612976}]}, {"text": "We then evaluated the translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9730843305587769}]}, {"text": "The baselines are fully word-based MT systems (WordSys), i.e. using word as both WSA and WSR, and fully character-based systems (CharSys).", "labels": [], "entities": [{"text": "WordSys", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9363629817962646}]}, {"text": "Translation evaluation of WordSys and proposed system using BLEU-SBP ( 4 compares WordSys to our proposed system.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9585616588592529}, {"text": "WordSys", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9415284395217896}, {"text": "BLEU-SBP", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9820218086242676}, {"text": "WordSys", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9455409049987793}]}, {"text": "Significant testing was carried out using bootstrap re-sampling method proposed by Koehn with a 95% confidence level.", "labels": [], "entities": []}, {"text": "We see that our proposed systems outperformed WordSys in all segmentation specifications settings.", "labels": [], "entities": [{"text": "WordSys", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.930492103099823}]}, {"text": "lists the results of CharSys in small-scale task.", "labels": [], "entities": []}, {"text": "In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by.", "labels": [], "entities": [{"text": "WordSys", "start_pos": 252, "end_pos": 259, "type": "DATASET", "confidence": 0.9582750201225281}]}, {"text": "Comparing and 5, we see that all CharSys underperformed WordSys.", "labels": [], "entities": [{"text": "WordSys", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9713427424430847}]}, {"text": "This observation is consistent with which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary.", "labels": [], "entities": []}, {"text": "We also see that CharSys underperformed our proposed systems, that's because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for translation, and vice versa.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 183, "end_pos": 200, "type": "TASK", "confidence": 0.7291833907365799}, {"text": "translation", "start_pos": 252, "end_pos": 263, "type": "TASK", "confidence": 0.9629001617431641}]}, {"text": "We finally compared our approaches to and, which proposed \"packed word (PW)\" and \"bilingual motivated word (BS)\" respectively.", "labels": [], "entities": []}, {"text": "Both methods iteratively learn word segmentation and alignment alternatively, with the former starting from word-based corpus and the latter starting from characters-based corpus.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7384147346019745}]}, {"text": "Therefore, PW can be experimented on all segmentations.", "labels": [], "entities": []}, {"text": "Comparison with other works scale task, we see that both PW and BS underperformed our approach.", "labels": [], "entities": [{"text": "PW", "start_pos": 57, "end_pos": 59, "type": "DATASET", "confidence": 0.735043466091156}, {"text": "BS", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9186946153640747}]}, {"text": "This maybe attributed to the low recall of the learned BS or PW in their approaches.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.999289870262146}]}, {"text": "BS underperformed both two baselines, one reason is that Ma and Way (2009) also employed word lattice decoding techniques ( to tackle the low recall of BS, which was removed from our experiments for fair comparison.", "labels": [], "entities": [{"text": "word lattice decoding", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7578804095586141}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9978557229042053}]}, {"text": "Interestingly, we found that using character as WSA and BS as WSR (Char+BS), a moderate gain (+0.43 point) was achieved compared with fully BS-based system; and using character as WSA and PW as WSR (Char+PW), significant gains were achieved compared with fully PW-based system, the result of CTB segmentation in this setting even outperformed our proposed approach (+0.42 point).", "labels": [], "entities": [{"text": "CTB segmentation", "start_pos": 292, "end_pos": 308, "type": "TASK", "confidence": 0.7007336765527725}]}, {"text": "This observation indicated that in our framework, better combinations of WSA and WSR can be found to achieve better translation performance.", "labels": [], "entities": [{"text": "WSA", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.5623729825019836}]}], "tableCaptions": [{"text": " Table 1 Token distribution of CCorpus and WCorpus", "labels": [], "entities": [{"text": "WCorpus", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.848646879196167}]}, {"text": " Table 3 Alignment evaluation. Precision (P), recall (R),  and F-score (F) with 0.5 (Fraser and Marcu, 2007)", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9490406811237335}, {"text": "recall (R)", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9521608948707581}, {"text": "F-score (F)", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9689154326915741}]}, {"text": " Table 4 Translation evaluation of WordSys and pro-", "labels": [], "entities": [{"text": "Translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9817750453948975}, {"text": "WordSys", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9389375448226929}]}, {"text": " Table 5 Translation evaluation of CharSys.", "labels": [], "entities": [{"text": "Translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9860349893569946}]}, {"text": " Table 6 Comparison with other works", "labels": [], "entities": []}]}