{"title": [{"text": "A Graphical Interface for MT Evaluation and Error Analysis", "labels": [], "entities": [{"text": "MT Evaluation and Error Analysis", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.7458263278007508}]}], "abstractContent": [{"text": "Error analysis in machine translation is a necessary step in order to investigate the strengths and weaknesses of the MT systems underdevelopment and allow fair comparisons among them.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.796785444021225}, {"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.9674872756004333}]}, {"text": "This work presents an application that shows how a set of heterogeneous automatic metrics can be used to evaluate a test bed of automatic translations.", "labels": [], "entities": []}, {"text": "To do so, we have setup an online graphical interface for the ASIYA toolkit, a rich repository of evaluation measures working at different linguistic levels.", "labels": [], "entities": []}, {"text": "The current implementation of the interface shows constituency and dependency trees as well as shallow syntactic and semantic annotations , and word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 144, "end_pos": 159, "type": "TASK", "confidence": 0.7394568622112274}]}, {"text": "The intelligent visualization of the linguistic structures used by the metrics, as well as a set of navi-gational functionalities, may lead towards advanced methods for automatic error analysis.", "labels": [], "entities": [{"text": "automatic error analysis", "start_pos": 169, "end_pos": 193, "type": "TASK", "confidence": 0.5733317931493124}]}], "introductionContent": [{"text": "Evaluation methods area key ingredient in the development cycle of machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.8405503451824188}]}, {"text": "As illustrated in, they are used to identify and analyze the system weak points (error analysis), to introduce new improvements and adjust the internal system parameters (system refinement), and to measure the system performance in comparison to other systems or previous versions of the same system (evaluation).", "labels": [], "entities": []}, {"text": "We focus hereon the processes involved in the error analysis stage in which MT developers need to understand the output of their systems and to assess the improvements introduced.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9843422770500183}]}, {"text": "Automatic detection and classification of the errors produced by MT systems is a challenging problem.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9615954756736755}]}, {"text": "The cause of such errors may depend not only on the translation paradigm adopted, but also on the language pairs, the availability of enough linguistic resources and the performance of the linguistic processors, among others.", "labels": [], "entities": []}, {"text": "Several past research works studied and defined fine-grained typologies of translation errors according to various criteria (, which helped manual annotation and human analysis of the systems during the MT development cycle.", "labels": [], "entities": [{"text": "MT development cycle", "start_pos": 203, "end_pos": 223, "type": "TASK", "confidence": 0.9027691086133321}]}, {"text": "Recently, the task has received increasing attention towards the automatic detection, classification and analysis of these errors, and new tools have been made available to the community.", "labels": [], "entities": [{"text": "automatic detection, classification and analysis", "start_pos": 65, "end_pos": 113, "type": "TASK", "confidence": 0.6236850619316101}]}, {"text": "Examples of such tools are AMEANA (, which focuses on morphologically rich languages, and Hjerson, which addresses automatic error classification at lexical level.", "labels": [], "entities": [{"text": "AMEANA", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.672256350517273}, {"text": "automatic error classification", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.6668280164400736}]}, {"text": "In this work we present an online graphical interface to access ASIYA, an existing software designed to evaluate automatic translations using an heterogeneous set of metrics and meta-metrics.", "labels": [], "entities": []}, {"text": "The primary goal of the online interface is to allow MT developers to upload their test beds, obtain a large set of metric scores and then, detect and analyze the errors of their systems using just their Internet browsers.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9889253973960876}]}, {"text": "Additionally, the graphical interface of the toolkit may help developers to better understand the strengths and weaknesses of the existing evaluation measures and to support the development of further improvements or even totally new evaluation metrics.", "labels": [], "entities": []}, {"text": "This information can be gathered both from the experi-: MT systems development cycle ence of ASIYA's developers and also from the statistics given through the interface to the ASIYA's users.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9134829640388489}]}, {"text": "In the following, Section 2 gives a general overview of the ASIYA toolkit.", "labels": [], "entities": [{"text": "ASIYA", "start_pos": 60, "end_pos": 65, "type": "TASK", "confidence": 0.9057719111442566}]}, {"text": "Section 3 describes the variety of information gathered during the evaluation process, and Section 4 provides details on the graphical interface developed to display this information.", "labels": [], "entities": []}, {"text": "Finally, Section 5 overviews recent work related to MT error analysis, and Section 6 concludes and reports some ongoing and future work.", "labels": [], "entities": [{"text": "MT error analysis", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.8927857677141825}]}], "datasetContent": [{"text": "ASIYA allows to compute scores at three granularity levels: system (entire test corpus), document and sentence (or segment).", "labels": [], "entities": [{"text": "ASIYA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.630585789680481}]}, {"text": "The online application obtains the measures for all the metrics and levels and generates an interactive table of scores displaying the values for all the measures.", "labels": [], "entities": []}, {"text": "Table organiza-: The bar charts plot to compare the metric scores for several systems tion can swap among the three levels of granularity, and it can also be transposed with respect to system and metric information (transposing rows and columns).", "labels": [], "entities": []}, {"text": "When the metric basis table is shown, the user can select one or more metric columns in order to re-rank the rows accordingly.", "labels": [], "entities": []}, {"text": "Moreover, the source, reference and candidate translation are displayed along with metric scores.", "labels": [], "entities": []}, {"text": "The combination of all these functionalities makes it easy to know which are the highest/lowest-scored sentences in a test set.", "labels": [], "entities": []}, {"text": "We have also integrated a graphical library 2 to generate real-time interactive plots to show the metric scores graphically.", "labels": [], "entities": []}, {"text": "The current version of the interface shows interactive bar charts, where different metrics and systems can be combined in the same plot.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}], "tableCaptions": []}