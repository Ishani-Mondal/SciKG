{"title": [], "abstractContent": [{"text": "This paper addresses the search problem in textual inference, where systems need to infer one piece of text from another.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7260604351758957}]}, {"text": "A prominent approach to this task is attempts to transform one text into the other through a sequence of inference-preserving transformations, a.k.a. a proof, while estimating the proof's validity.", "labels": [], "entities": []}, {"text": "This raises a search challenge of finding the best possible proof.", "labels": [], "entities": []}, {"text": "We explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components specifically designed for textual inference: a gradient-style evaluation function, and a local-lookahead node expansion method.", "labels": [], "entities": []}, {"text": "Evaluations , using the open-source system, BIUTEE, show the contribution of these ideas to search efficiency and proof quality.", "labels": [], "entities": [{"text": "BIUTEE", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8282179236412048}]}], "introductionContent": [{"text": "In many NLP settings it is necessary to identify that a certain semantic inference relation holds between two pieces of text.", "labels": [], "entities": []}, {"text": "For example, in paraphrase recognition it is necessary to identify that the meanings of two text fragments are roughly equivalent.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.9663985073566437}]}, {"text": "In passage retrieval for question answering, it is needed to detect text passages from which a satisfying answer can be inferred.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7739483118057251}, {"text": "question answering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8780079185962677}]}, {"text": "A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (), which is adapted here for our investigation.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 83, "end_pos": 119, "type": "TASK", "confidence": 0.6082910150289536}]}, {"text": "In this setting, a system is given two text fragments, termed \"text\" (T) and \"hypothesis\" (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text.", "labels": [], "entities": []}, {"text": "An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations ().", "labels": [], "entities": []}, {"text": "Examples of such possible transformations are lexical substitutions (e.g. \"letter\" \u2192 \"message\") and predicate-template substitutions (e.g. \"X [verbactive] Y\" \u2192 \"Y [verb-passive] by X\"), which are based on available knowledge resources.", "labels": [], "entities": []}, {"text": "Another example is coreference substitutions, such as replacing \"he\" with \"the employee\" if a coreference resolver has detected that these two expressions corefer.", "labels": [], "entities": [{"text": "coreference substitutions", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.9434536397457123}, {"text": "coreference resolver", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.8330976068973541}]}, {"text": "exemplifies this approach fora particular T-H pair.", "labels": [], "entities": []}, {"text": "The rationale behind this approach is that each transformation step should preserve inference validity, such that each text generated along this process is indeed inferred from the preceding one.", "labels": [], "entities": []}, {"text": "An inherent aspect in transformation-based inference is modeling the certainty that each inference step is valid.", "labels": [], "entities": []}, {"text": "This is usually achieved by a costbased or probabilistic model, which quantifies confidence in the validity of each individual transformation and consequently of the complete chain of inference.", "labels": [], "entities": []}, {"text": "Given a set of possible transformations, there maybe many transformation sequences that would transform T to H.", "labels": [], "entities": []}, {"text": "This creates a very large search space, where systems have to find the \"best\" transformation sequence -the one of lowest cost, or of highest probability.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this search challenge has not been investigated yet in a substan-: A sequence of transformations that transform the text \"He received the letter from the secretary.\" into the hypothesis \"The secretary delivered the message to the employee.\".", "labels": [], "entities": []}, {"text": "The knowledge required for such transformations is often obtained from available knowledge resources and NLP tools.", "labels": [], "entities": []}, {"text": "tial manner: each of the above-cited works described the search method they used, but none of them tried alternative methods while evaluating search performance.", "labels": [], "entities": []}, {"text": "Furthermore, while experimenting with our own open-source inference system, BIUTEE 1 , we observed that search efficiency is a major issue, often yielding practically unsatisfactory run-times.", "labels": [], "entities": [{"text": "BIUTEE", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.8525103330612183}]}, {"text": "This paper investigates the search problem in transformation-based textual inference, naturally falling within the framework of heuristic AI (Artificial Intelligence) search.", "labels": [], "entities": []}, {"text": "To facilitate such investigation, we formulate a generic search scheme which incorporates many search variants as special cases and enable a meaningful comparison between the algorithms.", "labels": [], "entities": []}, {"text": "Under this framework, we identify special characteristics of the textual inference search space, that lead to the development of two novel algorithmic components: a special lookahead method for node expansion, named local lookahead, and a gradient-based evaluation function.", "labels": [], "entities": []}, {"text": "Together, they yield anew search algorithm, which achieved substantially superior search performance in our evaluations.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of transformation-based inference systems, AI search algorithms, and search methods realized in prior inference systems.", "labels": [], "entities": []}, {"text": "Section 3 formulates the generic search scheme that we have investigated, which covers abroad range of known algorithms, and presents our own algorithmic contributions.", "labels": [], "entities": []}, {"text": "These new algorithmic contributions were implemented in our system, BIUTEE.", "labels": [], "entities": [{"text": "BIUTEE", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.8153699636459351}]}, {"text": "In Section 4 we evaluate them empirically, and show that they improve search efficiency as well as solution's quality.", "labels": [], "entities": []}, {"text": "Search performance is evaluated on two recent RTE benchmarks, in terms 1 www.cs.biu.ac.il/ \u02dc nlp/downloads/biutee of runtime, ability to find lower-cost transformation chains and impact on overall inference.", "labels": [], "entities": []}], "datasetContent": [{"text": "In most domains, the heuristic function h(s) estimates the cost of the minimal-cost path from a current state, s, to a goal state.", "labels": [], "entities": []}, {"text": "Having such a function, the value g(s) + h(s) estimates the expected total cost of a search path containing s.", "labels": [], "entities": []}, {"text": "In our domain, it is yet unclear how to calculate such a heuristic function.", "labels": [], "entities": []}, {"text": "Given a state s, systems typically estimate the difference (the gap) between sand the hypothesis t H (the goal state).", "labels": [], "entities": []}, {"text": "In BIUTEE this is quantified by the number of parse-tree nodes and edges oft H that do not exist in s.", "labels": [], "entities": [{"text": "BIUTEE", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.7039923071861267}]}, {"text": "However, this does not give an estimation for the expected cost of the path (the sequence of transformations) from s to the goal state.", "labels": [], "entities": []}, {"text": "This is because the number of nodes and edges that can be changed by a single transformation can vary from a single node to several nodes (e.g., by a lexical syntactic entailment rule).", "labels": [], "entities": []}, {"text": "Moreover, even if two transformations change the same number of nodes and edges, their costs might be significantly different.", "labels": [], "entities": []}, {"text": "Consequently, the measurement of the cost accumulated so far (g(s)) and the remaining gap tot H (h(s)) are unrelated.", "labels": [], "entities": []}, {"text": "We note that a more sophisticated heuristic function was suggested by, based on tree-kernels.", "labels": [], "entities": []}, {"text": "Nevertheless, this heuristic function, serving as h(s), is still unrelated to the transformation costs (g(s)).", "labels": [], "entities": []}, {"text": "We therefore propose a novel gradient-style function to overcome this difficulty.", "labels": [], "entities": []}, {"text": "Our function is designed fora greedy search in which OPEN always contains a single state, s.", "labels": [], "entities": []}, {"text": "Let s j be a state generated from s, the cost of deriving s j from sis \u2206 g (s j ) \u2261 g(s j ) \u2212 g(s).", "labels": [], "entities": []}, {"text": "Similarly, the reduction in the value of the heuristic function is de- . Informally, this function measures how costly it is to derive s j relative to the obtained decrease in the remaining gap to the goal state.", "labels": [], "entities": []}, {"text": "For the edge casein which h(s) \u2212 h(s j ) \u2264 0, we define f \u2206 (s j ) = \u221e.", "labels": [], "entities": []}, {"text": "Empirically, we show in our experiments that the function f \u2206 (s) performs better than the traditional functions f (s) = g(s) + h(s) and f w (s) = g(s) + w \u00b7 h(s) in our domain.", "labels": [], "entities": []}, {"text": "In this section we first evaluate the search performance in terms of efficiency (run time), the quality of the found proofs (as measured by proof cost), and overall inference performance achieved through various search algorithms.", "labels": [], "entities": []}, {"text": "Finally we analyze the contribution of our two novel components.", "labels": [], "entities": []}, {"text": "We performed our experiments on the last two published RTE datasets: RTE-5 (2009) and.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8318962454795837}, {"text": "RTE-5 (2009)", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.8773830085992813}]}, {"text": "The RTE-5 dataset is composed of a training and test corpora, each containing 600 texthypothesis pairs, wherein half of them the text entails the hypothesis and in the other half it does not.", "labels": [], "entities": [{"text": "RTE-5 dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8894525766372681}]}, {"text": "In RTE-6, each of the training and test corpora consists of 10 topics, where each topic contains 10 documents.", "labels": [], "entities": [{"text": "RTE-6", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8154653310775757}]}, {"text": "Each corpus contains a set of hypotheses (211 in the training dataset, and 243 in the test dataset), along with a set of candidate entailing sentences for each hypothesis.", "labels": [], "entities": []}, {"text": "The system has to find for each hypothesis which candidate sentences entail it.", "labels": [], "entities": []}, {"text": "To improve speed and results, we used the filtering mechanism suggested by, which filters the candidate sentences by the Lucene IR engine 3 . Thus, only top 20 candidates per hypothesis were tested Evaluation of each of the algorithms was performed by running BIUTEE while replacing BIUTEE-orig with this algorithm.", "labels": [], "entities": [{"text": "Lucene IR engine 3", "start_pos": 121, "end_pos": 139, "type": "DATASET", "confidence": 0.9347850829362869}, {"text": "BIUTEE", "start_pos": 260, "end_pos": 266, "type": "METRIC", "confidence": 0.6667255759239197}, {"text": "BIUTEE-orig", "start_pos": 283, "end_pos": 294, "type": "METRIC", "confidence": 0.7342846989631653}]}, {"text": "We employed a comprehensive set of knowledge resources (available in BIUTEE's web site): WordNet, Directional similarity (, DIRT () and generic syntactic rules.", "labels": [], "entities": [{"text": "BIUTEE's web site", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9257788807153702}, {"text": "WordNet", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9014257788658142}]}, {"text": "In addition, we used coreference substitutions, detected by ArkRef 4 . We evaluated several known algorithms, described in above, as well as BIUTEE-orig.", "labels": [], "entities": [{"text": "BIUTEE-orig", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.9507351517677307}]}, {"text": "The latter is a strong baseline, which outperforms known search algorithms in generating low cost proofs.", "labels": [], "entities": []}, {"text": "We compared all the above mentioned algorithms to our novel one, LLGS.", "labels": [], "entities": [{"text": "LLGS", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.8394030332565308}]}, {"text": "We used the training dataset for parameter tuning, which controls the trade-off between speed and quality.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.684164360165596}, {"text": "speed", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9594681262969971}]}, {"text": "For weighted A*, as well as for greedy search, we used w = 6.0, since, fora few instances, lower values of w resulted in prohibitive runtime.", "labels": [], "entities": []}, {"text": "For beam search we used k = 150, since higher val-ues of k did not improve the proof cost on the training dataset.", "labels": [], "entities": [{"text": "beam search", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9541205167770386}]}, {"text": "The value of din LLGS was set to 3.", "labels": [], "entities": [{"text": "LLGS", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.7257134914398193}]}, {"text": "d = 4 yielded the same proof costs, but was about 3 times slower.", "labels": [], "entities": []}, {"text": "Since lower values of w could be used by weighted A* for most instances, we also ran experiments where we varied the value of w according to the dovetailing method suggested in) (denoted dovetailing WA*) as follows.", "labels": [], "entities": []}, {"text": "When weighted A* has found a solution, we reran it with anew value of w, set to half of the previous value.", "labels": [], "entities": []}, {"text": "The idea is to guide the search for lower cost solutions.", "labels": [], "entities": []}, {"text": "This process was halted when the total number of states generated by all weighted A* instances exceeded a predefined constant (set to 10, 000).", "labels": [], "entities": []}, {"text": "In this experiment we examine separately our two novel components.", "labels": [], "entities": []}, {"text": "We examined f \u2206 by running LLGS with alternative evaluation functions.", "labels": [], "entities": [{"text": "LLGS", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.9122573733329773}]}, {"text": "The results, displayed in, show that using f \u2206 yields better proofs and also improves run time.", "labels": [], "entities": []}, {"text": "Our local-lookahead (Subsection 3.4) was examined by running LLGS with alternative node expansion methods.", "labels": [], "entities": []}, {"text": "One alternative to local-lookahead is standard expansion by generating all immediate derivations.", "labels": [], "entities": []}, {"text": "Another alternative is to use the standard lookahead, in which a brute-force depth-limited search is performed in each iteration, termed here \"exhaustive lookahead\".", "labels": [], "entities": []}, {"text": "The results, presented in, show that by avoiding any type of lookahead one can achieve fast runtime, while compromising proof quality.", "labels": [], "entities": []}, {"text": "On the other hand, both exhaustive and local lookahead yield better proofs and accuracy, while local lookahead is more than 4 times faster than exhaustive lookahead.", "labels": [], "entities": [{"text": "proofs", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9495671391487122}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9991095662117004}]}], "tableCaptions": [{"text": " Table 3: Comparison of algorithms on RTE-5 / RTE-6", "labels": [], "entities": [{"text": "RTE-5", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9050618410110474}, {"text": "RTE-6", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.6121349334716797}]}, {"text": " Table 4: Impact of algorithms on system success rate", "labels": [], "entities": []}, {"text": " Table 5: Impact of f \u2206 on RTE-5. w = 6.0. Accuracy  obtained by retraining with corresponding f .", "labels": [], "entities": [{"text": "RTE-5", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.740031898021698}, {"text": "Accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9972143769264221}]}, {"text": " Table 6: Impact of local and global lookahead on RTE-5.  Accuracy obtained by retraining with the corresponding  lookahead method.", "labels": [], "entities": [{"text": "RTE-5", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8716959953308105}, {"text": "Accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9857498407363892}]}]}