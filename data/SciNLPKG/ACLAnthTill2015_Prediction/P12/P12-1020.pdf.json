{"title": [{"text": "Bootstrapping a Unified Model of Lexical and Phonetic Acquisition", "labels": [], "entities": [{"text": "Lexical and Phonetic Acquisition", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6216540038585663}]}], "abstractContent": [{"text": "During early language acquisition, infants must learn both a lexicon and a model of phonetics that explains how lexical items can vary in pronunciation-for instance \"the\" might be realized as [Di] or [D@].", "labels": [], "entities": []}, {"text": "Previous models of acquisition have generally tackled these problems in isolation, yet behavioral evidence suggests infants acquire lexical and phonetic knowledge simultaneously.", "labels": [], "entities": []}, {"text": "We present a Bayesian model that clusters together phonetic variants of the same lexical item while learning both a language model over lexical items and a log-linear model of pronunciation variability based on ar-ticulatory features.", "labels": [], "entities": []}, {"text": "The model is trained on transcribed surface pronunciations, and learns by bootstrapping, without access to the true lexicon.", "labels": [], "entities": []}, {"text": "We test the model using a corpus of child-directed speech with realistic phonetic variation and either gold standard or automatically induced word boundaries.", "labels": [], "entities": []}, {"text": "In both cases modeling variability improves the accuracy of the learned lexicon over a system that assumes each lexical item has a unique pronunciation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9986293315887451}]}], "introductionContent": [{"text": "Infants acquiring their first language confront two difficult cognitive problems: building a lexicon of word forms, and learning basic phonetics and phonology.", "labels": [], "entities": []}, {"text": "The two tasks are closely related: knowing what sounds can substitute for one another helps in clustering together variant pronunciations of the same word, while knowing the environments in which particular words can occur helps determine which sound changes are meaningful and which are not (Feldman et al., 2009).", "labels": [], "entities": []}, {"text": "For instance, if an infant who already knows the word \u0091ju\u0093 \"you\" encounters anew word \u0091jd\u0093, they must decide whether it is anew lexical item or a variant of the word they already know.", "labels": [], "entities": []}, {"text": "Evidence for the correct conclusion comes from the pronunciation (many English vowels are reduced to in unstressed positions) and the context-if the next word is \"want\", \"you\" is a plausible choice.", "labels": [], "entities": []}, {"text": "To date, most models of infant language learning have focused on either lexicon-building or phonetic learning in isolation.", "labels": [], "entities": []}, {"text": "For example, many models of word segmentation implicitly or explicitly build a lexicon while segmenting the input stream of phonemes into word tokens; in nearly all cases the phonemic input is created from an orthographic transcription using a phonemic dictionary, thus abstracting away from any phonetic variability, among others).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7332023978233337}]}, {"text": "As illustrated in, these models attempt to infer line (a) from line (d).", "labels": [], "entities": []}, {"text": "However, (d) is an idealization: real speech has variability, and behavioral evidence suggests that infants are still learning about the phonetics and phonology of their language even after beginning to segment words, rather than learning to neutralize the variations first and acquiring the lexicon afterwards, and references therein).", "labels": [], "entities": []}, {"text": "Based on this evidence, a more realistic model of early language acquisition should propose a method of inferring the intended forms () from the unsegmented surface forms (1c) while also learning a model of phonetic variation relating the intended and surface forms (a) and (b).", "labels": [], "entities": [{"text": "early language acquisition", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.62551282842954}]}, {"text": "Previous models with similar goals have learned from an artificial corpus with a small vocabulary ( or have modeled variability only in vowels; to our knowledge, this paper is the first to use a naturalistic infant-directed corpus while modeling variability in all segments, and to incorporate word-level context (a bigram language model).", "labels": [], "entities": []}, {"text": "Our main contribution is a joint lexicalphonetic model that infers intended forms from segmented surface forms; we test the system using input with either gold standard word boundaries or boundaries induced by an existing unsupervised segmentation model.", "labels": [], "entities": []}, {"text": "We show that in both cases modeling variability improves the accuracy of the learned lexicon over a system that assumes each intended form has a unique surface form.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9986704587936401}]}, {"text": "Our model is conceptually similar to those used in speech recognition and other applications: we assume the intended tokens are generated from a bigram language model and then distorted by a noisy channel, in particular a log-linear model of phonetic variability.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7260888963937759}]}, {"text": "But unlike speech recognition, we have no intended-form, surface-form training pairs to train the phonetic model, nor even a dictionary of intended-form strings to train the language model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.728872150182724}]}, {"text": "Instead, we initialize the noise model using feature weights based on universal linguistic principles (e.g., a surface phone is likely to share articulatory features with the intended phone) and use a bootstrapping process to iteratively infer the intended forms and retrain the language model and noise model.", "labels": [], "entities": []}, {"text": "While we do not claim that the particular inference mechanism we use is cognitively plausible, our positive results further support the claim that infants can and do acquire phonetics and the lexicon in concert.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our corpus is a processed version of the BernsteinRatner corpus from CHILDES (MacWhinney, 2000), which contains orthographic transcriptions of parent-child dyads with infants aged 13-23 months.", "labels": [], "entities": [{"text": "BernsteinRatner corpus from CHILDES (MacWhinney, 2000)", "start_pos": 41, "end_pos": 95, "type": "DATASET", "confidence": 0.9108352528678046}]}, {"text": "created a phonemic version of this corpus by extracting all infant-directed utterances and converted them to a phonemic transcription using a dictionary.", "labels": [], "entities": []}, {"text": "This version, which contains 9790 utterances (33399 tokens, 1321 types), is now standard for word segmentation, but contains no phonetic variability.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7778628170490265}]}, {"text": "Since producing a close phonetic transcription of this data would be impractical, we instead construct an approximate phonetic version using information from the Buckeye corpus (.", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 162, "end_pos": 176, "type": "DATASET", "confidence": 0.9717172384262085}]}, {"text": "Buckeye is a corpus of adult-directed conversational American English, and has been phonetically transcribed: Empirical distribution of pronunciations of \"about\" and \"wanna\" in our dataset. by hand to indicate realistic pronunciation variability.", "labels": [], "entities": [{"text": "Buckeye is a corpus of adult-directed conversational American English", "start_pos": 0, "end_pos": 69, "type": "DATASET", "confidence": 0.9030828608406914}]}, {"text": "To create our phonetic corpus, we replace each phonemic word in the Bernstein-Ratner-Brent corpus with a phonetic pronunciation of that word sampled from the empirical distribution of pronunciations in Buckeye.", "labels": [], "entities": [{"text": "Buckeye", "start_pos": 202, "end_pos": 209, "type": "DATASET", "confidence": 0.9390618205070496}]}, {"text": "If the word never occurs in Buckeye, we use the original phonemic version.", "labels": [], "entities": [{"text": "Buckeye", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9275977611541748}]}, {"text": "Our corpus is not completely realistic as a sample of child-directed speech.", "labels": [], "entities": []}, {"text": "Since each pronunciation is sampled independently, it lacks coarticulation and prosodic effects, and the distribution of pronunciations is derived from adult-directed rather than child-directed speech.", "labels": [], "entities": []}, {"text": "Nonetheless, it represents phonetic variability more realistically than the BernsteinRatner-Brent corpus, while still maintaining the lexical characteristics of infant-directed speech (as compared to the Buckeye corpus, with its much larger vocabulary and more complex language model).", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 204, "end_pos": 218, "type": "DATASET", "confidence": 0.8673246800899506}]}, {"text": "We conduct our development experiments on the first 8000 input utterances, holding out the remaining 1790 for evaluation.", "labels": [], "entities": []}, {"text": "For evaluation experiments, we run the system on all 9790 utterances, reporting scores on only the last 1790.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on 1790 utterances (known boundaries).", "labels": [], "entities": []}, {"text": " Table 4: Degradation in dpseg segmentation perfor- mance caused by pronunciation variation.", "labels": [], "entities": []}, {"text": " Table 5: Results on 1790 utterances (induced boundaries).", "labels": [], "entities": []}]}