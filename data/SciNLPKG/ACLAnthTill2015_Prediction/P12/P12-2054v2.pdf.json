{"title": [], "abstractContent": [{"text": "Topic modeling with a tree-based prior has been used fora variety of applications because it can encode correlations between words that traditional topic modeling cannot.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7613436281681061}]}, {"text": "However , its expressive power comes at the cost of more complicated inference.", "labels": [], "entities": []}, {"text": "We extend the SPARSELDA (Yao et al., 2009) inference scheme for latent Dirichlet allocation (LDA) to tree-based topic models.", "labels": [], "entities": [{"text": "SPARSELDA (Yao et al., 2009)", "start_pos": 14, "end_pos": 42, "type": "DATASET", "confidence": 0.7295770347118378}, {"text": "latent Dirichlet allocation (LDA)", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.7148868093887965}]}, {"text": "This sampling scheme computes the exact conditional distribution for Gibbs sampling much more quickly than enumerating all possible latent variable assignments.", "labels": [], "entities": []}, {"text": "We further improve performance by iteratively refining the sampling distribution only when needed.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed techniques dramatically improve the computation time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models, exemplified by latent Dirichlet allocation (LDA) (, discover latent themes present in text collections.", "labels": [], "entities": []}, {"text": "\"Topics\" discovered by topic models are multinomial probability distributions over words that evince thematic coherence.", "labels": [], "entities": []}, {"text": "Topic models are used in computational biology, computer vision, music, and, of course, text analysis.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.8363195061683655}]}, {"text": "One of LDA's virtues is that it is a simple model that assumes asymmetric Dirichlet prior over its word distributions.", "labels": [], "entities": []}, {"text": "Recent work argues for structured distributions that constrain clusters (, span languages (, or incorporate human feedback ( to improve the quality and flexibility of topic modeling.", "labels": [], "entities": []}, {"text": "These models all use different tree-based prior distributions (Section 2).", "labels": [], "entities": []}, {"text": "These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling) straightforward.", "labels": [], "entities": []}, {"text": "While straightforward, inference isn't cheap.", "labels": [], "entities": []}, {"text": "Particularly for interactive settings), efficient inference would improve perceived latency.", "labels": [], "entities": []}, {"text": "SPARSELDA () is an efficient Gibbs sampling algorithm for LDA based on a refactorization of the conditional topic distribution (reviewed in Section 3).", "labels": [], "entities": [{"text": "SPARSELDA", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.598145604133606}]}, {"text": "However, it is not directly applicable to tree-based priors.", "labels": [], "entities": []}, {"text": "In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the running time 5 of our sampling algorithm (FAST) and our algorithm with the refined bucket (RB) against the unfactored Gibbs sampler (NA\u00a8IVENA\u00a8IVE) and examine the effect of sorting.", "labels": [], "entities": [{"text": "FAST", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7811580896377563}]}, {"text": "Our corpus has editorials from New York Times  from 1987 to 1996.", "labels": [], "entities": []}, {"text": "Since we are interested in varying vocabulary size, we rank types by average tf-idf and choose the top V . WordNet 3.0 generates the correlations between types.", "labels": [], "entities": []}, {"text": "For each synset in WordNet, we generate a subtree with all types in the synsetthat are also in our vocabulary-as leaves connected to a common parent.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9528750777244568}]}, {"text": "This subtree's common parent is then attached to the root node.", "labels": [], "entities": []}, {"text": "We compared the FAST and FAST-RB against NA\u00a8IVENA\u00a8IVE) on different numbers of topics, various vocabulary sizes and different numbers of correlations.", "labels": [], "entities": [{"text": "FAST", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.7062419056892395}, {"text": "FAST-RB", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.7236181497573853}, {"text": "NA\u00a8IVENA\u00a8IVE", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.8454603672027587}]}, {"text": "FAST is consistently faster than NA\u00a8IVENA\u00a8IVE and FAST-RB is consistently faster than FAST.", "labels": [], "entities": [{"text": "FAST", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8888137340545654}, {"text": "NA\u00a8IVENA\u00a8IVE", "start_pos": 33, "end_pos": 45, "type": "METRIC", "confidence": 0.8840684175491333}, {"text": "FAST-RB", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.7776990532875061}, {"text": "FAST", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.6219209432601929}]}, {"text": "Their benefits are clearer as distributions become sparse (e.g., the first iteration for FAST is slower than later iterations).", "labels": [], "entities": [{"text": "FAST", "start_pos": 89, "end_pos": 93, "type": "TASK", "confidence": 0.4099910259246826}]}, {"text": "Gains accumulate as the topic number increases, but decrease a little with the vocabulary size.", "labels": [], "entities": [{"text": "Gains", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9849442839622498}]}, {"text": "While both sorting strategies reduce time, sorting topics and paths fora word (SW) helps more than sorting topics in a document (SD), and combining the 6 13284 documents, 41554 types, and 2714634 tokens.", "labels": [], "entities": []}, {"text": "two is (with one exception) better than either alone.", "labels": [], "entities": []}, {"text": "As more correlations are added, NA\u00a8IVENA\u00a8IVE's time increases while that of FAST-RB decreases.", "labels": [], "entities": [{"text": "NA\u00a8IVENA\u00a8IVE's time", "start_pos": 32, "end_pos": 51, "type": "METRIC", "confidence": 0.9189698610986982}, {"text": "FAST-RB", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8083583116531372}]}, {"text": "This is because the number of non-zero paths for uncorrelated words decreases as more correlations are added to the model.", "labels": [], "entities": []}, {"text": "Since our techniques save computation for every zero path, the overall computation decreases as correlations push uncorrelated words to a limited number of topics.", "labels": [], "entities": []}, {"text": "Qualitatively, when the synset with \"king\" and \"baron\" is added to a model, it is associated with \"drug, inmate, colombia, waterfront, baron\" in a topic; when \"king\" is correlated with \"queen\", the associated topic has \"king, parade, museum, queen, jackson\" as its most probable words.", "labels": [], "entities": []}, {"text": "In contrast to previous approaches, inference speeds up as topics become more semantically coherent).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The average running time per iteration (S) over  100 iterations, averaged over 5 seeds. Experiments begin  with 100 topics, 100 correlations, vocab size 10000 and  then vary one dimension: number of topics (top), vocabu- lary size (middle), and number of correlations (bottom).", "labels": [], "entities": []}]}