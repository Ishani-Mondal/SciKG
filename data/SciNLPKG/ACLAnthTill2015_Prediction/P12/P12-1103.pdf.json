{"title": [{"text": "Improve SMT Quality with Automatically Extracted Paraphrase Rules", "labels": [], "entities": [{"text": "Improve SMT", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7228823602199554}]}], "abstractContent": [{"text": "1 We propose a novel approach to improve SMT via paraphrase rules which are automatically extracted from the bilingual training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9973276853561401}]}, {"text": "Without using extra paraphrase resources, we acquire the rules by comparing the source side of the parallel corpus with the target-to-source translations of the target side.", "labels": [], "entities": []}, {"text": "Besides the word and phrase paraphrases, the acquired paraphrase rules mainly cover the structured paraphrases on the sentence level.", "labels": [], "entities": []}, {"text": "These rules are employed to enrich the SMT inputs for translation quality improvement.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9916623830795288}, {"text": "translation quality", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8521956503391266}]}, {"text": "The experimental results show that our proposed approach achieves significant improvements of 1.6~3.6 points of BLEU in the oral domain and 0.5~1 points in the news domain.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9981731176376343}]}], "introductionContent": [{"text": "The translation quality of the SMT system is highly related to the coverage of translation models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9914616346359253}]}, {"text": "However, no matter how much data is used for training, it is still impossible to completely cover the unlimited input sentences.", "labels": [], "entities": []}, {"text": "This problem is more serious for online SMT systems in real-world applications.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9612959623336792}]}, {"text": "Naturally, a solution to the coverage problem is to bridge the gaps between the input sentences and the translation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from This work was done when the first author was visiting Baidu.", "labels": [], "entities": [{"text": "Baidu", "start_pos": 292, "end_pos": 297, "type": "DATASET", "confidence": 0.9376487731933594}]}, {"text": "*Correspondence author: tliu@ir.hit.edu.cn the side of translation models, which tries to enrich the translation models to cover more expressions.", "labels": [], "entities": []}, {"text": "In recent years, paraphrasing has been proven useful for improving SMT quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.998247504234314}]}, {"text": "The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs.", "labels": [], "entities": []}, {"text": "In the first category,, and enriched the SMT models via paraphrasing the training corpora. and used paraphrases to smooth translation models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9831119179725647}]}, {"text": "For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. and paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora.", "labels": [], "entities": []}, {"text": "rewrite OOVs with entailments and paraphrases acquired from WordNet. and use phrasal paraphrases to build a word lattice to get multiple input candidates.", "labels": [], "entities": [{"text": "WordNet.", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9556646943092346}]}, {"text": "In the above methods, only word or phrasal paraphrases are used for input sentence rewriting.", "labels": [], "entities": [{"text": "input sentence rewriting", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6685529251893362}]}, {"text": "No structured paraphrases on the sentence level have been investigated.", "labels": [], "entities": []}, {"text": "However, the information in the sentence level is very important for disambiguation.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.9676792025566101}]}, {"text": "For example, we can only substitute play with drama in a context related to stage or theatre.", "labels": [], "entities": []}, {"text": "Phrasal paraphrase substitutions can hardly solve such kind of problems.", "labels": [], "entities": [{"text": "Phrasal paraphrase substitutions", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7074531118075053}]}, {"text": "In this paper, we propose a method that rewrites the input sentences of the SMT system using automatically extracted paraphrase rules which can capture structures on sentence level in addition to paraphrases on the word or phrase level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9824491739273071}]}, {"text": "Without extra paraphrase resources, a novel approach is proposed to acquire paraphrase rules from the bilingual training corpus based on the results of Forward-Translation and Back-Translation.", "labels": [], "entities": []}, {"text": "The rules target on rewriting the input sentences to an MT-favored expression to ensure a better translation.", "labels": [], "entities": []}, {"text": "The paraphrase rules coverall kinds of paraphrases on the word, phrase and sentence levels, enabling structure reordering, word or phrase insertion, deletion and substitution.", "labels": [], "entities": [{"text": "word or phrase insertion", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.614916481077671}]}, {"text": "The experimental results show that our proposed approach achieves significant improvements of 1.6~3.6 points of BLEU in the oral domain and 0.5~1 points in the news domain.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9981731176376343}]}, {"text": "The remainder of the paper is organized as follows: Section 2 makes a comparison between the Forward-Translation and Back-Translation.", "labels": [], "entities": []}, {"text": "Section 3 introduces our methods that extract paraphrase rules from the bilingual corpus of SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9228670001029968}]}, {"text": "Section 4 describes the strategies for constructing word lattice with paraphrase rules.", "labels": [], "entities": []}, {"text": "The experimental results and some discussions are presented in Section 5 and Section 6.", "labels": [], "entities": []}, {"text": "Section 7 compares our work to the previous researches.", "labels": [], "entities": []}, {"text": "Finally, Section 8 concludes the paper and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we used Moses () as the baseline system which can support lattice decoding.", "labels": [], "entities": []}, {"text": "The alignment was obtained using GIZA++ and then we symmetrized the word alignment using the growdiag-final heuristic.", "labels": [], "entities": []}, {"text": "Parameters were tuned using Minimum Error Rate Training.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 28, "end_pos": 55, "type": "METRIC", "confidence": 0.856510579586029}]}, {"text": "To comprehensively evaluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (G oral ) and the newsgroup (G news ).", "labels": [], "entities": []}, {"text": "The experiments were conducted in both Chinese-English and EnglishChinese directions for the oral group, and ChineseEnglish direction for the newsgroup.", "labels": [], "entities": []}, {"text": "The English sentences were all tokenized and lowercased, and the Chinese sentences were segmented into words by Language Technology Platform (LTP) 1 . We used SRILM 2 for the training of language models (5-gram in all the experiments).", "labels": [], "entities": []}, {"text": "The metrics for automatic evaluation were BLEU and TER).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9994683861732483}, {"text": "TER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.997283935546875}]}, {"text": "The detailed statistics of the training data in G oral are showed in.", "labels": [], "entities": [{"text": "G oral", "start_pos": 48, "end_pos": 54, "type": "TASK", "confidence": 0.6761668622493744}]}, {"text": "For the bilingual corpus, we used the BTEC and PIVOT data of IWSLT 2008, HIT corpus 5 and other Chinese LDC (CLDC) corpora, including the Chinese-English Sentence Aligned Bilingual Corpus (CLDC-LAC-2003-004) and the Chinese-English Parallel Corpora (CLDC-LAC-2003-006).", "labels": [], "entities": [{"text": "BTEC and PIVOT data of IWSLT 2008", "start_pos": 38, "end_pos": 71, "type": "DATASET", "confidence": 0.8021299328122821}, {"text": "HIT corpus 5", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.895283838113149}]}, {"text": "We trained a Chinese language model for the E-C translation on the Chinese part of the bi-text.", "labels": [], "entities": []}, {"text": "For the English language model of C-E translation, an extra corpus named Tanaka was used besides the English part of the bilingual corpora.", "labels": [], "entities": [{"text": "C-E translation", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.746939480304718}, {"text": "Tanaka", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.7837634086608887}]}, {"text": "For testing and developing, we used six Chinese-English development corpora of IWSLT 2008.", "labels": [], "entities": [{"text": "Chinese-English development corpora of IWSLT 2008", "start_pos": 40, "end_pos": 89, "type": "DATASET", "confidence": 0.6587086220582327}]}, {"text": "The statistics are shown in.", "labels": [], "entities": []}, {"text": "In detail, we chose CSTAR03-test and IWSLT06-dev as the development set; and used IWSLT04-test, IWSLT05-test, IWSLT06-dev and IWSLT07-test for testing.", "labels": [], "entities": [{"text": "CSTAR03-test", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9609924554824829}, {"text": "IWSLT06-dev", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.8965226411819458}, {"text": "IWSLT04-test", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.894406795501709}, {"text": "IWSLT05-test", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.8342529535293579}, {"text": "IWSLT06-dev", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.8806333541870117}, {"text": "IWSLT07-test", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.89520263671875}]}, {"text": "For English-Chinese evaluation, we used IWSLT English-Chinese MT evaluation 2005 as the test set.", "labels": [], "entities": [{"text": "IWSLT English-Chinese MT evaluation 2005", "start_pos": 40, "end_pos": 80, "type": "DATASET", "confidence": 0.840580689907074}]}, {"text": "Due to the lacking of development set, we did not tune parameters on English-Chinese side, instead, we just used the default parameters of Moses.", "labels": [], "entities": []}, {"text": "In the experiments of the newsgroup, we used the Sinorama and FBIS corpora (LDC2005T10 and LDC2003E14) for bilingual corpus.", "labels": [], "entities": [{"text": "Sinorama and FBIS corpora", "start_pos": 49, "end_pos": 74, "type": "DATASET", "confidence": 0.738604910671711}]}, {"text": "After tokenization and filtering, this bilingual corpus contained 319,694 sentence pairs (7.9M tokens on Chinese side and 9.2M tokens on English side).", "labels": [], "entities": []}, {"text": "We trained a 5-gram language model on the English side of the bi-text.", "labels": [], "entities": []}, {"text": "The system was tested using the Chinese-English MT evaluation sets of.", "labels": [], "entities": [{"text": "Chinese-English MT evaluation sets", "start_pos": 32, "end_pos": 66, "type": "DATASET", "confidence": 0.7026490941643715}]}, {"text": "For development, we used the Chinese-English MT evaluation sets of NIST 2002 and NIST 2005.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.8510312736034393}, {"text": "NIST 2002", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.9023104310035706}, {"text": "NIST 2005", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.864986389875412}]}, {"text": "shows the statistics of test/development sets used in the newsgroup.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of Chinese Paraphrase rules, together with English translations for every word", "labels": [], "entities": []}, {"text": " Table 2: Statistics of training data in G oral", "labels": [], "entities": []}, {"text": " Table 3: Statistics of test/develop sets in G oral", "labels": [], "entities": []}, {"text": " Table 4: Statistics of test/develop sets in G news", "labels": [], "entities": []}, {"text": " Table 8: Human analysis of the paraphrasing  results in IWSLT 2007 CE translation", "labels": [], "entities": [{"text": "IWSLT 2007 CE translation", "start_pos": 57, "end_pos": 82, "type": "DATASET", "confidence": 0.9675952047109604}]}, {"text": " Table 5: Experimental results of G oral in Chinese-English direction", "labels": [], "entities": [{"text": "G oral", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.875942200422287}]}, {"text": " Table 6: Experimental results of G news in Chinese-English direction", "labels": [], "entities": [{"text": "G news", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.9240525662899017}]}, {"text": " Table 7: Experimental results of G oral in  English-Chinese direction", "labels": [], "entities": [{"text": "G oral", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.8719597458839417}]}]}