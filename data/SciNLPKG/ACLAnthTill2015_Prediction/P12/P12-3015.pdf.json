{"title": [{"text": "CSNIPER Annotation-by-query for non-canonical constructions in large corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "We present CSNIPER (Corpus Sniper), a tool that implements (i) a web-based multiuser scenario for identifying and annotating non-canonical grammatical constructions in large corpora based on linguistic queries and (ii) evaluation of annotation quality by measuring inter-rater agreement.", "labels": [], "entities": []}, {"text": "This annotation-by-query approach efficiently harnesses expert knowledge to identify instances of linguistic phenomena that are hard to identify by means of existing automatic annotation tools.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguistic annotation by means of automatic procedures, such as part-of-speech (POS) tagging, is a backbone of modern corpus linguistics; POS tagged corpora enhance the possibilities of corpus query.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6999266743659973}]}, {"text": "However, many linguistic phenomena are not amenable to automatic annotation and are not readily identifiable on the basis of surface features.", "labels": [], "entities": []}, {"text": "Non-canonical constructions (NCCs), which are the use-case of the tool presented in this paper, area casein point.", "labels": [], "entities": []}, {"text": "NCCs, of which cleft-sentences area well-known example, raise a number of issues that prevent their reliable automatic identification in corpora.", "labels": [], "entities": [{"text": "NCCs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8133888840675354}]}, {"text": "Yet, they warrant corpus study due to the relatively low frequency of individual instances, their deviation from canonical construction patterns and frequent ambiguity.", "labels": [], "entities": []}, {"text": "This makes them hard to distinguish from other, seemingly similar constructions.", "labels": [], "entities": []}, {"text": "Expert knowledge is thus required to reliably identify and annotate such phenomena in sufficiently large corpora like the 100 mil.", "labels": [], "entities": []}, {"text": "word British National Corpus (.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 5, "end_pos": 28, "type": "DATASET", "confidence": 0.9480692148208618}]}, {"text": "This necessitates manual annotation which is time-consuming and error-prone when carried out by individual linguists.", "labels": [], "entities": []}, {"text": "To overcome these issues, CSNIPER implements a web-based multi-user annotation scenario in which linguists formulate and refine queries that identify a given linguistic construction in a corpus and assess the query results to distinguish instances of the phenomenon understudy (true positives) from such examples that are wrongly identified by the query (false positives).", "labels": [], "entities": []}, {"text": "Each expert linguist thus acts as a rater rather than an annotator.", "labels": [], "entities": []}, {"text": "The tool records assessments made by each rater.", "labels": [], "entities": []}, {"text": "A subsequent evaluation step measures the inter-rater agreement.", "labels": [], "entities": []}, {"text": "The actual annotation step is deferred until after this evaluation in order to achieve high annotation confidence.", "labels": [], "entities": []}, {"text": "CSNIPER implements an annotation-by-query approach which entails the following interlinking functionalities (see): Query development: Corpus queries can be developed and refined within the tool.", "labels": [], "entities": [{"text": "CSNIPER", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8565143942832947}]}, {"text": "Based on query results which are assessed and labeled by the user, queries can be systematically evaluated and refined for precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9958005547523499}]}, {"text": "This transfers some of the ideas of relevance feedback, which is a common method of improving search results in information retrieval, to a linguistic corpus query system.", "labels": [], "entities": []}, {"text": "Assessment: Query results are presented to the user as a list of sentences with optional additional context; the user assesses and labels each sentence as representing or not representing an instance of the linguistic phenomenon understudy.", "labels": [], "entities": []}, {"text": "The tool implements a function that allows the user to comment on decisions and to temporarily mark sentences with uncertain assessments for later review.", "labels": [], "entities": []}, {"text": "Evaluation: Evaluation is a central functionality of CSNIPER serving three purposes.", "labels": [], "entities": []}, {"text": "1) It integrates with the query development by providing feedback to refine queries and improve query precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9646710753440857}]}, {"text": "2) It provides information on sentences not labeled consistently by all users, which can be used to review the assessments.", "labels": [], "entities": []}, {"text": "3) It calculates the interrater agreement which is used in the corpus annotation step to ensure high annotation confidence.", "labels": [], "entities": []}, {"text": "Corpus annotation: By assessing and labeling query results as corrector wrong, raters provide the tool with their annotation decisions.", "labels": [], "entities": [{"text": "Corpus annotation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.777131974697113}]}, {"text": "CSNIPER annotates the corpus with those annotation decisions that exceed a certain inter-rater agreement threshold.", "labels": [], "entities": [{"text": "CSNIPER annotates the corpus", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8185080587863922}]}, {"text": "This annotation-by-query approach of querying, assessing, evaluating and annotating allows multiple distributed raters to incrementally improve query results and achieve high quality annotations.", "labels": [], "entities": []}, {"text": "In this paper, we show how such an approach is well-suited for annotation tasks that require manual analysis overlarge corpora.", "labels": [], "entities": []}, {"text": "The approach is generalizable to any kind of linguistic phenomena that can be located in corpora on the basis of queries and require manual assessment by multiple expert raters.", "labels": [], "entities": []}, {"text": "In the next two sections, we are providing a more detailed description of the use-case driving the development of CSNIPER (sect.", "labels": [], "entities": []}, {"text": "2) and discuss why existing tools do not provide viable solutions (sect. 3).", "labels": [], "entities": []}, {"text": "4 discusses CSNIPER and sect.", "labels": [], "entities": []}, {"text": "5 draws some conclusions and offers an outlook on the next steps.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation function provides an overview of the current assessment state ().", "labels": [], "entities": []}, {"text": "We support two evaluation views: by construction type and by query.", "labels": [], "entities": []}, {"text": "By construction type: In this view, one or more 12 corpora, 13 types, and 14 users can be selected for evaluation.", "labels": [], "entities": []}, {"text": "For these, all annotation candidates and the respective statistics are displayed.", "labels": [], "entities": []}, {"text": "It is pos-: KWIC view of query results and assessments sible to filter for correct, wrong, disputed, incompletely assessed, and unassessed candidates.", "labels": [], "entities": []}, {"text": "A candidate is disputed if it is not labeled consistently by all selected users.", "labels": [], "entities": []}, {"text": "A candidate is incompletely assessed if at least one of the selected users labeled it and at least one other did not.", "labels": [], "entities": []}, {"text": "Investigating disputed cases and inter-rater agreement per type using are the main uses of this view.", "labels": [], "entities": []}, {"text": "The inter-rater agreement is calculated using only candidates labeled by all selected users.", "labels": [], "entities": []}, {"text": "By query: In this view, query precision and assessment completeness are calculated fora set of 17 queries and 18 users.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9877176284790039}, {"text": "completeness", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.813387393951416}]}, {"text": "The query precision is calculated from the labeled candidates as: We treat a candidate as a true positive (TP) if: 1) the number of correct labels is larger than the number of wrong labels; 2) the ratio of correct labels compared to the number of raters exceeds a given 19 threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9550634622573853}]}, {"text": "Candidates are conversely treated as false positives (FPs) if the number of wrong labels is larger and the threshold is exceeded.", "labels": [], "entities": []}, {"text": "The threshold controls the confidence of the TP and, thus, of the annotations generated from them (cf. sect. 4.5).", "labels": [], "entities": []}], "tableCaptions": []}