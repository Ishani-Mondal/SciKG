{"title": [{"text": "Sentence Dependency Tagging in Online Question Answering Forums", "labels": [], "entities": [{"text": "Sentence Dependency Tagging in Online Question Answering Forums", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.777082335203886}]}], "abstractContent": [{"text": "Online forums are becoming a popular resource in the state of the art question answering (QA) systems.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.854097044467926}]}, {"text": "Because of its nature as an online community, it contains more updated knowledge than other places.", "labels": [], "entities": []}, {"text": "However, going through tedious and redundant posts to look for answers could be very time consuming.", "labels": [], "entities": []}, {"text": "Most prior work focused on extracting only question answering sentences from user conversations.", "labels": [], "entities": [{"text": "extracting only question answering sentences from user conversations", "start_pos": 27, "end_pos": 95, "type": "TASK", "confidence": 0.8414779081940651}]}, {"text": "In this paper, we introduce the task of sentence dependency tagging.", "labels": [], "entities": [{"text": "sentence dependency tagging", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.7619575063387553}]}, {"text": "Finding dependency structure cannot only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations.", "labels": [], "entities": []}, {"text": "We use linear-chain conditional random fields (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences.", "labels": [], "entities": [{"text": "sentence type tagging", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6882077554861704}]}, {"text": "Our experimental results show that our proposed approach performs well for sentence dependency tagging.", "labels": [], "entities": [{"text": "sentence dependency tagging", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.7554066181182861}]}, {"text": "This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.", "labels": [], "entities": [{"text": "thread ranking", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.779671311378479}, {"text": "answer summarization", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.8907396495342255}]}], "introductionContent": [{"text": "Automatic Question Answering (QA) systems rely heavily on good sources of data that contain questions and answers.", "labels": [], "entities": [{"text": "Automatic Question Answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7699310133854548}]}, {"text": "Question answering forums, such as technical support forums, are places where users find answers through conversations.", "labels": [], "entities": [{"text": "Question answering forums", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8725911776224772}]}, {"text": "Because of their nature as online communities, question answering forums provide more updated answers to new problems.", "labels": [], "entities": [{"text": "question answering forums", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.8665260871251425}]}, {"text": "For example, when the latest release of Linux has a bug, we can expect to find solutions in forums first.", "labels": [], "entities": []}, {"text": "However, unlike other structured knowledge bases, often it is not straightforward to extract information such as questions and answers in online forums because such information spreads in the conversations among multiple users in a thread.", "labels": [], "entities": []}, {"text": "A lot of previous work has focused on extracting the question and answer sentences from forum threads.", "labels": [], "entities": [{"text": "extracting the question and answer sentences from forum threads", "start_pos": 38, "end_pos": 101, "type": "TASK", "confidence": 0.8245401647355821}]}, {"text": "However, there is much richer information in forum conversations, and simply knowing a sentence is a question or answer is not enough.", "labels": [], "entities": []}, {"text": "For example, in technical support forums, often it takes several iterations of asking and clarifications to describe the question.", "labels": [], "entities": []}, {"text": "The same happens to answers.", "labels": [], "entities": []}, {"text": "Usually several candidate answers are provided, and not all answers are useful.", "labels": [], "entities": []}, {"text": "In this case users' feedback is needed to judge the correctness of answers.", "labels": [], "entities": []}, {"text": "shows an example thread in a technical support forum.", "labels": [], "entities": []}, {"text": "Each sentence is labeled with its type (a detailed description of sentence types is provided).", "labels": [], "entities": []}, {"text": "We can see from the example that questions and answers are not expressed in a single sentence or a single post.", "labels": [], "entities": []}, {"text": "Only identifying question and answering sentences from the thread is not enough for automatic question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7500123083591461}]}, {"text": "For this example, in order to get the complete question, we would need to know that sentence S3 is a question that inquires for more details about the problem asked earlier, instead of stating its own question.", "labels": [], "entities": []}, {"text": "Also, sentence S5 should not be included in the correct answer since it is not a working solution, which is indicated by a negative feedback in sentence S6.", "labels": [], "entities": []}, {"text": "The correct solution should be sentence S7, because of a user's positive confirmation S9.", "labels": [], "entities": []}, {"text": "We define that there is a dependency between a pair of sentences if one sentence This example shows that in order to extract information from QA forums accurately, we need to understand the sentence dependency structure of a QA thread.", "labels": [], "entities": []}, {"text": "Towards this goal, in this paper, we define two tasks: labeling the types for sentences, and finding the dependency relations between sentences.", "labels": [], "entities": []}, {"text": "For the first task of sentence type labeling, we define a rich set of categories representing the purpose of the sentences.", "labels": [], "entities": [{"text": "sentence type labeling", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7260729571183523}]}, {"text": "We use linear-chain conditional random fields (CRF) to take advantage of many long-distance and non-local features.", "labels": [], "entities": []}, {"text": "The second task is to identify relations between sentences.", "labels": [], "entities": []}, {"text": "Most previous work only focused on finding the answerquestion relationship between sentences.", "labels": [], "entities": []}, {"text": "However, other relations can also be useful for information extraction from online threads, such as user's feedbacks on the answers, problem detail inquiry and question clarifications.", "labels": [], "entities": [{"text": "information extraction from online threads", "start_pos": 48, "end_pos": 90, "type": "TASK", "confidence": 0.8133069396018981}, {"text": "question clarifications", "start_pos": 160, "end_pos": 183, "type": "TASK", "confidence": 0.6796540170907974}]}, {"text": "In this study, we use two approaches for labeling of dependency relation between sentences.", "labels": [], "entities": [{"text": "labeling of dependency relation between sentences", "start_pos": 41, "end_pos": 90, "type": "TASK", "confidence": 0.776497095823288}]}, {"text": "First each sentence is considered as a source, and we run a linear-chain CRF to label whether each of the other sentences is its target.", "labels": [], "entities": []}, {"text": "Because multiple runs of separate linear-chain CRFs ignore the dependency between source sentences, the second approach we propose is to use a 2D CRF that models all pair relationships jointly.", "labels": [], "entities": []}, {"text": "The data we used was collected from Ubuntu forum general help section.", "labels": [], "entities": [{"text": "Ubuntu forum general help section", "start_pos": 36, "end_pos": 69, "type": "DATASET", "confidence": 0.9330310821533203}]}, {"text": "Our experimental results show that our proposed sentence type tagging method works very well, even for the minority categories, and that using 2D CRF further improves performance over linear-chain CRFs for identifying dependency relation between sentences.", "labels": [], "entities": [{"text": "sentence type tagging", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.6753528217474619}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the following section, we discuss related work on finding questions and answers in online environment as well as some dialog act tagging techniques.", "labels": [], "entities": [{"text": "dialog act tagging", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.6783963441848755}]}, {"text": "In Section 3, we introduce the use of CRFs for sentence type and dependency tagging.", "labels": [], "entities": [{"text": "sentence type", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7125537991523743}, {"text": "dependency tagging", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7200386226177216}]}, {"text": "Section 4 describes data collection, annotation, and some analysis.", "labels": [], "entities": [{"text": "data collection", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7332298159599304}]}, {"text": "In Section 5, we show that our approach achieves promising results in thread sentence dependency tagging.", "labels": [], "entities": [{"text": "thread sentence dependency tagging", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.6819860339164734}]}, {"text": "Finally we conclude the paper and suggest some possible future extensions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiment, we randomly split annotated threads into three disjoint sets, and run a three-fold cross validation.", "labels": [], "entities": []}, {"text": "Within each fold, first sentence types are labeled using linear-chain CRFs, then the resulting sentence type tagging is used in the second pass to determine dependency relations.", "labels": [], "entities": [{"text": "sentence type tagging", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.7107967138290405}]}, {"text": "For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger ().", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging of the sentences", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.7102776356041431}]}, {"text": "All the graphical inference and estimations are done using MALLET package.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.5232354402542114}]}, {"text": "In this paper, we evaluate the results using standard precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9983190894126892}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9987216591835022}]}, {"text": "In the sentence type tagging task, we calculate precision, recall, and F 1 score for each individual tag.", "labels": [], "entities": [{"text": "sentence type tagging task", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.7461385726928711}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9997465014457703}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9993927478790283}, {"text": "F 1 score", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9908113876978556}]}, {"text": "For the dependency tagging task, a pair identified by the system is correct only if the exact pair appears in the reference annotation.", "labels": [], "entities": [{"text": "dependency tagging task", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.8455105225245158}]}, {"text": "Precision and recall scores are calculated accordingly.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9921414852142334}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9945980310440063}]}], "tableCaptions": [{"text": " Table 4: Distribution and Inter-annotator Agreement of  Sentence Types in Data", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9532210230827332}]}, {"text": " Table 5: Sentence Type Tagging Performance Using  CRFs and HMM.", "labels": [], "entities": [{"text": "Sentence Type Tagging", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8987049261728922}]}, {"text": " Table 6: Sentence Dependency Tagging Performance", "labels": [], "entities": [{"text": "Sentence Dependency Tagging", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.950501004854838}]}]}