{"title": [{"text": "Estimating Compact Yet Rich Tree Insertion Grammars", "labels": [], "entities": [{"text": "Estimating Compact Yet Rich Tree Insertion Grammars", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8117515700204032}]}], "abstractContent": [{"text": "We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes.", "labels": [], "entities": [{"text": "estimating tree insertion grammars (TIG)", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.8044106321675437}, {"text": "Bayesian inference of tree substitution grammars (TSG)", "start_pos": 117, "end_pos": 171, "type": "TASK", "confidence": 0.6763496862517463}]}, {"text": "Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blun-som (2010).", "labels": [], "entities": [{"text": "cubic-time string parsing", "start_pos": 175, "end_pos": 200, "type": "TASK", "confidence": 0.705898384253184}]}, {"text": "We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9901447296142578}]}], "introductionContent": [{"text": "There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexitymaking induction of the grammars and parsing of novel sentences computationally practical.", "labels": [], "entities": [{"text": "statistical modeling of grammatical structure", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.8138388872146607}, {"text": "parsing of novel sentences", "start_pos": 226, "end_pos": 252, "type": "TASK", "confidence": 0.8430002182722092}]}, {"text": "Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (.", "labels": [], "entities": []}, {"text": "DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data.", "labels": [], "entities": []}, {"text": "The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal.", "labels": [], "entities": []}, {"text": "For instance, imagine modeling the following set of structures: \u2022 A natural recurring structure here would be the structure \"[ NP the [ N N president]]\", yet it occurs not at all in the data.", "labels": [], "entities": []}, {"text": "TSGs area special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (.", "labels": [], "entities": [{"text": "TSGs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5049625039100647}]}, {"text": "TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for \"splicing in\" of syntactic fragments within trees.", "labels": [], "entities": []}, {"text": "In the example, by augmenting a TSG with an operation of adjunction, a grammar that hypothesizes auxiliary trees corresponding to adjoining \"[ N N former N N ]\", \"[ N N N N of the university]\", and \"[ N N N N who resigned yesterday]\" would be able to reuse the basic structure \" Unfortunately, TAG's expressivity comes at the cost of greatly increased complexity.", "labels": [], "entities": []}, {"text": "Parsing complexity for unconstrained TAG scales as O(n 6 ), im- practical as compared to CFG and TSG's O(n 3 ).", "labels": [], "entities": [{"text": "O", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9955127835273743}, {"text": "CFG", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.9257342219352722}, {"text": "O", "start_pos": 103, "end_pos": 104, "type": "METRIC", "confidence": 0.8395496606826782}]}, {"text": "In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.", "labels": [], "entities": [{"text": "TAG", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8503903150558472}]}, {"text": "1 This has led researchers to resort to heuristic grammar extraction techniques or using a very small number of grammar categories.", "labels": [], "entities": [{"text": "heuristic grammar extraction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6671065092086792}]}, {"text": "first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference.", "labels": [], "entities": []}, {"text": "TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point.", "labels": [], "entities": [{"text": "TIG", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5848978161811829}]}, {"text": "By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguistic phenomena (.", "labels": [], "entities": [{"text": "TIG", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.8134186267852783}]}, {"text": "depicts some examples of TIG derivations.", "labels": [], "entities": [{"text": "TIG derivations", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7906806766986847}]}, {"text": "Sharing the same intuitions, Shindo et al.", "labels": [], "entities": []}, {"text": "(2011) have provided a previous attempt at combining TIG and Bayesian nonparametric principles, albeit with severe limitations.", "labels": [], "entities": [{"text": "TIG", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.5103107690811157}]}, {"text": "Their TIG variant (which we will refer to as TIG 0 ) is highly constrained in the following ways.", "labels": [], "entities": []}, {"text": "1. The foot node in an auxiliary tree must be the immediate child of the root node.", "labels": [], "entities": []}, {"text": "2. Only one adjunction can occur at a given node.", "labels": [], "entities": []}, {"text": "1 This can be seen by the fact that tree-path languages under TAG are context free, whereas they are regular for TSG.", "labels": [], "entities": []}, {"text": "(We suppress the rest of the transformational nodes.)", "labels": [], "entities": []}, {"text": "3. Even modeling multiple adjunction with root adjunction is disallowed.", "labels": [], "entities": []}, {"text": "There is thus no recursion possibility with adjunction, no stacking of auxiliary trees.", "labels": [], "entities": []}, {"text": "4. As a consequence of the prior two constraints, no adjunction along the spines of auxiliary trees is allowed.", "labels": [], "entities": []}, {"text": "5. As a consequence of the first constraint, all nonterminals along the spine of an auxiliary tree are identical.", "labels": [], "entities": []}, {"text": "In this paper we explore a Bayesian nonparametric model for estimating afar more expressive version of TIG, and compare its performance against TSG and the restricted TIG 0 variant.", "labels": [], "entities": [{"text": "TSG", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.7959890961647034}]}, {"text": "Our more general formulation avoids these limitations by supporting the following features and thus relaxing four of the five restrictions of TIG 0 . 1. Auxiliary trees may have the foot node at depth greater than one.", "labels": [], "entities": [{"text": "TIG 0 . 1. Auxiliary trees", "start_pos": 142, "end_pos": 168, "type": "DATASET", "confidence": 0.8854488631089529}]}, {"text": "Both left and right adjunctions may occur at the same node.", "labels": [], "entities": []}, {"text": "3. Simultanous adjunction (that is, more than one left or right adjunction per node) is allowed via root adjunction.", "labels": [], "entities": []}, {"text": "4. Adjunctions may occur along the spines of auxiliary trees.", "labels": [], "entities": []}, {"text": "The increased expressivity of our TIG variant is motivated both linguistically and practically.", "labels": [], "entities": []}, {"text": "From a linguistic point of view: Deeper auxiliary trees can help model large patterns of insertion and potential correlations between lexical items that extend over multiple levels of tree.", "labels": [], "entities": []}, {"text": "Combining left and right auxiliary trees can help model modifiers of the same node from left and right (combination of adjectives and relative clauses for instance).", "labels": [], "entities": []}, {"text": "Simultaneous insertion allows us to deal with multiple independent modifiers for the same constituent (for example, a series of adjectives).", "labels": [], "entities": []}, {"text": "From a practical point of view, we show that an induced TIG provides modeling performance superior to TSG and comparable with TIG 0 . However we show that the grammars we induce are compact yet rich, in that they succinctly represent complex linguistic structures.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the standard Penn treebank methodology of training on sections 2-21 and testing on section 23.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9836302101612091}]}, {"text": "All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser.", "labels": [], "entities": []}, {"text": "As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set.", "labels": [], "entities": []}, {"text": "All hyperparameters are resampled under appropriate vague gamma and beta priors.", "labels": [], "entities": []}, {"text": "All reported numbers are averages over three runs.", "labels": [], "entities": []}, {"text": "Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.7980333566665649}, {"text": "CFG", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.9577143788337708}]}, {"text": "We compare our system (referred to as TIG) to our implementation of the TSG system of) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011) (referred to as TIG 0 ).", "labels": [], "entities": []}, {"text": "The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85.4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance).", "labels": [], "entities": [{"text": "TIG", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.7478484511375427}, {"text": "parsing", "start_pos": 303, "end_pos": 310, "type": "TASK", "confidence": 0.9696998000144958}]}, {"text": "Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees.", "labels": [], "entities": []}, {"text": "As a result, TIG finds many chances to apply insertions to test sentences, whereas TIG 0 depends mostly on TSG rules.", "labels": [], "entities": [{"text": "TIG", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.58791184425354}, {"text": "TIG 0", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9181882739067078}]}, {"text": "If we look at the most likely derivations for the test data, TIG 0 assigns 663 insertions (351 left insertions) in the parsing of entire Section 23, meanwhile TIG assigns 3924 (2100 left insertions).", "labels": [], "entities": [{"text": "TIG 0", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.5404119789600372}, {"text": "insertions", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.8348757028579712}, {"text": "TIG", "start_pos": 159, "end_pos": 162, "type": "DATASET", "confidence": 0.8160448670387268}]}, {"text": "Some of these linguistically sophisticated auxiliary trees that apply to test data are listed in.", "labels": [], "entities": []}], "tableCaptions": []}