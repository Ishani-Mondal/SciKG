{"title": [{"text": "Reducing Wrong Labels in Distant Supervision for Relation Extraction", "labels": [], "entities": [{"text": "Distant Supervision", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8549096882343292}, {"text": "Relation Extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.9575582444667816}]}], "abstractContent": [{"text": "In relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as Freebase, as a source of supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.893743246793747}]}, {"text": "When a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base.", "labels": [], "entities": []}, {"text": "However, this heuristic can fail with the result that some sentences are labeled wrongly.", "labels": [], "entities": []}, {"text": "This noisy labeled data causes poor extraction performance.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to reduce the number of wrong labels.", "labels": [], "entities": []}, {"text": "We present a novel generative model that directly models the heuristic labeling process of distant supervision.", "labels": [], "entities": []}, {"text": "The model predicts whether assigned labels are corrector wrong via its hidden variables.", "labels": [], "entities": []}, {"text": "Our experimental results show that this model detected wrong labels with higher performance than baseline methods.", "labels": [], "entities": []}, {"text": "In the experiment , we also found that our wrong label reduction boosted the performance of relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8823642432689667}]}], "introductionContent": [{"text": "Machine learning approaches have been developed to address relation extraction, which is the task of extracting semantic relations between entities expressed in text.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8329334855079651}, {"text": "extracting semantic relations between entities expressed in text", "start_pos": 101, "end_pos": 165, "type": "TASK", "confidence": 0.663749597966671}]}, {"text": "Supervised approaches are limited in scalability because labeled data is expensive to produce.", "labels": [], "entities": []}, {"text": "A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase ( With DS it is assumed that if a sentence contains an entity pair in a knowledge base, such a sentence actually expresses the corresponding relation in the knowledge base.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 173, "end_pos": 181, "type": "DATASET", "confidence": 0.9559410214424133}]}, {"text": "However, the DS assumption can fail, which results in noisy labeled data and this causes poor extraction performance.", "labels": [], "entities": [{"text": "DS", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.8296377062797546}]}, {"text": "An entity pair in a target text generally expresses more than one relation while a knowledge base stores a subset of the relations.", "labels": [], "entities": []}, {"text": "The assumption ignores this possibility.", "labels": [], "entities": []}, {"text": "For instance, consider the place of birth relation between Michael Jackson and Gary in.", "labels": [], "entities": []}, {"text": "The upper sentence indeed expresses the place of birth relation between the two entities.", "labels": [], "entities": []}, {"text": "In DS place of birth is assigned to the sentence, and it becomes a useful training example.", "labels": [], "entities": [{"text": "DS place of birth", "start_pos": 3, "end_pos": 20, "type": "DATASET", "confidence": 0.6788591295480728}]}, {"text": "On the other hand, the lower sentence does not express this relation between the two entities, but the DS heuristic wrongly labels the sentence as expressing it.  relax the DS assumption as at least one sentence containing an entity pair ex-pressing the corresponding relation in the knowledge base.", "labels": [], "entities": []}, {"text": "They cast the relaxed assumption as multi-instance learning.", "labels": [], "entities": []}, {"text": "However, even the relaxed assumption can fail.", "labels": [], "entities": []}, {"text": "The relaxation is equivalent to the DS assumption when a labeled pair of entities is mentioned once in a target corpus ( . In fact, 91.7% of entity pairs appear only once in Wikipedia articles (see.", "labels": [], "entities": [{"text": "DS", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.6866457462310791}]}, {"text": "In this paper, we propose a method to reduce the number of wrong labels generated by DS without using either of these assumptions.", "labels": [], "entities": []}, {"text": "Given the labeled corpus created with the DS assumption, we first predict whether each pattern, which frequently appears in text to express a relation (see Section 4), expresses a target relation.", "labels": [], "entities": []}, {"text": "Patterns that are predicted not to express the relation are used to form a negative pattern list for removing wrong labels of the relation.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are as follows: \u2022 To make the pattern prediction, we propose a generative model that directly models the process of automatic labeling in DS.", "labels": [], "entities": [{"text": "pattern prediction", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.73326076567173}]}, {"text": "Without any strong assumptions like 's, the model predicts whether each pattern expresses each relation via hidden variables (see Section 5).", "labels": [], "entities": []}, {"text": "\u2022 Our variational inference for our generative model lets us automatically calibrate parameters for each relation, which are sensitive to the performance (see Section 6).", "labels": [], "entities": []}, {"text": "\u2022 We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS () and MultiR (), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 354, "end_pos": 373, "type": "TASK", "confidence": 0.7737719714641571}]}], "datasetContent": [{"text": "We performed two sets of experiments.", "labels": [], "entities": []}, {"text": "Experiment 1 aimed to evaluate the performance of our generative model itself, which predicts whether a pattern expresses a relation, given a labeled corpus created with the DS assumption.", "labels": [], "entities": []}, {"text": "Experiment 2 aimed to evaluate how much our wrong label reduction in Section 4 improved the performance of relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8752419650554657}]}, {"text": "In our method, we trained a classifier with a labeled corpus cleaned by Algorithm 1 using the negative pattern list predicted by the generative model.", "labels": [], "entities": []}, {"text": "Following, we carried out our experiments using Wikipedia as the target corpus and Freebase) as the knowledge base.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9565964937210083}, {"text": "Freebase", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.9424418210983276}]}, {"text": "We used more than 1,300,000 Wikipedia articles in the wex dump data).", "labels": [], "entities": [{"text": "wex dump data", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.8396960695584615}]}, {"text": "The properties of our data are shown in.", "labels": [], "entities": []}, {"text": "In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter ().", "labels": [], "entities": []}, {"text": "We applied Open NLP POS tagger to sentences containing more than one named entity.", "labels": [], "entities": [{"text": "Open NLP POS tagger", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6175059750676155}]}, {"text": "We then extracted sentences containing related entity pairs with the method explained in Section 3.", "labels": [], "entities": []}, {"text": "To match entity pairs, we used ID mapping between the dump data and Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9684159755706787}]}, {"text": "We used the most frequent 24 relations.", "labels": [], "entities": []}, {"text": "We compared our model with baseline methods in terms of ability to predict patterns that express a given relation.", "labels": [], "entities": []}, {"text": "The input of this task was X r s, which expresses whether or not each entity pair appearing with each pattern is labeled with relation r, as explained in Section 5.", "labels": [], "entities": []}, {"text": "In Experiment 1, since we needed entity types for patterns, we restricted ourselves to entities matched with Freebase, which also provides entity types for entities.", "labels": [], "entities": []}, {"text": "We used patterns that appear more than 20 times in the corpus.", "labels": [], "entities": []}, {"text": "We split the data into training data and test data.", "labels": [], "entities": []}, {"text": "The training data was X r s for 12 relations and the test data was that for the remaining 12 relations.", "labels": [], "entities": []}, {"text": "The training data was used to calibrate parameters (see the following subsection for details).", "labels": [], "entities": []}, {"text": "The test data was used for evaluation.", "labels": [], "entities": []}, {"text": "We randomly split the data five times and took the average of the following evaluation values.", "labels": [], "entities": []}, {"text": "We evaluated the performance by precision, recall, and F value.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9997738003730774}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9995898604393005}, {"text": "F value", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9849770665168762}]}, {"text": "They were calculated using gold standard data, which was constructed by hand.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.799915611743927}]}, {"text": "We manually selected patterns that actually express a target relation as positive patterns for the relation.", "labels": [], "entities": []}, {"text": "We averaged the evaluation values in terms of macro average over relations before averaging over the data splits.", "labels": [], "entities": []}, {"text": "We investigated the performance of relation extraction using our wrong label reduction, which uses the results of the pattern prediction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.877705991268158}]}, {"text": "Following, we performed an automatic held-out evaluation and a manual evaluation.", "labels": [], "entities": []}, {"text": "In both cases, we used 400,000 articles for testing and the remaining 903,000 for training.", "labels": [], "entities": []}, {"text": "In the held-out evaluation, relation instances discovered from testing articles were automatically compared with those in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.9591855406761169}]}, {"text": "This let us calculate the precision of each method for the best n relation instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9994643330574036}]}, {"text": "The precisions are underestimated because this evaluation suffers from false negatives due to the incompleteness of Freebase.", "labels": [], "entities": [{"text": "precisions", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9995279312133789}, {"text": "Freebase", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.9845635890960693}]}, {"text": "We changed n from 5 to 50,000 and measured precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9996685981750488}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.999319314956665}]}, {"text": "Precision-recall curves for the held-out data are shown in.", "labels": [], "entities": [{"text": "Precision-recall", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9918796420097351}]}, {"text": "PROP achieved comparable or higher precision at most recall levels compared with LR and MultiR.", "labels": [], "entities": [{"text": "PROP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5330215096473694}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9991773962974548}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9988712668418884}, {"text": "MultiR", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.8165518045425415}]}, {"text": "Its performance at n = 50,000 is much higher than that of the others.", "labels": [], "entities": []}, {"text": "While our generative model does not use unlabeled examples as negative ones in detecting wrong labels, classifier-based approaches including MultiR do, suffering from false negatives.", "labels": [], "entities": []}, {"text": "For manual evaluation, we picked the top ranked 50 relation instances for the most frequent 15 relations.", "labels": [], "entities": []}, {"text": "The manually evaluated precisions averaged over the 15 relations are shown in table 4.", "labels": [], "entities": [{"text": "precisions", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9964433312416077}]}, {"text": "PROP achieved the best average precision.", "labels": [], "entities": [{"text": "PROP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7271589636802673}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.954912006855011}]}, {"text": "For place of birth, LR wrongly extracted entity pairs with \" played with club\", which does not express the relation.", "labels": [], "entities": []}, {"text": "PROP and MultiR avoided this mistake.", "labels": [], "entities": [{"text": "PROP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9286816120147705}]}, {"text": "For place of death, LR and MultiR wrongly extracted entity pairs with \" moved to\".", "labels": [], "entities": []}, {"text": "Multi-instance learning does notwork for wrong labels assigned to entity pairs that appear only once in a corpus.", "labels": [], "entities": []}, {"text": "In fact, 72% of entity pairs that appeared with this pattern and were wrongly labeled as place of death appeared only once in the corpus.", "labels": [], "entities": []}, {"text": "Only PROP avoided mistakes of this kind because our method works in such situations.", "labels": [], "entities": [{"text": "PROP", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.45771920680999756}]}], "tableCaptions": [{"text": " Table 2: Averages of precision, recall, and F value in Ex- periment 1. The averages of threshold of RS(rank) and  RS(value) were 6.2 \u00b1 3.2 and 0.10 \u00b1 0.06, respectively.  The averages of hyperparameters of PROP were 0.84 \u00b1  0.05 for \u03bb and 0.85 \u00b1 0.10 for the threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9994992017745972}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.999237060546875}, {"text": "F value", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9842268228530884}, {"text": "RS(rank)", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9217352271080017}, {"text": "RS(value)", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.938781127333641}]}, {"text": " Table 3: Example of estimated \u03c6 rs for r =  place of birth. Entity types are omitted in patterns.  n rs /N s is the ratio of the number of labeled entity pairs  to the number of entity pairs appearing with pattern s.", "labels": [], "entities": []}, {"text": " Table 4: Averages of precisions at 50 for the most fre- quent 15 relations as well as example relations.", "labels": [], "entities": [{"text": "precisions", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9785363078117371}]}]}