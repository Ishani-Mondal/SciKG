{"title": [{"text": "Verb Classification using Distributional Similarity in Syntactic and Semantic Structures", "labels": [], "entities": [{"text": "Verb Classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8081084191799164}]}], "abstractContent": [{"text": "In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet.", "labels": [], "entities": [{"text": "automatic classification of verbs", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7205166071653366}]}, {"text": "First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined.", "labels": [], "entities": []}, {"text": "Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distri-butional and grammatical information in Support Vector Machines.", "labels": [], "entities": []}, {"text": "The extensive empirical analysis on VerbNet class and frame detection shows that our models capture meaningful syntactic/semantic structures, which allows for improving the state-of-the-art.", "labels": [], "entities": [{"text": "frame detection", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.743692934513092}]}], "introductionContent": [{"text": "Verb classification is a fundamental topic of computational linguistics research given its importance for understanding the role of verbs in conveying semantics of natural language (NL).", "labels": [], "entities": [{"text": "Verb classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8102385401725769}, {"text": "conveying semantics of natural language (NL)", "start_pos": 141, "end_pos": 185, "type": "TASK", "confidence": 0.8038858994841576}]}, {"text": "Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7162381112575531}, {"text": "shallow semantic parsing", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.6818911532560984}, {"text": "information extraction", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.8090609312057495}]}, {"text": "Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet () and FrameNet (, which has also fostered production of many automatic approaches to predicate argument extraction.", "labels": [], "entities": [{"text": "predicate argument extraction", "start_pos": 173, "end_pos": 202, "type": "TASK", "confidence": 0.6904729803403219}]}, {"text": "Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense ().", "labels": [], "entities": []}, {"text": "However, the definition of models for optimally combining lexical and syntactic constraints is still far for being accomplished.", "labels": [], "entities": []}, {"text": "In particular, the exhaustive design and experimentation of lexical and syntactic features for learning verb classification appears to be computationally problematic.", "labels": [], "entities": [{"text": "learning verb classification", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6978032986323038}]}, {"text": "For example, the verb order can belongs to the two VerbNet classes: -The class 60.1, i.e., order someone to do something as shown in: The Illinois Supreme Court ordered the commission to audit Commonwealth Edison 's construction expenses and refund any unreasonable expenses . -The class 13.5.1: order or request something like in: ...", "labels": [], "entities": []}, {"text": "Michelle blabs about it to a sandwich man while ordering lunch over the phone . Clearly, the syntactic realization can be used to discern the cases above but it would not be enough to correctly classify the following verb occurrence: ..", "labels": [], "entities": []}, {"text": "ordered the lunch to be delivered ..", "labels": [], "entities": []}, {"text": "For such a case, selectional restrictions are needed.", "labels": [], "entities": []}, {"text": "These have also been shown to be useful for semantic role classification ().", "labels": [], "entities": [{"text": "semantic role classification", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.8500551780064901}]}, {"text": "Note that their coding in learning algorithms is rather complex: we need to take into account syntactic structures, which may require an exponential number of syntactic features (i.e., all their possible substructures).", "labels": [], "entities": []}, {"text": "Moreover, these have to be enriched with lexical information to trig lexical preference.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the problem above by studying innovative representations for automatic verb classification according to VerbNet and FrameNet.", "labels": [], "entities": [{"text": "automatic verb classification", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.664066861073176}, {"text": "VerbNet", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9144141674041748}, {"text": "FrameNet", "start_pos": 141, "end_pos": 149, "type": "DATASET", "confidence": 0.8502321243286133}]}, {"text": "We define syntactic and semantic structures capturing essential lexical and syntactic properties of verbs.", "labels": [], "entities": []}, {"text": "Then, we apply similarity between such structures, i.e., kernel functions, which can also exploit distributional lexical semantics, to train automatic classifiers.", "labels": [], "entities": []}, {"text": "The basic idea of such functions is to compute the similarity between two verbs in terms of all the possible substructures of their syntactic frames.", "labels": [], "entities": []}, {"text": "We define and automatically extract a lexicalized approximation of the latter.", "labels": [], "entities": []}, {"text": "Then, we apply kernel functions that jointly model structural and lexical similarity so that syntactic properties are combined with generalized lexemes.", "labels": [], "entities": []}, {"text": "The nice property of kernel functions is that they can be used in place of the scalar product of feature vectors to train algorithms such as Support Vector Machines (SVMs).", "labels": [], "entities": []}, {"text": "This way SVMs can learn the association between syntactic (sub-) structures whose lexical arguments are generalized and target verb classes, i.e., they can also learn selectional restrictions.", "labels": [], "entities": []}, {"text": "We carried out extensive experiments on verb class and frame detection which showed that our models greatly improve on the state-of-the-art (up to about 13% of relative error reduction).", "labels": [], "entities": [{"text": "frame detection", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7342480570077896}, {"text": "relative error reduction", "start_pos": 160, "end_pos": 184, "type": "METRIC", "confidence": 0.7222134272257487}]}, {"text": "Such results are nicely assessed by manually inspecting the most important substructures used by the classifiers as they largely correlate with syntactic frames defined in VerbNet.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9337804317474365}]}, {"text": "In the rest of the paper, Sec.", "labels": [], "entities": []}, {"text": "2 reports on related work, Sec.", "labels": [], "entities": []}, {"text": "4 describe previous and our models for syntactic and semantic similarity, respectively, Sec.", "labels": [], "entities": []}, {"text": "5 illustrates our experiments, Sec.", "labels": [], "entities": []}, {"text": "6 discusses the output of the models in terms of error analysis and important structures and finally Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "In these experiments, we tested the impact of our different verb representations using different kernels, similarities and parameters.", "labels": [], "entities": []}, {"text": "We also compared with simple bag-of-words (BOW) models and the stateof-the-art.", "labels": [], "entities": []}, {"text": "We consider two different corpora: one for VerbNet and the other for FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9044066071510315}]}, {"text": "For the former, we used the same verb classification setting of).", "labels": [], "entities": []}, {"text": "Sentences are drawn from the Semlink corpus (, which consists of the PropBanked Penn Treebank portions of the Wall Street Journal.", "labels": [], "entities": [{"text": "Semlink corpus", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.826284646987915}, {"text": "PropBanked Penn Treebank portions of the Wall Street Journal", "start_pos": 69, "end_pos": 129, "type": "DATASET", "confidence": 0.9439244270324707}]}, {"text": "It contains 113K verb instances, 97K of which are verbs represented in at least one VerbNet class.", "labels": [], "entities": []}, {"text": "Semlink includes 495 verbs, whose instances are labeled with more than one class (including one single VerbNet class or none).", "labels": [], "entities": []}, {"text": "We used all instances of the corpus fora total of 45,584 instances for 180 verb classes.", "labels": [], "entities": []}, {"text": "When instances labeled with the none class are not included, the number of examples becomes 23,719.", "labels": [], "entities": []}, {"text": "The second corpus refers to FrameNet frame classification.", "labels": [], "entities": [{"text": "FrameNet frame classification", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.7515010039011637}]}, {"text": "The training and test data are drawn from the FrameNet 1.5 corpus 2 , which consists of 135K sentences annotated according the frame semantics).", "labels": [], "entities": [{"text": "FrameNet 1.5 corpus 2", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.916882649064064}]}, {"text": "We selected the subset of frames containing more than 100 sentences annotated with a verbal predicate fora total of 62,813 sentences in 187 frames (i.e., very close to the VerbNet datasets).", "labels": [], "entities": [{"text": "VerbNet datasets", "start_pos": 172, "end_pos": 188, "type": "DATASET", "confidence": 0.9753055572509766}]}, {"text": "For both the datasets, we used 70% of instances for training and 30% for testing.", "labels": [], "entities": []}, {"text": "Our verb (multi) classifier is designed with the one-vs-all () multiclassification schema.", "labels": [], "entities": []}, {"text": "This uses a set of binary SVM classifiers, one for each verb class (frame) i.", "labels": [], "entities": []}, {"text": "The sentences whose verb is labeled with the class i are positive examples for the classifier i.", "labels": [], "entities": []}, {"text": "The sentences whose verbs are compatible with the class i but evoking a different class or labeled with none (no current verb class applies) are added as negative examples.", "labels": [], "entities": []}, {"text": "In the classification phase the binary classifiers are applied by (i) only considering classes that are compatible with the target verbs; and (ii) selecting the class associated with the maximum positive SVM margin.", "labels": [], "entities": []}, {"text": "If all classifiers provide a negative score the example is labeled with none.", "labels": [], "entities": []}, {"text": "To learn the binary classifiers of the schema above, we coded our modified SPTK in SVM-Light-TK 3).", "labels": [], "entities": []}, {"text": "The parameterization of each classifier is carried on a held-out set (30% of the training) and is concerned with the setting of the trade-off parameter (option -c) and the leaf weight (lw) (see Alg.", "labels": [], "entities": [{"text": "leaf weight (lw)", "start_pos": 172, "end_pos": 188, "type": "METRIC", "confidence": 0.8261161923408509}]}, {"text": "1), which is used to linearly scale the contribution of the leaf nodes.", "labels": [], "entities": []}, {"text": "In contrast, the cost-factor parameter of SVM-Light-TK is set as the ratio between the number of negative and positive examples for attempting to have a balanced Precision/Recall.", "labels": [], "entities": [{"text": "Precision/Recall", "start_pos": 162, "end_pos": 178, "type": "TASK", "confidence": 0.558618426322937}]}, {"text": "Regarding SPTK setting, we used the lexical similarity \u03c3 defined in Sec.", "labels": [], "entities": []}, {"text": "In more detail, LSA was applied to ukWak (, which is a large scale document collection made up of 2 billion tokens.", "labels": [], "entities": [{"text": "LSA", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.8106035590171814}, {"text": "ukWak", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.979868471622467}]}, {"text": "M is constructed by applying POS tagging to build rows with pairs lemma, ::POS (lemma::POS in brief).", "labels": [], "entities": []}, {"text": "The contexts of such items are the columns of M and are short windows of size [\u22123, +3], centered on the items.", "labels": [], "entities": []}, {"text": "This allows for better capturing syntactic properties of words.", "labels": [], "entities": []}, {"text": "The most frequent 20,000 items are selected along with their 20k contexts.", "labels": [], "entities": []}, {"text": "The entries of M are the point-wise mutual (Structural kernels in SVMLight: FrameNet accuracy without the none class information between them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.7333130240440369}]}, {"text": "SVD reduction is then applied to M, with a dimensionality cut of l = 250.", "labels": [], "entities": [{"text": "SVD reduction", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7556490302085876}]}, {"text": "For generating the CT, GRCT and LCT structures, we used the constituency trees generated by the Charniak parser) and the dependency structures generated by the LTH syntactic parser (described in).", "labels": [], "entities": [{"text": "GRCT", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.7301313281059265}]}, {"text": "The classification performance is measured with accuracy (i.e., the percentage of correct classification).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9995899796485901}]}, {"text": "We also derive statistical significance of the results by using the model described in and implemented in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: VerbNet accuracy with the none class", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9761600494384766}]}, {"text": " Table 2: FrameNet accuracy without the none class", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.958407461643219}]}, {"text": " Table 3: VerbNet accuracy without the none class", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.975257933139801}]}]}