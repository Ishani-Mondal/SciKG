{"title": [], "abstractContent": [{"text": "To address semantic ambiguities in corefer-ence resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8874781429767609}]}, {"text": "Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence.", "labels": [], "entities": []}, {"text": "When added to a state-of-the-art coreference base-line, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B 3), resulting in the best results reported to date for the end-to-end task of coreference resolution.", "labels": [], "entities": [{"text": "ACE 2004 and ACE 2005", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.8315458416938781}, {"text": "MUC and B 3)", "start_pos": 154, "end_pos": 166, "type": "METRIC", "confidence": 0.6801551640033722}, {"text": "coreference resolution", "start_pos": 242, "end_pos": 264, "type": "TASK", "confidence": 0.9615097641944885}]}], "introductionContent": [{"text": "Many of the most difficult ambiguities in coreference resolution are semantic in nature.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9764933884143829}]}, {"text": "For instance, consider the following example: When Obama met Jobs, the president discussed the economy, technology, and education.", "labels": [], "entities": []}, {"text": "His election campaign is expected to For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president.", "labels": [], "entities": [{"text": "coreference", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9239699840545654}]}, {"text": "Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not.", "labels": [], "entities": []}, {"text": "Such ambiguities are difficult to resolve on purely syntactic or configurational grounds.", "labels": [], "entities": []}, {"text": "There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks.", "labels": [], "entities": [{"text": "coreference resolution tasks", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.9459896882375082}]}, {"text": "Most work () addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7404745072126389}]}, {"text": "There is also work on end-to-end coreference resolution that uses large noun-similarity lists) or structured knowledge bases such as Wikipedia () and).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.863188236951828}, {"text": "Wikipedia", "start_pos": 133, "end_pos": 142, "type": "DATASET", "confidence": 0.9470574855804443}]}, {"text": "However, such structured knowledge bases are of limited scope, and, while self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale.", "labels": [], "entities": []}, {"text": "In this paper, we look to the Web for broader if shallower sources of semantics.", "labels": [], "entities": []}, {"text": "In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts () which, in aggregate, contain diffuse and indirect, but often robust, cues to reference.", "labels": [], "entities": []}, {"text": "For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface affinities (i.e., versus).", "labels": [], "entities": []}, {"text": "We can also count co-occurrence statistics of competing antecedents when placed in the context of an anaphoric pronoun (i.e., Obama's election campaign versus Jobs' election campaign).", "labels": [], "entities": []}, {"text": "All of our features begin with a pair of headwords from candidate mention pairs and compute statistics derived from various potentially informative queries' counts.", "labels": [], "entities": []}, {"text": "We explore five major categories of semantically informative Web features, based on (1) general lexical affinities (via generic co-occurrence statistics), (2) lexical relations (via Hearst-style hypernymy patterns), (3) similarity of entity-based context (e.g., common values of y for which h is a y is attested), (4) matches of distributional soft cluster ids, and (5) attested substitutions of candidate antecedents in the context of a pronominal anaphor.", "labels": [], "entities": []}, {"text": "We first describe a strong baseline consisting of the mention-pair model of the Reconcile system () using a decision tree (DT) as its pairwise classifier.", "labels": [], "entities": []}, {"text": "To this baseline system, we add our suite of features in turn, each class of features providing substantial gains.", "labels": [], "entities": []}, {"text": "Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets and metrics (MUC and B 3 ), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.8986313045024872}, {"text": "MUC and B 3 )", "start_pos": 190, "end_pos": 203, "type": "METRIC", "confidence": 0.6441979467868805}, {"text": "Reconcile DT baseline", "start_pos": 249, "end_pos": 270, "type": "DATASET", "confidence": 0.8312636812527975}]}], "datasetContent": [{"text": "We evaluated our work on both MUC ( and B 3 (Bagga and Baldwin, 1998).", "labels": [], "entities": [{"text": "MUC", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.539865255355835}]}, {"text": "Both scorers are available in the Reconcile infrastructure.", "labels": [], "entities": [{"text": "Reconcile infrastructure", "start_pos": 34, "end_pos": 58, "type": "DATASET", "confidence": 0.9425890445709229}]}, {"text": "8 MUC measures how many predicted clusters need to be merged to cover the true gold clusters.", "labels": [], "entities": [{"text": "MUC", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.7360320687294006}]}, {"text": "B 3 computes precision and recall for each mention by computing the intersection of its predicted and gold cluster and dividing by the size of the predicted Note that the development set is used only for ACE04, because for ACE05, and ACE05-ALL, we directly test using the features tuned on ACE04.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9991495609283447}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9994470477104187}, {"text": "ACE04", "start_pos": 204, "end_pos": 209, "type": "DATASET", "confidence": 0.935436487197876}, {"text": "ACE05", "start_pos": 223, "end_pos": 228, "type": "DATASET", "confidence": 0.9492321014404297}, {"text": "ACE05-ALL", "start_pos": 234, "end_pos": 243, "type": "DATASET", "confidence": 0.926405668258667}, {"text": "ACE04", "start_pos": 290, "end_pos": 295, "type": "DATASET", "confidence": 0.9642179012298584}]}, {"text": "8 Note that B 3 has two versions which handle twinless (spurious) mentions in different ways (see for details).", "labels": [], "entities": []}, {"text": "We use the B 3 All version, unless mentioned otherwise.", "labels": [], "entities": []}, {"text": "Incremental results for the Web features on the ACE04 development set.", "labels": [], "entities": [{"text": "ACE04 development set", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9746234814325968}]}, {"text": "AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline.", "labels": [], "entities": []}, {"text": "The feature categories correspond to those described in Section 3. and gold cluster, respectively.", "labels": [], "entities": []}, {"text": "It is well known) that MUC is biased towards large clusters (chains) whereas B 3 is biased towards singleton clusters.", "labels": [], "entities": []}, {"text": "Therefore, fora more balanced evaluation, we show improvements on both metrics simultaneously.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset characteristics -docs: the total number of doc- uments; dev: the train/test split during development; test: the  train/test split during testing; ment: the number of gold men- tions in the test split; chn: the number of coreference chains in  the test split.", "labels": [], "entities": []}, {"text": " Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically  extracted system mentions. B 3 here is the B 3 All version of", "labels": [], "entities": [{"text": "ACE04", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9670777916908264}, {"text": "ACE05", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.6078048348426819}, {"text": "ACE05-ALL datasets", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.9719761908054352}]}]}