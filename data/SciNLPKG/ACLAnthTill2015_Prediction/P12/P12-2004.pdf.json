{"title": [{"text": "A Feature-Rich Constituent Context Model for Grammar Induction", "labels": [], "entities": [{"text": "Grammar Induction", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7926505506038666}]}], "abstractContent": [{"text": "We present LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7122478783130646}]}, {"text": "LLCCM retains the simplicity of the original CCM but extends robustly to long sentences.", "labels": [], "entities": [{"text": "LLCCM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9271925687789917}]}, {"text": "On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.", "labels": [], "entities": [{"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9420328140258789}]}], "introductionContent": [{"text": "Unsupervised grammar induction is a fundamental challenge of statistical natural language processing.", "labels": [], "entities": [{"text": "Unsupervised grammar induction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6432189444700877}, {"text": "statistical natural language processing", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.6991174668073654}]}, {"text": "The constituent context model (CCM) for inducing constituency parses () was the first unsupervised approach to surpass a right-branching baseline.", "labels": [], "entities": [{"text": "constituency parses", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.6369146257638931}]}, {"text": "However, the CCM only effectively models short sentences.", "labels": [], "entities": []}, {"text": "This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences.", "labels": [], "entities": []}, {"text": "Much recent research has explored dependency grammar induction.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.8887534141540527}]}, {"text": "For instance, the dependency model with valence (DMV) of has been extended to utilize multilingual information), lexical information, and linguistic universals (.", "labels": [], "entities": []}, {"text": "Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb.", "labels": [], "entities": []}, {"text": "Unsupervised constituency parsing is also an active research area.", "labels": [], "entities": [{"text": "Unsupervised constituency parsing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6472646097342173}]}, {"text": "Several studies have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags.", "labels": [], "entities": []}, {"text": "Additional advances have come from more complex models, such as combining CCM and DMV () and modeling large tree fragments).", "labels": [], "entities": []}, {"text": "The CCM scores each parse as a product of probabilities of span and context subsequences.", "labels": [], "entities": []}, {"text": "It was originally evaluated only on unpunctuated sentences up to length 10 (), which account for only 15% of the WSJ corpus; our experiments confirm the observation in () that performance degrades dramatically on longer sentences.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.9422808885574341}]}, {"text": "This problem is unsurprising: CCM scores each constituent type by a single, isolated multinomial parameter.", "labels": [], "entities": []}, {"text": "Our work leverages the idea that sharing information between local probabilities in a structured unsupervised model can lead to substantial accuracy gains, previously demonstrated for dependency grammar induction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9972301125526428}, {"text": "dependency grammar induction", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.8143171668052673}]}, {"text": "Our model, Log-Linear CCM (LLCCM), shares information between the probabilities of related constituents by expressing them as a log-linear combination of features trained using the gradient-based learning procedure of . In this way, the probability of generating a constituent is informed by related constituents.", "labels": [], "entities": []}, {"text": "Our model improves unsupervised constituency parsing of sentences longer than 10 words.", "labels": [], "entities": [{"text": "constituency parsing of sentences longer than 10 words", "start_pos": 32, "end_pos": 86, "type": "TASK", "confidence": 0.8328274451196194}]}, {"text": "On sentences of up to length 40 (96% of all sentences in the Penn Treebank), LLCCM outperforms CCM by 13.9% (unlabeled) bracketing F1 and, unlike CCM, outperforms a right-branching baseline on sentences longer than 15 words.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9951839745044708}]}], "datasetContent": [{"text": "We train our models on gold POS sequences from all sections (0-24) of the WSJ () with punctuation removed.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.8607083559036255}]}, {"text": "We report bracketing F1 scores between the binary trees predicted by the models on these sequences and the treebank parses.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9838743209838867}]}, {"text": "We train and evaluate both a CCM implementation (Luque, 2011) and our LLCCM on sentences up to a fixed length n, for n \u2208 {10, 15, . .", "labels": [], "entities": []}, {"text": "shows that LLCCM substantially outperforms the CCM on longer sentences.", "labels": [], "entities": []}, {"text": "After length 15, CCM accuracy falls below the right branching baseline, whereas LLCCM remains significantly better than right-branching through length 40.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9907910823822021}]}], "tableCaptions": [{"text": " Table 1: Span and context features for constituent spans (0, 3)", "labels": [], "entities": []}]}