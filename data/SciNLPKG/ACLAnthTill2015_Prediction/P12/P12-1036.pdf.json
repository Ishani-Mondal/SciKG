{"title": [], "abstractContent": [{"text": "Aspect extraction is a central problem in sentiment analysis.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9567324817180634}, {"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9699374139308929}]}, {"text": "Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling.", "labels": [], "entities": []}, {"text": "By categorizing, we mean the synonymous aspects should be clustered into the same category.", "labels": [], "entities": []}, {"text": "In this paper, we solve the problem in a different setting where the user provides some seed words fora few aspect categories and the model extracts and clusters aspect terms into categories simultaneously.", "labels": [], "entities": []}, {"text": "This setting is important because categorizing aspects is a subjective task.", "labels": [], "entities": []}, {"text": "For different application purposes, different categorizations maybe needed.", "labels": [], "entities": []}, {"text": "Some form of user guidance is desired.", "labels": [], "entities": []}, {"text": "In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants.", "labels": [], "entities": []}, {"text": "Our experimental results show that the two proposed models are indeed able to perform the task effectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aspect-based sentiment analysis is one of the main frameworks for sentiment analysis (.", "labels": [], "entities": [{"text": "Aspect-based sentiment analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8698053359985352}, {"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9648464918136597}]}, {"text": "A key task of the framework is to extract aspects of entities that have been commented in opinion documents.", "labels": [], "entities": []}, {"text": "The task consists of two sub-tasks.", "labels": [], "entities": []}, {"text": "The first subtask extracts aspect terms from an opinion corpus.", "labels": [], "entities": []}, {"text": "The second sub-task clusters synonymous aspect terms into categories where each category represents a single aspect, which we call an aspect category.", "labels": [], "entities": []}, {"text": "Existing research has proposed many methods for aspect extraction.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.9153919517993927}]}, {"text": "They largely fall into two main types.", "labels": [], "entities": []}, {"text": "The first type only extracts aspect terms without grouping them into categories (although a subsequent step maybe used for the grouping, see Section 2).", "labels": [], "entities": []}, {"text": "The second type uses statistical topic models to extract aspects and group them at the same time in an unsupervised manner.", "labels": [], "entities": []}, {"text": "However, in practice, one also encounters another setting, where grouping is not straightforward because for different applications the user may need different groupings to reflect the application needs.", "labels": [], "entities": []}, {"text": "This problem was reported in (, which gave the following example.", "labels": [], "entities": []}, {"text": "In car reviews, internal design and external design can be regarded as two separate aspects, but can also be regarded as one aspect, called \"design\", based on the level of details that the user wants to study.", "labels": [], "entities": []}, {"text": "It is also possible that the same word maybe put in different categories based on different needs.", "labels": [], "entities": []}, {"text": "However, () did not extract aspect terms.", "labels": [], "entities": []}, {"text": "It only categorizes a set of given aspect terms.", "labels": [], "entities": []}, {"text": "In this work, we propose two novel statistical models to extract and categorize aspect terms automatically given some seeds in the user interested categories.", "labels": [], "entities": []}, {"text": "It is thus able to best meet the user's specific needs.", "labels": [], "entities": []}, {"text": "Our models also jointly model both aspects and aspect specific sentiments.", "labels": [], "entities": []}, {"text": "The first model is called SAS and the second model is called ME-SAS.", "labels": [], "entities": [{"text": "SAS", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.7456446886062622}]}, {"text": "ME-SAS improves SAS by using Maximum-Entropy (or Max-Ent for short) priors to help separate aspects and sentiment terms.", "labels": [], "entities": [{"text": "SAS", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9739465117454529}]}, {"text": "However, to train Max-Ent, we do not need manually labeled training data (see Section 4).", "labels": [], "entities": []}, {"text": "In practical applications, asking users to provide some seeds is easy as they are normally experts in their trades and have a good knowledge what are important in their domains.", "labels": [], "entities": []}, {"text": "Our models are related to topic models in general () and joint models of aspects and sentiments in sentiment analysis in specific (e.g.,.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8938165009021759}]}, {"text": "However, these current models are typically unsupervised.", "labels": [], "entities": []}, {"text": "None of them can use seeds.", "labels": [], "entities": []}, {"text": "With seeds, our models are thus semi-supervised and need a different formulation.", "labels": [], "entities": []}, {"text": "Our models are also related to the DF-LDA model in (, which allows the user to set must-link and cannot-link constraints.", "labels": [], "entities": []}, {"text": "A must-link means that two terms must be in the same topic (aspect category), and a cannot-link means that two terms cannot be in the same topic.", "labels": [], "entities": []}, {"text": "Seeds maybe expressed with mustlinks and cannot-links constraints.", "labels": [], "entities": []}, {"text": "However, our models are very different from DF-LDA.", "labels": [], "entities": []}, {"text": "First of all, we jointly model aspect and sentiment, while DF-LDA is only for topics/aspects.", "labels": [], "entities": []}, {"text": "Joint modeling ensures clear separation of aspects from sentiments producing better results.", "labels": [], "entities": [{"text": "Joint modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7942266762256622}]}, {"text": "Second, our way of treating seeds is also different from DF-LDA.", "labels": [], "entities": []}, {"text": "We discuss these and other related work in Section 2.", "labels": [], "entities": []}, {"text": "The proposed models are evaluated using a large number of hotel reviews.", "labels": [], "entities": []}, {"text": "They are also compared with two state-of-the-art baselines.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed models outperform the two baselines by large margins.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section evaluates the proposed models.", "labels": [], "entities": []}, {"text": "Since the focus in this paper is to generate high quality aspects using seeds, we will not evaluate sentiments although both SAS and ME-SAS can also discover sentiments.", "labels": [], "entities": [{"text": "SAS", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9746427536010742}]}, {"text": "To compare the performance with our models, we use two existing state-of-the-art models, ME-LDA () and DF-LDA ().", "labels": [], "entities": [{"text": "ME-LDA", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.8956395983695984}]}, {"text": "As discussed in Section 2, there are two main flavors of aspect and sentiment models.", "labels": [], "entities": []}, {"text": "The first flavor does not separate aspect and sentiment, and the second flavor uses a switch to perform the separation.", "labels": [], "entities": []}, {"text": "Since our models also perform a switch, it is natural to compare with the latter flavor, which is also more advanced.", "labels": [], "entities": []}, {"text": "ME-LDA is the representative model in this flavor.", "labels": [], "entities": [{"text": "ME-LDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7326894402503967}]}, {"text": "DF-LDA adds constraints to LDA.", "labels": [], "entities": [{"text": "DF-LDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8371239900588989}]}, {"text": "We use our seeds to generate constraints for DF-LDA.", "labels": [], "entities": []}, {"text": "While ME-LDA cannot consider constraints, DF-LDA does not separate sentiments and aspects.", "labels": [], "entities": []}, {"text": "Apart from other modeling differences, our models can do both, which enable them to produce much better results.", "labels": [], "entities": []}, {"text": "Dataset and Settings: We used hotel reviews from tripadvisor.com.", "labels": [], "entities": []}, {"text": "Our corpus consisted of 101,234 reviews and 692,783 sentences.", "labels": [], "entities": []}, {"text": "Punctuations, stop words 3 , and words appearing less than 5 times in the corpus were removed.", "labels": [], "entities": []}, {"text": "For all models, the posterior inference was drawn after 5000 Gibbs iterations with an initial burn-in of 1000 iterations.", "labels": [], "entities": []}, {"text": "For SAS and ME-SAS, we set \u03b1 = 50/T, \u03b2 A = \u03b2 O = 0.1 as suggested in ().", "labels": [], "entities": [{"text": "SAS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.913504958152771}]}, {"text": "To make the seeds more effective, we set the seed set worddistribution hyper-parameter \u03b3 to be much larger than \u03b2 A , the hyper-parameter for the distribution over seed sets and aspect terms.", "labels": [], "entities": []}, {"text": "This results in higher weights to seeded words which in turn guide the sampler to cluster relevant terms better.", "labels": [], "entities": []}, {"text": "A more theoretical approach would involve performing hyper-parameter estimation () which may reveal specific properties of the dataset like the estimate of \u03b1 (indicating how different documents are in terms of their latent semantics), \u03b2 (suggesting how large the groups of frequently appearing aspect and sentiment terms are) and \u03b3 (giving a sense of which and how large groupings of seeds are good).", "labels": [], "entities": []}, {"text": "These are interesting questions and we defer it to our future work.", "labels": [], "entities": []}, {"text": "In this work, we found that the setting \u03b3 = 250, a larger value compared to \u03b2 A , produced good results.", "labels": [], "entities": []}, {"text": "For SAS, the asymmetric Beta priors were estimated using the method of moments (Section 3.1).", "labels": [], "entities": [{"text": "SAS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.987004816532135}, {"text": "asymmetric Beta priors", "start_pos": 13, "end_pos": 35, "type": "METRIC", "confidence": 0.6894694070021311}]}, {"text": "We sampled 500 random sentences from the corpus and for each sentence identified the aspects.", "labels": [], "entities": []}, {"text": "We thus computed the per-sentence probability of aspect emission (\ud97b\udf59 , ) and used Eq.", "labels": [], "entities": []}, {"text": "(4) to compute the final estimates, which give \u03b4 a = 2.35, \u03b4 b = 3.44.", "labels": [], "entities": []}, {"text": "To learn the Max-Ent parameters \u03bb of ME-SAS, we used the sentiment lexicon 4 of () to automatically generate training data (no manual labeling).", "labels": [], "entities": []}, {"text": "We randomly sampled 1000 terms from the corpus which have appeared at least 20 times (to ensure that the training set is reasonably representative of the corpus).", "labels": [], "entities": []}, {"text": "Of those 1000 terms if they appeared in the sentiment lexicon, they were treated as sentiment terms, else aspect terms.", "labels": [], "entities": []}, {"text": "Clearly, labeling words not in the sentiment lexicon as aspect terms may not always be correct.", "labels": [], "entities": []}, {"text": "Even with this noisy automaticallylabeled data, the proposed models can produce good results.", "labels": [], "entities": []}, {"text": "Since ME-LDA used manually labeled training data for Max-Ent, we again randomly sampled 1000 terms from our corpus appearing at least 20 times and labeled them as aspect terms or sentiment terms, so this labeled data clearly has less noise than our automatically labeled data.", "labels": [], "entities": []}, {"text": "For both ME-SAS and ME-LDA we used the corresponding feature vector of each labeled term (in the context of sentences where it occurs) to train the Max-Ent model.", "labels": [], "entities": []}, {"text": "As DF-LDA requires must-link and cannot-link constraints, we used our seed sets to generate intra-seed set mustlink and inter-seed set cannot-link constraints.", "labels": [], "entities": []}, {"text": "For its hyper-parameters, we used the default values in the package.", "labels": [], "entities": []}, {"text": "Setting the number of topics/aspects in topic models is often tricky as it is difficult to know the 5 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html exact number of topics that a corpus has.", "labels": [], "entities": []}, {"text": "While non-parametric Bayesian approaches () do exist for estimating the number of topics, T, they strongly depend on the hyper-parameters.", "labels": [], "entities": []}, {"text": "As we use fixed hyperparameters, we do not learn T from Bayesian nonparametrics.", "labels": [], "entities": []}, {"text": "We used 9 major aspects (T = 9) based on commonsense knowledge of what people usually talk about hotels and some experiments.", "labels": [], "entities": [{"text": "T", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.993297278881073}]}, {"text": "These are Dining, Staff, Maintenance, Check In, Cleanliness, Comfort, Amenities, Location and Value for Money (VFM).", "labels": [], "entities": [{"text": "Comfort", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9506264328956604}, {"text": "Amenities", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9947817921638489}, {"text": "Location", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9910889863967896}, {"text": "Value for Money (VFM)", "start_pos": 94, "end_pos": 115, "type": "METRIC", "confidence": 0.8810615936915079}]}, {"text": "However, it is important to note that the proposed models are flexible and do not need to have seeds for every aspect/topic.", "labels": [], "entities": []}, {"text": "Our experiments simulate the real-life situation where the user may not know all aspects or have no seeds for some aspects.", "labels": [], "entities": []}, {"text": "Thus, we provided seeds only to the first 6 of the 9 aspects/topics.", "labels": [], "entities": []}, {"text": "We will see that without seeds for all aspects, our models not only can improve the seeded aspects but also improve the non-seeded aspects.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Effect of performance on seeded and non-seeded aspects (5 seeds were used for the 6 seeded aspects).", "labels": [], "entities": []}]}