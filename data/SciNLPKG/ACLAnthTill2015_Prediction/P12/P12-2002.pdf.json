{"title": [{"text": "Joint Evaluation of Morphological Segmentation and Syntactic Parsing", "labels": [], "entities": [{"text": "Morphological Segmentation", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8471105694770813}, {"text": "Syntactic Parsing", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8249018788337708}]}], "abstractContent": [{"text": "We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance.", "labels": [], "entities": [{"text": "parse evaluation", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.9809865653514862}, {"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.8600202202796936}]}, {"text": "The protocol uses distance-based metrics defined for the space of trees over lattices.", "labels": [], "entities": []}, {"text": "Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assum-ing gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags).", "labels": [], "entities": []}, {"text": "Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9657861590385437}, {"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9313833117485046}]}], "introductionContent": [{"text": "A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentence's human-perceived interpretation.", "labels": [], "entities": []}, {"text": "Current state-of-the-art parsers assume that the spacedelimited words in the input are the basic units of syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7799748182296753}]}, {"text": "Standard evaluation procedures and metrics) accordingly assume that the yield of the parse tree is known in advance.", "labels": [], "entities": []}, {"text": "This assumption breaks down when parsing morphologically rich languages ( , where every space-delimited word maybe effectively composed of multiple morphemes, each of which having a distinct role in the syntactic parse tree.", "labels": [], "entities": [{"text": "parsing morphologically rich languages", "start_pos": 33, "end_pos": 71, "type": "TASK", "confidence": 0.8862038999795914}]}, {"text": "In order to parse such input the text needs to undergo morphological segmentation, that is, identifying the morphological segments of each word and assigning the corresponding part-ofspeech (PoS) tags to them.", "labels": [], "entities": []}, {"text": "Morphologically complex words maybe highly ambiguous and in order to segment them correctly their analysis has to be disambiguated.", "labels": [], "entities": []}, {"text": "The multiple morphological analyses of input words maybe represented via a lattice that encodes the different segmentation possibilities of the entire word sequence.", "labels": [], "entities": []}, {"text": "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding.", "labels": [], "entities": []}, {"text": "If the selected segmentation is different from the gold segmentation, the gold and parse trees are rendered incomparable and standard evaluation metrics breakdown.", "labels": [], "entities": []}, {"text": "Evaluation scenarios restricted to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upperbound on parser performance.", "labels": [], "entities": []}, {"text": "This paper presents a full treatment of evaluation in different parsing scenarios, using distance-based measures defined for trees over a shared common denominator defined in terms of a lattice structure.", "labels": [], "entities": []}, {"text": "We demonstrate the informativeness of our metrics by evaluating joint segmentation and parsing performance for the Semitic language Modern Hebrew, using the best performing systems, both constituencybased and dependency-based).", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that, for all parsers, significant performance gaps between realistic and non-realistic scenarios crucially depend on the kind of information initially provided to the parser.", "labels": [], "entities": []}, {"text": "The tool and metrics that we provide are completely general and can straightforwardly apply to other languages, treebanks and different tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In morphologically rich languages (MRLs) substantial information about the grammatical relations between entities is expressed at word level using inflectional affixes.", "labels": [], "entities": []}, {"text": "In particular, in MRLs such as Hebrew, Arabic, Turkish or Maltese, elements such as determiners, definite articles and conjunction markers appear as affixes that are appended to an openclass word.", "labels": [], "entities": []}, {"text": "Take, for example the Hebrew wordtoken BCLM, 1 which means \"in their shadow\".", "labels": [], "entities": []}, {"text": "This word corresponds to five distinctly tagged elements: B (\"in\"/IN), H (\"the\"/DEF), CL (\"shadow\"/NN), FL (\"of\"/POSS), HM (\"they\"/PRN).", "labels": [], "entities": []}, {"text": "Note that morphological segmentation is not the inverse of concatenation.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7734338045120239}]}, {"text": "For instance, the overt definite article H and the possessor FL show up only in the analysis.", "labels": [], "entities": [{"text": "FL", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.8625297546386719}]}, {"text": "The correct parse for the Hebrew phrase \"BCLM HNEIM\" is shown in (tree1), and it presupposes that these segments can be identified and assigned the correct PoS tags.", "labels": [], "entities": []}, {"text": "However, morphological segmentation is non-trivial due to massive wordlevel ambiguity.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7647090554237366}]}, {"text": "The word BCLM, for instance, can be segmented into the noun BCL (\"onion\") and M (a genitive suffix, \"of them\"), or into the prefix B (\"in\") followed by the noun CLM (\"image\").", "labels": [], "entities": []}, {"text": "The multitude of morphological analyses maybe encoded in a lattice structure, as illustrated in.", "labels": [], "entities": []}, {"text": "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.", "labels": [], "entities": []}, {"text": "This maybe done based on linear local context), or jointly with parsing.", "labels": [], "entities": []}, {"text": "Either way, an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis, ultimately providing a parse tree which spans a different yield than the gold terminals.", "labels": [], "entities": []}, {"text": "In such cases, existing evaluation metrics breakdown.", "labels": [], "entities": []}, {"text": "To understand why, consider the trees in.", "labels": [], "entities": []}, {"text": "Metrics like PARSEVAL ( calculate the harmonic means of precision and recall on labeled spans i, label, j where i, j are terminal boundaries.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9692636728286743}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9990869760513306}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9980886578559875}]}, {"text": "Now, the NP dominating \"shadow of them\" has been identified and labeled correctly in tree2, but in tree1 it spans 2, NP, 5 and in tree2 it spans 1, NP, 4.", "labels": [], "entities": []}, {"text": "This node will then be counted as an error for tree2, along with its dominated and dominating structure, and PARSEVAL will score 0.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9815226793289185}]}, {"text": "A generalized version of PARSEVAL which considers i, j character-based indices instead of terminal boundaries) will fail here too, since the missing overt definite article H will cause similar misalignments.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.47635895013809204}]}, {"text": "Metrics for dependencybased evaluation such as ATTACHMENT SCORES) suffer from similar problems, since they assume that both trees have the same nodes -an assumption that breaks down in the case of incorrect morphological segmentation.", "labels": [], "entities": [{"text": "ATTACHMENT", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.7629693150520325}]}, {"text": "Although great advances have been made in parsing MRLs in recent years, this evaluation challenge remained unsolved.", "labels": [], "entities": [{"text": "parsing MRLs", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9082228541374207}]}, {"text": "In this paper we present a solution to this challenge by extending TEDEVAL (Tsarfaty et al., 2011) for handling trees over lattices.", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9918760061264038}]}, {"text": "We aim to evaluate state-of-the-art parsing architectures on the morphosyntactic disambiguation of Hebrew texts in three different parsing scenarios: (i) Gold: assuming gold segmentation and PoS-tags, (ii) Predicted: assuming only gold segmentation, and (iii) Raw: assuming unanalyzed input text.", "labels": [], "entities": []}, {"text": "For constituency-based parsing we use two models trained by the Berkeley parser () one on phrase-structure (PS) trees and one on relational-realizational (RR) trees.", "labels": [], "entities": [{"text": "constituency-based parsing", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6936064958572388}]}, {"text": "In the raw scenario we let a latticebased parser choose its own segmentation and tags.", "labels": [], "entities": []}, {"text": "For dependency parsing we use MaltParser () optimized for Hebrew by, and the EasyFirst parser of with the features therein.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8358660042285919}]}, {"text": "Since these parsers cannot choose their own tags, automatically predicted segments and tags are provided by.", "labels": [], "entities": []}, {"text": "We use the standard split of the Hebrew treebank) and its conversion into unlabeled dependencies).", "labels": [], "entities": []}, {"text": "We use PARSEVAL for evaluating phrase-structure trees, ATTACHSCORES for evaluating dependency trees, and TEDEVAL for evaluating all trees in all scenarios.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9196693897247314}, {"text": "ATTACHSCORES", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9690367579460144}, {"text": "TEDEVAL", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9722515940666199}]}, {"text": "We implement SEGEVAL for evaluating segmentation based on our TEDEVAL implementation, replacing the tree distance and size with string terms.", "labels": [], "entities": [{"text": "SEGEVAL", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.599983811378479}]}, {"text": "shows the constituency-based parsing results for all scenarios.", "labels": [], "entities": [{"text": "constituency-based parsing", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6987942457199097}]}, {"text": "All of our results confirm that gold information leads to much higher scores.", "labels": [], "entities": []}, {"text": "TEDEVAL allows us to precisely quantify the drop inaccuracy from gold to predicted (as in PARSE-VAL) and than from predicted to raw on a single scale.", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5484846830368042}]}, {"text": "TEDEVAL further allows us to scrutinize the contribution of different sorts of information.", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5961213707923889}]}, {"text": "Unlabeled TEDEVAL shows a greater drop when moving from predicted to raw than from gold to predicted, and for labeled TEDEVAL it is the other way round.", "labels": [], "entities": []}, {"text": "This demonstrates the great importance of gold tags which provide morphologically disambiguated information for identifying phrase content.", "labels": [], "entities": [{"text": "identifying phrase content", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.8199880917867025}]}, {"text": "shows that dependency parsing results confirm the same trends, but we see a much smaller drop when moving from gold to predicted.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8129903078079224}]}, {"text": "This is due to the fact that we train the parsers for predicted on a treebank containing predicted tags.", "labels": [], "entities": []}, {"text": "There is however a great drop when moving from predicted to raw, which confirms that evaluation benchmarks on gold input as in do not provide a realistic indication of parser performance.", "labels": [], "entities": []}, {"text": "For all tables, TEDEVAL results are on a similar scale.", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9295364022254944}]}, {"text": "However, results are not yet comparable across parsers.", "labels": [], "entities": []}, {"text": "RR trees are flatter than bare-bone PS trees.", "labels": [], "entities": [{"text": "RR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7482900619506836}]}, {"text": "PS and DEP trees have different label sets.", "labels": [], "entities": []}, {"text": "Cross-framework evaluation maybe conducted by combining this metric with the cross-framework protocol of.", "labels": [], "entities": []}], "tableCaptions": []}