{"title": [{"text": "Collective Generation of Natural Image Descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.8550496300061544}]}, {"text": "More specifically, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image.", "labels": [], "entities": []}, {"text": "We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.7857807576656342}]}, {"text": "Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically describing images in natural language is an intriguing, but complex AI task, requiring accurate computational visual recognition, comprehensive world knowledge, and natural language generation.", "labels": [], "entities": [{"text": "Automatically describing images in natural language", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7862745126088461}, {"text": "computational visual recognition", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.8297160466512045}, {"text": "natural language generation", "start_pos": 179, "end_pos": 206, "type": "TASK", "confidence": 0.6404237747192383}]}, {"text": "Some past research has simplified the general image description goal by assuming that relevant text for an image is provided (e.g.,, ).", "labels": [], "entities": [{"text": "image description", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.6905364841222763}]}, {"text": "This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding.", "labels": [], "entities": [{"text": "summarization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9794977903366089}]}, {"text": "However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image's specific content and many natural images do not come with associated text for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 184, "end_pos": 197, "type": "TASK", "confidence": 0.977547287940979}]}, {"text": "In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g.,, ,, ), or by retrieving existing whole descriptions from visually similar images (e.g.,,).", "labels": [], "entities": []}, {"text": "For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image.", "labels": [], "entities": []}, {"text": "For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7450882792472839}, {"text": "language generation constructs", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.7845139106114706}]}, {"text": "Because the generation process sticks relatively closely to the recognized content, the resulting descriptions often lack the kind of coverage, creativity, and complexity typically found in humanwritten text.", "labels": [], "entities": []}, {"text": "In this paper, we propose a holistic datadriven approach that combines and extends the best aspects of these previous approaches -a) using visual recognition to directly predict individual image content elements, and b) using retrieval from existing human-composed descriptions to generate natural, creative, and inter-esting captions.", "labels": [], "entities": []}, {"text": "We also lift the restriction of retrieving existing whole descriptions by gathering visually relevant phrases which we combine to produce novel and query-image specific descriptions.", "labels": [], "entities": []}, {"text": "By judiciously exploiting the correspondence between image content elements and phrases, it is possible to generate natural language descriptions that are substantially richer in content and more linguistically interesting than previous work.", "labels": [], "entities": []}, {"text": "At a high level, our approach can be motivated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g.,,).", "labels": [], "entities": []}, {"text": "Analogously, our generation algorithm attains a higher level of linguistic sophistication by reading large amounts of descriptive text available online.", "labels": [], "entities": []}, {"text": "Our approach is also motivated by language grounding by visual worlds (e.g.,,,), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image.", "labels": [], "entities": []}, {"text": "Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics drawn from a large image-text parallel corpus.", "labels": [], "entities": []}, {"text": "This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., ).", "labels": [], "entities": []}, {"text": "For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful.", "labels": [], "entities": []}, {"text": "Content planning and phrase synthesis can be naturally viewed as constraint optimization problems.", "labels": [], "entities": [{"text": "Content planning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8178041577339172}, {"text": "phrase synthesis", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.8181995749473572}]}, {"text": "We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g.,,, ).", "labels": [], "entities": []}, {"text": "Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate that our final system generates linguistically more appealing and semantically more correct descriptions than two nontrivial baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "TestSet: Because computer vision is a challenging and unsolved problem, we restrict our query set to images where we have high confidence that visual recognition algorithms perform well.", "labels": [], "entities": []}, {"text": "We collect 1000 test images by running a large number (89) of object detectors on 20,000 images and selecting images that receive confident object detection scores, with some preference for images with multiple object detections to obtain good examples for testing discourse constraints.", "labels": [], "entities": []}, {"text": "Baselines: We compare our ILP approaches with two nontrivial baselines: the first is an HMM approach (comparable to), which takes as input the same set of candidate phrases described in \u00a72, but for decoding, we fix the ordering of phrases as [ NP -VP -Region PP -Scene PP] and find the best combination of phrases using the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "We use the same rich set of pairwise   phrase cohesion scores ( \u00a75.4) used for the ILP formulation, producing a strong baseline 6 . The second baseline is a recent Retrieval based description method (, that searches the large parallel corpus of images and captions, and transfers a caption from a visually similar database image to the query.", "labels": [], "entities": []}, {"text": "This again is a very strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators).", "labels": [], "entities": []}, {"text": "Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult.", "labels": [], "entities": []}, {"text": "BLEU score (), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions), as it correlates reasonably well with human evaluation (  Human Evaluation I -Ranking: We complement the automatic evaluation with Mechanical Turk evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9786933064460754}]}, {"text": "In ranking evaluation, we ask raters to choose a better caption between two choices 7 . We do this rating with and without showing the images, as summarized in Table 2 & 3.", "labels": [], "entities": []}, {"text": "When images are shown, raters evaluate content relevance as well as linguistic quality of the captions.", "labels": [], "entities": []}, {"text": "Without images, raters evaluate only linguistic quality.", "labels": [], "entities": []}, {"text": "We found that raters generally prefer ILP generated captions over HMM generated ones, twice as much (67.2% ILP V.S. 32.8% HMM), if images are not presented.", "labels": [], "entities": []}, {"text": "However the difference is less pronounced when images are shown.", "labels": [], "entities": []}, {"text": "There could be two possible reasons.", "labels": [], "entities": []}, {"text": "The first is that when images are shown, the Turkers do not try as hard to tell apart the subtle difference between the two imperfect captions.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8742743134498596}]}, {"text": "The second is that the relative content relevance of ILP generated captions is negating the superiority in linguistic quality.", "labels": [], "entities": [{"text": "ILP generated captions", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.5471073786417643}]}, {"text": "We explore this question using multi-aspect rating, described below.", "labels": [], "entities": []}, {"text": "Note that ILP generated captions are exceedingly (71.8 %) preferred over the Retrieval baseline (), despite the generated captions tendency to be more prone to grammatical and cognitive errors than retrieved ones.", "labels": [], "entities": []}, {"text": "This indicates that the generated captions must have substantially better content relevance to the query image, supporting the direction of this research.", "labels": [], "entities": []}, {"text": "Finally, notice that as much as 16% of the time, ILP generated captions are preferred over the original human generated ones (examples in).", "labels": [], "entities": [{"text": "ILP generated captions", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.5227334996064504}]}, {"text": "Human Evaluation II -Multi-Aspect Rating: presents rating in the 1-5 scale (5: perfect, 4: almost perfect, 3: 70\u223c80% good, 2: Found MIT boy gave me this quizical expression.", "labels": [], "entities": [{"text": "MIT boy gave", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.9701082309087118}]}, {"text": "One of the most shirt in the wall of the house.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Human Evaluation (without images)", "labels": [], "entities": [{"text": "Human Evaluation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6076062917709351}]}, {"text": " Table 3: Human Evaluation (with images)", "labels": [], "entities": [{"text": "Human Evaluation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6315282434225082}]}]}