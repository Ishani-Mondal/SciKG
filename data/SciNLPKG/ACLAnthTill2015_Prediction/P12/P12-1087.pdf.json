{"title": [{"text": "Big Data versus the Crowd: Looking for Relationships in All the Right Places", "labels": [], "entities": []}], "abstractContent": [{"text": "Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain.", "labels": [], "entities": [{"text": "training relation extractors", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.8374431729316711}]}, {"text": "To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing.", "labels": [], "entities": [{"text": "crowd sourcing", "start_pos": 175, "end_pos": 189, "type": "TASK", "confidence": 0.7780484557151794}]}, {"text": "There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers.", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9963980913162231}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9915063977241516}]}, {"text": "To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources.", "labels": [], "entities": []}, {"text": "We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples.", "labels": [], "entities": []}, {"text": "Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score).", "labels": [], "entities": [{"text": "quality", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9713199138641357}, {"text": "F1 score)", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9875643452008566}]}, {"text": "In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9992876648902893}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9968134760856628}]}], "introductionContent": [{"text": "Relation extraction is the problem of populating a target relation (representing an entity-level relationship or attribute) with facts extracted from naturallanguage text.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9501589238643646}]}, {"text": "Sample relations include people's titles, birth places, and marriage relationships.", "labels": [], "entities": []}, {"text": "Traditional relation-extraction systems rely on manual annotations or domain-specific rules provided by experts, both of which are scarce resources that are not portable across domains.", "labels": [], "entities": []}, {"text": "To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.93145751953125}]}, {"text": "The input to distant supervision is a set of seed facts for the target relation together with an (unlabeled) text corpus, and the output is a set of (noisy) annotations that can be used by any machine learning technique to train a statistical model for the target relation.", "labels": [], "entities": []}, {"text": "For example, given the target relation birthPlace(person, place) and a seed fact birthPlace(John, Springfield), the sentence \"John and his wife were born in Springfield in 1946\" (S1) would qualify as a positive training example.", "labels": [], "entities": []}, {"text": "Distant supervision replaces the expensive process of manually acquiring annotations that is required by direct supervision with resources that already exist in many scenarios (seed facts and a text corpus).", "labels": [], "entities": []}, {"text": "On the other hand, distantly labeled data may not be as accurate as manual annotations.", "labels": [], "entities": []}, {"text": "For example, \"John left Springfield when he was 16\" (S2) would also be considered a positive example about place of birth by distant supervision as it contains both John and Springfield.", "labels": [], "entities": [{"text": "John left Springfield when he was 16\" (S2)", "start_pos": 14, "end_pos": 56, "type": "DATASET", "confidence": 0.7605784481221979}, {"text": "Springfield", "start_pos": 174, "end_pos": 185, "type": "DATASET", "confidence": 0.8006957769393921}]}, {"text": "The hypothesis is that the broad coverage and high redundancy in a large corpus would compensate for this noise.", "labels": [], "entities": []}, {"text": "For example, with a large enough corpus, a distant supervision system may find that patterns in the sentence S1 strongly correlate with seed facts of birthPlace whereas patterns in S2 do not qualify as a strong indicator.", "labels": [], "entities": []}, {"text": "Thus, intuitively the quality of distant supervision should improve as we use larger corpora.", "labels": [], "entities": []}, {"text": "However, there has been no study on the impact of corpus size on distant supervision for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.9352721273899078}]}, {"text": "Our goal is to fill this gap.", "labels": [], "entities": []}, {"text": "Besides \"big data,\" another resource that maybe valuable to distant supervision is crowdsourc-ing.", "labels": [], "entities": []}, {"text": "For example, one could employ crowd workers to provide feedback on whether distant supervision examples are corrector not (.", "labels": [], "entities": []}, {"text": "Intuitively the crowd workforce is a perfect fit for such tasks since many erroneous distant labels could be easily identified and corrected by humans.", "labels": [], "entities": []}, {"text": "For example, distant supervision may mistakenly consider \"Obama took a vacation in Hawaii\" a positive example for birthPlace simply because a database says that Obama was born in Hawaii; a crowd worker would correctly point out that this sentence is not actually indicative of this relation.", "labels": [], "entities": [{"text": "birthPlace", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.9558092355728149}]}, {"text": "It is unclear however which strategy one should use: scaling the text corpus or the amount of human feedback.", "labels": [], "entities": []}, {"text": "Our primary contribution is to empirically assess how scaling these inputs to distant supervision impacts its result quality.", "labels": [], "entities": []}, {"text": "We study this question with input data sets that are orders of magnitude larger than those in prior work.", "labels": [], "entities": []}, {"text": "While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision () contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb.", "labels": [], "entities": []}, {"text": "1 While prior work ( on crowdsourcing for distant supervision used thousands of human feedback units, we acquire tens of thousands of human-provided labels.", "labels": [], "entities": []}, {"text": "Despite the large scale, we follow state-of-the-art distant supervision approaches and use deep linguistic features, e.g., part-of-speech tags and dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7332660853862762}]}, {"text": "Our experiments shed insight on the following two questions: 1.", "labels": [], "entities": []}, {"text": "How does increasing the corpus size impact the quality of distant supervision?", "labels": [], "entities": []}, {"text": "2. For a given corpus size, how does increasing the amount of human feedback impact the quality of distant supervision?", "labels": [], "entities": []}, {"text": "We found that increasing corpus size consistently and significantly improves recall and F1, despite reducing precision on small corpora; in contrast, human feedback has relatively small impact on precision and recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.999530553817749}, {"text": "F1", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9983062744140625}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.997606635093689}, {"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9983348250389099}, {"text": "recall", "start_pos": 210, "end_pos": 216, "type": "METRIC", "confidence": 0.9905804991722107}]}, {"text": "For example, on a TAC corpus with 1.8M documents, we found that increasing the corpus size ten-fold consistently results in statistically 1 http://lemurproject.org/clueweb09.php/ 2 We used 100K CPU hours to run such tools on ClueWeb.", "labels": [], "entities": []}, {"text": "significant improvement in F1 on two standardized relation extraction metrics (t-test with p=0.05).", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9992961883544922}, {"text": "relation extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7673125267028809}]}, {"text": "On the other hand, increasing human feedback amount ten-fold results in statistically significant improvement on F1 only when the corpus contains at least 1M documents; and the magnitude of such improvement was only one fifth compared to the impact of corpus-size increment.", "labels": [], "entities": [{"text": "F1", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9987000226974487}]}, {"text": "We find that the quality of distant supervision tends to be recall gated, that is, for any given relation, distant supervision fails to find all possible linguistic signals that indicate a relation.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9770330190658569}]}, {"text": "By expanding the corpus one can expand the number of patterns that occur with a known set of entities.", "labels": [], "entities": []}, {"text": "Thus, as a rule of thumb for developing distant supervision systems, one should first attempt to expand the training corpus and then worry about precision of labels only after having obtained a broad-coverage corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9961477518081665}]}, {"text": "Throughout this paper, it is important to understand the difference between mentions and entities.", "labels": [], "entities": []}, {"text": "Entities are conceptual objects that exist in the world (e.g., Barack Obama), whereas authors use a variety of wordings to refer to (which we call \"mention\") entities in text ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe our experiments to test the hypotheses that the following two factors improve distantsupervision quality: increasing the (1) corpus size, and (2) the amount of crowd-sourced feedback.", "labels": [], "entities": []}, {"text": "We confirm hypothesis (1), but, surprisingly, are unable to confirm (2).", "labels": [], "entities": []}, {"text": "Specifically, when using logistic regression to train relation extractors, increasing corpus size improves, consistently and significantly, the precision and recall produced by distant supervision, regardless of human feedback levels.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7248154580593109}, {"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9993627667427063}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9982927441596985}]}, {"text": "Using the We obtain the gold standard from a separate MTurk submission by taking examples that at least 10 out of 11 turkers answered yes, and then negate half of these examples by altering the relation names (e.g., spouse to sibling).", "labels": [], "entities": []}, {"text": "More details in our technical report).", "labels": [], "entities": []}, {"text": "methodology described in Section 3, human feedback has limited impact on the precision and recall produced from distant supervision by itself.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9994033575057983}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9978048205375671}]}, {"text": "Just as direct training data are scarce, ground truth for relation extraction is scarce as well.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.9564482569694519}]}, {"text": "As a result, prior work mainly considers two types of evaluation methods: (1) randomly sample a small portion of predictions (e.g., top-k) and manually evaluate precision/recall; and (2) use a held-out portion of seed facts (usually Freebase) as a kind of \"distant\" ground truth.", "labels": [], "entities": [{"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9988176226615906}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.8937773108482361}]}, {"text": "We replace manual evaluation with a standardized relation-extraction benchmark: TAC-KBP 2010.", "labels": [], "entities": [{"text": "TAC-KBP 2010", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.8093429505825043}]}, {"text": "TAC-KBP asks for extractions of 46 relations on a given set of 100 entities.", "labels": [], "entities": []}, {"text": "Interestingly, the Freebase held-out metric () turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6).", "labels": [], "entities": [{"text": "Freebase", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.950617790222168}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9985570311546326}]}, {"text": "Our first group of experiments use the 1.8M-doc TAC-KBP corpus for training.", "labels": [], "entities": [{"text": "TAC-KBP corpus", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.7197424471378326}]}, {"text": "We exclude from it the 33K documents that contain query entities in the TAC-KBP metrics.", "labels": [], "entities": [{"text": "TAC-KBP metrics", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.8059401512145996}]}, {"text": "There are two key parameters: the corpus size (#docs) M and human feedback budget (#examples) N . We perform different levels of down-sampling on the training corpus.", "labels": [], "entities": []}, {"text": "On TAC, we use subsets with M = 10 3 , 10 4 , 10 5 , and 10 6 documents respectively.", "labels": [], "entities": []}, {"text": "For each value of M , we perform 30 independent trials of uniform sampling, with each trial resulting in a training corpus For each training corpus D M i , we perform distant supervision to train a set of logistic regression classifiers.", "labels": [], "entities": []}, {"text": "From the full corpus, distant supervision creates around 72K training examples.", "labels": [], "entities": []}, {"text": "To evaluate the impact of human feedback, we randomly sample 20K examples from the input corpus (we remove any portion of the corpus that is used in an evaluation).", "labels": [], "entities": []}, {"text": "Then, we ask three different crowd workers to label each example as either positive or negative using the procedure described in Section 3.3.", "labels": [], "entities": []}, {"text": "We retain only credible answers using the gold-standard method (see Section 3.3), and use them as the pool of human feedback that we run experiments with.", "labels": [], "entities": []}, {"text": "About 46% of our human labels are negative.", "labels": [], "entities": []}, {"text": "Denote by N the number of examples that: Impact of input sizes under the TAC-KBP metric, which uses documents mentioning 100 predefined entities as testing corpus with entity-level ground truth.", "labels": [], "entities": []}, {"text": "We vary the sizes of the training corpus and human feedback while measuring the scores (F1, recall, and precision) on the TAC-KBP benchmark.", "labels": [], "entities": [{"text": "F1", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9996209144592285}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9973395466804504}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9995755553245544}, {"text": "TAC-KBP benchmark", "start_pos": 122, "end_pos": 139, "type": "DATASET", "confidence": 0.7688639163970947}]}, {"text": "we want to incorporate human feedback for; we vary N in the range of 0, 10, 10 2 , 10 3 , 10 4 , and 2 \u00d7 10 4 . For each selected corpus and value of N , we perform without-replacement sampling from examples of this corpus to select feedback for up to N examples.", "labels": [], "entities": []}, {"text": "In our experiments, we found that on average an M -doc corpus contains about 0.04M distant labels, out of which 0.01M have human feedback.", "labels": [], "entities": []}, {"text": "After incorporating human feedback, we evaluate the relation extractors on the TAC-KBP benchmark.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7920485138893127}, {"text": "TAC-KBP benchmark", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.7900235652923584}]}, {"text": "We then compute the average F1, recall, and precision scores among all trials for each metric and each (M,N) pair.", "labels": [], "entities": [{"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9996497631072998}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9981022477149963}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.999093770980835}]}, {"text": "Besides the KBP metrics, we also evaluate each (M,N) pair using Freebase held-out data.", "labels": [], "entities": [{"text": "Freebase held-out data", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.926354706287384}]}, {"text": "Furthermore, we experiment with a much larger corpus: ClueWeb09.", "labels": [], "entities": [{"text": "ClueWeb09", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.9290730357170105}]}, {"text": "On ClueWeb09, we vary M over 10 3 , . .", "labels": [], "entities": [{"text": "M", "start_pos": 22, "end_pos": 23, "type": "METRIC", "confidence": 0.9822710752487183}]}, {"text": ", 10 8 . Using the same metrics, we show at a larger scale that increasing corpus size can significantly improve both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9987094402313232}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9960466027259827}]}], "tableCaptions": [{"text": " Table 1: TAC F1 scores with max/min values of M /N .", "labels": [], "entities": [{"text": "TAC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9563842415809631}, {"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.5970673561096191}]}]}