{"title": [{"text": "Named Entity Disambiguation in Streaming Data", "labels": [], "entities": [{"text": "Entity Disambiguation", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7012011557817459}]}], "abstractContent": [{"text": "The named entity disambiguation task is to resolve the many-to-many correspondence between ambiguous names and the unique real-world entity.", "labels": [], "entities": [{"text": "named entity disambiguation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.736485222975413}]}, {"text": "This task can be modeled as a classification problem, provided that positive and negative examples are available for learning binary classifiers.", "labels": [], "entities": []}, {"text": "High-quality sense-annotated data, however, are hard to be obtained in streaming environments, since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream.", "labels": [], "entities": []}, {"text": "On the other hand, few positive examples plus large amounts of un-labeled data maybe easily acquired.", "labels": [], "entities": []}, {"text": "Producing binary classifiers directly from this data, however, leads to poor disambiguation performance.", "labels": [], "entities": []}, {"text": "Thus, we propose to enhance the quality of the classifiers using finer-grained variations of the well-known Expectation-Maximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "We conducted a systematic evaluation using Twitter streaming data and the results show that our clas-sifiers are extremely effective, providing improvements ranging from 1% to 20%, when compared to the current state-of-the-art biased SVMs, being more than 120 times faster.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human language is not exact.", "labels": [], "entities": []}, {"text": "For instance, an entity 1 maybe referred by multiple names (i.e., polysemy), and also the same name may refer to different entities depending on the surrounding context (i.e., homonymy).", "labels": [], "entities": []}, {"text": "The task of named entity disambiguation is to identify which names refer to the same entity in a textual collection ().", "labels": [], "entities": [{"text": "named entity disambiguation", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7288804650306702}]}, {"text": "The emergence of new communication technologies, such as micro-blog platforms, brought a humongous amount of textual mentions with ambiguous entity names, raising an urgent need for novel disambiguation approaches and algorithms.", "labels": [], "entities": []}, {"text": "In this paper we address the named entity disambiguation task under a particularly challenging scenario.", "labels": [], "entities": [{"text": "named entity disambiguation task", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7182569727301598}]}, {"text": "We are given a stream of messages from a micro-blog channel such as Twitter 2 and a list of names n 1 , n 2 , . .", "labels": [], "entities": []}, {"text": ", n N used for mentioning a specific entity e.", "labels": [], "entities": []}, {"text": "Our problem is to monitor the stream and predict whether an incoming message containing n i indeed refers toe (positive example) or not (negative example).", "labels": [], "entities": []}, {"text": "This scenario brings challenges that must be overcome.", "labels": [], "entities": []}, {"text": "First, micro-blog messages are composed of a small amount of words and they are written in informal, sometimes cryptic style.", "labels": [], "entities": []}, {"text": "These characteristics make hard the identification of entities and the semantics of their relationships ().", "labels": [], "entities": []}, {"text": "Further, the scarcity of text in the messages makes it even harder to properly characterize a common context for the entities.", "labels": [], "entities": []}, {"text": "Second, as we need to monitor messages that keep coming at a fast pace, we cannot afford to gather information from external sources on-the-fly.", "labels": [], "entities": []}, {"text": "Finally, fresh data coming in the stream introduces new patterns, quickly invalidating static disambiguation models.", "labels": [], "entities": []}, {"text": "We hypothesize that the lack of information in each individual message and from external sources can be compensated by using information obtained from the large and diverse amount of text in a stream of messages taken as a whole, that is, thousands of messages per second coming from distinct sources.", "labels": [], "entities": []}, {"text": "The information embedded in such a stream of messages maybe exploited for entity disambiguation through the application of supervised learning methods, for instance, with the application of binary classifiers.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7146464288234711}]}, {"text": "Such methods, however, suffer from a data acquisition bottleneck, since they are based on training datasets that are built by skilled human annotators who manually inspect the messages.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7548210024833679}]}, {"text": "This annotation process is usually lengthy and laborious, being clearly unfeasible to be adopted in data streaming scenarios.", "labels": [], "entities": []}, {"text": "As an alternative to such manual process, a large amount of unlabeled data, augmented with a small amount of (likely) positive examples, can be collected automatically from the message stream ().", "labels": [], "entities": []}, {"text": "Binary classifiers maybe learned from such data by considering unlabeled data as negative examples.", "labels": [], "entities": []}, {"text": "This strategy, however, leads to classifiers with poor disambiguation performance, due to a potentially large number of false-negative examples.", "labels": [], "entities": []}, {"text": "In this paper we propose to refine binary classifiers iteratively, by performing Expectation-Maximization (EM) approaches.", "labels": [], "entities": []}, {"text": "Basically, a partial classifier is used to evaluate the likelihood of an unlabeled example being a positive example or a negative example, thus automatically and (continuously) creating a labeled training corpus.", "labels": [], "entities": []}, {"text": "This process continues iteratively by changing the label of some examples (an operation we call label-transition), so that, after some iterations, the combination of labels is expected to converge to the one for which the observed data is most likely.", "labels": [], "entities": []}, {"text": "Based on such an approach, we introduce novel disambiguation algorithms that differ among themselves on the granularity in which the classifier is updated, and on the label-transition operations that are allowed.", "labels": [], "entities": []}, {"text": "An important feature of the proposed approach is that, at each iteration of the EM-process, anew classifier (an improved one) is produced in order to account for the current set of labeled examples.", "labels": [], "entities": []}, {"text": "We introduce a novel strategy to maintain the classifiers up-to-date incrementally after each iteration, or even after each label-transition operation.", "labels": [], "entities": []}, {"text": "Indeed, we theoretically show that our classifier needs to be updated just partially and we are able to determine exactly which parts must be updated, making our disambiguation methods extremely fast.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of the proposed algorithms, we performed a systematic set of experiments using large-scale Twitter data containing messages with ambiguous entity names.", "labels": [], "entities": []}, {"text": "In order to validate our claims, disambiguation performance is investigated by varying the proportion of falsenegative examples in the unlabeled dataset.", "labels": [], "entities": []}, {"text": "Our algorithms are compared against a state-of-the-art technique for named entity disambiguation based on classifiers, providing performance gains ranging from 1% to 20% and being roughly 120 times faster.", "labels": [], "entities": [{"text": "named entity disambiguation", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6378227969010671}]}], "datasetContent": [{"text": "In this section we analyze our algorithms using standard measures such as AUC values.", "labels": [], "entities": [{"text": "AUC", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9394190311431885}]}, {"text": "For each positive+unlabeled (PU) corpus used in our evaluation we randomly selected x% of the positive examples (P) to become unlabeled ones (U).", "labels": [], "entities": []}, {"text": "This procedure enables us to control the uncertainty level of the corpus.", "labels": [], "entities": []}, {"text": "For each level we have a different TPR-FPR combination, enabling us to draw ROC curves.We repeated this procedure five times, so that five executions were performed for each uncertainty level.", "labels": [], "entities": [{"text": "TPR-FPR", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9540947675704956}]}, {"text": "show the average for the five runs.", "labels": [], "entities": []}, {"text": "Wilcoxon significance tests were performed (p<0.05) and best results, including statistical ties, are shown in bold.", "labels": [], "entities": [{"text": "Wilcoxon", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7115414142608643}, {"text": "significance", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.7352877855300903}]}], "tableCaptions": [{"text": " Table 1: Characteristics of each collection.", "labels": [], "entities": []}, {"text": " Table 4: Average execution time (secs) for each algo- rithm. The time spent by algorithm A1 is similar to the  time spent by algorithm A2. The time spent by algorithm  A3 is similar to the time spent by algorithm A4.", "labels": [], "entities": [{"text": "Average execution time (secs)", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.8418354392051697}]}]}