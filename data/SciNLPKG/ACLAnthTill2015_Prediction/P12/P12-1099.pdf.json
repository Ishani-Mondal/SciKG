{"title": [{"text": "Mixing Multiple Translation Models in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.7081509232521057}]}], "abstractContent": [{"text": "Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in anew domain.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6748574574788412}]}, {"text": "We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step.", "labels": [], "entities": []}, {"text": "In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain.", "labels": [], "entities": [{"text": "domain adaptation setting", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.8011243939399719}]}, {"text": "Our experimental results show that ensemble decoding outper-forms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.7417129278182983}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems require large parallel corpora in order to be able to obtain a reasonable translation quality.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7855524222056071}]}, {"text": "In statistical learning theory, it is assumed that the training and test datasets are drawn from the same distribution, or in other words, they are from the same domain.", "labels": [], "entities": [{"text": "statistical learning theory", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.8009963830312093}]}, {"text": "However, bilingual corpora are only available in very limited domains and building bilingual resources in anew domain is usually very expensive.", "labels": [], "entities": []}, {"text": "It is an interesting question whether a model that is trained on an existing large bilingual corpus in a specific domain can be adapted to another domain for which little parallel data is present.", "labels": [], "entities": []}, {"text": "Domain adaptation techniques aim at finding ways to adjust an out-of-domain (OUT) model to represent a target domain (in-domain or IN).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7628448009490967}]}, {"text": "Common techniques for model adaptation adapt two main components of contemporary state-of-theart SMT systems: the language model and the translation model.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7291448265314102}, {"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9423420429229736}]}, {"text": "However, language model adaptation is a more straight-forward problem compared to translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.7286360263824463}, {"text": "translation model adaptation", "start_pos": 82, "end_pos": 110, "type": "TASK", "confidence": 0.8764346837997437}]}, {"text": "As a result, language model adaptation has been well studied in various work) both for speech recognition and for machine translation.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6687387824058533}, {"text": "speech recognition", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8008851706981659}, {"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7804532647132874}]}, {"text": "It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.9167619148890177}]}, {"text": "In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments.", "labels": [], "entities": []}, {"text": "We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7179938405752182}, {"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7620140612125397}, {"text": "translation model adaptation", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.8057735761006674}, {"text": "language model adaptation", "start_pos": 158, "end_pos": 183, "type": "TASK", "confidence": 0.6312993764877319}]}, {"text": "In this paper, anew approach for adapting the translation model is proposed.", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9632782936096191}]}, {"text": "We use a novel system combination approach called ensemble decoding in order to combine two or more translation models with the goal of constructing a system that outperforms all the component models.", "labels": [], "entities": []}, {"text": "The strength of this system combination method is that the systems are combined in the decoder.", "labels": [], "entities": []}, {"text": "This enables the decoder to pick the best hypotheses for each span of the input.", "labels": [], "entities": []}, {"text": "The main applications of ensemble models are domain adaptation, domain mixing and system combination.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7748562693595886}, {"text": "system combination", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7015337646007538}]}, {"text": "We have modified, an in-house implementation of hierarchical phrase-based translation system, to implement ensemble decoding using multiple translation models.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6723879426717758}]}, {"text": "We compare the results of ensemble decoding with a number of baselines for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7002174705266953}]}, {"text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model as well as the linear mixture model of) for conditional phrase-pair probabilities over IN and OUT.", "labels": [], "entities": []}, {"text": "Furthermore, within the framework of ensemble decoding, we study and evaluate various methods for combining translation tables.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out translation experiments using the The details of datasets used are summarized in.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9671273231506348}]}, {"text": "Sents For the mixture baselines, we used a standard one-pass phrase-based system (,), with the following 7 features: relative-frequency and lexical translation model (TM) probabilities in both directions; worddisplacement distortion model; language model (LM) and word count.", "labels": [], "entities": []}, {"text": "The corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7.", "labels": [], "entities": []}, {"text": "It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model.", "labels": [], "entities": []}, {"text": "For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system,) which uses the same features mentioned in (: forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty.", "labels": [], "entities": []}, {"text": "GIZA++) has been used for word alignment with phrase length limit of 7.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8071226477622986}]}, {"text": "In both systems, feature weights were optimized using MERT and with a 5-gram lan-1 www.statmt.org/europarl 2 Please contact the authors to access the data-sets.", "labels": [], "entities": [{"text": "MERT", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9032378792762756}]}, {"text": "guage model and Kneser-Ney smoothing was used in all the experiments.", "labels": [], "entities": [{"text": "Kneser-Ney smoothing", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.4894772469997406}]}, {"text": "We used SRILM) as the langugage model toolkit.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.6337136030197144}]}, {"text": "Fixing the language model allows us to compare various translation model combination techniques.", "labels": [], "entities": []}, {"text": "shows the results of the baselines.", "labels": [], "entities": []}, {"text": "The first group are the baseline results on the phrase-based system discussed in Section 2 and the second group are those of our hierarchical MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9125949144363403}]}, {"text": "Since the Hiero baselines results were substantially better than those of the phrase-based model, we also implemented the best-performing baseline, linear mixture, in our Hiero-style MT system and in fact it achieves the hights BLEU score among all the baselines as shown in shows the results of ensemble decoding with different mixture operations and model weight settings.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 228, "end_pos": 238, "type": "METRIC", "confidence": 0.9798104465007782}]}, {"text": "Each mixture operation has been evaluated on the test-set by setting the component weights uniformly (denoted by uniform) and by tuning the weights using CONDOR (denoted by tuned) on a held-out set.", "labels": [], "entities": [{"text": "CONDOR", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9848901629447937}]}, {"text": "The tuned scores (3rd column in Table 3) are averages of three runs with different initial points as in.", "labels": [], "entities": []}, {"text": "We also reported the BLEU scores when we applied the span-wise normalization heuristic.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9996501207351685}]}, {"text": "All of these mixture operations were able to significantly improve over the concatenation baseline.", "labels": [], "entities": []}, {"text": "In particular, Switching:Max could gain up to 2.2 BLEU points over the concatenation baseline and 0.39 BLEU points over the best performing baseline (i.e. linear mixture model implemented in Hiero) which is statistically significant based on", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9977771639823914}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9971740245819092}, {"text": "Hiero", "start_pos": 191, "end_pos": 196, "type": "DATASET", "confidence": 0.9134967923164368}]}], "tableCaptions": [{"text": " Table 2. This baseline is run three times  the score is averaged over the BLEU scores with  standard deviation of 0.34.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9946921467781067}]}, {"text": " Table 2: The results of various baselines implemented in  a phrase-based (PBS) and a Hiero SMT on EMEA.", "labels": [], "entities": [{"text": "Hiero SMT", "start_pos": 86, "end_pos": 95, "type": "TASK", "confidence": 0.5877674520015717}, {"text": "EMEA", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9317498207092285}]}, {"text": " Table 3: The results of ensemble decoding on EMEA for Fr2En when using uniform weights, tuned weights and  normalization heuristic. The tuned BLEU scores are averaged over three runs with multiple initial points, as in (Clark  et al., 2011), with the standard deviations in brackets .", "labels": [], "entities": [{"text": "Fr2En", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9052117466926575}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9980021119117737}]}]}