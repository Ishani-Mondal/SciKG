{"title": [{"text": "Building trainable taggers in a web-based, UIMA-supported NLP workbench", "labels": [], "entities": [{"text": "UIMA-supported NLP workbench", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.7851633230845133}]}], "abstractContent": [{"text": "Argo is a web-based NLP and text mining workbench with a convenient graphical user interface for designing and executing processing workflows of various complexity.", "labels": [], "entities": []}, {"text": "The workbench is intended for specialists and non-technical audiences alike, and provides the ever expanding library of analytics compliant with the Unstructured Information Management Architecture, a widely adopted interop-erability framework.", "labels": [], "entities": []}, {"text": "We explore the flexibility of this framework by demonstrating work-flows involving three processing components capable of performing self-contained machine learning-based tagging.", "labels": [], "entities": [{"text": "machine learning-based tagging", "start_pos": 148, "end_pos": 178, "type": "TASK", "confidence": 0.6290799379348755}]}, {"text": "The three components are responsible for the three distinct tasks of 1) generating observations or features, 2) training a statistical model based on the generated features, and 3) tagging unlabelled data with the model.", "labels": [], "entities": []}, {"text": "The learning and tagging components are based on an implementation of conditional random fields (CRF); whereas the feature generation component is an analytic capable of extending basic token information to a comprehensive set of features.", "labels": [], "entities": []}, {"text": "Users define the features of their choice directly from Argo's graphical interface, without resorting to programming (a commonly used approach to feature engineering).", "labels": [], "entities": []}, {"text": "The experimental results performed on two tagging tasks, chunk-ing and named entity recognition, showed that a tagger with a generic set of features builtin Argo is capable of competing with task-specific solutions.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.64246799548467}]}], "introductionContent": [{"text": "The applications of automatic recognition of categories, or tagging, in natural language processing (NLP), range from part of speech tagging to chunking to named entity recognition and complex scientific discourse analyses.", "labels": [], "entities": [{"text": "automatic recognition of categories, or tagging", "start_pos": 20, "end_pos": 67, "type": "TASK", "confidence": 0.6957728479589734}, {"text": "speech tagging", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.732620507478714}, {"text": "named entity recognition", "start_pos": 156, "end_pos": 180, "type": "TASK", "confidence": 0.6230725149313608}]}, {"text": "Currently, there is a variety of tools capable of performing these tasks.", "labels": [], "entities": []}, {"text": "A commonly used approach involves the use of machine learning to first build a statistical model based on a manually or semi-automatically tagged sample data and then to tag new data using this model.", "labels": [], "entities": []}, {"text": "Since the machine learning algorithms for building models are well established, the challenge shifted to feature engineering, i.e., developing task-specific features that form the basis of these statistical models.", "labels": [], "entities": []}, {"text": "This task is usually accomplished programmatically which pose an obstacle to a non-technically inclined audience.", "labels": [], "entities": []}, {"text": "We alleviate this problem by demonstrating Argo 1 , a web-based platform that allows the user to build NLP and other text analysis workflows via a graphical user interface (GUI) available in a web browser.", "labels": [], "entities": []}, {"text": "The system is equipped with an ever growing library of text processing components ranging from low-level syntactic analysers to semantic annotators.", "labels": [], "entities": []}, {"text": "It also allows for including user-interactive components, such as an annotation editor, into otherwise fully automatic workflows.", "labels": [], "entities": []}, {"text": "The interoperability of processing components is ensured in Argo by adopting Unstructured Information Management Architecture (UIMA)) as the system's framework.", "labels": [], "entities": []}, {"text": "In this work we explore the capabilities of this framework to support machine learning components for tagging textual content.", "labels": [], "entities": [{"text": "tagging textual content", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.882663369178772}]}, {"text": "In the following section we present related work.", "labels": [], "entities": []}, {"text": "Section 3 provides background information on Argo and its relationship to UIMA.", "labels": [], "entities": [{"text": "Argo", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.47371238470077515}, {"text": "UIMA", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.8587416410446167}]}, {"text": "The details of the three machine learning components are discussed in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 provides evaluation, whereas Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We show the performance of taggers trained with two distinct sets of features, basic and extended.", "labels": [], "entities": [{"text": "taggers", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9726461172103882}]}, {"text": "The basic set of features uses token fields such as the covered text and the part of speech without any transformations or context n-grams.", "labels": [], "entities": []}, {"text": "The extended set makes the full use of Feature Generator's settings and enriches the basic set with various transformations and context n-grams.", "labels": [], "entities": []}, {"text": "The transformations in-  clude surface shape, length, prefixes, suffixes, and the presence of various combinations of letters, digits and symbols.", "labels": [], "entities": []}, {"text": "The context n-grams include unigrams for all feature definitions and bigrams for selected ones.", "labels": [], "entities": []}, {"text": "shows a sample of the actual extended set.", "labels": [], "entities": []}, {"text": "We use two datasets, one prepared for the CoNLL 2000 shared task () and another prepared for the BioNLP/NLPBA 2004 shared task (.", "labels": [], "entities": [{"text": "CoNLL 2000 shared task", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.8760182857513428}, {"text": "BioNLP/NLPBA 2004 shared task", "start_pos": 97, "end_pos": 126, "type": "DATASET", "confidence": 0.8472900291283926}]}, {"text": "They represent two different tagging tasks, chunking and named entity recognition, respectively.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6214322845141093}]}, {"text": "The CoNLL 2000 chunking dataset involves 10 labels and comes pre-tokenised with 211,727 tokens in the training set and 47,377 tokens in the test set.", "labels": [], "entities": [{"text": "CoNLL 2000 chunking dataset", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.8617902845144272}]}, {"text": "The dataset also provides partof-speech tags for each token.", "labels": [], "entities": []}, {"text": "The BioNLP/NLPBA 2004 named entity recognition dataset involves five biology-related labels and consists of 472,006 and 96,780 tokens in the training and testing sets, respectively.", "labels": [], "entities": [{"text": "BioNLP/NLPBA 2004 named entity recognition dataset", "start_pos": 4, "end_pos": 54, "type": "DATASET", "confidence": 0.8418276458978653}]}, {"text": "Contrary to the former dataset, there is no other information supporting the tokens in the BioNLP/NLPBA dataset.", "labels": [], "entities": [{"text": "BioNLP/NLPBA dataset", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.8952761143445969}]}, {"text": "To compensate for it we automatically generated part of speech and chunk labels for each token.", "labels": [], "entities": []}, {"text": "The chosen datasets/tasks are by no means an exhaustive set of representative comparative-setup datasets available.", "labels": [], "entities": []}, {"text": "Our goal is not to claim the superiority of our approach over the solutions reported in the respective shared tasks.", "labels": [], "entities": []}, {"text": "Instead, we aim to show that our generic setup is comparable to those task-tuned solutions.", "labels": [], "entities": []}, {"text": "We further explore the options of both Feature Generator and CRF++ Trainer by manipulating labelling formats (IOB vs IOBES ()) for the former and parameter estimation algorithms (L 2 -vs L 1 -norm regularisation) for the latter.", "labels": [], "entities": []}, {"text": "Ultimately, there are 32 setups as the result of the combinations of the two feature sets, the two datasets, the two labelling formats and the two estimation algorithms.", "labels": [], "entities": []}, {"text": "shows the precision, recall and f-scores of our extended-feature setups against each other as well as with reference to the best and baseline solutions as reported in the respective shared tasks.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9996825456619263}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9986510872840881}, {"text": "f-scores", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9629595279693604}]}, {"text": "The gap to the best performing solution for the chunking task is about 1.3% points in F-score, ahead of the baseline by 15.7% points.", "labels": [], "entities": [{"text": "chunking task", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9203626811504364}, {"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9988100528717041}]}, {"text": "Respectively for the NER task, our best setup stands behind the best reported solution by about 7% points, ahead of the baseline by about 18% points.", "labels": [], "entities": [{"text": "NER task", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.92486372590065}]}, {"text": "In both instances our solution would be placed in the middle of the reported rankings, which is a promising result, especially that our setups are based solely on the tokens' surface form, part of speech, and (in the case of the NER task) chunk.", "labels": [], "entities": []}, {"text": "In contrast, the best solutions for the NER task involve the use of dictionaries and advanced analyses such as acronym resolution.", "labels": [], "entities": [{"text": "NER task", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.9387790858745575}, {"text": "acronym resolution", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.9106889367103577}]}], "tableCaptions": [{"text": " Table 1: Performance of various setups (L1 vs L2,  and IOB vs IOBES) on the chunking and NER tasks.  The setups are ordered by F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.9925434589385986}]}, {"text": " Table 2: Comparison of setups with basic and ex- tended features for the chunking and NER tasks.", "labels": [], "entities": []}, {"text": " Table 3: Number of iterations needed for the optimi- sation algorithm to converge.", "labels": [], "entities": []}]}