{"title": [{"text": "Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7279080748558044}]}], "abstractContent": [{"text": "This paper presents a novel way of improving POS tagging on heterogeneous data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.965427577495575}]}, {"text": "First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies.", "labels": [], "entities": []}, {"text": "During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data.", "labels": [], "entities": []}, {"text": "This dynamic model selection approach, coupled with a one-pass, left-to-right POS tagging algorithm, is evaluated on corpora from seven different genres.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.6313053518533707}]}, {"text": "Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9813135862350464}]}, {"text": "Furthermore, our system is able to tag about 32K tokens per second.", "labels": [], "entities": []}, {"text": "We believe that this model selection approach can be applied to more sophisticated tagging algorithms and improve their robustness even further.", "labels": [], "entities": [{"text": "tagging", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.9655493497848511}]}], "introductionContent": [{"text": "When it comes to POS tagging, two things must be checked.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8599868416786194}]}, {"text": "First, a POS tagger needs to be tested for its robustness in handling heterogeneous data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.7290780544281006}]}, {"text": "1 Statistical POS taggers perform very well when their training and testing data are from the same source, achieving over 97% tagging accuracy (.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7149218022823334}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9842931628227234}]}, {"text": "However, the performance degrades increasingly as the discrepancy between the training and testing data gets larger.", "labels": [], "entities": []}, {"text": "Thus, to ensure robustness, a tagger needs to be evaluated on several different kinds of data.", "labels": [], "entities": []}, {"text": "Second, a POS tagger should be tested for its speed.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.6929142773151398}]}, {"text": "POS tagging is often performed as a pre-processing step to other tasks (e.g., parsing, chunking) and it should not be a bottleneck for those tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8127340376377106}, {"text": "parsing, chunking", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.6567105452219645}]}, {"text": "Moreover, recent NLP tasks deal with very large-scale data where tagging speed is critical.", "labels": [], "entities": [{"text": "tagging", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9617753028869629}]}, {"text": "To improve robustness, we first train two separate models; one is optimized fora general domain and the other is optimized fora domain specific to the training data.", "labels": [], "entities": []}, {"text": "During decoding, we dynamically select one of the models by measuring similarities between input sentences and the training data.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the domain-specific and generalized models perform better for sentences similar and not similar to the training data, respectively.", "labels": [], "entities": []}, {"text": "In this paper, we describe how to build both models using the same training data and select an appropriate model given input sentences during decoding.", "labels": [], "entities": []}, {"text": "Each model uses a one-pass, left-to-right POS tagging algorithm.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.7148542106151581}]}, {"text": "Even with the simple tagging algorithm, our system gives results that are comparable to two other stateof-the-art systems when coupled with this dynamic model selection approach.", "labels": [], "entities": [{"text": "tagging", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9600822329521179}]}, {"text": "Furthermore, our system shows noticeably faster tagging speed compared to the other two systems.", "labels": [], "entities": [{"text": "tagging", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9584570527076721}]}, {"text": "For our experiments, we use corpora from seven different genres.", "labels": [], "entities": []}, {"text": "This allows us to check the performance of each system on different kinds of data when run individually or selectively.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first time that a POS tagger has been evaluated on such a wide variety of data in English.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.7138862609863281}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Distributions of evaluation sets. The Total column indicates a mixture of data from all genres.", "labels": [], "entities": []}, {"text": " Table 3: Tagging accuracies of all tokens (in %). Models D and G indicate domain-specific and generalized models,  respectively and Model S indicates the dynamic model selection approach. \"G over D\" shows how often Model G is  selected over Model D using the dynamic selection (in %).", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.972730278968811}]}, {"text": " Table 4: Tagging accuracies of unknown tokens (in %).", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.977242648601532}]}]}