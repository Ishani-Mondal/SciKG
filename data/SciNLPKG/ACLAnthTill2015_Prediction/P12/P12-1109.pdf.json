{"title": [{"text": "A Broad-Coverage Normalization System for Social Media Language", "labels": [], "entities": [{"text": "Broad-Coverage Normalization", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.6976025700569153}]}], "abstractContent": [{"text": "Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users.", "labels": [], "entities": []}, {"text": "It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques.", "labels": [], "entities": []}, {"text": "A major challenge facing this task is the system coverage , i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates.", "labels": [], "entities": [{"text": "coverage", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8197259306907654}]}, {"text": "In this paper, we propose a cognitively-driven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity.", "labels": [], "entities": [{"text": "letter transformation", "start_pos": 175, "end_pos": 196, "type": "TASK", "confidence": 0.7511720657348633}]}, {"text": "The system was evaluated on both word-and message-level using four SMS and Twitter data sets.", "labels": [], "entities": [{"text": "Twitter data sets", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.7861177126566569}]}, {"text": "Results show that our system achieves over 90% word-coverage across all data sets (a 10% absolute increase compared to state-of-the-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "The amount of user generated content has increased drastically in the past few years, driven by the prosperous development of the social media websites such as Twitter, Facebook, and Google+.", "labels": [], "entities": []}, {"text": "As of June 2011, Twitter has attracted over 300 million users and produces more than 2 billion tweets per week.", "labels": [], "entities": []}, {"text": "Ina broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc.", "labels": [], "entities": []}, {"text": "can all be considered as \"social text\", which is significantly different from the traditional news text due to the informal writing style and the conversational nature.", "labels": [], "entities": []}, {"text": "The social text serves as a very valuable information source for many NLP applications, such as the information extraction (), retrieval), summarization (), sentiment analysis (, etc.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7363936901092529}, {"text": "summarization", "start_pos": 139, "end_pos": 152, "type": "TASK", "confidence": 0.9874985814094543}, {"text": "sentiment analysis", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.9675278067588806}]}, {"text": "Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc.", "labels": [], "entities": []}, {"text": "It is reported that the Stanford named entity recognizer (NER) experienced a performance drop from 90.8% to 45.8% on tweets (); the part-of-speech (POS) tagger and dependency parser degraded 12.2% and 20.65% respectively on tweets.", "labels": [], "entities": [{"text": "Stanford named entity recognizer (NER", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.6500549167394638}]}, {"text": "It is therefore of great importance to normalize the social text before applying the standard NLP techniques.", "labels": [], "entities": []}, {"text": "Text normalization is also crucial for building robust text-to-speech (TTS) systems, which need to determine the pronunciations for nonstandard words in the social text.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7640154361724854}]}, {"text": "The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words.", "labels": [], "entities": []}, {"text": "We aim fora robust text normalization system with \"broad coverage\", i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3, 10...).", "labels": [], "entities": [{"text": "text normalization", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7195632755756378}]}, {"text": "This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens.", "labels": [], "entities": []}, {"text": "() found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (; second, the nonstandard tokens consist togetha tgthr togeda 2getha togather t0gether (57) toqethaa (10) 2gthr togehter togeter 2getter u ya yo yaa yaaa yew yuo youz (426) yoooooou (186) youy yoiu yoooouuuu (82): Nonstandard tokens and their frequencies in the Edinburgh Twitter corpus.", "labels": [], "entities": [{"text": "Edinburgh Twitter corpus", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.9837093154589335}, {"text": "Edinburgh Twitter corpus", "start_pos": 358, "end_pos": 382, "type": "DATASET", "confidence": 0.9825440247853597}]}, {"text": "The corresponding standard words are \"together\" and \"you\", respectively. of a mixture of both unintentional misspellings and intentionally-created tokens for various reasons 1 , including the needs for speed, ease of typing, sentiment expressing (e.g., \"coooool\" (Brody and Diakopoulos, 2011)), intimacy and social purpose, etc., making it even harder to decipher the social messages.", "labels": [], "entities": [{"text": "speed", "start_pos": 202, "end_pos": 207, "type": "METRIC", "confidence": 0.9804648160934448}, {"text": "sentiment expressing", "start_pos": 225, "end_pos": 245, "type": "TASK", "confidence": 0.7618587911128998}]}, {"text": "shows some example nonstandard tokens.", "labels": [], "entities": []}, {"text": "Existing spell checkers and normalization systems rely heavily on lexical/phonetic similarity to select the correct candidate words.", "labels": [], "entities": [{"text": "spell checkers", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7019252330064774}]}, {"text": "This may notwork well since a good portion of the correct words lie outside the specified similarity threshold (e.g., (tomorrow, \"tmrw\") ), yet the number of candidates increases dramatically as the system strives to increase the coverage by enlarging the threshold.", "labels": [], "entities": []}, {"text": "() reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%.", "labels": [], "entities": [{"text": "correct-word coverage", "start_pos": 72, "end_pos": 93, "type": "METRIC", "confidence": 0.8054236173629761}]}, {"text": "The low coverage score also enforces an undesirable performance ceiling for the candidate reranking approaches.", "labels": [], "entities": [{"text": "coverage score", "start_pos": 8, "end_pos": 22, "type": "METRIC", "confidence": 0.9785222113132477}]}, {"text": "Different from previous work, we tackle the text normalization problem from a cognitive-sensitive perspective and investigate the human rationales for normalizing the nonstandard tokens.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7910276055335999}]}, {"text": "We argue that there exists a set of letter transformation patterns that humans use to decipher the nonstandard tokens.", "labels": [], "entities": []}, {"text": "Moreover, the \"visual priming\" effect may play an important role inhuman comprehension of the noisy tokens.", "labels": [], "entities": []}, {"text": "\"Priming\" represents an implicit memory effect.", "labels": [], "entities": []}, {"text": "For example, if a person reads a list of words including the word table, and is later asked to complete a word starting with tab-, it is very likely that he answers table since the person is primed.", "labels": [], "entities": []}, {"text": "In this paper, we propose a broad-coverage normalization system by integrating three human per-spectives, including the enhanced letter transformation, visual priming, and the string and phonetic similarity.", "labels": [], "entities": [{"text": "broad-coverage normalization", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6881407797336578}, {"text": "letter transformation", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.7322285026311874}]}, {"text": "For an arbitrary nonstandard token, the three subnormalizers each suggest their most confident candidates from a different perspective.", "labels": [], "entities": []}, {"text": "The candidates can then be heuristically combined or reranked using a message-level decoding process.", "labels": [], "entities": []}, {"text": "We evaluate the system on both word-and messagelevel using four SMS and Twitter data sets.", "labels": [], "entities": [{"text": "Twitter data sets", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.7651507159074148}]}, {"text": "Results show that our system can achieve over 90% wordcoverage with limited number of candidates and the broad word-coverage can be successfully translated into message-level performance gain.", "labels": [], "entities": []}, {"text": "In addition, our system requires no human annotations, therefore can be easily adapted to different domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use four SMS and Twitter data sets to evaluate the system effectiveness.", "labels": [], "entities": []}, {"text": "Statistics of these data sets are summarized in.", "labels": [], "entities": []}, {"text": "Data set (1) to (3) are used for word-level evaluation; data set (4) for both word-and message-level evaluation.", "labels": [], "entities": []}, {"text": "In, we also present the number of distinct nonstandard tokens found in each data set, and notice that only a small portion of the nonstandard tokens correspond to multiple standard words.", "labels": [], "entities": []}, {"text": "We calculate the dictionary coverage of the manually annotated words since this sets an upper bound for any normalization system.", "labels": [], "entities": []}, {"text": "We use the Edinburgh Twitter corpus () as the background corpus for frequency calculation, and a dictionary containing 82,324 words.", "labels": [], "entities": [{"text": "Edinburgh Twitter corpus", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9802672863006592}, {"text": "frequency calculation", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.5645179003477097}]}, {"text": "The nonstandard tokens may consist of both numbers/characters and apostrophe.", "labels": [], "entities": []}, {"text": "The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words.", "labels": [], "entities": [{"text": "word-level normalization", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.7306393384933472}]}, {"text": "For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system.", "labels": [], "entities": []}, {"text": "We adopt this word-level n-best accuracy to make our results comparable to other state-of-the-art systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8868620991706848}]}, {"text": "On message-level, we evaluate the 1-best system output using precision, recall, and f-score, calculated respective to the nonstandard tokens.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996230602264404}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9986634254455566}, {"text": "f-score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9840901494026184}]}, {"text": "Word Level Accuracy (%) (303 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell   normalizer performs surprisingly well and shows robust performance across all data sets.", "labels": [], "entities": [{"text": "Word Level Accuracy", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.5440345605214437}, {"text": "Oracle Jazzy Spell   normalizer", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.4882837384939194}]}, {"text": "A minor sideeffect is that the candidates were restricted to have the same first letter with the noisy token, this sets the upper bound of the approach to 89.77%, 92.45%, and 93.51%, respectively on data set (1), (2), and (3).", "labels": [], "entities": []}, {"text": "Compared to other subnormalizers, the \"Enhanced Letter Tran.\" is effective at normalizing intentionally created tokens and has better precision regarding its top candidate (n = 1).", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9991668462753296}]}, {"text": "We demonstrate the context-aware training pair selection results in, by plotting the learning curve using different amounts of training data, ranging from 1,000 (word, token) pairs to the total 46,288 pairs.", "labels": [], "entities": [{"text": "context-aware training pair selection", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.5619708001613617}]}, {"text": "We notice that the system can effectively learn the letter transformation patterns from a small number of high quality training pairs.", "labels": [], "entities": [{"text": "letter transformation patterns", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.8127769033114115}]}, {"text": "The final system was trained using the top 5,000 pairs and the lookup table was created by generating 50 variations for each dictionary word.", "labels": [], "entities": []}, {"text": "We use the word-level \"Broad-Cov.", "labels": [], "entities": [{"text": "Broad-Cov.", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.9746226668357849}]}, {"text": "System\" for candidate suggestion and the Viterbi algorithm for message-level decoding.", "labels": [], "entities": [{"text": "candidate suggestion", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7874371707439423}]}, {"text": "The system is evaluated on data set (4) and results shown in.", "labels": [], "entities": []}, {"text": "Following research in (Han and Baldwin, 2011), we focus on the the normalization task and assume perfect nonstandard token detection.", "labels": [], "entities": [{"text": "token detection", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.698571965098381}]}, {"text": "The \"Word-level w/o Context\" results are generated by replacing each nonstandard token using the 1-best word-level candidate.", "labels": [], "entities": [{"text": "Word-level w/o Context\"", "start_pos": 5, "end_pos": 28, "type": "TASK", "confidence": 0.5346020509799322}]}, {"text": "Although the replacement process is static, it results in 70.97% fscore due to the high performance of the word-level system.", "labels": [], "entities": [{"text": "fscore", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9975280165672302}]}, {"text": "We explore two language models (LM) for the Viterbi decoding process.", "labels": [], "entities": []}, {"text": "First, a bigram LM is trained using the Edinburgh Twitter corpus (53,794,549 English tweets) with the SRILM toolkit) and Kneser-Ney smoothing; second, we retrieve the bigram probabilities from the Microsoft Web N-gram API () since this represents a more comprehensive web-based corpus.", "labels": [], "entities": [{"text": "Edinburgh Twitter corpus", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.9762364427248637}, {"text": "SRILM toolkit", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.8925627171993256}]}, {"text": "During decoding, we use the \"VisualPrim\" score as the emission probability, since this score best fits the log scale and applies to all candidates.", "labels": [], "entities": [{"text": "VisualPrim\" score", "start_pos": 29, "end_pos": 46, "type": "METRIC", "confidence": 0.836353083451589}]}, {"text": "For the Twitter LM, we apply a scaling factor of 0.5 to the \"VisualPrim\" score to make it comparable in scale to the LM probabilities.", "labels": [], "entities": []}, {"text": "We use the 3-best word-level candidates for Viterbi decoding.", "labels": [], "entities": []}, {"text": "In addition, we add the commonly used corrections for: Message-level results on data set (4).", "labels": [], "entities": [{"text": "corrections", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9752443432807922}]}, {"text": "denotes system requires human annotations for training.", "labels": [], "entities": []}, {"text": "16 single-characters, e.g., for \"r\", \"c\", we add \"are\", \"see\" to the candidate list if they are not already presented.", "labels": [], "entities": []}, {"text": "A default \"VisualPrim\" score (\u03b7 = 25) is used for these candidates.", "labels": [], "entities": [{"text": "VisualPrim\" score", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.773658275604248}]}, {"text": "As seen from, both Web LM and Twitter LM achieve better performance than the best prior results, with Twitter LM outperforms the Web LM, yielding a f-score of 81%.", "labels": [], "entities": [{"text": "f-score", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.993966817855835}]}, {"text": "This shows that a vanilla Viterbi decoding process is able to outperform the fine-tuned supervised system given competitive word-level candidates.", "labels": [], "entities": []}, {"text": "In future, we will investigate other comprehensive messagelevel candidate reranking process.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics of different SMS and Twitter data sets.", "labels": [], "entities": []}, {"text": " Table 4: Word-level results on data set (1). denotes  system requires human annotations for training.", "labels": [], "entities": []}, {"text": " Table 5: Word-level results on data set (2).", "labels": [], "entities": []}, {"text": " Table 6: Word-level results on data set (3). denotes  system requires human annotations for training.", "labels": [], "entities": []}, {"text": " Table 7: Message-level results on data set (4). denotes  system requires human annotations for training.", "labels": [], "entities": []}]}