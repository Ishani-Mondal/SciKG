{"title": [{"text": "Native Language Detection with Tree Substitution Grammars", "labels": [], "entities": [{"text": "Native Language Detection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6229958236217499}]}], "abstractContent": [{"text": "We investigate the potential of Tree Substitution Grammars as a source of features for native language detection, the task of inferring an author's native language from text in a different language.", "labels": [], "entities": [{"text": "native language detection", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.6365570922692617}]}, {"text": "We compare two state of the art methods for Tree Substitution Grammar induction and show that features from both methods outperform previous state of the art results at native language detection.", "labels": [], "entities": [{"text": "Tree Substitution Grammar induction", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.79547318816185}, {"text": "native language detection", "start_pos": 169, "end_pos": 194, "type": "TASK", "confidence": 0.6499855915705363}]}, {"text": "Furthermore , we contrast these two induction algorithms and show that the Bayesian approach produces superior classification results with a smaller feature set.", "labels": [], "entities": []}], "introductionContent": [{"text": "The correlation between a person's native language (L1) and aspects of their writing in a second language (L2) can be exploited to predict L1 label given L2 text.", "labels": [], "entities": []}, {"text": "The International Corpus of Learner English (), or ICLE, is a large set of English student essays annotated with L1 labels that allows us to bring the power of supervised machine learning techniques to bear on this task.", "labels": [], "entities": [{"text": "International Corpus of Learner English", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.942182970046997}]}, {"text": "In this work we explore the possibility of automatically induced Tree Substitution Grammar (TSG) rules as features fora logistic regression model 1 trained to predict these L1 labels.", "labels": [], "entities": []}, {"text": "Automatic TSG induction is made difficult by the exponential number of possible TSG rules given a corpus.", "labels": [], "entities": [{"text": "TSG induction", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9674874842166901}]}, {"text": "This is an active area of research with two distinct effective solutions.", "labels": [], "entities": []}, {"text": "The first uses a nonparametric Bayesian model to handle the large number a.k.a.", "labels": [], "entities": []}, {"text": "Maximum Entropy Model of rules , while the second is inspired by tree kernel methods and extracts common subtrees from pairs of parse trees ().", "labels": [], "entities": []}, {"text": "While both are effective, we show that the Bayesian method of TSG induction produces superior features and achieves anew best result at the task of native language detection.", "labels": [], "entities": [{"text": "TSG induction", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9702005088329315}, {"text": "native language detection", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.6309660772482554}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Sentence based classification accuracy", "labels": [], "entities": [{"text": "Sentence based classification", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.9350593487421671}]}]}