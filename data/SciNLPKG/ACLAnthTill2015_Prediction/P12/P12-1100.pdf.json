{"title": [], "abstractContent": [{"text": "We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrase-based model and the tree-to-string model, to combine the merits of the two models.", "labels": [], "entities": [{"text": "hierarchical chunk-to-string translation", "start_pos": 13, "end_pos": 53, "type": "TASK", "confidence": 0.6853790283203125}]}, {"text": "With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion.", "labels": [], "entities": []}, {"text": "Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously.", "labels": [], "entities": []}, {"text": "Our experiments show that our model significantly outperforms the hierarchical phrase-based model and the tree-to-string model on English-Chinese Translation tasks.", "labels": [], "entities": [{"text": "English-Chinese Translation tasks", "start_pos": 130, "end_pos": 163, "type": "TASK", "confidence": 0.7397566537062327}]}], "introductionContent": [{"text": "The hierarchical phrase-based model makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6389690935611725}]}, {"text": "Besides, this model is formal syntax-based and does not need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely.", "labels": [], "entities": []}, {"text": "* This work was done when the first author visited Microsoft Research Asia as an intern.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.9117896954218546}]}, {"text": "However, it is often desirable to consider syntactic constituents of subphrases, e.g. the hierarchical phrase X \u2192 X 1 for X 2 , X 2 de X 1 can be applied to both of the following strings in \"A request fora purchase of shares\" \"filed for bankruptcy\", and get the following translation, respectively \"goumai gufen de shenqing\" \"pochan de shenqing\".", "labels": [], "entities": []}, {"text": "In the former, \"A request\" is a NP and this rule acts correctly while in the latter \"filed\" is a VP and this rule gives a wrong reordering.", "labels": [], "entities": []}, {"text": "If we specify the first X on the right-hand side to NP, this kind of errors can be avoided.", "labels": [], "entities": []}, {"text": "The tree-to-string model () introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering.", "labels": [], "entities": []}, {"text": "Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion.", "labels": [], "entities": []}, {"text": "Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous.", "labels": [], "entities": []}, {"text": "shows that translation between English and French satisfies cohesion in the majority cases.", "labels": [], "entities": [{"text": "translation between English and French", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.8769700765609741}]}, {"text": "Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs.", "labels": [], "entities": []}, {"text": "But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence.", "labels": [], "entities": [{"text": "rule matching", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.7071496546268463}]}, {"text": "This will lead to data sparseness and being vulnerable to parse errors.", "labels": [], "entities": []}, {"text": "In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two models.", "labels": [], "entities": []}, {"text": "Instead of parse trees, our model introduces linguistic information in the form of chunks, so it does not need to care the internal structures and the roles in the main sentence of chunks.", "labels": [], "entities": []}, {"text": "Based on shallow parsing results, it learns rules consisting of either words (terminals) or chunks (nonterminals), where adjacent chunks are packed into one nonterminal.", "labels": [], "entities": []}, {"text": "It searches for the best derivation through the SCFG-motivated space defined by these rules and get target translation simultaneously.", "labels": [], "entities": []}, {"text": "In some sense, our model can be seen as a compromise between the hierarchical phrase-based model and the tree-tostring model, specifically \u2022 Compared with the hierarchical phrase-based model, it integrates linguistic syntax and satisfies syntactic cohesion.", "labels": [], "entities": []}, {"text": "\u2022 Compared with the tree-to-string model, it only needs to perform shallow parsing which introduces less parsing errors.", "labels": [], "entities": []}, {"text": "Besides, our model allows a nonterminal in a rule to cover several chunks, which can alleviate data sparseness and the influence of parsing errors.", "labels": [], "entities": []}, {"text": "\u2022 we refine our hierarchical chunk-to-string model into two models: a loose model (Section 2.1) which is more similar to the hierarchical phrase-based model and a tight model (Section 2.2) which is more similar to the tree-to-string model.", "labels": [], "entities": []}, {"text": "The experiments show that on the 2008 NIST English-Chinese MT translation test set, both the loose model and the tight model outperform the hierarchical phrase-based model and the tree-to-string model, where the loose model has a better performance.", "labels": [], "entities": [{"text": "NIST English-Chinese MT translation test set", "start_pos": 38, "end_pos": 82, "type": "DATASET", "confidence": 0.7583240866661072}]}, {"text": "While in terms of speed, the tight model runs faster and its speed ranking is between the treeto-string model and the hierarchical phrase-based model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Shallow parsing result. The collum Found gives  the number of chunks recognized by CRF, the row All  represents all types of chunks, and the row One represents  the chunks that consist of one word.", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison. Phrase represents  the hierarchical phrase-based decoder, tree represents the  tree-to-string decoder, tight represents our tight decoder  and loose represents our loose decoder. The speed is re- ported by seconds per sentence. The speed for the tree-to- string decoder includes the parsing time (0.23s) and the  speed for the tight and loose models includes the shallow  parsing time, too.", "labels": [], "entities": [{"text": "parsing", "start_pos": 317, "end_pos": 324, "type": "TASK", "confidence": 0.9512748718261719}]}, {"text": " Table 3: Influence of cohesion. The row cohesive rep- resents the loose system where nonterminals satisfy co- hesion, and the row noncohesive represents the modified  version of the loose system where nonterminals can be  noncohesive.", "labels": [], "entities": []}, {"text": " Table 4: The influence of the number of nonterminals.  The column number lists the number of nonterminals  used at most in a rule.", "labels": [], "entities": []}]}