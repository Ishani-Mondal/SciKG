{"title": [{"text": "Large-Scale Syntactic Language Modeling with Treelets", "labels": [], "entities": [{"text": "Syntactic Language Modeling", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7975131471951803}]}], "abstractContent": [{"text": "We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context.", "labels": [], "entities": []}, {"text": "We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours.", "labels": [], "entities": []}, {"text": "We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines.", "labels": [], "entities": []}, {"text": "Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone.", "labels": [], "entities": []}, {"text": "We also show fluency improvements in a preliminary machine translation experiment.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7164343595504761}]}], "introductionContent": [{"text": "N -gram language models area central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models, efficient storage (, and integration into decoders.", "labels": [], "entities": [{"text": "speech recognition and machine translation", "start_pos": 54, "end_pos": 96, "type": "TASK", "confidence": 0.7397430956363678}]}, {"text": "At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies.", "labels": [], "entities": []}, {"text": "Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models.", "labels": [], "entities": [{"text": "generative syntactic language", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.8726567427317301}]}, {"text": "In this paper, we describe a generative, syntactic language model that conditions on local context treelets 1 in a parse tree, backing off to smaller treelets as necessary.", "labels": [], "entities": []}, {"text": "Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (, enabling us to apply techniques developed for scaling n-gram models out of the box ().", "labels": [], "entities": []}, {"text": "The simplicity of our training procedure allows us to train a model on a billion tokens of data in a matter of hours on a single machine, which compares favorably to the more involved training algorithm of, who use a two-pass EM training algorithm that takes several days on several hundred CPUs using similar amounts of data.", "labels": [], "entities": []}, {"text": "The simplicity of our approach also contrasts with recent work on language modeling with tree substitution grammars, where larger treelet contexts are incorporated by using sophisticated priors to learn a segmentation of parse trees.", "labels": [], "entities": []}, {"text": "Such an approach implicitly assumes that a \"correct\" segmentation exists, but it is not clear that this is true in practice.", "labels": [], "entities": []}, {"text": "Instead, we build upon the success of n-gram language models, which do not assume a segmentation and instead score all overlapping contexts.", "labels": [], "entities": []}, {"text": "We evaluate our model in terms of perplexity, and show that we achieve the same performance as a state-of-the-art n-gram model.", "labels": [], "entities": []}, {"text": "We also evaluate our model on several grammaticality tasks proposed in", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model along several dimensions.", "labels": [], "entities": []}, {"text": "We first show some sample generated sentences in Section 5.1.", "labels": [], "entities": []}, {"text": "We report perplexity results in Section 5.2.", "labels": [], "entities": []}, {"text": "In Section 5.3, we measure its ability to distinguish between grammatical English and various types of automatically generated, or pseudonegative, English.", "labels": [], "entities": []}, {"text": "We report machine translation reranking results in Section 5.4.", "labels": [], "entities": [{"text": "machine translation reranking", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8499763011932373}]}], "tableCaptions": [{"text": " Table 2: Perplexity of several generative models on Sec- tion 0 of the WSJ. The differences between scores marked  with  \u2020 are not statistically significant. PCFG-LA (marked  with **) was only trained on the WSJ and Brown corpora  because it does not scale to large amounts of data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.7440739870071411}, {"text": "WSJ and Brown corpora", "start_pos": 209, "end_pos": 230, "type": "DATASET", "confidence": 0.7674296647310257}]}, {"text": " Table 4: Classification accuracy for trigram pseudo-negative", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.5865043997764587}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8078771829605103}]}, {"text": " Table 5: Classification accuracies on the noisy WSJ for mod-", "labels": [], "entities": [{"text": "WSJ", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.7819709777832031}]}]}