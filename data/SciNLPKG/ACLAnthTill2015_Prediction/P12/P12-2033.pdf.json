{"title": [{"text": "A Two-step Approach to Sentence Compression of Spoken Utterances", "labels": [], "entities": [{"text": "Sentence Compression of Spoken Utterances", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.9234305262565613}]}], "abstractContent": [{"text": "This paper presents a two-step approach to compress spontaneous spoken utterances.", "labels": [], "entities": [{"text": "compress spontaneous spoken utterances", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.8649725466966629}]}, {"text": "In the first step, we use a sequence labeling method to determine if a word in the utterance can be removed, and generate n-best compressed sentences.", "labels": [], "entities": []}, {"text": "In the second step, we use a discriminative training approach to capture sentence level global information from the candidates and rerank them.", "labels": [], "entities": []}, {"text": "For evaluation , we compare our system output with multiple human references.", "labels": [], "entities": []}, {"text": "Our results show that the new features we introduced in the first compression step improve performance upon the previous work on the same data set, and reranking is able to yield additional gain, especially when training is performed to take into account multiple references.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression aims to preserve the most important information in the original sentence with fewer words.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9396902918815613}]}, {"text": "It can be used for abstractive summarization where extracted important sentences often need to be compressed and merged.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.7909489274024963}]}, {"text": "For summarization of spontaneous speech, sentence compression is especially important, since unlike fluent and wellstructured written text, spontaneous speech contains a lot of disfluencies and much redundancy.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9882822632789612}, {"text": "sentence compression", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7436445206403732}]}, {"text": "The following shows an example of a pair of source and compressed spoken sentences 1 from human annotation (removed words shown in bold): [original sentence] For speech domains, \"sentences\" are not clearly defined.", "labels": [], "entities": []}, {"text": "We use sentences and utterances interchangeably when there is no ambiguity. and then um in terms of the source the things uh the only things that we had on there I believe were whether...", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform a cross-validation evaluation where one meeting is used for testing and the rest of them are used as the training set.", "labels": [], "entities": []}, {"text": "When evaluating the system performance, we do not consider filled pauses and incomplete words since they can be easily identified and removed.", "labels": [], "entities": []}, {"text": "We use two different performance metrics in this study.", "labels": [], "entities": []}, {"text": "\u2022 Word-level accuracy and F1 score based on the minor class (removed words).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9575337171554565}, {"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9832205474376678}]}, {"text": "This was used in (Liu and Liu, 2010).", "labels": [], "entities": []}, {"text": "These measures are obtained by comparing with the best compression.", "labels": [], "entities": []}, {"text": "In evaluation we map the result using 'BIO' labels from the first-step compression to binary labels that indicate a word is removed or not.", "labels": [], "entities": []}, {"text": "BLEU is a widely used metric in evaluating machine translation systems that often use multiple references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9832503199577332}, {"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7384470999240875}]}, {"text": "Since there is a great variation inhuman compression results, and we have 8 reference compressions, we explore using BLEU for our sentence compression task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9973266124725342}, {"text": "sentence compression", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7846441864967346}]}, {"text": "BLEU is calculated based on the precision of n-grams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9880650639533997}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9983862638473511}]}, {"text": "In our experiments we use up to 4-grams.", "labels": [], "entities": []}, {"text": "shows the averaged scores of the cross validation evaluation using the above metrics for several methods.", "labels": [], "entities": []}, {"text": "Also shown in the table is the compression ratio of the system output.", "labels": [], "entities": [{"text": "compression ratio", "start_pos": 31, "end_pos": 48, "type": "METRIC", "confidence": 0.9837042689323425}]}, {"text": "For \"reference\", we randomly choose one compression from 8 references, and use the rest of them as references in calculating the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9778063595294952}]}, {"text": "The row \"basic features\" shows the result of using all features in (Liu and Liu, 2010) except discourse parsing tree based features, and using binary labels (removed or not).", "labels": [], "entities": [{"text": "discourse parsing tree", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7842316528161367}]}, {"text": "The next row uses this same basic feature set and \"BIO\" labels.", "labels": [], "entities": [{"text": "BIO", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9799485802650452}]}, {"text": "Row \"expanded features\" shows the result of our expanded feature set using \"BIO\" label set from the first step of compression.", "labels": [], "entities": [{"text": "BIO", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9110832810401917}]}, {"text": "The last two rows show the results after reranking, trained using one best reference or 8 reference compressions, respectively.", "labels": [], "entities": []}, {"text": "Our result using the basic feature set is similar to that in (Liu and Liu, 2010) (their accuracy is 76.27% when compression ratio is 0.7), though the experimental setups are different: they used 6 meetings as the test set while we performed cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9995342493057251}]}, {"text": "Using the \"BIO\" label set instead of binary labels has marginal improvement for the three scores.", "labels": [], "entities": [{"text": "BIO", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9322270750999451}]}, {"text": "From the table, we can see that our expanded feature set is able to significantly improve the result, suggesting the effectiveness of the new introduced features.", "labels": [], "entities": []}, {"text": "Regarding the two training settings in reranking, we find that there is no gain from reranking when using only one best compression, however, training with multiple references improves BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9993877410888672}]}, {"text": "This indicates the discriminative training used in maximum entropy reranking is consistent with the performance metrics.", "labels": [], "entities": []}, {"text": "Another reason for the performance gain for this condition is that there is less data imbalance in model training (since we split the n-best list, each containing fewer negative examples).", "labels": [], "entities": []}, {"text": "We also notice that the compression ratio after reranking is more similar to the reference.", "labels": [], "entities": [{"text": "compression ratio", "start_pos": 24, "end_pos": 41, "type": "METRIC", "confidence": 0.9693814218044281}]}, {"text": "As suggested in), it is not appropriate to compare compression systems with different compression ratios, especially when considering grammars and meanings.", "labels": [], "entities": []}, {"text": "Therefore for the compression system without reranking, we generated results with the same compression ratio (77.15%), and found that using reranking still outperforms this result, 1.19% higher in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 197, "end_pos": 207, "type": "METRIC", "confidence": 0.97429358959198}]}, {"text": "For an analysis, we check how often our system output contains reference compressions based on the 8 references.", "labels": [], "entities": []}, {"text": "We found that 50.8% of system generated compressions appear in the 8 references when using CRF output with a compression ration of 77.15%; and after reranking this number increases to 54.8%.", "labels": [], "entities": []}, {"text": "This is still far from the oracle result -for 84.7% of sentences, the 25-best list contains one or more reference sentences, that is, there is still much room for improvement in the reranking process.", "labels": [], "entities": []}, {"text": "The results above also show that the token level measures by comparing to one best reference do not always correlate well with BLEU scores obtained by comparing with multiple references, which shows the need of considering multiple metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9989439845085144}]}], "tableCaptions": [{"text": " Table 1: Compression results using different systems.", "labels": [], "entities": []}]}