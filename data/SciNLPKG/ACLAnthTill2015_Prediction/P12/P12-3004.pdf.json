{"title": [{"text": "NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation", "labels": [], "entities": [{"text": "Syntax-based Machine Translation", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.6159786184628805}]}], "abstractContent": [{"text": "We present anew open source toolkit for phrase-based and syntax-based machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7308226972818375}]}, {"text": "The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntax-based models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6368979612986246}]}, {"text": "The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding.", "labels": [], "entities": []}, {"text": "Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning.", "labels": [], "entities": [{"text": "weight tuning", "start_pos": 206, "end_pos": 219, "type": "TASK", "confidence": 0.7570642232894897}]}], "introductionContent": [{"text": "We present NiuTrans, anew open source machine translation toolkit, which was developed for constructing high quality machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.715003564953804}, {"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7263188809156418}]}, {"text": "The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (, the hierarchical phrase-based model, and various syntax-based models ().", "labels": [], "entities": [{"text": "statistical machine translation (SMT) paradigms", "start_pos": 35, "end_pos": 82, "type": "TASK", "confidence": 0.8051737334047046}]}, {"text": "In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms.", "labels": [], "entities": []}, {"text": "Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system.", "labels": [], "entities": []}, {"text": "In addition, the toolkit provides easy-to-use APIs for the development of new features.", "labels": [], "entities": []}, {"text": "The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the NTCIR-9 Chinese-to-English PatentMT task).", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.8941437304019928}, {"text": "NTCIR-9 Chinese-to-English PatentMT task", "start_pos": 115, "end_pos": 155, "type": "DATASET", "confidence": 0.7185070365667343}]}, {"text": "We implemented the toolkit in C++ language, with special consideration of extensibility and efficiency.", "labels": [], "entities": []}, {"text": "C++ enables us to develop efficient translation engines which have high running speed for both training and decoding stages.", "labels": [], "entities": []}, {"text": "This property is especially important when the programs are used for large scale translation.", "labels": [], "entities": []}, {"text": "While the development of C++ program is slower than that of the similar programs written in other popular languages such as Java, the modern compliers generally result in C++ programs being consistently faster than the Java-based counterparts.", "labels": [], "entities": []}, {"text": "The toolkit is available under the GNU general public license 1 . The website of NiuTrans is http://www.nlplab.com/NiuPlan/NiuTrans.html.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our systems on NIST ChineseEnglish MT tasks.", "labels": [], "entities": [{"text": "NIST ChineseEnglish MT tasks", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.7480812966823578}]}, {"text": "Our training corpus consists of 1.9M bilingual sentences.", "labels": [], "entities": []}, {"text": "We used GIZA++ and the \"grow-diag-final-and\" heuristics to generate word alignment for the bilingual data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7100862860679626}]}, {"text": "The parse trees on both the Chinese and English sides were: BLEU scores of various systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.99932861328125}]}, {"text": "t2s, t2t, and s2t represent the tree-to-string, tree-to-tree, and string-to-tree systems, respectively.", "labels": [], "entities": []}, {"text": "generated using the Berkeley Parser, which were then binarized in a head-out fashion . A 5-gram language model was trained on the Xinhua portion of the Gigaword corpus in addition to the English part of the LDC bilingual training data.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 152, "end_pos": 167, "type": "DATASET", "confidence": 0.8270976543426514}, {"text": "LDC bilingual training data", "start_pos": 207, "end_pos": 234, "type": "DATASET", "confidence": 0.8118058890104294}]}, {"text": "We used the NIST 2003 MT evaluation set as our development set (919 sentences) and the NIST 2005 MT evaluation set as our test set (1,082 sentences).", "labels": [], "entities": [{"text": "NIST 2003 MT evaluation set", "start_pos": 12, "end_pos": 39, "type": "DATASET", "confidence": 0.9279969811439515}, {"text": "NIST 2005 MT evaluation set", "start_pos": 87, "end_pos": 114, "type": "DATASET", "confidence": 0.9491890907287598}]}, {"text": "The translation quality was evaluated with the case-insensitive IBM-version BLEU4.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9671735763549805}, {"text": "IBM-version", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.6596662998199463}, {"text": "BLEU4", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.5951215028762817}]}, {"text": "For the phrase-based system, phrases are of at most 7 words on either source or target-side.", "labels": [], "entities": []}, {"text": "For the hierarchical phrase-based system, all SCFG rules have at most two variables.", "labels": [], "entities": []}, {"text": "For the syntaxbased systems, minimal rules were extracted from the binarized trees on both (either) languageside(s).", "labels": [], "entities": []}, {"text": "Larger rules were then generated by composing two or three minimal rules.", "labels": [], "entities": []}, {"text": "By default, all these systems used abeam of size 30 for decoding.", "labels": [], "entities": []}, {"text": "shows the BLEU scores of different MT systems built using our toolkit.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984209537506104}, {"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9852568507194519}]}, {"text": "For comparison, the result of the Moses system is also reported.", "labels": [], "entities": []}, {"text": "We see, first of all, that our phrase-based and hierarchical phrase-based systems achieve competitive performance, even outperforms the Moses system over 0.3 BLEU points in some cases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.998630166053772}]}, {"text": "Also, the syntax-based systems obtain very promising results.", "labels": [], "entities": []}, {"text": "For example, the string-to-tree system significantly outperforms the phrase-based and hierarchical phrase-based counterparts.", "labels": [], "entities": []}, {"text": "In addition, gives a test of different decoding methods (for syntax-based systems).", "labels": [], "entities": []}, {"text": "We see that the parsing-based method achieves the best BLEU score.", "labels": [], "entities": [{"text": "parsing-based", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9748805165290833}, {"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.977687269449234}]}, {"text": "On the other hand, as expected, it runs slowest due to its large search space.", "labels": [], "entities": []}, {"text": "For example, it is 5-8 times slower than the tree-parsing-based method in our experiments.", "labels": [], "entities": []}, {"text": "The forest-based decoding further improves the BLEU scores on top of tree-parsing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9988735318183899}]}, {"text": "In most cases, it obtains a +0.6 BLEU improvement but is 2-3 times slower than the tree-parsing-based method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9992625117301941}]}], "tableCaptions": [{"text": " Table 1: BLEU scores of various systems. t2s, t2t,  and s2t represent the tree-to-string, tree-to-tree, and  string-to-tree systems, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9972591400146484}]}, {"text": " Table 2: Effects of pruning and multithreading  techniques.", "labels": [], "entities": []}]}