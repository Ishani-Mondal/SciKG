{"title": [{"text": "Machine Translation without Words through Substring Alignment", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7294637858867645}, {"text": "Substring Alignment", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7247718870639801}]}], "abstractContent": [{"text": "In this paper, we demonstrate that accurate machine translation is possible without the concept of \"words,\" treating MT as a problem of transformation between character strings.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.6836284846067429}, {"text": "MT", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9879199266433716}]}, {"text": "We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework.", "labels": [], "entities": [{"text": "phrasal inversion transduction grammar alignment", "start_pos": 35, "end_pos": 83, "type": "TASK", "confidence": 0.6872993469238281}, {"text": "MT", "start_pos": 197, "end_pos": 199, "type": "TASK", "confidence": 0.8053598403930664}]}, {"text": "We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment.", "labels": [], "entities": []}, {"text": "In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.", "labels": [], "entities": [{"text": "character-based translation", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.7137277126312256}]}], "introductionContent": [{"text": "Traditionally, the task of statistical machine translation (SMT) is defined as translating a source sentence f J 1 = {f 1 , . .", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.8356160918871561}]}, {"text": ", f J } to a target sentence e I 1 = {e 1 , . .", "labels": [], "entities": []}, {"text": ", e I }, where each element off J 1 and e I 1 is assumed to be a word in the source and target languages.", "labels": [], "entities": []}, {"text": "However, the definition of a \"word\" is often problematic.", "labels": [], "entities": []}, {"text": "The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy ( . Even for languages with explicit word The first author is now affiliated with the Nara Institute of Science and Technology.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.953969419002533}, {"text": "Nara Institute of Science and Technology", "start_pos": 307, "end_pos": 347, "type": "DATASET", "confidence": 0.9434065520763397}]}, {"text": "boundaries, all machine translation systems perform at least some precursory form of tokenization, splitting punctuation and words to prevent the sparsity that would occur if punctuated and non-punctuated words were treated as different entities.", "labels": [], "entities": []}, {"text": "Sparsity also manifests itself in other forms, including the large vocabularies produced by morphological productivity, word compounding, numbers, and proper names.", "labels": [], "entities": [{"text": "word compounding", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7491945326328278}]}, {"text": "A myriad of methods have been proposed to handle each of these phenomena individually, including morphological analysis, stemming, compound breaking, number regularization, optimizing word segmentation, and transliteration, which we outline in more detail in Section 2.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.7629128992557526}, {"text": "compound breaking", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7180329710245132}, {"text": "number regularization", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.7084058374166489}, {"text": "word segmentation", "start_pos": 184, "end_pos": 201, "type": "TASK", "confidence": 0.6925752311944962}]}, {"text": "These difficulties occur because we are translating sequences of words as our basic unit.", "labels": [], "entities": []}, {"text": "On the other hand, examine the possibility of instead treating each sentence as sequences of characters to be translated.", "labels": [], "entities": []}, {"text": "This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (, Swedish-Norwegian (, and ThaiLao (, which have a strong co-occurrence between single characters.", "labels": [], "entities": []}, {"text": "As state and we confirm, accurate translations cannot be achieved when applying traditional translation techniques to character-based translation for less similar language pairs.", "labels": [], "entities": []}, {"text": "In this paper, we propose improvements to the alignment process tailored to character-based machine translation, and demonstrate that it is, in fact, possible to achieve translation accuracies that ap-proach those of traditional word-based systems using only character strings.", "labels": [], "entities": [{"text": "character-based machine translation", "start_pos": 76, "end_pos": 111, "type": "TASK", "confidence": 0.6684494217236837}]}, {"text": "We draw upon recent advances in many-to-many alignment, which allows for the automatic choice of the length of units to be aligned.", "labels": [], "entities": []}, {"text": "As these units maybe at the character, subword, word, or multi-word phrase level, we conjecture that this will allow for better character alignments than one-to-many alignment techniques, and will allow for better translation of uncommon words than traditional word-based models by breaking down words into their component parts.", "labels": [], "entities": [{"text": "character alignments", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7208026945590973}]}, {"text": "We also propose two improvements to the manyto-many alignment method of . One barrier to applying many-to-many alignment models to character strings is training cost.", "labels": [], "entities": []}, {"text": "In the inversion transduction grammar (ITG) framework, which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG)", "start_pos": 7, "end_pos": 43, "type": "TASK", "confidence": 0.822383056084315}]}, {"text": "As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of by augmenting it with look-ahead probabilities in the spirit of A* search.", "labels": [], "entities": []}, {"text": "Secondly, we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7711576223373413}]}, {"text": "We do this by defining prior probabilities based on these substring counts within the Bayesian phrasal ITG framework.", "labels": [], "entities": []}, {"text": "An evaluation on four language pairs with differing morphological properties shows that for distant language pairs, character-based SMT can achieve translation accuracy comparable to word-based systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.8391114473342896}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.8750818371772766}]}, {"text": "In addition, we perform ablation studies, showing that these results were not possible without the proposed enhancements to the model.", "labels": [], "entities": [{"text": "ablation", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9576199650764465}]}, {"text": "Finally, we perform a qualitative analysis, which finds that character-based translation can handle unsegmented text, conjugation, and proper names in a unified framework with no additional processing.", "labels": [], "entities": [{"text": "character-based translation", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.7101736068725586}]}], "datasetContent": [{"text": "In order to test the effectiveness of character-based translation, we performed experiments over a variety of language pairs and experimental settings.", "labels": [], "entities": [{"text": "character-based translation", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6900586187839508}]}, {"text": "We use a combination of four languages with English, using freely available data.", "labels": [], "entities": []}, {"text": "We selected French-English, German-English, Finnish-English data from EuroParl (, with development and test sets designated for the 2005 ACL shared task on machine translation.", "labels": [], "entities": [{"text": "EuroParl", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9386767745018005}, {"text": "machine translation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.6761415153741837}]}, {"text": "We also did experiments with Japanese-English Wikipedia articles from the Kyoto Free Translation Task (Neubig, 2011) using the designated training and tuning sets, and reporting results on the test set.", "labels": [], "entities": [{"text": "Kyoto Free Translation Task (Neubig, 2011)", "start_pos": 74, "end_pos": 116, "type": "DATASET", "confidence": 0.8871525194909837}]}, {"text": "These languages were chosen as they have a variety of interesting characteristics.", "labels": [], "entities": []}, {"text": "French has some inflection, but among the test languages has the strongest oneto-one correspondence with English, and is generally considered easy to translate.", "labels": [], "entities": []}, {"text": "German has many compound words, which must be broken apart to translate properly into English.", "labels": [], "entities": []}, {"text": "Finnish is an agglutinative language with extremely rich morphology, resulting in long words and the largest vocabulary of the languages in EuroParl.", "labels": [], "entities": [{"text": "EuroParl", "start_pos": 140, "end_pos": 148, "type": "DATASET", "confidence": 0.980000376701355}]}, {"text": "Japanese does not have any clear word boundaries, and uses logographic characters, which contain more information than phonetic characters.", "labels": [], "entities": []}, {"text": "With regards to data preparation, the EuroParl data was pre-tokenized, so we simply used the tokenized data as-is for the training and evaluation of all models.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.703122466802597}, {"text": "EuroParl data", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.994592934846878}]}, {"text": "For word-based translation in the Kyoto task, training was performed using the provided tokenization scripts.", "labels": [], "entities": [{"text": "word-based translation", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7271791994571686}]}, {"text": "For character-based translation, no tokenization was performed, using the original text for both training and decoding.", "labels": [], "entities": [{"text": "character-based translation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8046879172325134}]}, {"text": "For both tasks, we selected as training data all sentences for which both: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05).", "labels": [], "entities": [{"text": "Translation", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9610373973846436}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.903326153755188}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.8936103582382202}, {"text": "METEOR", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9972013235092163}, {"text": "word and character-based translation", "start_pos": 190, "end_pos": 226, "type": "TASK", "confidence": 0.6318134665489197}]}, {"text": "source and target were 100 characters or less, 6 the total size of which is shown in.", "labels": [], "entities": []}, {"text": "In characterbased translation, white spaces between words were treated as any other character and not given any special treatment.", "labels": [], "entities": [{"text": "characterbased translation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6773870885372162}]}, {"text": "Evaluation was performed on tokenized and lower-cased data.", "labels": [], "entities": []}, {"text": "For alignment, we use the GIZA++ implementation of one-to-many alignment and the pialign implementation of the phrasal ITG models 8 modified with the proposed improvements.", "labels": [], "entities": [{"text": "alignment", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9873930811882019}]}, {"text": "For GIZA++, we used the default settings for word-based alignment, but used the HMM model for character-based alignment to allow for alignment of longer sentences.", "labels": [], "entities": [{"text": "word-based alignment", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7503572702407837}, {"text": "character-based alignment", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.7165456414222717}]}, {"text": "For pialign, default settings were used except for character-based ITG alignment, which used a probability beam of 10 \u22124 instead 10 \u221210 . 9 For decoding, we use the Moses decoder, 10 using the default settings except for the stack size, which we set to 1000 instead of 200.", "labels": [], "entities": [{"text": "character-based ITG alignment", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.6629285713036855}]}, {"text": "Minimum error rate training was performed to maximize word-based BLEU score for all systems.", "labels": [], "entities": [{"text": "Minimum error rate", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.7541155219078064}, {"text": "BLEU score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9624282121658325}]}, {"text": "11 For language models, word-based translation uses a word 5-gram model, and characterbased translation uses a character 12-gram model, both smoothed using interpolated Kneser-Ney.", "labels": [], "entities": [{"text": "word-based translation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7249794900417328}]}, {"text": "100 characters is an average of 18.8 English words 7 http://code.google.com/p/giza-pp/ 8 http://phontron.com/pialign/ 9 Improvement by using abeam larger than 10 \u22124 was marginal, especially with co-occurrence prior probabilities.", "labels": [], "entities": []}, {"text": "10 http://statmt.org/moses/ 11 We chose this set-up to minimize the effect of tuning criterion on our experiments, although it does indicate that we must have access to tokenized data for the development set.", "labels": [], "entities": []}, {"text": "presents a quantitative analysis of the translation results for each of the proposed methods.", "labels": [], "entities": []}, {"text": "As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (, we perform experiments translating in both directions for all language pairs.", "labels": [], "entities": []}, {"text": "We evaluate translation quality using BLEU score (), both on the word and character level (with n = 4), as well as METEOR () on the word level.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9908532202243805}, {"text": "METEOR", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9987416863441467}]}, {"text": "It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments.", "labels": [], "entities": [{"text": "character-based translation", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7634547352790833}, {"text": "character-based translation", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.691355973482132}]}, {"text": "When compared with word-based translation, character-based translation achieves better, comparable, or inferior results on character-based BLEU, comparable or inferior results on METEOR, and inferior results on word-based BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9251665472984314}, {"text": "METEOR", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.8271622657775879}]}, {"text": "The differences between the evaluation metrics are due to the fact that character-based translation often gets words mostly correct other than one or two letters.", "labels": [], "entities": [{"text": "character-based translation", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.697367250919342}]}, {"text": "These are given partial credit by character-based BLEU (and to a lesser extent METEOR), but marked entirely wrong by word-based BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9807006120681763}, {"text": "METEOR", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.8960486054420471}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9468752145767212}]}, {"text": "Interestingly, for translation into English, character-based translation achieves higher accuracy compared to word-based translation on Japanese and Finnish input, followed by German, fi-en ja-en ITG-word 2.851 2.085 ITG-char 2.826 2.154  and finally French.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9629545211791992}, {"text": "character-based translation", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6321475058794022}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.998636782169342}]}, {"text": "This confirms that characterbased translation is performing well on languages that have long words or ambiguous boundaries, and less well on language pairs with relatively strong one-to-one correspondence between words.", "labels": [], "entities": [{"text": "characterbased translation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.7281850278377533}]}, {"text": "In addition, we performed a subjective evaluation of Japanese-English and Finnish-English translations.", "labels": [], "entities": []}, {"text": "Two raters evaluated 100 sentences each, assigning a score of 0-5 based on how well the translation conveys the information contained in the reference.", "labels": [], "entities": []}, {"text": "We focus on shorter sentences of 8-16 English words to ease rating and interpretation.", "labels": [], "entities": []}, {"text": "shows that the results are comparable, with no significant difference in average scores for either language pair.", "labels": [], "entities": []}, {"text": "shows a breakdown of the sentences for which character-based translation received a score of at 2+ points more than word-based.", "labels": [], "entities": []}, {"text": "It can be seen that character-based translation is properly handling sparsity phenomena.", "labels": [], "entities": [{"text": "character-based translation", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7188034355640411}]}, {"text": "On the other hand, word-based translation was generally stronger with reordering and lexical choice of more common words.", "labels": [], "entities": [{"text": "word-based translation", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7316627502441406}]}], "tableCaptions": [{"text": " Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal  ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ- ence from the best system according to the bootstrap resampling method at p = 0.05", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9746374487876892}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9100381731987}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9105203151702881}, {"text": "METEOR", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9963220357894897}, {"text": "word and character-based translation", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.6080463230609894}]}, {"text": " Table 5: METEOR scores for alignment with and without  look-ahead and co-occurrence priors.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9892818927764893}]}]}