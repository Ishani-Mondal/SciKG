{"title": [{"text": "Learning the Latent Semantics of a Concept from its Definition", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we study unsupervised word sense disambiguation (WSD) based on sense definition.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7765534122784933}]}, {"text": "We learn low-dimensional latent semantic vectors of concept definitions to construct a more robust sense similarity measure wmfvec.", "labels": [], "entities": []}, {"text": "Experiments on four all-words WSD data sets show significant improvement over the baseline WSD systems and LDA based similarity measures, achieving results comparable to state of the art WSD systems.", "labels": [], "entities": [{"text": "WSD data sets", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9144607583681742}]}], "introductionContent": [{"text": "To date, many unsupervised WSD systems rely on a sense similarity module that returns a similarity score given two senses.", "labels": [], "entities": [{"text": "WSD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9674553275108337}]}, {"text": "Many similarity measures use the taxonomy structure of WordNet (, which allows only noun-noun and verb-verb pair similarity computation since the other parts of speech (adjectives and adverbs) do not have a taxonomic representation structure.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9467447400093079}]}, {"text": "For example, the jcn similarity measure computes the sense pair similarity score based on the information content of three senses: the two senses and their least common subsumer in the noun/verb hierarchy.", "labels": [], "entities": [{"text": "sense pair similarity score", "start_pos": 53, "end_pos": 80, "type": "METRIC", "confidence": 0.6391338780522346}]}, {"text": "The most popular sense similarity measure is the Extended Lesk measure.", "labels": [], "entities": []}, {"text": "In elesk, the similarity score is computed based on the length of overlapping words/phrases between two extended dictionary definitions.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 14, "end_pos": 30, "type": "METRIC", "confidence": 0.9717828035354614}]}, {"text": "The definitions are extended by definitions of neighbor senses to discover more overlapping words.", "labels": [], "entities": []}, {"text": "However, exact word matching is lossy.", "labels": [], "entities": [{"text": "word matching", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.6832064688205719}]}, {"text": "Below are two definitions from WN: bank#n#1: a financial institution that accepts deposits and channels the money into lending activities stock#n#1: the capital raised by a corporation through the issue of shares entitling holders to an ownership interest (equity) Despite the high semantic relatedness of the two senses, the overlapping words in the two definitions are only a, the, leading to a very low similarity score.", "labels": [], "entities": [{"text": "WN", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.7375310063362122}, {"text": "similarity score", "start_pos": 406, "end_pos": 422, "type": "METRIC", "confidence": 0.9770538806915283}]}, {"text": "Accordingly we are interested in extracting latent semantics from sense definitions to improve elesk.", "labels": [], "entities": []}, {"text": "However, the challenge lies in that sense definitions are typically too short/sparse for latent variable models to learn accurate semantics, since these models are designed for long documents.", "labels": [], "entities": []}, {"text": "For example, topic models such as LDA (, can only find the dominant topic based on the observed words in a definition (f inancial topic in bank#n#1 and stock#n#1) without further discernibility.", "labels": [], "entities": []}, {"text": "In this case, many senses will share the same latent semantics profile, as long as they are in the same topic/domain.", "labels": [], "entities": []}, {"text": "To solve the sparsity issue we use missing words as negative evidence of latent semantics, as in.", "labels": [], "entities": []}, {"text": "We define missing words of a sense definition as the whole vocabulary in a corpus minus the observed words in the sense definition.", "labels": [], "entities": []}, {"text": "Since observed words in definitions are too few to reveal the semantics of senses, missing words can be used to tell the model what the definition is not about.", "labels": [], "entities": []}, {"text": "Therefore, we want to find a latent semantics profile that is related to observed words in a definition, but also not related to missing words, so that the induced latent semantics is unique for the sense.", "labels": [], "entities": []}, {"text": "Finally we also show how to use WN neighbor sense definitions to construct a nuanced sense similarity wmfvec, based on the inferred latent semantic vectors of senses.", "labels": [], "entities": []}, {"text": "We show that wmfvec outperforms elesk and LDA based approaches in four All-words WSD data sets.", "labels": [], "entities": [{"text": "WSD data sets", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.8399061759312948}]}, {"text": "To our best knowledge, wmfvec is the first sense similarity measure based on latent semantics of sense definitions.", "labels": [], "entities": []}, {"text": "2 Learning Latent Semantics of Definitions", "labels": [], "entities": []}], "datasetContent": [{"text": "Task: We choose the fine-grained All-Words Sense Disambiguation task, where systems are required to disambiguate all the content words (noun, adjective, adverb and verb) in documents.", "labels": [], "entities": [{"text": "All-Words Sense Disambiguation task", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.7126358970999718}]}, {"text": "The data sets we use are all-words tasks in SENSEVAL2, SENSE-VAL3 [SE3], SEMEVAL-2007, and Semcor.", "labels": [], "entities": []}, {"text": "We tune the parameters in wmfvec and other baselines based on SE2, and then directly apply the tuned models on other three data sets.", "labels": [], "entities": [{"text": "wmfvec", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.9304521083831787}, {"text": "SE2", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9113266468048096}]}, {"text": "Data: The sense inventory is WN3.0 for the four WSD data sets.", "labels": [], "entities": [{"text": "WN3.0", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.8988576531410217}, {"text": "WSD data sets", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8849008480707804}]}, {"text": "WMF and LDA are built on the corpus of sense definitions of two dictionaries: WN and Wiktionary.", "labels": [], "entities": [{"text": "WMF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8485805988311768}]}, {"text": "We do not link the senses across dictionaries, hence Wik is only used as augmented data for WMF to better learn the semantics of words.", "labels": [], "entities": []}, {"text": "All data is tokenized, POS tagged ( and lemmatized, resulting in 341,557 sense definitions and 3,563,649 words.", "labels": [], "entities": []}, {"text": "WSD Algorithm: To perform WSD we need two components: (1) a sense similarity measure that returns a similarity score given two senses; (2) a disambiguation algorithm that determines which senses to choose as final answers based on the sense pair similarity scores.", "labels": [], "entities": [{"text": "WSD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8879740238189697}]}, {"text": "We choose the Indegree algorithm used in as our disambiguation algorithm.", "labels": [], "entities": [{"text": "Indegree", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8757201433181763}]}, {"text": "It is a graphbased algorithm, where nodes are senses, and edge weight equals to the sense pair similarity.", "labels": [], "entities": []}, {"text": "The final answer is chosen as the sense with maximum indegree.", "labels": [], "entities": []}, {"text": "Using the Indegree algorithm allows us to easily replace the sense similarity with wmfvec.", "labels": [], "entities": []}, {"text": "In Indegree, two senses are connected if their words are within a local window.", "labels": [], "entities": []}, {"text": "We use the optimal window size of 6 tested in (.", "labels": [], "entities": []}, {"text": "Baselines: We compare with (1) elesk, the most widely used sense similarity.", "labels": [], "entities": []}, {"text": "We use the implementation in).", "labels": [], "entities": []}, {"text": "We believe WMF is a better approach to model latent semantics than LDA, hence the second baseline (2) LDA using Gibbs sampling (  tor of a definition by summing up the P (z|w) of all constituent words weighted by X ij , which gives much better WSD results.", "labels": [], "entities": []}, {"text": "We produce LDA vectors in the same setting as wmfvec, which means it is trained on the same corpus, uses WN neighbors, and is tuned on SE2.", "labels": [], "entities": []}, {"text": "At last, we compare wmfvec with a mature WSD system based on sense similarities, (3) ( [jcn+elesk], where they evaluate six sense similarities, select the best of them and combine them into one system.", "labels": [], "entities": []}, {"text": "Specifically, in their implementation they use jcn for noun-noun and verbverb pairs, and elesk for other pairs.", "labels": [], "entities": []}, {"text": "() used to be the state-of-the-art system on SE2 and SE3.", "labels": [], "entities": [{"text": "SE2", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9456515312194824}, {"text": "SE3", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.49833065271377563}]}, {"text": "The disambiguation results (K = 100) are summarized in.", "labels": [], "entities": []}, {"text": "We also present in results using other values of dimensions K for wmfvec and ldavec.", "labels": [], "entities": []}, {"text": "There are very few words that are not covered due to failure of lemmatization or POS tag mismatches, thereby F-measure is reported.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9967848062515259}]}, {"text": "Based on SE2, wmfvec's parameters are tuned as \u03bb = 20, w m = 0.01; ldavec's parameters are tuned as \u03b1 = 0.05, \u03b2 = 0.05.", "labels": [], "entities": [{"text": "SE2", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8357601165771484}]}, {"text": "We run WMF on WN+Wik for 30 iterations, and LDA for 2000 iterations.", "labels": [], "entities": [{"text": "WMF", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.7819408774375916}, {"text": "WN+Wik", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.8890983462333679}]}, {"text": "For LDA, more robust P (w|z) is generated by averaging over the last 10 sampling iterations.", "labels": [], "entities": []}, {"text": "We also set a threshold to elesk similarity values, which yields better performance.", "labels": [], "entities": [{"text": "similarity", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9017112255096436}]}, {"text": "Same as), values of elesk larger than 240 are set to 1, and the rest are mapped to. elesk vs wmfvec: wmfvec outperforms elesk consistently in all POS cases (noun, adjective, adverb and verb) on four datasets by a large margin (2.9% \u2212 4.5% in total case).", "labels": [], "entities": []}, {"text": "Observing the results yielded per POS, we find a large improvement comes from nouns.", "labels": [], "entities": []}, {"text": "Same trend has been reported in other distributional methods based on word co-occurrence).", "labels": [], "entities": []}, {"text": "More interestingly, wmfvec also improves verbs accuracy significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9919146299362183}]}, {"text": "ldavec vs wmfvec: ldavec also performs very well, again proving the superiority of latent semantics over surface words matching.", "labels": [], "entities": []}, {"text": "However, wmfvec also outperforms ldavec in every POS case except Semcor adverbs (at least +1% in total case).", "labels": [], "entities": []}, {"text": "We observe the trend is consistent in where different dimensions are used for ldavec and wmfvec.", "labels": [], "entities": []}, {"text": "These results show that given the same text data, WMF outperforms LDA on modeling latent semantics of senses by exploiting missing words.", "labels": [], "entities": [{"text": "WMF", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.5047053694725037}]}, {"text": "jcn+elesk vs jcn+wmfvec: jcn+elesk is a very mature WSD system that takes advantage of the great performance of jcn on noun-noun and verb-verb pairs.", "labels": [], "entities": [{"text": "WSD", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9073191285133362}]}, {"text": "Although wmfvec does much better than elesk, using wmfvec solely is sometimes outperformed by jcn+elesk on nouns and verbs.", "labels": [], "entities": []}, {"text": "Therefore to beat jcn+elesk, we replace the elesk in jcn+elesk with wmfvec (hence jcn+wmfvec).", "labels": [], "entities": []}, {"text": "Similar to, we normalize wmfvec similarity such that values greater than 400 are set to 1, and the rest values are mapped to.", "labels": [], "entities": []}, {"text": "We choose the value 400 based on the WSD performance on tuning set SE2.", "labels": [], "entities": [{"text": "WSD", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.5204779505729675}]}, {"text": "As expected, the resulting jcn+wmfvec can further improve jcn+elesk for all cases.", "labels": [], "entities": []}, {"text": "Moreover, jcn+wmfvec produces similar results to stateof-the-art unsupervised systems on SE02, 61.92% F-mearure in) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7.", "labels": [], "entities": [{"text": "SE02", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9251724481582642}, {"text": "F-mearure", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9567556977272034}, {"text": "WN1.7.1", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9690434336662292}, {"text": "SE03", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.7531308531761169}, {"text": "WN1.7", "start_pos": 181, "end_pos": 186, "type": "DATASET", "confidence": 0.9738686084747314}]}, {"text": "It shows wmfvec is robust that it not only performs very well individually, but also can be easily incorporated with existing evidence as represented using jcn.", "labels": [], "entities": []}, {"text": "We look closely into WSD results to obtain an intuitive feel for what is captured by wmfvec.", "labels": [], "entities": [{"text": "WSD", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8970348834991455}]}, {"text": "For example, the target word mouse in the context: ...", "labels": [], "entities": []}, {"text": "in experiments with mice that a gene called p53 could transform normal cells into cancerous ones...", "labels": [], "entities": []}, {"text": "elesk returns the wrong sense computer device, due to the sparsity of overlapping words between definitions of animal mouse and the context words.", "labels": [], "entities": []}, {"text": "wmfvec chooses the correct sense animal mouse, by recognizing the biology element of animal mouse and related context words gene, cell, cancerous.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Three possible hypotheses of latent vectors for", "labels": [], "entities": []}, {"text": " Table 2: WSD results per POS (K = 100)", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5992568135261536}, {"text": "POS", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9526124596595764}]}, {"text": " Table 2. We also present in", "labels": [], "entities": []}, {"text": " Table 3: ldavec and wmfvec (latter) results per # of dimensions  4.1 Discussion", "labels": [], "entities": []}]}