{"title": [{"text": "Cross-lingual Parse Disambiguation based on Semantic Correspondence", "labels": [], "entities": [{"text": "Parse Disambiguation", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.8396615386009216}]}], "abstractContent": [{"text": "We present a system for cross-lingual parse disambiguation, exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different languages have different ambiguities.", "labels": [], "entities": [{"text": "cross-lingual parse disambiguation", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.8276679515838623}]}, {"text": "We simultaneously reduce ambiguity in multiple languages in a fully automatic way.", "labels": [], "entities": []}, {"text": "Evaluation shows that the system reliably discards dispreferred parses from the raw parser output, which results in a pre-selection that can speedup manual treebanking.", "labels": [], "entities": []}], "introductionContent": [{"text": "Treebanks, sets of parsed sentences annotated with a sytactic structure, are an important resource in NLP.", "labels": [], "entities": []}, {"text": "The manual construction of treebanks, where a human annotator selects a gold parse from all parses returned by a parser, is a tedious and error prone process.", "labels": [], "entities": []}, {"text": "We present a system for simultaneous and accurate partial parse disambiguation of multiple languages.", "labels": [], "entities": [{"text": "partial parse disambiguation", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6956304709116617}]}, {"text": "Using the pre-selected set of parses returned by the system, the treebanking process for multiple languages can be sped up.", "labels": [], "entities": []}, {"text": "The system operates on an aligned parallel corpus.", "labels": [], "entities": []}, {"text": "The languages of the parallel corpus are considered as mutual semantic tags: As the meaning of a sentence stays constant during translation, we are able to resolve ambiguities which exist in only one of the langauges by only accepting those interpretations which are licensed by the other language.", "labels": [], "entities": []}, {"text": "In particular, we select one language as the target language, translate the other language's semantics for every parse into the target language and thus align maximally similar semantic representations.", "labels": [], "entities": []}, {"text": "The parses with the most overlapping semantics are selected as preferred parses.", "labels": [], "entities": []}, {"text": "As an example consider the English sentence They closed the shop at five, which has the following two interpretations due to PP attachment ambiguity: (1) \"At five, they closed the shop\" close(they, shop); at(close, 5) (2) \"The shop at five was closed by them\" close(they, shop); at(shop, 5) The Japanese translation is also ambiguous, but in a completely different way: it has the possibility of a zero pronoun (we show the translated semantics).", "labels": [], "entities": []}, {"text": "We show the semantic representation of the ambiguity with each sentence.", "labels": [], "entities": []}, {"text": "Both languages are disambiguated by the other language as only the English interpretation (1) is supported in Japanese, and only the Japanese interpretation (3) leads to a grammatical English sentence.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on the task of parse disambiguation.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.9685853719711304}]}, {"text": "We use full sentence match as evaluation metric, a challenging target.", "labels": [], "entities": []}, {"text": "The Tanaka corpus is used for training and testing.", "labels": [], "entities": [{"text": "Tanaka corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9293826520442963}]}, {"text": "It is an open corpus of JapaneseEnglish sentence pairs.", "labels": [], "entities": []}, {"text": "We use version  which contains 147,190 sentence pairs.", "labels": [], "entities": []}, {"text": "We holdout 4,500 sentence pairs each for development and test.", "labels": [], "entities": []}, {"text": "For each sentence, we compare the number of theoretically possible alignments with the number of preferred alignments returned by our system.", "labels": [], "entities": []}, {"text": "On average, ambiguity is reduced down to 30%.", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9924782514572144}]}, {"text": "For English 3.76 and for Japanese 3.87 parses out of (at most) 11 analyses remain in the partially disambiguated list: both languages benefit equally from the disambiguation.", "labels": [], "entities": []}, {"text": "We evaluate disambiguation accuracy by counting the number of times the gold parse was present in the partially disambiguated set (full sentence match).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.956142783164978}]}, {"text": "shows the alignment accuracy results.", "labels": [], "entities": [{"text": "alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.8263358473777771}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9764580130577087}]}, {"text": "The correct parse is included in the reduced set in 80% of the cases for Japanese, and for 82% of the cases in English.", "labels": [], "entities": []}, {"text": "We match atomic relations when aligning the semantic structures, which is a very generic method applicable to the vast majority of sentence pairs.", "labels": [], "entities": []}, {"text": "This leads to a recall score of", "labels": [], "entities": [{"text": "recall score", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.9894542992115021}]}], "tableCaptions": [{"text": " Table 1: Accuracy and F-scores for disambiguation per- formance of our system. Recall was 99% in every case.  'Included': inclusion of the gold parse in the reduced set  of parses or not. 'First Rank': ranking of the preferred  parse as top in the reduced list. 'MRR': mean reciprocal  rank of the gold parse in the list.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994118213653564}, {"text": "F-scores", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9976709485054016}, {"text": "Recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9995946288108826}, {"text": "MRR'", "start_pos": 264, "end_pos": 268, "type": "METRIC", "confidence": 0.9569178819656372}]}]}