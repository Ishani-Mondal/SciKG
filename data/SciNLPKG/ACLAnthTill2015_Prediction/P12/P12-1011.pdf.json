{"title": [{"text": "Labeling Documents with Timestamps: Learning from their Time Expressions", "labels": [], "entities": [{"text": "Labeling Documents", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9136048853397369}]}], "abstractContent": [{"text": "Temporal reasoners for document understanding typically assume that a document's creation date is known.", "labels": [], "entities": [{"text": "Temporal reasoners", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9117736220359802}, {"text": "document understanding", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.6982011198997498}]}, {"text": "Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner.", "labels": [], "entities": []}, {"text": "Unfortunately, the timestamp is not always known, particularly on the Web.", "labels": [], "entities": []}, {"text": "This paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time.", "labels": [], "entities": []}, {"text": "The first is a discriminative classifier with new features extracted from the text's time expressions (e.g., 'since 1999').", "labels": [], "entities": []}, {"text": "This model alone improves on previous generative models by 77%.", "labels": [], "entities": []}, {"text": "The second model learns prob-abilistic constraints between time expressions and the unknown document time.", "labels": [], "entities": []}, {"text": "Imposing these learned constraints on the discriminative model further improves its accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9982819557189941}]}, {"text": "Finally, we present anew experiment design that facilitates easier comparison by future work.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses a relatively new task in the NLP community: automatic document dating.", "labels": [], "entities": [{"text": "automatic document dating", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6293733914693197}]}, {"text": "Given a document with unknown origins, what characteristics of its text indicate the year in which the document was written?", "labels": [], "entities": []}, {"text": "This paper proposes a learning approach that builds constraints from a document's use of time expressions, and combines them with anew discriminative classifier that greatly improves previous work.", "labels": [], "entities": []}, {"text": "The temporal reasoning community has long depended on document timestamps to ground relative time expressions and events).", "labels": [], "entities": []}, {"text": "For instance, consider the following passage from the TimeBank corpus (): And while there was no profit this year from discontinued operations, last year they contributed 34 million, before tax.", "labels": [], "entities": [{"text": "TimeBank corpus", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9783535897731781}]}, {"text": "Reconstructing the timeline of events from this document requires extensive temporal knowledge, most notably, the document's creation date to ground its relative expressions (e.g., this year = 2012).", "labels": [], "entities": []}, {"text": "Not only did the latest TempEval competitions) include tasks to link events to the (known) document creation time, but state-of-the-art event-event ordering algorithms also rely on these timestamps).", "labels": [], "entities": []}, {"text": "This knowledge is assumed to be available, but unfortunately this is not often the case, particularly on the Web.", "labels": [], "entities": []}, {"text": "Document timestamps are growing in importance to the information retrieval (IR) and management communities as well.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7974529504776001}]}, {"text": "Several IR applications depend on knowledge of when documents were posted, such as computing document relevance ( and labeling search queries with temporal profiles ().", "labels": [], "entities": [{"text": "IR", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9836580157279968}, {"text": "labeling search queries", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.877263347307841}]}, {"text": "Dating documents is similarly important to processing historical and heritage collections of text.", "labels": [], "entities": [{"text": "Dating documents", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9235749244689941}]}, {"text": "Some of the early work that motivates this paper arose from the goal of automatically grounding documents in their historical contexts (de).", "labels": [], "entities": []}, {"text": "This paper builds on their work by incorporating more linguistic knowledge and explicit reasoning into the learner.", "labels": [], "entities": []}, {"text": "The first part of this paper describes a novel learning approach to document dating, presenting a discriminative model and rich linguistic features that have not been applied to document dating.", "labels": [], "entities": [{"text": "document dating", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7915922701358795}, {"text": "document dating", "start_pos": 178, "end_pos": 193, "type": "TASK", "confidence": 0.7750864923000336}]}, {"text": "Further, we introduce new features specific to absolute time expressions.", "labels": [], "entities": []}, {"text": "Our model outperforms the generative models of previous work by 77%.", "labels": [], "entities": []}, {"text": "The second half of this paper describes a novel learning algorithm that orders time expressions against the unknown timestamp.", "labels": [], "entities": []}, {"text": "For instance, the phrase the second quarter of 1999 might be labeled as being before the timestamp.", "labels": [], "entities": []}, {"text": "These labels impose constraints on the possible timestamp and narrow down its range of valid dates.", "labels": [], "entities": []}, {"text": "We combine these constraints with our discriminative learner and see another relative improvement inaccuracy by 9%.", "labels": [], "entities": []}], "datasetContent": [{"text": "This paper uses the New York Times section of the Gigaword Corpus () for evaluation.", "labels": [], "entities": [{"text": "New York Times section of the Gigaword Corpus", "start_pos": 20, "end_pos": 65, "type": "DATASET", "confidence": 0.8691025078296661}]}, {"text": "Most previous work on document dating evaluates on the news genre, so we maintain the pattern for consistency.", "labels": [], "entities": [{"text": "document dating", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.758576899766922}, {"text": "consistency", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.9726807475090027}]}, {"text": "Unfortunately, we cannot compare to these previous experiments because of differing evaluation setups. is most similar in their use of Gigaword, but they chose a random set of documents that cannot be reproduced.", "labels": [], "entities": []}, {"text": "We instead define specific segments of the corpus for evaluation.", "labels": [], "entities": []}, {"text": "The main goal for this experiment setup was to establish specific training, development, and test sets.", "labels": [], "entities": []}, {"text": "One of the potential difficulties in testing with news articles is that the same story is often reprinted with very minimal (or no) changes.", "labels": [], "entities": []}, {"text": "Over 10% of the documents in the New York Times section of the Gigaword Corpus are exact or approximate duplicates of another document in the corpus 2 . A training set for document dating must not include duplicates from the test set.", "labels": [], "entities": [{"text": "New York Times section of the Gigaword Corpus", "start_pos": 33, "end_pos": 78, "type": "DATASET", "confidence": 0.8578819930553436}]}, {"text": "We adopt the intuition behind the experimental setup used in other NLP domains, like parsing, where the entire test set is from a contiguous section of the corpus (as opposed to randomly selected examples across the corpus).", "labels": [], "entities": [{"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9764516353607178}]}, {"text": "As the parsing community trains on sections 2-21 of the Penn Treebank () and tests on section 23, we create Gigaword sections by isolating specific months.", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9769357442855835}, {"text": "Penn Treebank", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9950597584247589}]}, {"text": "We experiment on the Gigaword corpus as described in Section 5.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.9536831676959991}]}, {"text": "Documents are tokenized and parsed with the Stanford Parser.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9510898292064667}]}, {"text": "The year in the timestamp is retrieved from the document's Gigaword ID which contains the year and day the article was retrieved.", "labels": [], "entities": [{"text": "Gigaword ID", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.8623582124710083}]}, {"text": "Year mentions are extracted from documents by matching all tokens with exactly four digits whose integer is in the range of 1900 and 2100.", "labels": [], "entities": []}, {"text": "The MaxEnt classifiers are also from the Stanford toolkit, and both the document and year mention classifiers use its default settings (quadratic prior).", "labels": [], "entities": []}, {"text": "The \u03bb factor in the joint classifier is optimized on the development set as described in Section 4.3.", "labels": [], "entities": []}, {"text": "We also found that dev results improved when training ignores the border months of Jan, Feb, and Dec. The features described in this paper were selected solely by studying performance on the development set.", "labels": [], "entities": []}, {"text": "The final reported results come from running on the test set once at the end of this study.", "labels": [], "entities": []}, {"text": "shows the results on the Test set for all document classifiers.", "labels": [], "entities": []}, {"text": "We measure accuracy to compare overall performance since the test set is a balanced set (each year has the same number of test documents).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9991432428359985}]}, {"text": "Unigram NLLR and Filtered NLLR are the language model implementations of previous work as described in Section 3.1.", "labels": [], "entities": [{"text": "Unigram NLLR", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7678202390670776}]}, {"text": "MaxEnt Unigram is our new discriminative model for this task.", "labels": [], "entities": [{"text": "MaxEnt Unigram", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8985871970653534}]}, {"text": "MaxEnt Time is the discriminative model with rich time features (but not NER) as described in Section 3.3.2 (Time+NER includes NER).", "labels": [], "entities": []}, {"text": "Finally, the Joint model is the combined document and year mention classifiers as described in Section 4.3.   7%, and adding NER by another 6%.", "labels": [], "entities": [{"text": "NER", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.5418875813484192}]}, {"text": "Total relative improvement inaccuracy is thus almost 77% from the Time+NER model over Filtered NLLR.", "labels": [], "entities": [{"text": "Filtered NLLR", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.7694463729858398}]}, {"text": "Further, the temporal constraint model increases this best classifier by another 3.9%.", "labels": [], "entities": []}, {"text": "All improvements are statistically significant (p < 0.000001, McNemar's test, 2-tailed).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.7519322832425436}]}, {"text": "shows that performance increased most on the documents that contain at least one year mention (60% of the corpus).", "labels": [], "entities": []}, {"text": "Finally, shows the results of the temporal constraint classifiers on year mentions.", "labels": [], "entities": []}, {"text": "Not surprisingly, the fine-grained performance is quite a bit lower than the core relations.", "labels": [], "entities": []}, {"text": "The full Joint results in use the three core relations, but the seven fine-grained relations give approximately the same results.", "labels": [], "entities": []}, {"text": "Its lower accuracy is mitigated by the finer granularity (i.e., the majority class basline is lower).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992978572845459}]}], "tableCaptions": [{"text": " Table 3: Performance as measured by accuracy. The pre- dicted year must exactly match the actual year.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995532631874084}]}, {"text": " Table 4: Yearly results for the Joint model. 2005/06 are  omitted due to space, with F1 .56 and .63, respectively.", "labels": [], "entities": [{"text": "Joint model. 2005/06", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.9091932674249014}, {"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9964099526405334}]}, {"text": " Table 5: Precision, recall, and F1 for the core relations.  Accuracy for both core and fine-grained.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9983426332473755}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9987047910690308}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9995717406272888}, {"text": "Accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9990787506103516}]}, {"text": " Table 6: Accuracy on all documents and documents with  at least one year mention (about 60% of the corpus).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987240433692932}]}]}