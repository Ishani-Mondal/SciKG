{"title": [{"text": "Using Search-Logs to Improve Query Tagging", "labels": [], "entities": [{"text": "Improve Query Tagging", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8348374168078104}]}], "abstractContent": [{"text": "Syntactic analysis of search queries is important fora variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult.", "labels": [], "entities": [{"text": "Syntactic analysis of search queries", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8535837173461914}]}, {"text": "We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time.", "labels": [], "entities": []}, {"text": "Unlike previous work, our final model does not require any additional resources at run-time.", "labels": [], "entities": []}, {"text": "Compared to a state-of-the-art approach, we achieve more than 20% relative error reduction.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 66, "end_pos": 90, "type": "METRIC", "confidence": 0.765825629234314}]}, {"text": "Additionally, we annotate a corpus of search queries with part-of-speech tags, providing a resource for future work on syntactic query analysis.", "labels": [], "entities": [{"text": "syntactic query analysis", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.7542285720507304}]}], "introductionContent": [{"text": "Syntactic analysis of search queries is important fora variety of tasks including better query refinement, improved matching and better ad targeting (.", "labels": [], "entities": []}, {"text": "However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora (.", "labels": [], "entities": []}, {"text": "In this paper we focus on part-of-speech (POS) tagging queries entered into commercial search engines and compare different strategies for learning from search logs.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging queries", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.7020555833975474}]}, {"text": "The search logs consist of user queries and relevant search results retrieved by a search engine.", "labels": [], "entities": []}, {"text": "We use a supervised POS tagger to label the result snippets and then transfer the tags to the queries, producing a set of noisy labeled queries.", "labels": [], "entities": []}, {"text": "These labeled queries are then added to the training data and the tagger is retrained.", "labels": [], "entities": []}, {"text": "We evaluate different strategies for selecting which annotation to transfer and find that using the result that was clicked by the user gives comparable performance to using just the top result or to aggregating over the top-k results.", "labels": [], "entities": []}, {"text": "The most closely related previous work is that of.", "labels": [], "entities": []}, {"text": "In their work, unigram POS tag priors generated from a large corpus are blended with information from the top-50 results from a search engine at prediction time.", "labels": [], "entities": []}, {"text": "Such an approach has the disadvantage that it necessitates access to a search engine at run-time and is computationally very expensive.", "labels": [], "entities": []}, {"text": "We re-implement their method and show that our direct transfer approach is more effective, while being simpler to instrument: since we use information from the search engine only during training, we can train a stand-alone POS tagger that can be run without access to additional resources.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 223, "end_pos": 233, "type": "TASK", "confidence": 0.7427306473255157}]}, {"text": "We also perform an error analysis and find that most of the remaining errors are due to errors in POS tagging of the snippets.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.6917132139205933}]}], "datasetContent": [{"text": "We assume that we have access to labeled English sentences from the PennTreebank () and the QuestionBank (), as well as large amounts of unlabeled search queries.", "labels": [], "entities": [{"text": "PennTreebank", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.990298867225647}]}, {"text": "Each query is paired with a set of relevant results represented by snippets (sentence fragments containing the search terms), as well as information about the order in which the results were shown to the user and possibly the result the user clicked on.", "labels": [], "entities": []}, {"text": "Note that different sets of results are possible for the same query, because of personalization and ranking changes overtime.", "labels": [], "entities": []}, {"text": "We  The second evaluation set consists of 500 so called \"long-tail\" queries.", "labels": [], "entities": []}, {"text": "These are queries that occurred rarely in the search logs, and are typically difficult to tag because they are searching for lessfrequent information.", "labels": [], "entities": []}, {"text": "They do not contain navigational queries.", "labels": [], "entities": [{"text": "navigational queries", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.9124225080013275}]}, {"text": "First, we compared different strategies for selecting relevant snippets from which to transfer the tags.", "labels": [], "entities": []}, {"text": "These systems are: DIRECT-CLICK, which uses snippets clicked on by users; DIRECT-ALL, which uses all the returned snippets seen by the user; 2 and DIRECT-TOP-1, which uses just the snippet in the top result.", "labels": [], "entities": []}, {"text": "compares these systems on our three evaluation sets.", "labels": [], "entities": []}, {"text": "While DIRECT-ALL and DIRECT-TOP-1 perform best on the MS-251 data sets, DIRECT-CLICK has an advantage on the long tail queries.", "labels": [], "entities": [{"text": "DIRECT-ALL", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.7799598574638367}, {"text": "MS-251 data sets", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9801497459411621}]}, {"text": "However, these differences are small (<0.6%) suggesting that any strategy for selecting relevant snippet sets will return comparable results when aggregated overlarge amounts of data.", "labels": [], "entities": []}, {"text": "We then compared our method to the baseline models and a re-implementation of, which we denote BSC.", "labels": [], "entities": [{"text": "BSC", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9076955318450928}]}, {"text": "We use the same matching scheme for both BSC and our system, including the URL matching described in Section 2.", "labels": [], "entities": [{"text": "BSC", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.9350181818008423}, {"text": "URL matching", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.6430960446596146}]}, {"text": "The URL matching improves performance by 0.4-3.0% across all models and evaluation settings.", "labels": [], "entities": [{"text": "URL matching", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7262839674949646}]}, {"text": "For comparison, report 91.6% for their final system, which is comparable to our implementation of their system when the baseline tagger is trained on just the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 159, "end_pos": 169, "type": "DATASET", "confidence": 0.9596093595027924}]}, {"text": "Our best system achieves a 21.2% relative reduction in error on their annotations.", "labels": [], "entities": [{"text": "error", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9016710519790649}]}, {"text": "Some other trends become appar-   ent in.", "labels": [], "entities": []}, {"text": "Firstly, a large part of the benefit of transfer has to do with case information that is available in the snippets but is missing in the query.", "labels": [], "entities": [{"text": "transfer", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.9640170335769653}]}, {"text": "The uncased tagger is insensitive to this mismatch and achieves significantly better results than the cased taggers.", "labels": [], "entities": []}, {"text": "However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers.", "labels": [], "entities": []}, {"text": "This is consistent with the analysis in.", "labels": [], "entities": []}, {"text": "Finally, we see that the direct transfer method from Section 2 significantly outperforms the method described in. confirms this trend when focusing on proper nouns, which are particularly difficult to identify in queries.", "labels": [], "entities": []}, {"text": "We also manually examined a set of 40 queries with their associated snippets, for which our best DIRECT-CLICK system made mistakes.", "labels": [], "entities": []}, {"text": "In 32 cases, the errors in the query tagging could be traced back to errors in the snippet tagging.", "labels": [], "entities": []}, {"text": "A better snippet tagger could alleviate that problem.", "labels": [], "entities": [{"text": "snippet tagger", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7851008772850037}]}, {"text": "In the remaining 8 cases there were problems with the matching -either the mis-tagged word was not found at all, or it was matched incorrectly.", "labels": [], "entities": []}, {"text": "For example one of the results for the query \"bell helmet\" had a snippet containing \"Bell cycling helmets\" and we failed to match helmet to helmets.: Precision and recall of the NNP tag on the longtail data for the best baseline method and the three transfer methods using that baseline.", "labels": [], "entities": [{"text": "Precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9520174860954285}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9989516735076904}]}, {"text": "create features based on search engine results, that they use in an NER system applied to queries.", "labels": [], "entities": []}, {"text": "They report report significant improvements when incorporating features from the snippets.", "labels": [], "entities": []}, {"text": "In particular, they exploit capitalization and query terms matching URL components; both of which we have used in this work.", "labels": [], "entities": []}, {"text": "use clicks in a product database to train a tagger for product queries, but they do not use snippets and do not annotate syntax. and also work on adding tags to queries, but do not use snippets or search logs as a source of information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of snippet selection strategies.", "labels": [], "entities": [{"text": "snippet selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7323063910007477}]}, {"text": " Table 2: Tagging accuracies for different baseline settings  and two transfer methods.DIRECT-CLICK is the approach  we propose (see text). Column MS-251 NVX evaluates  with tags from Bendersky et al. (2010). Their baseline  is 89.3% and they report 91.6% for their method. MS- 251 and Long-tail use tags from Section 3.1. We observe  snippets for 2/500 long-tail queries and 31/251 MS-251  queries.", "labels": [], "entities": [{"text": "MS- 251", "start_pos": 274, "end_pos": 281, "type": "DATASET", "confidence": 0.7997371554374695}]}, {"text": " Table 3: Precision and recall of the NNP tag on the long- tail data for the best baseline method and the three trans- fer methods using that baseline.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9921368360519409}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9993606209754944}]}]}