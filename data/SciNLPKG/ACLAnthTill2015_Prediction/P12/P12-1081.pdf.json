{"title": [{"text": "Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations", "labels": [], "entities": [{"text": "Attacking Parsing Bottlenecks", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.873515248298645}, {"text": "Relevant Factorizations", "start_pos": 54, "end_pos": 77, "type": "METRIC", "confidence": 0.907258540391922}]}], "abstractContent": [{"text": "Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9631173014640808}]}, {"text": "Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications.", "labels": [], "entities": []}, {"text": "Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution.", "labels": [], "entities": []}, {"text": "As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem.", "labels": [], "entities": []}, {"text": "By including un-labeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve anew state-of-the-art for English dependencies with 93.55% correct attachments on the current standard.", "labels": [], "entities": []}, {"text": "Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9993315935134888}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.998925507068634}]}], "introductionContent": [{"text": "Prepositions and conjunctions are two large remaining bottlenecks in parsing.", "labels": [], "entities": []}, {"text": "Across various existing parsers, these two categories have the lowest accuracies, and mistakes made on these have consequences for downstream applications.", "labels": [], "entities": []}, {"text": "Machine translation is sensitive to parsing errors involving prepositions and conjunctions, because in some languages different attachment decisions in the parse of the source language sentence produce different translations.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.795487105846405}]}, {"text": "Preposition attachment mistakes are particularly bad when translating into Japanese () which uses a different postposition for different attachments; conjunction mistakes can cause word ordering mistakes when translating into Chinese.", "labels": [], "entities": []}, {"text": "Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (.", "labels": [], "entities": []}, {"text": "However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance.", "labels": [], "entities": []}, {"text": "Unlabeled data can help ameliorate this sparsity problem.", "labels": [], "entities": []}, {"text": "Backing off to cluster membership features (  or by using association statistics from a larger corpus, such as the web (), have both improved parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 142, "end_pos": 149, "type": "TASK", "confidence": 0.9641358852386475}]}, {"text": "Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9984094500541687}]}, {"text": "However, it has so far been less effective within full parsing -while first-order web-scale counts noticeably improved overall parsing in, the accuracy on conjunctions actually decreased when the web-scale features were added in that paper).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9988217949867249}]}, {"text": "In this paper we show that unlabeled data can help prepositions and conjunctions, provided that the dependency representation is compatible with how the parsing problem is decomposed for learning and inference.", "labels": [], "entities": []}, {"text": "By incorporating unlabeled data into factorizations which capture the relevant dependencies for prepositions and conjunctions, we produce a parser for English which has an unlabeled attachment accuracy of 93.5%, over an 18% reduction in error over the best previously published parser () on the current standard for dependency parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.5398625731468201}, {"text": "dependency parsing", "start_pos": 316, "end_pos": 334, "type": "TASK", "confidence": 0.7923644781112671}]}, {"text": "The best model for conjunctions at-taches them with 90.8% accuracy (42.5% reduction in error over MSTParser), and the best model for prepositions with 87.4% accuracy (18.2% reduction in error over MSTParser).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9988225102424622}, {"text": "MSTParser", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.8063610792160034}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9978189468383789}]}, {"text": "We describe the dependency representations of prepositions and conjunctions in Section 2.", "labels": [], "entities": []}, {"text": "We discuss the implications of these representations for how learning and inference for parsing are decomposed (Section 3) and how unlabeled data maybe used (Section 4).", "labels": [], "entities": []}, {"text": "We then present experiments exploring the connection between representation, factorization, and unlabeled data in Sections 5 and 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The previous section motivated the use of unlabeled data for attaching prepositions and conjunctions.", "labels": [], "entities": []}, {"text": "We have also hypothesized that these features will be most effective when the data representation and the learning representation both capture relevant properties of prepositions and conjunctions.", "labels": [], "entities": []}, {"text": "We predict that Conversion 2 and a factorization which includes grand-parent scoring will achieve the highest performance.", "labels": [], "entities": [{"text": "Conversion", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.7707216143608093}]}, {"text": "In this section, we investigate the impact of unlabeled data on parsing accuracy using the two conversions and using each of the factorizations described in Section 3.1-3.4.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9768726825714111}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9608478546142578}]}, {"text": "Training was done on Section 2-21 of the Penn Treebank.", "labels": [], "entities": [{"text": "Section 2-21 of the Penn Treebank", "start_pos": 21, "end_pos": 54, "type": "DATASET", "confidence": 0.8447790642579397}]}, {"text": "Section 22 was used for development, and Section 23 for test.", "labels": [], "entities": []}, {"text": "We use automatic partof-speech tags for both training and testing).", "labels": [], "entities": []}, {"text": "The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (  as in . We train the full parser for 15 iterations of averaged perceptron training), choose the iteration with the best unlabeled attachment score (UAS) on the development set, and apply the model after that iteration to the test set.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 259, "end_pos": 291, "type": "METRIC", "confidence": 0.8326940635840098}]}, {"text": "http://groups.csail.mit.edu/nlp/dpo3/ We also ran MSTParser (), the Berkeley constituency parser, and the unmodified dpo3 Model 1 ( ) using Conversion 2 (the current recommendations) for comparison.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9278860092163086}]}, {"text": "Since the converted Penn Treebank now contains a few non-projective sentences, we ran both the projective and non-projective versions of the second order (sibling) MSTParser.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9945636689662933}, {"text": "MSTParser", "start_pos": 164, "end_pos": 173, "type": "DATASET", "confidence": 0.9467807412147522}]}, {"text": "The Berkeley parser was trained on the constituency trees of the PTB patched with, and then the predicted parses were converted using pennconverter.", "labels": [], "entities": [{"text": "PTB", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9337354898452759}]}, {"text": "shows the unlabeled attachment scores, complete sentence exact match accuracies, and the accuracies of conjunctions and prepositions under Conversion 2.", "labels": [], "entities": []}, {"text": "The incorporation of the unlabeled data features (clusters and web counts) into the dpo3 parser yields a significantly better parser than dpo3 alone (93.54 UAS versus 93.21) 8 , and is more than a 1.5% improvement over MSTParser.", "labels": [], "entities": [{"text": "UAS", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9636451601982117}]}], "tableCaptions": [{"text": " Table 1: Test set accuracies under Conversion 2 of unlabeled attachment scores, complete sentence exact match accu- racies, conjunction accuracy, and preposition accuracy. Bolded items are the best in each column, or not significantly  different from the best in that column (sign test, p < .05).", "labels": [], "entities": [{"text": "complete sentence exact match accu- racies", "start_pos": 81, "end_pos": 123, "type": "METRIC", "confidence": 0.700564580304282}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.7056449055671692}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.8707429766654968}]}, {"text": " Table 2: Unlabeled attachment accuracy for conjunc- tions. Bolded items are the best in each column, or not  significantly different (sign test, p < .05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9739121198654175}]}, {"text": " Table 3: Unlabeled attachment accuracy for prepositions.  Bolded items are the best in each column, or not signifi- cantly different (sign test, p < .05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.936091423034668}]}]}