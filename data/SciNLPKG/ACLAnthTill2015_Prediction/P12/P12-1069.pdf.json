{"title": [{"text": "Head-driven Transition-based Parsing with Top-down Prediction", "labels": [], "entities": [{"text": "Head-driven Transition-based Parsing", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.4560624957084656}]}], "abstractContent": [{"text": "This paper presents a novel top-down head-driven parsing algorithm for data-driven pro-jective dependency analysis.", "labels": [], "entities": [{"text": "pro-jective dependency analysis", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.5981821616490682}]}, {"text": "This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms.", "labels": [], "entities": []}, {"text": "Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.", "labels": [], "entities": [{"text": "English Penn Treebank data", "start_pos": 19, "end_pos": 45, "type": "DATASET", "confidence": 0.9459918588399887}, {"text": "Chinese CoNLL-06 data", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.8332021633783976}, {"text": "dependency parsing", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.7091164588928223}]}], "introductionContent": [{"text": "Transition-based parsing algorithms, such as shiftreduce algorithms, are widely used for dependency analysis because of the efficiency and comparatively good performance.", "labels": [], "entities": [{"text": "Transition-based parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6184972524642944}, {"text": "dependency analysis", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8768289983272552}]}, {"text": "However, these parsers have one major problem that they can handle only local information.", "labels": [], "entities": []}, {"text": "pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding.", "labels": [], "entities": [{"text": "root finding", "start_pos": 118, "end_pos": 130, "type": "TASK", "confidence": 0.7250174880027771}]}, {"text": "This work presents an O(n 2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers.", "labels": [], "entities": [{"text": "headdriven transition-based parsing", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.5211683412392935}]}, {"text": "The deductive system is very similar to Earley parsing.", "labels": [], "entities": [{"text": "Earley parsing", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.5942128151655197}]}, {"text": "The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.6820150911808014}]}, {"text": "To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model.", "labels": [], "entities": []}, {"text": "To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9702239632606506}, {"text": "deterministic parsing", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.5565806776285172}]}, {"text": "The complexity becomes O(n 2 * b) where b is the beam size.", "labels": [], "entities": [{"text": "complexity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9615199565887451}, {"text": "O", "start_pos": 23, "end_pos": 24, "type": "METRIC", "confidence": 0.9961866736412048}]}, {"text": "To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser.", "labels": [], "entities": [{"text": "FIRST", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.6535564661026001}]}, {"text": "Experimental results show that the proposed top-down parser achieves competitive results with other data-driven parsing algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were performed on the English Penn Treebank data and the Chinese CoNLL-06 data.", "labels": [], "entities": [{"text": "English Penn Treebank data", "start_pos": 34, "end_pos": 60, "type": "DATASET", "confidence": 0.912954106926918}, {"text": "Chinese CoNLL-06 data", "start_pos": 69, "end_pos": 90, "type": "DATASET", "confidence": 0.8763876159985861}]}, {"text": "For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing.", "labels": [], "entities": [{"text": "English data", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8133066594600677}, {"text": "WSJ", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8690752387046814}]}, {"text": "We used   we used the information of words and fine-grained POS-tags for features.", "labels": [], "entities": []}, {"text": "We also implemented and experimented's arc-standard shift-reduce parser.", "labels": [], "entities": []}, {"text": "For the 2nd-order Eisner-Satta algorithm, we used MSTParser.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9484677314758301}]}, {"text": "We used an early update version of averaged perceptron algorithm () for training of shift-reduce and top-down parsers.", "labels": [], "entities": []}, {"text": "A set of feature templates in were used for the stack-based model, and a set of feature templates in) were used for the 2nd-order prediction model.", "labels": [], "entities": []}, {"text": "The weighted prediction and stack-based models of topdown parser were jointly trained.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9987097978591919}]}, {"text": " Table 2: Oracle score, choosing the highest accuracy  parse for each sentence on test data from results of top- down (beam 8, pred 5) and shift-reduce (beam 8) and  MST(2nd) parsers in Table 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9931569695472717}]}, {"text": " Table 3: Results for Chinese Data (CoNLL-06)", "labels": [], "entities": [{"text": "Chinese Data (CoNLL-06)", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.891160535812378}]}]}