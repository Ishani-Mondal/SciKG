{"title": [{"text": "Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7409853637218475}, {"text": "POS Tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8066513240337372}]}], "abstractContent": [{"text": "We propose the first joint model for word segmen-tation, POS tagging, and dependency parsing for Chinese.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.8484213948249817}, {"text": "dependency parsing", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7339410483837128}]}, {"text": "Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.8118308484554291}, {"text": "dependency parsing", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7923588156700134}, {"text": "POS tagging", "start_pos": 233, "end_pos": 244, "type": "TASK", "confidence": 0.7485754787921906}, {"text": "dependency parsing", "start_pos": 250, "end_pos": 268, "type": "TASK", "confidence": 0.8004305362701416}]}, {"text": "We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework.", "labels": [], "entities": []}, {"text": "In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing.", "labels": [], "entities": [{"text": "Chinese Treebank (CTB)", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.9753636121749878}, {"text": "accuracies", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9806674718856812}, {"text": "POS tagging", "start_pos": 177, "end_pos": 188, "type": "TASK", "confidence": 0.8069988191127777}, {"text": "dependency parsing", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.8797808289527893}]}, {"text": "We also perform comparison experiments with the partially joint models.", "labels": [], "entities": []}], "introductionContent": [{"text": "In processing natural languages that do not include delimiters (e.g. spaces) between words, word segmentation is the crucial first step that is necessary to perform virtually all NLP tasks.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7053114026784897}]}, {"text": "Furthermore, the word-level information is often augmented with the POS tags, which, along with segmentation, form the basic foundation of statistical NLP.", "labels": [], "entities": []}, {"text": "Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g.).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7400476634502411}, {"text": "POS tagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.7935282289981842}, {"text": "word segmentation", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.7439408898353577}, {"text": "POS tagging", "start_pos": 155, "end_pos": 166, "type": "TASK", "confidence": 0.7849600911140442}]}, {"text": "This is because some of the segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags.", "labels": [], "entities": []}, {"text": "The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7754152417182922}, {"text": "POS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.7664429545402527}, {"text": "word segmentation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7387149035930634}, {"text": "POS tagging", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.7590519189834595}, {"text": "accuracies", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.8003481030464172}]}, {"text": "In addition, some researchers recently proposed a joint approach to Chinese POS tagging and dependency parsing); particularly, proposed an incremental approach to this joint task, and showed that the joint approach improves the accuracies of these two tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.6631999164819717}, {"text": "dependency parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7070014625787735}]}, {"text": "In this context, it is natural to consider further a question regarding the joint framework: how strongly do the tasks of word segmentation and dependency parsing interact?", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7178149223327637}, {"text": "dependency parsing", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7131044268608093}]}, {"text": "In the following Chinese sentences: S\u00ca \u008csV \u008cs \u008b \u00f8s current peace-prize and peace operation related The current peace prize and peace operations are related.", "labels": [], "entities": []}, {"text": "S\u00ca \u008cs V \u008cs \u008b \u00f8s \u00e2S current peace award peace operation related group The current peace is awarded to peace-operation-related groups.", "labels": [], "entities": [{"text": "\u008b \u00f8s \u00e2S", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.8147867918014526}]}, {"text": "the only difference is the existence of the last word \u00e2S; however, whether or not this word exists changes the whole syntactic structure and segmentation of the sentence.", "labels": [], "entities": [{"text": "\u00e2S", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9739274978637695}]}, {"text": "This is an example in which word segmentation cannot be handled properly without considering long-range syntactic information.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.72145976126194}]}, {"text": "Syntactic information is also considered beneficial to improve the segmentation of out-ofvocabulary (OOV) words.", "labels": [], "entities": [{"text": "segmentation of out-ofvocabulary (OOV) words", "start_pos": 67, "end_pos": 111, "type": "TASK", "confidence": 0.7599014128957476}]}, {"text": "Unlike languages such as Japanese that use a distinct character set (i.e. katakana) for foreign words, the transliterated words in Chinese, many of which are OOV words, frequently include characters that are also used as common or function words.", "labels": [], "entities": []}, {"text": "In the current systems, the existence of these characters causes numerous oversegmentation errors for OOV words.", "labels": [], "entities": []}, {"text": "Based on these observations, we aim at building a joint model that simultaneously processes word segmentation, POS tagging, and dependency parsing, trying to capture global interaction among these three tasks.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7617580592632294}, {"text": "POS tagging", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.885114997625351}, {"text": "dependency parsing", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7079183459281921}]}, {"text": "To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming, and propose an efficient method of character-based decoding over candidate structures.", "labels": [], "entities": []}, {"text": "Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.8032557169596354}]}, {"text": "First, we must address the problem of how to align comparable states effectively in the beam.", "labels": [], "entities": []}, {"text": "Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks.", "labels": [], "entities": []}, {"text": "Second, although the feature set is fundamentally a combination of those used in previous works, to integrate them in a single incremental framework is not straightforward.", "labels": [], "entities": []}, {"text": "Because we must perform decisions of three kinds (segmentation, tagging, and parsing) in an incremental framework, we must adjust which features are to be activated when, and how they are combined with which action labels.", "labels": [], "entities": []}, {"text": "We have also found that we must balance the learning rate between features for segmentation and tagging decisions, and those for dependency parsing.", "labels": [], "entities": [{"text": "segmentation and tagging decisions", "start_pos": 79, "end_pos": 113, "type": "TASK", "confidence": 0.8106730729341507}, {"text": "dependency parsing", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.846866250038147}]}, {"text": "We perform experiments using the Chinese Treebank (CTB) corpora, demonstrating that the accuracies of the three tasks can be improved significantly over the pipeline combination of the state-of-the-art joint segmentation and POS tagging model, and the dependency parser.", "labels": [], "entities": [{"text": "Chinese Treebank (CTB) corpora", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.9719059069951376}, {"text": "POS tagging", "start_pos": 225, "end_pos": 236, "type": "TASK", "confidence": 0.6854808777570724}]}, {"text": "We also perform comparison experiments with partially joint models, and investigate the tradeoff between the running speed and the model performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Chinese Penn Treebank ver.", "labels": [], "entities": [{"text": "Chinese Penn Treebank ver", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.9246697574853897}]}, {"text": "5.1, 6.0, and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7) for evaluation.", "labels": [], "entities": []}, {"text": "These corpora are split into training, development, and test sets, according to previous works.", "labels": [], "entities": []}, {"text": "For CTB-5, we refer to the split by as CTB-5d, and to the split by as CTB-5j.", "labels": [], "entities": [{"text": "CTB-5", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8123169541358948}, {"text": "CTB-5d", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.8778610229492188}, {"text": "CTB-5j", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.9483963251113892}]}, {"text": "We also prepare a dataset for cross validation: the dataset CTB-5c consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j.", "labels": [], "entities": []}, {"text": "We split CTB5c into five sets (CTB-5c-n), and alternatively use four of these as the training set and the rest as the test set.", "labels": [], "entities": []}, {"text": "CTB-6 is split according to the official split described in the documentation, and CTB-7 is split according to.", "labels": [], "entities": [{"text": "CTB-6", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9681901931762695}, {"text": "CTB-7", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.8870221972465515}]}, {"text": "The statistics of these splits are shown in.", "labels": [], "entities": []}, {"text": "As external dictionaries, we use the HowNet Word List 3 , consisting of 91,015 words, and page names from the Chinese Wikipedia as of, consisting of 709,352 words.", "labels": [], "entities": [{"text": "HowNet Word List 3", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.9555603414773941}]}, {"text": "These dictionaries only consist of word forms with no frequency or POS information.", "labels": [], "entities": [{"text": "POS", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9709839224815369}]}, {"text": "We use standard measures of word-level precision, recall, and F1 score, for evaluating each task.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9267815947532654}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9994107484817505}, {"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9904946982860565}]}, {"text": "The output of dependencies cannot be correct unless the syntactic head and dependent of the dependency relation are both segmented correctly.", "labels": [], "entities": []}, {"text": "Following the standard setting in dependency parsing works, we evaluate the task of dependency parsing with the unlabeled attachment scores excluding punctuations.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8757738769054413}, {"text": "dependency parsing", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8198087811470032}]}, {"text": "Statistical significance is tested by McNemar's test ( \u2020 : p < 0.05, \u2021 : p < 0.01).", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.6567374765872955}, {"text": "McNemar's test", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.9163391590118408}]}], "tableCaptions": [{"text": " Table 2: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 2. As external dic- tionaries, we use the HowNet Word List 3 , consist- ing of 91,015 words, and page names from the Chi- nese Wikipedia", "labels": [], "entities": [{"text": "HowNet Word List 3", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.9486459940671921}, {"text": "Chi- nese Wikipedia", "start_pos": 124, "end_pos": 143, "type": "DATASET", "confidence": 0.7113772183656693}]}, {"text": " Table 3: F1 scores and speed (in sentences per sec.)  of SegTagDep on CTB-5c-1 w.r.t. the beam size.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9695058763027191}, {"text": "speed", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9808885455131531}, {"text": "CTB-5c-1", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9058759808540344}]}, {"text": " Table 5: Final results on CTB-5j", "labels": [], "entities": [{"text": "CTB-5j", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.8765437006950378}]}, {"text": " Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( \u2021 : p < 0.01).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.7627888023853302}, {"text": "attachment) dependency F1 scores", "start_pos": 52, "end_pos": 84, "type": "METRIC", "confidence": 0.6534478783607482}, {"text": "CTB-5c", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.9810037016868591}]}, {"text": " Table 6: Final results on CTB-6 and CTB-7", "labels": [], "entities": [{"text": "CTB-6", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.9192807674407959}, {"text": "CTB-7", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8149437308311462}]}]}