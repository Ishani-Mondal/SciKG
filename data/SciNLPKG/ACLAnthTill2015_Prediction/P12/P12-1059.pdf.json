{"title": [{"text": "Mining Entity Types from Query Logs via User Intent Modeling", "labels": [], "entities": [{"text": "User Intent Modeling", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.5796041786670685}]}], "abstractContent": [{"text": "We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entity-bearing queries are generated.", "labels": [], "entities": []}, {"text": "We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks.", "labels": [], "entities": []}, {"text": "We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base.", "labels": [], "entities": []}, {"text": "Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries.", "labels": [], "entities": []}, {"text": "We show that modeling user intent significantly improves entity type resolution for head queries over the state of the art, on several metrics, without degradation in tail query performance.", "labels": [], "entities": [{"text": "entity type resolution", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6481174329916636}]}], "introductionContent": [{"text": "Commercial search engines are providing everricher experiences around entities.", "labels": [], "entities": []}, {"text": "Querying fora dish on Google yields recipe filters such as cook time, calories, and ingredients.", "labels": [], "entities": []}, {"text": "Querying fora movie on Yahoo triggers user ratings, cast, tweets and showtimes.", "labels": [], "entities": []}, {"text": "Bing further allows the movie to be directly added to the user's Netflix queue.", "labels": [], "entities": []}, {"text": "Entity repositories such as Freebase, IMDB, Facebook Pages, Factual, Pricegrabber, and Wikipedia are increasingly leveraged to enable such experiences.", "labels": [], "entities": [{"text": "Factual", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9264840483665466}]}, {"text": "There are, however, inherent problems in the entity repositories: (a) coverage: although coverage of head entity types is often reliable, the tail can be sparse; (b) noise: created by spammers, extraction errors or errors in crowdsourced content; (c) ambiguity: multiple types or entity identifiers are often associated with the same surface string; and (d) over-expression: many entities have types that are never used in the context of Web search.", "labels": [], "entities": []}, {"text": "There is an opportunity to automatically tailor knowledge repositories to the Web search scenario.", "labels": [], "entities": []}, {"text": "Desirable capabilities of such a system include: (a) determining the prior type distribution in Web search for each entity in the repository; (b) assigning a type distribution to new entities; (c) inferring the correct sense of an entity in a particular query context; and (d) adapting to a search engine and time period.", "labels": [], "entities": []}, {"text": "In this paper, we build such a system by leveraging Web search usage logs with large numbers of user sessions seeking or transacting on entities.", "labels": [], "entities": []}, {"text": "We cast the task as performing probabilistic inference in a graphical model that captures how queries are generated, and then apply the model to contextually recognize entity types in new queries.", "labels": [], "entities": []}, {"text": "We motivate and design several generative models based on the theory that search users' (unobserved) intents govern the types of entities, the query formulations, and the ultimate clicks on Web documents.", "labels": [], "entities": []}, {"text": "We show that jointly modeling user intent and entity type significantly outperforms the current state of the art on the task of entity type resolution in queries.", "labels": [], "entities": [{"text": "entity type resolution", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.6561464468638102}]}, {"text": "The major contributions of our research are: \u2022 We introduce the idea that latent user intents can bean important factor in modeling type distributions over entities in Web search.", "labels": [], "entities": []}, {"text": "\u2022 We propose generative models and inference procedures using signals from query context, click, entity, entity type, and user intent.", "labels": [], "entities": []}, {"text": "\u2022 We propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types.", "labels": [], "entities": []}, {"text": "\u2022 We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We refer to QL as a set of English Web search queries issued to a commercial search engine over a period of several months.", "labels": [], "entities": []}, {"text": "We observe a different behavior on tail queries where all models significantly outperform the baseline B FB , but are not significantly different from each other.", "labels": [], "entities": []}, {"text": "In short, the strength of our proposed model is in improving performance on the head at no noticeable cost in the tail.", "labels": [], "entities": []}, {"text": "We separately tested the effect of adding the empty context parameter \u03c3. illustrates the result on the HEAD data.", "labels": [], "entities": [{"text": "HEAD data", "start_pos": 103, "end_pos": 112, "type": "DATASET", "confidence": 0.9063902497291565}]}, {"text": "Across all metrics, \u03c3 improved performance overall models . The more expressive models benefitted more than the less expressive ones.", "labels": [], "entities": []}, {"text": "reports results for Model IM using K = 200 user intents.", "labels": [], "entities": []}, {"text": "This was determined by varying K and selecting the top-performing value.", "labels": [], "entities": []}, {"text": "illustrates the performance of Model IM with different values of K on the HEAD.", "labels": [], "entities": [{"text": "HEAD", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9466648101806641}]}, {"text": "Our models can also assign a prior type distribution to each entity by further marginalizing Eq.", "labels": [], "entities": []}, {"text": "1 over query contexts n 1 and n 2 . We measured the quality of our learned type priors using the subset of queries in our HEAD test set that consisted of only an entity without any refiners.", "labels": [], "entities": [{"text": "HEAD test set", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.8782671888669332}]}, {"text": "The results for Model IM were: nDCG = 0.86, M AP = 0.80, M AP W = 0.75, and P rec@1 = 0.70.", "labels": [], "entities": [{"text": "M AP", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.8305528461933136}, {"text": "M AP W", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.708878219127655}, {"text": "P rec@1", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9264554679393768}]}, {"text": "All metrics are statistically significantly better than B FB , Guo 09 and M0, with 95% confidence.", "labels": [], "entities": [{"text": "B FB", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.8609968423843384}, {"text": "Guo 09", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.8149450421333313}, {"text": "M0", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.5317628383636475}]}, {"text": "Compared to Model M1, Model IM is statistically significantly better on P rec@1 and not significantly different on the other metrics.", "labels": [], "entities": []}, {"text": "Discussion and Error Analysis: Contrary to our results, we had expected improvements for both HEAD and TAIL.", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 15, "end_pos": 29, "type": "METRIC", "confidence": 0.9089567065238953}, {"text": "HEAD", "start_pos": 94, "end_pos": 98, "type": "TASK", "confidence": 0.51826411485672}, {"text": "TAIL", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.49278557300567627}]}, {"text": "Inspection of the TAIL queries revealed that entities were greatly skewed towards people (e.g., actor, author, and politician).", "labels": [], "entities": [{"text": "TAIL queries", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.5589582324028015}]}, {"text": "Analysis of the latent user intent parameter \u0398 in Model IM showed that most people types had most of their probability mass assigned to the same three generic and common intents for people types: 'see pictures of', 'find biographical information about', and 'see video of'.", "labels": [], "entities": []}, {"text": "In other words, latent intents in Model IM are overexpressive and they do not help in differentiating people types.", "labels": [], "entities": []}, {"text": "The largest class of errors came from queries bearing an entity with semantically very similar types where our highest ranked type was not judged correct by the annotators.", "labels": [], "entities": []}, {"text": "For example, for the query \"philippine daily inquirer\" our system ranked newspaper ahead of periodical but a judge rejected the former and approved the latter.", "labels": [], "entities": []}, {"text": "For \"ikea catalogue\", our system ranked magazine ahead of periodical, but again a judge rejected magazine in favor of periodical.", "labels": [], "entities": [{"text": "ikea catalogue", "start_pos": 5, "end_pos": 19, "type": "DATASET", "confidence": 0.8290843069553375}]}, {"text": "An interesting success casein the TAIL is highlighted by two queries involving the entity \"ymca\", which in our data can either be a song, place, or educational institution.", "labels": [], "entities": [{"text": "TAIL", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.44113484025001526}]}, {"text": "Our system learns the following priors: 0.63, 0.29, and 0.08, respectively.", "labels": [], "entities": []}, {"text": "For the query \"jamestown ymca ny\", IM correctly classified \"ymca\" as a place and for the query \"ymca palomar\" it correctly classified it as an educational institution.", "labels": [], "entities": [{"text": "IM", "start_pos": 35, "end_pos": 37, "type": "DATASET", "confidence": 0.4802180826663971}]}, {"text": "We further issued the query \"ymca lyrics\" and the type song was then highest ranked.", "labels": [], "entities": []}, {"text": "Our method is generalizable to any entity collection.", "labels": [], "entities": []}, {"text": "Since our evaluation focused on the Freebase collection, it remains an open question how noise level, coverage, and breadth in a collection will affect our model performance.", "labels": [], "entities": [{"text": "Freebase collection", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9583861827850342}, {"text": "coverage", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.957403838634491}, {"text": "breadth", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9783117175102234}]}, {"text": "Finally, although we do not formally evaluate it, it is clear that training our model on different time spans of queries should lead to type distributions adapted to that time period.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Model analysis on HEAD and TAIL.  \u2020 indicates statistical significance over B FB , and  \u2021 over both B FB and  Guo 09. Bold indicates statistical significance over all non-bold models in the column. Significance is measured  using the Student's t-test at 95% confidence.", "labels": [], "entities": [{"text": "Significance", "start_pos": 208, "end_pos": 220, "type": "METRIC", "confidence": 0.9553058743476868}]}]}