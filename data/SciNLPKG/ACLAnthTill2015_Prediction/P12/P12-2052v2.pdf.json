{"title": [{"text": "Authorship Attribution with Author-aware Topic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Authorship attribution deals with identifying the authors of anonymous texts.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6907916963100433}]}, {"text": "Building on our earlier finding that the Latent Dirichlet Allocation (LDA) topic model can be used to improve authorship attribution accuracy, we show that employing a previously-suggested Author-Topic (AT) model outperforms LDA when applied to scenarios with many authors.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.8019182980060577}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.7062903046607971}]}, {"text": "In addition, we define a model that combines LDA and AT by representing authors and documents over two disjoint topic sets, and show that our model outperforms LDA, AT and support vector machines on datasets with many authors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Authorship attribution (AA) has attracted much attention due to its many applications in, e.g., computer forensics, criminal law, military intelligence, and humanities research.", "labels": [], "entities": [{"text": "Authorship attribution (AA)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8720519721508027}]}, {"text": "The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classification problem).", "labels": [], "entities": []}, {"text": "While most of the early work on AA focused on formal texts with only a few possible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors ().", "labels": [], "entities": [{"text": "AA", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9807757139205933}]}, {"text": "In parallel, topic models have gained popularity as a means of analysing such large text corpora).", "labels": [], "entities": []}, {"text": "In), we showed that methods based on Latent Dirichlet Allocation (LDA) -a popular topic model by -yield good AA performance.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 37, "end_pos": 69, "type": "METRIC", "confidence": 0.8483678817749023}, {"text": "AA", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.5406792163848877}]}, {"text": "However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA.", "labels": [], "entities": []}, {"text": "This paper aims to address this gap.", "labels": [], "entities": []}, {"text": "In addition to being the first (to the best of our knowledge) to apply Rosen- Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which addresses AT's limitations in the context of AA.", "labels": [], "entities": []}, {"text": "We show that DADT outperforms AT, LDA, and linear support vector machines on AA with many authors.", "labels": [], "entities": [{"text": "DADT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.6252002120018005}]}], "datasetContent": [{"text": "We compare the performance of the methods on two publicly-available datasets: (1) PAN'11: emails with 72 authors (Argamon and Juola, 2011); and (2) Blog: blogs with 19,320 authors ().", "labels": [], "entities": []}, {"text": "These datasets represent realistic scenarios of AA of user-generated texts with many candidate authors.", "labels": [], "entities": [{"text": "AA of user-generated texts", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.8343991786241531}]}, {"text": "For example, notes a case where an employee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email.", "labels": [], "entities": []}, {"text": "Experiments on the PAN'11 dataset followed the setup of the PAN'11 competition (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the parameters according to the results on the given validation subset, and ran the tuned methods on the given testing subset.", "labels": [], "entities": [{"text": "PAN'11 dataset", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9736599028110504}, {"text": "PAN'11 competition", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.8652747571468353}]}, {"text": "In the Blog experiments, we used tenfold cross validation as in).", "labels": [], "entities": [{"text": "Blog", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.8405243754386902}]}, {"text": "We used collapsed Gibbs sampling to train all the topic models ( ), running 4 chains with a burn-in of 1,000 iterations.", "labels": [], "entities": []}, {"text": "In the PAN'11 experiments, we retained 8 samples per chain with spacing of 100 iterations.", "labels": [], "entities": [{"text": "PAN'11", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.765120804309845}, {"text": "spacing", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9724709391593933}]}, {"text": "In the Blog experiments, we retained 1 sample per chain due to runtime constraints.", "labels": [], "entities": [{"text": "Blog", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.7792962193489075}]}, {"text": "Since we cannot average topic distribution estimates obtained from training samples due to topic exchangeability, we averaged the distances and probabilities calculated from the retained samples.", "labels": [], "entities": []}, {"text": "For test text sampling, we used a burn-in of 100 iterations and averaged the parameter estimates over the next 100 iterations in a similar manner to.", "labels": [], "entities": [{"text": "test text sampling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.604597012201945}]}, {"text": "We found that these settings yield stable results across different random seed values.", "labels": [], "entities": []}, {"text": "We found that the number of topics has a larger impact on accuracy than other configurable parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9986498951911926}]}, {"text": "Hence, we used symmetric topic priors, setting all the elements of \u03b1 (D) and \u03b1 (A) to min{0.1, 5/T (D) } and min{0.1, 5/T (A) } respectively.", "labels": [], "entities": []}, {"text": "For all models, we set \u03b2 w = 0.01 for each word was the base measure for the prior of words in topics.", "labels": [], "entities": []}, {"text": "Since DADT allows us to encode our prior knowledge that stopword use is indicative of authorship, we set \u03b2 (D) w = 0.01 \u2212 and \u03b2 (A) w = 0.01 + for all w, where w is a stopword.", "labels": [], "entities": []}, {"text": "We set = 0.009, which improved accuracy by up to one percentage point over using = 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995263814926147}]}, {"text": "Finally, we set \u03b4 (A) = 4.889 and \u03b4 (D) = 1.222 for DADT.", "labels": [], "entities": [{"text": "\u03b4 (A)", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.8801447004079819}, {"text": "\u03b4 (D)", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.8088784515857697}, {"text": "DADT", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.626785159111023}]}, {"text": "This encodes our prior  belief that 0.8 \u00b1 0.15 of each document is composed of author words.", "labels": [], "entities": []}, {"text": "We found that this yields better results than an uninformed uniform prior of).", "labels": [], "entities": []}, {"text": "In addition, we set \u03b7 a = 1 for each author a, yielding smoothed estimates for the corpus distribution of authors \u03c7.", "labels": [], "entities": []}, {"text": "To fairly compare the topic-based methods, we used the same overall number of topics for all the topic models.", "labels": [], "entities": []}, {"text": "We present only the results obtained with the best topic settings: 100 for PAN'11 and 400 for Blog, with DADT's author/document topic splits being 90/10 for PAN'11, and 390/10 for Blog.", "labels": [], "entities": [{"text": "Blog", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.8935608267784119}, {"text": "Blog", "start_pos": 180, "end_pos": 184, "type": "DATASET", "confidence": 0.9544900059700012}]}, {"text": "These splits allow DADT to de-noise the author representations by allocating document words to a relatively small number of document topics.", "labels": [], "entities": []}, {"text": "It is worth noting that AT can be seen as an extreme version of DADT, where all the topics are author topics.", "labels": [], "entities": [{"text": "AT", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.826690673828125}]}, {"text": "A future extension is to learn the topic balance automatically, e.g., in a similar manner to method of inferring the number of topics in LDA.", "labels": [], "entities": []}, {"text": "shows the results of our experiments in terms of classification accuracy (i.e., the percentage of test texts correctly attributed to their author).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.868233859539032}]}, {"text": "The PAN'11 results are shown for the validation and testing subsets, and the Blog results are shown fora subset containing the 1,000 most prolific authors and for the full dataset of 19,320 authors.", "labels": [], "entities": [{"text": "PAN'11", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.5481027364730835}, {"text": "Blog", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.6512676477432251}]}, {"text": "Our DADT model yielded the best results in all cases (the differences between DADT and the other methods are statistically significant according to a paired two-tailed t-test with p < 0.05).", "labels": [], "entities": []}, {"text": "We attribute DADT's superior performance to the de-noising effect of the disjoint topic sets, which appear to yield author representations of higher predictive quality than those of the other models.", "labels": [], "entities": []}, {"text": "As expected, AT significantly outperformed LDA-H.", "labels": [], "entities": [{"text": "AT", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9494858980178833}]}, {"text": "On the other hand, AT-FA performed much worse than all the other methods on PAN'11, probably because of the inherent noisiness in using the same topics to model both authors and documents.", "labels": [], "entities": [{"text": "AT-FA", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.7173406481742859}, {"text": "PAN'11", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8061750531196594}]}, {"text": "Hence, we did not run AT-FA on the Blog dataset.", "labels": [], "entities": [{"text": "AT-FA", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7745888829231262}, {"text": "Blog dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9889155924320221}]}, {"text": "DADT's PAN'11 testing result is close to the third-best accuracy from the PAN'11 competition (.", "labels": [], "entities": [{"text": "DADT's PAN'11 testing", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.7252103611826897}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9984084963798523}]}, {"text": "However, to the best of our knowledge, DADT obtained the best accuracy fora fully-supervised method that uses only unigram features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.999219536781311}]}, {"text": "Specifically,, who obtained the highest accuracy (65.8%), assumed that all the test texts are given to the classifier at the same time, and used this additional information with a semi-supervised method; while, who obtained the second-best (64.2%) and third-best (59.4%) accuracies respectively, used various feature types (e.g., features obtained from parse trees).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.997803270816803}]}, {"text": "Further, preprocessing differences make it hard to compare the methods on a level playing field.", "labels": [], "entities": []}, {"text": "Nonetheless, we note that extending DADT to enable semi-supervised classification and additional feature types are promising future work directions.", "labels": [], "entities": []}, {"text": "While all the methods yielded relatively low accuracies on Blog due to its size, topic-based methods were more strongly affected than SVM by the transition from the 1,000 author subset to the full dataset.", "labels": [], "entities": [{"text": "Blog", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9347003102302551}]}, {"text": "This is probably because topic-based methods use a single model, making them more sensitive to corpus size than SVM's one-versus-all setup that uses one model per author.", "labels": [], "entities": []}, {"text": "Notably, an oracle that chooses the correct answer between SVM and DADT when they disagree yields an accuracy of 37.15% on the full dataset, suggesting it is worthwhile to explore ensembles that combine the outputs of SVM and DADT (we tried using DADT topics as additional SVM features, but this did not outperform DADT).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9992189407348633}]}], "tableCaptions": []}