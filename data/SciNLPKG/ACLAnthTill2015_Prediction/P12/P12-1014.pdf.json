{"title": [], "abstractContent": [{"text": "Comprehending action preconditions and effects is an essential step in modeling the dynamics of the world.", "labels": [], "entities": []}, {"text": "In this paper, we express the semantics of precondition relations extracted from text in terms of planning operations.", "labels": [], "entities": []}, {"text": "The challenge of modeling this connection is to ground language at the level of relations.", "labels": [], "entities": []}, {"text": "This type of grounding enables us to create high-level plans based on language abstractions.", "labels": [], "entities": []}, {"text": "Our model jointly learns to predict precondition relations from text and to perform high-level planning guided by those relations.", "labels": [], "entities": []}, {"text": "We implement this idea in the reinforcement learning framework using feedback automatically obtained from plan execution attempts.", "labels": [], "entities": []}, {"text": "When applied to a complex virtual world and text describing that world, our relation extraction technique performs on par with a supervised baseline, yielding an F-measure of 66% compared to the baseline's 65%.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8113583028316498}, {"text": "F-measure", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9989738464355469}]}, {"text": "Additionally , we show that a high-level planner utilizing these extracted relations significantly outperforms a strong, text unaware baseline-successfully completing 80% of planning tasks as compared to 69% for the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding action preconditions and effects is a basic step in modeling the dynamics of the world.", "labels": [], "entities": []}, {"text": "For example, having seeds is a precondition for growing wheat.", "labels": [], "entities": []}, {"text": "Not surprisingly, preconditions have been extensively explored in various sub-fields of AI.", "labels": [], "entities": []}, {"text": "However, existing work on action models has largely focused on tasks and techniques specific to individual sub-fields with little or no interconnection between them.", "labels": [], "entities": []}, {"text": "In NLP, precondition relations have been studied in terms of the linguistic mechanisms The code, data and experimental setup for this work are available at http://groups.csail.mit.edu/rbg/code/planning A pickaxe, which is used to harvest stone, can be made from wood.", "labels": [], "entities": []}, {"text": "(a) Low Level Actions for: wood \u2192 pickaxe \u2192 stone step 1: move from (0,0) to (2,0) step 2: chop tree at: (2,0) step 3: get wood at: (2,0) step 4: craft plank from wood step 5: craft stick from plank step 6: craft pickaxe from plank and stick \u00b7 \u00b7 \u00b7 step N-1: pickup tool: pickaxe step N: harvest stone with pickaxe at: (5,5) (b) that realize them, while in classical planning, these relations are viewed as apart of world dynamics.", "labels": [], "entities": []}, {"text": "In this paper, we bring these two parallel views together, grounding the linguistic realization of these relations in the semantics of planning operations.", "labels": [], "entities": []}, {"text": "The challenge and opportunity of this fusion comes from the mismatch between the abstractions of human language and the granularity of planning primitives.", "labels": [], "entities": []}, {"text": "Consider, for example, text describing a virtual world such as Minecraft 2 and a formal description of that world using planning primitives.", "labels": [], "entities": []}, {"text": "Due to the mismatch in granularity, even the simple relations between wood, pickaxe and stone described in the sentence in results in dozens of lowlevel planning actions in the world, as can be seen in.", "labels": [], "entities": []}, {"text": "While the text provides a high-level description of world dynamics, it does not provide sufficient details for successful plan execution.", "labels": [], "entities": []}, {"text": "On the other hand, planning with low-level actions does not suffer from this limitation, but is computationally intractable for even moderately complex tasks.", "labels": [], "entities": []}, {"text": "As a consequence, in many practical domains, planning algorithms rely on manually-crafted high-level abstractions to make search tractable (.", "labels": [], "entities": []}, {"text": "The central idea of our work is to express the semantics of precondition relations extracted from text in terms of planning operations.", "labels": [], "entities": []}, {"text": "For instance, the precondition relation between pickaxe and stone described in the sentence in indicates that plans which involve obtaining stone will likely need to first obtain a pickaxe.", "labels": [], "entities": []}, {"text": "The novel challenge of this view is to model grounding at the level of relations, in contrast to prior work which focused on objectlevel grounding.", "labels": [], "entities": []}, {"text": "We build on the intuition that the validity of precondition relations extracted from text can be informed by the execution of a low-level planner.", "labels": [], "entities": []}, {"text": "This feedback can enable us to learn these relations without annotations.", "labels": [], "entities": []}, {"text": "Moreover, we can use the learned relations to guide a high level planner and ultimately improve planning performance.", "labels": [], "entities": []}, {"text": "We implement these ideas in the reinforcement learning framework, wherein our model jointly learns to predict precondition relations from text and to perform high-level planning guided by those relations.", "labels": [], "entities": []}, {"text": "For a given planning task and a set of candidate relations, our model repeatedly predicts a sequence of subgoals where each subgoal specifies an attribute of the world that must be made true.", "labels": [], "entities": []}, {"text": "It then asks the low-level planner to find a plan between each consecutive pair of subgoals in the sequence.", "labels": [], "entities": []}, {"text": "The observed feedback -whether the lowlevel planner succeeded or failed at each step -is utilized to update the policy for both text analysis and high-level planning.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.8293976783752441}]}, {"text": "We evaluate our algorithm in the Minecraft virtual world, using a large collection of user-generated online documents as our source of textual information.", "labels": [], "entities": [{"text": "Minecraft virtual world", "start_pos": 33, "end_pos": 56, "type": "DATASET", "confidence": 0.8790645003318787}]}, {"text": "Our results demonstrate the strength of our relation extraction technique -while using planning feedback as its only source of supervision, it achieves a precondition relation extraction accuracy on par with that of a supervised SVM baseline.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8766420185565948}, {"text": "relation extraction", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.6757138520479202}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9231730699539185}]}, {"text": "Specifically, it yields an F-score of 66% compared to the 65% of the baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9998581409454346}]}, {"text": "In addition, we show that these extracted relations can be used to improve the performance of a high-level planner.", "labels": [], "entities": []}, {"text": "As baselines for this evaluation, we employ the Metric-FF planner (), as well as a textunaware variant of our model.", "labels": [], "entities": []}, {"text": "Our results show that our text-driven high-level planner significantly outperforms all baselines in terms of completed planning tasks -it successfully solves 80% as compared to 41% for the Metric-FF planner and 69% for the text unaware variant of our model.", "labels": [], "entities": []}, {"text": "In fact, the performance of our method approaches that of an oracle planner which uses manually-annotated preconditions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets As the text description of our virtual world, we use documents from the Minecraft Wiki, 7 the most popular information source about the game.", "labels": [], "entities": [{"text": "Minecraft Wiki, 7", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.950116291642189}]}, {"text": "Our manually constructed seed grounding of predicates contains 74 entries, examples of which can be seen in.", "labels": [], "entities": []}, {"text": "We use this seed grounding to identify a set of 242 sentences that reference predicates in the Minecraft domain.", "labels": [], "entities": []}, {"text": "This results in a set of 694 Candidate Relations.", "labels": [], "entities": []}, {"text": "We also manually annotated the relations expressed in the text, identifying 94 of the Candidate Relations as valid.", "labels": [], "entities": []}, {"text": "Our corpus contains 979 unique word types and is composed of sentences with an average length of 20 words.", "labels": [], "entities": []}, {"text": "We test our system on a set of 98 problems that involve collecting resources and constructing objects in the Minecraft domain -for example, fishing, cooking and making furniture.", "labels": [], "entities": []}, {"text": "To assess the complexity of these tasks, we manually constructed high-level plans for these goals and solved them using the Metric-FF planner.", "labels": [], "entities": []}, {"text": "On average, the execu-   tion of the sequence of low-level plans takes 35 actions, with 3 actions for the shortest plan and 123 actions for the longest.", "labels": [], "entities": []}, {"text": "The average branching factor is 9.7, leading to an average search space of more than 10 34 possible action sequences.", "labels": [], "entities": []}, {"text": "For evaluation purposes we manually identify a set of Gold Relations consisting of all precondition relations that are valid in this domain, including those not discussed in the text.", "labels": [], "entities": []}, {"text": "Evaluation Metrics We use our manual annotations to evaluate the type-level accuracy of relation extraction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9878595471382141}, {"text": "relation extraction", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.8201502561569214}]}, {"text": "To evaluate our high-level planner, we use the standard measure adopted by the IPC.", "labels": [], "entities": [{"text": "IPC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.9684092402458191}]}, {"text": "This evaluation measure simply assesses whether the planner completes a task within a predefined time.", "labels": [], "entities": []}, {"text": "Baselines To evaluate the performance of our relation extraction, we compare against an SVM classifier 8 trained on the Gold Relations.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8168641030788422}]}, {"text": "We test the SVM baseline in a leave-one-out fashion.", "labels": [], "entities": [{"text": "SVM baseline", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.7658774554729462}]}, {"text": "To evaluate the performance of our text-aware high-level planner, we compare against five baselines.", "labels": [], "entities": []}, {"text": "The first two baselines -FF and No Textdo not use any textual information.", "labels": [], "entities": [{"text": "FF", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.8899036049842834}]}, {"text": "The FF baseline directly runs the Metric-FF planner on the given task, while the No Text baseline is a variant of our model that learns to plan in the reinforcement learning framework.", "labels": [], "entities": []}, {"text": "It uses the same state-level features", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A comparison of complexity between Minecraft  and some domains used in the IPC-2011 sequential satis- ficing track. In the Minecraft domain, the number of ob- jects, predicate types, and actions is significantly larger.", "labels": [], "entities": [{"text": "Minecraft", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.9135674834251404}]}, {"text": " Table 4: Percentage of tasks solved successfully by our  model and the baselines. All performance differences be- tween methods are statistically significant at p \u2264 .01.", "labels": [], "entities": []}]}