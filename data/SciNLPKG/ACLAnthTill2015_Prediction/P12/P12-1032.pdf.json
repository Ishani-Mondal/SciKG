{"title": [{"text": "Learning Translation Consensus with Structured Label Propagation \u2020Shujie", "labels": [], "entities": [{"text": "Learning Translation Consensus", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7600159744421641}, {"text": "Shujie", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.657436192035675}]}], "abstractContent": [{"text": "In this paper, we address the issue for learning better translation consensus in machine translation (MT) research, and explore the search of translation consensus from similar, rather than the same, source sentences or their spans.", "labels": [], "entities": [{"text": "learning better translation consensus in machine translation (MT)", "start_pos": 40, "end_pos": 105, "type": "TASK", "confidence": 0.7570480406284332}]}, {"text": "Unlike previous work on this topic, we formulate the problem as structured labeling over a much smaller graph, and we propose a novel structured label propagation for the task.", "labels": [], "entities": []}, {"text": "We convert such graph-based translation consensus from similar source strings into useful features both for n-best output re-ranking and for decoding algorithm.", "labels": [], "entities": []}, {"text": "Experimental results show that, our method can significantly improve machine translation performance on both IWSLT and NIST data, compared with a state-of-the-art baseline.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8001693487167358}, {"text": "IWSLT", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.8436562418937683}, {"text": "NIST data", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.874504804611206}]}], "introductionContent": [{"text": "Consensus in translation has\uf02a gained more and more attention in recent years.", "labels": [], "entities": [{"text": "Consensus in translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6762068867683411}]}, {"text": "The principle of consensus can be sketched as \"a translation candidate is deemed more plausible if it is supported by other translation candidates.\"", "labels": [], "entities": []}, {"text": "The actual formulation of the principle depends on whether the translation candidate is a complete sentence or just a span of it, whether the candidate is the same as or similar to the supporting candidates, and whether the supporting candidates come from the same or different MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 278, "end_pos": 280, "type": "TASK", "confidence": 0.9353529214859009}]}, {"text": "\uf02a This work has been done while the first author was visiting Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.8276354471842448}]}, {"text": "Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates.", "labels": [], "entities": [{"text": "Translation consensus", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9357596039772034}, {"text": "Bayes risk (MBR", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.8570612519979477}]}, {"text": "That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates.", "labels": [], "entities": []}, {"text": "These approaches include the work of, which re-ranks the nbest output of a MT decoder, and the work of and, which does MBR decoding for lattices and hypergraphs.", "labels": [], "entities": []}, {"text": "Others extend consensus among translations from the same MT system to those from different MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.82601398229599}]}, {"text": "Collaborative decoding () scores the translation of a source span by its n-gram similarity to the translations by other systems.", "labels": [], "entities": []}, {"text": "Hypothesis mixture decoding) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems.", "labels": [], "entities": [{"text": "Hypothesis mixture decoding", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6545923749605814}]}, {"text": "All these approaches are about utilizing consensus among translations for the same (span of) source sentence.", "labels": [], "entities": []}, {"text": "It should be noted that consensus among translations of similar source sentences/spans is also helpful for good candidate selection.", "labels": [], "entities": []}, {"text": "For the source (Chinese) span \"\u4e94\u767e \u5143 \u4ee5\u4e0b \u7684 \u8336 \", the MT system produced the correct translation for the second sentence, but it failed to do so for the first one.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.613520622253418}]}, {"text": "If the translation of the first sentence could take into consideration the translation of the second sentence, which is similar to but not exactly the same as the first one, the final translation output maybe improved.", "labels": [], "entities": []}, {"text": "Following this line of reasoning, a discriminative learning method is proposed to constrain the translation of an input sentence using the most similar translation examples from translation memory (TM) systems.", "labels": [], "entities": []}, {"text": "A classifier is applied to re-rank the n-best output of a decoder, taking as features the information about the agreement with those similar translation examples.", "labels": [], "entities": []}, {"text": "proposed a graph-based semi-supervised model to re-rank n-best translation output.", "labels": [], "entities": []}, {"text": "Note that these two attempts are about translation consensus for similar sentences, and about reranking of n-best output.", "labels": [], "entities": [{"text": "translation consensus", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.8932279348373413}]}, {"text": "It is still an open question whether translation consensus for similar sentences/spans can be applied to the decoding process.", "labels": [], "entities": [{"text": "translation consensus", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8509189188480377}]}, {"text": "Moreover, the method in is formulated as atypical and simple label propagation, which leads to very large graph, thus making learning and search inefficient.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7238994985818863}]}, {"text": "In this paper, we attempt to leverage translation consensus among similar (spans of) source sentences in bilingual training data, by a novel graph-based model of translation consensus.", "labels": [], "entities": [{"text": "translation consensus", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.9105653166770935}, {"text": "translation consensus", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.948394238948822}]}, {"text": "Unlike Alexandrescu and Kirchhoff (2009), we reformulate the task of seeking translation consensus among source sentences as structured labeling.", "labels": [], "entities": [{"text": "translation consensus among source sentences", "start_pos": 77, "end_pos": 121, "type": "TASK", "confidence": 0.8811771750450135}]}, {"text": "We propose a novel label propagation algorithm for structured labeling, which is much more efficient than simple label propagation, and derive useful MT decoder features out of it.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7205798923969269}, {"text": "label propagation", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7719016671180725}, {"text": "MT decoder", "start_pos": 150, "end_pos": 160, "type": "TASK", "confidence": 0.8291985094547272}]}, {"text": "We conduct experiments with IWSLT and NIST data, and experimental results show that, our method can improve the translation performance significantly on both data sets, compared with a state-of-the-art baseline.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.8507267236709595}, {"text": "NIST data", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9245150089263916}, {"text": "translation", "start_pos": 112, "end_pos": 123, "type": "TASK", "confidence": 0.9569498300552368}]}], "datasetContent": [{"text": "In this section, graph-based translation consensus is tested on the Chinese to English translation tasks.", "labels": [], "entities": [{"text": "translation consensus", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7299153506755829}, {"text": "Chinese to English translation tasks", "start_pos": 68, "end_pos": 104, "type": "TASK", "confidence": 0.6678135812282562}]}, {"text": "The evaluation method is the case insensitive IBM BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9346420764923096}]}, {"text": "Significant testing is carried out using bootstrap re-sampling method proposed by Koehn (2004) with a 95% confidence level.", "labels": [], "entities": []}, {"text": "We test our method with two data settings: one is IWSLT data set, the other is NIST data set.", "labels": [], "entities": [{"text": "IWSLT data set", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.8999837239583334}, {"text": "NIST data set", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9519378542900085}]}, {"text": "Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy ().", "labels": [], "entities": [{"text": "Bracketing Transduction Grammar", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6975027521451315}, {"text": "Dekai Wu, 1997) (BTG)", "start_pos": 87, "end_pos": 108, "type": "DATASET", "confidence": 0.5393867082893848}]}, {"text": "The features we used are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities.", "labels": [], "entities": [{"text": "BTG decoder", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9093469083309174}]}, {"text": "Our IWSLT data is the IWSLT 2009 dialog task data set.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9550983011722565}, {"text": "IWSLT 2009 dialog task data set", "start_pos": 22, "end_pos": 53, "type": "DATASET", "confidence": 0.9277766545613607}]}, {"text": "The training data include the BTEC and SLDB training data.", "labels": [], "entities": [{"text": "BTEC", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9533497095108032}, {"text": "SLDB training data", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.818399707476298}]}, {"text": "The training data contains 81k sentence pairs, 655k Chinese words and 806 English words.", "labels": [], "entities": []}, {"text": "The language model is 5-gram language model trained with the target sentences in the training data.", "labels": [], "entities": []}, {"text": "The test set is devset9, and the development set for MERT comprises both devset8 and the Chinese DIALOG set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.5729934573173523}, {"text": "Chinese DIALOG set", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.8405815164248148}]}, {"text": "The baseline results on IWSLT data are shown in.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.9410824775695801}]}, {"text": "Baselines for NIST data shows the performance of our consensusbased re-ranking and decoding on the IWSLT data set.", "labels": [], "entities": [{"text": "NIST data", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.9440423250198364}, {"text": "IWSLT data set", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9791619380315145}]}, {"text": "To perform consensus-based re-ranking, we first use the baseline decoder to get the n-best list for each sentence of development and test data, then we create graph using the n-best lists and training data as we described in section 5.1, and perform semi-supervised training as mentioned in section 4.3.", "labels": [], "entities": []}, {"text": "As we can see from, our consensus-based re-ranking (G-Re-Rank) outperforms the baseline significantly, not only for the development data, but also for the test data.", "labels": [], "entities": [{"text": "re-ranking", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.7831788659095764}, {"text": "G-Re-Rank", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.8954564929008484}]}, {"text": "Instead of using graph-based consensus confidence as features in the log-linear model, we perform structured label propagation (Struct-LP) to re-rank the n-best list directly, and the similarity measures for source sentences and translation candidates are symmetrical sentence level BLEU (equation  We use the baseline system to perform forced alignment procedure on the training data, and create span nodes using the derivation tree of the forced alignment.", "labels": [], "entities": [{"text": "structured label propagation", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.7024681170781454}, {"text": "BLEU", "start_pos": 283, "end_pos": 287, "type": "METRIC", "confidence": 0.9542727470397949}]}, {"text": "We also saved the spans of the sentences from development and test data, which will be used to create the responding nodes for consensus-based decoding.", "labels": [], "entities": []}, {"text": "In such away, we create the graph for decoding, and perform semi-supervised training to calculate graph-based consensus features, and tune the weights for all the features we used.", "labels": [], "entities": []}, {"text": "In, we can see that our consensus-based decoding (G-Decode) is much better than baseline, and also better than consensus-based re-ranking method.", "labels": [], "entities": []}, {"text": "That is reasonable since the neighbor/local similarity features not only re-rank the final n-best output, but also the spans during decoding.", "labels": [], "entities": []}, {"text": "To test the contribution of each kind of features, we first remove all the local consensus features and perform consensus-based re-ranking and decoding (G-Re-Rank-GC and G-Decode-GC), and then we remove all the graph-based consensus features to test the contribution of local consensus features (G-Re-Rank-LC and G-Decode-LC).", "labels": [], "entities": []}, {"text": "Without the graph-based consensus features, our consensus-based re-ranking and decoding is simplified into a consensus re-ranking and consensus decoding system, which only re-rank the candidates according to the consensus information of other candidates in the same n-best list.", "labels": [], "entities": []}, {"text": "From, we can see, the G-Re-Rank-LC and G-Decode-LC improve the performance of development data and test data, but not as much as G-Re-Rank and G-Decode do.", "labels": [], "entities": []}, {"text": "G-Re-Rank-GC and G-Decode-GC improve the performance of machine translation according to the baseline.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8186067342758179}]}, {"text": "GRe-Rank-GC does not achieve the same performance as G-Re-Rank-LC does.", "labels": [], "entities": []}, {"text": "Compared with G-Decode-LC, the performance with GDecode-GC is much better..", "labels": [], "entities": [{"text": "GDecode-GC", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.8655869364738464}]}, {"text": "Consensus-based re-ranking and decoding for NIST data set.", "labels": [], "entities": [{"text": "NIST data set", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9477886954943339}]}, {"text": "The results in bold type are significantly better than the baseline.", "labels": [], "entities": []}, {"text": "We also conduct experiments on NIST data, and results are shown in.", "labels": [], "entities": [{"text": "NIST data", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.9480818212032318}]}, {"text": "The consensus-based re-ranking methods are performed in the same way as for IWSLT data, but for consensus-based decoding, the data set contains too many sentence pairs to beheld in one graph for our machine.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.8785104155540466}]}, {"text": "We apply the method of to construct separate graphs for each development and test sentence without losing global connectivity information.", "labels": [], "entities": []}, {"text": "We perform modified label propagation with the separate graphs to get the graph-based consensus for n-best list of each sentence, and the graph-based consensus will be recorded for the MERT to tune the weights.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7567537724971771}, {"text": "MERT", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.5986382961273193}]}, {"text": "From, we can see that, Struct-LP improves the performance slightly, but not significantly.", "labels": [], "entities": []}, {"text": "Local consensus features (G-ReRank-LC and G-Decode-LC) improve the performance slightly.", "labels": [], "entities": []}, {"text": "The combination of graphbased and local consensus features can improve the translation performance significantly on SMT re-ranking.", "labels": [], "entities": [{"text": "SMT re-ranking", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.9202224612236023}]}, {"text": "With graph-based consensus features, G-Decode-GC achieves significant performance gain, and combined with local consensus features, G-Decode performance is improved farther.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Baselines for IWSLT data", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.8210999667644501}]}, {"text": " Table 2. Baselines for NIST data", "labels": [], "entities": [{"text": "NIST data", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.8737166821956635}]}, {"text": " Table 3. Consensus-based re-ranking and decoding  for IWSLT data set. The results in bold type are  significantly better than the baseline.", "labels": [], "entities": [{"text": "IWSLT data set", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9541646639506022}]}, {"text": " Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.", "labels": [], "entities": [{"text": "NIST data set", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9586170514424642}]}]}