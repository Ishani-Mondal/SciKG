{"title": [{"text": "Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing", "labels": [], "entities": [{"text": "Multiword Expression Recognition", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.7373145918051401}, {"text": "Parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.5522034168243408}]}], "abstractContent": [{"text": "The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.998224675655365}]}, {"text": "This paper evaluates two empirical strategies to integrate multiword units in areal constituency parsing context and shows that the results are not as promising as has sometimes been suggested.", "labels": [], "entities": [{"text": "areal constituency parsing context", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.727445125579834}]}, {"text": "Firstly, we show that pre-grouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score.", "labels": [], "entities": [{"text": "multiword recognition", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.8772503733634949}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9464076161384583}, {"text": "attachment score", "start_pos": 161, "end_pos": 177, "type": "METRIC", "confidence": 0.8135031461715698}]}, {"text": "However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing.", "labels": [], "entities": [{"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9985705614089966}, {"text": "multiword expression recognition", "start_pos": 85, "end_pos": 117, "type": "TASK", "confidence": 0.691644012928009}]}, {"text": "Secondly , integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "The integration of Multiword Expressions (MWE) in real-life applications is crucial because such expressions have the particularity of having a certain level of idiomaticity.", "labels": [], "entities": [{"text": "Multiword Expressions (MWE)", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6859976708889007}]}, {"text": "They form complex lexical units which, if they are considered, should significantly help parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 89, "end_pos": 96, "type": "TASK", "confidence": 0.9679182767868042}]}, {"text": "From a theoretical point of view, the integration of multiword expressions in the parsing procedure has been studied for different formalisms: Head-Driven Phrase Structure Grammar ( ), Tree Adjoining Grammars (), etc.", "labels": [], "entities": []}, {"text": "From an empirical point of view, their incorporation has also been considered such as in () for dependency parsing and in) in constituency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8579885065555573}, {"text": "constituency parsing", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.8502678573131561}]}, {"text": "Although experiments always relied on a corpus where the MWEs were perfectly pre-identified, they showed that pre-grouping such expressions could significantly improve parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.9663184881210327}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.8370263576507568}]}, {"text": "Recently, proposed integrating the multiword expressions directly in the grammar without pre-recognizing them.", "labels": [], "entities": []}, {"text": "The grammar was trained with a reference treebank where MWEs were annotated with a specific non-terminal node.", "labels": [], "entities": []}, {"text": "Our proposal is to evaluate two discriminative strategies in areal constituency parsing context: (a) pre-grouping MWE before parsing; this would be done with a state-of-the-art recognizer based on Conditional Random Fields; (b) parsing with a grammar including MWE identification and then reranking the output parses thanks to a Maximum Entropy model integrating MWE-dedicated features.", "labels": [], "entities": [{"text": "constituency parsing context", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.7519978483517965}, {"text": "MWE identification", "start_pos": 261, "end_pos": 279, "type": "TASK", "confidence": 0.7295065522193909}]}, {"text": "(a) is the direct realistic implementation of the standard approach that was shown to reach the best results (Arun and).", "labels": [], "entities": [{"text": "Arun", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9893850684165955}]}, {"text": "We will evaluate if real MWE recognition (MWER) still positively impacts parsing, i.e., whether incorrect MWER does not negatively impact the overall parsing system.", "labels": [], "entities": [{"text": "MWE recognition (MWER", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.8439932018518448}, {"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.982284665107727}]}, {"text": "(b) is a more innovative approach to MWER (despite not being new in parsing): we select the final MWE segmentation after parsing in order to explore as many parses as possible (as opposed to method (a)).", "labels": [], "entities": [{"text": "MWER", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.9445950984954834}]}, {"text": "The experiments were carried out on the French Treebank ( where MWEs are annotated.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9835494458675385}]}, {"text": "The paper is organized as follows: section 2 is an overview of the multiword expressions and their identification in texts; section 3 presents the two different strategies and their associated models; section 4 describes the resources used for our experiments (the corpus and the lexical resources); section 5 details the features that are incorporated in the models; section 6 reports on the results obtained.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out 3 different experiments.", "labels": [], "entities": []}, {"text": "We first tested a standalone MWE recognizer based on CRF.", "labels": [], "entities": [{"text": "MWE recognizer", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9437268376350403}]}, {"text": "We then combined MWE pregrouping based on this recognizer and the Berkeley parser) trained on the FTB where the compounds were concatenated (BKYc).", "labels": [], "entities": [{"text": "FTB", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.9754433631896973}, {"text": "BKYc", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.6444213390350342}]}, {"text": "Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker.", "labels": [], "entities": [{"text": "FTB", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9472283720970154}, {"text": "BKY", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9624658823013306}]}, {"text": "In all experiments, we varied the set of features: endo are all endogenous features; coll and lex include all endogenous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features.", "labels": [], "entities": []}, {"text": "The CRF recognizer relies on the software Wapiti 6 () to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources.", "labels": [], "entities": [{"text": "CRF recognizer", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9090495705604553}]}, {"text": "The part-of-speech tagger used to extract POS features was lgtagger 7 (Constant and Sigogne, 2011).", "labels": [], "entities": []}, {"text": "To train the reranker, we used a MaxEnt algorithm 8 as in.", "labels": [], "entities": []}, {"text": "Results are reported using several standard measures, the F 1 score, unlabeled attachment and Leaf Ancestor scores.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9844173789024353}, {"text": "attachment", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.606050431728363}, {"text": "Leaf Ancestor scores", "start_pos": 94, "end_pos": 114, "type": "METRIC", "confidence": 0.8208421468734741}]}, {"text": "The labeled F 1 score [F1] 9 , defined by the standard protocol called PARSEVAL (, takes into account the bracketing and labeling of nodes.", "labels": [], "entities": [{"text": "F 1 score [F1] 9", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.894920825958252}, {"text": "PARSEVAL", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8693559169769287}]}, {"text": "The unlabeled attachement score evaluates the quality of unlabeled We used the version adapted to French in the software We trained the parser as follows: right binarization, no parent annotation, six split-merge cycles and default random seed initialisation (8).", "labels": [], "entities": []}, {"text": "Wapiti can be found at http://wapiti.limsi.fr/.", "labels": [], "entities": []}, {"text": "It was configured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping criterion value (0.02%).", "labels": [], "entities": []}, {"text": "Available at http://igm.univmlv.fr/\u02dcmconstan/research/software/.", "labels": [], "entities": []}, {"text": "8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/ Evalb tool available at http://nlp.cs.nyu.edu/evalb/.", "labels": [], "entities": []}, {"text": "We also used the evaluation by category implemented in the class EvalbByCat in the Stanford Parser.", "labels": [], "entities": []}, {"text": "dependencies between words of the sentence . And finally, the Leaf-Ancestor score 11 computes the similarity between all paths (sequence of nodes) from each terminal node to the root node of the tree.", "labels": [], "entities": []}, {"text": "The global score of a generated parse is equal to the average score of all terminal nodes.", "labels": [], "entities": []}, {"text": "Punctuation tokens are ignored in all metrics.", "labels": [], "entities": []}, {"text": "The quality of MWE identification was evaluated by computing the F 1 score on MWE nodes.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.9645169675350189}, {"text": "F 1 score", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9824667175610861}]}, {"text": "We also evaluated the MWE segmentation by using the unlabeled F 1 score (U).", "labels": [], "entities": [{"text": "MWE segmentation", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.9626434743404388}, {"text": "F 1 score (U)", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9597276250521342}]}, {"text": "In order to compare both approaches, parse trees generated by BKYc were automatically transformed in trees with the same MWE annotation scheme as the trees generated by BKY.", "labels": [], "entities": [{"text": "BKYc", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.941009521484375}, {"text": "BKY", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.9663516879081726}]}, {"text": "In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples 12 . The statistical significance between two MWE identification experiments was established by using the McNemar-s test.", "labels": [], "entities": [{"text": "F 1", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9923568964004517}, {"text": "UAS", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9659098386764526}, {"text": "MWE identification", "start_pos": 225, "end_pos": 243, "type": "TASK", "confidence": 0.914342999458313}, {"text": "McNemar-s test", "start_pos": 285, "end_pos": 299, "type": "DATASET", "confidence": 0.7863087952136993}]}, {"text": "The results of the two experiments are considered statistically significant with the computed value p < 0.01.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Simple context-free application of the lexical  resources on the development corpus: T is the MWE lex- icon of the training corpus, L is the lefff, D is the Dela.  The given scores solely evaluate MWE segmentation and  not tagging.", "labels": [], "entities": [{"text": "MWE segmentation", "start_pos": 207, "end_pos": 223, "type": "TASK", "confidence": 0.8975471556186676}]}, {"text": " Table 3: MWE identification with CRF: base are the  features corresponding to token properties and word n- grams. The differences between all systems are statisti- cally significant with respect to McNemar's test", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9530632197856903}]}, {"text": " Table 4: Parsing evaluation: pre indicates a MWE pre- grouping strategy, whereas post is a reranking strategy  with n = 50. The feature gold means that we have ap- plied the parser on a gold MWE segmentation.", "labels": [], "entities": [{"text": "MWE pre- grouping", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.772983968257904}]}, {"text": " Table 6: Reranker F 1 evaluation with respect to n and the  types of features. The F 1 (MWE) is given in parenthesis.", "labels": [], "entities": [{"text": "Reranker F 1", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.5528020958105723}, {"text": "F 1 (MWE)", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9681466579437256}]}, {"text": " Table 7: Evaluation by category with respect to BKY  parser. The BKY column indicates the F 1 of BKY parser.", "labels": [], "entities": [{"text": "BKY", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.5579694509506226}, {"text": "F 1", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9846030175685883}, {"text": "BKY", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.9085524082183838}]}]}