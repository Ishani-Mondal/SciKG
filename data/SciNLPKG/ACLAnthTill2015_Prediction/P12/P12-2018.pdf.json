{"title": [{"text": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "labels": [], "entities": [{"text": "Topic Classification", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6801314800977707}]}], "abstractContent": [{"text": "Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/ dataset.", "labels": [], "entities": [{"text": "text classification", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.77510866522789}]}, {"text": "We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.9204732775688171}]}, {"text": "Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing anew state-of-the-art performance level.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8929665386676788}]}], "introductionContent": [{"text": "Naive Bayes (NB) and Support Vector Machine (SVM) models are often used as baselines for other methods in text categorization and sentiment analysis research.", "labels": [], "entities": [{"text": "text categorization and sentiment analysis research", "start_pos": 106, "end_pos": 157, "type": "TASK", "confidence": 0.714436799287796}]}, {"text": "However, their performance varies significantly depending on which variant, features and datasets are used.", "labels": [], "entities": []}, {"text": "We show that researchers have not paid sufficient attention to these model selection issues.", "labels": [], "entities": []}, {"text": "Indeed, we show that the better variants often outperform recently published state-of-the-art methods on many datasets.", "labels": [], "entities": []}, {"text": "We attempt to categorize which method, which variants and which features perform better under which circumstances.", "labels": [], "entities": []}, {"text": "First, we make an important distinction between sentiment classification and topical text classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.951617032289505}, {"text": "topical text classification", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.7074182828267416}]}, {"text": "We show that the usefulness of bigram features in bag of features sentiment classification has been underappreciated, perhaps because their usefulness is more of a mixed bag for topical text classification tasks.", "labels": [], "entities": [{"text": "bag of features sentiment classification", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.6462295949459076}, {"text": "topical text classification tasks", "start_pos": 178, "end_pos": 211, "type": "TASK", "confidence": 0.7038569450378418}]}, {"text": "We then distinguish between short snippet sentiment tasks and longer reviews, showing that for the former, NB outperforms SVMs.", "labels": [], "entities": []}, {"text": "Contrary to claims in the literature, we show that bag of features models are still strong performers on snippet sentiment classification tasks, with NB models generally outperforming the sophisticated, structure-sensitive models explored in recent work.", "labels": [], "entities": [{"text": "snippet sentiment classification tasks", "start_pos": 105, "end_pos": 143, "type": "TASK", "confidence": 0.759061872959137}]}, {"text": "Furthermore, by combining generative and discriminative classifiers, we present a simple model variant where an SVM is built over NB log-count ratios as feature values, and show that it is a strong and robust performer overall the presented tasks.", "labels": [], "entities": []}, {"text": "Finally, we confirm the wellknown result that MNB is normally better and more stable than multivariate Bernoulli NB, and the increasingly known result that binarized MNB is better than standard MNB.", "labels": [], "entities": []}, {"text": "The code and datasets to reproduce the results in this paper are publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare with published results on the following datasets.", "labels": [], "entities": []}, {"text": "Detailed statistics are shown in table 1.", "labels": [], "entities": []}, {"text": "RT-s: Short movie reviews dataset containing one sentence per review (Pang and).", "labels": [], "entities": [{"text": "Pang", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8032231330871582}]}, {"text": "Subj: The subjectivity dataset with subjective reviews and objective plot summaries ().", "labels": [], "entities": []}, {"text": "RT-2k: The standard 2000 full-length movie review dataset ().", "labels": [], "entities": [{"text": "RT-2k", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.810335636138916}, {"text": "movie review dataset", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.6335942248503367}]}, {"text": "IMDB: A large movie review dataset with 50k fulllength reviews).", "labels": [], "entities": [{"text": "IMDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8826098442077637}, {"text": "movie review dataset", "start_pos": 14, "end_pos": 34, "type": "DATASET", "confidence": 0.5930021504561106}]}, {"text": "4 AthR, XGraph, BbCrypt: Classify pairs of newsgroups in the 20-newsgroups dataset with all headers stripped off (the third   We use the provided tokenizations when they exist.", "labels": [], "entities": [{"text": "BbCrypt", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.8626666069030762}]}, {"text": "If not, we split at spaces for unigrams, and we filter out anything that is not [A-Za-z] for bigrams.", "labels": [], "entities": []}, {"text": "We do not use stopwords, lexicons or other resources.", "labels": [], "entities": []}, {"text": "All results reported use \u03b1 = 1, C = 1, \u03b2 = 0.25 for NBSVM, and C = 0.1 for SVM.", "labels": [], "entities": [{"text": "NBSVM", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8970916271209717}]}, {"text": "For comparison with other published results, we use either 10-fold cross-validation or train/test split depending on what is standard for the dataset.", "labels": [], "entities": []}, {"text": "The CV column of table 1 specifies what is used.", "labels": [], "entities": []}, {"text": "The standard splits are used when they are available.", "labels": [], "entities": []}, {"text": "The approximate upper-bounds on the difference required to be statistically significant at the p < 0.05 level are listed in table 1, column \u2206.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics. (N + , N \u2212 ): number of  positive and negative examples. l: average num- ber of words per example. CV: number of cross- validation splits, or N for train/test split. |V |: the  vocabulary size. \u2206: upper-bounds of the differences  required to be statistically significant at the p < 0.05  level.", "labels": [], "entities": [{"text": "\u2206", "start_pos": 224, "end_pos": 225, "type": "METRIC", "confidence": 0.9678035974502563}]}, {"text": " Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert", "labels": [], "entities": []}, {"text": " Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. \u2206t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.\u2206idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).", "labels": [], "entities": [{"text": "IMDB", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.8940590023994446}, {"text": "BoWSVM", "start_pos": 339, "end_pos": 345, "type": "DATASET", "confidence": 0.8536199927330017}, {"text": "Ap", "start_pos": 479, "end_pos": 481, "type": "METRIC", "confidence": 0.941898763179779}]}, {"text": " Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (", "labels": [], "entities": [{"text": "DiscLDA", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8825803995132446}]}]}