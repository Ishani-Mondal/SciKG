{"title": [{"text": "Topic Models, Latent Space Models, Sparse Coding, and All That: A systematic understanding of probabilistic semantic extraction in large corpus", "labels": [], "entities": [{"text": "Sparse Coding", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8276195526123047}, {"text": "probabilistic semantic extraction", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.6932590901851654}]}], "abstractContent": [{"text": "Probabilistic topic models have recently gained much popularity in informational retrieval and related areas.", "labels": [], "entities": [{"text": "informational retrieval", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.804282158613205}]}, {"text": "Via such models , one can project high-dimensional objects such as text documents into a low dimensional space where their latent semantics are captured and modeled; can integrate multiple sources of information-to \"share statistical strength\" among components of a hierarchical probabilistic model; and can structurally display and classify the otherwise unstructured object collections.", "labels": [], "entities": []}, {"text": "However, to many practitioners , how topic models work, what to and not to expect from a topic model, how is it different from and related to classical matrix algebraic techniques such as LSI, NMF in NLP, how to empower topic models to deal with complex scenarios such as multimodal data, contractual text in social media, evolving corpus , or presence of supervision such as labeling and rating, how to make topic mod-eling computationally tractable even on web-scale data, etc., in a principled way, remain unclear.", "labels": [], "entities": [{"text": "labeling and rating", "start_pos": 376, "end_pos": 395, "type": "TASK", "confidence": 0.790003220240275}]}, {"text": "In this tutorial, I will demystify the conceptual , mathematical, and computational issues behind all such problems surrounding the topic models and their applications by presenting a systematic overview of the mathematical foundation of topic modeling, and its connections to a number of related methods popular in other fields such as the LDA, admix-ture model, mixed membership model, latent space models, and sparse coding.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 238, "end_pos": 252, "type": "TASK", "confidence": 0.7076896280050278}]}, {"text": "I will offer a simple and unifying view of all these techniques under the framework multi-view latent space embedding, and online the roadmap of model extension and algorithmic design toward different applications in IR and NLP.", "labels": [], "entities": [{"text": "model extension", "start_pos": 145, "end_pos": 160, "type": "TASK", "confidence": 0.7934646606445312}, {"text": "IR", "start_pos": 217, "end_pos": 219, "type": "TASK", "confidence": 0.9457044005393982}]}, {"text": "A main theme of this tutorial that tie together a wide range of issues and problems will build on the \"probabilistic graphical model\" formalism , a formalism that exploits the conjoined talents of graph theory and probability theory to build complex models out of simpler pieces.", "labels": [], "entities": []}, {"text": "I will use this formalism as a main aid to discuss both the mathematical underpinnings for the models and the related computational issues in a unified, simplistic, transparent, and actionable fashion.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}