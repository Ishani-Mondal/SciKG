{"title": [{"text": "Tokenization: Returning to a Long Solved Problem A Survey, Contrastive Experiment, Recommendations, and Toolkit", "labels": [], "entities": []}], "abstractContent": [{"text": "We examine some of the frequently disregarded subtleties of tokenization in Penn Tree-bank style, and present anew rule-based pre-processing toolkit that not only reproduces the Treebank tokenization with unmatched accuracy , but also maintains exact stand-off pointers to the original text and allows flexible configuration to diverse use cases (e.g. to genre-or domain-specific idiosyncrasies).", "labels": [], "entities": [{"text": "Penn Tree-bank style", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.9751227299372355}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9946423768997192}]}], "introductionContent": [], "datasetContent": [{"text": "To get an overview of current tokenization methods, we recovered and tokenized the raw text which was the source of the (Wall Street Journal portion of the) PTB, and compared it to the gold tokenization in the syntactic annotation in the treebank.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the) PTB", "start_pos": 121, "end_pos": 160, "type": "DATASET", "confidence": 0.9545425176620483}]}, {"text": "We used three common methods of tokenization: (a) the original See http://www.cis.upenn.edu/~treebank/ tokenization.html for available 'documentation' and a sed script for PTB-style tokenization.", "labels": [], "entities": [{"text": "PTB-style tokenization", "start_pos": 172, "end_pos": 194, "type": "TASK", "confidence": 0.5761534124612808}]}, {"text": "3 observe that tokenizing with the GE-NIA tagger yields mismatches in one of five sentences of the GENIA Treebank, although the GENIA guidelines refer to scripts that maybe available on request).", "labels": [], "entities": [{"text": "tokenizing", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.9641571640968323}, {"text": "GE-NIA tagger", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8667478263378143}, {"text": "GENIA Treebank", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9795168936252594}, {"text": "GENIA", "start_pos": 128, "end_pos": 133, "type": "DATASET", "confidence": 0.9262572526931763}]}, {"text": "The original WSJ text was last included with the 1995 release of the PTB (LDC #95T07) and required alignment with the treebank, with some manual correction so that the same text is represented in both raw and parsed formats.", "labels": [], "entities": [{"text": "WSJ text", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8292571604251862}, {"text": "PTB (LDC #95T07)", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.91435573498408}]}, {"text": "PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools 5 ; and (c) tokenization from the parser of. shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance) over tokens (for a total of 49,208 sentences and 1,173,750 gold-standard tokens).", "labels": [], "entities": [{"text": "PTB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8720374703407288}, {"text": "Stanford CoreNLP tools 5", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.7662796080112457}, {"text": "PTB", "start_pos": 194, "end_pos": 197, "type": "DATASET", "confidence": 0.8828040361404419}]}, {"text": "Looking at the differences qualitatively, the most consistent issue across all tokenization methods was ambiguity of sentence-final periods.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.9620424509048462}]}, {"text": "In the treebank, final periods are always (with about 10 exceptions) a separate token.", "labels": [], "entities": []}, {"text": "If the sentence ends in U.S. (but not other abbreviations, oddly), an extra period is hallucinated, so the abbreviation also has one.", "labels": [], "entities": []}, {"text": "In contrast, C&J add a period to all final abbreviations, CoreNLP groups the final period with a final abbreviation and hence lacks a sentence-final period token, and the sed script strips the period off U.S. The 'correct' choice in this case is not obvious and will depend on how the tokens are to be used.", "labels": [], "entities": []}, {"text": "The majority of the discrepancies in the sed script tokenization come from an under-restricted punctuation rule that incorrectly splits on commas within numbers or ampersands within names.", "labels": [], "entities": []}, {"text": "Other than that, the problematic cases are mostly shared across tokenization methods, and include issues with currencies, Irish names, hyphenization, and quote disambiguation.", "labels": [], "entities": [{"text": "quote disambiguation", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.7177790999412537}]}, {"text": "Expression-Based Pre-Processing)-essentially a cascade of ordered finite-state string rewriting rules, though transcending the formal complexity of regular languages by inclusion of (a) full perl-compatible regular expressions and (b) fixpoint iteration over groups of rules.", "labels": [], "entities": []}, {"text": "In this approach, a first phase of string-level substitutions inserts whitespace around, for example, punctuation marks; upon completion of string rewriting, token boundaries are stipulated between all whitespace-separated substrings (and only these).", "labels": [], "entities": []}, {"text": "For a good balance of human and machine readability, REPP tokenization rules are specified in a simple, line-oriented textual form.", "labels": [], "entities": [{"text": "REPP tokenization", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7299543023109436}]}, {"text": "shows a (simplified) excerpt from our PTB-style tokenizer, where the first character on each line is one of four REPP operators, as follows: (a) '#' for group formation; (b) '>' for group invocation, (c) '!'", "labels": [], "entities": []}, {"text": "for substitution (allowing capture groups), and (d) ':' for token boundary detection.", "labels": [], "entities": [{"text": "token boundary detection", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7467938860257467}]}, {"text": "In, the two rules stripping off prefix and suffix punctuation marks adjacent to whitespace (i.e. matching the tab-separated left-hand side of the rule, to replace the match with its right-hand side) form a numbered group ('#1'), which will be iterated when called ('>1') until none of the rules in the group fires (a fixpoint).", "labels": [], "entities": []}, {"text": "In this example, conditioning on whitespace adjacency avoids the issues observed with the PTB sed script (e.g. token boundaries within comma-separated numbers) and also protects against infinite loops in the group.", "labels": [], "entities": []}, {"text": "REPP rule sets can be organized as modules, typ-6 Strictly speaking, there are another two operators, for lineoriented comments and automated versioning of rule files.", "labels": [], "entities": []}, {"text": "For this example, the same effects seemingly could be obtained without iteration (using greatly more complex rules); our actual, non-simplified rules, however, further deal with punctuation marks that can function as prefixes or suffixes, as well as with corner cases like factor(s) or Ca.", "labels": [], "entities": []}, {"text": "Also in mark-up removal and normalization, we have found it necessary to 'parse' nested structures by means of iterative groups.", "labels": [], "entities": [{"text": "mark-up removal", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.778423547744751}]}, {"text": "ically each in a file of its own, and invoked selectively by name (e.g. '>wiki' in; to date, there exist modules for quote disambiguation, (relevant subsets of) various mark-up languages (HTML, LA T E X, wiki, and XML), and a handful of robustness rules (e.g. seeking to identify and repair 'sandwiched' inter-token punctuation).", "labels": [], "entities": [{"text": "quote disambiguation", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.747802346944809}]}, {"text": "Individual tokenizers are configured at run-time, by selectively activating a set of modules (through command-line options).", "labels": [], "entities": []}, {"text": "An open-source reference implementation of the REPP framework (in C ++ ) is available, together with a library of modules for English.", "labels": [], "entities": []}, {"text": "In our own work on preparing various (non-PTB) genres for parsing, we devised a set of REPP rules with the goal of following the PTB conventions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9772646427154541}]}, {"text": "When repeating the experiment of \u00a7 3 above using REPP tokenization, we obtained an initial difference in 1505 sentences, with a Levenshtein dis-tance of 3543 (broadly comparable to CoreNLP, if marginally more accurate).", "labels": [], "entities": [{"text": "Levenshtein dis-tance", "start_pos": 128, "end_pos": 149, "type": "METRIC", "confidence": 0.8216812312602997}]}, {"text": "Examining these discrepancies, we revealed some deficiencies in our rules, as well as some peculiarities of the 'raw' Wall Street Journal text from the PTB distribution.", "labels": [], "entities": [{"text": "Wall Street Journal text from the PTB distribution", "start_pos": 118, "end_pos": 168, "type": "DATASET", "confidence": 0.9080532416701317}]}, {"text": "A little more than 200 mismatches were owed to improper treatment of currency symbols (AU$) and decade abbreviations ('60s), which led to the refinement of two existing rules.", "labels": [], "entities": [{"text": "AU$)", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8312878906726837}]}, {"text": "Notable PTB idiosyncrasies (in the sense of deviations from common typography) include ellipses with spaces separating the periods and a fairly large number of possessives ('s) being separated from their preceding token.", "labels": [], "entities": []}, {"text": "Other aspects of gold-standard PTB tokenization we consider unwarranted 'damage' to the input text, such as hallucinating an extra period after U.S. and splitting cannot (which adds spurious ambiguity).", "labels": [], "entities": [{"text": "PTB tokenization", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6339671909809113}]}, {"text": "For use cases where the goal were strict compliance, for instance in pre-processing inputs fora PTB-derived parser, we added an optional REPP module (of currently half a dozen rules) to cater to these corner cases-in a spirit similar to the CoreNLP mode we used in \u00a7 3.", "labels": [], "entities": [{"text": "REPP", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9367101788520813}]}, {"text": "With these extra rules, remaining tokenization discrepancies are contained in 603 sentences (just over 1 %), which gives a Levenshtein distance of 1389.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 123, "end_pos": 143, "type": "METRIC", "confidence": 0.8272970616817474}]}], "tableCaptions": [{"text": " Table 1: Quantitative view on tokenization differences.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9818809032440186}]}]}