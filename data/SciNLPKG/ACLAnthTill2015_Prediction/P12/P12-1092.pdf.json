{"title": [{"text": "Improving Word Representations via Global Context and Multiple Word Prototypes", "labels": [], "entities": [{"text": "Improving Word Representations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.906026562054952}]}], "abstractContent": [{"text": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.", "labels": [], "entities": []}, {"text": "However, most of these models are built with only local context and one representation per word.", "labels": [], "entities": []}, {"text": "This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings.", "labels": [], "entities": [{"text": "learning word meanings", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.732541561126709}]}, {"text": "We present anew neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word.", "labels": [], "entities": []}, {"text": "We introduce anew dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper-forms competitive baselines and other neural language models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector-space models (VSM) represent word meanings with vectors that capture semantic and syntactic information of words.", "labels": [], "entities": []}, {"text": "These representations can be used to induce similarity measures by computing distances between the vectors, leading to many useful applications, such as information retrieval, document classification) and question answering (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.8137944936752319}, {"text": "document classification", "start_pos": 176, "end_pos": 199, "type": "TASK", "confidence": 0.8029571175575256}, {"text": "question answering", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.9408825933933258}]}, {"text": "Despite their usefulness, most VSMs share a common problem that each word is only represented with one vector, which clearly fails to capture homonymy and polysemy.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9164743423461914}]}, {"text": "introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6786493460337321}]}, {"text": "However, in order to cluster accurately, it is important to capture both the syntax and semantics of words.", "labels": [], "entities": []}, {"text": "While many approaches use local contexts to disambiguate word meaning, global contexts can also provide useful topical information).", "labels": [], "entities": []}, {"text": "Several studies in psychology have also shown that global context can help language comprehension () and acquisition ().", "labels": [], "entities": []}, {"text": "We introduce anew neural-network-based language model that distinguishes and uses both local and global context via a joint training objective.", "labels": [], "entities": []}, {"text": "The model learns word representations that better capture the semantics of words, while still keeping syntactic information.", "labels": [], "entities": []}, {"text": "These improved representations can be used to represent contexts for clustering word instances, which is used in the multi-prototype version of our model that accounts for words with multiple senses.", "labels": [], "entities": []}, {"text": "We evaluate our new model on the standard WordSim-353 () dataset that includes human similarity judgments on pairs of words, showing that combining both local and global context outperforms using only local or global context alone, and is competitive with stateof-the-art methods.", "labels": [], "entities": [{"text": "WordSim-353 () dataset", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.8807680010795593}]}, {"text": "However, one limitation of this evaluation is that the human judgments are on pairs  of words presented in isolation, ignoring meaning variations in context.", "labels": [], "entities": []}, {"text": "Since word interpretation in context is important especially for homonymous and polysemous words, we introduce anew dataset with human judgments on similarity between pairs of words in sentential context.", "labels": [], "entities": [{"text": "word interpretation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7271607965230942}]}, {"text": "To capture interesting word pairs, we sample different senses of words using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.977061927318573}]}, {"text": "The dataset includes verbs and adjectives, in addition to nouns.", "labels": [], "entities": []}, {"text": "We show that our multi-prototype model improves upon the single-prototype version and outperforms other neural language models and baselines on this dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first present a qualitative analysis comparing the nearest neighbors of our model's embeddings with those of others, showing our embeddings better capture the semantics of words, with the use of global context.", "labels": [], "entities": []}, {"text": "Our model also improves the correlation with human judgments on a word similarity task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7871840298175812}]}, {"text": "Because word interpretation in context is important, we introduce anew dataset with human judgments on similarity of pairs of words in sentential context.", "labels": [], "entities": [{"text": "word interpretation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7281167358160019}]}, {"text": "Finally, we show that our model outperforms other methods on this dataset and also that the multi-prototype approach improves over the singleprototype approach.", "labels": [], "entities": []}, {"text": "We chose Wikipedia as the corpus to train all models because of its wide range of topics and word usages, and its clean organization of document by topic.", "labels": [], "entities": []}, {"text": "We used the April 2010 snapshot of the Wikipedia corpus (, with a total of about 2 million articles and 990 million tokens.", "labels": [], "entities": [{"text": "April 2010 snapshot of the Wikipedia corpus", "start_pos": 12, "end_pos": 55, "type": "DATASET", "confidence": 0.651314011641911}]}, {"text": "We use a dictionary of the 30,000 most frequent words in Wikipedia, converted to lowercase.", "labels": [], "entities": []}, {"text": "In preprocessing, we keep the frequent numbers intact and replace each digit of the uncommon numbers to \"DG\" so as to preserve information such as it being a year (e.g. \"DGDGDGDG\").", "labels": [], "entities": []}, {"text": "The converted numbers that are rare are mapped to a NUM-BER token.", "labels": [], "entities": [{"text": "NUM-BER token", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.8559130132198334}]}, {"text": "Other rare words not in the dictionary are mapped to an UNKNOWN token.", "labels": [], "entities": [{"text": "UNKNOWN token", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.8518310487270355}]}, {"text": "For all experiments, our models use 50-dimensional embeddings.", "labels": [], "entities": []}, {"text": "We use 10-word windows of text as the local context, 100 hidden units, and no weight regularization for both neural networks.", "labels": [], "entities": []}, {"text": "For multi-prototype variants, we fix the number of prototypes to be 10.", "labels": [], "entities": []}, {"text": "In order to show that our model learns more semantic word representations with global context, we give the nearest neighbors of our single-prototype model versus C&W's, which only uses local context.", "labels": [], "entities": []}, {"text": "The nearest neighbors of a word are computed by comparing the cosine similarity between the center word and all other words in the dictionary.", "labels": [], "entities": []}, {"text": "shows the nearest neighbors of some words.", "labels": [], "entities": []}, {"text": "The nearest neighbors of \"market\" that C&W's embeddings give are more constrained by the syntactic constraint that words in plural form are only close to other words in plural form, whereas our model captures that the singular and plural forms of a word are similar in meaning.", "labels": [], "entities": []}, {"text": "Other examples show that our model induces nearest neighbors that better capture semantics.", "labels": [], "entities": []}, {"text": "shows the nearest neighbors of our model using the multi-prototype approach.", "labels": [], "entities": []}, {"text": "We see that the clustering is able to group contexts of different: Nearest neighbors of word embeddings learned by our model using the multi-prototype approach based on cosine similarity.", "labels": [], "entities": []}, {"text": "The clustering is able to find the different meanings, usages, and parts of speech of the words.", "labels": [], "entities": []}, {"text": "meanings of a word into separate groups, allowing our model to learn multiple meaningful representations of a word.", "labels": [], "entities": []}, {"text": "The many previous datasets that associate human judgments on similarity between pairs of words, such as WordSim-353, MC and RG (, have helped to advance the development of vectorspace models.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.9739087224006653}]}, {"text": "However, common to all datasets is that similarity scores are given to pairs of words in isolation.", "labels": [], "entities": []}, {"text": "This is problematic because the meanings of homonymous and polysemous words depend highly on the words' contexts.", "labels": [], "entities": []}, {"text": "For example, in the two phrases, \"he swings the baseball bat\" and \"the Word 2 Located downtown along the east bank of the Des Moines River ...", "labels": [], "entities": [{"text": "Des Moines River", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.845939040184021}]}, {"text": "This is the basis of all money laundering , a track record of depositing clean money before slipping through dirty money ...", "labels": [], "entities": [{"text": "money laundering", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7645114958286285}]}, {"text": "Inside the ruins , there are bats and a bowl with Pokeys that fills with sand over the course of the race , and the music changes somewhat while inside ...", "labels": [], "entities": []}, {"text": "An aggressive lower order batsman who usually bats at No. 11 , Muralitharan is known for his tendency to back away to leg and slog ...", "labels": [], "entities": []}, {"text": "An example of legacy left in the Mideast from these nobles is the Krak des Chevaliers ' enlargement by the Counts of Tripoli and Toulouse ...", "labels": [], "entities": [{"text": "Krak des Chevaliers", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.8113682071367899}]}, {"text": "one should not adhere to a particular explanation , only in such measure as to be ready to abandon it if it be proved with certainty to be false ...", "labels": [], "entities": []}, {"text": "... and Andy 's getting ready to pack his bags and head up to Los Angeles tomorrow to get ready to fly back home on Thursday ...", "labels": [], "entities": []}, {"text": "she encounters Ben ( Duane Jones ) , who arrives in a pickup truck and defends the house against another pack of zombies ...", "labels": [], "entities": []}, {"text": "In practice , there is an unknown phase delay between the transmitter and receiver that must be compensated by \" synchronization \" of the receivers local oscillator ...", "labels": [], "entities": []}, {"text": "but Gilbert did not believe that she was dedicated enough , and when she missed a rehearsal , she was dismissed ...: Example pairs from our new dataset.", "labels": [], "entities": []}, {"text": "Note that words in a pair can be the same word and have different parts of speech.", "labels": [], "entities": []}, {"text": "bat flies\", bat has completely different meanings.", "labels": [], "entities": []}, {"text": "It is unclear how this variation in meaning is accounted for inhuman judgments of words presented without context.", "labels": [], "entities": []}, {"text": "One of the main contributions of this paper is the creation of anew dataset that addresses this issue.", "labels": [], "entities": []}, {"text": "The dataset has three interesting characteristics: 1) human judgments are on pairs of words presented in sentential context, 2) word pairs and their contexts are chosen to reflect interesting variations in meanings of homonymous and polysemous words, and 3) verbs and adjectives are present in addition to nouns.", "labels": [], "entities": []}, {"text": "We now describe our methodology in constructing the dataset.", "labels": [], "entities": []}, {"text": "Our procedure of constructing the dataset consists of three steps: 1) select a list a words, 2) for each word, select another word to form a pair, 3) for each word in a pair, find a sentential context.", "labels": [], "entities": []}, {"text": "We now describe each step in detail.", "labels": [], "entities": []}, {"text": "In step 1, in order to make sure we select a diverse list of words, we consider three attributes of a word: frequency in a corpus, number of parts of speech, and number of synsets according to WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 193, "end_pos": 200, "type": "DATASET", "confidence": 0.9768313765525818}]}, {"text": "For frequency, we divide words into three groups, top 2,000 most frequent, between 2,000 and 5,000, and between 5,000 to 10,000 based on occurrences in Wikipedia.", "labels": [], "entities": [{"text": "frequency", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9591018557548523}]}, {"text": "For number of parts of speech, we group words based on their number of possible parts of speech (noun, verb or adjective), from 1 to 3.", "labels": [], "entities": []}, {"text": "We also group words by their number of synsets:,,, and.", "labels": [], "entities": []}, {"text": "Finally, we sample at most 15 words from each combination in the Cartesian product of the above groupings.", "labels": [], "entities": []}, {"text": "In step 2, for each of the words selected in step 1, we want to choose the other word so that the pair captures an interesting relationship.", "labels": [], "entities": []}, {"text": "Similar to, we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9466004967689514}]}, {"text": "We randomly select a word from this set of words as the second word in the pair.", "labels": [], "entities": []}, {"text": "We try to repeat the above twice to generate two pairs for each word.", "labels": [], "entities": []}, {"text": "In addition, for words with more than five synsets, we allow the second word to be the same as the first, but with different synsets.", "labels": [], "entities": []}, {"text": "We end up with pairs of words as well as the one chosen synset for each word in the pairs.", "labels": [], "entities": []}, {"text": "In step 3, we aim to extract a sentence from Wikipedia for each word, which contains the word and corresponds to a usage of the chosen synset.", "labels": [], "entities": []}, {"text": "We first find all sentences in which the word occurs.", "labels": [], "entities": []}, {"text": "We then POS tag 2 these sentences and filter out those that do not match the chosen POS.", "labels": [], "entities": []}, {"text": "To find the Model \u03c1 \u00d7 100 C&W-S 57.0 Our Model-S 58.6 Our Model-M AvgSim 62.8 Our Model-M AvgSimC 65.7 tf-idf-S 26.3 Pruned tf-idf-S 62.5 Pruned tf-idf-M AvgSim 60.4 Pruned tf-idf-M AvgSimC 60.5: Spearman's \u03c1 correlation on our new dataset.", "labels": [], "entities": []}, {"text": "Our Model-S uses the single-prototype approach, while Our Model-M uses the multi-prototype approach.", "labels": [], "entities": []}, {"text": "AvgSim calculates similarity with each prototype contributing equally, while AvgSimC weighs the prototypes according to probability of the word belonging to that prototype's cluster.", "labels": [], "entities": [{"text": "similarity", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9869374632835388}]}, {"text": "word usages that correspond to the chosen synset, we first construct a set of related words of the chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes.", "labels": [], "entities": []}, {"text": "Using this set of related words, we filter out a sentence if the document in which the sentence appears does not include one of the related words.", "labels": [], "entities": []}, {"text": "Finally, we randomly select one sentence from those that are left.", "labels": [], "entities": []}, {"text": "shows some examples from the dataset.", "labels": [], "entities": []}, {"text": "Note that the dataset also includes pairs of the same word.", "labels": [], "entities": []}, {"text": "Single-prototype models would give the max similarity score for those pairs, which can be problematic depending on the words' contexts.", "labels": [], "entities": [{"text": "max similarity score", "start_pos": 39, "end_pos": 59, "type": "METRIC", "confidence": 0.90402219692866}]}, {"text": "This dataset requires models to examine context when determining word meaning.", "labels": [], "entities": []}, {"text": "Using Amazon Mechanical Turk, we collected 10 human similarity ratings for each pair, as found that 10 non-expert annotators can achieve very close inter-annotator agreement with expert raters.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 6, "end_pos": 28, "type": "DATASET", "confidence": 0.9183801015218099}]}, {"text": "To ensure worker quality, we only allowed workers with over 95% approval rate to work on our task.", "labels": [], "entities": [{"text": "approval rate", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9227275252342224}]}, {"text": "Furthermore, we discarded all ratings by a worker if he/she entered scores out of the accepted range or missed a rating, signaling lowquality work.", "labels": [], "entities": []}, {"text": "We obtained a total of 2,003 word pairs and their sentential contexts.", "labels": [], "entities": []}, {"text": "The word pairs consist of 1,712 unique words.", "labels": [], "entities": []}, {"text": "Of the 2,003 word pairs, 1328 are noun-noun pairs, 399 verb-verb, 140 verb-noun, 97 adjective-adjective, 30 noun-adjective, and 9 verbadjective.", "labels": [], "entities": []}, {"text": "241 pairs are same-word pairs.", "labels": [], "entities": []}, {"text": "For evaluation, we also compute Spearman correlation between a model's computed similarity scores and human judgments.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.9363488554954529}]}, {"text": "compares different models' results on this dataset.", "labels": [], "entities": []}, {"text": "We compare against the following baselines: tf-idf represents words in a word-word matrix capturing co-occurrence counts in all 10-word context windows.", "labels": [], "entities": []}, {"text": "found pruning the low-value tf-idf features helps performance.", "labels": [], "entities": []}, {"text": "We report the result of this pruning technique after tuning the threshold value on this dataset, removing all but the top 200 features in each word vector.", "labels": [], "entities": []}, {"text": "We tried the same multi-prototype approach and used spherical k-means 3 to cluster the contexts using tf-idf representations, but obtained lower numbers than singleprototype (55.4 with AvgSimC).", "labels": [], "entities": [{"text": "AvgSimC", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.5954903960227966}]}, {"text": "We then tried using pruned tf-idf representations on contexts with our clustering assignments (included in), but still got results worse than the single-prototype version of the pruned tf-idf model (60.5 with AvgSimC).", "labels": [], "entities": []}, {"text": "This suggests that the pruned tf-idf representations might be more susceptible to noise or mistakes in context clustering.", "labels": [], "entities": []}, {"text": "By utilizing global context, our model outperforms C&W's vectors and the above baselines on this dataset.", "labels": [], "entities": []}, {"text": "With multiple representations per word, we show that the multi-prototype approach can improve over the single-prototype version without using context (62.8 vs. 58.6).", "labels": [], "entities": []}, {"text": "Moreover, using AvgSimC 4 which takes contexts into account, the multi-prototype model obtains the best performance (65.7).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Nearest neighbors of words based on cosine sim- ilarity. Our model is less constrained by syntax and is  more semantic.", "labels": [], "entities": []}, {"text": " Table 2: Nearest neighbors of word embeddings learned  by our model using the multi-prototype approach based  on cosine similarity. The clustering is able to find the dif- ferent meanings, usages, and parts of speech of the words.", "labels": [], "entities": []}]}