{"title": [{"text": "A Nonparametric Bayesian Approach to Acoustic Model Discovery", "labels": [], "entities": [{"text": "Acoustic Model Discovery", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.7268669406572977}]}], "abstractContent": [{"text": "We investigate the problem of acoustic mod-eling in which prior language-specific knowledge and transcribed data are unavailable.", "labels": [], "entities": []}, {"text": "We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit.", "labels": [], "entities": []}, {"text": "Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit.", "labels": [], "entities": []}, {"text": "We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline.", "labels": [], "entities": [{"text": "TIMIT corpus", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8645442128181458}]}, {"text": "We test the quality of the learned acoustic models on a spoken term detection task.", "labels": [], "entities": [{"text": "spoken term detection task", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6999046951532364}]}, {"text": "Compared to the baselines, our model improves the relative precision of top hits by at least 22.1% and outper-forms a language-mismatched acoustic model.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9720855951309204}]}], "introductionContent": [{"text": "Acoustic models are an indispensable component of speech recognizers.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6981527358293533}]}, {"text": "However, the standard process of training acoustic models is expensive, and requires not only language-specific knowledge, e.g., the phone set of the language, a pronunciation dictionary, but also a large amount of transcribed data.", "labels": [], "entities": []}, {"text": "Unfortunately, these necessary data are only available fora very small number of languages in the world.", "labels": [], "entities": []}, {"text": "Therefore, a procedure for training acoustic models without annotated data would not only be a breakthrough from the traditional approach, but would also allow us to build speech recognizers for any language efficiently.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the problem of unsupervised acoustic modeling with only spoken utterances as training data.", "labels": [], "entities": [{"text": "unsupervised acoustic modeling", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.684850811958313}]}, {"text": "As suggested in, unsupervised acoustic modeling can be broken down to three sub-tasks: segmentation, clustering segments, and modeling the sound pattern of each cluster.", "labels": [], "entities": []}, {"text": "In previous work, the three subproblems were often approached sequentially and independently in which initial steps are not related to later ones ().", "labels": [], "entities": []}, {"text": "For example, the speech data was usually segmented regardless of the clustering results and the learned acoustic models.", "labels": [], "entities": []}, {"text": "In contrast to the previous methods, we approach the problem by modeling the three sub-problems as well as the unknown set of sub-word units as latent variables in one nonparametric Bayesian model.", "labels": [], "entities": []}, {"text": "More specifically, we formulate a Dirichlet process mixture model where each mixture is a Hidden Markov Model (HMM) used to model a subword unit and to generate observed segments of that unit.", "labels": [], "entities": []}, {"text": "Our model seeks the set of sub-word units, segmentation, clustering and HMMs that best represent the observed data through an iterative inference process.", "labels": [], "entities": []}, {"text": "We implement the inference process using Gibbs sampling.", "labels": [], "entities": []}, {"text": "We test the effectiveness of our model on the TIMIT database (.", "labels": [], "entities": [{"text": "TIMIT database", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.9515440165996552}]}, {"text": "Our model shows its ability to discover sub-word units that are highly correlated with standard English phones and to capture acoustic context information.", "labels": [], "entities": []}, {"text": "For the segmentation task, our model outperforms the state-of-the-art unsupervised method and improves the relative F-score by 18.8 points ().", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.9238729774951935}, {"text": "F-score", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9969580173492432}]}, {"text": "Finally, we test the quality of the learned acoustic models through a keyword spotting task.", "labels": [], "entities": [{"text": "keyword spotting", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7736862897872925}]}, {"text": "Compared to the state-of-the-art unsupervised methods (, our model yields a relative improvement in precision of top hits by at least 22.1% with only some degradation in equal error rate (EER), and outperforms a language-mismatched acoustic model trained with supervised data.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.998664140701294}, {"text": "equal error rate (EER)", "start_pos": 170, "end_pos": 192, "type": "METRIC", "confidence": 0.9318027297655741}]}], "datasetContent": [{"text": "To the best of our knowledge, there are no standard corpora for evaluating unsupervised methods for acoustic modeling.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.8307860791683197}]}, {"text": "However, numerous related studies have reported performance on the TIMIT corpus (, which creates a set of strong baselines for us to compare against.", "labels": [], "entities": [{"text": "TIMIT corpus", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.8660229444503784}]}, {"text": "Therefore, the TIMIT corpus is chosen as the evaluation set for our model.", "labels": [], "entities": [{"text": "TIMIT corpus", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.8037494421005249}]}, {"text": "In this section, we describe the methods used to measure the performance of our model on the following three tasks: sub-word acoustic modeling, segmentation and nonparametric clustering.", "labels": [], "entities": [{"text": "sub-word acoustic modeling", "start_pos": 116, "end_pos": 142, "type": "TASK", "confidence": 0.6278546452522278}]}, {"text": "Unsupervised Segmentation We compare the phonetic boundaries proposed by our model to the manual labels provided in the TIMIT dataset.", "labels": [], "entities": [{"text": "TIMIT dataset", "start_pos": 120, "end_pos": 133, "type": "DATASET", "confidence": 0.9237387776374817}]}, {"text": "We follow the suggestion of ( and use a 20-ms tolerance window to compute recall, precision rates and F-score of the segmentation our model proposed for TIMIT's training set.", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9991594552993774}, {"text": "precision rates", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.9836938381195068}, {"text": "F-score", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9983763694763184}, {"text": "TIMIT's training set", "start_pos": 153, "end_pos": 173, "type": "DATASET", "confidence": 0.8099181056022644}]}, {"text": "We compare our model against the state-of-the-art unsupervised and semi-supervised segmentation methods that were also evaluated on the TIMIT training set (.", "labels": [], "entities": [{"text": "TIMIT training set", "start_pos": 136, "end_pos": 154, "type": "DATASET", "confidence": 0.9438077807426453}]}, {"text": "Nonparametric Clustering Our model automatically groups speech segments into different clusters.", "labels": [], "entities": []}, {"text": "One question we are interested in answering is whether these learned clusters correlate to English phones.", "labels": [], "entities": []}, {"text": "To answer the question, we develop a method to map cluster labels to the phone set in a dataset.", "labels": [], "entities": []}, {"text": "We align each cluster label in an utterance to the phone(s) it overlaps within time by using the boundaries proposed by our model and the manually-labeled ones.", "labels": [], "entities": []}, {"text": "When a cluster label overlaps with more than one phone, we align it to the phone with the largest overlap.", "labels": [], "entities": []}, {"text": "We compile the alignment results for 3696 training utterances and present a confusion matrix between the learned cluster labels and the 48 phonetic units used in TIMIT (.", "labels": [], "entities": []}, {"text": "Sub-word Acoustic Modeling Finally, and most importantly, we need to gauge the quality of the learned sub-word acoustic models.", "labels": [], "entities": [{"text": "Sub-word Acoustic Modeling", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8087707360585531}]}, {"text": "In previous work, and tested their models on a phone recognition task and a term detection task respectively.", "labels": [], "entities": [{"text": "phone recognition task", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.844214141368866}, {"text": "term detection task", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8263983329137167}]}, {"text": "These two tasks are fair measuring methods, but performance on these tasks depends not only on the learned acoustic models, but also other components such as the label-to-phone transducer in) and the graphone model in ().", "labels": [], "entities": []}, {"text": "To reduce performance dependencies on components other than the acoustic model, we turn to the task of spoken term detection, which is also the measuring method used in (Jansen and Church, 2011).", "labels": [], "entities": [{"text": "spoken term detection", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.7407399217287699}]}, {"text": "We compare our unsupervised acoustic model with three supervised ones: 1) an English triphone model, 2) an English monophone model and 3) a Thai monophone model.", "labels": [], "entities": []}, {"text": "The first two were trained on TIMIT, while the Thai monophone model was trained with 32 hour clean read Thai speech from the LOTUS corpus ().", "labels": [], "entities": [{"text": "TIMIT", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.8337652683258057}, {"text": "LOTUS corpus", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.7561598420143127}]}, {"text": "All of the three models, as well as ours, used threestate HMMs to model phonetic units.", "labels": [], "entities": []}, {"text": "To conduct spoken term detection experiments on the TIMIT dataset, we computed a posteriorgram representation for both training and test feature frames over the: The values of the hyperparameters of our model, where \u00b5 d and \u03bb dare the d th entry of the mean and the diagonal of the inverse covariance matrix of training data.", "labels": [], "entities": [{"text": "spoken term detection", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6795708735783895}, {"text": "TIMIT dataset", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9516128599643707}]}, {"text": "HMM states for each of the four models.", "labels": [], "entities": [{"text": "HMM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7887329459190369}]}, {"text": "Ten keywords were randomly selected for the task.", "labels": [], "entities": []}, {"text": "For every keyword, spoken examples were extracted from the training set and were searched for in the test set using segmental dynamic time warping.", "labels": [], "entities": []}, {"text": "In addition to the supervised acoustic models, we also compare our model against the state-ofthe-art unsupervised methods for this task.", "labels": [], "entities": []}, {"text": "trained a GMM with 50 components to decode posteriorgrams for the feature frames, and used a deep Boltzmann machine (DBM) trained with pseudo phone labels generated from an unsupervised GMM to produce a posteriorgram representation.", "labels": [], "entities": []}, {"text": "The evaluation metrics they used were: 1) P@N, the average precision of the top N hits, where N is the number of occurrences of each keyword in the test set; 2) EER: the average equal error rate at which the false acceptance rate is equal to the false rejection rate.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9522154927253723}, {"text": "EER", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9983901977539062}, {"text": "equal error rate", "start_pos": 178, "end_pos": 194, "type": "METRIC", "confidence": 0.7277845740318298}]}, {"text": "We also report experimental results using the P@N and EER metrics.", "labels": [], "entities": [{"text": "EER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9798452258110046}]}], "tableCaptions": [{"text": " Table 3: The performance of our model and the GMM  and DBM baselines on the spoken term detection task.", "labels": [], "entities": [{"text": "GMM", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8454269170761108}, {"text": "spoken term detection task", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7558680325746536}]}, {"text": " Table 4: The segmentation performance of the baselines,  our model and the heuristic pre-segmentation on TIMIT  training set. *The number of phone boundaries in each  utterance was assumed to be known in this model.", "labels": [], "entities": [{"text": "TIMIT  training set", "start_pos": 106, "end_pos": 125, "type": "DATASET", "confidence": 0.8039021690686544}]}]}