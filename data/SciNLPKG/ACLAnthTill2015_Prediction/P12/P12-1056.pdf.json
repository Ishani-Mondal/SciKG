{"title": [], "abstractContent": [{"text": "Microblogs such as Twitter reflect the general public's reactions to major events.", "labels": [], "entities": []}, {"text": "Bursty topics from microblogs reveal what events have attracted the most online attention.", "labels": [], "entities": []}, {"text": "Although bursty event detection from text streams has been studied before, previous work may not be suitable for microblogs because compared with other text streams such as news articles and scientific publications, microblog posts are particularly diverse and noisy.", "labels": [], "entities": [{"text": "bursty event detection from text streams", "start_pos": 9, "end_pos": 49, "type": "TASK", "confidence": 0.736693541208903}]}, {"text": "To find topics that have bursty patterns on microblogs, we propose a topic model that simultaneously captures two observations: (1) posts published around the same time are more likely to have the same topic, and (2) posts published by the same user are more likely to have the same topic.", "labels": [], "entities": []}, {"text": "The former helps find event-driven posts while the latter helps identify and filter out \"personal\" posts.", "labels": [], "entities": []}, {"text": "Our experiments on a large Twitter dataset show that there are more meaningful and unique bursty topics in the top-ranked results returned by our model than an LDA baseline and two degenerate variations of our model.", "labels": [], "entities": []}, {"text": "We also show some case studies that demonstrate the importance of considering both the temporal information and users' personal interests for bursty topic detection from microblogs.", "labels": [], "entities": [{"text": "bursty topic detection", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.6568441788355509}]}], "introductionContent": [{"text": "With the fast growth of Web 2.0, avast amount of user-generated content has accumulated on the social Web.", "labels": [], "entities": []}, {"text": "In particular, microblogging sites such as Twitter allow users to easily publish short instant posts about any topic to be shared with the general public.", "labels": [], "entities": []}, {"text": "The textual content coupled with the temporal patterns of these microblog posts provides important insight into the general public's interest.", "labels": [], "entities": []}, {"text": "A sudden increase of topically similar posts usually indicates a burst of interest in some event that has happened offline (such as a product launch or a natural disaster) or online (such as the spread of a viral video).", "labels": [], "entities": []}, {"text": "Finding bursty topics from microblogs therefore can help us identify the most popular events that have drawn the public's attention.", "labels": [], "entities": []}, {"text": "In this paper, we study the problem of finding bursty topics from a stream of microblog posts generated by different users.", "labels": [], "entities": []}, {"text": "We focus on retrospective detection, where the text stream within a certain period is analyzed in its entirety.", "labels": [], "entities": [{"text": "retrospective detection", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.9656964540481567}]}, {"text": "Retrospective bursty event detection from text streams is not new, but finding bursty topics from microblog steams has not been well studied.", "labels": [], "entities": [{"text": "Retrospective bursty event detection from text streams", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7205080262252263}]}, {"text": "In his seminal work, proposed a state machine to model the arrival times of documents in a stream in order to identify bursts.", "labels": [], "entities": []}, {"text": "This model has been widely used.", "labels": [], "entities": []}, {"text": "However, this model assumes that documents in the stream are all about a given topic.", "labels": [], "entities": []}, {"text": "In contrast, discovering interesting topics that have drawn bursts of interest from a stream of topically diverse microblog posts is itself a challenge.", "labels": [], "entities": []}, {"text": "To discover topics, we can certainly apply standard topic models such as LDA (, but with standard LDA temporal information is lost during topic discovery.", "labels": [], "entities": [{"text": "topic discovery", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.7399519383907318}]}, {"text": "For microblogs, where posts are short and often event-driven, temporal information can sometimes be critical in determining the topic of a post.", "labels": [], "entities": []}, {"text": "For example, typically a post containing the word \"jobs\" is likely to be about employment, but right after October 5, 2011, a post containing \"jobs\" is more likely to be related to Steve Jobs' death.", "labels": [], "entities": []}, {"text": "Essentially, we expect that on microblogs, posts published around the same time have a higher probability to belong to the same topic.", "labels": [], "entities": []}, {"text": "To capture this intuition, one solution is to assume that posts published within the same short time window follow the same topic distribution.", "labels": [], "entities": []}, {"text": "proposed a PLSA-based topic model that exploits this idea to find correlated bursty patterns across multiple text streams.", "labels": [], "entities": []}, {"text": "However, their model is not immediately applicable for our problem.", "labels": [], "entities": []}, {"text": "First, their model assumes multiple text streams where word distributions for the same topic are different on different streams.", "labels": [], "entities": []}, {"text": "More importantly, their model was applied to news articles and scientific publications, where most documents follow the global topical trends.", "labels": [], "entities": []}, {"text": "On microblogs, besides talking about global popular events, users also often talk about their daily lives and personal interests.", "labels": [], "entities": []}, {"text": "In order to detect global bursty events from microblog posts, it is important to filter out these \"personal\" posts.", "labels": [], "entities": []}, {"text": "In this paper, we propose a topic model designed for finding bursty topics from microblogs.", "labels": [], "entities": []}, {"text": "Our model is based on the following two assumptions: (1) If a post is about a global event, it is likely to follow a global topic distribution that is time-dependent.", "labels": [], "entities": []}, {"text": "(2) If a post is about a personal topic, it is likely to follow a personal topic distribution that is more or less stable overtime.", "labels": [], "entities": []}, {"text": "Separation of \"global\" and \"personal\" posts is done in an unsupervised manner through hidden variables.", "labels": [], "entities": [{"text": "Separation of \"global\" and \"personal\" posts", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7270735561847687}]}, {"text": "Finally, we apply a state machine to detect bursts from the discovered topics.", "labels": [], "entities": []}, {"text": "We evaluate our model on a large Twitter dataset.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.7731195390224457}]}, {"text": "We find that compared with bursty topics discovered by standard LDA and by two degenerate variations of our model, bursty topics discovered by our model are more accurate and less redundant within the topranked results.", "labels": [], "entities": []}, {"text": "We also use some example bursty topics to explain the advantages of our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we show the quantitative evaluation of the four models we consider, namely, LDA, TimeLDA, UserLDA and TimeUserLDA.", "labels": [], "entities": [{"text": "TimeLDA", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.6334506869316101}, {"text": "TimeUserLDA", "start_pos": 119, "end_pos": 130, "type": "DATASET", "confidence": 0.9301407337188721}]}, {"text": "For each model, we set the number of topics C to 80, \u03b1 to 50 C and \u03b2 to 0.01 after some preliminary experiments.", "labels": [], "entities": []}, {"text": "Each model was run for 500 iterations of Gibbs sampling.", "labels": [], "entities": []}, {"text": "We take 40 samples with a gap of 5 iterations in the last 200 iterations to help us assign values to all the hidden variables.", "labels": [], "entities": []}, {"text": "shows the comparison between these models in terms of the precision of the top-K results.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9991569519042969}]}, {"text": "As we can see, our model outperforms all other models for K <= 20.", "labels": [], "entities": []}, {"text": "For K = 30, the UserLDA model performs the best followed by our model.", "labels": [], "entities": []}, {"text": "As we have pointed out, some of the bursty topics are redundant, i.e. they are about the same bursty event.", "labels": [], "entities": []}, {"text": "We therefore also calculated precision at K for unique topics, where for redundant topics the one ranked the highest is scored 1 and the other ones are scored 0.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996515512466431}]}, {"text": "The comparison of the performance is shown in.", "labels": [], "entities": []}, {"text": "As we can see, in this case, our model outperforms other models with all K.", "labels": [], "entities": []}, {"text": "We will further discuss redundant bursty topics in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision at K for the various models.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9925204515457153}]}, {"text": " Table 2: Precision at K for the various models after we  remove redundant bursty topics.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.995468258857727}]}]}