{"title": [{"text": "Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information *", "labels": [], "entities": [{"text": "Translation Model Adaptation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8185420433680216}, {"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7620900273323059}]}], "abstractContent": [{"text": "To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.9403608043988546}]}, {"text": "Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain mono-lingual corpora.", "labels": [], "entities": []}, {"text": "Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.", "labels": [], "entities": [{"text": "NIST Chinese-English translation task", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7939553111791611}]}], "introductionContent": [{"text": "In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice;.", "labels": [], "entities": [{"text": "statistical machine translation(SMT)", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.841101735830307}]}, {"text": "However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9927412271499634}, {"text": "domain adaptation", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7321393191814423}]}, {"text": "The simple reason is that the underlying statistical models always tend to closely approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences.", "labels": [], "entities": []}, {"text": "When the translated texts and the training data come from the same domain, SMT systems can achieve good performance, otherwise the translation quality degrades dramatically.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9933362007141113}]}, {"text": "Therefore, it is of significant importance to develop translation systems which can be effectively transferred from one domain to another, for example, from newswire to weblog.", "labels": [], "entities": []}, {"text": "According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9724436402320862}, {"text": "language model adaptation", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6262467900911967}]}, {"text": "Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work.", "labels": [], "entities": [{"text": "domain-specific translation task", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.6666295329729716}]}, {"text": "In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology) or synthesized parallel sentences (, and the other exploited the full potential of existing parallel corpus in a mixture-modeling) framework.", "labels": [], "entities": []}, {"text": "However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement.", "labels": [], "entities": [{"text": "bilingual corpus synthesis and exploitation", "start_pos": 52, "end_pos": 95, "type": "TASK", "confidence": 0.6793106019496917}]}, {"text": "In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain monolingual corpora.", "labels": [], "entities": [{"text": "domainspecific translation task", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.7539662718772888}]}, {"text": "Our approach is inspired by the recent studies (;) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", "labels": [], "entities": [{"text": "translation selection", "start_pos": 206, "end_pos": 227, "type": "TASK", "confidence": 0.9334394335746765}]}, {"text": "For example, \"bank\" often occurs in the sentences related to the economy topic when translated into \"y \u00b4 inh\u00e1ng\", and occurs in the sentences related to the geography topic when translated to \"h\u00e9\u00e0n\".", "labels": [], "entities": []}, {"text": "Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases.", "labels": [], "entities": []}, {"text": "Ina monolingual corpus, if \"bank\" occurs more often in the sentences related to the economy topic than the ones related to the geography topic, it is more likely that \"bank\" is translated to \"y \u00b4 inh\u00e1ng\" than to \"h\u00e9\u00e0n\".", "labels": [], "entities": []}, {"text": "With the out-of-domain bilingual corpus, we first incorporate the topic information into translation probability estimation, aiming to quantify the effect of the topical context information on translation selection.", "labels": [], "entities": [{"text": "translation selection", "start_pos": 193, "end_pos": 214, "type": "TASK", "confidence": 0.951585590839386}]}, {"text": "Then, we rescore all phrase pairs according to the phrasetopic and the word-topic posterior distributions of the additional in-domain monolingual corpora.", "labels": [], "entities": []}, {"text": "As compared to the previous works, our method takes advantage of both the in-domain monolingual corpora and the out-of-domain bilingual corpus to incorporate the topic information into our translation model, thus breaking down the corpus barrier for translation quality improvement.", "labels": [], "entities": []}, {"text": "The experimental results on the NIST data set demonstrate the effectiveness of our method.", "labels": [], "entities": [{"text": "NIST data set", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9864081541697184}]}, {"text": "The reminder of this paper is organized as follows: Section 2 provides a brief description of translation probability estimation.", "labels": [], "entities": [{"text": "translation probability estimation", "start_pos": 94, "end_pos": 128, "type": "TASK", "confidence": 0.8777929345766703}]}, {"text": "Section 3 introduces the adaptation method which incorporates the topic information into the translation model; Section 4 describes and discusses the experimental results; Section 5 briefly summarizes the recent related work about translation model adaptation.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 231, "end_pos": 259, "type": "TASK", "confidence": 0.9229085842768351}]}, {"text": "Finally, we end with a conclusion and the future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on the Chinese-to-English translation task for the weblog text.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.7723899483680725}]}, {"text": "After a brief description of the experimental setup, we investigate the effects of various factors on the translation system performance.", "labels": [], "entities": []}, {"text": "In our experiments, the out-of-domain training corpus comes from the FBIS corpus and the Hansard- To obtain various topic distributions for the outof-domain training corpus and the in-domain monolingual corpora in the source language and the target language respectively, we use HTMM tool developed by to conduct topic model training.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9623400568962097}, {"text": "Hansard", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9265784025192261}]}, {"text": "During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, \u03b1 = 1.5, \u03b2 = 1.01, iters = 100.", "labels": [], "entities": []}, {"text": "See ( for the meanings of these parameters.", "labels": [], "entities": []}, {"text": "Besides, we set the interpolation weight \u03b8 in formula (10) to 0.5 by observing the results on development set in the additional experiments.", "labels": [], "entities": [{"text": "interpolation weight \u03b8", "start_pos": 20, "end_pos": 42, "type": "METRIC", "confidence": 0.8838668465614319}]}, {"text": "We choose MOSES, a famous open-source phrase-based machine translation system (, as the experimental decoder.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6583356658617655}]}, {"text": "GIZA++) and the heuristics \"grow-diag-final-and\" are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7.", "labels": [], "entities": []}, {"text": "We use SRILM Toolkits) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively.", "labels": [], "entities": [{"text": "English Blog Authorship corpus", "start_pos": 75, "end_pos": 105, "type": "DATASET", "confidence": 0.8720913827419281}, {"text": "Gigaword corpus", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.9048855304718018}]}, {"text": "During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training to tune the feature weights for the log-linear model.", "labels": [], "entities": []}, {"text": "The translation quality is evaluated by case-insensitive BLEU-4 metric ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9517716765403748}, {"text": "BLEU-4", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9856300950050354}]}, {"text": "Finally, we conduct paired bootstrap sampling) to test the significance in BLEU score differences.", "labels": [], "entities": [{"text": "BLEU score differences", "start_pos": 75, "end_pos": 97, "type": "METRIC", "confidence": 0.9664664069811503}]}], "tableCaptions": [{"text": " Table 1: Experimental results using different smoothing  methods.", "labels": [], "entities": []}, {"text": " Table 2: Experimental results using different phrase ta- bles. OutBp: the out-of-domain phrase table. AdapBp:  the adapted phrase table.", "labels": [], "entities": []}]}