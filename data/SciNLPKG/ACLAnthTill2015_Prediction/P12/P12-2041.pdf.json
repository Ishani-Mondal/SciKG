{"title": [{"text": "Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions", "labels": [], "entities": [{"text": "Combining Textual Entailment and Argumentation Theory", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.697747215628624}, {"text": "Interactions", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.5380507111549377}]}], "abstractContent": [{"text": "Blogs and forums are widely adopted by on-line communities to debate about various issues.", "labels": [], "entities": []}, {"text": "However, a user that wants to cut in on a debate may experience some difficulties in extracting the current accepted positions, and can be discouraged from interacting through these applications.", "labels": [], "entities": []}, {"text": "In our paper, we combine textual entailment with argumentation theory to automatically extract the arguments from debates and to evaluate their acceptability.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7019271701574326}, {"text": "argumentation theory", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8592639863491058}]}], "introductionContent": [{"text": "Online debate platforms, like Debatepedia 1 , Twitter 2 and many others, are becoming more and more popular on the Web.", "labels": [], "entities": []}, {"text": "In such applications, users are asked to provide their own opinions about selected issues.", "labels": [], "entities": []}, {"text": "However, it may happen that the debates become rather complicated, with several arguments supporting and contradicting each others.", "labels": [], "entities": []}, {"text": "Thus, it is difficult for potential participants to understand the way the debate is going on, i.e., which are the current accepted arguments in a debate.", "labels": [], "entities": []}, {"text": "In this paper, we propose to support participants of online debates with a framework combining Textual Entailment (TE) () and abstract argumentation theory.", "labels": [], "entities": []}, {"text": "In particular, TE is adopted to extract the abstract arguments from natural language debates and to provide the relations among these arguments; argumentation theory is then used to compute the set of accepted arguments among those obtained from the TE module, http://debatepedia.idebate.org 2 http://twitter.com/ i.e., the arguments shared by the majority of the participants without being attacked by other accepted arguments.", "labels": [], "entities": []}, {"text": "The originality of the proposed framework lies in the combination of two existing approaches with the goal of supporting participants in their interactions with online debates, by automatically detecting the arguments in natural language text, and identifying the accepted ones.", "labels": [], "entities": []}, {"text": "We evaluate the feasibility of our combined approach on a set of arguments extracted from a sample of Debatepedia.", "labels": [], "entities": [{"text": "Debatepedia", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.879057765007019}]}], "datasetContent": [{"text": "We experiment the combination of TE and argumentation theory to support the interaction of online debates participants on Debatepedia, an encyclopedia of pro and con arguments on critical issues.", "labels": [], "entities": [{"text": "TE", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.7125524282455444}]}, {"text": "To create the data set of arguments pairs to evaluate our task 3 , we randomly selected a set of topics (reported in column Topics,) of Debatepedia debates, and for each topic we coupled all the pros and cons arguments both with the main argument (the issue of the debate, as in Example 1 and 2) and/or with other arguments to which the most recent argument refers, e.g., Example 3.", "labels": [], "entities": []}, {"text": "Using Debatepedia as case study provides us with already annotated arguments (pro \u21d2 entailment 4 , and cons \u21d2 contradiction), and casts our task as a yes/no entailment task.", "labels": [], "entities": []}, {"text": "As shown in, we collected 200 T-H pairs, 100 used to train the TE system, and 100 to test it (each data set is composed by 55 entailment and 45 contradiction pairs).", "labels": [], "entities": []}, {"text": "Test set pairs concern completely new topics, never seen by the system.", "labels": [], "entities": []}, {"text": "To detect which kind of relation underlies each couple of arguments, we used the EDITS system (Edit Distance Textual Entailment Suite), an open-source software package for recognizing TE 6 (.", "labels": [], "entities": []}, {"text": "EDITS implements a distance-based framework which assumes that the probability of an entailment relation between a given T-H pair is inversely proportional to the distance between T and H.", "labels": [], "entities": [{"text": "EDITS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9450823664665222}]}, {"text": "Within this framework, the system implements different approaches to distance computation, providing both edit distance algorithms and similarity algorithms.", "labels": [], "entities": []}, {"text": "To evaluate our combined approach, we carryout a two-step evaluation: we assess (i) the performances of the TE system to correctly assign the entailment/contradiction relations to the pairs of arguments in the Debatepedia data set; (ii) how much such performances impact on the goals of the argumentation module, i.e. how much a wrong assignment of a relation between two arguments leads to an incorrect evaluation of the accepted arguments.", "labels": [], "entities": [{"text": "Debatepedia data set", "start_pos": 210, "end_pos": 230, "type": "DATASET", "confidence": 0.9249193867047628}]}, {"text": "For the first evaluation, we run the EDITS system off-the-shelf on the Debatepedia data set, applying one of its basic configurations (i.e. the distance entailment engine combines cosine similarity as the core distance algorithm; distance calculated on lemmas; stopword list included).", "labels": [], "entities": [{"text": "Debatepedia data set", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9809396862983704}]}, {"text": "EDITS accuracy on the training set is 0.69, on the test set 0.67 (a baseline applying a Word Overlap algorithm on tokenized text is also considered, and obtains an accuracy of 0.61 on the training set and 0.62 on the test set).", "labels": [], "entities": [{"text": "EDITS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6224877834320068}, {"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.8207269310951233}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9953814744949341}]}, {"text": "Even using a basic configuration of EDITS, and a small data set (100 pairs for training) performances  on Debatepedia test set are promising, and inline with performances of TE systems on RTE data sets.", "labels": [], "entities": [{"text": "Debatepedia test set", "start_pos": 106, "end_pos": 126, "type": "DATASET", "confidence": 0.9673255880673727}, {"text": "RTE data sets", "start_pos": 188, "end_pos": 201, "type": "DATASET", "confidence": 0.8776766856511434}]}, {"text": "As a second step of the evaluation, we consider the impact of EDITS performances on arguments acceptability, i.e., how much a wrong assignment of a relation to a pair of arguments affects the computation of the set of accepted arguments.", "labels": [], "entities": []}, {"text": "We identify the accepted arguments both in the correct AF of each Debatepedia debate of the data set (the goldstandard, where relations are correctly assigned), and on the AF generated basing on the relations assigned by EDITS.", "labels": [], "entities": [{"text": "AF", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9831582307815552}, {"text": "Debatepedia debate of the data set", "start_pos": 66, "end_pos": 100, "type": "DATASET", "confidence": 0.7884403467178345}, {"text": "AF", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9802425503730774}, {"text": "EDITS", "start_pos": 221, "end_pos": 226, "type": "DATASET", "confidence": 0.9328762292861938}]}, {"text": "Our combined approach obtained the following performances: precision 0.74, recall 0.76, accuracy 0.75, meaning that the TE system mistakes in relation assignment propagate in the AF , but results are still satisfying and foster further research in this direction.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9993196725845337}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9992197751998901}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9993811845779419}, {"text": "AF", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.7054593563079834}]}], "tableCaptions": [{"text": " Table 1: The Debatepedia data set.", "labels": [], "entities": [{"text": "Debatepedia data set", "start_pos": 14, "end_pos": 34, "type": "DATASET", "confidence": 0.8495367765426636}]}]}