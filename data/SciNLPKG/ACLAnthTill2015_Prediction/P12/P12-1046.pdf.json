{"title": [{"text": "Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing", "labels": [], "entities": [{"text": "Syntactic Parsing", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8109689354896545}]}], "abstractContent": [{"text": "We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7610103785991669}]}, {"text": "An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data.", "labels": [], "entities": []}, {"text": "We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion.", "labels": [], "entities": [{"text": "TSG", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9368717074394226}, {"text": "symbol refinement", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6948354691267014}]}, {"text": "We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling.", "labels": [], "entities": []}, {"text": "Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discrim-inative reranking parsers.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9868459105491638}, {"text": "Wall Street Journal (WSJ) English Penn Treebank parsing task", "start_pos": 55, "end_pos": 115, "type": "DATASET", "confidence": 0.8729260888966647}]}], "introductionContent": [{"text": "Syntactic parsing has played a central role in natural language processing.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8753117322921753}, {"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6594551106293997}]}, {"text": "The resulting syntactic analysis can be used for various applications such as machine translation (), sentence compression, and question answering (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7949033081531525}, {"text": "sentence compression", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8265482485294342}, {"text": "question answering", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.9130517542362213}]}, {"text": "Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (.", "labels": [], "entities": []}, {"text": "In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees.", "labels": [], "entities": [{"text": "tree substitution grammar (TSG)", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.8225393295288086}]}, {"text": "TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments.", "labels": [], "entities": [{"text": "CFG", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.913865864276886}]}, {"text": "These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements ( . Previous work on TSG parsing ( has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser () and the Charniak parser).", "labels": [], "entities": [{"text": "TSG parsing", "start_pos": 210, "end_pos": 221, "type": "TASK", "confidence": 0.7891522347927094}]}, {"text": "One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols.", "labels": [], "entities": [{"text": "TSG", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8965917825698853}]}, {"text": "Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments.", "labels": [], "entities": []}, {"text": "On the other hand, current state-of-the-art parsers use symbol refinement techniques).", "labels": [], "entities": []}, {"text": "Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments.", "labels": [], "entities": [{"text": "Symbol refinement", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9253026247024536}]}, {"text": "As shown in several studies on TSG parsing, large tree fragments and symbol refinement work complementarily for syntactic parsing.", "labels": [], "entities": [{"text": "TSG parsing", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9602454006671906}, {"text": "syntactic parsing", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7512983977794647}]}, {"text": "For example, have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser.", "labels": [], "entities": [{"text": "deterministic symbol refinement", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.5975363155206045}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9985570311546326}]}, {"text": "In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7956053912639618}]}, {"text": "SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data.", "labels": [], "entities": []}, {"text": "Our work differs from previous studies in that we focus on a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion.", "labels": [], "entities": [{"text": "TSG rules", "start_pos": 81, "end_pos": 90, "type": "TASK", "confidence": 0.8403611183166504}, {"text": "symbol refinement", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.6691128462553024}]}, {"text": "We also propose a novel probabilistic SR-TSG model with the hierarchical Pitman-Yor Process, namely a sort of nonparametric Bayesian model, to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on blocked MCMC sampling.", "labels": [], "entities": []}, {"text": "Our SR-TSG parser achieves an F1 score of 92.4% in the WSJ English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and superior to state-of-the-art discriminative reranking parsers.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.98851278424263}, {"text": "WSJ English Penn Treebank", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.8823016732931137}, {"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.44101467728614807}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison of parsing accuracy with the  small and full training sets. *Our reimplementation  of (Cohn et al., 2010).", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9731146097183228}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9466840624809265}]}, {"text": " Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the  development set (\u2264 100).", "labels": [], "entities": []}]}