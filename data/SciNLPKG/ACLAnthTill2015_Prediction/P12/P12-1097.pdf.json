{"title": [{"text": "Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries", "labels": [], "entities": [{"text": "Character-Level Machine Translation Evaluation", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6803903505206108}]}], "abstractContent": [{"text": "In this work, we introduce the TESLA-CELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis-Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation.", "labels": [], "entities": [{"text": "TESLA-CELAB metric", "start_pos": 31, "end_pos": 49, "type": "METRIC", "confidence": 0.963842362165451}, {"text": "Translation Evaluation of Sentences", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.9071084260940552}, {"text": "machine translation evaluation", "start_pos": 213, "end_pos": 243, "type": "TASK", "confidence": 0.8247896830240885}]}, {"text": "For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation.", "labels": [], "entities": [{"text": "TESLA-CELAB", "start_pos": 122, "end_pos": 133, "type": "METRIC", "confidence": 0.9170656800270081}]}, {"text": "By reformulating the problem in the linear programming framework, TESLA-CELAB addresses several drawbacks of the character-level metrics, in particular the mod-eling of synonyms spanning multiple characters.", "labels": [], "entities": []}, {"text": "We show empirically that TESLA-CELAB significantly outperforms character-level BLEU in the English-Chinese translation evaluation tasks.", "labels": [], "entities": [{"text": "TESLA-CELAB", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9634523391723633}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9709393382072449}, {"text": "English-Chinese translation evaluation", "start_pos": 91, "end_pos": 129, "type": "TASK", "confidence": 0.6347146332263947}]}], "introductionContent": [{"text": "Since the introduction of BLEU (), automatic machine translation (MT) evaluation has received a lot of research interest.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9939121007919312}, {"text": "automatic machine translation (MT) evaluation", "start_pos": 35, "end_pos": 80, "type": "TASK", "confidence": 0.7939543383462089}]}, {"text": "The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.7956678569316864}, {"text": "machine translation evaluation", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.7520341177781423}]}, {"text": "In the WMT shared tasks, many new generation metrics, such as METEOR (), TER), and TESLA ( ) have consistently outperformed BLEU as judged by the correlations with human judgments.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.8617036739985148}, {"text": "METEOR", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9799627065658569}, {"text": "TER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9938767552375793}, {"text": "TESLA", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9960479140281677}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9978929162025452}]}, {"text": "The research on automatic machine translation evaluation is important fora number of reasons.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8340006669362386}]}, {"text": "Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8331691026687622}, {"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7907419502735138}, {"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7061248421669006}]}, {"text": "In addition, machine translation system parameters are tuned by maximizing the automatic scores.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7775872945785522}]}, {"text": "Some recent research ( ) has shown evidence that replacing BLEU by a newer metric, TESLA, can improve the human judged translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9977622032165527}, {"text": "TESLA", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9918538331985474}]}, {"text": "Despite the importance and the research interest on automatic MT evaluation, almost all existing work has focused on European languages, in particular on English.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9590478837490082}]}, {"text": "Although many methods aim to be language neutral, languages with very different characteristics such as Chinese do present additional challenges.", "labels": [], "entities": []}, {"text": "The most obvious challenge for Chinese is that of word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7253816723823547}]}, {"text": "Unlike European languages, written Chinese is not split into words.", "labels": [], "entities": []}, {"text": "Segmenting Chinese sentences into words is a natural language processing task in its own right.", "labels": [], "entities": [{"text": "Segmenting Chinese sentences into words", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9037585854530334}]}, {"text": "However, many different segmentation standards exist for different purposes, such as Microsoft Research Asia (MSRA) for Named Entity Recognition (NER), Chinese Treebank (CTB) for parsing and part-of-speech (POS) tagging, and City University of Hong Kong (CITYU) and Academia Sinica (AS) for general word segmentation and POS tagging.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.7848378320535024}, {"text": "Chinese Treebank (CTB)", "start_pos": 152, "end_pos": 174, "type": "DATASET", "confidence": 0.9129669547080994}, {"text": "parsing and part-of-speech (POS) tagging", "start_pos": 179, "end_pos": 219, "type": "TASK", "confidence": 0.840363051210131}, {"text": "general word segmentation", "start_pos": 291, "end_pos": 316, "type": "TASK", "confidence": 0.6466974119345347}, {"text": "POS tagging", "start_pos": 321, "end_pos": 332, "type": "TASK", "confidence": 0.8039827346801758}]}, {"text": "It is not clear which standard is the best in a given scenario.", "labels": [], "entities": []}, {"text": "The only prior work attempting to address the problem of word segmentation in automatic MT evaluation for Chinese that we are aware of is Li et", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7082384079694748}, {"text": "MT evaluation", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.960724264383316}]}], "datasetContent": [{"text": "show that character-based metrics consistently outperform their word-based counterparts.", "labels": [], "entities": []}, {"text": "Despite that, we observe two important problems for the character-based metrics: 1.", "labels": [], "entities": []}, {"text": "Although partial matches are partially awarded, the mechanism breaks down for n-grams where n > 1.", "labels": [], "entities": []}, {"text": "For example, between \u4e70_\u96e8_\u4f1e and \u4e70_\u4f1e, higher-order n-grams such as \u4e70_\u96e8 and \u96e8_\u4f1e still have no match, and will be penalized accordingly, even though \u4e70_\u96e8_\u4f1e and \u4e70_\u4f1e should match exactly.", "labels": [], "entities": []}, {"text": "N-grams such as \u4e70_\u96e8 which cross natural word boundaries and are meaningless by themselves can be particularly tricky.", "labels": [], "entities": []}, {"text": "2. Character-level metrics can utilize only a small part of the Chinese synonym dictionary, such as \u4f60 and \u60a8 (you).", "labels": [], "entities": []}, {"text": "The majority of Chinese synonyms involve more than one character, such as \u96e8\u4f1e and \u4f1e (umbrella), and \u513f\u7ae5 and \u5c0f\u5b69 (child).", "labels": [], "entities": []}, {"text": "In this work, we attempt to address both of these issues by introducing TESLA-CELAB, a characterlevel metric that also models word-level linguistic phenomenon.", "labels": [], "entities": [{"text": "TESLA-CELAB", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9782917499542236}]}, {"text": "We formulate the n-gram matching process as a real-valued linear programming problem, which can be solved efficiently.", "labels": [], "entities": []}, {"text": "The metric is based on the TESLA automatic MT evaluation framework ( ).", "labels": [], "entities": [{"text": "TESLA automatic MT evaluation framework", "start_pos": 27, "end_pos": 66, "type": "DATASET", "confidence": 0.6179868876934052}]}, {"text": "In this section, we test the effectiveness of TESLA-CELAB on some real-world English-Chinese translation tasks.", "labels": [], "entities": [{"text": "TESLA-CELAB", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.7330642342567444}, {"text": "English-Chinese translation tasks", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.7169772783915201}]}], "tableCaptions": [{"text": " Table 1: Inter-judge Kappa for the NIST 2008 English- Chinese task", "labels": [], "entities": [{"text": "NIST 2008 English- Chinese task", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.9452537993590037}]}, {"text": " Table 2: Correlation with human judgment on the IWSLT 2008 English-Chinese challenge task. * denotes better than  the BLEU baseline at 5% significance level. ** denotes better than the BLEU baseline at 1% significance level.", "labels": [], "entities": [{"text": "IWSLT 2008 English-Chinese challenge task", "start_pos": 49, "end_pos": 90, "type": "DATASET", "confidence": 0.8386993288993836}, {"text": "BLEU baseline", "start_pos": 119, "end_pos": 132, "type": "METRIC", "confidence": 0.9701405763626099}, {"text": "BLEU", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9826825857162476}]}, {"text": " Table 3: Correlation with human judgment on the NIST 2008 English-Chinese MT task. ** denotes better than the  BLEU baseline at 1% significance level.", "labels": [], "entities": [{"text": "NIST 2008 English-Chinese MT task", "start_pos": 49, "end_pos": 82, "type": "DATASET", "confidence": 0.8446022033691406}, {"text": "BLEU baseline", "start_pos": 112, "end_pos": 125, "type": "METRIC", "confidence": 0.9683041870594025}]}]}