{"title": [{"text": "Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9584655165672302}]}], "abstractContent": [{"text": "With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.7875537971655527}]}, {"text": "Evidence from machine learning indicates that increasing the training sample size results in better prediction.", "labels": [], "entities": []}, {"text": "The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9840297698974609}]}, {"text": "We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies 1 // 2 regulariza-tion for joint feature selection over distributed stochastic learning processes.", "labels": [], "entities": [{"text": "SCFG-based SMT", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.47058865427970886}]}, {"text": "We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The standard SMT training pipeline combines scores from large count-based translation models and language models with a few other features and tunes these using the well-understood line-search technique for error minimization of.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.8992526531219482}, {"text": "error minimization", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.6635334938764572}]}, {"text": "If only a handful of dense features need to be tuned, minimum error rate training can be done on small tuning sets and is hard to beat in terms of accuracy and efficiency.", "labels": [], "entities": [{"text": "minimum error rate", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.7595851023991903}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9994338154792786}]}, {"text": "In contrast, the promise of largescale discriminative training for SMT is to scale to arbitrary types and numbers of features and to provide sufficient statistical support by parameter estimation on large sample sizes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.996850311756134}]}, {"text": "Features maybe lexicalized and sparse, non-local and overlapping, or be designed to generalize beyond surface statistics by incorporating part-of-speech or syntactic labels.", "labels": [], "entities": []}, {"text": "The modeler's goals might be to identify complex properties of translations, or to counter errors of pretrained translation models and language models by explicitly down-weighting translations that exhibit certain undesired properties.", "labels": [], "entities": []}, {"text": "Various approaches to feature engineering for discriminative models have been presented (see Section 2), however, with a few exceptions, discriminative learning in SMT has been confined to training on small tuning sets of a few thousand examples.", "labels": [], "entities": [{"text": "SMT", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.9937025308609009}]}, {"text": "This contradicts theoretical and practical evidence from machine learning that suggests that larger training samples should be beneficial to improve prediction also in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.9942874312400818}]}, {"text": "One possible reason why discriminative SMT has mostly been content with small tuning sets lies in the particular design of the features themselves.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8860325813293457}]}, {"text": "For example, the features introduced by and for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.", "labels": [], "entities": []}, {"text": "These features are specified in handcrafted lists based on a thorough analysis of a tuning set.", "labels": [], "entities": []}, {"text": "Such finely hand-crafted features will find sufficient statistical support on a few thousand examples and thus do not benefit from larger training sets.", "labels": [], "entities": []}, {"text": "The second type of features deploys external information such as syntactic parses or word alignments to penalize bad reorderings or undesired translations of phrases that cross syntactic constraints.", "labels": [], "entities": [{"text": "syntactic parses or word alignments", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.6421364128589631}]}, {"text": "At large scale, extraction of such features quickly becomes infeasible because of costly generation and storage of linguistic annotations.", "labels": [], "entities": []}, {"text": "Another possible reason why large training data did not yet show the expected improvements in discriminative SMT is a special overfitting problem of current popular online learning techniques.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9348510503768921}]}, {"text": "This is due to stochastic learning on a per-example basis where a weight update on a misclassified example may apply only to a small fraction of data that have been seen before.", "labels": [], "entities": []}, {"text": "Thus many features will not generalize well beyond the training examples on which they were introduced.", "labels": [], "entities": []}, {"text": "The goal of this paper is to investigate if and how it is possible to benefit from scaling discriminative training for SMT to large training sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9939250349998474}]}, {"text": "We deploy generic features for SCFG-based SMT that can efficiently be read off from rules at runtime.", "labels": [], "entities": [{"text": "SCFG-based SMT", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.5326097011566162}]}, {"text": "Such features include rule ids, rule-local n-grams, or types of rule shapes.", "labels": [], "entities": []}, {"text": "Another crucial ingredient of our approach is a combination of parallelized stochastic learning with feature selection inspired by multi-task learning.", "labels": [], "entities": []}, {"text": "The simple but effective idea is to randomly divide training data into evenly sized shards, use stochastic learning on each shard in parallel, while performing 1 // 2 regularization for joint feature selection on the shards after each epoch, before starting anew epoch with a reduced feature vector averaged across shards.", "labels": [], "entities": []}, {"text": "Iterative feature selection procedure is the key to both efficiency and improved prediction: Without interleaving parallelized stochastic learning with feature selection our largest experiments would not be feasible.", "labels": [], "entities": []}, {"text": "Selecting features jointly across shards and averaging does counter the overfitting effect that is inherent to stochastic updating.", "labels": [], "entities": []}, {"text": "Our resulting models are learned on large data sets, but they are small and outperform models that tune feature sets of various sizes on small development sets.", "labels": [], "entities": []}, {"text": "Our software is freely available as apart of the cdec 1 framework.", "labels": [], "entities": [{"text": "cdec 1 framework", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.7814284364382426}]}, {"text": "1 https://github.com/redpony/cdec", "labels": [], "entities": []}], "datasetContent": [{"text": "The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in.", "labels": [], "entities": [{"text": "News Commentary (nc)", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.911471152305603}, {"text": "Europarl (ep) corpora", "start_pos": 102, "end_pos": 123, "type": "DATASET", "confidence": 0.8664426922798156}]}, {"text": "The translation direction is German-to-English.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9653378129005432}]}, {"text": "The SMT framework used in our experiments is hierarchical phrase-based translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9904111623764038}, {"text": "phrase-based translation", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7055222243070602}]}, {"text": "We use the cdec decoder) and induce SCFG grammars from two sets of symmetrized alignments using the method described by.", "labels": [], "entities": []}, {"text": "All data was tokenized and lowercased; German compounds were split.", "labels": [], "entities": []}, {"text": "For word alignment of the news-commentary data, we used GIZA++); for aligning the Europarl data, we used the Berkeley aligner ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.72721067070961}, {"text": "news-commentary data", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.7113940119743347}, {"text": "Europarl data", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9917542040348053}]}, {"text": "Before training, we collect all the grammar rules necessary to translate each individual sentence into separate files (so-called per-sentence grammars).", "labels": [], "entities": []}, {"text": "When decoding, cdec loads the appropriate file immediately prior to translation of the sentence.", "labels": [], "entities": [{"text": "translation of the sentence", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.8444070369005203}]}, {"text": "The computational overhead is minimal compared to the expense of decoding.", "labels": [], "entities": []}, {"text": "Also, deploying disk space instead of memory fits perfectly into the MapReduce framework we are working in.", "labels": [], "entities": []}, {"text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion () where rules are extracted fora parallel sentence pair only if the same rules are found in other sentences of the corpus as well.", "labels": [], "entities": []}, {"text": "3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in, using the SRILM toolkit) and binarized for efficient querying using kenlm.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9406552314758301}, {"text": "SRILM toolkit", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.8987973928451538}]}, {"text": "For the 5-gram language models, we replaced every word in the lm training data with <unk> that did not appear in the English part of the parallel training data to build an open vocabulary language model.", "labels": [], "entities": []}, {"text": "Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed persentence BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.8742247819900513}]}, {"text": "From the BLEU-reordered n-best list, translations were put into sets for the top 10% level (HI), the middle 80% level (MID), and the bottom 10% level (LOW).", "labels": [], "entities": [{"text": "BLEU-reordered", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.9914669990539551}, {"text": "80% level (MID)", "start_pos": 108, "end_pos": 123, "type": "METRIC", "confidence": 0.6608240902423859}, {"text": "LOW", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9525443911552429}]}, {"text": "These level sets are used for multipartite ranking: Overview of data used for train/dev/test.", "labels": [], "entities": []}, {"text": "News Commentary (nc) and Europarl (ep) training data and also News Crawl (crawl) dev/test data were taken from the WMT11 translation task (http://statmt.org/ wmt11/translation-task.html).", "labels": [], "entities": [{"text": "News Commentary (nc)", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8842530250549316}, {"text": "Europarl (ep) training data", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.8847932120164236}, {"text": "News Crawl (crawl) dev/test data", "start_pos": 62, "end_pos": 94, "type": "DATASET", "confidence": 0.8985871606402926}, {"text": "WMT11 translation task", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.6368438601493835}]}, {"text": "The dev/test data of nc are the sets provided with the WMT07 shared task (http://statmt.org/wmt07/shared-task.html).", "labels": [], "entities": [{"text": "WMT07 shared task", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.7935258944829305}]}, {"text": "Ep dev/test data is from WMT08 shared task (http://statmt.org/wmt08/shared-task.html).", "labels": [], "entities": [{"text": "WMT08 shared task", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.8609628478686014}]}, {"text": "The numbers in brackets for the rule counts of ep/nc training data are total counts of rules in the per-sentence grammars.", "labels": [], "entities": []}, {"text": "where translation pairs are built between the elements in HI-MID, HI-LOW, and MID-LOW, but not between translations inside sets on the same level.", "labels": [], "entities": []}, {"text": "This idea is depicted graphically in.", "labels": [], "entities": []}, {"text": "The intuition is to ensure that good translations are preferred over bad translations without teasing apart small differences.", "labels": [], "entities": []}, {"text": "For evaluation, we used the mteval-v11b.pl script to compute lowercased BLEU-4 scores).", "labels": [], "entities": [{"text": "BLEU-4 scores", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.9678545594215393}]}, {"text": "Statistical significance was measured using an Approximate Randomization test.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.6488325595855713}, {"text": "Approximate", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9931322932243347}]}, {"text": "All experiments for training on dev sets were carried out on a single computer.", "labels": [], "entities": []}, {"text": "For grammar extraction and training of the full data set we used a 30 node hadoop Map/Reduce cluster that can handle 300 jobs at once.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9012707769870758}]}, {"text": "We split the data into 2290 shards for the ep runs and 141 shards for the nc runs, each shard holding about 1,000 sentences, which corresponds to the dev set size of the nc data set.", "labels": [], "entities": []}, {"text": "The baseline learner in our experiments is a pairwise ranking perceptron that is used on various features and training data and plugged into various meta- algorithms for distributed processing.", "labels": [], "entities": []}, {"text": "The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of. gives a boxplot depicting BLEU-4 results for 100 runs of the MIRA implementation of the cdec package, tuned on dev-nc, and evaluated on the respective test set test-nc.", "labels": [], "entities": [{"text": "MIRA adaptation", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.5166804194450378}, {"text": "BLEU-4", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9986312985420227}]}, {"text": "We see a high variance (whiskers denote standard deviations) around a median of 27.2 BLEU and a mean of 27.1 BLEU.: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on newscommentary (nc) data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9983487129211426}, {"text": "BLEU.", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.9951541423797607}, {"text": "BLEU-4", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9985148310661316}, {"text": "newscommentary (nc) data", "start_pos": 205, "end_pos": 229, "type": "DATASET", "confidence": 0.6933496057987213}]}, {"text": "Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).", "labels": [], "entities": []}, {"text": "Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the same feature group is indicated by raised algorithm number.", "labels": [], "entities": []}, {"text": "\u2020 indicates statistically significant differences to best result across features groups for same algorithm, indicated in boldface.", "labels": [], "entities": []}, {"text": "@ indicates the optimal number of epochs chosen on the devtest set.", "labels": [], "entities": []}, {"text": "pergraph as is done in the cdec implementation of MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.652225136756897}]}, {"text": "We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hypergraph-MERT () both of which depend on hypergraph sampling.", "labels": [], "entities": [{"text": "PRO", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8079197406768799}]}, {"text": "In contrast, the perceptron is deterministic when started from a zero-vector of weights and achieves favorable 28.0 BLEU on the news-commentary test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9463666081428528}, {"text": "news-commentary test set", "start_pos": 128, "end_pos": 152, "type": "DATASET", "confidence": 0.8837958772977194}]}, {"text": "Since we are interested in relative improvements over a stable baseline, we restrict our attention in all following experiments to the perceptron.", "labels": [], "entities": []}, {"text": "7 shows the results of the experimental comparison of the 4 algorithms of Section 4.", "labels": [], "entities": []}, {"text": "The 7 Absolute improvements would be possible, e.g., by using larger language models or by adding news data to the ep training set when evaluating on crawl test sets (see, e.g.,), however, this is not the focus of this paper.", "labels": [], "entities": []}, {"text": "default features include 12 dense models defined on SCFG rules; The sparse features are the 3 templates described in Section 3.", "labels": [], "entities": []}, {"text": "All feature weights were tuned together using algorithms 1-4.", "labels": [], "entities": []}, {"text": "If not indicated otherwise, the perceptron was run for 10 epochs with learning rate \u03b7 = 0.0001, started at zero weight vector, using deduplicated 100-best lists.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.9619619250297546}]}, {"text": "The results on the news-commentary (nc) data show that training on the development set does not benefit from adding large feature sets -BLEU result differences between tuning 12 default features: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9982876181602478}, {"text": "BLEU-4", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9984055161476135}, {"text": "Europarl (ep) and news crawl (crawl) test data", "start_pos": 256, "end_pos": 302, "type": "DATASET", "confidence": 0.7460523719588915}]}, {"text": "Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).", "labels": [], "entities": []}, {"text": "Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the same feature group is indicated by raised algorithm number.", "labels": [], "entities": []}, {"text": "\u2020 indicates statistically significant differences to best result across features groups for same algorithm, indicated in boldface.", "labels": [], "entities": []}, {"text": "@ indicates the optimal number of epochs chosen on the devtest set. and tuning the full set of 180,000 features are not significant.", "labels": [], "entities": []}, {"text": "However, scaling all features to the full training set shows significant improvements for algorithm 3, and especially for algorithm 4, which gains 0.8 BLEU points over tuning 12 features on the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9990971088409424}]}, {"text": "The number of features rises to 4.7 million without feature selection, which iteratively selects 100,000 features with best 2 norm values across shards.", "labels": [], "entities": []}, {"text": "Feature templates such as rule n-grams and rule shapes only work if iterative mixing (algorithm 3) or feature selection (algorithm 4) are used.", "labels": [], "entities": []}, {"text": "Adding rule id features works in combination with other sparse features.", "labels": [], "entities": []}, {"text": "shows results for algorithms 1 and 4 on the Europarl data (ep) for different devtest and test sets.", "labels": [], "entities": [{"text": "Europarl data (ep)", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.9698371529579163}]}, {"text": "Europarl data were used in all runs for training and for setting the meta-parameter of number of epochs.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9768925905227661}]}, {"text": "Testing was done on the Europarl test set and news crawl test data from the years 2010 and 2011.", "labels": [], "entities": [{"text": "Europarl test set", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.9962749679883321}, {"text": "news crawl test data", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.8830111175775528}]}, {"text": "Here tuning large feature sets on the respective dev sets yields significant improvements of around 2 BLEU points over tuning the 12 default features on the dev sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9995633959770203}]}, {"text": "Another 0.5 BLEU points (test-crawl11) or even 1.3 BLEU points (testcrawl10) are gained when scaling to the full training set using iterative features selection.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9987139701843262}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9974181652069092}]}, {"text": "Result differences on the Europarl test set were not significant for moving from dev to full train set.", "labels": [], "entities": [{"text": "Europarl test set", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9929566383361816}]}, {"text": "Algorithms 2 and 3 were infeasible to run on Europarl data beyond one epoch because features vectors grew too large to be kept in memory.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9864958822727203}]}], "tableCaptions": [{"text": " Table 1: Overview of data used for train/dev/test. News Commentary (nc) and Europarl (ep) training data and  also News Crawl (crawl) dev/test data were taken from the WMT11 translation task (http://statmt.org/  wmt11/translation-task.html). The dev/test data of nc are the sets provided with the WMT07 shared  task (http://statmt.org/wmt07/shared-task.html). Ep dev/test data is from WMT08 shared task  (http://statmt.org/wmt08/shared-task.html). The numbers in brackets for the rule counts of ep/nc  training data are total counts of rules in the per-sentence grammars.", "labels": [], "entities": [{"text": "Europarl (ep) training data", "start_pos": 77, "end_pos": 104, "type": "DATASET", "confidence": 0.8392954766750336}, {"text": "News Crawl (crawl) dev/test data", "start_pos": 115, "end_pos": 147, "type": "DATASET", "confidence": 0.8740243713061014}, {"text": "WMT11 translation task", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.6906726559003195}, {"text": "WMT07 shared  task", "start_pos": 297, "end_pos": 315, "type": "DATASET", "confidence": 0.8436989784240723}, {"text": "WMT08 shared task", "start_pos": 385, "end_pos": 402, "type": "DATASET", "confidence": 0.8744313716888428}]}, {"text": " Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  \u2020 indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.99752277135849}]}, {"text": " Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test  data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).  Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the  same feature group is indicated by raised algorithm number.  \u2020 indicates statistically significant differences to best  result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs  chosen on the devtest set.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9983371496200562}, {"text": "Europarl (ep) and news crawl (crawl) test  data", "start_pos": 70, "end_pos": 117, "type": "DATASET", "confidence": 0.7530463387568792}]}]}