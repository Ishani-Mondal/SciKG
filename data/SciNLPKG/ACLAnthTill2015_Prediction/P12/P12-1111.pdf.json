{"title": [{"text": "Exploring Deterministic Constraints: From a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.5179688334465027}]}], "abstractContent": [{"text": "We show for both English POS tagging and Chinese word segmentation that with proper representation, large number of deterministic constraints can be learned from training examples , and these are useful in constraining prob-abilistic inference.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.8280949592590332}, {"text": "Chinese word segmentation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.5902453263600668}]}, {"text": "For tagging, learned constraints are directly used to constrain Viterbi decoding.", "labels": [], "entities": [{"text": "tagging", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9729387760162354}]}, {"text": "For segmentation, character-based tagging constraints can be learned with the same templates.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9776248931884766}]}, {"text": "However, they are better applied to a word-based model, thus an integer linear programming (ILP) formulation is proposed.", "labels": [], "entities": []}, {"text": "For both problems, the corresponding constrained solutions have advantages in both efficiency and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9976953864097595}]}], "introductionContent": [{"text": "In recent work, interesting results are reported for applications of integer linear programming (ILP) such as semantic role labeling (SRL)), dependency parsing ( and soon.", "labels": [], "entities": [{"text": "semantic role labeling (SRL))", "start_pos": 110, "end_pos": 139, "type": "TASK", "confidence": 0.7751326759656271}, {"text": "dependency parsing", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.8845905065536499}]}, {"text": "In an ILP formulation, 'non-local' deterministic constraints on output structures can be naturally incorporated, such as \"a verb cannot take two subject arguments\" for SRL, and the projectivity constraint for dependency parsing.", "labels": [], "entities": [{"text": "SRL", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.9559877514839172}, {"text": "dependency parsing", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.81126669049263}]}, {"text": "In contrast to probabilistic constraints that are estimated from training examples, this type of constraint is usually hand-written reflecting one's linguistic knowledge.", "labels": [], "entities": []}, {"text": "Dynamic programming techniques based on Markov assumptions, such as Viterbi decoding, cannot handle those 'non-local' constraints as discussed above.", "labels": [], "entities": []}, {"text": "However, it is possible to constrain Viterbi decoding by 'local' constraints, e.g. \"assign label t to word w\" for POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.6984783113002777}]}, {"text": "This type of constraint may come from human input solicited in interactive inference procedure (.", "labels": [], "entities": []}, {"text": "In this work, we explore deterministic constraints for two fundamental NLP problems, English POS tagging and Chinese word segmentation.", "labels": [], "entities": [{"text": "English POS tagging", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.5507530272006989}, {"text": "Chinese word segmentation", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.6084021429220835}]}, {"text": "We show by experiments that, with proper representation, large number of deterministic constraints can be learned automatically from training data, which can then be used to constrain probabilistic inference.", "labels": [], "entities": []}, {"text": "For POS tagging, the learned constraints are directly used to constrain Viterbi decoding.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8656575977802277}]}, {"text": "The corresponding constrained tagger is 10 times faster than searching in a raw space pruned with beam-width 5.", "labels": [], "entities": []}, {"text": "Tagging accuracy is moderately improved as well.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9482398629188538}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9828439950942993}]}, {"text": "For Chinese word segmentation (CWS), which can be formulated as character tagging, analogous constraints can be learned with the same templates as English POS tagging.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7828523069620132}, {"text": "character tagging", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7180737406015396}, {"text": "POS tagging", "start_pos": 155, "end_pos": 166, "type": "TASK", "confidence": 0.6445761770009995}]}, {"text": "High-quality constraints can be learned with respect to a special tagset, however, with this tagset, the best segmentation accuracy is hard to achieve.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9136661887168884}]}, {"text": "Therefore, these character-based constraints are not directly used for determining predictions as in English POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.7704050838947296}]}, {"text": "We propose an ILP formulation of the CWS problem.", "labels": [], "entities": [{"text": "CWS problem", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.906758576631546}]}, {"text": "By adopting this ILP formulation, segmentation F-measure is increased from 0.968 to 0.974, as compared to Viterbi decoding with the same feature set.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.950870931148529}, {"text": "F-measure", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.8427416086196899}]}, {"text": "Moreover, the learned constraints can be applied to reduce the number of possible words over a character sequence, i.e. to reduce the number of variables to set.", "labels": [], "entities": []}, {"text": "This reduction of problem size immediately speeds up an ILP solver by more than 100 times.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9367255866527557}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 6: POS tagging with deterministic constraints.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8035959005355835}]}, {"text": " Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. \u2212es. A  constraint is composed of w \u22121 , w 0 and w 1 , as well  as the morph features m \u22121 , m 0 and m 1 . For ex-", "labels": [], "entities": []}, {"text": " Table 8: Character tagging with deterministic constraints.", "labels": [], "entities": [{"text": "Character tagging", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9189768731594086}]}, {"text": " Table 10: F-measure on Chinese word segmentation.  Only character-based features are used. POS-/+: percep- tron trained without/with POS.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9869612455368042}, {"text": "Chinese word segmentation", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.5997030933698019}]}, {"text": " Table 11: ILP problem size and segmentation speed.", "labels": [], "entities": [{"text": "ILP problem size", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8763648668924967}, {"text": "segmentation", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.9651869535446167}]}]}