{"title": [{"text": "Computational Generation of Referring Expressions: A Survey", "labels": [], "entities": [{"text": "Computational Generation of Referring Expressions", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8634315133094788}]}], "abstractContent": [{"text": "This article offers a survey of computational research on referring expression generation (REG).", "labels": [], "entities": [{"text": "referring expression generation (REG)", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.8466422160466512}]}, {"text": "It introduces the REG problem and describes early work in this area, discussing what basic assumptions lie behind it, and showing how its remit has widened in recent years.", "labels": [], "entities": [{"text": "REG problem", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9319484829902649}]}, {"text": "We discuss computational frameworks underlying REG, and demonstrate a recent trend that seeks to link REG algorithms with well-established Knowledge Representation techniques.", "labels": [], "entities": [{"text": "Knowledge Representation", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.6861226111650467}]}, {"text": "Considerable attention is given to recent efforts at evaluating REG algorithms and the lessons that they allow us to learn.", "labels": [], "entities": [{"text": "REG algorithms", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.9126402735710144}]}, {"text": "The article concludes with a discussion of the way forward in REG, focusing on references in larger and more realistic settings.", "labels": [], "entities": [{"text": "REG", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9440883994102478}]}], "introductionContent": [{"text": "Suppose one wants to point out a person in to an addressee.", "labels": [], "entities": []}, {"text": "Most speakers have no difficulty in accomplishing this task, by producing a referring expression such as \"the man in a suit,\" for example.", "labels": [], "entities": []}, {"text": "Now imagine a computer being confronted with the same task, aiming to point out individual d . Assuming it has access to a database containing all the relevant properties of the people in the scene, it needs to find some combination of properties which applies to d 1 , and not to the other two.", "labels": [], "entities": []}, {"text": "There is a choice though: There are many ways in which d 1 can beset apart from the rest (\"the man on the left,\" \"the man with the glasses,\" \"the man with the tie\"), and the computer has to decide which of these is optimal in the given context.", "labels": [], "entities": []}, {"text": "Moreover, optimality can mean different things.", "labels": [], "entities": []}, {"text": "It might bethought, for instance, that references are optimal when they are minimal in length, containing just enough information to single out the target.", "labels": [], "entities": []}, {"text": "But, as we shall see, finding minimal references is computationally expensive, and it is not necessarily what speakers do, nor what is most useful to hearers.", "labels": [], "entities": []}, {"text": "So, what is Referring Expression Generation?", "labels": [], "entities": [{"text": "Referring Expression Generation", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.8299412926038107}]}, {"text": "Referring expressions play a central role in communication, and have been studied extensively in many branches of (computational) linguistics, including Natural Language Generation (NLG).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 153, "end_pos": 186, "type": "TASK", "confidence": 0.8085910975933075}]}, {"text": "NLG is concerned with the process of automatically converting non-linguistic information (e.g.,", "labels": [], "entities": []}], "datasetContent": [{"text": "Text corpora are full of referring expressions.", "labels": [], "entities": []}, {"text": "For evaluating the realization of referring expressions, such corpora are very suitable, and various researchers have used them, for instance, to evaluate algorithms for modifier orderings).", "labels": [], "entities": []}, {"text": "Text corpora are also important for the study of anaphoric links between referring expressions.", "labels": [], "entities": []}, {"text": "The texts that makeup the GNOME corpus (), for instance, contain descriptions of museum objects and medical patient information leaflets, with each of the two subcorpora containing some 6,000 NPs.", "labels": [], "entities": [{"text": "GNOME corpus", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.9232436120510101}]}, {"text": "Much information is marked up, including anaphoric links.", "labels": [], "entities": []}, {"text": "Yet, text corpora of this kind are of limited value for evaluating the content selection part of REG algorithms.", "labels": [], "entities": []}, {"text": "For that, one needs a corpus that is fully \"semantically transparent\" (van Deemter, van der Sluis, and Gatt 2006): A corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target.", "labels": [], "entities": []}, {"text": "Text corpora such as GNOME do not meet this requirement, and it is often difficult or impossible to add all necessary information, because of the size and complexity of the relevant domains.", "labels": [], "entities": []}, {"text": "For this reason, data sets for content selection evaluation are typically collected via experiments with human participants in simple and controlled settings.", "labels": [], "entities": [{"text": "content selection evaluation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.8395006656646729}]}, {"text": "Broadly speaking, two kinds of experimental corpora can be distinguished: corpora specifically collected with reference in mind, and corpora collected wholly or partly for other purposes but which have nevertheless been analyzed for the referring expressions in them.", "labels": [], "entities": []}, {"text": "We will briefly sketch some corpora of the latter kind, after which we shall discuss the former in more detail.", "labels": [], "entities": []}, {"text": "One way to elicit \"natural\" references is to let participants perform a task for which they need to refer to objects.", "labels": [], "entities": []}, {"text": "An example is the corpus of so-called pear stories of, in which people were asked to describe a movie about a man harvesting pears, in a fluent narrative.", "labels": [], "entities": []}, {"text": "The resulting narratives featured such sequences as \"And he fills his thing with pears, and comes down and there's a basket he puts them in.", "labels": [], "entities": []}, {"text": "And then a boy comes by, on a bicycle, the man is in the tree, and the boy gets off his bicycle . .", "labels": [], "entities": []}, {"text": ",\" where a limited set of individuals come up several times.", "labels": [], "entities": []}, {"text": "The referring expressions in a subset of these stories were analyzed by, who asked how the form of the re-descriptions (such as \"he,\" \"them,\" and \"the man\") in these narratives might best be predicted, comparing \"informational\" considerations (which form the core of most algorithms in the tradition started by Dale and Reiter, as we have seen) with considerations based on Centering Theory).", "labels": [], "entities": []}, {"text": "Passonneau, who tested her rules on 319 noun phrases, found support for an integrated model, where centering constraints take precedence over informational considerations.", "labels": [], "entities": []}, {"text": "The well-known Map Task corpus () is another example of a corpus in which reference plays an important role.", "labels": [], "entities": []}, {"text": "It consists of dialogues between two participants; both have maps with landmarks indicated, but only one (the instruction giver) has a route on the map and he or she instructs the other (the follower) about this particular route.", "labels": [], "entities": []}, {"text": "Referring expressions are routinely produced in this task to refer to the landmarks on the maps (\"the cliff\").", "labels": [], "entities": []}, {"text": "Participants use these not only for identification purposes but also, for instance, to verify whether they understood their dialogue partner correctly.", "labels": [], "entities": []}, {"text": "In the original Map Task corpus, the landmarks were labeled with proper names (\"cliff\"), making them less suitable for studying content determination.", "labels": [], "entities": [{"text": "studying content determination", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.6547952890396118}]}, {"text": "To facilitate the study of reference, the iMap corpus was created, a modified version of the Map Task corpus where landmarks are not labelled, and systematically differ along a number of dimensions, including type (owl, penguin, etc.), number (singular, plural) and color; a target may thus be referred to as \"the two purple owls.\"", "labels": [], "entities": [{"text": "iMap corpus", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9181132316589355}]}, {"text": "Because participants may refer to targets more than once, it becomes possible to study initial and subsequent reference ).", "labels": [], "entities": []}, {"text": "Yet another example is the Coconut corpus), a set of taskoriented dialogues in which participants negotiate which furniture items they want to buy on a fixed, shared budget.", "labels": [], "entities": []}, {"text": "Referring expressions in this corpus (\"a yellow rug for 150 dollars\") do not only contain information to identify a particular piece of furniture, but also include properties which directly refer to the task at hand (e.g., how much money is still available fora particular furniture item and what the state of agreement between the negotiators is).", "labels": [], "entities": []}, {"text": "An attractive aspect of these corpora is that they represent fairly realistic communication, related to a more or less natural task.", "labels": [], "entities": []}, {"text": "However, in these corpora, the identification of objects tends to be mixed with other communicative tasks (verification, negotiating).", "labels": [], "entities": []}, {"text": "This does not mean that the corpora in question are unsuitable for the study of reference, of course.", "labels": [], "entities": []}, {"text": "More specifically, they have been used for evaluating REG algorithms, to compare the performance of traditional algorithms with special-purpose algorithms that take dialogue context into account).", "labels": [], "entities": [{"text": "REG algorithms", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.840663731098175}]}, {"text": "For example, when the speaker attempts to persuade the hearer to buy an item, Jordan's Intentional Influences algorithm selects those properties of the item that make it a better solution than a previously discussed item.", "labels": [], "entities": []}, {"text": "In yet other situations-for example, when a summarization is offered-all mutually known properties of the item are selected.", "labels": [], "entities": []}, {"text": "Jordan's algorithm outperforms traditional algorithms, which is perhaps not surprising given that the latter were not designed to deal with references in interactive settings.", "labels": [], "entities": []}, {"text": "In recent years, a number of new corpora have been collected, specifically focusing on the types of referring expressions that we are focusing on in this survey.", "labels": [], "entities": []}, {"text": "A number of such corpora are summarized in.", "labels": [], "entities": []}, {"text": "In some ways, these corpora are remarkably similar.", "labels": [], "entities": []}, {"text": "Reflecting the prevalent aims of research on REG, for example, they focus on descriptions that aim to identify their referent \"in one shot,\" disregarding the linguistic context of the expression (i.e., in the \"null context,\" as it is sometimes called).", "labels": [], "entities": [{"text": "REG", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9737512469291687}]}, {"text": "In all these corpora, participants were asked to refer to targets in a visual scene also containing the distractors.", "labels": [], "entities": []}, {"text": "This set-up means that the properties of target objects and their distractors are known, which makes it comparatively easy to make these corpora semantically transparent by annotating the references that were produced.", "labels": [], "entities": []}, {"text": "In addition, most corpora are \"pragmatically transparent\" as well, meaning that the communicative goals of the participants were known (typically identification).", "labels": [], "entities": []}, {"text": "An early example is the Bishop corpus.", "labels": [], "entities": [{"text": "Bishop corpus", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9390597641468048}]}, {"text": "For this data set, participants were asked to describe objects in various computer generated scenes.", "labels": [], "entities": []}, {"text": "Each of these scenes contained up to 30 objects (\"cones\") randomly positioned on a virtual surface.", "labels": [], "entities": []}, {"text": "All objects had the same shape and size, and hence targets could only be distinguished using their color (either green or purple) and their location on the surface (\"the green cone at the left bottom\").", "labels": [], "entities": []}, {"text": "Each participant was asked to identify targets in one shot, and for the benefit of an addressee who was physically present but did not interact with the participant.", "labels": [], "entities": []}, {"text": "The Drawer corpus, collected by, has a similar objective, but here targets are real, being one of 16 colored drawers in a filing cabinet.", "labels": [], "entities": [{"text": "Drawer corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8384493887424469}]}, {"text": "On different occasions, participants were given a random number between 1 and 16 and asked to refer to the corresponding drawer for an onlooker.", "labels": [], "entities": []}, {"text": "Naturally, they were asked not to use the number; instead they could refer to the target drawers using color, row, and column, or some combination of these.", "labels": [], "entities": []}, {"text": "In this corpus, referring expressions (\"the pink drawer in the first row, third column\") once again solely serve an identification purpose.", "labels": [], "entities": []}, {"text": "also collected another corpus (GRE3D3) specifically looking at when participants use spatial relations.", "labels": [], "entities": []}, {"text": "For this data collection, participants were presented with 3D scenes (made with Google SketchUp) containing three simple geometric objects (spheres and cubes of different colors and sizes, and in different configurations), of which one was the target.", "labels": [], "entities": []}, {"text": "found that spatial relations were frequently used (\"the ball in front of the cube\"), even though they were never required for identification.", "labels": [], "entities": []}, {"text": "Whether this generalizes to other visual scenes (in which spatial relations are less immediately 'available') is an interesting question for future research.", "labels": [], "entities": []}, {"text": "The TUNA corpus) was collected via a Web-based experiment, in which singular and plural descriptions were gathered by showing participants one or two targets, where the plural targets could either be similar (same type) or dissimilar (different type).", "labels": [], "entities": [{"text": "TUNA corpus)", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9191698034604391}]}, {"text": "Targets were always displayed with six distractors, and the resulting domain objects were randomly positioned in a 3 x 5 grid, with targets surrounded by a red border.", "labels": [], "entities": []}, {"text": "Example trials are shown in.", "labels": [], "entities": []}, {"text": "The corpus contains two different domains: a furniture domain and a people domain.", "labels": [], "entities": []}, {"text": "The first domain is based on pictures of furniture and household items, taken from the Object Databank (see http://www.tarrlab.org/).", "labels": [], "entities": [{"text": "Object Databank", "start_pos": 87, "end_pos": 102, "type": "DATASET", "confidence": 0.9240440726280212}]}, {"text": "These were manipulated so that besides type (chair, desk, fan) also color, orientation, and size could systematically be varied.", "labels": [], "entities": []}, {"text": "The number of possible attributes and values in the people domain is much Example trials from the TUNA corpus, a singular trial for the furniture domain (\"the small blue fan,\" left) and a plural trial for the people domain (\"the men with glasses,\" right).", "labels": [], "entities": [{"text": "TUNA corpus", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.9317180514335632}]}, {"text": "larger (and more difficult to pin down); this domain consists of a set of black and white photographs of people (all famous mathematicians) used in an earlier study of van der.", "labels": [], "entities": []}, {"text": "Properties of these photographs include gender, head orientation, age, beard, hair, glasses, suit, shirt, and tie.", "labels": [], "entities": []}, {"text": "It is interesting to note that the TUNA corpus was designed to have one shortest description for each target, whereas in other data sets, such as Viethen and drawer corpus, a single shortest description does not always exist.", "labels": [], "entities": [{"text": "TUNA corpus", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8672590851783752}, {"text": "Viethen and drawer corpus", "start_pos": 146, "end_pos": 171, "type": "DATASET", "confidence": 0.6980685815215111}]}, {"text": "The TUNA corpus has formed the basis of three shared REG challenges, to which we turn now.", "labels": [], "entities": [{"text": "TUNA corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9371778070926666}]}, {"text": "How should we compare human descriptions with those produced by a REG algorithm?", "labels": [], "entities": []}, {"text": "When looking for measures that compute the content overlap, one source of inspiration may come from biology and information retrieval.", "labels": [], "entities": []}, {"text": "One measure used in these fields is the Dice (1945) coefficient, which was originally proposed to quantify ecologic association between species, and was first applied to REG by.", "labels": [], "entities": [{"text": "Dice (1945) coefficient", "start_pos": 40, "end_pos": 63, "type": "METRIC", "confidence": 0.5175980150699615}, {"text": "REG", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.8851950168609619}]}, {"text": "The Dice coefficient-which is not dissimilar to the \"match\" function used by Jordan (2000)-is computed by scaling the number of elements that two sets have in common, by the size of the two sets combined: The Dice measure ranges from 0 (no agreement; i.e., no elements shared between A and B) to 1 (complete agreement; A and B share all elements).", "labels": [], "entities": []}, {"text": "For REG, A and B can be understood as attributes (e.g., type) or as attribute-value pairs (properties; \ud97b\udf59type, man\ud97b\udf59).", "labels": [], "entities": [{"text": "REG", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9258238077163696}]}, {"text": "The former option tends to be used in earlier work, but has the somewhat counterintuitive consequence that two descriptions which express different values of the same attribute (\"the man\" and \"the woman,\" say, or \"the dog\" and \"the chihuahua,\" in the earlier discussed cats-and-dogs example) have a Dice score of 1.", "labels": [], "entities": [{"text": "Dice score", "start_pos": 299, "end_pos": 309, "type": "METRIC", "confidence": 0.7920643389225006}]}, {"text": "Hence, in the following discussion we shall measure overlap in terms of properties.", "labels": [], "entities": []}, {"text": "An alternative to Dice that is sometimes used is the MASI (Measuring Agreement on Set-valued Items) metric of Passonneau: This is basically an extension of the well-known Jaccard (1901) metric with a weighting function \u03b4 which biases the score in favor of similarity where one set is a sub-or a superset of the other: Dice and MASI are straightforward measures for overlap, but they do have their disadvantages.", "labels": [], "entities": [{"text": "MASI", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9008579254150391}]}, {"text": "For example, they assume that all properties are independent and that all are equally different from each other.", "labels": [], "entities": []}, {"text": "Suppose a human participant referred to d 1 in our example domain as \"the man in the suit next to a woman,\" and consider the following two references produced by a REG algorithm: \"the man in the suit\" and \"the man next to a woman.\"", "labels": [], "entities": []}, {"text": "Both omit one property from the human reference and thus have the same Dice and MASI scores.", "labels": [], "entities": [{"text": "Dice and MASI scores", "start_pos": 71, "end_pos": 91, "type": "METRIC", "confidence": 0.7950402349233627}]}, {"text": "But only the former reference is distinguishing; the latter is not.", "labels": [], "entities": []}, {"text": "This problem could be solved, for example, by adopting a binary weighted version of the metrics which multiply the resulting score with 1 fora distinguishing description and with 0 fora non-distinguishing one.", "labels": [], "entities": []}, {"text": "A more general issue with these overlap metrics can be illustrated with an example from Richard Power (personal communication).", "labels": [], "entities": []}, {"text": "Consider the two (roughly equivalent) expressions \"the palomino\" and \"the horse with the gold coat and white mane and tail.\"", "labels": [], "entities": []}, {"text": "Straightforward counting of attribute-value pairs would result in an overlap score of zero, which would be misleading, because the two descriptions express essentially the same content, with the latter description combining, in one property, all properties expressed in the former.", "labels": [], "entities": [{"text": "overlap score", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9655520617961884}]}, {"text": "This problem clearly calls fora more principled approach to representing and counting properties.", "labels": [], "entities": []}, {"text": "During evaluations, Dice or MASI scores are typically averaged over references for different trials and produced by different human participants, making them fairly rough measures.", "labels": [], "entities": [{"text": "Dice or MASI scores", "start_pos": 20, "end_pos": 39, "type": "METRIC", "confidence": 0.6857409328222275}]}, {"text": "It could be that an algorithm's predictions match the descriptions of some participants very well, but those of other participants not at all.", "labels": [], "entities": []}, {"text": "To partially compensate for this, sometimes also the proportion of times an algorithm achieves a perfect match with a human reference is reported.", "labels": [], "entities": []}, {"text": "This measure is known, somewhat confusingly, as), the Perfect Recall Percentage (PRP), and Accuracy . The measures discussed so far do not take the actual linguistic realization of the referring expressions into account.", "labels": [], "entities": [{"text": "Perfect Recall Percentage (PRP)", "start_pos": 54, "end_pos": 85, "type": "METRIC", "confidence": 0.965804934501648}, {"text": "Accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9989857077598572}]}, {"text": "For these, string distance metrics are obvious candidates, because these have proven their worth in various other areas of computational linguistics.", "labels": [], "entities": []}, {"text": "One well-known string distance metric, which has also been proposed for REG evaluation, is the Levenshtein (1966) distance: The minimal number of insertions, deletions, and substitutions needed to convert one string into another, possibly normalized with respect to length (Bangalore, Rambow, and Whittaker 2000).", "labels": [], "entities": [{"text": "REG evaluation", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.9698688387870789}]}, {"text": "The BLEU ( and NIST (Doddington 2002) metrics, which have their origin in machine translation evaluation, have also been proposed for REG evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9949297308921814}, {"text": "NIST (Doddington 2002) metrics", "start_pos": 15, "end_pos": 45, "type": "DATASET", "confidence": 0.9104091227054596}, {"text": "machine translation evaluation", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.8561685681343079}, {"text": "REG evaluation", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.9650885164737701}]}, {"text": "BLEU measures n-gram overlap between strings; for machine translation n is often set to 4, but given that referring expressions tend to be short, n = 3 seems a better option for REG evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9878436326980591}, {"text": "machine translation n", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7981426318486532}, {"text": "REG evaluation", "start_pos": 178, "end_pos": 192, "type": "TASK", "confidence": 0.9151189923286438}]}, {"text": "NIST is a BLEU variant giving more importance to less frequent (and hence more informative) n-grams.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9504758715629578}, {"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978092312812805}]}, {"text": "Finally,  also use the rouge-2 and rouge-su4 measures (Lin and Hovy 2003), originally proposed for evaluating automatically generated summaries.", "labels": [], "entities": []}, {"text": "An obvious benefit of these string metrics is that they are easy to compute automatically, whereas property-based evaluation measures such as Dice require an extensive manual annotation of selected properties.", "labels": [], "entities": []}, {"text": "However, the added value of string-based metrics for REG is relatively unclear.", "labels": [], "entities": [{"text": "REG", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9399029016494751}]}, {"text": "It is not obvious, for instance, that a smaller Levenshtein distance is always to be preferred over a longer one; the expressions \"the man wearing a t-shirt\" and \"the woman wearing a t-shirt\" are at a mere Levenshtein distance of 2 from each other, but only the former would be a good description for target d 3 . On the other hand, \"the male person on the right\" is at a Levenshtein distance of 15 from \"the man wearing a t-shirt,\" and both are perfect descriptions of d 3 . Alternatively, referring expressions could also be evaluated by human judges, although this obviously is more time consuming than an automatic evaluation.", "labels": [], "entities": []}, {"text": "collected judgments of Adequacy (\"How clear is this description?", "labels": [], "entities": []}, {"text": "Try to imagine someone who could seethe same grid with the same pictures, but didn't know which of the pictures was the target.", "labels": [], "entities": []}, {"text": "How easily would they be able to find it, based on the phrase given?\") and Fluency (\"How fluent is this description?.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9982430934906006}]}, {"text": "Is it good, clear English?\").", "labels": [], "entities": []}, {"text": "One may also be interested in the extent to which references are useful for addressees.", "labels": [], "entities": []}, {"text": "This can be evaluated in a number of different ways.", "labels": [], "entities": []}, {"text": ", for example, first showed participants a generated description fora trial.", "labels": [], "entities": []}, {"text": "After participants read this description, a scene appeared and participants were asked to click on the intended target.", "labels": [], "entities": []}, {"text": "This allowed them to compute three extrinsic evaluation metrics: the reading time, the identification time, and the error rate, which they defined as the number of incorrectly identified targets.", "labels": [], "entities": [{"text": "error rate", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9839776456356049}]}, {"text": "In early REG research, including, it was often remarkably unclear what exactly the proposed algorithms aimed to achieve.", "labels": [], "entities": [{"text": "REG", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9796575903892517}]}, {"text": "It was only when evaluation studies were starting to be conducted that researchers had to \"show their cards\" and say what they regarded as their criterion for success.", "labels": [], "entities": []}, {"text": "In most cases, they used a form of human-likeness as their success criterion, by comparing the expressions generated by an algorithm with those in a corpus.", "labels": [], "entities": []}, {"text": "The human-likeness criterion dictates that REG algorithms are to mimic humans \"warts and all\": If speakers produce unclear descriptions, then so should algorithms.", "labels": [], "entities": []}, {"text": "But, of course, human-likeness is not the only yardstick that can be used.", "labels": [], "entities": []}, {"text": "In NLG systems whose main aim is to be practically useful, for example, it maybe more important for referring expressions to be clear than to be human-like in all respects.", "labels": [], "entities": []}, {"text": "The difference is important because psycholinguists have shown that human speakers have only limited capabilities for taking the addressee into account, frequently producing expressions that cannot be interpreted correctly by an addressee-for example, when they are under time pressure.", "labels": [], "entities": []}, {"text": "If usefulness or successfulness (, rather than human-likeness, is the yardstick for success then a different type of evaluation test needs to be used.", "labels": [], "entities": []}, {"text": "Possible tests include, for example, speed and accuracy of task completion (i.e., how often and how fast do readers find the referent?).", "labels": [], "entities": [{"text": "speed", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.99592125415802}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9987014532089233}]}, {"text": "A variety of hearer-oriented tests is starting to be used in recent REG research, but evaluation of REG algorithms (and of NLG in general) remains difficult (see, e.g.,.", "labels": [], "entities": []}, {"text": "Arguably, a central problem is that many different evaluations metrics are conceivable, and an REG algorithm (like an NLG system in general) may well score high on some and poorly on other metrics.", "labels": [], "entities": []}, {"text": "Hearer-oriented experiments may also be useful for evaluating referring expressions that are logically complex (cf. Section 4.4).", "labels": [], "entities": []}, {"text": "It is one thing for an REG algorithm to use logical quantification to generate a fairly simple description, such as \"the woman who owns four cats,\" but quite another to generate a highly complex description (\"the woman who owns four cats that are chased by between three and six dogs each of which is fed only by men\"), which can be generated using the same methods.", "labels": [], "entities": []}, {"text": "There are difficult methodological questions to be answered hereabout whether the aim of the generator is to model human competence or human performance.", "labels": [], "entities": []}, {"text": "And if it is performance that is to be modeled, then this raises the question of what types of complexities are exploited by human speakers, and what types of complexities are understandable to human hearers.", "labels": [], "entities": []}, {"text": "Such questions can only be answered by new empirical studies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Overview of dedicated Referring Expression corpora (alphabetical), with for each corpus a  representative reference, an indication of the domain, and the number of participants and  collected distinguishing descriptions.", "labels": [], "entities": []}]}