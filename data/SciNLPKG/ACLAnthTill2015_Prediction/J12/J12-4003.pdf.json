{"title": [{"text": "Semantic Role Labeling of Implicit Arguments for Nominal Predicates", "labels": [], "entities": [{"text": "Semantic Role Labeling of Implicit Arguments for Nominal Predicates", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.7464662790298462}]}], "abstractContent": [{"text": "Nominal predicates often carry implicit arguments.", "labels": [], "entities": [{"text": "Nominal predicates", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8251611292362213}]}, {"text": "Recent work on semantic role labeling has focused on identifying arguments within the local context of a predicate; implicit arguments, however, have not been systematically examined.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.6760445038477579}]}, {"text": "To address this limitation, we have manually annotated a corpus of implicit arguments for ten predicates from NomBank.", "labels": [], "entities": []}, {"text": "Through analysis of this corpus, we find that implicit arguments add 71% to the argument structures that are present in NomBank.", "labels": [], "entities": []}, {"text": "Using the corpus, we train a discriminative model that is able to identify implicit arguments with an F 1 score of 50%, significantly outperforming an informed baseline model.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9876558383305868}]}, {"text": "This article describes our investigation, explores a wide variety of features important for the task, and discusses future directions for work on implicit argument identification.", "labels": [], "entities": [{"text": "implicit argument identification", "start_pos": 146, "end_pos": 178, "type": "TASK", "confidence": 0.6979512572288513}]}], "introductionContent": [{"text": "Recent work has shown that semantic role labeling (SRL) can be applied to nominal predicates in much the same way as verbal predicates (.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7945378869771957}]}, {"text": "In general, the nominal SRL problem is formulated as follows: Given a predicate that is annotated in NomBank as bearing arguments, identify these arguments within the clause or sentence that contains the predicate.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8335981965065002}]}, {"text": "As shown in our previous work, this problem definition ignores the important fact that many nominal predicates do not bear arguments in the local context.", "labels": [], "entities": []}, {"text": "Such predicates need to be addressed in order for nominal SRL to be used by downstream applications such as automatic question answering, information extraction, and statistical machine translation.", "labels": [], "entities": [{"text": "SRL", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8763163089752197}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8138355314731598}, {"text": "information extraction", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.8517716825008392}, {"text": "statistical machine translation", "start_pos": 166, "end_pos": 197, "type": "TASK", "confidence": 0.7502983609835306}]}, {"text": "Gerber, showed that it is possible to accurately identify nominal predicates that bear arguments in the local context.", "labels": [], "entities": []}, {"text": "This makes the nominal SRL system applicable to text that does not contain annotated predicates.", "labels": [], "entities": [{"text": "SRL", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8356998562812805}]}, {"text": "The system does not address a fundamental question regarding arguments of nominal predicates, however: If an argument is missing from the local context of a predicate, might the argument be located somewhere in the wider discourse?", "labels": [], "entities": []}, {"text": "Most prior work on nominal and verbal SRL has stopped short of answering this question, opting instead for an approach that only labels local arguments and thus ignores predicates whose arguments are entirely non-local.", "labels": [], "entities": [{"text": "nominal and verbal SRL", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.5738771036267281}]}, {"text": "This article directly addresses the issue of non-local (or implicit) argument identification for nominal predicates.", "labels": [], "entities": [{"text": "non-local (or implicit) argument identification", "start_pos": 45, "end_pos": 92, "type": "TASK", "confidence": 0.8228175384657723}]}, {"text": "As an initial example, consider the following sentence, which is taken from the Penn TreeBank (Marcus, Santorini, and Marcinkiewicz 1993): (1) A SEC proposal to ease [arg reporting] [predicate requirements] [arg for some company executives] would undermine the usefulness of information on insider trades, professional money managers contend.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9924580454826355}]}, {"text": "The NomBank (Meyers 2007) role set for requirement is shown here: Frame for requirement, role set 1: arg 0 : the entity that is requiring something arg 1 : the entity that is required arg 2 : the entity of which something is being required In Example (1), the predicate has been annotated with the local argument labels provided by NomBank.", "labels": [], "entities": []}, {"text": "As shown, NomBank does not annotate an arg 0 for this instance of the requirement predicate; a reasonable interpretation of the sentence, however, is that SEC is the entity that is requiring something.", "labels": [], "entities": []}, {"text": "This article refers to arguments such as SEC in Example (1) as implicit.", "labels": [], "entities": [{"text": "SEC", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.7784777879714966}]}, {"text": "In this work, the notion of implicit argument covers any argument that is not annotated by NomBank.", "labels": [], "entities": []}, {"text": "Building on Example (1), consider the following sentence, which directly follows Example (1) in the corresponding TreeBank document: (2) Money managers make the argument in letters to the agency about [arg 1 rule] [predicate changes] proposed this past summer.", "labels": [], "entities": []}, {"text": "The NomBank role set for change is as follows: Frame for change, role set 1: arg 0 : the entity that initiates the change arg 1 : the entity that is changed arg 2 : the initial state of the changed entity arg 3 : the final state of the changed entity Similarly to the previous example, Example (2) shows the local argument labels provided by NomBank.", "labels": [], "entities": []}, {"text": "These labels only indicate that rules have been changed.", "labels": [], "entities": []}, {"text": "For a full interpretation, Example (2) requires an understanding of Example (1 the sentence from Example 1, the reader has noway of knowing that the agency in Example (2) actually refers to the same entity as SEC in Example (1).", "labels": [], "entities": []}, {"text": "As part of the reader's comprehension process, this entity is identified as the filler for the arg 0 role in Example (2).", "labels": [], "entities": []}, {"text": "This identification must occur in order for these two sentences to form a coherent discourse.", "labels": [], "entities": []}, {"text": "From these examples, it is clear that the scope of implicit arguments quite naturally spans sentence boundaries.", "labels": [], "entities": []}, {"text": "Thus, if one wishes to recover implicit arguments as part of the SRL process, the argument search space must be expanded beyond the traditional, single-sentence window used in virtually all prior SRL research.", "labels": [], "entities": [{"text": "SRL process", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.90221306681633}, {"text": "SRL", "start_pos": 196, "end_pos": 199, "type": "TASK", "confidence": 0.9702550768852234}]}, {"text": "What can we hope to gain from such a fundamental modification of the problem?", "labels": [], "entities": []}, {"text": "Consider the following question, which targets Examples (1) and (2): (3) Who changed the rules regarding reporting requirements?", "labels": [], "entities": []}, {"text": "Question is a factoid question, meaning it has a short, unambiguous answer in the targeted text.", "labels": [], "entities": []}, {"text": "This type of question has been studied extensively in the Text Retrieval Conference Question Answering (QA) Track.", "labels": [], "entities": [{"text": "Text Retrieval Conference Question Answering (QA)", "start_pos": 58, "end_pos": 107, "type": "TASK", "confidence": 0.88581682741642}]}, {"text": "Using the evaluation data from this track, showed that SRL can improve the accuracy of a QA system; a traditional SRL system alone, however, is not enough to recover the implied answer to Question (3): SEC or the agency.", "labels": [], "entities": [{"text": "SRL", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.947566568851471}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9990218877792358}, {"text": "SEC", "start_pos": 202, "end_pos": 205, "type": "DATASET", "confidence": 0.7782506346702576}]}, {"text": "Successful implicit argument identification provides the answer in this case.", "labels": [], "entities": [{"text": "implicit argument identification", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.705703854560852}]}, {"text": "This article presents an in-depth study of implicit arguments for nominal predicates.", "labels": [], "entities": []}, {"text": "The following section surveys research related to implicit argument identification.", "labels": [], "entities": [{"text": "implicit argument identification", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.7242395083109537}]}, {"text": "Section 3 describes the study's implicit argument annotation process and the data it produced.", "labels": [], "entities": []}, {"text": "The implicit argument identification model is formulated in Section 4 and evaluated in Section 5.", "labels": [], "entities": [{"text": "implicit argument identification", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.5941575268904368}]}, {"text": "Discussion of results is provided in Section 6, and the article concludes in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Annotation data analysis. Columns are defined as follows: (1) the annotated predicate, (2) the number of predicate instances that were annotated,  (3) the average number of implicit arguments per predicate instance, (4) of all roles for all predicate instances, the percentage filled by NomBank  arguments, (5) the average number of NomBank arguments per predicate instance, (6) the average number of PropBank arguments per instance of the  verb form of the predicate, (7) of all roles for all predicate instances, the percentage filled by either NomBank or implicit arguments, (8) the average  number of combined NomBank/implicit arguments per predicate instance. SD indicates the standard deviation with respect to an average.", "labels": [], "entities": []}, {"text": " Table 2  Primary feature groups used by the model. The third column gives the number of features in the  group, and the final column gives the number of features from the group that were ranked in the  top 20 among all features.", "labels": [], "entities": []}, {"text": " Table 3  Targeted PMI scores between the arg 1 of loss and other argument positions. The second column  gives the number of times that the argument position in the row is found to be coreferent with  the arg 1 of the loss predicate. A higher value in this column results in a lower discount factor.  See Equation (4) for the discount factor.", "labels": [], "entities": [{"text": "PMI", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8095525503158569}]}, {"text": " Table 4  Coreference probabilities between reassess.arg 1 and other argument positions. See Equation (9)  for details on the discount factor.", "labels": [], "entities": []}, {"text": " Table 6  Implicit argument error analysis. The second column indicates the type of error that was made  and the third column gives the percentage of all errors that fall into each type.", "labels": [], "entities": []}]}