{"title": [{"text": "Speculation and Negation: Rules, Rankers, and the Role of Syntax", "labels": [], "entities": [{"text": "Speculation and Negation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8221046924591064}]}], "abstractContent": [{"text": "This article explores a combination of deep and shallow approaches to the problem of resolving the scope of speculation and negation within a sentence, specifically in the domain of biomedical research literature.", "labels": [], "entities": [{"text": "resolving the scope of speculation and negation within a sentence", "start_pos": 85, "end_pos": 150, "type": "TASK", "confidence": 0.8312033712863922}]}, {"text": "The first part of the article focuses on speculation.", "labels": [], "entities": []}, {"text": "After first showing how speculation cues can be accurately identified using a very simple classifier informed only by local lexical context, we goon to explore two different syntactic approaches to resolving the in-sentence scopes of these cues.", "labels": [], "entities": []}, {"text": "Whereas one uses manually crafted rules operating over dependency structures, the other automatically learns a discriminative ranking function over nodes in constituent trees.", "labels": [], "entities": []}, {"text": "We provide an in-depth error analysis and discussion of various linguistic properties characterizing the problem, and show that although both approaches perform well in isolation, even better results can be obtained by combining them, yielding the best published results to date on the CoNLL-2010 Shared Task data.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task data", "start_pos": 286, "end_pos": 313, "type": "DATASET", "confidence": 0.8686158508062363}]}, {"text": "The last part of the article describes how our speculation system is ported to also resolve the scope of negation.", "labels": [], "entities": [{"text": "negation", "start_pos": 105, "end_pos": 113, "type": "TASK", "confidence": 0.9526734352111816}]}, {"text": "With only modest modifications to the initial design, the system obtains state-of-the-art results on this task also.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of providing a principled treatment of speculation and negation is a problem that has received increased interest within the NLP community during recent years.", "labels": [], "entities": [{"text": "treatment of speculation and negation", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.5761518537998199}]}, {"text": "This is witnessed not only by this Special Issue, but also by the themes of several recent shared tasks and dedicated workshops.", "labels": [], "entities": []}, {"text": "The Shared Task at the 2010 Conference on Natural Language Learning (CoNLL) has been of central importance in this respect, where the topic was speculation detection for the domain of biomedical research literature).", "labels": [], "entities": [{"text": "Shared Task at the 2010 Conference on Natural Language Learning (CoNLL)", "start_pos": 4, "end_pos": 75, "type": "TASK", "confidence": 0.8408710452226492}, {"text": "speculation detection", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.8001019358634949}]}, {"text": "This particular area has been the focus of much current research, triggered by the release of the BioScope corpus ()-a collection of scientific abstracts, full papers, and clinical reports with manual annotations of words that signal speculation or negation (so-called cues), as well as of the scopes of these cues within the sentences.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.8290750980377197}]}, {"text": "The following examples from BioScope illustrate how sentences are annotated with respect to speculation.", "labels": [], "entities": []}, {"text": "Cues are here shown using angle brackets, with braces corresponding to their annotated scopes: (1) {The specific role of the chromodomain is \ud97b\udf59unknown\ud97b\udf59} but chromodomain swapping experiments in Drosophila {\ud97b\udf59suggest\ud97b\udf59 that they {\ud97b\udf59might\ud97b\udf59 be protein interaction modules}}.", "labels": [], "entities": []}, {"text": "(2) These data {\ud97b\udf59indicate that\ud97b\udf59 IL-10 and IL-4 inhibit cytokine production by different mechanisms}.", "labels": [], "entities": []}, {"text": "Negation is annotated in the same way, as shown in the following examples: (3) Thus, positive autoregulation is {\ud97b\udf59neither\ud97b\udf59 a consequence \ud97b\udf59nor\ud97b\udf59 the sole cause of growth arrest}.", "labels": [], "entities": []}, {"text": "(4) Samples of the protein pair space were taken {\ud97b\udf59instead of\ud97b\udf59 considering the whole space} as this was more computationally tractable.", "labels": [], "entities": []}, {"text": "In this article we develop several linguistically informed approaches to automatically identify cues and resolve their scope within sentences, as in the example annotations.", "labels": [], "entities": []}, {"text": "Our starting point is the system developed by for the CoNLL-2010 Shared Task challenge.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task challenge", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.7975978553295135}]}, {"text": "This system implements a two-stage hybrid approach for resolving speculation: First, a binary classifier is applied for identifying cues, and then their in-sentence scope is resolved using a small set of manually defined rules operating on dependency structures.", "labels": [], "entities": [{"text": "resolving speculation", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.8869068622589111}]}, {"text": "In the current article we present several important extensions to the initial system design of: First, in Section 5, we present a simplified approach to cue classification, greatly reducing the model size and complexity of our Support Vector Machine (SVM) classifier while at the same time giving better accuracy.", "labels": [], "entities": [{"text": "cue classification", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.8635912835597992}, {"text": "accuracy", "start_pos": 304, "end_pos": 312, "type": "METRIC", "confidence": 0.9969992637634277}]}, {"text": "Then, after reviewing the manually defined dependency-based scope rules (Section 6.1), we show how the scope resolution task can be handled using an alternative approach based on learning a discriminative ranking function over subtrees of HPSGderived constituent trees (Section 6.2).", "labels": [], "entities": [{"text": "scope resolution task", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.8346057732899984}]}, {"text": "Moreover, by combining this empirical ranking approach with the manually defined rules (Section 6.3), we are able to obtain the best published results so far (to the best of our knowledge) on the CoNLL-2010 Shared Task evaluation data.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task evaluation data", "start_pos": 196, "end_pos": 234, "type": "DATASET", "confidence": 0.8774805068969727}]}, {"text": "Finally, in Section 7, we show how our speculation system can be ported to also resolve the scope of negation.", "labels": [], "entities": [{"text": "negation", "start_pos": 101, "end_pos": 109, "type": "TASK", "confidence": 0.9613050222396851}]}, {"text": "Only requiring modest modifications, the system also obtains state-of-the-art results on this task.", "labels": [], "entities": []}, {"text": "Rather than merely presenting the implementation details of the new approaches we develop, we also provide in-depth error analyses and discussion on the linguistic properties of the phenomena of both speculation and negation.", "labels": [], "entities": []}, {"text": "Before turning to the details of our approach, however, we start by presenting the relevant data sets and the resources used for pre-processing in Section 2, followed by a presentation of the various evaluation measures we will use in Section 3.", "labels": [], "entities": []}, {"text": "We also provide a brief review of relevant previous work in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we seek to clarify the type of measures we will be using for evaluating both the cue detection components (Section 3.1) and the scope resolution components (Section 3.2).", "labels": [], "entities": [{"text": "cue detection", "start_pos": 97, "end_pos": 110, "type": "TASK", "confidence": 0.734471470117569}, {"text": "scope resolution", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7548653185367584}]}, {"text": "Essentially, we here follow the evaluation scheme established by the CoNLL-2010 Shared Task on speculation detection, also applying this when evaluating results for the negation task.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.8566546638806661}, {"text": "speculation detection", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.8181135654449463}, {"text": "negation task", "start_pos": 169, "end_pos": 182, "type": "TASK", "confidence": 0.9204812347888947}]}, {"text": "For the approaches presented for cue detection in this article (for both speculation and negation), we will be reporting precision, recall, and F 1 for three different levels of evaluation; the sentence-level, the token-level, and the cue-level.", "labels": [], "entities": [{"text": "cue detection", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.8001720011234283}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9991402626037598}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9991043210029602}, {"text": "F 1", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9876475036144257}]}, {"text": "The sentence-level scores correspond to Task 1 in the CoNLL-2010 Shared Task, that is, correctly identifying whether a sentence contains uncertainty or not.", "labels": [], "entities": []}, {"text": "The scores at the token-level measure the number of individual tokens within the span of a cue annotation that the classifier has correctly labeled as a cue.", "labels": [], "entities": []}, {"text": "Finally, the stricter cue-level scores measure how well a classifier succeeds in identifying entire cues (which will in turn provide the input for the downstream components that later try to resolve the scope of the speculation or negation within the sentence).", "labels": [], "entities": []}, {"text": "A true positive at the cue-level requires that the predicted cue exactly matches the annotation in its entirety (full multiword cues included).", "labels": [], "entities": []}, {"text": "For assessing the statistical significance of any observed differences in performance, we will be using a two-tailed sign-test applied to the token-level predictions.", "labels": [], "entities": []}, {"text": "This is a standard non-parametric test for paired samples, which in our setting considers how often the predictions of two given classifiers differ.", "labels": [], "entities": []}, {"text": "Note that we will only be performing significance testing for the token-level evaluation (unless otherwise stated), as this is the level that most directly corresponds to the classifier decisions.", "labels": [], "entities": []}, {"text": "We will be assuming a significance level of \u03b1 = 0.05, but also reporting actual p-values in cases where differences are not found to be significant.", "labels": [], "entities": [{"text": "significance level", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.963363528251648}]}, {"text": "When evaluating scope resolution we will be following the methodology of the CoNLL-2010 Shared Task, also using the scoring software made available by the task organizers.", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.8410025238990784}]}, {"text": "We have modified the software trivially so that it can also be used to evaluate negation labeling.", "labels": [], "entities": [{"text": "negation labeling", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9480637907981873}]}, {"text": "As pointed out by, this way of evaluating scope is rather strict: A true positive (TP) requires an exact match for both the entire cue and the entire scope.", "labels": [], "entities": []}, {"text": "On the other hand, a false positive (FP) can be incurred by three different events; (1) incorrect cue labeling with correct scope boundaries, (2) correct cue labeling with incorrect scope boundaries, or (3) incorrectly labeled cue and scope.", "labels": [], "entities": [{"text": "false positive (FP)", "start_pos": 21, "end_pos": 40, "type": "METRIC", "confidence": 0.8248126149177551}]}, {"text": "Moreover, conditions (1) and (2) will give a double penalty, in the sense that they also count as false negatives given that the gold-standard cue or scope is missed).", "labels": [], "entities": []}, {"text": "Finally, false negatives are of course also incurred by cases where the gold-standard annotations specify a scope but the system makes no such prediction.", "labels": [], "entities": []}, {"text": "Of course, the evaluation scheme outlined here corresponds to an end-to-end evaluation of the overall system, where the cue detection performance carries over to the scope-level performance.", "labels": [], "entities": [{"text": "cue detection", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.6569121927022934}]}, {"text": "In order to better assess the performance of a scope resolution component in isolation, we will also report scope results against gold-standard cues.", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.8465963900089264}]}, {"text": "Note that, when using gold-standard cues, the number of false negatives and false positives will always be identical, meaning that the scope-level figures for recall, precision, and F 1 will all be identical as well, and we will therefore only be reporting the latter in this setup.", "labels": [], "entities": [{"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9992885589599609}, {"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.999204695224762}, {"text": "F 1", "start_pos": 182, "end_pos": 185, "type": "METRIC", "confidence": 0.9952088594436646}]}, {"text": "(The reason for this is that, when assuming gold-standard cues, only error condition (2) can occur, which will in turn always count both a false positive and a false negative, making the two figures identical.)", "labels": [], "entities": [{"text": "error", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9724875092506409}]}, {"text": "Exactly how to define the paired samples that form the basis of the statistical significance testing is less straightforward for the end-to-end scope-level predictions than for the cue identification.", "labels": [], "entities": [{"text": "cue identification", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.711998924612999}]}, {"text": "It is also worth noting that the CoNLL-2010 Shared Task organizers themselves refrained from including any significance testing when reporting the official results.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task organizers", "start_pos": 33, "end_pos": 66, "type": "DATASET", "confidence": 0.8828493505716324}]}, {"text": "In this article we follow a recall-centered approach: For each cue/scope pair in the gold standard, we simply note whether it is correctly identified or not by a given system.", "labels": [], "entities": [{"text": "recall-centered", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.9885228872299194}]}, {"text": "The sequence of boolean values that results (FP = 0, TP = 1) can be directly paired with the corresponding sequence fora different system so that the sign-test can be applied as above.", "labels": [], "entities": [{"text": "FP", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.995298445224762}, {"text": "TP", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9702494740486145}]}, {"text": "Note that our modified scorer for negation is available from our Web page of supplemental materials, 2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our final model configurations.", "labels": [], "entities": [{"text": "negation", "start_pos": 34, "end_pos": 42, "type": "TASK", "confidence": 0.9839004278182983}, {"text": "BioScope DTD", "start_pos": 157, "end_pos": 169, "type": "DATASET", "confidence": 0.8508085310459137}]}, {"text": "We now turn to evaluating our end-to-end negation system with SVM-based cue classification and scope resolution using the combination of constituent ranking and dependency-based rules.", "labels": [], "entities": [{"text": "SVM-based cue classification", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.7726324399312338}, {"text": "scope resolution", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7280255258083344}]}, {"text": "To put the evaluation in perspective we also compare our results against the results of other state-of-the-art approaches to negation detection.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.987181156873703}]}, {"text": "Comparison to previous work is complicated slightly by the fact that different data splits and evaluation measures have been used across various studies.", "labels": [], "entities": []}, {"text": "A commonly reported measure in the literature on resolving negation scope is the percentage of correct scopes (PCS) as used by, among others.", "labels": [], "entities": [{"text": "resolving negation scope", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8582890232404073}, {"text": "percentage of correct scopes (PCS)", "start_pos": 81, "end_pos": 115, "type": "METRIC", "confidence": 0.8135976961680821}]}, {"text": "define PCS as the number of correct spans divided by the number of true spans.", "labels": [], "entities": [{"text": "PCS", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9290302395820618}]}, {"text": "It therefore corresponds roughly to the scope-level recall as reported in the current article.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.884125292301178}]}, {"text": "The PCS notion of a correct scope, however, is less strict than in our set-up (Section 3.2): Whereas we require an exact match of both the cue and the scope, do not include the cue identification in their evaluation.", "labels": [], "entities": []}, {"text": "Moreover, whereas the work of both and is based on the BioScope corpus, only follow the same set-up assumed in the current article., on the other hand, evaluate by 5-fold cross-validation on the papers alone, reporting a PCS score of 53.7%.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9570408761501312}, {"text": "PCS", "start_pos": 221, "end_pos": 224, "type": "METRIC", "confidence": 0.974275529384613}]}, {"text": "When running our negation cue classifier and constituent ranker (in the hybrid mode using the dependency features) by 5-fold cross-validation on the papers we achieve a scope-level recall of 68.62 (and an F 1 of 64.50).", "labels": [], "entities": [{"text": "recall", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9397146105766296}, {"text": "F 1", "start_pos": 205, "end_pos": 208, "type": "METRIC", "confidence": 0.9945693910121918}]}, {"text": "shows a comparison of our negation scope resolution system with that of.", "labels": [], "entities": [{"text": "negation scope resolution", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.8716064890225729}]}, {"text": "Rather than using the PCS measure reported by, we have re-scored the output of their system according to the CoNLL-2010 shared task scoring scheme, and it should therefore be kept in mind that the system of Morante and Daelemans (2009b) originally was optimized with respect to a slightly different metric.", "labels": [], "entities": [{"text": "CoNLL-2010 shared task scoring scheme", "start_pos": 109, "end_pos": 146, "type": "DATASET", "confidence": 0.7722211360931397}]}, {"text": "For the cross-validated BSA experiments we find the results of the two systems to be fairly similar, although the F 1 achieved by our system is higher by more than 5 percentage points, mostly due to higher recall.", "labels": [], "entities": [{"text": "BSA", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7077178955078125}, {"text": "F 1", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9951970279216766}, {"text": "recall", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9987970590591431}]}, {"text": "For the cross-text experiments, the differences are much more pronounced, with the F 1 of our system being more than 22 points higher on BSP and more than 17 points higher on BSR.", "labels": [], "entities": [{"text": "F 1", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.976738303899765}, {"text": "BSP", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.9674192070960999}, {"text": "BSR", "start_pos": 175, "end_pos": 178, "type": "DATASET", "confidence": 0.986219584941864}]}, {"text": "Again, the largest differences are to be found for recall-even though this is the score that most closely corresponds to the PCS metric used by-but as seen in there are substantial differences in precision as well.", "labels": [], "entities": [{"text": "recall-even", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.99895179271698}, {"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9993085861206055}]}, {"text": "The scope-level differences between the two systems are found to be statistically significant across all the three BioScope sub-corpora.", "labels": [], "entities": [{"text": "BioScope sub-corpora", "start_pos": 115, "end_pos": 135, "type": "DATASET", "confidence": 0.843265950679779}]}], "tableCaptions": [{"text": " Table 1  The top three rows summarize the components of the BioScope corpus-abstracts (BSA), full  papers (BSP), and clinical reports (BSR)-annotated for speculation and negation. The bottom  row details the held-out evaluation data (BSE) provided for the CoNLL-2010 Shared Task.  Columns indicate the total number of sentences and their average length, the number of  hedged/negated sentences, the number of cues, and the number of multiword cues. (Note that  BSE is not annotated for negation, and we do not provide speculation statistics for BSR as this  data set will only be used for the negation experiments.", "labels": [], "entities": [{"text": "BioScope corpus-abstracts", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.7470874488353729}]}, {"text": " Table 3  Development results for detecting speculation CUES: Averaged 10-fold cross-validation results for  the cue classifiers on both the abstracts and full papers in the BioScope training data (BSA and BSP).", "labels": [], "entities": [{"text": "detecting speculation CUES", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.8645125230153402}, {"text": "BioScope training data (BSA and BSP)", "start_pos": 174, "end_pos": 210, "type": "DATASET", "confidence": 0.7750014886260033}]}, {"text": " Table 4  Held-out results for identifying speculation cues: Applying the cue classifiers to the 5,003  sentences in BSE-the biomedical papers provided for the CoNLL-2010 Shared Task evaluation.", "labels": [], "entities": [{"text": "BSE-the biomedical papers", "start_pos": 117, "end_pos": 142, "type": "DATASET", "confidence": 0.8619336088498434}, {"text": "CoNLL-2010 Shared Task evaluation", "start_pos": 160, "end_pos": 193, "type": "DATASET", "confidence": 0.7479258626699448}]}, {"text": " Table 5  Stacked dependency representation of the sentence in Example (13), lemmatized and annotated  with GENIA PoS tags, Malt parses (Head, DepRel), and XLE parses (XHead, XDep), as well as  other morphological and lexical semantic features extracted from the XLE analysis (Features).", "labels": [], "entities": [{"text": "Example", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9117547869682312}, {"text": "GENIA PoS tags", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.5700896382331848}]}, {"text": " Table 8  Ranker optimization on BSP: Showing ranker performance for various feature type  combinations compared with a random-choice baseline, only considering instances  where the gold-standard scope aligns to a constituent within the 1-best parse.", "labels": [], "entities": []}, {"text": " Table 9  Resolving the scope of speculation cues using the dependency rules, the constituent ranker,  and their combination. Whereas table (a) shows results for gold-standard cues, table (b) shows  end-to-end results for the cues predicted by the classifier of Section 5.2. Results are shown both  for the BioScope development data (for which both the scope ranker and the cue classifier is  applied using 10-fold cross-validation) and the CoNLL-2010 Shared Task evaluation data.", "labels": [], "entities": [{"text": "BioScope development data", "start_pos": 307, "end_pos": 332, "type": "DATASET", "confidence": 0.8354372978210449}, {"text": "CoNLL-2010 Shared Task evaluation data", "start_pos": 441, "end_pos": 479, "type": "DATASET", "confidence": 0.8853167057037353}]}, {"text": " Table 10  Final end-to-end results for scope resolution: Held-out testing on BSE, using the cue classifier  described in Section 5.2 while combining the dependency rules and the constituent ranker for  scope resolution. The results are compared to the system with the best end-to-end performance  in the", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9557481110095978}, {"text": "BSE", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9801628589630127}, {"text": "scope resolution", "start_pos": 203, "end_pos": 219, "type": "TASK", "confidence": 0.8808141946792603}]}, {"text": " Table 14  Scope resolution for gold-standard negation cues across the BioScope sub-corpora.", "labels": [], "entities": [{"text": "BioScope sub-corpora", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.920985221862793}]}]}