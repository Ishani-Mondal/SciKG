{"title": [{"text": "Empirical Methods for the Study of Denotation in Nominalizations in Spanish", "labels": [], "entities": [{"text": "Study of Denotation in Nominalizations", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.6133575201034546}]}], "abstractContent": [{"text": "This article deals with deverbal nominalizations in Spanish; concretely, we focus on the deno-tative distinction between event and result nominalizations.", "labels": [], "entities": []}, {"text": "The goals of this work is twofold: first, to detect the most relevant features for this denotative distinction; and, second, to build an automatic classification system of deverbal nominalizations according to their denotation.", "labels": [], "entities": []}, {"text": "We have based our study on theoretical hypotheses dealing with this semantic distinction and we have analyzed them empirically by means of Machine Learning techniques which are the basis of the ADN-Classifier.", "labels": [], "entities": []}, {"text": "This is the first tool that aims to automatically classify deverbal nominalizations in event, result, or underspecified denotation types in Spanish.", "labels": [], "entities": []}, {"text": "The ADN-Classifier has helped us to quantitatively evaluate the validity of our claims regarding deverbal nominalizations.", "labels": [], "entities": []}, {"text": "We setup a series of experiments in order to test the ADN-Classifier with different models and in different realistic scenarios depending on the knowledge resources and natural language processors available.", "labels": [], "entities": []}, {"text": "The ADN-Classifier achieved good results (87.20% accuracy).", "labels": [], "entities": [{"text": "ADN-Classifier", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7450656294822693}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9987008571624756}]}], "introductionContent": [{"text": "The last few years have seen an increasing amount of work in the semantic treatment of unrestricted text, such as Minimal Recursive Semantics in Lingo/LKB, Frame Semantics in Shalmaneser (), Discourse Representation Structures in Boxer, and the automatic learning of Semantic Grammars (Mooney 2007), but we are still along way from representing the full meaning of texts when not restricted to narrow domains.", "labels": [], "entities": []}, {"text": "Many Natural Language Processing (NLP) applications such as Question Answering, Information Extraction, Machine Reading and high-quality Machine Translation or Summarization systems, and many NLP intermediate level tasks such as Textual Entailment, Paraphrase Detection, or Word Sense Disambiguation (WSD), have almost reached their practical upper bounds and it is difficult to move forward without using a serious semantic representation of the text under consideration.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8726129829883575}, {"text": "Information Extraction", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.787544459104538}, {"text": "Machine Reading", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.8189600110054016}, {"text": "Machine Translation or Summarization", "start_pos": 137, "end_pos": 173, "type": "TASK", "confidence": 0.8527771532535553}, {"text": "Paraphrase Detection", "start_pos": 249, "end_pos": 269, "type": "TASK", "confidence": 0.8324744403362274}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 274, "end_pos": 305, "type": "TASK", "confidence": 0.7661399493614832}]}, {"text": "Given the limitations and the difficulties in obtaining an in-depth semantic representation of texts as a whole, most efforts have been focused on partial semantic representation using less expressive semantic formalisms, such as those that come under the umbrella of Description Logic variants, or on discarding the whole semantic interpretation task in order to focus on smaller (and easier) subtasks.", "labels": [], "entities": []}, {"text": "This is the case, for instance, in Semantic Role Labeling (SRL) systems, which indicate the semantic relations that hold between a predicate and its associated participants and properties, the relations of which are drawn from a pre-specified list of possible semantic roles for that predicate or class of predicates.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7828567226727804}]}, {"text": "See and for recent surveys.", "labels": [], "entities": []}, {"text": "Closely related to SRL is the task of learning Selectional Restrictions fora predicate, for example, the kind of semantic class each argument of the predicate must belong to.", "labels": [], "entities": [{"text": "SRL", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9544199705123901}]}, {"text": "In this case a predefined set of semantic classes must also be used to perform the classification task.", "labels": [], "entities": []}, {"text": "WordNet), VerbNet (), PropBank,), and OntoNotes () are resources frequently used for this purpose.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9669721126556396}, {"text": "PropBank", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8989362716674805}]}, {"text": "Most of these efforts are verb-centered and reduce role labeling to the roles played by entities around a predicate instantiated as a verb.", "labels": [], "entities": [{"text": "role labeling", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.7330427318811417}]}, {"text": "At a finer level, there is the task of WSD, for example, assigning the most appropriate sense to each lexical unit of the text from a predefined lexical-semantic resource.", "labels": [], "entities": [{"text": "WSD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.820486307144165}]}, {"text": "Once again a catalogue of classes has to be used as a range for the assignment.", "labels": [], "entities": []}, {"text": "In this case as well, despite its excessive finer granularity, WordNet is the most widely used reference.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9697464108467102}]}, {"text": "See Navigli (2009) fora recent survey.", "labels": [], "entities": []}, {"text": "In this line of research, there has recently been a growing interest in going beyond verb-centered hypotheses to tackle the computational treatment of deverbal nominalizations (nouns derived from verbs), in order to move forward to the full comprehension of texts.", "labels": [], "entities": [{"text": "computational treatment of deverbal nominalizations (nouns derived from verbs)", "start_pos": 124, "end_pos": 202, "type": "TASK", "confidence": 0.7595730396834287}]}, {"text": "Deverbal nominalizations are lexical units that contain rich semantic information equivalent to a clausal structure.", "labels": [], "entities": []}, {"text": "Many recent studies have focused on the detection of semantic relations between pairs of nominals that belong to different Nominal Phrases (NPs), such as Task 4 of SemEval 2007 ( and, or between nominals taking part in noun compound constructions.", "labels": [], "entities": []}, {"text": "In the latter case, they take into account a predefined set of semantic relations () or use verb paraphrases with prepositions).", "labels": [], "entities": []}, {"text": "Although these works include nominalizations, they are not strictly focused on them but coverall type of nouns.", "labels": [], "entities": []}, {"text": "Actually, most of the work studying only deverbal nominalizations is focused on their argument structure: Some authors focus on the detection of arguments within the NP headed by the nominalization; Pad\u00f3Pad\u00b4Pad\u00f3, Pennacchiotti, and Sporleder 2008;, whereas others center their attention on detecting the implicit arguments of the nominalizations which are outside the NP ().", "labels": [], "entities": []}, {"text": "Among the former group, there are different approaches to the problem: and use probabilistic models, and) develop heuristic rules, Pad\u00f3Pad\u00b4Pad\u00f3, Pennacchiotti, and Resolution tasks it would be useful to have the nominalizations classified into denotations in order to detect coreference types.", "labels": [], "entities": []}, {"text": "For instance, if a nominalization has a verbal antecedent (anchor) and its denotation is of the event type, an identity coreference relation could be established between them (Example (1)).", "labels": [], "entities": []}, {"text": "If the nominalization is of the result type, however, the relation established between verb and noun would be a bridging coreference relation (Example).", "labels": [], "entities": []}, {"text": "(1) En Francia los precios cayeron un 0,1% en septiembre.", "labels": [], "entities": []}, {"text": "La ca\u00edda<event> ha provocado que la inflac\u00ed on quedara en el 2,2%.", "labels": [], "entities": []}, {"text": "'In France prices fell by 0.1 % in September.", "labels": [], "entities": []}, {"text": "The fall<event> caused inflation to remain at 2.2 %.'", "labels": [], "entities": [{"text": "inflation", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9990365505218506}]}, {"text": "La imprenta se invent\u00f3invent\u00b4invent\u00f3 en 1440.", "labels": [], "entities": []}, {"text": "El invento<result> permit\u00ed o difundir las ideas y conocimientos con eficacia.", "labels": [], "entities": []}, {"text": "'The printing press was invented in 1440.", "labels": [], "entities": []}, {"text": "This invention<result> allowed for ideas and knowledge to be spread efficiently.'", "labels": [], "entities": []}, {"text": "As for Paraphrase Detection, event nouns (but not result nouns) are paraphrases for full sentences, so this type of information can also be useful for this task.", "labels": [], "entities": [{"text": "Paraphrase Detection", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.9830473363399506}]}, {"text": "For instance, the sentence in Example (3) and the NP headed by an event nominalization in Example (4) are typically considered to be paraphrases.", "labels": [], "entities": []}, {"text": "If the nominalization, however, has a result interpretation as in Example (5)-traducciones, 'translations' refers to the concrete object, that is, the book translatedit is impossible to have a paraphrase with a clausal structure.", "labels": [], "entities": []}, {"text": "This is due to the fact that result nominalizations can denote an object whereas verbs cannot denote objects.", "labels": [], "entities": []}, {"text": "In fact, result nominalizations can only be paraphrases of other NPs denoting objects (Example (6)).", "labels": [], "entities": []}, {"text": "The AnCora-ES corpus enriched with denotative information could be used as training and test data for WSD systems.", "labels": [], "entities": [{"text": "AnCora-ES corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8342630863189697}, {"text": "WSD", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9799086451530457}]}, {"text": "The work presented in this article also provides an additional insight into the linguistic question underlying it: the characterization of deverbal nominalizations according to their denotation and the identification of the most useful criteria for distinguishing between these denotation types.", "labels": [], "entities": []}, {"text": "The remainder of this article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes the theoretical approaches to the semantic denotation with which we deal here.", "labels": [], "entities": []}, {"text": "Section 3 describes the methodology used in this work.", "labels": [], "entities": []}, {"text": "Section 4 presents the empirical linguistic study in which the initial model is established; in Section 5 the different knowledge resources used are presented, paying special attention to the nominal lexicon, AnCoraNom.", "labels": [], "entities": []}, {"text": "In Section 6, the ADN-Classifier is presented and in Section 7 the different experiments conducted are described and their results are reported.", "labels": [], "entities": []}, {"text": "Section 8 reviews related work and, finally, our conclusions are drawn in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present and evaluate the experiments carried outwith the ADNClassifier.", "labels": [], "entities": [{"text": "ADNClassifier", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.91110759973526}]}, {"text": "First we present the settings of these experiments, then we focus on the experiments themselves, and finally we evaluate the results.", "labels": [], "entities": []}, {"text": "The experiments carried outwith the ADN-Classifier-R3 are presented here.", "labels": [], "entities": [{"text": "ADN-Classifier-R3", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.9308678507804871}]}, {"text": "Firstly, we describe those experiments related to the different models of the classifier and secondly, we focus on how some of these models are applied in different scenarios.", "labels": [], "entities": []}, {"text": "We apply the ADN-Classifier in different modes that correspond to the following five dimensions.", "labels": [], "entities": []}, {"text": "28 J48.Part learns first a decision tree and then builds the rules traversing all the branches of the tree.", "labels": [], "entities": [{"text": "J48.Part", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8145800232887268}]}, {"text": "Ripper, instead, learns the rules one by one (increasing the learning cost).", "labels": [], "entities": []}, {"text": "This can result in a more accurate and smaller rule set just in the case of splitting numerical attributes; that is not our case.", "labels": [], "entities": []}, {"text": "29 In n-fold cross-validation, the original sample is randomly partitioned into n subsamples.", "labels": [], "entities": []}, {"text": "Of then subsamples, a single subsample is retained as the validation data for testing the model, and the remaining n \u2212 1 subsamples are used as training data.", "labels": [], "entities": []}, {"text": "The cross-validation process is then repeated n times (the folds), with each of then subsamples used exactly once as the validation data.", "labels": [], "entities": []}, {"text": "The n results from the folds can then be averaged (or otherwise combined) to produce a single estimation.", "labels": [], "entities": []}, {"text": "The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once.", "labels": [], "entities": [{"text": "validation", "start_pos": 119, "end_pos": 129, "type": "TASK", "confidence": 0.9635952115058899}]}, {"text": "n is commonly set to 10 (McLachlan, Do, and Ambroise 2004).", "labels": [], "entities": []}, {"text": "The classifier performance of the different models was evaluated by a tenfold crossvalidation method.", "labels": [], "entities": []}, {"text": "Next, we focus on the results of the 32 models resulting from the five dimensions described in Section 7.2.", "labels": [], "entities": []}, {"text": "used (column 1), the number of instances used for learning (column 2), the number of attributes used, and the number of rules built by the classifier (columns 3 and 4), and finally, the baseline, the accuracy, the decrease error over the baseline (\u2206-error), and the relative error-reduction ratio (Red-\u2206-error) obtained by each model (columns 5, 6, 7, and 8).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9993348717689514}, {"text": "error-reduction ratio (Red-\u2206-error)", "start_pos": 275, "end_pos": 310, "type": "METRIC", "confidence": 0.7334014347621373}]}, {"text": "The rows correspond to the different models presented.", "labels": [], "entities": []}, {"text": "Recall that the names of the models are assigned according to the five dimensions presented in Section 7.2.", "labels": [], "entities": []}, {"text": "It should be borne in mind here that in column 2, the number of instances for learning depends on the type of unit used for learning and classification (senses in sense-based models, lemmas in lemma-based models, and examples) and on the vocabulary and corpus size.", "labels": [], "entities": []}, {"text": "The interaction between these three dimensions also explains why the figures for the baseline change for each model.", "labels": [], "entities": []}, {"text": "The baseline is a majority baseline which assigns all the instances to the result class.", "labels": [], "entities": []}, {"text": "In general, when the unit used is from the lexicon, the lemma baseline increases relative to the sense baseline.", "labels": [], "entities": []}, {"text": "This is because in lemma-based models we group the senses that share all the features under a lemma; because different senses do not normally share all the features, in the end, only monosemic lemmas are in fact taken into account.", "labels": [], "entities": []}, {"text": "This fact, therefore, shows that there are more result type monosemic lemmas than event and underspecified monosemic lemmas.", "labels": [], "entities": []}, {"text": "Furthermore, it is worth noting that when the unit of learning and classification used are the examples from the AnCora-ES corpus and not the senses from the lexicon, the baseline also increases.", "labels": [], "entities": [{"text": "AnCora-ES corpus", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.933693915605545}, {"text": "baseline", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9635559320449829}]}, {"text": "Therefore, it seems that proportionally result nominalizations are more highly represented in the corpus than event and underspecified nominalizations.", "labels": [], "entities": []}, {"text": "Regarding the number of features used for learning, the type of feature involved and the vocabulary size (when features from the lexicon are used) are the two relevant dimensions.", "labels": [], "entities": []}, {"text": "Finally, it should be said that the accuracy and the other two correlated measures are obtained by evaluating the performance of the different models by tenfold cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9997314810752869}]}, {"text": "As can be seen in, the sense-based models (the first 16 rows) outperform the corresponding lemma-based models (the last 16 rows).", "labels": [], "entities": []}, {"text": "This is explained by the fact that there are features in the lexicon coded at the sense level that cannot be recovered at the lemma level because in lemma-based models we only use as features for the classification those attributes whose values are shared by all senses of the same lemma, and this does not commonly happen.", "labels": [], "entities": []}, {"text": "At the sense level, the best results are achieved when the features used in the classification come exclusively from the lexicon, with the unit of classification being senses from the lexicon (the first block of four rows) or examples from the corpus (the second block of four rows).", "labels": [], "entities": []}, {"text": "The contextual features (those coming from the corpus) can only be applied to models using examples from the corpus as the unit of classification.", "labels": [], "entities": []}, {"text": "These features harm accuracy: When they are used alone (the third block of rows) they yield accuracy values that are below the baseline and when they are used in combination with features obtained from the lexicon (the fourth block of rows) the accuracy decreases in relation to the models that use only the lexicon as the source of the features (the second block of four rows).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9985522627830505}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9987467527389526}, {"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9990371465682983}]}, {"text": "This shows that there is crucial information in the lexicon that is not possible to recover from the corpus.", "labels": [], "entities": []}, {"text": "Furthermore, it should be mentioned that there is a generalized improvement across sense-based models correlated to the vocabulary and especially corpus size: The bigger the set of vocabulary and corpus, the better the result.", "labels": [], "entities": []}, {"text": "This fact is also present in lemma-based models.", "labels": [], "entities": []}, {"text": "The sense-based models represent the upper bounds for our task.", "labels": [], "entities": []}, {"text": "Ina realistic scenario, however, given the state-of-the-art results in WSD, we would not have access to sense labels, so we are much more interested in the performance of lemma-based models.", "labels": [], "entities": []}, {"text": "The best results are achieved when features from the lexicon and from the corpus are combined (the last block of rows), showing that the sum of both types of features gives rise to positive results, which are not achieved by lexical features or contextual features on their own.", "labels": [], "entities": []}, {"text": "When the features used in the classification come exclusively from the lexicon, with the unit of classification being lemmas from the lexicon (the fifth block of four rows) or the examples from the corpus (the sixth block of four rows), the results are negative (below the baseline) except when the vocabulary and corpus size are both the full sets (1.54% and 0.16% improvement, respectively).", "labels": [], "entities": []}, {"text": "In these cases, the information from the lexicon is not as accurate as in sense-based models.", "labels": [], "entities": []}, {"text": "The contextual features alone do not achieve positive results, not even with the full vocabulary and the full corpus.", "labels": [], "entities": []}, {"text": "Therefore, the combination of features is needed in a realistic scenario in order to achieve good performance of the classifier.", "labels": [], "entities": []}, {"text": "In these cases, only when the reduced vocabulary and the reduced corpus are used are the results slightly negative.", "labels": [], "entities": []}, {"text": "From now on, we will focus on the last model (LEAFF) because even if it has a lower accuracy than the corresponding sense-model, we expect it to exhibit a more robust behavior when tackling unseen data.", "labels": [], "entities": [{"text": "LEAFF", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9920702576637268}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9962571859359741}]}, {"text": "An important point for the classifier to learn a model is whether or not the sample size is large enough for accurate learning.", "labels": [], "entities": []}, {"text": "We performed a learning curve analysis of the LEAFF model for different sample sizes (from 1,000 examples to the whole set of 23,431 examples).", "labels": [], "entities": [{"text": "LEAFF", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.958696186542511}]}, {"text": "The results are depicted in.", "labels": [], "entities": []}, {"text": "We have also plotted the confidence intervals at 95%.", "labels": [], "entities": [{"text": "confidence intervals", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.9712014198303223}]}, {"text": "The results seem to imply that for sizes over 5,000 examples the accuracy tends to stabilize; we are, therefore, highly confident of our results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9997153878211975}]}, {"text": "As expected, the confidence intervals consistently diminish as the corpus grows.", "labels": [], "entities": []}, {"text": "The results of the experiments applying the scenarios described in Section 6 (see) are presented in.", "labels": [], "entities": []}, {"text": "The table shows the results of the ten scenarios set out in rows, and in columns we provide the scenario identification (column 1); the model applied out of the 32 generated, following the notation in Section 7.2 (column 2); the number of features in the original model (column 3); the number of features in the model adapted for that scenario after removing noninformed features, that is, the features used in the original model that do not fit in the description of a concrete scenario (column 4); and the accuracies of the original and final model (columns 5 and 6, respectively).", "labels": [], "entities": []}, {"text": "In each scenario we applied the best model of the 32 we generated taking into account the features that each model uses and that fit the best in each scenario according to the hypothesized available linguistic processors.", "labels": [], "entities": []}, {"text": "When there is no concrete model to project how the ADN-Classifier would perform in a concrete scenario, we selected the model that fits approximately in that scenario and removed the noninformed features.", "labels": [], "entities": []}, {"text": "For instance, Scenario 10 describes the case where the nominal lexicon is not available or the nominalization candidate is a noun that does not occur in the nominal lexicon, and the features used are extracted from the parsed tree at lemma level and from the SRL process in order to obtain argument structure information.", "labels": [], "entities": []}, {"text": "Because we do not have a model that perfectly fits in that scenario, we select the LEAFF (lemma based model using examples from the corpus as the unit of classification and obtaining the features from both lexicon and corpus, with full vocabulary and full corpus sets), and we removed all the features from the lexicon except the ones related to the argument structure, simulating an SRL process.", "labels": [], "entities": [{"text": "LEAFF", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9943740367889404}]}, {"text": "These results show that the difference between lemma-based and sense-based models shown in is also present here.", "labels": [], "entities": []}, {"text": "There is a decrease inaccuracy in all the cases in which features are removed, although this decrease is not statistically significant.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9783685207366943}]}, {"text": "This could be due to the large number of features available for rule learning and the possibility of using alternate features when some of the original ones are removed.", "labels": [], "entities": [{"text": "rule learning", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9506948590278625}]}], "tableCaptions": [{"text": " Table 1  Descriptive content of AnCora-ES and its 100Kw subset. In each cell the values corresponding to  the subset and the whole corpus are present.", "labels": [], "entities": [{"text": "AnCora-ES", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8696392178535461}]}, {"text": " Table 3  Distribution of the denotation types according to the criteria evaluated.", "labels": [], "entities": []}, {"text": " Table 6  Experiment and evaluation of scenarios.", "labels": [], "entities": []}, {"text": " Table 7  Confusion matrix for the LEAFF model.", "labels": [], "entities": [{"text": "LEAFF", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.924134373664856}]}]}