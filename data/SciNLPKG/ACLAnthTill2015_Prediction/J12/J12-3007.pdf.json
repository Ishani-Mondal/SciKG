{"title": [{"text": "A Scalable Distributed Syntactic, Semantic, and Lexical Language Model", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content.", "labels": [], "entities": []}, {"text": "The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 150, "end_pos": 165, "type": "TASK", "confidence": 0.7402747571468353}]}, {"text": "The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and \"readability\" of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 171, "end_pos": 181, "type": "METRIC", "confidence": 0.9634487926959991}, {"text": "parsing-based machine translation", "start_pos": 295, "end_pos": 328, "type": "TASK", "confidence": 0.7745242714881897}]}], "introductionContent": [{"text": "The Markov chain (n-gram) source models, which predict each word on the basis of the previous n \u2212 1 words, have been the workhorses of state-of-the-art speech recognizers and machine translators that help to resolve acoustic or foreign language ambiguities by placing higher probability on more likely original underlying word strings.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.7338595688343048}, {"text": "resolve acoustic or foreign language ambiguities", "start_pos": 208, "end_pos": 256, "type": "TASK", "confidence": 0.8012951016426086}]}, {"text": "Although the Markov chains are efficient at encoding local word interactions, the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages.", "labels": [], "entities": []}, {"text": "Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality ().", "labels": [], "entities": []}, {"text": "The performance of conventional n-gram technology has essentially reached a plateau, and it has proven remarkably difficult to improve on n-grams.", "labels": [], "entities": []}, {"text": "Research groups have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams.", "labels": [], "entities": []}, {"text": "As the machine translation (MT) working groups stated in their final report, page 3), \"These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8710001826286315}, {"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.9857158064842224}]}, {"text": "There is a dire need for developing novel approaches to language modeling.\"", "labels": [], "entities": [{"text": "language modeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7471730709075928}]}, {"text": "Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models) that exploit document-level semantic content.", "labels": [], "entities": []}, {"text": "Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena; thus, each captures and exploits different aspects of natural language regularity.", "labels": [], "entities": []}, {"text": "A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (.", "labels": [], "entities": []}, {"text": "Several techniques for combining language models have been investigated.", "labels": [], "entities": []}, {"text": "The most commonly used method is linear interpolation, where each individual model is trained separately and then combined by a weighted linear combination.", "labels": [], "entities": []}, {"text": "All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone).", "labels": [], "entities": []}, {"text": "The weights in this case are trained using held-out data.", "labels": [], "entities": []}, {"text": "Even though this technique is simple and easy to implement, it does not generally yield very effective combinations) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A).", "labels": [], "entities": []}, {"text": "The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and, and.", "labels": [], "entities": []}, {"text": "In fact, fora complete data case, maximum entropy is nothing but maximum likelihood estimation for undirected Markov random fields (MRFs) (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and.", "labels": [], "entities": []}, {"text": "As stated in, however, there are two weaknesses with maximum entropy approach.", "labels": [], "entities": []}, {"text": "The first weakness is that this approach can only model distributions over explicitly observed features, but we know there is hidden A Scalable Distributed Syntactic, Semantic, and Lexical Language Model information in natural language, such as syntactic structure and semantic topic.", "labels": [], "entities": []}, {"text": "The second weakness is that if the statistical model is too complex it becomes intractable to estimate model parameters; computationally very expensive Markov chain Monte Carlo sampling methods would have to be used.", "labels": [], "entities": []}, {"text": "One way to overcome the first hurdle is to use a preprocessing tool to extract hidden features (e.g., used mutual information clustering method to find word pair triggers) then combine these triggers with trigrams through a maximum conditional entropy approach to allow the discourse topic to influence word prediction; used Chelba and Jelinek's structured language model and a word clustering model to extract relevant grammatical and semantic features, then to again combine these features with trigrams through a maximum conditional entropy approach to form a syntactic, semantic, and lexical language model.", "labels": [], "entities": [{"text": "mutual information clustering", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.6496645410855612}, {"text": "word prediction", "start_pos": 303, "end_pos": 318, "type": "TASK", "confidence": 0.7485490441322327}]}, {"text": "Wang and colleagues (; Wang, Schuurmans, and Zhao 2012) have proposed the latent maximum entropy (LME) principle, which extends standard maximum entropy estimation by incorporating hidden dependency structure, but still the LME wouldn't overcome the second hurdle.", "labels": [], "entities": [{"text": "latent maximum entropy (LME)", "start_pos": 74, "end_pos": 102, "type": "METRIC", "confidence": 0.8635762433211008}]}, {"text": "The third method is directed Markov random field () that overcomes both weaknesses in the maximum entropy approach.", "labels": [], "entities": []}, {"text": "used this approach to combine trigram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside-outside algorithm is derived that alters the wellknown inside-outside algorithm for PCFG with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity.", "labels": [], "entities": []}, {"text": "When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 26, "end_pos": 52, "type": "DATASET", "confidence": 0.9727241545915604}]}, {"text": "Because the probabilistic dependency structure in a structured language model (SLM) is more complex and powerful than that in a PCFG,) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework () and derived another generalized inside-outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) algorithm by following Jelinek's ingenious definition of the inside and outside probabilities for SLM).", "labels": [], "entities": []}, {"text": "Again, the generalized inside-outside algorithm alters Jelinek's inside-outside algorithm with modular modification and has the same sixth order of sentence-length time complexity.", "labels": [], "entities": []}, {"text": "Unfortunately, there are no experimental results reported.", "labels": [], "entities": []}, {"text": "In this article, we study the same composite n-gram, SLM, and PLSA model under the directed MRF framework as in.", "labels": [], "entities": []}, {"text": "The composite n-gram/ SLM/PLSA language model under the directed MRF paradigm is first introduced in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, instead of using the sixth order generalized inside-outside algorithm proposed in, we show how to train this composite model via an N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 255, "end_pos": 270, "type": "TASK", "confidence": 0.7906174957752228}]}, {"text": "We prove the convergence of the N-best list approximate EM algorithm.", "labels": [], "entities": [{"text": "convergence", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9658584594726562}]}, {"text": "To resolve the data sparseness problem, we generalize Jelinek and Mercer's recursive mixing scheme for Markov source to a mixture of Markov chains.", "labels": [], "entities": []}, {"text": "To handle large-scale corpora up to a billion tokens, we demonstrate how to implement these algorithms under a distributed computing environment and how to store this language model on a supercomputer.", "labels": [], "entities": []}, {"text": "In Section 4, we describe how to use the model for testing.", "labels": [], "entities": []}, {"text": "Related works are then summarized and compared in Section 5.", "labels": [], "entities": []}, {"text": "Because language modeling is a data-rich and featurerich density estimation problem, there is always a trade-off between approximate error and estimation error, thus in Section 6 we conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens, and compare perplexity results with n-grams (n = 3, 4, 5 respectively) on these three corpora under various situations; drastic perplexity reductions are obtained.", "labels": [], "entities": []}, {"text": "We explain why the composite language models lead to better predictive capacity than linear interpolation.", "labels": [], "entities": []}, {"text": "The proposed composite language models are applied to the task of re-ranking the N-best list from Hiero, a state-of-the-art parsing-based machine translation system; we achieve significantly better translation quality measured by the Bleu score and \"readability\" of translations.", "labels": [], "entities": [{"text": "parsing-based machine translation", "start_pos": 124, "end_pos": 157, "type": "TASK", "confidence": 0.722123364607493}, {"text": "Bleu score", "start_pos": 234, "end_pos": 244, "type": "METRIC", "confidence": 0.9576290249824524}]}, {"text": "Finally, we draw our conclusions and propose future work in Section 7.", "labels": [], "entities": []}, {"text": "The main theme of our approach is \"to exploit information, be it syntactic structure or semantic fabric, which involves a fairly high degree of cognition.", "labels": [], "entities": []}, {"text": "This is precisely the kind of knowledge that humans naturally and inherently use to process natural language, so it can be reasonably conjectured to represent a key ingredient for success\".", "labels": [], "entities": []}, {"text": "In that light, the directed MRF framework, \"whose ultimate goal is to integrate all available knowledge sources, appears most likely to harbor a potential breakthrough.", "labels": [], "entities": [{"text": "MRF", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8985464572906494}]}, {"text": "It is hoped that the on-going effort conducted in this work to leverage such latent synergies will lead, in the not-too-distant future, to more polyvalent, multi-faceted, effective and tractable solutions for language modeling -this is only beginning to scratch the surface in developing systems capable of deep understanding of natural language\" (Bellegarda 2003, p. 105).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 209, "end_pos": 226, "type": "TASK", "confidence": 0.7414126694202423}]}], "datasetContent": [{"text": "In this section, we first explain the experimental set-up for our experiments, we then show comprehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.", "labels": [], "entities": [{"text": "parsing-based machine translation", "start_pos": 293, "end_pos": 326, "type": "TASK", "confidence": 0.8152175545692444}]}, {"text": "In previous work), all complex language models have been trained on relatively small data sets.", "labels": [], "entities": []}, {"text": "There is the impression that complex language models only lead to better results than n-grams on small training corpora.", "labels": [], "entities": []}, {"text": "For example, Jurafsky and Martin (2008, page 482), state, \"We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 214, "end_pos": 231, "type": "TASK", "confidence": 0.7418216168880463}, {"text": "word prediction", "start_pos": 232, "end_pos": 247, "type": "TASK", "confidence": 0.755986750125885}]}, {"text": "It turns out that if we have a very large amount of training data, a 4-gram or 5-gram is nonetheless still the best way to do language modeling.\"", "labels": [], "entities": [{"text": "language modeling", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.8464928865432739}]}, {"text": "To verify whether this is true, we have trained our language models using three different training sets: one has 44 million tokens, another has 230 million tokens, and the third has 1.3 billion tokens.", "labels": [], "entities": []}, {"text": "An independent test set with 354k tokens is chosen.", "labels": [], "entities": []}, {"text": "The independent check data set used to determine the linear interpolation coefficients has 1.7 million tokens for the 44 million token training corpus, and 13.7 million tokens for both the 230 million and 1.3 billion token training corpora.", "labels": [], "entities": []}, {"text": "All these data sets are taken from the LDC English Gigaword corpus with nonverbalized punctuation and we remove all punctuation.", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 39, "end_pos": 66, "type": "DATASET", "confidence": 0.9379451721906662}]}, {"text": "provides the detailed information on how these data sets were chosen from the LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 78, "end_pos": 105, "type": "DATASET", "confidence": 0.9345070719718933}]}, {"text": "r POS tag (also TAGGER operation) vocabulary: 69, closed; r non-terminal tag vocabulary: 54, closed; r CONSTRUCTOR operation vocabulary: 157, closed.", "labels": [], "entities": []}, {"text": "The out-of-vocabulary (OOV) rate on the 44 million, 230 million, 1.3 billion token training corpora is 0.6%, 0.9%, and 1.2%, respectively.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) rate", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.9382163286209106}]}, {"text": "The OOV rate on the 1.7 million and 13.7 million token check corpora is 0.6% and 1.3%, respectively.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9830717444419861}]}, {"text": "The OOV rate on the 354k token test corpus is 2.0%.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9874100983142853}, {"text": "354k token test corpus", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.7152628377079964}]}, {"text": "lists the statistics about the number of types of n-grams on these three corpora.", "labels": [], "entities": []}, {"text": "Similar to SLM, after the parse undergoes headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences.", "labels": [], "entities": [{"text": "SLM", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9552351832389832}, {"text": "TAGGER", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9420768022537231}]}, {"text": "We use the openNLP software 2 to parse a large number of sentences in the LDC English Gigaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Penn Treebank used in and.", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 74, "end_pos": 101, "type": "DATASET", "confidence": 0.8980648666620255}, {"text": "Penn Treebank", "start_pos": 231, "end_pos": 244, "type": "DATASET", "confidence": 0.9943711161613464}]}, {"text": "For the 44 and 230 million token corpora, all sentences are automatically parsed and used to initialize model parameters, whereas for the 1.3 billion token corpus, we parse the sentences from a portion of the corpus that contains 230 million tokens, then use them to initialize model parameters.", "labels": [], "entities": []}, {"text": "The parser at openNLP is trained on the Penn Treebank, which has only one million tokens, and there is a mismatch between the Penn Treebank and the LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "openNLP", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.907582700252533}, {"text": "Penn Treebank", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9962157905101776}, {"text": "Penn Treebank", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9941744208335876}, {"text": "LDC English Gigaword corpus", "start_pos": 148, "end_pos": 175, "type": "DATASET", "confidence": 0.8791732937097549}]}, {"text": "Nevertheless, experimental results show that this approach is effective to provide initial values of model parameters.", "labels": [], "entities": []}, {"text": "gives the perplexity results ( of n-grams (n = 3, 4, and 5) using linear interpolation and Kneser-Ney (1995) smoothing when the training corpus has 44 million, 230 million, and 1.3 billion tokens, respectively.", "labels": [], "entities": []}, {"text": "We have implemented a distributed n-gram with linear interpolation smoothing, but we don't have distributed n-grams with Kneser-Ney smoothing implemented by us.", "labels": [], "entities": []}, {"text": "Instead, we use the SRI Language Modeling Toolkit to obtain perplexity results of n-grams with Kneser-Ney smoothing for the 44 million and 230 million token corpora using a single machine that has 20G memory at the Ohio Supercomputer center.", "labels": [], "entities": [{"text": "SRI Language Modeling", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.6007391214370728}, {"text": "Ohio Supercomputer center", "start_pos": 215, "end_pos": 240, "type": "DATASET", "confidence": 0.9750315149625143}]}, {"text": "We are notable to compute perplexity results of n-grams with Kneser-Ney smoothing on the 1.3 billion token corpus, thus we leave these results blank in.", "labels": [], "entities": []}, {"text": "From the results in, we decided to use a linearly smoothed trigram as the baseline model for the 44 million token corpus, a linearly smoothed 4-gram as the baseline model for the 230 million token corpus, and a linearly smoothed 5-gram as the baseline model for the 1.3 billion token corpus.", "labels": [], "entities": [{"text": "44 million token corpus", "start_pos": 97, "end_pos": 120, "type": "DATASET", "confidence": 0.5894030332565308}, {"text": "1.3 billion token corpus", "start_pos": 266, "end_pos": 290, "type": "DATASET", "confidence": 0.6349377408623695}]}], "tableCaptions": [{"text": " Table 1  The corpora used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2  Statistics about the number of types of n-grams (n = 3, 4, 5) on the 44 million, 230 million, and  1.3 billion token corpora.", "labels": [], "entities": []}, {"text": " Table 3  Perplexity results of n-grams (n = 3, 4, and 5) using linear interpolation and Kneser-Ney  smoothing when training set is a 44 million, 230 million, or 1.3 billion token corpus, respectively.", "labels": [], "entities": []}, {"text": " Table 4  Perplexity (ppl) results and time consumed of the composite n-gram/PLSA language model  trained on three corpora when different numbers of most-likely topics are kept for each  document in PLSA.", "labels": [], "entities": []}, {"text": " Table 5  Perplexity results for various language models on test corpora, where + denotes linear combination, / denotes composite model; n denotes the order  of the n-gram, and m denotes the order of the SLM; the topic nodes are pruned from 200 to 5.", "labels": [], "entities": []}, {"text": " Table 7  Counts of the types in the predictor of the 5-gram/PLSA, 5-gram/2-SLM (or 2-gram/4-SLM),  and 4-SLM/PLSA models when trained on the 1.3B corpus. Fractional expected counts that are  less than a threshold are pruned; this significantly reduces the number of predictor's types  by 85%.", "labels": [], "entities": []}, {"text": " Table 8  Perplexity results for the composite n-gram/PLSA and n-gram/m-SLM/PLSA language models on the test corpus, where + denotes linear  combination, / denotes composite model; n is the order of the n-gram and m is the order of the SLM, and superscripts 1, 2, 3 denote using one-step  on-line EM, on-line EM with fixed learning rate, and batch EM during testing, respectively.", "labels": [], "entities": []}, {"text": " Table 11  Bleu score results for the task of re-ranking the 1,000-best list generated on 191 sentences of 20  documents from the MT04 Chinese-English evaluation set.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9699110984802246}, {"text": "MT04 Chinese-English evaluation set", "start_pos": 130, "end_pos": 165, "type": "DATASET", "confidence": 0.9235542863607407}]}, {"text": " Table 12  Results of \"readability\" evaluation on 919 translated sentences of 100 documents. P = perfect;  S = only semantically correct; G = only grammatically correct; W = wrong.", "labels": [], "entities": []}]}