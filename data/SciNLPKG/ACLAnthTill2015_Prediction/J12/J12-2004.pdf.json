{"title": [{"text": "Cross-Genre and Cross-Domain Detection of Semantic Uncertainty", "labels": [], "entities": [{"text": "Cross-Domain Detection", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6682336181402206}]}], "abstractContent": [{"text": "Uncertainty is an important linguistic phenomenon that is relevant in various Natural Language Processing applications, in diverse genres from medical to community generated, newswire or scientific discourse, and domains from science to humanities.", "labels": [], "entities": []}, {"text": "The semantic uncertainty of a proposition can be identified inmost cases by using a finite dictionary (i.e., lexical cues) and the key steps of uncertainty detection in an application include the steps of locating the (genre-and domain-specific) lexical cues, disambiguating them, and linking them with the units of interest for the particular application (e.g., identified events in information extraction).", "labels": [], "entities": [{"text": "uncertainty detection", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.7909432947635651}, {"text": "information extraction", "start_pos": 384, "end_pos": 406, "type": "TASK", "confidence": 0.7431880235671997}]}, {"text": "In this study, we focus on the genre and domain differences of the context-dependent semantic uncertainty cue recognition task.", "labels": [], "entities": [{"text": "context-dependent semantic uncertainty cue recognition task", "start_pos": 67, "end_pos": 126, "type": "TASK", "confidence": 0.6727679967880249}]}, {"text": "We introduce a unified subcategorization of semantic uncertainty as different domain applications can apply different uncertainty categories.", "labels": [], "entities": []}, {"text": "Based on this categorization, we normalized the annotation of three corpora and present results with a state-of-the-art uncertainty cue recognition model for four fine-grained categories of semantic uncertainty.", "labels": [], "entities": [{"text": "uncertainty cue recognition", "start_pos": 120, "end_pos": 147, "type": "TASK", "confidence": 0.7177016735076904}]}, {"text": "Computational Linguistics Volume 38, Number 2 Our results reveal the domain and genre dependence of the problem; nevertheless, we also show that even a distant source domain data set can contribute to the recognition and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to cover anew domain.", "labels": [], "entities": [{"text": "Computational Linguistics Volume 38", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.6694806441664696}]}, {"text": "Thus, the unified subcategorization and domain adaptation for training the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty recognition.", "labels": [], "entities": [{"text": "cross-genre semantic uncertainty recognition", "start_pos": 131, "end_pos": 175, "type": "TASK", "confidence": 0.6504846438765526}]}], "introductionContent": [{"text": "In computational linguistics, especially in information extraction and retrieval, it is of the utmost importance to distinguish between uncertain statements and factual information.", "labels": [], "entities": [{"text": "information extraction and retrieval", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.720964603126049}]}, {"text": "In most cases, what the user needs is factual information, hence uncertain propositions should be treated in a special way: Depending on the exact task, the system should either ignore such texts or separate them from factual information.", "labels": [], "entities": []}, {"text": "In machine translation, it is also necessary to identify linguistic cues of uncertainty because the source and the target language may differ in their toolkit to express uncertainty (one language uses an auxiliary, the other uses just a morpheme).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.746471643447876}]}, {"text": "To cite another example, in clinical document classification, medical reports can be grouped according to whether the patient definitely suffers, probably suffers, or does not suffer from an illness.", "labels": [], "entities": [{"text": "clinical document classification", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.6960984071095785}]}, {"text": "There are several linguistic phenomena that are referred to as uncertainty in the literature.", "labels": [], "entities": []}, {"text": "We consider propositions to which no truth value can be attributed, given the speaker's mental state, as instances of semantic uncertainty.", "labels": [], "entities": []}, {"text": "In contrast, uncertainty may also arise at the discourse level, when the speaker intentionally omits some information from the statement, making it vague, ambiguous, or misleading.", "labels": [], "entities": []}, {"text": "Determining whether a given proposition is uncertain or not may involve using a finite dictionary of linguistic devices (i.e., cues).", "labels": [], "entities": []}, {"text": "Lexical cues (such as modal verbs or adverbs) are responsible for semantic uncertainty whereas discourse-level uncertainty maybe expressed by lexical cues and syntactic cues (such as passive constructions) as well.", "labels": [], "entities": []}, {"text": "We focus on four types of semantic uncertainty in this study and henceforth the term cue will betaken to mean lexical cue.", "labels": [], "entities": []}, {"text": "The key steps of recognizing semantically uncertain propositions in a natural language processing (NLP) application include the steps of locating lexical cues for uncertainty, disambiguating them (as not all occurrences of the cues indicate uncertainty), and finally linking them with the textual representation of the propositions in question.", "labels": [], "entities": []}, {"text": "The linking of a cue to the textual representation of the proposition can be performed on the basis of syntactic rules that depend on the word class of the lexical cue, but they are independent of the actual application domain or text type where the cue is observed.", "labels": [], "entities": []}, {"text": "The set of cues used and the frequency of their certain and uncertain usages are domain and genre dependent, however, and this has to be addressed if we seek to craft automatic uncertainty detectors.", "labels": [], "entities": []}, {"text": "Here we interpret genre as the basic style and formal characteristics of the writing that is independent of its topic (e.g., scientific papers, newswire texts, or business letters), and domain as a particular field of knowledge and is related to the topic of the text (e.g., medicine, archeology, or politics).", "labels": [], "entities": []}, {"text": "Uncertainty cue candidates do not display uncertainty in all of their occurrences.", "labels": [], "entities": []}, {"text": "For instance, the mathematical sense of probable is dominant in mathematical texts whereas its ratio can be relatively low in papers in the humanities.", "labels": [], "entities": []}, {"text": "The frequency of the two distinct meanings of the verb evaluate (which can be a synonym of judge [an uncertain meaning] and calculate) is also different in the bioinformatics and cell biology domains.", "labels": [], "entities": []}, {"text": "Compare: (1) To evaluate CUE the PML/RARalpha role in myelopoiesis, transgenic mice expressing PML/RARalpha were engineered.", "labels": [], "entities": [{"text": "CUE", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6254885792732239}]}, {"text": "(2) Our method was evaluated on the Lindahl benchmark for fold recognition.", "labels": [], "entities": [{"text": "fold recognition", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.9110093712806702}]}, {"text": "In this article we focus on the domain-dependent aspects of uncertainty detection and we examine the recognition of uncertainty cues in context.", "labels": [], "entities": [{"text": "uncertainty detection", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7932355105876923}]}, {"text": "We do not address the problem of linking cues to propositions in detail (see, e.g., Chapman, Chu, and Dowling and for the information extraction case).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.8004213273525238}]}, {"text": "For the empirical investigation of the domain dependent aspects, data sets are required from various domains.", "labels": [], "entities": []}, {"text": "To date, several corpora annotated for uncertainty have been constructed for different genres and domains (BioScope, FactBank, WikiWeasel, and MPQA, to name but a few).", "labels": [], "entities": [{"text": "MPQA", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.9041616320610046}]}, {"text": "These corpora cover different aspects of uncertainty, however, being grounded on different linguistic models, which makes it hard to exploit cross-domain knowledge in applications.", "labels": [], "entities": []}, {"text": "These differences in part stem from the varied application needs across application domains.", "labels": [], "entities": []}, {"text": "Different types of uncertainty and classes of linguistic expressions are relevant for different domains.", "labels": [], "entities": []}, {"text": "Although hypotheses and investigations form a crucial part of the relevant cases in scientific applications, they are less prominent in newswire texts, where beliefs and rumors play a major role.", "labels": [], "entities": []}, {"text": "This finding motivates a more fine-grained treatment of uncertainty.", "labels": [], "entities": []}, {"text": "In order to bridge the existing gaps between application goals, these typical cases need to be differentiated.", "labels": [], "entities": []}, {"text": "A fine-grained categorization enables the individual treatment of each subclass, which is less dependent on domain differences than using one coarse-grained uncertainty class.", "labels": [], "entities": []}, {"text": "Moreover, this approach enables each particular application to identify and select from a pool of models only those aspects of uncertainty that are relevant in the specific domain.", "labels": [], "entities": []}, {"text": "As one of the main contributions of this study, we propose a uniform subcategorization of semantic uncertainty in which all the previous corpus annotation works can be placed, and which reveals the fundamental differences between the currently existing resources.", "labels": [], "entities": []}, {"text": "In addition, we manually harmonized the annotations of three corpora and performed the fine-grained labeling according to the suggested subcategorization so as to be able to perform cross-domain experiments.", "labels": [], "entities": []}, {"text": "An important factor in training robust cross-domain models is to focus on shallow features that can be reliably obtained for many different domains and text types, and to craft models that exploit the shared knowledge from different sources as much as possible, making the adaptation to new domains efficient.", "labels": [], "entities": []}, {"text": "The study of learning efficient models across different domains is the subject of transfer learning and domain adaptation research (cf..", "labels": [], "entities": []}, {"text": "The domain adaptation setting assumes a target domain (for which an accurate model should be learned with a limited amount of labeled training data), a source domain (with characteristics different from the target and for which a substantial amount of labeled data is available), and an arbitrary supervised learning model that exploits both the target and source domain data in order to learn an improved target domain model.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7420530617237091}]}, {"text": "The success of domain adaptation mainly depends on two factors: (i) the similarity of the target and source domains (the two domains should be sufficiently similar to allow knowledge transfer); and (ii) the application of an efficient domain adaptation method (which permits the learning algorithm to exploit the commonalities of the domains while preserving the special characteristics of the target domain).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7475987076759338}, {"text": "knowledge transfer", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.7131689488887787}]}, {"text": "As our second main contribution, we study the impact of domain differences on uncertainty detection, how this impact depends on the distance between target and source domains concerning their domains and genres, and how these differences can be reduced to produce accurate target domain models with limited annotation effort.", "labels": [], "entities": [{"text": "uncertainty detection", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.7483615577220917}]}, {"text": "Because previously existing resources exhibited fundamental differences that made domain adaptation difficult, 1 to our knowledge this is the first study to analyze domain differences and adaptability in the context of uncertainty detection in depth, and also the first study to report consistently positive results in cross-training.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7530990242958069}, {"text": "uncertainty detection", "start_pos": 219, "end_pos": 240, "type": "TASK", "confidence": 0.7502734959125519}]}, {"text": "The main contributions of the current paper can be summarized as follows: r We provide a uniform subcategorization of semantic uncertainty (with definitions, examples, and test batteries for annotation) and classify all major previous studies on uncertainty corpus annotation into the proposed categorization system, in order to reveal and analyze the differences.", "labels": [], "entities": []}, {"text": "r We provide a harmonized, fine-grained reannotation of three corpora, according to the suggested subcategorization, to allow an in-depth analysis of the domain dependent aspects of uncertainty detection.", "labels": [], "entities": [{"text": "uncertainty detection", "start_pos": 182, "end_pos": 203, "type": "TASK", "confidence": 0.7262866050004959}]}, {"text": "r We compare the two state-of-the-art approaches to uncertainty cue detection (i.e., the one based on token classification and the one on sequence labeling models), using a shared feature set, in the context of the CoNLL-2010 shared task, to understand their strengths and weaknesses.", "labels": [], "entities": [{"text": "uncertainty cue detection", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6375328401724497}, {"text": "token classification", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7635020911693573}]}, {"text": "r We train an accurate semantic uncertainty detector that distinguishes four fine-grained categories of semantic uncertainty (epistemic, doxastic, investigation, and condition types) and thus is better for future applications in various domains than previous models.", "labels": [], "entities": []}, {"text": "Our experiments reveal that, similar to the best model of the CoNLL-2010 shared task for biological texts but in a fine-grained context, shallow features provide good results in recognizing semantic uncertainty.", "labels": [], "entities": [{"text": "recognizing semantic uncertainty", "start_pos": 178, "end_pos": 210, "type": "TASK", "confidence": 0.8537110288937887}]}, {"text": "We also show that this representation is less suited to detecting discourse-level uncertainty (which was part of the CoNLL task for Wikipedia texts).", "labels": [], "entities": []}, {"text": "r We examine in detail the differences between domains and genres as regards the language used to express semantic uncertainty, and learn how the domain or genre distance affects uncertainty recognition in texts with unseen characteristics.", "labels": [], "entities": [{"text": "uncertainty recognition", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.7791595160961151}]}, {"text": "r We apply domain adaptation techniques to fully exploit out-of-domain data and minimize annotation costs to adapt to anew domain, and we report successful results for various text domains and genres.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, our classification of uncertainty phenomena is presented in detail and it is compared with the concept of uncertainty used in existing corpora.", "labels": [], "entities": []}, {"text": "A framework for detecting semantic uncertainty is then presented in Section 3.", "labels": [], "entities": [{"text": "detecting semantic uncertainty", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.8675819436709086}]}, {"text": "Related work on cue detection is summarized in Section 4, which is followed by a description of our cue recognition system and a presentation of our experimental set-up using various source and target genre and domain pairs for cross-domain learning and domain adaptation in Section 5.", "labels": [], "entities": [{"text": "cue detection", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8322010040283203}, {"text": "cue recognition", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7067165821790695}, {"text": "domain adaptation", "start_pos": 254, "end_pos": 271, "type": "TASK", "confidence": 0.7136728763580322}]}, {"text": "Our results are elaborated on in Section 6 with a focus on the effect of domain similarities and on the annotation effort needed to cover anew domain.", "labels": [], "entities": []}, {"text": "We then conclude with a summary of our results and make some suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "As evaluation metrics, we used cue-level and sentence-level F \u03b2=1 scores for the uncertain class (the standard evaluation metrics of Task 1 of the CoNLL-2010 shared task) and denote them by F cue and F sent , respectively.", "labels": [], "entities": [{"text": "CoNLL-2010 shared task", "start_pos": 147, "end_pos": 169, "type": "DATASET", "confidence": 0.821449895699819}]}, {"text": "We report cue-level F \u03b2=1 scores on the individual subcategories of uncertainty and the unlabeled (binary) F \u03b2=1 scores as well.", "labels": [], "entities": [{"text": "cue-level F \u03b2=1 scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.88829438885053}, {"text": "F \u03b2=1 scores", "start_pos": 107, "end_pos": 119, "type": "METRIC", "confidence": 0.9602183103561401}]}, {"text": "A sentence is treated as uncertain (in the gold standard and prediction) iff it contains at least one cue.", "labels": [], "entities": []}, {"text": "Note that the cue-level metric is quite strict as it is based on recognized phrases-that is, only cues with perfect boundary matches are true positives.", "labels": [], "entities": []}, {"text": "For the sentence-level evaluation we simply labeled those sentences as uncertain that contained at least one recognized cue.", "labels": [], "entities": []}, {"text": "Learning to detect hedges and their scope in natural language text focused on uncertainty detection.", "labels": [], "entities": [{"text": "uncertainty detection", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.7119453102350235}]}, {"text": "Two subtasks were defined at the shared task: The first task sought to recognize sentences that contain some uncertain language in two different domains and the second task sought to recognize lexical cues together with their linguistic scope in biological texts (i.e., the text span in terms of constituency grammar that covers the part of the sentence that is modified by the cue).", "labels": [], "entities": []}, {"text": "The lexical cue recognition subproblem of the second task 7 is identical to the problem setting used in this study, with the only major difference being the types of uncertainty addressed: In the CoNLL-2010 task biological texts contained only epistemic, doxastic, and investigation types of uncertainty.", "labels": [], "entities": [{"text": "CoNLL-2010 task biological texts", "start_pos": 196, "end_pos": 228, "type": "DATASET", "confidence": 0.8224072605371475}]}, {"text": "Apart from these differences, the CoNLL-2010 shared task offers an excellent testbed for comparing our uncertainty detection model with other state-of-the-art approaches for uncertainty detection and to compare different classification approaches.", "labels": [], "entities": [{"text": "CoNLL-2010 shared task", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.7909080982208252}, {"text": "uncertainty detection", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.7335713654756546}, {"text": "uncertainty detection", "start_pos": 174, "end_pos": 195, "type": "TASK", "confidence": 0.7927950024604797}]}, {"text": "Here we present our detailed experiments using the CoNLL data sets, analyze the performance of our models, and select the most suitable models for further experiments.", "labels": [], "entities": [{"text": "CoNLL data sets", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9774206678072611}]}, {"text": "The uncertainty detection systems that were submitted to the CoNLL shared task can be classified into three major types.", "labels": [], "entities": [{"text": "uncertainty detection", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7859581410884857}, {"text": "CoNLL shared task", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.7127820054690043}]}, {"text": "The first set of systems treats the problem as a sentence classification task, that is, one to decide whether a sentence contains any uncertain element or not.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7893551786740621}]}, {"text": "These models operate at the sentence level and are unsuitable for cue detection.", "labels": [], "entities": [{"text": "cue detection", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8076607882976532}]}, {"text": "The second group handles the problem as a token Results on the original CoNLL-2010 data sets.", "labels": [], "entities": [{"text": "CoNLL-2010 data sets", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.9811106125513712}]}, {"text": "The first three rows correspond to our baseline, token-based, and sequence labeling models.", "labels": [], "entities": []}, {"text": "The BEST/SEQ row shows the results of the best sequence labeling approach of the CoNLL shared task (for both domains), the BEST/TOK rows show the best token-based models, and the BEST/SENT rows show the best sentence-level classifiers (these models did not produce cue-level results).", "labels": [], "entities": [{"text": "BEST", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.991647481918335}, {"text": "BEST", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9905073642730713}, {"text": "BEST", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9375697374343872}]}, {"text": "classification task, and classifies each token independently as uncertain (or not).", "labels": [], "entities": [{"text": "classification task", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8417097330093384}]}, {"text": "Contextual information is only included in the form of feature functions.", "labels": [], "entities": []}, {"text": "The third group of systems handled the task as a sequential token labeling problem, that is, determined the most likely label sequence of a sentence in one step, taking the information about neighboring labels into account.", "labels": [], "entities": [{"text": "sequential token labeling", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6024993658065796}]}, {"text": "Sequence labeling and token classification approaches performed best for biological texts and sentence-level models and token classification approaches gave the best results for Wikipedia texts (see in Farkas et al.).", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8408375978469849}, {"text": "token classification", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8142045736312866}, {"text": "token classification", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.850215882062912}]}, {"text": "Here we compare a state-of-the-art token classification and sequence labeling approach using a shared feature representation to decide which model to use in further experiments.", "labels": [], "entities": [{"text": "token classification", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.873280793428421}, {"text": "sequence labeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7152815461158752}]}, {"text": "We defined several settings (target and source pairs) with varied domain and genre distances and target data set sizes.", "labels": [], "entities": []}, {"text": "These experiments allowed us to study the potential of transferring knowledge across existing corpora for the accurate detection of uncertain language in a wide variety of text types.", "labels": [], "entities": []}, {"text": "In our experiments, we used all the combinations of genres and domains that we found plausible.", "labels": [], "entities": []}, {"text": "News texts (and their subdomains) were not used as source data because FactBank is significantly smaller than the other corpora (WikiWeasel or scientific texts).", "labels": [], "entities": [{"text": "FactBank", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9052745699882507}]}, {"text": "As the source data set is typically larger than the target data set in practical scenarios, news texts can only be used as target data.", "labels": [], "entities": []}, {"text": "Abstracts were only used as source data because information extraction typically addresses full texts whereas abstracts just provide annotated data for development purposes.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7251092940568924}]}, {"text": "Besides these restrictions, we experimented with all possible target and source pairs.", "labels": [], "entities": []}, {"text": "We used four different machine-learning settings for each target-source pair in our investigations.", "labels": [], "entities": []}, {"text": "In the purely cross-domain (CROSS) setting, the model was trained on the source domain and evaluated on the target (i.e., no labeled target domain data sets were used for training).", "labels": [], "entities": []}, {"text": "In the purely in-domain setting (TARGET), we performed Experimental results on different target and source domain pairs.", "labels": [], "entities": [{"text": "TARGET", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.8982037901878357}]}, {"text": "The third column contains the ratio of the target train and source data sets' sizes in terms of sentences.", "labels": [], "entities": []}, {"text": "DIST shows the distance of the source and target domain/genre ('-' same; '+' fine-grade difference; '++' coarse-grade difference; bio = biological; enc = encyclopedia; sci paper = scientific paper; sci abs = scientific abstract; sci paper hbc = scientific papers on human blood cell experiments; sci paper fly = scientific papers on Drosophila; sci paper bmc = scientific papers on bioinformatics).", "labels": [], "entities": [{"text": "DIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.783913254737854}]}, {"text": "10-fold cross-validation on the target data (i.e., no source domain data were used).", "labels": [], "entities": []}, {"text": "In the two domain adaptation settings, we again performed 10-fold cross-validation on the target data but exploited the source data set (as described in Section 5.3).", "labels": [], "entities": []}, {"text": "Here, we either used each sentence of the source data set (DA/ALL) or only those sentences that contained a cue observed in the target train data set (DA/CUE).", "labels": [], "entities": []}, {"text": "lists the results obtained on various target and source domains in various machine learning settings and contains the absolute differences between a particular result and the in-domain (TARGET) results.", "labels": [], "entities": [{"text": "TARGET", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9053612947463989}]}], "tableCaptions": [{"text": " Table 2  Data on the corpora. sent. = sentence; epist. = epistemic cue; dox. = doxastic cue;  inv. = investigation cue; cond. = condition cue.", "labels": [], "entities": []}, {"text": " Table 4  Results on the original CoNLL-2010 data sets. The first three rows correspond to our baseline,  token-based, and sequence labeling models. The BEST/SEQ row shows the results of the best  sequence labeling approach of the CoNLL shared task (for both domains), the BEST/TOK rows  show the best token-based models, and the BEST/SENT rows show the best sentence-level  classifiers (these models did not produce cue-level results).", "labels": [], "entities": [{"text": "CoNLL-2010 data sets", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.9487512111663818}, {"text": "BEST/SEQ row", "start_pos": 153, "end_pos": 165, "type": "METRIC", "confidence": 0.863861009478569}, {"text": "BEST", "start_pos": 273, "end_pos": 277, "type": "METRIC", "confidence": 0.9813854694366455}, {"text": "BEST", "start_pos": 330, "end_pos": 334, "type": "METRIC", "confidence": 0.9788079261779785}]}, {"text": " Table 5  Experimental results on different target and source domain pairs. The third column contains the  ratio of the target train and source data sets' sizes in terms of sentences. DIST shows the distance  of the source and target domain/genre ('-' same; '+' fine-grade difference; '++' coarse-grade  difference; bio = biological; enc = encyclopedia; sci paper = scientific paper; sci abs = scientific  abstract; sci paper hbc = scientific papers on human blood cell experiments; sci paper fly =  scientific papers on Drosophila; sci paper bmc = scientific papers on bioinformatics).", "labels": [], "entities": [{"text": "DIST", "start_pos": 184, "end_pos": 188, "type": "DATASET", "confidence": 0.6908473372459412}]}, {"text": " Table 8  The per class cue-level F-scores in fine-grained classification. F crs , F tgt , and F da correspond to the  CROSS, TARGET, and DA/CUE settings, respectively (same as previous). The DA/ALL setting  is not shown for space reasons and due to its similarity to the DA/CUE results.", "labels": [], "entities": [{"text": "CROSS", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.6511846780776978}, {"text": "TARGET", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9648367166519165}]}]}