{"title": [{"text": "Learning Entailment Relations by Global Graph Structure Optimization", "labels": [], "entities": [{"text": "Learning Entailment Relations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7288332382837931}, {"text": "Global Graph Structure Optimization", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.6706249639391899}]}], "abstractContent": [{"text": "Identifying entailment relations between predicates is an important part of applied semantic inference.", "labels": [], "entities": [{"text": "Identifying entailment relations between predicates", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.9040986776351929}]}, {"text": "In this article we propose a global inference algorithm that learns such entailment rules.", "labels": [], "entities": []}, {"text": "First, we define a graph structure over predicates that represents entailment relations as directed edges.", "labels": [], "entities": []}, {"text": "Then, we use a global transitivity constraint on the graph to learn the optimal set of edges, formulating the optimization problem as an Integer Linear Program.", "labels": [], "entities": []}, {"text": "The algorithm is applied in a setting where, given a target concept, the algorithm learns on the fly all entailment rules between predicates that co-occur with this concept.", "labels": [], "entities": []}, {"text": "Results show that our global algorithm improves performance over baseline algorithms by more than 10%.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Textual Entailment (TE) paradigm is a generic framework for applied semantic inference.", "labels": [], "entities": [{"text": "Textual Entailment (TE) paradigm", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8133635024229685}]}, {"text": "The objective of TE is to recognize whether a target textual meaning can be inferred from another given text.", "labels": [], "entities": [{"text": "TE", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9694446921348572}]}, {"text": "For example, a question answering system has to recognize that alcohol affects blood pressure is inferred from the text alcohol reduces blood pressure to answer the question What affects blood pressure?", "labels": [], "entities": [{"text": "question answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7749786078929901}]}, {"text": "In the TE framework, entailment is defined as a directional relationship between pairs of text expressions, denoted by T, the entailing text, and H, the entailed hypothesis.", "labels": [], "entities": []}, {"text": "The text T is said to entail the hypothesis H if, typically, a human reading T would infer that H is most likely true ().", "labels": [], "entities": []}, {"text": "TE systems require extensive knowledge of entailment patterns, often captured as entailment rules-rules that specify a directional inference relation between two text fragments (when the rule is bidirectional this is known as paraphrasing).", "labels": [], "entities": []}, {"text": "A common type of text fragment is a proposition, which is a simple natural language expression that contains a predicate and arguments (such as alcohol affects blood pressure), where the predicate denotes some semantic relation between the concepts that are expressed by the arguments.", "labels": [], "entities": []}, {"text": "One important type of entailment rule specifies entailment between propositional templates, that is, propositions where the arguments are possibly replaced by variables.", "labels": [], "entities": []}, {"text": "A rule corresponding to the aforementioned example maybe X reduce blood pressure \u2192 X affect blood pressure.", "labels": [], "entities": []}, {"text": "Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task.", "labels": [], "entities": [{"text": "TE task", "start_pos": 107, "end_pos": 114, "type": "TASK", "confidence": 0.9213940501213074}]}, {"text": "This has led to active research on broad-scale acquisition of entailment rules for predicates.", "labels": [], "entities": []}, {"text": "Previous work has focused on learning each entailment rule in isolation.", "labels": [], "entities": []}, {"text": "It is clear, however, that there are interactions between rules.", "labels": [], "entities": []}, {"text": "A prominent phenomenon is that entailment is inherently a transitive relation, and thus the rules X \u2192 Y and Y \u2192 Z imply the rule X \u2192 Z.", "labels": [], "entities": []}, {"text": "In this article we take advantage of these global interactions to improve entailment rule learning.", "labels": [], "entities": [{"text": "entailment rule learning", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.6953292489051819}]}, {"text": "After reviewing relevant background (Section 2), we describe a structure termed an entailment graph that models entailment relations between propositional templates (Section 3).", "labels": [], "entities": []}, {"text": "Next, we motivate and discuss a specific type of entailment graph, termed a focused entailment graph, where a target concept instantiates one of the arguments of all propositional templates.", "labels": [], "entities": []}, {"text": "For example, a focused entailment graph about the target concept nausea might specify the entailment relations between propositional templates like X induce nausea, X prevent nausea, and nausea is a symptom of X.", "labels": [], "entities": []}, {"text": "In the core section of the article, we present an algorithm that uses a global approach to learn the entailment relations, which comprise the edges of focused entailment graphs (Section 4).", "labels": [], "entities": []}, {"text": "We define a global objective function and look for the graph that maximizes that function given scores provided by a local entailment classifier and a global transitivity constraint.", "labels": [], "entities": []}, {"text": "The optimization problem is formulated as an Integer Linear Program (ILP) and is solved with an ILP solver, which leads to an optimal solution with respect to the global function.", "labels": [], "entities": []}, {"text": "In Section 5 we demonstrate that this algorithm outperforms by 12-13% methods that utilize only local information as well as methods that employ a greedy optimization algorithm rather than an ILP solver.", "labels": [], "entities": []}, {"text": "The article also includes a comprehensive investigation of the algorithm and its components.", "labels": [], "entities": []}, {"text": "First, we perform manual comparison between our algorithm and the baselines and analyze the reasons for the improvement in performance (Sections 5.3.1 and.", "labels": [], "entities": []}, {"text": "Then, we analyze the errors made by the algorithm against manually prepared gold-standard graphs and compare them to the baselines (Section 5.4).", "labels": [], "entities": []}, {"text": "Last, we perform a series of experiments in which we investigate the local entailment classifier and specifically experiment with various sets of features (Section 6).", "labels": [], "entities": []}, {"text": "We conclude and suggest future research directions in Section 7.", "labels": [], "entities": []}, {"text": "This article is based on previous work, while substantially expanding upon it.", "labels": [], "entities": []}, {"text": "From a theoretical point of view, we reformulate the two ILPs previously introduced by incorporating a prior.", "labels": [], "entities": [{"text": "ILPs", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.9213802814483643}]}, {"text": "We show a theoretical relation between the two ILPs and prove that the optimization problem tackled is NP-hard.", "labels": [], "entities": []}, {"text": "From an empirical point of view, we conduct many new experiments that examine both the local entailment classifier as well as the global algorithm.", "labels": [], "entities": []}, {"text": "Last, a rigorous analysis of the algorithm is performed and an extensive survey of previous work is provided.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents an evaluation and analysis of our algorithm.", "labels": [], "entities": []}, {"text": "A health-care corpus of 632MB was harvested from the Web and parsed using the Minipar parser (Lin 1998b).", "labels": [], "entities": [{"text": "Minipar parser", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9146942794322968}]}, {"text": "The corpus contains 2,307,585 sentences and almost 50 million The similarity score features used to represent pairs of templates.", "labels": [], "entities": []}, {"text": "The columns specify the corpus over which the similarity score was computed, the template representation, the similarity measure employed, and the feature representation (as described in Section 4.1 to annotate medical concepts in the corpus.", "labels": [], "entities": []}, {"text": "The UMLS is a database that maps natural language phrases to over one million concept identifiers in the health-care domain (termed CUIs).", "labels": [], "entities": []}, {"text": "We annotated all nouns and noun phrases that are in the UMLS with their (possibly multiple) CUIs.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.933164656162262}]}, {"text": "We now provide the details of training an entailment classifier as explained in Section 4.1.", "labels": [], "entities": []}, {"text": "We extracted all templates from the corpus where both argument instantiations are medical concepts, that is, annotated with a CUI (\u223c50,000 templates).", "labels": [], "entities": []}, {"text": "This was done to increase the likelihood that the extracted templates are related to the health-care domain and reduce problems of ambiguity.", "labels": [], "entities": []}, {"text": "As explained in Section 4.1, a pair of templates constitutes an input example for the entailment classifier, and should be represented by a set of features.", "labels": [], "entities": []}, {"text": "The features we used were different distributional similarity scores for the pair of templates, as summarized in.", "labels": [], "entities": []}, {"text": "Twelve distributional similarity measures were computed over the health-care corpus using the aforementioned variations (Section 4.1), where two feature representations were considered: in the UMLS each natural language phrase maybe mapped not to a single CUI, but to a tuple of CUIs.", "labels": [], "entities": []}, {"text": "Therefore, in the first representation, each feature vector coordinate counts the number of times a tuple of CUIs was mapped to the term instantiating the template argument, and in the second representation it counts the number of times each single CUI was one of the CUIs mapped to the term instantiating the template argument.", "labels": [], "entities": []}, {"text": "In addition, we obtained the original template similarity lists learned by, and had available three distributional similarity measures learned by, over the RCV1 corpus, 7 as detailed in.", "labels": [], "entities": [{"text": "RCV1 corpus", "start_pos": 156, "end_pos": 167, "type": "DATASET", "confidence": 0.9811257421970367}]}, {"text": "Thus, each pair of templates is represented by a total of 16 distributional similarity scores.", "labels": [], "entities": []}, {"text": "We automatically generated a balanced training set of 20,144 examples using WordNet and the procedure described in Section 4.1, and trained the entailment classifier with SVMperf).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9724324941635132}]}, {"text": "We use the trained classifier to obtain estimates for P uv and S uv , given that the score-based and probabilistic scoring functions are equivalent (cf. Section 4.2.3).", "labels": [], "entities": []}, {"text": "To evaluate the performance of our algorithm, we manually constructed goldstandard entailment graphs.", "labels": [], "entities": []}, {"text": "First, 23 medical target concepts, representing typical topics of interest in the medical domain, were manually selected from a (longer) list of the most frequent concepts in the health-care corpus.", "labels": [], "entities": []}, {"text": "The 23 target concepts are: alcohol, asthma, biopsy, brain, cancer, CDC, chemotherapy, chest, cough, diarrhea, FDA, headache, HIV, HPV, lungs, mouth, muscle, nausea, OSHA, salmonella, seizure, smoking, and x-ray.", "labels": [], "entities": []}, {"text": "For each concept, we wish to learn a focused entailment graph (cf..", "labels": [], "entities": []}, {"text": "Thus, the nodes of each graph were defined by extracting all propositional templates in which the corresponding target concept instantiated an argument at least K(= 3) times in the healthcare corpus (average number of graph nodes = 22.04, std = 3.66, max = 26, min = 13).", "labels": [], "entities": [{"text": "max", "start_pos": 251, "end_pos": 254, "type": "METRIC", "confidence": 0.9517275094985962}]}, {"text": "Ten medical students were given the nodes of each graph (propositional templates) and constructed the gold standard of graph edges using a Web interface.", "labels": [], "entities": []}, {"text": "We gave an oral explanation of the annotation process to each student, and the first two graphs annotated by every student were considered part of the annotator training phase and were discarded.", "labels": [], "entities": []}, {"text": "The annotators were able to select every propositional template and observe all of the instantiations of that template in our health-care corpus.", "labels": [], "entities": []}, {"text": "For example, selecting the template X helps with nausea might show the propositions relaxation helps with nausea, acupuncture helps with nausea, and Nabilone helps with nausea.", "labels": [], "entities": []}, {"text": "The concept of entailment was explained under the framework of TE (, that is, the template t 1 entails the template t 2 if given that the instantiation oft 1 with some concept is true then the instantiation oft 2 with the same concept is most likely true.", "labels": [], "entities": [{"text": "TE", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.8209758400917053}]}, {"text": "As explained in Section 3.2, we did not perform any disambiguation because a target concept disambiguates the propositional templates in focused entailment graphs.", "labels": [], "entities": []}, {"text": "In practice, cases of ambiguity were very rare, except fora single scenario wherein templates such as X treats asthma, annotators were unclear whether X is a type of doctor or a type of drug.", "labels": [], "entities": []}, {"text": "The annotators were instructed in such cases to select the template, read the instantiations of the template in the corpus, and choose the sense that is most prevalent in the corpus.", "labels": [], "entities": []}, {"text": "This instruction was applicable to all cases of ambiguity.", "labels": [], "entities": []}, {"text": "Each concept graph was annotated by two students.", "labels": [], "entities": []}, {"text": "Following the current recognizing TE (RTE) practice (), after initial annotation the two students met fora reconciliation phase.", "labels": [], "entities": [{"text": "recognizing TE (RTE)", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8064182162284851}]}, {"text": "They worked to reach an agreement on differences and corrected their graphs.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was calculated using the kappa statistic ( both before (\u03ba = 0.59) and after (\u03ba = 0.9) reconciliation.", "labels": [], "entities": []}, {"text": "Each learned graph was evaluated against the two reconciliated graphs.", "labels": [], "entities": []}, {"text": "Summing the number of possible edges overall 23 concept graphs we get 10,364 possible edges, of which 882 on average were included by the annotators (averaging over the two gold-standard annotations for each graph).", "labels": [], "entities": []}, {"text": "The concept graphs were randomly split into a development set (11 concepts) and a test set (12 concepts).", "labels": [], "entities": []}, {"text": "We used the lpsolve 8 package to learn the edges of the graphs.", "labels": [], "entities": []}, {"text": "This package efficiently solves the model without imposing integer restrictions and then uses the branch-and-bound method to find an optimal integer solution.", "labels": [], "entities": []}, {"text": "We note that in the experiments reported in this article the optimal solution without integer restrictions was already integer.", "labels": [], "entities": []}, {"text": "Thus, although in general our optimization problem is NP-hard, in our experiments we were able to reach an optimal solution for the input graphs very efficiently (we note that in some scenarios not reported in this article the optimal solution was not integer and so an integer solution is not guaranteed a priori).", "labels": [], "entities": []}, {"text": "As mentioned in Section 4.2, we added a few constraints in cases where there was strong evidence that edges are not in the graph.", "labels": [], "entities": []}, {"text": "This is done in the following scenarios (examples given in In addition, in some cases we have strong evidence that edges do exist in the graph.", "labels": [], "entities": []}, {"text": "This is done in a single scenario (see), which is specific to the output of Minipar: when two templates differ by a single edge and the first is of the type Altogether, these initializations took place in less than 1% of the node pairs in the graphs.", "labels": [], "entities": [{"text": "Minipar", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9278452396392822}]}, {"text": "We note that we tried to use WordNet relations such as hypernym and synonym as \"positive\" hard constraints (using the constraint I uv = 1), but this resulted in reduced performance because the precision of WordNet was not high enough.", "labels": [], "entities": [{"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9983592629432678}, {"text": "WordNet", "start_pos": 206, "end_pos": 213, "type": "DATASET", "confidence": 0.9547097682952881}]}, {"text": "The graphs learned by our algorithm were evaluated by two measures.", "labels": [], "entities": []}, {"text": "The first measure evaluates the graph edges directly, and the second measure is motivated by semantic inference applications that utilize the rules in the graph.", "labels": [], "entities": []}, {"text": "The first measure is simply the F 1 of the set of learned edges compared to the set of gold-standard edges.", "labels": [], "entities": [{"text": "F 1", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9728476107120514}]}, {"text": "In the second measure we take the set of learned rules and infer new propositions by applying the rules overall propositions extracted from the health-care corpus.", "labels": [], "entities": []}, {"text": "We apply the rules iteratively overall propositions until no new propositions are inferred.", "labels": [], "entities": []}, {"text": "For example, given the corpus proposition relaxation reduces nausea and the edges X reduces nausea \u2192 X helps with nausea and X helps with nausea \u2192 X related to nausea, we evaluate the set {relaxation reduces nausea, relaxation helps with nausea, relaxation related to nausea}.", "labels": [], "entities": []}, {"text": "For each graph we measure the F 1 of the set of propositions inferred by the learned graphs when compared to the set of propositions inferred by the gold-standard graphs.", "labels": [], "entities": [{"text": "F 1", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9900195300579071}]}, {"text": "For both measures the final score of an algorithm is a macro-average F 1 over the 24 gold-standard test-set graphs (two gold-standard graphs for each of the 12 test concepts).", "labels": [], "entities": []}, {"text": "Learning the edges of a graph given an input concept takes about 1-2 seconds on a standard desktop.", "labels": [], "entities": []}, {"text": "In this section we present experimental results and analysis that show that the ILP-Global algorithm improves performance over baselines, specifically in terms of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9972568154335022}]}, {"text": "Tables 4-7 and summarize the performance of the algorithms.", "labels": [], "entities": []}, {"text": "shows our main result when the parameters \u03bb and K are optimized to maximize performance over the development set.", "labels": [], "entities": []}, {"text": "Notice that the algorithm ILP-Global-Likelihood is omitted, because when optimizing \u03bb over the development set it conflates with ILP-Global.", "labels": [], "entities": []}, {"text": "The rows Local and Local 2 present the best algorithms that use a single distributional similarity resource.", "labels": [], "entities": []}, {"text": "Local 1 and Local 2 correspond to the configurations Results with prior estimated on the development set, that is \u03b7 = 0.1, which is equivalent to \u03bb = 2.3.", "labels": [], "entities": []}, {"text": "In previous sections we employed a distant supervision framework: We generated training examples automatically with WordNet, and represented each example with distributional similarity features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9777293801307678}]}, {"text": "Distant supervision comes with a price, however-it prevents us from utilizing all sources of information.", "labels": [], "entities": []}, {"text": "For example, looking at the pair of gold-standard templates X manages asthma and X improves asthma management, one can exploit the fact that management is a derivation of manage to improve the estimation of entailment.", "labels": [], "entities": []}, {"text": "The automatically generated training set was generated by looking at WordNet's hypernym, synonym, and co-hyponyms relations, however, and hence no such examples appear in the training set, rendering this type of feature useless.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.92760169506073}]}, {"text": "Moreover, one cannot use WordNet's hypernym, synonym, and co-hyponym relations as features because the generated training set is highly biased-all positive training examples are either hypernyms or synonyms and all negative examples are co-hyponyms.", "labels": [], "entities": []}, {"text": "In this section we would like to examine the utility of various features, while avoiding the biases that occur due to distant supervision.", "labels": [], "entities": []}, {"text": "Therefore, we use the 23 manually annotated gold-standard graphs for both training and testing, in a cross-validation setting.", "labels": [], "entities": []}, {"text": "Although this reduces the size of the training set it allows us to estimate the utility of various features in a setting where the training set and test set are sampled from the same underlying distribution, without the aforementioned biases.", "labels": [], "entities": []}, {"text": "We would like to extract features that express information that is diverse and orthogonal to the one given by distributional similarity.", "labels": [], "entities": []}, {"text": "Therefore, we turn to existing knowledge resources that were created using both manual and automatic methods, expressing various types of linguistic and statistical information that is relevant for entailment prediction: 1.", "labels": [], "entities": [{"text": "entailment prediction", "start_pos": 198, "end_pos": 219, "type": "TASK", "confidence": 0.8534172177314758}]}, {"text": "WordNet: contains manually annotated relations such as hypernymy, synonymy, antonymy, derivation, and entailment.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9617530107498169}]}, {"text": "Table 13 describes the macro-average recall, precision, and F 1 of all classifiers both with and without the new features on the development set and test set.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9728971123695374}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9992915391921997}, {"text": "F 1", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9792759418487549}]}, {"text": "Using all features is denoted by X all , and using the original features is denoted by X old . Examining the results it does not appear that the new features improve performance.", "labels": [], "entities": []}, {"text": "Whereas on the development set the new features add 1.2-1.5 F 1 points for all SVM classifiers, on the test set using the new features decreases performance for the linear and square classifiers.", "labels": [], "entities": [{"text": "F 1 points", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9472288886706034}]}, {"text": "This shows that even if there is some slight increase in performance when using SVM on the development set, it is masked by the variance added in the process of parameter tuning.", "labels": [], "entities": []}, {"text": "In general, including the new features does not yield substantial differences in performance.", "labels": [], "entities": []}, {"text": "Secondly, the SVM classifiers perform better than the logistic and naive Bayes classifiers.", "labels": [], "entities": []}, {"text": "Using the more complex square and Gaussian kernels does not seem justified, however, as the differences between the various kernels are negligible.", "labels": [], "entities": []}, {"text": "Therefore, in our analysis we will use a linear kernel SVM classifier.", "labels": [], "entities": []}, {"text": "Last, we note that although we use supervised learning rather than distant supervision, the results we get are slightly lower than those presented in Section 5.", "labels": [], "entities": []}, {"text": "This is probably due to the fact that our manually annotated data set is rather small.", "labels": [], "entities": []}, {"text": "Nevertheless, this shows that the quality of the distant supervision training set generated automatically from WordNet is reasonable.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.6538227200508118}]}, {"text": "Next, we perform analysis of the different features of the classifier to better understand the reasons for the negative result obtained.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4  Results when tuning for performance over the development set.", "labels": [], "entities": []}, {"text": " Table 5  Results when the development set is not used to estimate \u03bb and K.", "labels": [], "entities": []}, {"text": " Table 6  Results with prior estimated on the development set, that is \u03b7 = 0.1, which is equivalent to  \u03bb = 2.3.", "labels": [], "entities": []}, {"text": " Table 7  Results per concept for the ILP-Global.", "labels": [], "entities": [{"text": "ILP-Global", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9181035161018372}]}, {"text": " Table 9  Comparing disagreements between ILP-Global and ILP-Local against the gold-standard graphs.", "labels": [], "entities": []}, {"text": " Table 11  Error analysis for false positives and false negatives.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9268101453781128}]}, {"text": " Table 12  The set of new features. The last two columns denote the number and percentage of examples  for which the value of the feature is non-zero in examples generated from the 23 gold-standard  graphs.", "labels": [], "entities": []}, {"text": " Table 13  Macro-average recall, precision, and F 1 on the development set and test set using the parameters  that maximize F 1 of the learned edges over the development set.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9924502372741699}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9996354579925537}, {"text": "F 1", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9953675270080566}, {"text": "F 1", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9899838864803314}]}, {"text": " Table 14  Results of feature analysis. The second column denotes the proportion of manually annotated  examples for which the feature value is non-zero. A detailed explanation of the other columns is  provided in the body of the article.", "labels": [], "entities": [{"text": "feature analysis", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8640592992305756}]}]}