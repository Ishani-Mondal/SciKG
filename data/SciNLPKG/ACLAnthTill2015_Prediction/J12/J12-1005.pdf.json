{"title": [{"text": "Semi-Supervised Semantic Role Labeling via Structural Alignment", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6219711204369863}, {"text": "Structural Alignment", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.8701703548431396}]}], "abstractContent": [{"text": "Large-scale annotated corpora area prerequisite to developing high-performance semantic role labeling systems.", "labels": [], "entities": []}, {"text": "Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative.", "labels": [], "entities": []}, {"text": "Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7205314834912618}]}, {"text": "The key idea of our approach is to find novel instances for classifier training based on their similarity to manually labeled seed instances.", "labels": [], "entities": []}, {"text": "The underlying assumption is that sentences that are similar in their lexical material and syntactic structure are likely to share a frame semantic analysis.", "labels": [], "entities": []}, {"text": "We formalize the detection of similar sentences and the projection of role annotations as a graph alignment problem, which we solve exactly using integer linear programming.", "labels": [], "entities": [{"text": "graph alignment", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.7977935075759888}]}, {"text": "Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7504430810610453}]}], "introductionContent": [{"text": "Recent years have seen growing interest in the shallow semantic analysis of natural language text.", "labels": [], "entities": [{"text": "shallow semantic analysis of natural language text", "start_pos": 47, "end_pos": 97, "type": "TASK", "confidence": 0.8176808442388263}]}, {"text": "The term is most commonly used to refer to the automatic identification and labeling of the semantic roles conveyed by sentential constituents.", "labels": [], "entities": [{"text": "automatic identification and labeling of the semantic roles conveyed by sentential constituents", "start_pos": 47, "end_pos": 142, "type": "TASK", "confidence": 0.6567722236116728}]}, {"text": "Semantic roles themselves have a long-standing tradition in linguistic theory, dating back to the seminal work of.", "labels": [], "entities": [{"text": "linguistic theory", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7045819461345673}]}, {"text": "They describe the relations that hold between a predicate and its arguments, abstracting over surface syntactic configurations.", "labels": [], "entities": []}, {"text": "Consider the following example sentences: (1) a.", "labels": [], "entities": []}, {"text": "The burglar broke the window with a hammer. b. A hammer broke the window. c. The window broke.", "labels": [], "entities": []}, {"text": "Here, the phrase the window occupies different syntactic positions-it is the object of break in sentences (1a) and (1b), and the subject in (1c)-and yet bears the same semantic role denoting the affected physical object of the breaking event.", "labels": [], "entities": []}, {"text": "Analogously, hammer is the instrument of break both when attested with a prepositional phrase in (1a) and as a subject in (1b).", "labels": [], "entities": []}, {"text": "The examples represent diathesis alternations 1 (Levin 1993), namely, regular variations in the syntactic expressions of semantic roles, and their computational treatment is one of the main challenges faced by automatic semantic role labelers.", "labels": [], "entities": []}, {"text": "Several theories of semantic roles have been proposed in the literature, differing primarily in the number and type of roles they postulate.", "labels": [], "entities": []}, {"text": "These range from small set of universal roles (e.g., Agentive, Instrumental, Dative) to individual roles for each predicate.", "labels": [], "entities": []}, {"text": "Frame semantic theory) occupies the middle ground by postulating situations (or frames) that can be evoked by different predicates.", "labels": [], "entities": [{"text": "Frame semantic theory", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8660176197687784}]}, {"text": "In this case, roles are not specific to predicates but to frames, and therefore ought to generalize among semantically related predicates.", "labels": [], "entities": []}, {"text": "As an example, consider the sentences in Example Here, the verbs punch, crush, slap, and injure are all frame evoking elements (FEEs), that is, they evoke the CAUSE HARM frame, which in turn exhibits the frame-specific (or \"core\") roles Agent, Victim, Body part, and Cause, and the more general (\"non-core\") roles Degree, Reason, and Means.", "labels": [], "entities": [{"text": "FEEs", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9829337000846863}, {"text": "HARM", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.6130755543708801}, {"text": "Degree", "start_pos": 314, "end_pos": 320, "type": "METRIC", "confidence": 0.9444296956062317}]}, {"text": "A frame maybe evoked by different lexical items, which may in turn inhabit several frames.", "labels": [], "entities": []}, {"text": "For instance, the verb crush may also evoke the GRINDING frame, and slap the IMPACT frame.", "labels": [], "entities": [{"text": "GRINDING", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9760352373123169}]}, {"text": "The creation of resources that document the realization of semantic roles in example sentences such as FrameNet and has greatly facilitated the development of learning algorithms capable of automatically analyzing the role semantic structure of input sentences.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.8737488985061646}]}, {"text": "Moreover, the shallow semantic analysis produced by existing systems has been shown to benefit a wide spectrum of applications ranging from information extraction () and question answering, to machine translation (Wu and Fung 2009) and summarization ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7864606976509094}, {"text": "question answering", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.8912817537784576}, {"text": "machine translation", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.8125331997871399}, {"text": "summarization", "start_pos": 236, "end_pos": 249, "type": "TASK", "confidence": 0.9909287095069885}]}, {"text": "Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.7827311108509699}]}, {"text": "Supervised methods deliver reasonably good performance 2 (F 1 measures in the low 80s on standard test collections for English); however, the reliance on labeled training data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres.", "labels": [], "entities": [{"text": "F 1", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9888474345207214}, {"text": "semantic role labeling", "start_pos": 293, "end_pos": 315, "type": "TASK", "confidence": 0.6518559555212656}]}, {"text": "And although nowadays corpora with semantic role annotations exist in F \u00a8 urstenau and Lapata Semi-Supervised SRL via Structural Alignment other languages (e.g., German, Spanish, Catalan, Chinese, Korean), they tend to be smaller than their English equivalents and of limited value for modeling purposes.", "labels": [], "entities": [{"text": "F \u00a8 urstenau", "start_pos": 70, "end_pos": 82, "type": "METRIC", "confidence": 0.5011133849620819}]}, {"text": "It is also important to note that the performance of supervised systems degrades considerably (by 10%) on out-of-domain data even within English, a language for which two major annotated corpora are available.", "labels": [], "entities": []}, {"text": "And this is without taking unseen events into account, which unavoidably affect coverage.", "labels": [], "entities": []}, {"text": "The latter is especially an issue for FrameNet (version 1.3) which is still underdevelopment, despite being a relatively large resource-it contains almost 140,000 annotated sentences fora total of 502 frames, which are evoked by over 5,000 different lexical units.", "labels": [], "entities": []}, {"text": "Coverage issues involve not only lexical units but also missing frames and incompletely exemplified semantic roles.", "labels": [], "entities": []}, {"text": "In this article, we attempt to alleviate some of these problems by using semisupervised methods that make use of a small number of manually labeled training instances and a large number of unlabeled instances.", "labels": [], "entities": []}, {"text": "Whereas manually labeled data are expensive to create, unlabeled data are often readily available in large quantities.", "labels": [], "entities": []}, {"text": "Our approach aims to improve the performance of a supervised SRL system by enlarging its training set with automatically inferred annotations of unlabeled sentences.", "labels": [], "entities": [{"text": "SRL", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9497864842414856}]}, {"text": "The key idea of our approach is to find novel instances for classifier training based on their similarity to manually labeled seed instances.", "labels": [], "entities": []}, {"text": "The underlying assumption is that sentences that are similar in their lexical material and syntactic structure are likely to share a frame semantic analysis.", "labels": [], "entities": []}, {"text": "The annotation of an unlabeled sentence can therefore be inferred from a sufficiently similar labeled sentence.", "labels": [], "entities": []}, {"text": "For example, given the labeled sentence (3) and the unlabeled sentence (4), we wish to recognize that they are lexically and structurally similar; and infer that thumped also evokes the IMPACT frame, whereas the rest of his body and against the front of the cage represent the Impactor and Impactee roles, respectively.", "labels": [], "entities": []}, {"text": "The rest of his body thumped against the front of the cage.", "labels": [], "entities": []}, {"text": "We formalize the detection of similar sentences and the projection of role annotations in graph-theoretic terms by conceptualizing the similarity between labeled and unlabeled sentences as a graph alignment problem.", "labels": [], "entities": []}, {"text": "Specifically, we represent sentences as dependency graphs and seek an optimal (structural) alignment between them.", "labels": [], "entities": []}, {"text": "Given this alignment, we then project annotations from the labeled onto the unlabeled sentence.", "labels": [], "entities": []}, {"text": "Graphs are scored using a function based on lexical and syntactic similarity which allows us to identify alternations like those presented in Example (1) and more generally to obtain training instances with novel structure and lexical material.", "labels": [], "entities": []}, {"text": "We obtain the best scoring graph alignment using integer linear programming, a general-purpose exact optimization framework.", "labels": [], "entities": [{"text": "scoring graph alignment", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.6786630551020304}]}, {"text": "Importantly, our approach is not tied to a particular SRL system.", "labels": [], "entities": [{"text": "SRL", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8390944600105286}]}, {"text": "We obtain additional annotations irrespective of the architecture or implementation details of the supervised role labeler that uses them.", "labels": [], "entities": []}, {"text": "This renders our approach portable across learning paradigms, languages, and domains.", "labels": [], "entities": []}, {"text": "After discussing related work (Section 2), we describe the details of our semisupervised method (Section 3) and then move onto evaluate its performance (Section 4).", "labels": [], "entities": []}, {"text": "We conduct two sets of experiments using data from the FrameNet corpus: In Section 5, we apply our method to increase the training data for known predicates, that is, words for which some seed annotations already exist.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9269095063209534}]}, {"text": "In Section 6, we focus on the complementary task of creating training instances for unknown predicates, that is, words that do not occur in the FrameNet corpus at all.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 144, "end_pos": 159, "type": "DATASET", "confidence": 0.9030913412570953}]}, {"text": "Section 7 concludes the article.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the data and supervised semantic role labeler used in our experiments and explain how the free parameters of our method were instantiated.", "labels": [], "entities": []}, {"text": "We then move onto present two experiments that evaluate our semi-supervised method.", "labels": [], "entities": []}, {"text": "In this section, we describe a first set of experiments with the aim of automatically creating novel annotation instances for SRL training.", "labels": [], "entities": [{"text": "SRL training", "start_pos": 126, "end_pos": 138, "type": "TASK", "confidence": 0.959403395652771}]}, {"text": "We assume that a small number of manually labeled instances are available and apply our method to obtain more annotations for the FEEs attested in the seed corpus.", "labels": [], "entities": [{"text": "FEEs attested in the seed corpus", "start_pos": 130, "end_pos": 162, "type": "DATASET", "confidence": 0.7107155919075012}]}, {"text": "The FEE of the labeled sentence and the target verb of the unlabeled sentence are presumed identical.", "labels": [], "entities": [{"text": "FEE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9989522695541382}]}, {"text": "However, we waive this restriction in Experiment 2, where we acquire annotations for unknown FEEs, that is, predicates for which no manual annotations are available.", "labels": [], "entities": [{"text": "FEEs", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.5293112397193909}]}, {"text": "In this section, we describe a second set of experiments, where our method is applied to acquire novel instances for unknown FEEs, that is, predicates for which no manually labeled instances are available.", "labels": [], "entities": []}, {"text": "Unknown predicates present a major obstacle to existing supervised SRL systems.", "labels": [], "entities": [{"text": "SRL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.979005753993988}]}, {"text": "Labeling performance on such predicates is typically poor due to the lack of specific training material for learning (Baker, Ellsworth, and Erk 2007).", "labels": [], "entities": []}, {"text": "In this appendix, we give complete results for the expansion experiments discussed in Sections 5 and 6.", "labels": [], "entities": []}, {"text": "Asterisks in the tables indicate levels of significance.", "labels": [], "entities": []}, {"text": "For simplicity, we only present two levels of significance, p < 0.05 with a single asterisk (*) and p < 0.001 with two asterisks (**).", "labels": [], "entities": [{"text": "significance", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.9558162093162537}]}, {"text": "Significance tests for exact match and frame labeling accuracy were performed using McNemar's test.", "labels": [], "entities": [{"text": "exact", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.955564022064209}, {"text": "match", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.5076797604560852}, {"text": "frame labeling", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7046578675508499}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9528103470802307}, {"text": "McNemar's test", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.8887323339780172}]}, {"text": "We used stratified shuffling to examine whether differences in labeled F 1 were significant.", "labels": [], "entities": []}, {"text": "12 Experiments on Known Predicates.", "labels": [], "entities": []}, {"text": "The following table shows the performance of expanded classifiers when automatically generated nearest neighbors (NN) are added to seed corpora containing manually labeled sentences per verb.", "labels": [], "entities": []}, {"text": "We report precision (Prec), recall (Rec), their harmonic mean (F 1 ), and exact match (ExMatch; the proportion of sentences that receive entirely correct frame and role annotations).", "labels": [], "entities": [{"text": "precision (Prec)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8053940683603287}, {"text": "recall (Rec)", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9323422610759735}, {"text": "harmonic mean (F 1 )", "start_pos": 48, "end_pos": 68, "type": "METRIC", "confidence": 0.7907967468102773}, {"text": "exact", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9991028308868408}, {"text": "match", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.586574912071228}]}, {"text": "Some of these results were visualized in.", "labels": [], "entities": []}, {"text": "In the following, we show the performance of unexpanded and expanded classifiers when selecting among frame candidates generated by the WordNet-based method.", "labels": [], "entities": []}, {"text": "We report frame labeling accuracy, role labeling performance, and exact match scores.", "labels": [], "entities": [{"text": "frame labeling", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.5740957409143448}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9595862627029419}, {"text": "role labeling", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8179463744163513}, {"text": "exact match scores", "start_pos": 66, "end_pos": 84, "type": "METRIC", "confidence": 0.9235747257868449}]}, {"text": "Asterisks indicate that the expanded classifier is significantly better than an unexpanded classifier choosing among the same number of candidates.", "labels": [], "entities": []}, {"text": "For frame labeling accuracy, we additionally provide the results of the random baseline and an upper bound, which always chooses the correct frame if it is among the candidates.", "labels": [], "entities": [{"text": "frame labeling", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7132723033428192}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9399735927581787}]}, {"text": "Some of these results were shown in.", "labels": [], "entities": []}, {"text": "For two candidates, the expanded classifier also performs significantly better than the best unexpanded classifier (i.e., the one given only one candidate) in terms of frame labeling accuracy, F 1 , and exact match (p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9339510202407837}, {"text": "F 1", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9944011270999908}, {"text": "exact match", "start_pos": 203, "end_pos": 214, "type": "METRIC", "confidence": 0.8302038013935089}]}, {"text": "In terms of exact match, it also performs significantly better for three candidates (p < 0.05), four candidates (p < 0.05), and five candidates (p < 0.001).", "labels": [], "entities": [{"text": "exact match", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.8614139258861542}]}, {"text": "The following graphs show the performance of the unexpanded and expanded classifiers when frame candidates are selected by the vectorbased method.", "labels": [], "entities": []}, {"text": "The expanded classifiers significantly outperform the unexpanded ones in terms of frame labeling accuracy and role labeling F 1 for", "labels": [], "entities": [{"text": "frame labeling", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.590150386095047}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9329153895378113}, {"text": "role labeling", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.7831307649612427}]}], "tableCaptions": []}