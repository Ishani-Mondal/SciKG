{"title": [{"text": "Language Models for Machine Translation: Original vs. Translated Texts", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7164265811443329}]}], "abstractContent": [{"text": "We investigate the differences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language.", "labels": [], "entities": []}, {"text": "Corroborating established observations of Translation Studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better.", "labels": [], "entities": [{"text": "Translation Studies", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9472154378890991}]}, {"text": "Furthermore, translated texts yield better language models for statistical machine translation than original texts.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.7113769054412842}]}], "introductionContent": [{"text": "Statistical machine translation (MT) uses large target language models (LMs) to improve the fluency of generated texts, and it is commonly assumed that for constructing language models, \"more data is better data\".", "labels": [], "entities": [{"text": "Statistical machine translation (MT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8262436340252558}]}, {"text": "Not all data, however, are created the same.", "labels": [], "entities": []}, {"text": "In this work we explore the differences between language models compiled from texts originally written in the target language (O) and language models compiled from translated texts (T).", "labels": [], "entities": []}, {"text": "This work is motivated by much research in Translation Studies that suggests that original texts are significantly different from translated ones in various aspects.", "labels": [], "entities": [{"text": "Translation Studies", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.9776549637317657}]}, {"text": "Recently, corpus-based computational analysis corroborated this observation, and apply it to statistical machine translation, showing that for an English-to-French MT system, a translation model trained on an English-translated-to-French parallel corpus is better than one trained on French-translated-to-English texts.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.6309798061847687}, {"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.8763424158096313}]}, {"text": "The main research question we investigate here is whether a language model compiled from translated texts may similarly improve the results of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7127158641815186}]}, {"text": "We test this hypothesis on several translation tasks, including translation from several languages to English, and two additional tasks where the target language is not English.", "labels": [], "entities": []}, {"text": "For each language pair we build two language models from two types of corpora: texts originally written in the target language, and human translations from the source language into the target language.", "labels": [], "entities": []}, {"text": "We show that for each language pair, the latter language model better fits a set of reference translations in terms of perplexity.", "labels": [], "entities": []}, {"text": "We also demonstrate that the differences between the two LMs are not biased by content, but rather reflect differences on abstract linguistic features.", "labels": [], "entities": []}, {"text": "Research in Translation Studies holds a dual view on translationese, the sublanguage of translated texts.", "labels": [], "entities": [{"text": "Translation Studies", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9493556618690491}, {"text": "translationese", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.9612088203430176}]}, {"text": "On the one hand, there is a claim for so-called translation universals, traits of translationese which occur in any translated text irrespective of the source language.", "labels": [], "entities": []}, {"text": "Others hold, on the other hand, that each source language \"spills over\" to the target text, and therefore creates a sub-translationese, the result of a pair-specific encounter between two specific languages.", "labels": [], "entities": []}, {"text": "If both these claims are true then language models based on translations from the source language should best fit target language reference sentences, and language models based on translations from other source languages should fit reference sentences to a lesser extent yet outperform originally written texts.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we compile additional English LMs, this time using texts translated to English from languages other than the source.", "labels": [], "entities": []}, {"text": "Again, we use perplexity to assess the fit of these LMs to reference sets of translated-to-English sentences.", "labels": [], "entities": []}, {"text": "We show that these LMs depend on the source language and differ from each other.", "labels": [], "entities": []}, {"text": "Whereas they outperform O-based LMs, LMs compiled from texts that were translated from the source language still fit the reference set best.", "labels": [], "entities": []}, {"text": "Finally, we train phrase-based MT systems (Koehn, Och, and Marcu 2003) for each language pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.8819273114204407}]}, {"text": "We use four types of LMs: original; translated from the source language; translated from other languages; and a mixture of translations from several languages.", "labels": [], "entities": []}, {"text": "We show that the translated-from-source-language LMs provide a significant improvement in the quality of the translation output overall other LMs, and that the mixture LMs always outperform the original LMs.", "labels": [], "entities": []}, {"text": "This improvement persists even when the original LMs are up to ten times larger than the translated ones.", "labels": [], "entities": []}, {"text": "In other words, one has to collect ten times more original material in order to reach the same quality as is provided with translated material.", "labels": [], "entities": []}, {"text": "It is important to emphasize that translated texts abound: in fact, show (quantitatively!) that the rate of translations into a language is inversely proportional to the number of books published in that language: So whereas in English only around 2% of texts published are translations, in languages such as Albanian, Arabic, Danish, Finnish, or Hebrew translated texts constitute between 20% and 25% of the total publications.", "labels": [], "entities": []}, {"text": "Furthermore, such data can be automatically identified (see Section 2).", "labels": [], "entities": []}, {"text": "The practical impact of our work on MT is therefore potentially dramatic.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.991363525390625}]}, {"text": "The main contributions of this work are thus a computational corroboration of the following hypotheses: 1.", "labels": [], "entities": []}, {"text": "Original and translated texts exhibit significant, measurable differences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We detail in this section the experiments performed to test the three hypotheses: that translated texts can be distinguished from original ones, and provide better language models for other translated texts; that texts translated from other languages than the source are still better predictors of translations than original texts (Section 4.1); and that these differences are important for SMT (Section 4.2).", "labels": [], "entities": [{"text": "SMT", "start_pos": 391, "end_pos": 394, "type": "TASK", "confidence": 0.994010329246521}]}, {"text": "First, we use two alternative automatic evaluation metrics, METEOR 6 (Denkowski and Lavie 2011) and TER (), to assess the quality of the MT systems described in Section 4.2.", "labels": [], "entities": [{"text": "METEOR 6", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9532784521579742}, {"text": "TER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9990192651748657}, {"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9798259139060974}]}, {"text": "We focus on four translation tasks: From German, French, Italian, and Dutch to English.", "labels": [], "entities": []}, {"text": "7 For each task we report the performance of two MT systems: One that uses a language model compiled from original-English texts, and one that uses a language model trained on texts translated from the source language.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9814183115959167}]}, {"text": "The results, which are reported in, fully support our previous findings (recall that lower TER is better): MT systems that use T-based LMs significantly outperform systems that use O-based LMs.", "labels": [], "entities": [{"text": "TER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9987735152244568}, {"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.9841116666793823}]}, {"text": "To further establish the qualitative difference between translations produced with an English-original language model and translations produced with a LM created from French-translated-to-English texts, we conducted a human evaluation campaign, using Amazon's Mechanical Turk as an inexpensive, reliable, and accessible pool of annotators.", "labels": [], "entities": []}, {"text": "We created a small evaluation corpus of 100 sentences, selected randomly among all (Europarl) reference sentences whose length is between 15 and 25 words.", "labels": [], "entities": [{"text": "Europarl) reference sentences", "start_pos": 84, "end_pos": 113, "type": "DATASET", "confidence": 0.9167780429124832}]}, {"text": "Each instance of the evaluation task includes two English sentences, obtained from the two MT systems that use the O-EN and the T-FR language models, respectively.", "labels": [], "entities": []}, {"text": "Annotators are presented with these two translations, and are requested to determine which one is better.", "labels": [], "entities": []}, {"text": "The definition given to annotators is: \"A better translation is more fluent, reflecting better use of English.\"", "labels": [], "entities": []}, {"text": "Observe that because the only variable that distinguishes between the two MT systems is the different language model, we only have to evaluate the fluency of the target sentence, not its faithfulness to the source.", "labels": [], "entities": [{"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9681983590126038}]}, {"text": "Consequently, we do not present the source or the reference translation to the annotators.", "labels": [], "entities": []}, {"text": "All annotators were located in the United States (and, therefore, are presumably English speakers).", "labels": [], "entities": []}, {"text": "As a control set, we added a set of 10 sentences produced with the O-based LM, which were paired with their (manually created) reference translations, and 10 sentences produced with the T-based LM, again paired with their references.", "labels": [], "entities": []}, {"text": "Each of the 120 evaluation instances was assigned to 10 different Mechanical Turk annotators.", "labels": [], "entities": []}, {"text": "We report two evaluation metrics: score and majority.", "labels": [], "entities": [{"text": "score", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9856249690055847}]}, {"text": "The score of a given sentence pair \ud97b\udf59e 1 , e 2 \ud97b\udf59 is i/j, where i is the number of annotators who preferred e 1 over e 2 , and j = 10 \u2212 i is the number of annotators preferring e 2 . For such a sentence pair, the majority is e 1 if i > j, e 2 if i < j, and undefined otherwise.", "labels": [], "entities": []}, {"text": "The average score of the 10 sentences in the O-vs.-reference control set is 22/78, and the majority is the reference translation in all but one of the instances.", "labels": [], "entities": []}, {"text": "As for the T-vs.-reference control set, the average score is 18/82, and the majority is the reference in all of the instances.", "labels": [], "entities": []}, {"text": "This indicates that the annotators are reliable, and also that it is unrealistic to expect a clear-cut distinction even between human translations and machine-generated output.", "labels": [], "entities": []}, {"text": "As for the actual evaluation set, the average score of O-EN vs. T-FR is 38/62, and the majority is T-FR in 75% of the cases, O-EN in only 25% of the sentence pairs.", "labels": [], "entities": [{"text": "T-FR", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9414515495300293}]}, {"text": "We take these results as a very strong indication that English sentences generated by an MT system whose language model is compiled from translated texts are perceived by humans as more fluent than ones generated by a system built with an O-based language model.", "labels": [], "entities": []}, {"text": "Not only is the improvement reflected in significantly higher Bleu (and METEOR, TER) scores, but it is undoubtedly also perceived as such by human annotators.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9901278614997864}, {"text": "METEOR", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9071956872940063}, {"text": "TER) scores", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9233778119087219}]}], "tableCaptions": [{"text": " Table 1  Europarl English-target corpus statistics, translation from Lang. to English.", "labels": [], "entities": [{"text": "Europarl English-target corpus statistics", "start_pos": 10, "end_pos": 51, "type": "DATASET", "confidence": 0.9698097705841064}, {"text": "Lang.", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9405508637428284}]}, {"text": " Table 2  Europarl corpus statistics, translation from Lang. to German and French.", "labels": [], "entities": [{"text": "Europarl corpus statistics", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.9843957225481669}]}, {"text": " Table 3  Hansard corpus statistics.", "labels": [], "entities": [{"text": "Hansard corpus statistics", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9645632902781168}]}, {"text": " Table 4  Gigaword corpus statistics.", "labels": [], "entities": [{"text": "Gigaword corpus statistics", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.7435160974661509}]}, {"text": " Table 5  Hebrew-to-English corpus statistics.", "labels": [], "entities": []}, {"text": " Table 6  Parallel corpora used for SMT training.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9546230733394623}]}, {"text": " Table 8  Fitness of various LMs to the reference set.", "labels": [], "entities": []}, {"text": " Table 9  Fitness of O-vs. T-based LMs to the reference set (FR-EN), reflecting different abstraction levels.", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9749939441680908}]}, {"text": " Table 10  Fitness of O-vs. T-based LMs to the reference set (HE-EN).", "labels": [], "entities": [{"text": "HE-EN", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.8504936695098877}]}, {"text": " Table 11  Fitness of O-vs. T-based LMs to the reference set (HE-EN), reflecting different abstraction levels.", "labels": [], "entities": []}, {"text": " Table 12  Fitness of O-vs. T-based LMs to the reference set (EN-DE and EN-FR).", "labels": [], "entities": []}, {"text": " Table 13  The effect of LM training corpus size on the fitness of LMs to the reference sets.", "labels": [], "entities": []}, {"text": " Table 14  Machine translation with various LMs; English target language.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7657157778739929}]}, {"text": " Table 15  Machine translation with various LMs; non-English target language.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7802844047546387}]}, {"text": " Table 16  The effect of LM size on MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9907570481300354}]}, {"text": " Table 17  Various combinations of original and translated texts and their effect on perplexity (PPL) and  translation quality (Bleu).", "labels": [], "entities": [{"text": "translation quality (Bleu)", "start_pos": 107, "end_pos": 133, "type": "METRIC", "confidence": 0.7141014933586121}]}, {"text": " Table 18  MT system performance as measured by METEOR and TER.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.944974422454834}, {"text": "METEOR", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.890843391418457}, {"text": "TER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9881666302680969}]}]}