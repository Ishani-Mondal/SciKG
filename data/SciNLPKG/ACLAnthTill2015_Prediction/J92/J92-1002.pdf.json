{"title": [{"text": "An Estimate of an Upper Bound for the Entropy of English", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printed English, obtained by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text.", "labels": [], "entities": []}, {"text": "We suggest the well-known and widely available Brown Corpus of printed English as a standard against which to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds.", "labels": [], "entities": [{"text": "Brown Corpus of printed English", "start_pos": 47, "end_pos": 78, "type": "DATASET", "confidence": 0.9798867344856262}, {"text": "language modeling", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7092780321836472}]}], "introductionContent": [{"text": "We present an estimate of an upper bound for the entropy of characters in printed English.", "labels": [], "entities": []}, {"text": "The estimate is the cross-entropy of the 5.96 million character Brown Corpus ( as measured by a word trigram language model that we constructed from 583 million words of training text.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9260478317737579}]}, {"text": "We obtain an upper bound of 1.75 bits per character.", "labels": [], "entities": []}, {"text": "Since Shannon's 1951 paper, there have been a number of estimates of the entropy of English.", "labels": [], "entities": []}, {"text": "Our approach differs from previous work in that 1.", "labels": [], "entities": []}, {"text": "We use a much larger sample of English text; previous estimates were based on samples of at most a few hundred letters.", "labels": [], "entities": []}, {"text": "2. We use a language model to approximate the probabilities of character strings; previous estimates employed human subjects from whom probabilities were elicited through various clever experiments.", "labels": [], "entities": []}, {"text": "3. We predict all printable ASCII characters.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2  Tokens in the test sample but not in the 293,181-token vocabulary.", "labels": [], "entities": []}, {"text": " Table 3  Component contributions to the cross-entropy.", "labels": [], "entities": []}]}