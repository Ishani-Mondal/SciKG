{"title": [{"text": "Class-Based n-gram Models of Natural Language", "labels": [], "entities": []}], "abstractContent": [{"text": "We address the problem of predicting a word from previous words in a sample of text.", "labels": [], "entities": [{"text": "predicting a word from previous words in a sample of text", "start_pos": 26, "end_pos": 83, "type": "TASK", "confidence": 0.8264057311144742}]}, {"text": "In particular, we discuss n-gram models based on classes of words.", "labels": [], "entities": []}, {"text": "We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.", "labels": [], "entities": []}, {"text": "We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina number of natural language processing tasks, we face the problem of recovering a string of English words after it has been garbled by passage through a noisy channel.", "labels": [], "entities": []}, {"text": "To tackle this problem successfully, we must be able to estimate the probability with which any particular string of English words will be presented as input to the noisy channel.", "labels": [], "entities": []}, {"text": "In this paper, we discuss a method for making such estimates.", "labels": [], "entities": []}, {"text": "We also discuss the related topic of assigning words to classes according to statistical behavior in a large body of text.", "labels": [], "entities": []}, {"text": "In the next section, we review the concept of a language model and give a definition of n-gram models.", "labels": [], "entities": []}, {"text": "In Section 3, we look at the subset of n-gram models in which the words are divided into classes.", "labels": [], "entities": []}, {"text": "We show that for n = 2 the maximum likelihood assignment of words to classes is equivalent to the assignment for which the average mutual information of adjacent classes is greatest.", "labels": [], "entities": []}, {"text": "Finding an optimal assignment of words to classes is computationally hard, but we describe two algorithms for finding a suboptimal assignment.", "labels": [], "entities": []}, {"text": "In Section 4, we apply mutual information to two other forms of word clustering.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7255510538816452}]}, {"text": "First, we use it to find pairs of words that function together as a single lexical entity.", "labels": [], "entities": []}, {"text": "Then, by examining the probability that two words will appear within a reasonable distance of one another, we use it to find classes that have some loose semantic coherence.", "labels": [], "entities": []}, {"text": "In describing our work, we draw freely on terminology and notation from the mathematical theory of communication.", "labels": [], "entities": []}, {"text": "The reader who is unfamiliar with this field or who has allowed his or her facility with some of its concepts to fall into disrepair may profit from a brief perusal of and.", "labels": [], "entities": []}, {"text": "In the first of these, the reader should focus on conditional probabilities and on Markov chains; in the second, on entropy and mutual information.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}