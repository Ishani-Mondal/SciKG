{"title": [{"text": "Generalized LR Parsing Masaru Tomita (editor)", "labels": [], "entities": [{"text": "Generalized LR Parsing Masaru Tomita", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7888932466506958}]}], "abstractContent": [{"text": "Tomita's algorithm, also called the Generalized LR (GLR) parser, is a method for natural language parsing that extends standard parsing techniques for LR(k) grammars to cases of nondeterminism (see Tomita 1986).", "labels": [], "entities": [{"text": "Generalized LR (GLR) parser", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.5437412361303965}, {"text": "natural language parsing", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.6955227653185526}]}, {"text": "This algorithm has became quite popular within the computational linguistics community.", "labels": [], "entities": []}, {"text": "At the first International Workshop on Parsing Technologies, held in Pittsburgh in 1989, several papers were presented reporting experimental work and theoretical results based upon Tomita's algorithm.", "labels": [], "entities": []}, {"text": "These papers have been revised and collected in the book Generalized LR Parsing for the purpose of providing a reference for researchers having some interest in this algorithm specifically.", "labels": [], "entities": [{"text": "Generalized LR Parsing", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8146071036656698}]}, {"text": "I now discuss the contents of each contribution and conclude with some general remarks.", "labels": [], "entities": []}, {"text": "The opening chapter by Tomita and Ng introduces the Graph-Structured Stack (GSS), which is the basic idea in Tomita's algorithm for the simulation of nonde-terminism.", "labels": [], "entities": []}, {"text": "The authors then present a slightly modified version of the algorithm in (Tomita 1986) by discussing a sample computation on an ambiguous English sentence.", "labels": [], "entities": []}, {"text": "The next three chapters deal with performance issues, although from different perspectives.", "labels": [], "entities": []}, {"text": "The contribution by Shann entitled \"Experiments with GLR and chart parsing\" reports on some experiments of average time evaluation of different parsing strategies implemented within a chart framework and an implementation of Tomita's parsing algorithm.", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7167640626430511}, {"text": "Tomita's parsing", "start_pos": 225, "end_pos": 241, "type": "TASK", "confidence": 0.4399588604768117}]}, {"text": "The results can be summarized as follows: Tomita's algorithm performs best in cases of high ambiguity sentences, but shows the same performance as the left-corner method with top-down filtering in the case of a restricted domain corpus with low ambiguity sentences.", "labels": [], "entities": []}, {"text": "Comparisons are made on the basis of net running time and the number of edges constructed by the different methods, with a shift operation in Tomita's algorithm counting as an edge construction.", "labels": [], "entities": []}, {"text": "Both of those measures, however, allow comparisons only indirectly related to average-case time complexity.", "labels": [], "entities": []}, {"text": "The first measure depends on how different methods are implemented.", "labels": [], "entities": []}, {"text": "The latter is very weakly related to time, because an edge construction depends on a number of tests that can grow with the grammar and the input string length, and also since a reduction operation associated with a nonterminal shift cannot be considered an elementary operation as well.", "labels": [], "entities": []}, {"text": "The reader should therefore be careful in the interpretation of the results, since these kinds of experiments require a uniform framework to be carried out (see for example Billot and Lang 1989).", "labels": [], "entities": []}, {"text": "The reader should also be warned that the bottom-up and the bidirectional methods defined in this chapter involve polynomial computations of unbounded degree, but methods are found in the literature that perform much better, still using general context-free grammars.", "labels": [], "entities": []}, {"text": "The contribution by Johnson, entitled \"The computational complexity of GLR parsing,\" discusses two issues regarding the worst-case computational complexity of 377 Computational Linguistics Volume 18, Number 3 Tomita's algorithm.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.8956987857818604}]}, {"text": "Johnson points out that a crude representation of a packed parse forest can lead the algorithm to use an amount of space exponential in the grammar size.", "labels": [], "entities": []}, {"text": "This has to do with the fact that, fora given production, there are exponentially many different ways of matching immediate constituent boundaries within along enough string.", "labels": [], "entities": []}, {"text": "A similar point is made by Billot and Lang (1989), who give efficient solutions to this problem (based upon bilinear covers for the input grammar).", "labels": [], "entities": []}, {"text": "The second point in this chapter is a demonstration that there exist context-free grammars G whose collection of LR(0) items is exponentially larger than IGI.", "labels": [], "entities": []}, {"text": "Johnson exhibits input strings such that all these items are exploited by the algorithm, forcing exponential running time.", "labels": [], "entities": []}, {"text": "Although these worst cases may not be relevant for natural language processing applications, it is interesting to note that nondeterministic LR automata are found in the literature that use a set of states always proportional to IGI-for example the LL/LR automaton proposed by Leermakers (1989); see also Schabes (1991) for computational complexity issues.", "labels": [], "entities": []}, {"text": "The contribution by Kipps entitled \"GLR parsing in time O(n3)\" is also based on the above observation about the number of different matchings of immediate constituent boundaries.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9170906841754913}]}, {"text": "As a consequence, Kipps shows that in the worst case, an exponential amount of time with respect to the grammar length maybe required by reduction operations in the original version of Tomita's algorithm.", "labels": [], "entities": []}, {"text": "The author redesigns the reduce procedure of the algorithm using a tabular technique, thereby solving the problem in an efficient way.", "labels": [], "entities": []}, {"text": "As a minor note, in the discussion of Earley's algorithm, Kipps attributes an overall amount of time O(Iwl 2) to the predictor operation, w being the input string.", "labels": [], "entities": [{"text": "O(Iwl 2)", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.8003291547298431}]}, {"text": "However, there are known implementations of such an operation that take O(Iwl) time (see for example Graham, Harrison, and Ruzzo 1980).", "labels": [], "entities": [{"text": "O(Iwl) time", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.8669035911560059}]}, {"text": "The contribution entitled \"GLR parsing for c-grammars\" by Farshi focuses on a subclass of (noncyclic) context-free grammars with null productions that cannot be handled by the original version of Tomita's algorithm.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9517441093921661}]}, {"text": "This class has the following property: the number of null constituents preceding a symbol in a sentence of the language cannot in general be inferred using bounded lookahead.", "labels": [], "entities": []}, {"text": "Farshi discusses a modification of Tomita's algorithm that repairs this shortcoming.", "labels": [], "entities": []}, {"text": "The solution consists of extending the GSS to include cycles corresponding to possible parses of empty constituents.", "labels": [], "entities": [{"text": "GSS", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6025307178497314}]}, {"text": "The author also discusses how to handle cyclic grammars, thereby extending the coverage of Tomita's algorithm to general-form context-free grammars.", "labels": [], "entities": []}, {"text": "Note incidentally that, in discussing the importance of null productions, the author claims that a general context-free grammar can be exponentially more succinct than a context-free grammar in e-free form; this is not true, and the size of the two grammars are related by a linear equation (see Sippu and Soisalon-Soininen 1988).", "labels": [], "entities": []}, {"text": "The contribution by Tanaka and Numazaki, entitled \"Parallel GLR parsing based on logic programming,\" reports on an implementation of Tomita's algorithm within the framework of a concurrent logic programming language that is briefly introduced to the reader.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.7867369651794434}]}, {"text": "The basic idea is to represent each entry in the LR table as a sequential process, and to run in parallel the processes corresponding to multiple actions within the same entry.", "labels": [], "entities": []}, {"text": "Apart from the fact that the English of this chapter is rather clumsy, I find the presentation unconvincing for the following reasons: first, important details in the implementation of nondeterminism are missing.", "labels": [], "entities": []}, {"text": "The reader is told that the operation of process splitting due to action conflicts involves a stack-copying operation and the GSS is replaced by a tree-structured stack, but the authors do not discuss the computational consequences of this.", "labels": [], "entities": [{"text": "process splitting", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7901144325733185}]}, {"text": "Furthermore, no comparison at all is offered to the reader with the existing literature on parallel context-free grammar parsing.", "labels": [], "entities": [{"text": "parallel context-free grammar parsing", "start_pos": 91, "end_pos": 128, "type": "TASK", "confidence": 0.6123103350400925}]}, {"text": "The only work cited in the bibliography that describes a parallel parsing method is never mentioned in the chapter.", "labels": [], "entities": [{"text": "parallel parsing", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.6004154980182648}]}, {"text": "378 Book Reviews The two following chapters deal with the issue of stochastic parsing.", "labels": [], "entities": [{"text": "stochastic parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8250886499881744}]}, {"text": "The contribution entitled \"GLR parsing with scoring\" by Su, Wang, Su, and Chang, emphasizes the importance of scoring in best-solution-oriented parsing (in the context of a machine translation system, in this particular case) and focuses on syntactic scoring functions.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9272035956382751}]}, {"text": "The authors propose a general definition for the score of a derivation and adapt it to Tomita's algorithm by means of a decomposition in which each term corresponds to a transition between two shift actions.", "labels": [], "entities": []}, {"text": "They also present a general truncation algorithm and provide an interesting discussion of its expected behavior as well as experimental results.", "labels": [], "entities": []}, {"text": "One is left wondering how a GSS can be used in this framework: the authors do not deal with the problem of combining equal states having different scores in order to achieve further reduction of the search space.", "labels": [], "entities": []}, {"text": "The contribution by Wright and Wrigley, entitled \"GLR parsing with probability,\" presents a theory for the construction of different kinds of stochastic LR tables from a stochastic context-free grammar.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9279490411281586}]}, {"text": "This allows the computation at parsing time of prob-abilistic distributions for next words, given the prefix of the input sentence analyzed so far.", "labels": [], "entities": []}, {"text": "On the basis of the proposed framework, the authors discuss an application in uncertain input parsing.", "labels": [], "entities": [{"text": "uncertain input parsing", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.6036312878131866}]}, {"text": "This chapter assumes considerable confidence with stochastic context-free grammars on the part of the reader.", "labels": [], "entities": []}, {"text": "As a technical note, I observe that the authors compute the probability pBB of all possible left-recursive derivations B ~ Bw, Ba nonterminal and w a terminal string, as Y~w Pr(B ~ Bw).", "labels": [], "entities": []}, {"text": "But events B ~ Bw are not mutually disjointed, and this summation is no longer a probability (such a quantity corresponds to quantity QL(B ~ B) studied in Jelinek and Lafferty 1991).", "labels": [], "entities": []}, {"text": "Probabilities PBB for every nonterminal B can be correctly computed by solving a system of linear equations.", "labels": [], "entities": [{"text": "PBB", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.609661877155304}]}, {"text": "The last three chapters discuss applications of Tomita's algorithm to cases of corrupted input.", "labels": [], "entities": []}, {"text": "The contribution by Malone and Felshin, entitled \"GLR parsing for erroneous input,\" describes a system developed for use by language learners.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9447270333766937}]}, {"text": "The system is based on Tomita's algorithm, and is able to parse in the presence of ill-formed input of various kinds.", "labels": [], "entities": []}, {"text": "Errors are grouped in different categories, and techniques to handle them are discussed along with the use of a scoring method.", "labels": [], "entities": []}, {"text": "Among other things, the authors propose a standard lattice representation for the input sentence in order to deal with competing hypotheses deriving from typographical errors.", "labels": [], "entities": []}, {"text": "This representation is well suited to the GSS, but any tabular parsing technique could have been used as well.", "labels": [], "entities": [{"text": "GSS", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.649002194404602}, {"text": "tabular parsing", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.6251152157783508}]}, {"text": "In fact, I find the relationship of this paper to the rest of the book to be rather marginal: none of the proposed techniques depends at all upon the fact that the system is based on a nondeterministic shift-reduce automaton, and very little is said about LR parsing.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 256, "end_pos": 266, "type": "TASK", "confidence": 0.8820740282535553}]}, {"text": "The contribution by Saito and Tomita in the next chapter, entitled \"GLR parsing for noisy input,\" describes an application of Tomita's algorithm to a problem of error-correcting parsing within an automatic speech understanding system.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9002152979373932}, {"text": "error-correcting parsing within an automatic speech understanding", "start_pos": 161, "end_pos": 226, "type": "TASK", "confidence": 0.6117193187986102}]}, {"text": "A speech recognition device that produces a noisy phoneme sequence is coupled with Tomita's algorithm in an attempt to recover and parse the original sentence.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.716906264424324}]}, {"text": "The authors present a running example and briefly discuss the adopted scoring method and the pruning strategy.", "labels": [], "entities": []}, {"text": "The reader is told that equal states having different scores are combined applying a Viterbi-like technique, but this important point is not discussed in any detail.", "labels": [], "entities": []}, {"text": "Also, the proposal that is advanced is not related to the relevant literature on error-correcting parsing.", "labels": [], "entities": [{"text": "error-correcting parsing", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.589890331029892}]}, {"text": "The closing chapter, by Kita, Kawabata, and Saito, is entitled \"GLR parsing in a hidden Markov model.\"", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.923614501953125}]}, {"text": "It discusses an interesting continuous speech-recognition system based on hidden Markov models (HMMs), which uses phone units and is driven 379 Computational Linguistics Volume 18, Number 3 by Tomita's algorithm.", "labels": [], "entities": []}, {"text": "The basic idea is to save each state reached in the parsing analysis along with a probability array obtained by the HMM probability calculation process (the row of the trellis corresponding to the final state of the HMM associated with the predicted phone).", "labels": [], "entities": []}, {"text": "In this way acoustic recognition can be resumed.", "labels": [], "entities": [{"text": "acoustic recognition", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7789562940597534}]}, {"text": "The algorithm performs a breadth-first search using a threshold.", "labels": [], "entities": []}, {"text": "Unfortunately, because of the nature of HMMs, it is no longer possible to directly apply the state-combining operation, which is one of the basic ideas in Tomita's algorithm.", "labels": [], "entities": []}, {"text": "As seen so far, this book presents a considerable range of applications in which Tomita's algorithm has been employed showing noteworthy performance.", "labels": [], "entities": []}, {"text": "Therefore the book succeeds in providing a convenient reference for the researcher who plans to use Tomita's algorithm in practical natural language systems.", "labels": [], "entities": []}, {"text": "At the same time, this book discusses important improvements to the original specification of the algorithm, and I especially recommend the implementation that is suggested by Kipps.", "labels": [], "entities": []}, {"text": "This said, I find that the overall picture of Tomita's algorithm that emerges through this book is not entirely satisfactory.", "labels": [], "entities": []}, {"text": "This is mainly due to the fact that, beyond reporting some original ideas, many contributions fail to offer an adequate comparison with well-known alternative methods, or to relate at all the proposal that is advanced with the standard literature (parallel parsing and error recovery contributions are the most evident cases).", "labels": [], "entities": [{"text": "parallel parsing and error recovery", "start_pos": 248, "end_pos": 283, "type": "TASK", "confidence": 0.6469142496585846}]}, {"text": "I find also that, as a result of putting together disparate works, an adequate discussion of some important issues that are often alluded to in the individual chapters is missing from this book.", "labels": [], "entities": []}, {"text": "One case is the issue of control.", "labels": [], "entities": []}, {"text": "The original specification of Tomita's algorithm employs a breadth-first strategy in the analysis of ambiguous input sentences.", "labels": [], "entities": []}, {"text": "Since the GSS can be manipulated in a nondestructive way, the question arises of how to weaken the control in order to have a more flexible method (compare for example with chart-parsing techniques and the use of the agenda data structure [Kay 1980]).", "labels": [], "entities": []}, {"text": "Although some of the contributions mention the use of a depth-first strategy, the problem is never discussed in any detail.", "labels": [], "entities": []}, {"text": "A second very important (and much debated) issue, alluded to throughout this book but never adequately discussed, is the one of average-case parsing efficiency.", "labels": [], "entities": [{"text": "average-case parsing", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.4971272945404053}]}, {"text": "I share the belief expressed by the editor that, in cases of natural language grammars used by \"practical\" systems, Tomita's algorithm performs better than methods using nonprecompiled grammars.", "labels": [], "entities": []}, {"text": "This might be true not only because of the precompilation of rule predictions, as pointed out by Kipps, but especially because of the achieved compression of the input grammar.", "labels": [], "entities": []}, {"text": "(However, I should mention that I do not know of any experimental comparison between Tomita's algorithm and very efficient parsing algorithms that only use a mild form of precompilation, as for example the method presented by Graham, Harrison, and Ruzzo [1980].)", "labels": [], "entities": []}, {"text": "Given that, which is the most successful way of precompiling an input grammar?", "labels": [], "entities": []}, {"text": "There is evidence that this question should be answered case by case.", "labels": [], "entities": []}, {"text": "For example, the precompilation adopted by the already mentioned LL/LR automaton repairs the worst cases studied by Johnson in this book, but there are many cases in which an LR table is favorable, resulting in a sublinear representation of the input grammar (see also Schabes 1991 for the proposal of a \"mixed\" precompilation technique).", "labels": [], "entities": []}, {"text": "Furthermore, if the productions of the input grammar can be favorably factorized, then the CNLR automaton proposed by Leermakers (1989) provides an even more succinct representation.", "labels": [], "entities": []}, {"text": "Since I did not find a satisfactory discussion of this important issue in this book, I refer the interested reader to Billot and Lang (1989) fora general presentation of the problem and some experimental results.", "labels": [], "entities": []}, {"text": "I should also add that this book has not been edited well: there are many typographical errors and, more astonishingly, unprocessed I~TEX commands are found in a couple of mathematical expressions.", "labels": [], "entities": [{"text": "TEX", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.5027725696563721}]}, {"text": "The purpose of the book aside, I conclude by 380 Book Reviews warning the reader that the use of GSS in the simulation of nondeterminism is only one particular approach to the problem of nondeterministic LR parsing.", "labels": [], "entities": [{"text": "380 Book Reviews", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.8839874664942423}, {"text": "LR parsing", "start_pos": 204, "end_pos": 214, "type": "TASK", "confidence": 0.7813036143779755}]}, {"text": "Once a precompilation of the input grammar is achieved, in the form of the transition map of a pushdown automaton or in the form of a context-free grammar, general dynamic programming or memoizing techniques have successfully been employed in the simulation of nondeterminism; see for instance Lang (1974) and Leermakers (1989, 1991).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}