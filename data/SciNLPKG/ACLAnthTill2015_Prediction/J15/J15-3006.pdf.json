{"title": [{"text": "Squibs Evaluation Methods for Statistically Dependent Text", "labels": [], "entities": [{"text": "Squibs Evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9477444589138031}, {"text": "Statistically Dependent Text", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8072899381319681}]}], "abstractContent": [{"text": "In recent years, many studies have been published on data collected from social media, especially microblogs such as Twitter.", "labels": [], "entities": []}, {"text": "However, rather few of these studies have considered evaluation methodologies that take into account the statistically dependent nature of such data, which breaks the theoretical conditions for using cross-validation.", "labels": [], "entities": []}, {"text": "Despite concerns raised in the past about using cross-validation for data of similar characteristics, such as time series, some of these studies evaluate their work using standard k-fold cross-validation.", "labels": [], "entities": []}, {"text": "Through experiments on Twitter data collected during a two-year period that includes disastrous events, we show that by ignoring the statistical dependence of the text messages published in social media, standard cross-validation can result in misleading conclusions in a machine learning task.", "labels": [], "entities": []}, {"text": "We explore alternative evaluation methods that explicitly deal with statistical dependence in text.", "labels": [], "entities": []}, {"text": "Our work also raises concerns for any other data for which similar conditions might hold.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the emergence of popular social media services such as Twitter and Facebook, many studies in the area of Natural Language Processing (NLP) have been published that analyze the text data from these services fora variety of applications, such as opinion mining, sentiment analysis, event detection, or crisis management).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 249, "end_pos": 263, "type": "TASK", "confidence": 0.8185959458351135}, {"text": "sentiment analysis", "start_pos": 265, "end_pos": 283, "type": "TASK", "confidence": 0.9496871531009674}, {"text": "event detection", "start_pos": 285, "end_pos": 300, "type": "TASK", "confidence": 0.7767836153507233}, {"text": "crisis management", "start_pos": 305, "end_pos": 322, "type": "TASK", "confidence": 0.7039084434509277}]}, {"text": "Many of these studies have primarily relied on building classification models for different learning tasks, such as text classification or Named Entity Recognition.", "labels": [], "entities": [{"text": "text classification", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8300054371356964}, {"text": "Named Entity Recognition", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.6892164746920267}]}, {"text": "The effectiveness of these models is often evaluated using cross-validation techniques.", "labels": [], "entities": []}, {"text": "Cross-validation, first introduced by, has been acclaimed as the most popular evaluation method for estimating prediction errors in regression and classification problems.", "labels": [], "entities": []}, {"text": "In that method, the data Dare randomly partitioned into k non-overlapping subsets (folds) D k of approximately equal size.", "labels": [], "entities": []}, {"text": "The validation step is repeated k times, using a different D v = D k as the validation data and D t = D \\ D k as the training data each time.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9802661538124084}]}, {"text": "The final evaluation is the average over the k validation steps.", "labels": [], "entities": []}, {"text": "Cross-validation is found to have a lower variance than a single hold-out set validation and thus it is commonly used on both moderate and large amounts of data without introducing efficiency concerns.", "labels": [], "entities": []}, {"text": "Compared with other choices of k, 10-fold cross-validation has been accepted as the most reliable method, which gives a highly accurate estimate of the generalization error of a given model fora variety of algorithms and applications.", "labels": [], "entities": []}, {"text": "Despite its wide applications, debates on the appropriateness of cross-validation have been raised in a number of areas, particularly in time series analysis and chemical engineering.", "labels": [], "entities": [{"text": "time series analysis", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.6618203421433767}]}, {"text": "A fundamental assumption of cross-validation is that the data need to be independent and identically distributed between folds (Arlot and.", "labels": [], "entities": []}, {"text": "Therefore, if the data points used for training and validation are not independent, cross-validation is no longer valid and would usually overestimate the validity of the model.", "labels": [], "entities": []}, {"text": "For time series forecasting, because the data are comprised of correlated observations and might be generated by a process that evolves overtime, the training set and the validation set are not independent if randomly chosen for cross-validation.", "labels": [], "entities": [{"text": "time series forecasting", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.5931873818238577}]}, {"text": "Researchers since the early 1990s have used modified variants of cross-validation to compensate for time dependence within times series (.", "labels": [], "entities": []}, {"text": "In the area of chemical engineering, the work of (Sheridan 2013) investigates the dependence in chemical data and observes that the existence of similar compounds or molecules across the data set leads to overoptimistic results using standard k-fold cross-validation.", "labels": [], "entities": []}, {"text": "We take such observations from the time series and chemical domains as a warning to investigate the data dependence in computational linguistics.", "labels": [], "entities": []}, {"text": "We argue that even when the data appear to be independently generated and when there is no reason to believe that temporal dependencies are present, unexpected statistical dependencies maybe induced through an incorrect application of cross-validation.", "labels": [], "entities": []}, {"text": "Once there is a chance of having similar or otherwise dependent data points, random split of the data without taking this factor into account would cause incorrect or at least unreliable evaluation, which may lead to invalid or at least unjustified conclusions.", "labels": [], "entities": []}, {"text": "Although similar concerns have been raised by a prior study () that cross-validation might not be suitable for measuring the accuracy of public opinion forecasting, there is alack of systematic analysis on how potential data dependency might invalidate cross-validation and what alternative evaluation methods exist.", "labels": [], "entities": [{"text": "accuracy of public opinion forecasting", "start_pos": 125, "end_pos": 163, "type": "TASK", "confidence": 0.6225856423377991}]}, {"text": "With the aim of gaining further insight into this issue, we perform a detailed empirical study based on text classification in Twitter, and show that inappropriate choice of cross-validation techniques could potentially lead to misleading conclusions.", "labels": [], "entities": [{"text": "text classification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7253601253032684}]}, {"text": "This concern could apply more generally to other data types of similar nature.", "labels": [], "entities": []}, {"text": "We also explore several evaluation methods, mostly borrowed from research on time series, that are better suited for statistically dependent data and that could be adapted by researchers working in the NLP area.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our tweet classification tasks, we setup experiments to compare five validation methods-standard 10-fold cross-validation, border-split cross-validation, neighborsplit validation, time-split validation, and time-border-split validation-for identifying tweets that are relevant to a disaster, and whether or not we can broadly identify the type of disaster.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8753738105297089}]}, {"text": "We evaluate classification effectiveness using the accuracy metric, which is the percentage of correctly classified tweets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9992874264717102}]}, {"text": "We used the following settings for our evaluation schemes: r k-fold cross-validation: We used k = 10 folds.", "labels": [], "entities": []}, {"text": "r Border-split cross-validation: We used k = 10 folds for border-split and a radius h = 20 days.", "labels": [], "entities": []}, {"text": "We assume that for most events, the social media activity on the topic dies after three weeks of their occurrence.", "labels": [], "entities": []}, {"text": "r Neighbor-split validation: We used cosine similarity to find a subset of the data that has the least number of neighbors in the data set.", "labels": [], "entities": []}, {"text": "We weighted hashtags double and disregarded mentions in calculating the similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9621748328208923}]}, {"text": "Neighborhood was decided using a minimum threshold of 0.25 on the resulting cosine similarity.", "labels": [], "entities": []}, {"text": "The size of the test data set was the same as in time-split (below) but the size of the training data set was 5,868.", "labels": [], "entities": [{"text": "training data set", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.7456573843955994}]}, {"text": "r Time-split validation: We chose the cut-off time t * so that 90% of the data was used as training (5,922 tweets) and 10% as validation (658 tweets).", "labels": [], "entities": []}, {"text": "r Time-border-split validation: We used the same t * and h as time-split and border-split.", "labels": [], "entities": []}, {"text": "This resulted in a training data set containing 87.1% of the data and a validation data set identical to the one in time-split at 10% of the data (658 tweets).", "labels": [], "entities": []}, {"text": "In removing the border, 2.9% of the data were discarded.", "labels": [], "entities": []}, {"text": "The size of the training set was 5,750 tweets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Comparison of different evaluation methods for tweet classification with different feature  combinations. Standard deviations are given in parentheses when applicable.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.808736264705658}]}]}