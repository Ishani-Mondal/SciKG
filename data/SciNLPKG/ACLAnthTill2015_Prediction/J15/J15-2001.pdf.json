{"title": [{"text": "The Operation Sequence Model -Combining N-Gram-Based and Phrase-Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation", "start_pos": 57, "end_pos": 101, "type": "TASK", "confidence": 0.5703161433339119}]}], "abstractContent": [{"text": "In this article, we present a novel machine translation model, the Operation Sequence Model (OSM), which combines the benefits of phrase-based and N-gram-based statistical machine translation (SMT) and remedies their drawbacks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7219778299331665}, {"text": "N-gram-based statistical machine translation (SMT)", "start_pos": 147, "end_pos": 197, "type": "TASK", "confidence": 0.7695772733007159}]}, {"text": "The model represents the translation process as a linear sequence of operations.", "labels": [], "entities": []}, {"text": "The sequence includes not only translation operations but also reordering operations.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9646034240722656}]}, {"text": "As in N-gram-based SMT, the model is: (i) based on minimal translation units, (ii) takes both source and target information into account, (iii) does not make a phrasal independence assumption, and (iv) avoids the spurious phrasal segmentation problem.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7146663665771484}]}, {"text": "As in phrase-based SMT, the model (i) has the ability to memorize lexical reordering triggers, (ii) builds the search graph dynamically, and (iii) decodes with large translation units during search.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7338595390319824}]}, {"text": "The unique properties of the model are (i) its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and (ii) the ability to model local and long-range reorderings consistently.", "labels": [], "entities": []}, {"text": "Using BLEU as a metric of translation accuracy, we found that our system performs significantly Computational Linguistics Volume 41, Number 2 better than state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems (Ncode) on standard translation tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9972838163375854}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7645922303199768}, {"text": "Computational Linguistics Volume 41", "start_pos": 96, "end_pos": 131, "type": "DATASET", "confidence": 0.6310568451881409}]}, {"text": "We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses.", "labels": [], "entities": []}, {"text": "Our results show that OSM outperforms lexicalized reordering on all translation tasks.", "labels": [], "entities": [{"text": "OSM", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9669606685638428}]}, {"text": "The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9513304233551025}]}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) advanced near the beginning of the century from word-based models () towards more advanced models that take contextual information into account.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.857379694779714}]}, {"text": "Phrase-based) and N-gram-based) models are two instances of such frameworks.", "labels": [], "entities": []}, {"text": "Although the two models have some common properties, they are substantially different.", "labels": [], "entities": []}, {"text": "The present work is a step towards combining the benefits and remedying the flaws of these two frameworks.", "labels": [], "entities": []}, {"text": "Phrase-based systems have a simple but effective mechanism that learns larger chunks of translation called bilingual phrases.", "labels": [], "entities": []}, {"text": "Memorizing larger units enables the phrase-based model to learn local dependencies such as short-distance reorderings, idiomatic collocations, and insertions and deletions that are internal to the phrase pair.", "labels": [], "entities": []}, {"text": "The model, however, has the following drawbacks: (i) it makes independence assumptions over phrases, ignoring the contextual information outside of phrases, (ii) the reordering model has difficulties in dealing with long-range reorderings, (iii) problems in both search and modeling require the use of a hard reordering limit, and (iv) it has the spurious phrasal segmentation problem, which allows multiple derivations of a bilingual sentence pair that have the same word alignment but different model scores.", "labels": [], "entities": []}, {"text": "N-gram-based models are Markov models over sequences of tuples that are generated monotonically.", "labels": [], "entities": []}, {"text": "Tuples are minimal translation units (MTUs) composed of source and target cepts.", "labels": [], "entities": [{"text": "minimal translation units (MTUs)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.637353777885437}]}, {"text": "The N-gram-based model has the following drawbacks: (i) only precalculated orderings are hypothesized during decoding, (ii) it cannot memorize and use lexical reordering triggers, (iii) it cannot perform long distance reorderings, and (iv) using tuples presents a more difficult search problem than in phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 315, "end_pos": 318, "type": "TASK", "confidence": 0.7153414487838745}]}, {"text": "In this article we present a novel model that tightly integrates translation and reordering into a single generative process.", "labels": [], "entities": []}, {"text": "Our model explains the translation process as a linear sequence of operations that generates a source and target sentence in parallel, in a target left-to-right order.", "labels": [], "entities": []}, {"text": "Possible operations are (i) generation of a sequence of source and target words, (ii) insertion of gaps as explicit target positions for reordering operations, and (iii) forward and backward jump operations that do the actual reordering.", "labels": [], "entities": []}, {"text": "The probability of a sequence of operations is defined according to an N-gram model, that is, the probability of an operation depends on then \u2212 1 preceding operations.", "labels": [], "entities": []}, {"text": "Because the translation (lexical generation) and reordering operations are coupled in a single generative story, the reordering decisions may depend on preceding translation decisions and translation decisions may depend on preceding reordering decisions.", "labels": [], "entities": [{"text": "translation (lexical generation)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.7441836774349213}]}, {"text": "This provides a natural reordering mechanism that is able to deal with local and long-distance reorderings in a consistent way.", "labels": [], "entities": []}, {"text": "Like the N-gram-based SMT model, the operation sequence model (OSM) is based on minimal translation units and takes both source and target information into account.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.912083089351654}]}, {"text": "This mechanism has several useful properties.", "labels": [], "entities": []}, {"text": "Firstly, no phrasal independence assumption is made.", "labels": [], "entities": []}, {"text": "The model has access to both source and target context outside of phrases.", "labels": [], "entities": []}, {"text": "Secondly the model learns a unique derivation of a bilingual sentence given its alignments, thus avoiding the spurious phrasal segmentation problem.", "labels": [], "entities": []}, {"text": "The OSM, however, uses operation N-grams (rather than tuple N-grams), which encapsulate both translation and reordering information.", "labels": [], "entities": []}, {"text": "This allows the OSM to use lexical triggers for reordering like phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.651135265827179}]}, {"text": "Our reordering approach is entirely different from the tuple N-gram model.", "labels": [], "entities": []}, {"text": "We consider all possible orderings instead of a small set of POS-based pre-calculated orderings, as is used in N-gram-based SMT, which makes their approach dependent on the availability of a source and target POS-tagger.", "labels": [], "entities": [{"text": "SMT", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.8716516494750977}]}, {"text": "We show that despite using POS tags the reordering patterns learned by N-gram-based SMT are not as general as those learned by our model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.8740373253822327}]}, {"text": "Combining MTU-model with Phrase-Based Decoding.", "labels": [], "entities": []}, {"text": "Using minimal translation units makes the search much more difficult because of the poor translation coverage, inaccurate future cost estimates, and pruning of correct hypotheses because of insufficient context.", "labels": [], "entities": []}, {"text": "The ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding, in terms of better search performance and superior selection of translation units.", "labels": [], "entities": []}, {"text": "In this article, we combine N-gram-based modeling with phrase-based decoding to benefit from both approaches.", "labels": [], "entities": []}, {"text": "Our model is based on minimal translation units, but we use phrases during decoding.", "labels": [], "entities": []}, {"text": "Through an extensive evaluation we found that this combination not only improves the search accuracy but also the BLEU scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9922282099723816}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9993612170219421}]}, {"text": "Our in-house phrase-based decoder outperformed state-of-the-art phrase-based (Moses and Phrasal) and N-gram-based (NCode) systems on three translation tasks.", "labels": [], "entities": []}, {"text": "Motivated by these results, we integrated the OSM into the state-of-the-art phrase-based system Moses ( ).", "labels": [], "entities": []}, {"text": "Our aim was to directly compare the performance of the lexicalized reordering model to the OSM and to see whether we can improve the performance further by using both models together.", "labels": [], "entities": []}, {"text": "Our integration of the OSM into Moses gave a statistically significant improvement over a competitive baseline system inmost cases.", "labels": [], "entities": []}, {"text": "In order to assess the contribution of improved reordering versus the contribution of better modeling with MTUs in the OSM-augmented Moses system, we removed the reordering operations from the stream of operations.", "labels": [], "entities": [{"text": "OSM-augmented Moses system", "start_pos": 119, "end_pos": 145, "type": "DATASET", "confidence": 0.8758426705996195}]}, {"text": "This is equivalent to integrating the conventional N-gram tuple sequence model) into a phrasebased decoder, as also tried by.", "labels": [], "entities": []}, {"text": "Small gains were observed inmost cases, showing that much of the improvement obtained by the OSM is due to better reordering.", "labels": [], "entities": [{"text": "OSM", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.8145321607589722}]}, {"text": "The primary strength of the OSM over the lexicalized reordering model is its ability to take advantage of the wider contextual information.", "labels": [], "entities": [{"text": "OSM", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9546940326690674}]}, {"text": "In an error analysis we found that the lexically driven OSM often falls back to very small context sizes because of data sparsity.", "labels": [], "entities": [{"text": "OSM", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9030241966247559}]}, {"text": "We show that this problem can be addressed by learning operation sequences over generalized representations such as POS tags.", "labels": [], "entities": []}, {"text": "The article is organized into seven sections.", "labels": [], "entities": []}, {"text": "Section 2 is devoted to a literature review.", "labels": [], "entities": []}, {"text": "We discuss the pros and cons of the phrase-based and N-gram-based SMT frameworks in terms of both model and search.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8831533193588257}]}, {"text": "Section 3 presents our model.", "labels": [], "entities": []}, {"text": "We show how our model combines the benefits of both of the frameworks and removes their drawbacks.", "labels": [], "entities": []}, {"text": "Section 4 provides an empirical evaluation of our preliminary system, which uses an MTU-based decoder, against state-of-the-art phrase-based (Moses and Phrasal) and N-gram-based (Ncode) systems on three standard tasks of translating German-to-English, Spanish-to-English, and French-to-English.", "labels": [], "entities": []}, {"text": "Our results show improvements over the baseline systems, but we noticed that using minimal translation units during decoding makes the search problem difficult, which suggests using larger units in search.", "labels": [], "entities": []}, {"text": "Section 5 presents an extension to our system to combine phrasebased decoding with the operation sequence model to address the problems in search.", "labels": [], "entities": []}, {"text": "Section 5.1 empirically shows that information available in phrases can be used to improve the search performance and translation quality.", "labels": [], "entities": []}, {"text": "Finally, we probe whether integrating our model into the phrase-based SMT framework addresses the mentioned drawbacks and improves translation quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8412711024284363}]}, {"text": "Section 6 provides an empirical evaluation of our integration on six standard tasks of translating German-English, French-English, and Spanish-English pairs.", "labels": [], "entities": []}, {"text": "Our integration gives statistically significant improvements over submission quality baseline systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our systems on German-to-English, French-to-English, and Spanish-toEnglish news translation for the purpose of development and evaluation.", "labels": [], "entities": [{"text": "Spanish-toEnglish news translation", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.6614471177260081}]}, {"text": "We used data from the eighth version of the Europarl Corpus and the News Commentary made available for the translation task of the Eighth Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Europarl Corpus", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9488992393016815}, {"text": "translation task", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.895185261964798}, {"text": "Eighth Workshop on Statistical Machine Translation", "start_pos": 131, "end_pos": 181, "type": "TASK", "confidence": 0.5652646869421005}]}, {"text": "The bilingual corpora contained roughly 2M bilingual sentence pairs, which we obtained by concatenating news commentary (\u2248 184K sentences) and Europarl for the estimation of the translation model.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.9360981583595276}]}, {"text": "Word alignments were generated with GIZA++, using the grow-diag-final-and heuristic 8 ().", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6363110244274139}]}, {"text": "All data are lowercased, and we use the Moses tokenizer.", "labels": [], "entities": []}, {"text": "We took news-test-2008 as the dev set for optimization and news-test 2009-2012 for testing.", "labels": [], "entities": []}, {"text": "The feature weights are tuned with Z-MERT (Zaidan 2009).", "labels": [], "entities": []}, {"text": "Our model, like the reordering models ( used in phrase-based decoders, is lexicalized.", "labels": [], "entities": []}, {"text": "However, our model has richer conditioning as it considers both translation and reordering context across phrasal boundaries.", "labels": [], "entities": []}, {"text": "The lexicalized reordering model used in phrase-based SMT only accounts for how a phrase pair was reordered with respect to its previous phrase (or block of phrases).", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.7709047198295593}]}, {"text": "Although such an independence assumption is useful to reduce sparsity, it is overgeneralizing, with only three possible orientations.", "labels": [], "entities": []}, {"text": "Moreover, because most of the extracted phrases are observed only once, the corresponding probability of orientation given phrase-pair estimates is very sparse.", "labels": [], "entities": []}, {"text": "The model often has to fallback to short oneword phrases.", "labels": [], "entities": []}, {"text": "However, most short phrases are observed frequently with all possible orientations during training.", "labels": [], "entities": []}, {"text": "This makes it difficult for the decoder to decide which orientation should be picked during decoding.", "labels": [], "entities": []}, {"text": "The model therefore overly relies on the language model to break such ties.", "labels": [], "entities": []}, {"text": "The OSM may also suffer from data sparsity and the back-off smoothing may fallback to very short contexts.", "labels": [], "entities": [{"text": "OSM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8867764472961426}]}, {"text": "But it might still be able to disambiguate better than the lexicalized reordering models.", "labels": [], "entities": []}, {"text": "Also these drawbacks can be addressed by learning an OSM over generalized word representation such as POS tags, as we show in this section.", "labels": [], "entities": []}, {"text": "In an effort to make a comparison of the operation sequence model with the lexicalized reordering model, we incorporate the OSM into the phrase-based Moses decoder.", "labels": [], "entities": []}, {"text": "This allows us to exactly compare the two models in identical settings.", "labels": [], "entities": []}, {"text": "We integrate the OSM into the hypothesis extension process of the phrase-based decoder.", "labels": [], "entities": []}, {"text": "We convert each phrase pair into a sequence of operations by extracting the MTUs within the phrase pair and using phrase internal alignments.", "labels": [], "entities": []}, {"text": "The OSM is used as a feature in the log-linear framework.", "labels": [], "entities": []}, {"text": "We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1).", "labels": [], "entities": [{"text": "Gap", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9790442585945129}, {"text": "Open Gap", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9108459055423737}, {"text": "Deletion counts", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.9812875390052795}]}], "tableCaptions": [{"text": " Table 2  Comparison on five test sets -OSM mtu = OSM MTU-based decoder.", "labels": [], "entities": []}, {"text": " Table 3  Comparing search accuracies of MTU-based (OSM mtu ) and phrase-based (OSM phr ) decoders.", "labels": [], "entities": []}, {"text": " Table 4  Comparison on four test sets -OSM mtu = MTU-based decoder with stack size 500, OSM phr =  phrase-based decoder with stack size 200.", "labels": [], "entities": []}, {"text": " Table 5  Comparison against the lexicalized reordering model -Pb = baseline without lexical reordering.  An asterisk indicates statistical significance over baseline (Pb lex = Pb + lexicalized reordering).", "labels": [], "entities": []}, {"text": " Table 6  Comparing the operation sequence model versus the tuple sequence model.", "labels": [], "entities": []}, {"text": " Table 7  Using generalized OSMs. s = surface; p = pos.", "labels": [], "entities": []}, {"text": " Table 8  Wall-clock decoding times (in minutes) on WMT-13.", "labels": [], "entities": [{"text": "Wall-clock decoding", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.614129513502121}, {"text": "WMT-13", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9891295433044434}]}, {"text": " Table 9  Data sizes (in number of sentences) and memory usage (in giga-bytes). Columns: Phrase  translation and lexicalized reordering tables give overall model sizes/sizes when filtered on  WMT-2013.", "labels": [], "entities": [{"text": "Phrase  translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8196333944797516}, {"text": "WMT-2013", "start_pos": 192, "end_pos": 200, "type": "DATASET", "confidence": 0.9798130989074707}]}]}