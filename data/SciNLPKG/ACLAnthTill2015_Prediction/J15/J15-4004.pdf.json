{"title": [{"text": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation", "labels": [], "entities": []}], "abstractContent": [{"text": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways.", "labels": [], "entities": []}, {"text": "First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9631051421165466}, {"text": "MEN", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.7745910882949829}]}, {"text": "We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association.", "labels": [], "entities": []}, {"text": "Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair.", "labels": [], "entities": []}, {"text": "This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved.", "labels": [], "entities": []}, {"text": "Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999.", "labels": [], "entities": []}, {"text": "There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is very little similar about coffee and cups.", "labels": [], "entities": []}, {"text": "Coffee refers to a plant, which is a living organism or a hot brown (liquid) drink.", "labels": [], "entities": []}, {"text": "In contrast, a cup is a man-made solid of broadly well-defined shape and size with a specific function relating to the consumption of liquids.", "labels": [], "entities": []}, {"text": "Perhaps the only clear trait these concepts have in common is that they are concrete entities.", "labels": [], "entities": []}, {"text": "Nevertheless, in what is currently the most popular evaluation gold standard for semantic similarity, WordSim(WS)-353 (), coffee and cup are rated as more \"similar\" than pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.).", "labels": [], "entities": [{"text": "WordSim(WS)-353", "start_pos": 102, "end_pos": 117, "type": "DATASET", "confidence": 0.8915930986404419}]}, {"text": "Such anomalies also exist in other gold standards such as the MEN data set ().", "labels": [], "entities": [{"text": "MEN data set", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.9587607185045878}]}, {"text": "As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar.", "labels": [], "entities": []}, {"text": "Although clearly different, coffee and cup are very much related.", "labels": [], "entities": []}, {"text": "The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness), topical similarity (, and domain similarity).", "labels": [], "entities": []}, {"text": "Association contrasts with similarity, the relation connecting cup and mug.", "labels": [], "entities": [{"text": "similarity", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9845101237297058}]}, {"text": "At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents.", "labels": [], "entities": []}, {"text": "Computational models that effectively capture similarity as distinct from association have numerous applications.", "labels": [], "entities": []}, {"text": "Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools).", "labels": [], "entities": [{"text": "language correction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.6498185247182846}]}, {"text": "Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8147038221359253}]}, {"text": "Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown.", "labels": [], "entities": [{"text": "similarity estimation", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.7650127112865448}]}, {"text": "As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity.", "labels": [], "entities": [{"text": "semantic evaluation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8097263276576996}]}, {"text": "In particular, in both WS-353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating = 6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high rating regardless of whether or not their constituents are similar.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.7610737085342407}, {"text": "MEN", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8792589902877808}]}, {"text": "Thus, the utility of such resources to the development and application of similarity models is limited, a problem exacerbated by the fact that many researchers appear unaware of what their evaluation resources actually measure.", "labels": [], "entities": []}, {"text": "Although certain smaller gold standards-those of (RG) and (WS-Sim)-do focus clearly on similarity, these resources suffer from other important limitations.", "labels": [], "entities": []}, {"text": "For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.8457001447677612}, {"text": "MEN", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8304787874221802}]}, {"text": "It is common practice in NLP to define the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.9508818089962006}, {"text": "similarity modeling", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7596574127674103}]}, {"text": "However, circumstantial evidence suggests that distributional models are far from perfect.", "labels": [], "entities": []}, {"text": "For instance, we are someway from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents.", "labels": [], "entities": []}, {"text": "Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity.", "labels": [], "entities": []}, {"text": "SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk, 2 who were asked to rate the similarity, as opposed to association, of concepts via a simple visual interface.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.9365805586179098}]}, {"text": "The choice of evaluation pairs in SimLex-999 was motivated by empirical evidence that humans represent concepts of distinct part-of-speech (POS) and conceptual concreteness) differently.", "labels": [], "entities": []}, {"text": "Whereas existing gold standards contain only concrete noun concepts (MEN) or cover only some of these distinctions via a random selection of items (WS-353, RG), SimLex-999 contains a principled selection of adjective, verb, and noun concept pairs covering the full concreteness spectrum.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 148, "end_pos": 154, "type": "DATASET", "confidence": 0.7745251655578613}]}, {"text": "This design enables more nuanced analyses of how computational models overcome the distinct challenges of representing concepts of these types.", "labels": [], "entities": []}, {"text": "In Section 4 we present quantitative and qualitative analyses of the SimLex-999 ratings, which indicate that participants found it unproblematic to quantify consistently the similarity of the full range of concepts and to distinguish it from association.", "labels": [], "entities": [{"text": "SimLex-999 ratings", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.8149836659431458}]}, {"text": "Unlike existing data sets, SimLex-999 therefore contains a significant number of pairs, such as, which are strongly associated but receive low similarity scores.", "labels": [], "entities": []}, {"text": "The second main contribution of this paper, presented in Section 5, is the evaluation of state-of-the-art distributional semantic models using SimLex-999.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 143, "end_pos": 153, "type": "DATASET", "confidence": 0.9347952008247375}]}, {"text": "These include the well-known neural language models (NLMs) of,, and, which we compare with traditional vectorspace co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimensionality reduction (SVD).", "labels": [], "entities": []}, {"text": "Our analyses demonstrate how SimLex-999 can be applied to uncover substantial differences in the ability of models to represent concepts of different types.", "labels": [], "entities": []}, {"text": "Despite these differences, the models we consider each share the characteristic of being better able to capture association than similarity.", "labels": [], "entities": []}, {"text": "We show that the difficulty of estimating similarity is driven primarily by those strongly associated pairs with a high (association) rating in gold standards such as WS-353 and MEN, but a low similarity rating in SimLex-999.", "labels": [], "entities": [{"text": "similarity", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.7347235083580017}, {"text": "WS-353", "start_pos": 167, "end_pos": 173, "type": "DATASET", "confidence": 0.9253014326095581}, {"text": "MEN", "start_pos": 178, "end_pos": 181, "type": "DATASET", "confidence": 0.7049337029457092}, {"text": "similarity", "start_pos": 193, "end_pos": 203, "type": "METRIC", "confidence": 0.9559851288795471}, {"text": "SimLex-999", "start_pos": 214, "end_pos": 224, "type": "DATASET", "confidence": 0.9236624240875244}]}, {"text": "As a result of including these challenging cases, together with a wider diversity of lexical concepts in general, current models achieve notably lower scores on SimLex-999 than on existing gold standard evaluations, and well below the SimLex-999 inter-human agreement ceiling.", "labels": [], "entities": []}, {"text": "Finally, we explore ways in which distributional models might improve on this performance in similarity modeling.", "labels": [], "entities": [{"text": "similarity modeling", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8147375583648682}]}, {"text": "To do so, we evaluate the models on the SimLex-999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets and subsets of more and less strongly associated pairs (Sections 5.2.2-5.2.4).", "labels": [], "entities": []}, {"text": "As part of these analyses, we confirm the hypothesis () that models learning from input informed by dependency parsing, rather than simple running-text input, yield improved similarity estimation and, specifically, clearer distinction between similarity and association.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7213694751262665}]}, {"text": "In contrast, we find no evidence fora related hypothesis () that smaller context windows improve the ability of models to capture similarity.", "labels": [], "entities": []}, {"text": "We do, however, observe clear differences in model performance on the distinct concept types included in SimLex-999.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.9071857929229736}]}, {"text": "Taken together, these experiments demonstrate the benefit of the diversity of concepts included in SimLex-999; it would not have been possible to derive similar insights by evaluating based on existing gold standards.", "labels": [], "entities": []}, {"text": "We conclude by discussing how observations such as these can guide future research into distributional semantic models.", "labels": [], "entities": []}, {"text": "By facilitating better-defined evaluations and finer-grained analyses, we hope that SimLex-999 will ultimately contribute to the development of models that accurately reflect human intuitions of similarity for the full range of concepts in language.", "labels": [], "entities": []}], "datasetContent": [{"text": "For brevity, we do not exhaustively review all methods that have been used to evaluate semantic models, but instead focus on the similarity or association-based gold standards that are most commonly applied in recent work in NLP.", "labels": [], "entities": []}, {"text": "In each case, we consider how well the data set satisfies one of the three following criteria: Representative.", "labels": [], "entities": []}, {"text": "The resource should cover the full range of concepts that occur in natural language.", "labels": [], "entities": []}, {"text": "In particular, it should include cases representing the different ways in which humans represent or process concepts, and cases that are both challenging and straightforward for computational models.", "labels": [], "entities": []}, {"text": "In order fora gold standard to be diagnostic of how well a model can be applied to downstream applications, a clear understanding is needed of what exactly the gold standard measures.", "labels": [], "entities": []}, {"text": "In particular, it must clearly distinguish between dissociable semantic relations such as association and similarity.", "labels": [], "entities": []}, {"text": "Untrained native speakers must be able to quantify the target property consistently, without requiring lengthy or detailed instructions.", "labels": [], "entities": []}, {"text": "This ensures that the data reflect a meaningful cognitive or semantic phenomenon, and also enables the data set to be scaled up or transferred to other languages at minimal cost and effort.", "labels": [], "entities": []}, {"text": "We begin our review of existing evaluation with the gold standard most commonly applied in current NLP research.", "labels": [], "entities": []}, {"text": "WS-353 () is perhaps the most commonly used evaluation gold standard for semantic models.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9013242125511169}]}, {"text": "Despite its name, and the fact that it is often referred to as a \"similarity gold standard,\" 4 in fact, the instructions given to annotators when producing WS-353 were ambiguous with respect to similarity and association.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 156, "end_pos": 162, "type": "DATASET", "confidence": 0.72232586145401}, {"text": "similarity", "start_pos": 194, "end_pos": 204, "type": "METRIC", "confidence": 0.9456819891929626}]}, {"text": "Subjects were asked to: Assign a numerical similarity score between 0 and 10 (0 = words totally unrelated, 10 = words VERY closely related) ...", "labels": [], "entities": [{"text": "numerical similarity score", "start_pos": 33, "end_pos": 59, "type": "METRIC", "confidence": 0.7152262330055237}]}, {"text": "when estimating similarity of antonyms, consider them \"similar\" (i.e., belonging to the same domain or representing features of the same concept), not \"dissimilar\".", "labels": [], "entities": []}, {"text": "As we confirm analytically in Section 5.2, these instructions result in pairs being rated according to association rather than similarity.", "labels": [], "entities": []}, {"text": "5 WS-353 consequently suffers two important limitations as an evaluation of similarity (which also afflict other resources to a greater or lesser degree): 1.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 2, "end_pos": 8, "type": "DATASET", "confidence": 0.6373682618141174}, {"text": "similarity", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9636328220367432}]}, {"text": "Many dissimilar word pairs receive a high rating.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Top: Concept pairs with the lowest WupSim scores in the USF data set overall. Bottom: Pairs  with the largest discrepancy in rank between association strength (high) and WupSim (low).", "labels": [], "entities": [{"text": "USF data set", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.977459192276001}, {"text": "association strength", "start_pos": 148, "end_pos": 168, "type": "METRIC", "confidence": 0.9573794603347778}, {"text": "WupSim", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.716031551361084}]}, {"text": " Table 2  Top: Similarity aligns with association. Pairs with a small difference in rank between USF  (association) and SimLex-999 (similarity) scores for each POS category. Bottom: Similarity  contrasts with association. Pairs with a high difference in rank for each POS category. *Note that  the distribution of USF association scores on the interval", "labels": [], "entities": [{"text": "USF", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.8226646780967712}]}]}