{"title": [{"text": "CODRA: A Novel Discriminative Framework for Rhetorical Analysis", "labels": [], "entities": [{"text": "CODRA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7903828024864197}, {"text": "Rhetorical Analysis", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8028839826583862}]}], "abstractContent": [{"text": "Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts.", "labels": [], "entities": []}, {"text": "Rhetorical analysis seeks to uncover this coherence structure.", "labels": [], "entities": []}, {"text": "In this article, we present CODRA-a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.", "labels": [], "entities": [{"text": "Rhetorical Analysis", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7344038188457489}]}, {"text": "CODRA comprises a discourse segmenter and a discourse parser.", "labels": [], "entities": [{"text": "CODRA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8396948575973511}]}, {"text": "First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text.", "labels": [], "entities": []}, {"text": "Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing.", "labels": [], "entities": []}, {"text": "We present two approaches to combine these two stages of parsing effectively.", "labels": [], "entities": [{"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9817237854003906}]}, {"text": "By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art, often by a wide margin.", "labels": [], "entities": []}, {"text": "We also show that a reranking of the k-best parse hypotheses generated by CODRA can potentially improve the accuracy even further.", "labels": [], "entities": [{"text": "CODRA", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8814036846160889}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9993728995323181}]}], "introductionContent": [{"text": "A well-written text is not merely a sequence of independent and isolated sentences, but instead a sequence of structured and related sentences, where the meaning of a sentence relates to the previous and the following ones.", "labels": [], "entities": []}, {"text": "In other words, a well-written where rhetorical relations were originally signaled (i.e., the discourse cues were artificially removed), and did not verify how well this approach performs on the instances that are not originally signaled.", "labels": [], "entities": []}, {"text": "Subsequent studies confirm that classifiers trained on instances stripped of their original discourse cues do not generalize well to implicit cases because they are linguistically quite different.", "labels": [], "entities": []}, {"text": "Note that this approach to identifying discourse relations in the absence of manually labeled data does not fully solve the parsing problem (i.e., building DTs); rather, it only attempts to identify a small subset of coarser relations between two (flat) text segments (i.e., a tagging problem).", "labels": [], "entities": []}, {"text": "Arguably, to perform a complete rhetorical analysis, one needs to use supervised machine learning techniques based on human-annotated data.", "labels": [], "entities": [{"text": "rhetorical analysis", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.9303133487701416}]}], "datasetContent": [{"text": "In this section we present our experimental results.", "labels": [], "entities": []}, {"text": "First, we describe the corpora on which the experiments were performed and the evaluation metrics used to measure the performance of the discourse segmenter and the parser.", "labels": [], "entities": []}, {"text": "Then we show the performance of our discourse segmenter, followed by the performance of our discourse parser.", "labels": [], "entities": []}, {"text": "In this subsection we describe the metrics used to measure both how much the annotators agree with each other, and how well the systems perform when their outputs are compared with human annotations for the discourse analysis tasks.", "labels": [], "entities": []}, {"text": "In this section we present our experiments on discourse segmentation.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7191287279129028}]}, {"text": "We compare the performance of our discourse segmenter with the performance of the two publicly available discourse segmenters, namely, the discourse segmenters of the HILDA () and SPADE (Soricut and Marcu 2003) systems.", "labels": [], "entities": []}, {"text": "We also compare our results with the stateof-the-art results reported by on the RST-DT test set.", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9075078765551249}]}, {"text": "In all our experiments when comparing two systems, we use paired t-test on the F-scores to measure statistical significance and report the p-value.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9486830830574036}]}, {"text": "We ran HILDA with its default settings.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.7834901213645935}]}, {"text": "For SPADE, we applied the same modifications to its default settings as described in, which delivers significantly improved performance over its original version.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.7638755440711975}]}, {"text": "Specifically, in our experiments on the RST-DT corpus, we trained SPADE using the human-annotated syntactic trees extracted from the Penn Treebank, and, during testing, we replaced the Charniak parser (Charniak 2000) with a more.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8400597870349884}, {"text": "Penn Treebank", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.9156562685966492}]}, {"text": "However, because of the lack of gold syntactic trees in the instructional corpus, we trained SPADE in this corpus using the syntactic trees produced by the reranking parser.", "labels": [], "entities": []}, {"text": "To avoid using the gold syntactic trees, we used the reranking parser in our system for both training and testing purposes.", "labels": [], "entities": []}, {"text": "This syntactic parser was trained on the sections of the Penn Treebank not included in our test set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9915486574172974}]}, {"text": "We applied the same canonical lexical head projection rules to lexicalize the syntactic trees as done in HILDA and SPADE.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.8372951149940491}]}, {"text": "Note that previous studies) on discourse segmentation only report their performance on the RST-DT test set.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7374467551708221}, {"text": "RST-DT test set", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.8362284104029337}]}, {"text": "To compare our results with them, we evaluated our model on the RST-DT test set.", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8600726922353109}]}, {"text": "In addition, we showed a more general performance of SPADE and our system on the two corpora based on 10-fold cross validation.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 53, "end_pos": 58, "type": "TASK", "confidence": 0.46000751852989197}]}, {"text": "However, SPADE does not come with a training module for its segmenter.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 9, "end_pos": 14, "type": "TASK", "confidence": 0.8881598114967346}]}, {"text": "We reimplemented this module and verified its correctness by reproducing the results on the RST-DT test set.", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.858522633711497}]}, {"text": "shows the discourse segmentation results of different systems in Precision, Recall, and F-score on the two corpora.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6933237165212631}, {"text": "Precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9922662377357483}, {"text": "Recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9880776405334473}, {"text": "F-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9976397752761841}]}, {"text": "On the RST-DT corpus, HILDA's segmenter delivers the weakest performance, having an F-score of only 74.1.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8279237151145935}, {"text": "F-score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9992862343788147}]}, {"text": "Note that the high segmentation accuracy reported by is due to a less stringent evaluation metric.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9619214534759521}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8725101947784424}]}, {"text": "SPADE performs much better than HILDA with an absolute F-score improvement of 11.1%.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5813263058662415}, {"text": "HILDA", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.8152512311935425}, {"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9904876351356506}]}, {"text": "Our segmenter DS outperforms SPADE with an absolute F-score improvement of 4.9% (p-value < 2.4e-06), and also achieves comparable results to the ones of, even though we use fewer features.", "labels": [], "entities": [{"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9911662936210632}]}, {"text": "Notice that human agreement for this task is quite high-namely, an F-score of 98.3 computed on the doubly-annotated portion of the RST-DT corpus mentioned in Section 6.1.", "labels": [], "entities": [{"text": "agreement", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9436365365982056}, {"text": "F-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9992372989654541}, {"text": "RST-DT corpus", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.7875255644321442}]}, {"text": "In this section we present our experiments on discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7064858824014664}]}, {"text": "First, we describe the experimental set-up.", "labels": [], "entities": []}, {"text": "Then, we present the results of the parsers.", "labels": [], "entities": []}, {"text": "While presenting the performance of our discourse parser, we show a breakdown of intra-sentential versus inter-sentential results, in addition to the aggregated results at the document level.", "labels": [], "entities": []}, {"text": "In our experiments on sentence-level (i.e., intra-sentential) discourse parsing, we compare our approach with SPADE (Soricut and Marcu 2003) on the RST-DT corpus, and with the ILP-based approach of Subba and Di-Eugenio (2009) on the instructional corpus, because they are the state of the art in their respective genres.", "labels": [], "entities": [{"text": "sentence-level (i.e., intra-sentential) discourse parsing", "start_pos": 22, "end_pos": 79, "type": "TASK", "confidence": 0.7733738943934441}, {"text": "SPADE", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.7409422397613525}, {"text": "RST-DT corpus", "start_pos": 148, "end_pos": 161, "type": "DATASET", "confidence": 0.7308703064918518}]}, {"text": "For SPADE, we applied the same modifications to its default settings as described in Section 6.3.1, which leads to improved performance.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.8510899543762207}]}, {"text": "Similarly, in our experiments on document-level (i.e., multi-sentential) parsing, we compare our approach with HILDA () on the RST-DT corpus, and with the ILPbased approach (Subba and Di-Eugenio 2009) on the instructional corpus.", "labels": [], "entities": [{"text": "multi-sentential) parsing", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7129130164782206}, {"text": "HILDA", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9473504424095154}, {"text": "RST-DT corpus", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.7389543354511261}]}, {"text": "The results for HILDA were obtained by running the system with default settings on the same inputs we provided to our system.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.5188197493553162}]}, {"text": "Because we could not run the ILP-based system (not publicly available), we report the performance presented in their paper.", "labels": [], "entities": []}, {"text": "Our experiments on the RST-DT corpus use the same 18 coarser coherence relations (see later in this article), defined by and also used in SPADE and HILDA systems.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.8002531230449677}]}, {"text": "More specifically, the relation set consists of 16 relation categories and two pseudo-relations, namely, Textual-Organization and Same-Unit.", "labels": [], "entities": []}, {"text": "After attaching the nuclearity statuses (NS, SN, NN) to these relations, we obtain 41 distinct relations.", "labels": [], "entities": []}, {"text": "Our experiments on the instructional corpus consider the same 26 primary relations (e.g., Goal:Act, Cause:Effect) used by Subba and Di-Eugenio (2009) and also treat the reversals of non-commutative relations as separate relations.", "labels": [], "entities": []}, {"text": "That is, Goal-Act and Act-Goal are considered to be two different coherence relations.", "labels": [], "entities": []}, {"text": "Attaching the nuclearity statuses to these relations provides 76 distinct relations.", "labels": [], "entities": []}, {"text": "Based on our experiments on the development set, the size of the automatically built bi-gram and tri-gram dictionaries was set to 95% of their total number of items, and the size of the unigram dictionary was set to 100%.", "labels": [], "entities": []}, {"text": "Note that the unigram dictionary contains only special tags denoting EDU, sentence, and paragraph boundaries.", "labels": [], "entities": []}, {"text": "This section presents our experimental evaluation on intra-sentential discourse parsing.", "labels": [], "entities": [{"text": "intra-sentential discourse parsing", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.7401666045188904}]}, {"text": "First, we show the performance of the sentence-level parsers when they are provided with manual (or gold) discourse segmentations.", "labels": [], "entities": []}, {"text": "This allows us to judge the parsing performance independently of the segmentation task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9644272923469543}]}, {"text": "Then, we show the end-to-end performance of our intra-sentential framework, that is, the intra-sentential parsing performance based on automatic discourse segmentation.", "labels": [], "entities": []}, {"text": "presents the intra-sentential discourse parsing results when manual discourse segmentation is used.", "labels": [], "entities": [{"text": "intra-sentential discourse parsing", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.7009773254394531}]}, {"text": "Recall from our discussion on evaluation metrics in Section 6.2.2 that precision, recall, and F-score are the same when manual segmentation is used.", "labels": [], "entities": [{"text": "Section 6.2.2", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.8752629458904266}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9995707869529724}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9981081485748291}, {"text": "F-score", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9989263415336609}]}, {"text": "Therefore, we report only one of them.", "labels": [], "entities": []}, {"text": "Notice that our sentence-level discourse parser PAR-S consistently outperforms SPADE on the RST-DT test set in all three metrics, and the improvements are statistically significant (p-value < 0.01).", "labels": [], "entities": [{"text": "sentence-level discourse parser PAR-S", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.6167128011584282}, {"text": "RST-DT test set", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.8057277699311575}]}, {"text": "Especially, on the relation labeling task, which is the hardest among the three tasks, we achieve an absolute F-score improvement of 12.2 percentage points, which represents a relative error rate reduction of 37.7%.", "labels": [], "entities": [{"text": "relation labeling task", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8996872305870056}, {"text": "F-score", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9860889315605164}, {"text": "error rate reduction", "start_pos": 185, "end_pos": 205, "type": "METRIC", "confidence": 0.9422990878423055}]}, {"text": "We experiment with our full document-level discourse parser on the two corpora using the two parsing approaches described in Section 4.3, namely, 1S-1S and the sliding window.", "labels": [], "entities": []}, {"text": "On RST-DT, the standard split was used for training and testing.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.7486026883125305}]}, {"text": "On the instructional corpus, Subba and Di-Eugenio (2009) used 151 documents for training and 25 documents for testing.", "labels": [], "entities": []}, {"text": "Because we did not have access to their particular split, we took five random samples of 151 documents for training and 25 documents for testing, and report the average performance over the five test sets.", "labels": [], "entities": []}, {"text": "presents results for our two-stage discourse parser (TSP) using approaches 1S-1S (TSP 1-1) and the sliding window (TSP SW) on manually segmented texts.", "labels": [], "entities": []}, {"text": "Recall that precision, recall, and F-score are the same when manual segmentation is used.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9997928738594055}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9995986819267273}, {"text": "F-score", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9991549253463745}]}, {"text": "We compare our parser with the state-of-the-art on the two corpora: HILDA (Hernault et al.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.8740494251251221}]}, {"text": "2010) on RST-DT, and the ILP-based approach on the instructional domain.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 9, "end_pos": 15, "type": "TASK", "confidence": 0.822288990020752}]}, {"text": "On both corpora, our systems outperform existing systems by a wide margin (p-value <7.1e-05 on RST-DT).", "labels": [], "entities": []}, {"text": "On RST-DT, our parser TSP 1-1 achieves absolute improvements of 7.9 percentage points, 9.3 percentage points, and 11.5 percentage points in span, nuclearity, and relation, respectively, over HILDA.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.6939523220062256}, {"text": "span", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.98447185754776}, {"text": "relation", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9815981388092041}]}, {"text": "This represents relative error reductions of 31.2%, 22.7%, and 20.7% in span, nuclearity, and relation, respectively.", "labels": [], "entities": [{"text": "span", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9416346549987793}]}, {"text": "Beside HILDA, we also compare our results with two baseline parsers on RST-DT: (1) CRF-O, which uses a single unified CRF-based parsing model shown in (the one used for multi-sentential parsing) without distinguishing between intra-and multisentential parsing, and (2) CRF-T, which uses two different CRF-based parsing models for intra-and multi-sentential parsing in the two-stage approach 1S-1S, both models having the same structure as in.", "labels": [], "entities": []}, {"text": "Thus, CRF-T is a variation of TSP 1-1, where the DCRF-based (chain-structured) intra-sentential parsing model is replaced with a simpler CRF-based parsing model.", "labels": [], "entities": []}, {"text": "Note that although CRF-O does not explicitly discriminate between intra-and multi-sentential parsing, it uses N-gram features that include sentence and EDU boundaries to encode this information into the model.", "labels": [], "entities": []}, {"text": "shows that both CRF-O and CRF-T outperform HILDA by a good margin (p-value <0.0001).", "labels": [], "entities": [{"text": "HILDA", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.825888991355896}]}, {"text": "This improvement can be attributed to the optimal parsing algorithm and better feature selection strategy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9667677283287048}]}, {"text": "When we compare CRF-T with CRF-O, we notice significant performance gains for CRF-T (p-value <0.001).", "labels": [], "entities": []}, {"text": "The absolute gains are 4.32 percentage points, 2.68 percentage points, and 4.55 percentage points in span, nuclearity, and relation, respectively.", "labels": [], "entities": [{"text": "span", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9847522377967834}, {"text": "relation", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9758902192115784}]}, {"text": "This comparison clearly demonstrates the benefit of using a two-stage approach with two different parsing models over a framework with one single unified parsing model.", "labels": [], "entities": []}, {"text": "Finally, when we compare our best results with the human agreements, we still observe room for further improvement in all three measures.", "labels": [], "entities": []}, {"text": "On the instructional genre, our parser TSP 1-1 delivers absolute F-score improvements of 10.3 percentage points, 13.6 percentage points, and 8.1 percentage points in span, nuclearity, and relations, respectively, over the ILP-based approach of.", "labels": [], "entities": [{"text": "F-score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.995492696762085}, {"text": "span", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9693743586540222}]}, {"text": "Our parser, therefore, reduces errors by 34.7%, 26.9%, and 12.5% in span, nuclearity, and relations, respectively.", "labels": [], "entities": [{"text": "errors", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9953091740608215}, {"text": "span", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9590615034103394}]}, {"text": "If we compare the performance of our discourse parsers on the two corpora, we observe lower results on the instructional corpus.", "labels": [], "entities": []}, {"text": "There could be two reasons for this.", "labels": [], "entities": []}, {"text": "First, the instructional corpus has a smaller amount of data with a larger set of relations (76 with nuclearity attached).", "labels": [], "entities": []}, {"text": "Second, some of the frequent relations are semantically very similar (e.g., Preparation-Act, Step1-Step2), which makes it difficult even for the human annotators to distinguish them.", "labels": [], "entities": []}, {"text": "Comparison between our two document-level parsing approaches reveals that TSP SW significantly outperforms TSP 1-1 only in finding the right structure on both corpora (p-value <0.01).", "labels": [], "entities": [{"text": "document-level parsing", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.620599240064621}]}, {"text": "Not surprisingly, the improvement is higher on the instructional corpus.", "labels": [], "entities": []}, {"text": "A likely explanation is that the instructional corpus contains more leaky boundaries (12%), allowing the sliding window approach to be more effective in finding those, without inducing much noise for the labels.", "labels": [], "entities": []}, {"text": "This demonstrates the potential of TSP SW for data sets with even more leaky boundaries, e.g., the Dutch (Vliet and Redeker 2011) and the German Potsdam (Stede 2004) corpora.", "labels": [], "entities": [{"text": "TSP SW", "start_pos": 35, "end_pos": 41, "type": "TASK", "confidence": 0.9231185019016266}]}, {"text": "However, it would be interesting to see how other heuristics to do consolidation in the cross condition (Section 4.3.2) perform.", "labels": [], "entities": []}, {"text": "To analyze errors made by TSP SW, we looked at some poorly parsed examples and found that although TSP SW finds more correct structures, a corresponding improvement in labeling relations is not present because in some cases, it tends to induce noise from the neighboring sentences for the labels.", "labels": [], "entities": []}, {"text": "For example, when parsing is performed on the first sentence in in isolation using 1S-1S, our parser rightly identifies the Contrast relation between EDUs 2 and 3.", "labels": [], "entities": []}, {"text": "But, when it is considered with its neighboring sentences by the sliding window, the parser labels it as Elaboration.", "labels": [], "entities": []}, {"text": "A promising strategy to deal with this and similar problems would be to apply both approaches to each sentence and combine them by consolidating three probabilistic decisions, namely, the one from 1S-1S and the two from the sliding window.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Discourse segmentation results of different models on the two corpora. Performances  significantly superior to SPADE are denoted by *.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7851155996322632}]}, {"text": " Table 7  Oracle scores as a function of k of k-best sentence-level parses on RST-DT.", "labels": [], "entities": []}, {"text": " Table 8  Oracle scores as a function of k of k-best document-level parses on RST-DT.", "labels": [], "entities": []}, {"text": " Table 9  Parsing results using different subsets of features on RST-DT test set.", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.8880894184112549}]}]}