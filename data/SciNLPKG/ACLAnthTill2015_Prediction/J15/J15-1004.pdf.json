{"title": [{"text": "Concrete Models and Empirical Evaluations for the Categorical Compositional Distributional Model of Meaning", "labels": [], "entities": [{"text": "Categorical Compositional Distributional Model of Meaning", "start_pos": 50, "end_pos": 107, "type": "TASK", "confidence": 0.5992980500062307}]}], "abstractContent": [{"text": "Modeling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.", "labels": [], "entities": [{"text": "Modeling compositional meaning for sentences", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.9020529150962829}]}, {"text": "The categorical model of Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) provides a solution by unifying a categorial grammar and a distributional model of meaning.", "labels": [], "entities": []}, {"text": "It takes into account syntactic relations during semantic vector composition operations.", "labels": [], "entities": [{"text": "semantic vector composition", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6330548922220866}]}, {"text": "But the setting is abstract: It has not been evaluated on empirical data and applied to any language tasks.", "labels": [], "entities": []}, {"text": "We generate concrete models for this setting by developing algorithms to construct tensors and linear maps and instantiate the abstract parameters using empirical data.", "labels": [], "entities": []}, {"text": "We then evaluate our concrete models against several experiments, both existing and new, based on measuring how well models align with human judgments in a paraphrase detection task.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 156, "end_pos": 181, "type": "TASK", "confidence": 0.8459358215332031}]}, {"text": "Our results show the implementation of this general abstract framework to perform on par with or outperform other leading models in these experiments.", "labels": [], "entities": []}], "introductionContent": [{"text": "The distributional approach to the semantic modeling of natural language, inspired by the notion-presented by and-that the meaning of a word is tightly related to its context of use, has grown in popularity as a method of semantic representation.", "labels": [], "entities": [{"text": "semantic modeling of natural language", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.8390276670455933}, {"text": "semantic representation", "start_pos": 222, "end_pos": 245, "type": "TASK", "confidence": 0.7181698530912399}]}, {"text": "It draws from the frequent use of vector-based document models in information retrieval, modeling the meaning of words as vectors based on the distribution of co-occurring terms within the context of a word.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7004591971635818}]}, {"text": "Using various vector similarity metrics as a measure of semantic similarity, these distributional semantic models (DSMs) are used fora variety of NLP tasks, from automated thesaurus building) to automated essay marking.", "labels": [], "entities": [{"text": "automated essay marking", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.6212666630744934}]}, {"text": "The broader connection to information retrieval and its applications is also discussed by.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8445646166801453}]}, {"text": "The success of DSMs in essentially word-based tasks such as thesaurus extraction and construction) invites an investigation into how DSMs can be applied to NLP and information retrieval (IR) tasks revolving around larger units of text, using semantic representations for phrases, sentences, or documents, constructed from lemma vectors.", "labels": [], "entities": [{"text": "thesaurus extraction and construction", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.8059981763362885}, {"text": "information retrieval (IR)", "start_pos": 164, "end_pos": 190, "type": "TASK", "confidence": 0.7880733609199524}]}, {"text": "However, the problem of compositionality in DSMs-of how to go from word to sentence and beyond-has proved to be non-trivial.", "labels": [], "entities": []}, {"text": "A new framework, which we refer to as DisCoCat, initially presented in Clark, Coecke, and and reconciles distributional approaches to natural language semantics with the structured, logical nature of formal semantic models.", "labels": [], "entities": []}, {"text": "This framework is abstract; its theoretical predictions have not been evaluated on real data, and its applications to empirical natural language processing tasks have not been studied.", "labels": [], "entities": []}, {"text": "This article is the journal version of, which fill this gap in the DisCoCat literature; in it, we develop a concrete model and an unsupervised learning algorithm to instantiate the abstract vectors, linear maps, and vector spaces of the theoretical framework; we develop a series of empirical natural language processing experiments and data sets and implement our algorithm on large scale real data; we analyze the outputs of the algorithm in terms of linear algebraic equations; and we evaluate the model on these experiments and compare the results with other competing unsupervised models.", "labels": [], "entities": [{"text": "DisCoCat literature", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.8517568409442902}]}, {"text": "Furthermore, we provide a linear algebraic analysis of the algorithm of and present an in-depth study of the better performance of the method of.", "labels": [], "entities": []}, {"text": "We begin in Section 2 by presenting the background to the task of developing compositional distributional models.", "labels": [], "entities": []}, {"text": "We briefly introduce two approaches to semantic modeling: formal semantic models and distributional semantic models.", "labels": [], "entities": [{"text": "semantic modeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9000073969364166}]}, {"text": "We discuss their differences, and relative advantages and disadvantages.", "labels": [], "entities": []}, {"text": "We then present and critique various approaches to bridging the gap between these models, and their limitations.", "labels": [], "entities": []}, {"text": "In Section 3, we summarize the categorical compositional distributional framework of and and provide the theoretical background necessary to understand it; we also sketch the road map of the literature leading to the development of this setting and outline the contributions of this paper to the field.", "labels": [], "entities": []}, {"text": "In Section 4, we present the details of an implementation of this framework, and introduce learning algorithms used to build semantic representations in this implementation.", "labels": [], "entities": []}, {"text": "In Section 5, we present a series of experiments designed to evaluate this implementation against other unsupervised distributional compositional models.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we discuss these results, and posit future directions for this research area.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating compositional models of semantics is no easy task: First, we are trying to evaluate how well the compositional process works; second, we also are trying to determine how useful the final representation-the output of the composition-is, relative to our needs.", "labels": [], "entities": []}, {"text": "The scope of this second problem covers most phrase and sentence-level semantic models, from bag-of-words approaches in information retrieval to logic-based formal semantic models, via language models used for machine translation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.7220553010702133}, {"text": "machine translation", "start_pos": 210, "end_pos": 229, "type": "TASK", "confidence": 0.7245531976222992}]}, {"text": "It is heavily task dependent, in that a representation that is suitable for machine translation may not be appropriate for textual inference tasks, and one that is appropriate for IR may not be ideal for paraphrase detection.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7183854430913925}, {"text": "IR", "start_pos": 180, "end_pos": 182, "type": "TASK", "confidence": 0.9774990081787109}, {"text": "paraphrase detection", "start_pos": 204, "end_pos": 224, "type": "TASK", "confidence": 0.9582872092723846}]}, {"text": "Therefore this aspect of semantic model evaluation ideally should take the form of application-oriented testing.", "labels": [], "entities": [{"text": "semantic model evaluation", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.766012171904246}]}, {"text": "For instance, to test semantic representations designed for machine translation purposes, we should use a machine translation evaluation task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7296240925788879}, {"text": "machine translation evaluation", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.7616039315859476}]}, {"text": "The DisCoCat framework, described in Section 3, allows for the composition of any words of any syntactic type.", "labels": [], "entities": []}, {"text": "The general learning algorithm presented in Section 4.3 technically can be applied to learn and model relations of any semantic type.", "labels": [], "entities": []}, {"text": "However, many open questions remain, such as how to deal with logical words, determiners, and quantification, and how to reconcile the different semantic types used for sentences with transitive and intransitive sentences.", "labels": [], "entities": []}, {"text": "We will leave these questions for future work, briefly discussed in Section 6.", "labels": [], "entities": []}, {"text": "In the meantime, we concretely are left with away of satisfactorily modeling only simple sentences without having to answer these bigger questions.", "labels": [], "entities": []}, {"text": "With this in mind, in this section we present a series of experiments centered around evaluating how well various models of semantic vector composition perform (along with the one described in Section 4) in a phrase similarity comparison task.", "labels": [], "entities": [{"text": "semantic vector composition", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.6646967728932699}, {"text": "phrase similarity comparison task", "start_pos": 209, "end_pos": 242, "type": "TASK", "confidence": 0.8412982523441315}]}, {"text": "This task aims to test the quality of a compositional process by determining how well it forms a clear joint meaning for potentially ambiguous words.", "labels": [], "entities": []}, {"text": "The intuition here is that tokens, on their own, can have several meanings; and that it is through the compositional process-through giving them context-that we understand their specific meaning.", "labels": [], "entities": []}, {"text": "For example, bank itself could (among other meanings) mean a riverbank or a financial bank; yet in the context of a sentence such as The bank refunded the deposit, it is likely we are talking about the financial institution.", "labels": [], "entities": []}, {"text": "In this section, we present three data sets designed to evaluate how well word-sense disambiguation occurs as a byproduct of composition.", "labels": [], "entities": []}, {"text": "We begin by describing the first data set, based around noun-intransitive verb phrases, in Section 5.1.", "labels": [], "entities": []}, {"text": "In Section 5.2, we present a data set based around short transitive-verb phrases (a transitive verb with subject and object).", "labels": [], "entities": []}, {"text": "In Section 5.3, we discuss anew data set, based around short transitive-verb phrases where the subject and object are qualified by adjectives.", "labels": [], "entities": []}, {"text": "We leave discussion of these results for Section 6.", "labels": [], "entities": []}, {"text": "This first experiment, originally presented in, evaluates the degree to which an ambiguous intransitive verb (e.g., draws) is disambiguated by combination with its subject.", "labels": [], "entities": []}, {"text": "The data set 4 comprises 120 pairs of intransitive sentences, each of the form NOUN VERB.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.5715382099151611}, {"text": "VERB", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.591439425945282}]}, {"text": "These sentence pairs are generated according to the following procedure, which will be the basis for the construction of the other data sets discussed subsequently: 1.", "labels": [], "entities": []}, {"text": "A number of ambiguous intransitive verbs (15, in this case) are selected from frequently occurring verbs in the corpus.", "labels": [], "entities": []}, {"text": "The first experiment was followed by a second similar experiment, in, covering different sorts of composition operations for binary combinations of syntactic types (adjective-noun, noun-noun, verb-object).", "labels": [], "entities": []}, {"text": "Such further experiments are interesting, but rather than continue down this binary road, we now turn to the development of our second experiment, involving sentences with larger syntactic structures, to examine how well various compositional models cope with more complex syntactic and semantic relations.", "labels": [], "entities": []}, {"text": "This second experiment, which we initially presented in, is an extension of the first in the case of sentences centered around transitive verbs, composed with a subject and an object.", "labels": [], "entities": []}, {"text": "The results of the first experiment did not demonstrate any difference between the multiplicative model, which takes into account no syntactic information or word ordering, and our syntactically motivated categorical compositional model.", "labels": [], "entities": []}, {"text": "By running the same experiment over anew data set, where the relations expressed by the verb have a higher arity than in the first, we hope to demonstrate that added structure leads to better results for our syntax-sensitive model.", "labels": [], "entities": []}, {"text": "The construction procedure for this data set 6 is almost exactly as for the first data set, with the following differences: r Verbs are transitive instead of intransitive.", "labels": [], "entities": []}, {"text": "r We arbitrarily took 10 verbs from the most frequent verbs in the BNC, and for each verb, took two maximally distant synonyms (again, using WordNet) to obtain 10 pairs of pairs.", "labels": [], "entities": [{"text": "BNC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9418529868125916}, {"text": "WordNet", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9522092342376709}]}, {"text": "r For each pair of verb pairs, we selected a set of subject and object nouns to use as context, as opposed to just a subject noun.", "labels": [], "entities": []}, {"text": "r Each subject-object pair was manually chosen so that one of the verb pairs would have high similarity in the context of that subject and object, and the other would have low similarity.", "labels": [], "entities": []}, {"text": "We used these choices to annotate the entries with HIGH and LOW tags.", "labels": [], "entities": [{"text": "HIGH", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.87606281042099}, {"text": "LOW", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9548609256744385}]}, {"text": "r Each combination of a verb pair with a subject-object pair constitutes an entry of our data set, of which there are 200.", "labels": [], "entities": []}, {"text": "As a form of quality control, we inserted \"gold standard\" sentences in the form of identical sentence pairs and rejected annotators who did not score these gold standard sentences with a high score of 6 or 7.", "labels": [], "entities": []}, {"text": "Lemmatized sentences from sample entries of this data set and our HIGH-LOW tags for them are shown in.", "labels": [], "entities": [{"text": "HIGH-LOW", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.7977573275566101}]}, {"text": "The data set was passed to a group of 50 annotators on Amazon Mechanical Turk, as for the previous data set.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.948511004447937}]}, {"text": "The annotators were shown, for each entry, a pair of sentences created by adding the in front of the subject and object nouns and putting the verbs in the past tense, 8 and were instructed to score each pair of sentences on the same scale of 1 (not similar in meaning) to 7 (similar in meaning), based on how similar in meaning they believed the sentence pair was.", "labels": [], "entities": []}, {"text": "The methodology for this experiment is exactly that of the previous experiment.", "labels": [], "entities": []}, {"text": "Models compositionally construct sentence representations, and compare them using a distance metric (all vector-based models once again used cosine similarity).", "labels": [], "entities": []}, {"text": "The rank correlation of models scores with annotator scores is calculated using Spearman's \u03c1, which is in turn used to rank models.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.5053543547789255}]}, {"text": "The models compared in this experiment are those of the first experiment, with the addition of an extra trigram-based baseline (trained with SRILM, using the addition of log-probability of the sentence as a similarity metric), and a variation on our categorical model, presented subsequently.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.6612743735313416}]}, {"text": "With W as the distributional semantic space for all words in the corpus, trained using the same parameters as in the first experiment, and \u2212 \u2212 \u2192 subj, \u2212 \u2212 \u2192 verb, \u2212 \u2212\u2212 \u2192 object \u2208 W as the vectors for subject, verb, and object of a sentence, respectively, and with verb cat as the compact representation of a transitive verb learned using the algorithm presented in this paper, we have the following compositional methods: All of these models have been explained earlier, with the exception of Kronecker.", "labels": [], "entities": []}, {"text": "We first presented this new addition in, where we observed that the compact representation of a verb in the DisCoCat framework, under the assumptions presented in Section 4, can be viewed as dim(N) \u00d7 dim(N) matrices in N \u2297 N.", "labels": [], "entities": []}, {"text": "We considered alternatives to the algorithm presented earlier for the construction of such matrices, and were surprised by the results of the Kronecker method, wherein we replaced the matrix learned by our algorithm with the Kronecker product of the lexical semantic vectors for the verb.", "labels": [], "entities": []}, {"text": "Further analysis performed since the publication of that paper can help to understand why this method might work.", "labels": [], "entities": []}, {"text": "Using the following property, for any vectors \u2212 \u2192 a , we can see that the Kronecker model's composition operation can be expressed as  The third and final experiment we present is a modified version of the second data set presented earlier, where the nouns in each entry are under the scope of adjectives applied to them.The intuition behind the data sets presented in Section 5.1 and Section 5.2 was that ambiguous verbs are disambiguated through composition with nouns.", "labels": [], "entities": []}, {"text": "These nouns themselves may also be ambiguous, and a good compositional model will be capable of separating the noise produced by other meanings through its compositional mechanism to produce unambiguous phrase representations.", "labels": [], "entities": []}, {"text": "The intuition behind this data set is similar, in that adjectives provide both additional information for disambiguation of the nouns they apply to, but also additional semantic noise.", "labels": [], "entities": []}, {"text": "Therefore, a  Example entries from the adjective-transitive data set without annotator score, third experiment.", "labels": [], "entities": []}, {"text": "good model will also be able to separate the useful information of the adjective from its semantic noise when composing it with its argument, in addition to doing this when composing the noun phrases with the verb.", "labels": [], "entities": []}, {"text": "The construction procedure for this data set was to take the data set from Section 5.2, and, for each entry, add a pair of adjectives from those most frequently occurring in the corpus.", "labels": [], "entities": [{"text": "Section 5.2", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9116975963115692}]}, {"text": "The first adjective from the pair is applied to the first noun (subject) of the entry when forming the sentences, and the second adjective is applied to the second noun (object).", "labels": [], "entities": []}, {"text": "For each entry, we chose adjectives which best preserved the meaning of the phrase constructed by combining the first verb with its subject and object.", "labels": [], "entities": []}, {"text": "This new data set 9 was then annotated again by a group of 50 annotators using Amazon's Mechanical Turk service.", "labels": [], "entities": []}, {"text": "The annotators were shown, for each entry, a pair of sentences created by adding the in front of the subject and object noun phrases and putting the verbs in the past tense, 10 and asked to give each sentence pair a meaning similarity score between 1 and 7, as for the previous data sets.", "labels": [], "entities": []}, {"text": "We applied the same quality control mechanism as in the second experiment.", "labels": [], "entities": []}, {"text": "Some 94 users returned annotations, of which we kept 50 according to our gold standard tests.", "labels": [], "entities": []}, {"text": "We are unaware of whether or not it was applied in the production of the first data set, but believe that this can only lead to the production of higher quality annotations.", "labels": [], "entities": []}, {"text": "Sample sentences from this data set are shown in.", "labels": [], "entities": []}, {"text": "The evaluation methodology in this experiment is identical to that of the previous experiments.", "labels": [], "entities": []}, {"text": "In this experiment, in lieu of simply comparing compositional models \"across the board\" (e.g., using the multiplicative model for both adjective-noun composition and verb-argument composition), we experimented with different combinations of models.", "labels": [], "entities": []}, {"text": "This evaluation procedure was chosen because we believe that adjective-noun composition need not necessarily be the same kind of compositional process as subjectverb-object composition, and also because different models may latch onto different semantic features during the compositional process, and it would be interesting to see what model mixtures work well together.", "labels": [], "entities": []}, {"text": "Naturally, this is not a viable approach to selecting composition operations in general, as we will not have the luxury of trying every combination of composition operations for every combination of syntactic types, but it is worthwhile performing these tests to at least verify the hypothesis that operation-specific composition operations (or parameters) is a good thing.", "labels": [], "entities": []}, {"text": "Notably, this idea has been explored very recently, within the context of deep learning networks, by.", "labels": [], "entities": []}, {"text": "Each mixed model has two components: a verb-argument composition model and a adjective-noun composition model.", "labels": [], "entities": []}, {"text": "For verb-argument composition, we used the three best models from the previous experiment, namely, Multiply, Categorical, and Kronecker.", "labels": [], "entities": [{"text": "verb-argument composition", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7437184751033783}, {"text": "Multiply", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9449726343154907}]}, {"text": "For adjective composition we used three different methods of adjectivenoun composition.", "labels": [], "entities": [{"text": "adjective composition", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7752899825572968}, {"text": "adjectivenoun composition", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7313167601823807}]}, {"text": "With \u2212 \u2212\u2212\u2212\u2212 \u2192 adjective and \u2212\u2212\u2192 noun being the vectors for an adjective and a noun in our distributional lexical semantic space W (built using the same procedure as the previous experiments) and \u2212\u2212\u2192 adj cat being the compact representation in the Categorical model, built according to the algorithm from Section 4.3, we have the following models: In addition to these models, we also evaluated three baselines: Verb Baseline, Bigram Baseline, and Trigram Baseline.", "labels": [], "entities": []}, {"text": "As in previous experiments, the verb baseline uses the verb vector as sentence vector, ignoring the information provided by other words.", "labels": [], "entities": []}, {"text": "The bigram and trigram baselines are calculated from the same language model as used in the second experiment.", "labels": [], "entities": []}, {"text": "In both cases, the log-probability of each sentence is calculated using SRLIM, and the sum of log-probabilities of two sentences is used as a similarity measure.", "labels": [], "entities": [{"text": "SRLIM", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.7723031044006348}]}, {"text": "Finally, for comparison, we also considered the full additive model: Results.", "labels": [], "entities": []}, {"text": "The results for the third experiment are shown in.", "labels": [], "entities": []}, {"text": "The best performing adjective-noun combination operations for each verb-argument combination operation are shown in bold.", "labels": [], "entities": []}, {"text": "Going through the combined models, we notice that inmost cases the results stay the same whether the adjective-noun combination method is AdjMult or CategoricalAdj.", "labels": [], "entities": [{"text": "AdjMult", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.8971440196037292}]}, {"text": "This is because, as was shown in the first experiment, composition of a unary-relation such as an adjective or intransitive verb with its sole argument, under the categorical model with reduced representations, is mathematically equivalent to the multiplicative model.", "labels": [], "entities": []}, {"text": "The sole difference is the way the adjective or intransitive verb vector is constructed.", "labels": [], "entities": []}, {"text": "We note, however, that with Categorical as a verb-argument composition method, the CategoricalAdj outperforms AdjMult by a non-negligible margin (0.19 vs. 0.14), indicating that the difference in learning procedure can lead to different results depending on what other models it is combined with.", "labels": [], "entities": []}, {"text": "Overall, the best results are obtained for AdjMult+Kronecker (\u03c1 = 0.26) and CategoricalAdj+Kronecker (\u03c1 = 0.27).", "labels": [], "entities": []}, {"text": "Combinations of the adjective composition methods with other composition methods at best matches the best-performing baseline, Verb Baseline.", "labels": [], "entities": []}, {"text": "In all cases, the holistic model AdjNoun provides the worst results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Sample weights for selected noun vectors.", "labels": [], "entities": []}, {"text": " Table 2  Sample semantic matrix for show.  far  room scientific  elect  far  79.24  47.41  119.96  27.72  room  232.66  80.75  396.14  113.2  scientific  32.94  31.86  32.94  0  elect  0  0  0  0", "labels": [], "entities": []}, {"text": " Table 4  Model correlation coefficients with human judgments, first experiment. p < 0.05 for each \u03c1.", "labels": [], "entities": []}, {"text": " Table 6  Model correlation coefficients with human judgments, second experiment. p < 0.05 for each \u03c1.", "labels": [], "entities": [{"text": "correlation", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9476660490036011}]}, {"text": " Table 8  Model correlation coefficients with human judgments, third experiment. p < 0.05 for each \u03c1.", "labels": [], "entities": []}]}