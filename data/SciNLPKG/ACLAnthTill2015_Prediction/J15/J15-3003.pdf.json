{"title": [{"text": "The Unified and Holistic Method Gamma (\u03b3) for Inter-Annotator Agreement Measure and Alignment", "labels": [], "entities": [{"text": "Inter-Annotator Agreement Measure and Alignment", "start_pos": 46, "end_pos": 93, "type": "TASK", "confidence": 0.7573715329170227}]}], "abstractContent": [{"text": "Agreement measures have been widely used in computational linguistics for more than 15 years to check the reliability of annotation processes.", "labels": [], "entities": []}, {"text": "Although considerable effort has been made concerning categorization, fewer studies address unitizing, and when both paradigms are combined even fewer methods are available and discussed.", "labels": [], "entities": []}, {"text": "The aim of this article is threefold.", "labels": [], "entities": []}, {"text": "First, we advocate that to deal with unitizing, alignment and agreement measures should be considered as a unified process, because a relevant measure should rely on an alignment of the units from different annotators, and this alignment should be computed according to the principles of the measure.", "labels": [], "entities": [{"text": "unitizing", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.9657955169677734}]}, {"text": "Second, we propose the new versatile measure \u03b3, which fulfills this requirement and copes with both paradigms, and we introduce its implementation.", "labels": [], "entities": []}, {"text": "Third, we show that this new method performs as well as, or even better than, other more specialized methods devoted to categorization or segmentation, while combining the two paradigms at the same time.", "labels": [], "entities": [{"text": "categorization or segmentation", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.6024816632270813}]}], "introductionContent": [{"text": "A growing body of work in computational linguistics (CL hereafter) or natural language processing manifests an interest in corpus studies, and requires reference annotations for system evaluation or machine learning purposes.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.6963073213895162}]}, {"text": "The question is how to ensure that an annotation can be considered, if not as the \"truth,\" than at least as a suitable reference.", "labels": [], "entities": []}, {"text": "For some simple and systematic tasks, domain experts maybe able to annotate texts with almost total confidence, but this is generally not the case when no expert is available, or when the tasks become harder.", "labels": [], "entities": []}, {"text": "The very notion of \"truth\" may even be utopian when the annotation process includes a certain degree of interpretation, and we should in such cases look fora consensus, also called the \"gold standard,\" rather than for the \"truth.\"", "labels": [], "entities": []}, {"text": "For these reasons, a classic strategy for building annotated corpora with sufficient confidence is to give the same annotation task to several annotators, and to analyze to what extent they agree in order to assess the reliability of their annotations.", "labels": [], "entities": []}, {"text": "This is the aim of inter-annotator agreement measures.", "labels": [], "entities": []}, {"text": "It is important to point out that most of these measures do not evaluate the distance from annotations to the \"truth,\" but rather the distance across annotators.", "labels": [], "entities": []}, {"text": "Of course, the hope is that the annotators will agree as far as possible, and it is usually considered that a good inter-annotator agreement ensures the constancy and the reproducibility of the annotations: When agreement is high, then the task is consistent and correctly defined, and the annotators can be expected to agree on another part of the corpus, or at another time, and their annotations therefore constitute a consensual reference (even if, as shown for example by, such an agreement is not necessarily informative for machine learning purposes).", "labels": [], "entities": []}, {"text": "Moreover, once several annotators reach good agreement on a given part of a corpus, then each of them can annotate alone other parts of the corpus with great confidence in the reproducibility (see the preface to Gwet for illuminating considerations).", "labels": [], "entities": []}, {"text": "Consequently, inter-annotator agreement measurement is an important point for all annotation efforts because it is often considered that a given agreement value provided by a given method validates or invalidates the consistency of an annotation effort.", "labels": [], "entities": []}, {"text": "How to measure agreement, and how we define a good measure, is another part of the problem.", "labels": [], "entities": []}, {"text": "There is no universal answer, because how to measure depends on the nature of the task, hence on the kind of annotations.", "labels": [], "entities": []}, {"text": "Admittedly, much work has already been done for some kinds of annotation efforts, namely, when annotators have to choose a category for previously identified entities.", "labels": [], "entities": []}, {"text": "This approach, which we will call pure categorization, has led to several well-known and widely discussed coefficients such as \u03ba, \u03c0, or \u03b1, since the 1950s.", "labels": [], "entities": []}, {"text": "Some more recent efforts have been made in the domain of unitizing, following Krippendorff's terminology, where annotators have to identify by themselves what the elements to be annotated in a text are, and where they are located.", "labels": [], "entities": [{"text": "unitizing", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.9638912677764893}]}, {"text": "Studies are scarce, however, as Krippendorff pointed out: \"Measuring the reliability of unitizing has been largely ignored in favor of coding predefined units\" (.", "labels": [], "entities": [{"text": "reliability", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9787735342979431}, {"text": "unitizing", "start_pos": 88, "end_pos": 97, "type": "TASK", "confidence": 0.9601249694824219}]}, {"text": "This scarcity concerns either segmentation, where annotators simply have to mark boundaries in texts to separate contiguous segments, or more generally unitizing, where gaps may exist between units.", "labels": [], "entities": []}, {"text": "Moreover, some even more complex configurations may occur (overlapping or embedding units), which are more rarely taken into account.", "labels": [], "entities": []}, {"text": "And when categorization meets unitizing, as is the casein CL in such fields as, for example, NAMED ENTITY RECOGNITION 1 or DISCOURSE FRAMING, very few methods are proposed and discussed.", "labels": [], "entities": [{"text": "NAMED ENTITY RECOGNITION 1", "start_pos": 93, "end_pos": 119, "type": "METRIC", "confidence": 0.8640044629573822}, {"text": "FRAMING", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.855351448059082}]}, {"text": "That is the main problem we focus on in this article and to which \u03b3 provides solutions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3  Categorial matrix of dist cat for 3 categories.", "labels": [], "entities": []}]}