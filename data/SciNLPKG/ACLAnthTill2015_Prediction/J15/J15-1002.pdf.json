{"title": [{"text": "Cross-lingual Sentiment Lexicon Learning With Bilingual Word Graph Label Propagation", "labels": [], "entities": [{"text": "Cross-lingual Sentiment Lexicon Learning", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7165145948529243}]}], "abstractContent": [{"text": "In this article we address the task of cross-lingual sentiment lexicon learning, which aims to automatically generate sentiment lexicons for the target languages with available English sentiment lexicons.", "labels": [], "entities": [{"text": "cross-lingual sentiment lexicon learning", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.7760722488164902}]}, {"text": "We formalize the task as a learning problem on a bilingual word graph, in which the intra-language relations among the words in the same language and the inter-language relations among the words between different languages are properly represented.", "labels": [], "entities": []}, {"text": "With the words in the English sentiment lexicon as seeds, we propose a bilingual word graph label propagation approach to induce sentiment polarities of the unlabeled words in the target language.", "labels": [], "entities": [{"text": "bilingual word graph label propagation", "start_pos": 71, "end_pos": 109, "type": "TASK", "confidence": 0.6133000969886779}]}, {"text": "Particularly, we show that both synonym and antonym word relations can be used to build the intra-language relation, and that the word alignment information derived from bilingual parallel sentences can be effectively leveraged to build the inter-language relation.", "labels": [], "entities": []}, {"text": "The evaluation of Chinese sentiment lexicon learning shows that the proposed approach outperforms existing approaches in both precision and recall.", "labels": [], "entities": [{"text": "Chinese sentiment lexicon learning", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.6386457532644272}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9994587302207947}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9946905970573425}]}, {"text": "Experiments conducted on the NTCIR data set further demonstrate the effectiveness of the learned sentiment lexicon in sentence-level sentiment classification.", "labels": [], "entities": [{"text": "NTCIR data set", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9757280349731445}, {"text": "sentence-level sentiment classification", "start_pos": 118, "end_pos": 157, "type": "TASK", "confidence": 0.7612490355968475}]}], "introductionContent": [{"text": "A sentiment lexicon is regarded as the most valuable resource for sentiment analysis, and lays the groundwork of much sentiment analysis research, for example, sentiment classification ( and opinion summarization (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9516108334064484}, {"text": "sentiment analysis", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8134860098361969}, {"text": "sentiment classification", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.9074872732162476}, {"text": "opinion summarization", "start_pos": 191, "end_pos": 212, "type": "TASK", "confidence": 0.6927620023488998}]}, {"text": "To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.9023189842700958}]}, {"text": "The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet).", "labels": [], "entities": [{"text": "word sentiment polarities", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7413435081640879}, {"text": "WordNet", "start_pos": 304, "end_pos": 311, "type": "DATASET", "confidence": 0.9310953617095947}]}, {"text": "However, current work mainly focuses on English sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied.", "labels": [], "entities": [{"text": "English sentiment lexicon generation or expansion", "start_pos": 40, "end_pos": 89, "type": "TASK", "confidence": 0.7180145432551702}, {"text": "sentiment lexicon learning", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.8372213045756022}]}, {"text": "In this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons fora non-English language (hereafter referred to as \"the target language\") with the help of the available English sentiment lexicons.", "labels": [], "entities": [{"text": "cross-lingual sentiment lexicon learning", "start_pos": 41, "end_pos": 81, "type": "TASK", "confidence": 0.7701507210731506}]}, {"text": "The underlying motivation of this task is to leverage the existing English sentiment lexicons and substantial linguistic resources to label the sentiment polarities of the words in the target language.", "labels": [], "entities": []}, {"text": "To this end, we need an approach to transferring the sentiment information from English words to the words in the target language.", "labels": [], "entities": [{"text": "transferring the sentiment information from English words", "start_pos": 36, "end_pos": 93, "type": "TASK", "confidence": 0.8070349948746818}]}, {"text": "The few existing approaches first build word relations between English and the target language.", "labels": [], "entities": []}, {"text": "Then, based on the word relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language.", "labels": [], "entities": []}, {"text": "In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages.", "labels": [], "entities": [{"text": "transfer of sentiment information", "start_pos": 97, "end_pos": 130, "type": "TASK", "confidence": 0.7356465011835098}]}, {"text": "Two approaches are often used to connect the words in different languages in the literature.", "labels": [], "entities": []}, {"text": "One is based on translation entries in cross-lingual dictionaries ().", "labels": [], "entities": []}, {"text": "The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language ().", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8360924363136292}]}, {"text": "The two approaches in and tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning.", "labels": [], "entities": [{"text": "cross-lingual sentiment lexicon learning", "start_pos": 79, "end_pos": 119, "type": "TASK", "confidence": 0.8084690570831299}]}, {"text": "Specifically, we model this task with a bilingual word graph, which is composed of two intra-language subgraphs and an interlanguage subgraph.", "labels": [], "entities": []}, {"text": "The intra-language subgraphs are used to model the semantic relations among the words in the same languages.", "labels": [], "entities": []}, {"text": "When building them, we incorporate both synonym and antonym word relations in a novel manner, represented by positive and negative sign weights in the subgraphs, respectively.", "labels": [], "entities": []}, {"text": "These two intra-language subgraphs are then connected by the inter-language subgraph.", "labels": [], "entities": []}, {"text": "We propose Bilingual word graph Label Propagation (BLP), which simultaneously takes the inter-language relations and the intra-language relations into account in an iterative way.", "labels": [], "entities": [{"text": "Bilingual word graph Label Propagation (BLP", "start_pos": 11, "end_pos": 54, "type": "TASK", "confidence": 0.5516475822244372}]}, {"text": "Moreover, we leverage the word alignment information derived from a parallel corpus to build the inter-language relations.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7112008184194565}]}, {"text": "We connect two words from different languages that are aligned to each other in a parallel sentence pair.", "labels": [], "entities": []}, {"text": "Taking advantage of a large parallel corpus, this approach significantly improves the coverage of the generated sentiment lexicon.", "labels": [], "entities": []}, {"text": "The experimental results on Chinese sentiment lexicon learning show the effectiveness of the proposed approach in terms of both precision and recall.", "labels": [], "entities": [{"text": "Chinese sentiment lexicon learning", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.7557422369718552}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9994696974754333}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9974986910820007}]}, {"text": "We further evaluate the impact of the learned sentiment lexicon on sentence-level sentiment classification.", "labels": [], "entities": [{"text": "sentence-level sentiment classification", "start_pos": 67, "end_pos": 106, "type": "TASK", "confidence": 0.7661967476209005}]}, {"text": "When using words in the learned sentiment lexicon as features for sentiment classification of the target language, the sentiment classification can achieve a high performance.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.8645438551902771}, {"text": "sentiment classification", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.8826131522655487}]}, {"text": "We make the following contributions in this article.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this set of experiments, we examine the influence of graph topologies on sentiment lexicon learning.", "labels": [], "entities": [{"text": "sentiment lexicon learning", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.922644297281901}]}, {"text": "Mono: This approach learns the Chinese sentiment lexicon based only on the Chinese monolingual word graph . Because it needs labeled sentiment words, we incorporate the English labeled sentiment words X E and the interlanguage relation WA in the first iteration.", "labels": [], "entities": [{"text": "WA", "start_pos": 236, "end_pos": 238, "type": "METRIC", "confidence": 0.6288365721702576}]}, {"text": "Then we set X E and WA to be zero in later iterations.", "labels": [], "entities": [{"text": "WA", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9924387335777283}]}, {"text": "In these approaches, \u00b5 is set to 0.1 as in.", "labels": [], "entities": [{"text": "\u00b5", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.972701370716095}]}, {"text": "The precision of these approaches are shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9986203908920288}]}, {"text": "The figure shows that the approaches based on the bilingual word graph significantly outperform the one based on the monolingual word graph.", "labels": [], "entities": []}, {"text": "The bilingual word graph can bring in more word relations and accelerate the sentiment propagation.", "labels": [], "entities": [{"text": "sentiment propagation", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.946061372756958}]}, {"text": "Besides, in the bilingual word graph, the English sentiment seed words can continually provide accurate sentiment information.", "labels": [], "entities": []}, {"text": "Thus we observe the increase in the approaches based on the bilingual word graph in term of both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9994558691978455}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.99713134765625}]}, {"text": "Meanwhile, we find that adding the antonym relation in the bilingual word graph slightly enhances precision in top-ranked words and similar findings are observed in our later experiments.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.998650848865509}]}, {"text": "It appears that the antonym relations depict word relations in a more accurate way and can refine the word sentiment scores more precisely.", "labels": [], "entities": []}, {"text": "However, the synonym relation and word alignment relation dominate, whereas the antonym relation accounts for only a small percentage of the graph.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7608446180820465}]}, {"text": "It is hard for the antonym relation to introduce new relations into the graph and thus it cannot help to further improve recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9944136738777161}]}, {"text": "This set of experiments is to examine the ways to build the inter-language relation.", "labels": [], "entities": []}, {"text": "BLP-dict: The inter-language relation is built upon the translation entries from LDC 7 and Universal Dictionary (UD).", "labels": [], "entities": [{"text": "BLP-dict", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9697934985160828}, {"text": "Universal Dictionary (UD)", "start_pos": 91, "end_pos": 116, "type": "DATASET", "confidence": 0.908727765083313}]}, {"text": "8 From these dictionaries (both English-Chinese and Chinese-English dictionaries), we collect 41,034 translation entries between the English and Chinese words.", "labels": [], "entities": []}, {"text": "If the English word xi can be translated to the Chinese word x j in UD dictionary, w A (i, j) and w A (j, i) are set to 1.", "labels": [], "entities": [{"text": "UD dictionary", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9632823765277863}]}, {"text": "BLP-MT: All the Chinese (English) words are translated into English (Chinese) by Google Translator.", "labels": [], "entities": [{"text": "BLP-MT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5765013694763184}]}, {"text": "If the Chinese word xi can be translated to the English word x j , thew A (i, j) and w A (j, i) are set to 1.", "labels": [], "entities": []}, {"text": "If a Chinese word is translated to an English phrase, we assume that the Chinese word is projected to each word in the English phrase.", "labels": [], "entities": []}, {"text": "To improve the coverage, we translate the English sentiment seed words with three other methods; they are word collocation, coordinated phrase, and punctuation, as mentioned in.", "labels": [], "entities": []}, {"text": "The learned Chinese sentiment word lists are also evaluated with precision at k.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9980087876319885}]}, {"text": "As shown in, we find that the alignment-based approach outperforms the dictionary-based and MT-based approaches.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 92, "end_pos": 100, "type": "TASK", "confidence": 0.9060171246528625}]}, {"text": "The reason that contributes to this is that we can build more inter-language relations based on word alignment, compared with the translation entries from the dictionary and the translation pairs from Google Translator.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.7066684365272522}]}, {"text": "For example, the English word move is often translated to (shift) and (affect, touch) by dictionaries or MT engines.", "labels": [], "entities": [{"text": "MT engines", "start_pos": 105, "end_pos": 115, "type": "TASK", "confidence": 0.8891414403915405}]}, {"text": "From the parallel sentences, besides these word translation pairs, the word move can be also aligned to (plain sailing bon voyage) that is commonly used in Chinese greeting texts.", "labels": [], "entities": []}, {"text": "This translation entry is hard to find in dictionaries or by MT engines.", "labels": [], "entities": []}, {"text": "The words are aligned between the two parallel sentences.", "labels": [], "entities": []}, {"text": "Sometimes the word move maybe forced to be aligned to in the parallel sentences good luck and best wishes on your career move and . Thus, when building the inter-language relations with word alignment, our approach is likely to learn more sentiment word candidates.", "labels": [], "entities": []}, {"text": "It is also the reason why the dictionary-based and MT-based approaches learn fewer sentiment words than our approach, as indicated in.", "labels": [], "entities": []}, {"text": "According to our statistic, on average a Chinese word is connected to 2.3 and 2.1 English words if we build the inter-language relations with the dictionary and Google Translator, respectively.", "labels": [], "entities": []}, {"text": "By building the inter-language relation with word alignment, our approach connects a Chinese word to 16.21 English words an average, which greatly increases the coverage of the learned sentiment lexicon.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7545992434024811}]}, {"text": "The following set of experiments reveals the influence of the intra-language relation.", "labels": [], "entities": []}, {"text": "BLP-A: As the baseline of this set of experiments, it does not build the intralanguage relations with either English or Chinese WordNet synsets.", "labels": [], "entities": [{"text": "BLP-A", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.941457986831665}]}, {"text": "Only the Influence on Recall of the inter-language relation.", "labels": [], "entities": [{"text": "Influence", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.94927978515625}, {"text": "Recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.8612427115440369}]}, {"text": "As shows, when combining both English and Chinese intra-language relations, the precision curves of both positive and negative predictions increase.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994779229164124}]}, {"text": "This indicates that adding the intra-language relations has a positive influence.", "labels": [], "entities": []}, {"text": "The improvement can be explained by the ability of the intra-language relations to refine the polarity scores.", "labels": [], "entities": []}, {"text": "For example, the English word sophisticated can be aligned to the positive Chinese word (delicate) as well as the negative Chinese word (wily, wicked).", "labels": [], "entities": []}, {"text": "In the GI lexicon, the English word sophisticated is labeled as positive.", "labels": [], "entities": []}, {"text": "In the bilingual word graph that contains only the inter-language relations, the negative Chinese word is likely to be labeled as positive.", "labels": [], "entities": []}, {"text": "However, with the intra-language relation, the negative Chinese word may connect to the other negative Chinese words, like (foxy); and the Chinese positive word may connect to the other positive Chinese words, like (elaborate).", "labels": [], "entities": []}, {"text": "Thus the polarity score of the word can be refined by the intra-language relation in each iteration of propagation.", "labels": [], "entities": []}, {"text": "Another advantage of the intra-language relation is that it helps to reduce the noise introduced by the inter-language relation.", "labels": [], "entities": []}, {"text": "For example, sometimes the Chinese positive word (help) is misaligned to the negative English word freak by the inter-language relation, but it is also connected to the synonyms (help) and (salutary) (which are positive) by the intra-language relations.", "labels": [], "entities": []}, {"text": "The polarity score of the word can be adjusted by the intra-language relation.", "labels": [], "entities": []}, {"text": "Thus, though the inter-language relation brings in certain noisy alignments, the intra-language relation can help to refine the polarity score of the word using its intra-language relation.", "labels": [], "entities": []}, {"text": "Sentiment classification is one of the most extensively studied tasks in the community of sentiment analysis.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9712645411491394}, {"text": "sentiment analysis", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.9386108219623566}]}, {"text": "To see whether the performance improvement in lexicon learning also improves the results of sentiment classification, we apply the generated Chinese sentiment lexicons to sentence-level sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8986973762512207}, {"text": "sentence-level sentiment classification", "start_pos": 171, "end_pos": 210, "type": "TASK", "confidence": 0.715375562508901}]}, {"text": "Data set: The NTCIR sentiment-labeled corpus is used for sentiment classification (.", "labels": [], "entities": [{"text": "NTCIR sentiment-labeled corpus", "start_pos": 14, "end_pos": 44, "type": "DATASET", "confidence": 0.936156690120697}, {"text": "sentiment classification", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.9278677701950073}]}, {"text": "We extract the Chinese sentences that have positive, negative, or neutral labels.", "labels": [], "entities": []}, {"text": "The numbers of extracted sentences are shown in.", "labels": [], "entities": []}, {"text": "The learned sentiment words in the Mono and BLP approaches are used as classification features.", "labels": [], "entities": []}, {"text": "We implement the following baselines for comparison.", "labels": [], "entities": []}, {"text": "BSL DF: The Chinese word unigrams and bigrams are extracted from the NTCIR data set as features.", "labels": [], "entities": [{"text": "BSL DF", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8846109211444855}, {"text": "NTCIR data set", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9758363564809164}]}, {"text": "We rank the features according to their frequencies and gradually increase the value of N for the Top-N classification features.", "labels": [], "entities": []}, {"text": "BSL LF: The words in existing Chinese sentiment lexicons are used as features.", "labels": [], "entities": [{"text": "BSL LF", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8047650456428528}]}, {"text": "A total of 836 positive words and 1,254 negative words are collected from HowNet.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.9886980056762695}]}, {"text": "We use LibSVM 10 and perform 10-fold cross-validation on the NTCIR polarity sentences.", "labels": [], "entities": [{"text": "NTCIR polarity sentences", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.8342206875483195}]}, {"text": "The accuracies over N number of features are plotted in.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9920056462287903}]}, {"text": "Our approach achieves a very promising improvement, although the features and the sentences that need to be classified are selected from different corpora.", "labels": [], "entities": []}, {"text": "This suggests that the generated sentiment lexicon is adaptive and qualitative enough for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.9626342356204987}]}], "tableCaptions": [{"text": " Table 2  Recall evaluation of the experiments based on monolingual (Mono), bilingual without antonym  (BLP-WOA), and bilingual (BLP) word graphs.", "labels": [], "entities": []}, {"text": " Table 3  Recall evaluation of the Rule, SOP, MAD, and BLP approaches.", "labels": [], "entities": [{"text": "MAD", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.8891540169715881}, {"text": "BLP", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9971747398376465}]}, {"text": " Table 4  Influence on Recall of the inter-language relation.", "labels": [], "entities": [{"text": "Recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9456468224525452}]}]}