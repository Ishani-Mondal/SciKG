{"title": [], "abstractContent": [{"text": "Manually annotated corpora are indispensable resources, yet for many annotation tasks, such as the creation of treebanks, there exist multiple corpora with different and incompatible annotation guidelines.", "labels": [], "entities": []}, {"text": "This leads to an inefficient use of human expertise, but it could be remedied by integrating knowledge across corpora with different annotation guidelines.", "labels": [], "entities": []}, {"text": "In this article we describe the problem of annotation adaptation and the intrinsic principles of the solutions, and present a series of successively enhanced models that can automatically adapt the divergence between different annotation formats.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8665635287761688}]}, {"text": "We evaluate our algorithms on the tasks of Chinese word segmentation and dependency parsing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.5771320958932241}, {"text": "dependency parsing", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7788350880146027}]}, {"text": "For word segmentation, where there are no universal segmentation guidelines because of the lack of morphology in Chinese, we perform annotation adaptation from the much larger People's Daily corpus to the smaller but more popular Penn Chinese Treebank.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7635254263877869}, {"text": "People's Daily corpus", "start_pos": 176, "end_pos": 197, "type": "DATASET", "confidence": 0.920951321721077}, {"text": "Penn Chinese Treebank", "start_pos": 230, "end_pos": 251, "type": "DATASET", "confidence": 0.9651570518811544}]}, {"text": "For dependency parsing, we perform annotation adaptation from the Penn Chinese Treebank to a semantics-oriented Dependency Treebank, which is annotated using significantly different annotation guidelines.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8616383671760559}, {"text": "Penn Chinese Treebank", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.9873543977737427}]}, {"text": "In both experiments, automatic annotation adaptation brings significant improvement, achieving state-of-the-art performance despite the use of purely local features in training.", "labels": [], "entities": [{"text": "automatic annotation adaptation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6187414427598318}]}], "introductionContent": [{"text": "Much of statistical NLP research relies on some sorts of manually annotated corpora to train models, but annotated resources are extremely expensive to build, especially on a large scale.", "labels": [], "entities": []}, {"text": "The creation of treebanks is a prime example.", "labels": [], "entities": []}, {"text": "However, the linguistic theories motivating these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies.", "labels": [], "entities": []}, {"text": "For example, there are several treebanks for English, including the Chomskian-style Penn Treebank, the HPSG LinGo Redwoods Treebank (, and a smaller dependency treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9760566055774689}, {"text": "HPSG LinGo Redwoods Treebank", "start_pos": 103, "end_pos": 131, "type": "DATASET", "confidence": 0.9293175637722015}]}, {"text": "From the perspective of resource accumulation, it seems a waste inhuman efforts.", "labels": [], "entities": []}, {"text": "A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (Penn Treebank/Wall Street Journal) to transcribed dialog (LinGo).", "labels": [], "entities": [{"text": "Penn Treebank/Wall Street Journal)", "start_pos": 142, "end_pos": 176, "type": "DATASET", "confidence": 0.9490445000784737}]}, {"text": "It would be nice if a system could be automatically ported from one set of guidelines and/or domain to another, in order to exploit a much larger data set.", "labels": [], "entities": []}, {"text": "The second problem, domain adaptation, is very well studied (e.g.,.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8256937861442566}]}, {"text": "This work focuses on the widely existing and equally important problem, annotation adaptation, in order to adapt the divergence between different annotation guidelines and integrate linguistic knowledge in corpora with incongruent annotation formats.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.8368936777114868}]}, {"text": "In this article, we describe the problem of annotation adaptation and the intrinsic principles of the solutions, and present a series of successively improved concrete models, the goal being to transfer the annotations of a corpus (source corpus) to the annotation format of another corpus (target corpus).", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8491276800632477}]}, {"text": "The transfer classifier is the fundamental component for annotation adaptation algorithms.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8001174330711365}]}, {"text": "It learns correspondence regularities between annotation guidelines from a parallel annotated corpus, which has two kinds of annotations for the same data.", "labels": [], "entities": []}, {"text": "In the simplest model (Model 1), the source classifier trained on the source corpus gives its predications to the transfer classifier trained on the parallel annotated corpus, so as to integrate the knowledge in the two corpora.", "labels": [], "entities": []}, {"text": "Ina variant of the simplest model (Model 2), the transfer classifier is used to transform the annotations in the source corpus into the annotation format of the target corpus; then the transformed source corpus and the target corpus are merged in order to train a more accurate classifier.", "labels": [], "entities": []}, {"text": "Based on the second model, we finally develop an optimized model (Model 3), where two optimization strategies, iterative training and predict-self re-estimation, are integrated to further improve the efficiency of annotation adaptation.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 214, "end_pos": 235, "type": "TASK", "confidence": 0.7955635786056519}]}, {"text": "We experiment on Chinese word segmentation and dependency parsing to test the efficacy of our methods.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.5710082848866781}, {"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7189343273639679}]}, {"text": "For word segmentation, the problem of incompatible annotation guidelines is one of the most glaring: No segmentation guideline has been widely accepted due to the lack of a clear definition of Chinese word morphology.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7652903497219086}]}, {"text": "For dependency parsing there also exist multiple disparate annotation guidelines.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8772519528865814}]}], "datasetContent": [{"text": "To evaluate the performance of annotation adaptation, we experiment on two important NLP tasks, Chinese word segmentation and dependency parsing, both of which can be modeled as discriminative classification problems.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8322485983371735}, {"text": "Chinese word segmentation", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.6062316497166952}, {"text": "dependency parsing", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7742131054401398}]}, {"text": "For both tasks, we give the performances of the baseline models and the annotation adaptation algorithms.", "labels": [], "entities": []}, {"text": "We perform annotation adaptation for word segmentation from People's Daily (PD) () to Penn Chinese Treebank 5.0 (CTB) ().", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7170848697423935}, {"text": "People's Daily (PD)", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.9214235544204712}, {"text": "Penn Chinese Treebank 5.0 (CTB)", "start_pos": 86, "end_pos": 117, "type": "DATASET", "confidence": 0.9725328172956195}]}, {"text": "The two corpora are built according to different segmentation guidelines and differ largely in quantity of data.", "labels": [], "entities": []}, {"text": "CTB is smaller in size with about 0.5M words, whereas PD is much larger, containing nearly 6M words.", "labels": [], "entities": [{"text": "CTB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7837868332862854}, {"text": "PD", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.885910153388977}]}, {"text": "shows the data partitioning for the two corpora.", "labels": [], "entities": []}, {"text": "To approximate more general scenarios of annotation adaptation problems, we extract from PD a subset that is comparable to CTB in size.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7664663791656494}]}, {"text": "Because there are many extremely long sentences in the original PD corpus, we first split them into normal sentences according to the full-stop punctuation symbol.", "labels": [], "entities": []}, {"text": "We randomly select 20, 000 sentences (0.45M words) from the PD training data as the new training set, and 1, 000/1, 000 sentences from the PD test data as the new testing/developing set.", "labels": [], "entities": [{"text": "PD training data", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.7519923249880472}, {"text": "PD test data", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.7864671349525452}]}, {"text": "We label the smaller version of PD as SPD.", "labels": [], "entities": []}, {"text": "The balanced source corpus and target corpus also facilitate the investigation of annotation adaptation.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7003986090421677}]}, {"text": "Annotation adaptation for dependency parsing is performed from the CTB-derived syntactic dependency treebank (DCTB) ( to the Semantic Dependency Treebank (SDT) ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8440272212028503}, {"text": "CTB-derived syntactic dependency treebank (DCTB)", "start_pos": 67, "end_pos": 115, "type": "DATASET", "confidence": 0.7765334291117532}]}, {"text": "Semantic dependency encodes the semantic relationships between words, which are very different from syntactic dependencies.", "labels": [], "entities": []}, {"text": "SDT is annotated on a small portion of the CTB text as depicted in; therefore, we use the subset of DCTB covering the remaining CTB text as the source corpus.", "labels": [], "entities": [{"text": "CTB text", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9450072646141052}, {"text": "CTB text", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.836906224489212}]}, {"text": "We still denote the source corpus as DCTB in the following for simplicity.", "labels": [], "entities": [{"text": "DCTB", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9210115075111389}]}], "tableCaptions": [{"text": " Table 4  Data partitioning for CTB and PD.", "labels": [], "entities": [{"text": "Data partitioning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5727544277906418}, {"text": "CTB", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8555946946144104}]}, {"text": " Table 5  Data partitioning for SDT.", "labels": [], "entities": [{"text": "Data partitioning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6872532069683075}, {"text": "SDT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9445310831069946}]}, {"text": " Table 6  Performance of the baseline word segmenters.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7342501878738403}]}, {"text": " Table 7  Performance of the baseline dependency parsers.", "labels": [], "entities": []}, {"text": " Table 8  Performance of automatic annotation adaptation models for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7973221242427826}]}, {"text": " Table 9  Performance of automatic annotation adaptation models for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8802402913570404}]}]}