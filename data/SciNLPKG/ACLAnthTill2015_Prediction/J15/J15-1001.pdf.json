{"title": [], "abstractContent": [{"text": "This paper is concerned with automatic generation of all possible questions from a topic of interest.", "labels": [], "entities": [{"text": "automatic generation of all possible questions from a topic", "start_pos": 29, "end_pos": 88, "type": "TASK", "confidence": 0.8411494526598189}]}, {"text": "Specifically, we consider that each topic is associated with a body of texts containing useful information about the topic.", "labels": [], "entities": []}, {"text": "Then, questions are generated by exploiting the named entity information and the predicate argument structures of the sentences present in the body of texts.", "labels": [], "entities": []}, {"text": "The importance of the generated questions is measured using Latent Dirichlet Allocation by identifying the subtopics (which are closely related to the original topic) in the given body of texts and applying the Extended String Subsequence Kernel to calculate their similarity with the questions.", "labels": [], "entities": []}, {"text": "We also propose the use of syntactic tree kernels for the automatic judgment of the syntactic correctness of the questions.", "labels": [], "entities": []}, {"text": "The questions are ranked by considering both their importance (in the context of the given body of texts) and syntactic correctness.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no previous study has accomplished this task in our setting.", "labels": [], "entities": []}, {"text": "A series of experiments demonstrate that the proposed topic-to-question generation approach can significantly outperform the state-of-the-art results.", "labels": [], "entities": [{"text": "topic-to-question generation", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.7069976329803467}]}], "introductionContent": [{"text": "We live in an information age where all kinds of information is easily accessible through the Internet.", "labels": [], "entities": []}, {"text": "The increasing demand for access to different types of information available online have interested researchers in abroad range of Information Retrieval-related areas, such as question answering, topic detection and tracking, summarization, multimedia retrieval, chemical and biological informatics, text structuring, and text mining.", "labels": [], "entities": [{"text": "Information Retrieval-related", "start_pos": 131, "end_pos": 160, "type": "TASK", "confidence": 0.7108712643384933}, {"text": "question answering", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.870702475309372}, {"text": "topic detection and tracking", "start_pos": 196, "end_pos": 224, "type": "TASK", "confidence": 0.8577037006616592}, {"text": "summarization", "start_pos": 226, "end_pos": 239, "type": "TASK", "confidence": 0.9874802231788635}, {"text": "multimedia retrieval", "start_pos": 241, "end_pos": 261, "type": "TASK", "confidence": 0.6922920346260071}, {"text": "text structuring", "start_pos": 300, "end_pos": 316, "type": "TASK", "confidence": 0.7950393557548523}, {"text": "text mining", "start_pos": 322, "end_pos": 333, "type": "TASK", "confidence": 0.825063556432724}]}, {"text": "Although search engines do a remarkable job in searching through a heap of information, they have certain limitations, as they cannot satisfy the end users' information need to have more direct access to relevant documents.", "labels": [], "entities": []}, {"text": "For example, if we ask for the impact of the current global financial crisis in different parts of the world, we can expect to sift through thousands of results for the answer.", "labels": [], "entities": []}, {"text": "This fact can be more understandable by the following scenario.", "labels": [], "entities": []}, {"text": "When a user enters a query, they are served with a ranked list of relevant documents by the standard document retrieval systems (i.e., search engines), Decomposing a complex question automatically into simpler questions in this manner such that each of them can be answered individually by using the state-of-the-art QA systems, and then combining the individual answers to form a single answer to the original complex question, has proven effective to deal with the complex question answering problem.", "labels": [], "entities": [{"text": "complex question answering", "start_pos": 467, "end_pos": 493, "type": "TASK", "confidence": 0.6216561396916708}]}, {"text": "Moreover, the generated simple questions can be used as the list of important aspects to act as a guide 2 for selecting the most relevant sentences in producing more focused and more accurate summaries as the output of a summarization system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 221, "end_pos": 234, "type": "TASK", "confidence": 0.9625412821769714}]}, {"text": "From this discussion, it is obvious that the complex question decomposition problem can be generalized to the problem of topic-to-question generation to help improve the complex question answering systems.", "labels": [], "entities": [{"text": "complex question decomposition", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6731326878070831}, {"text": "topic-to-question generation", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.7766187787055969}, {"text": "question answering", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7508130967617035}]}, {"text": "In this article 3 , we consider the task of automatically generating questions from topics and assume that each topic is associated with a body of texts having useful information about the topic.", "labels": [], "entities": []}, {"text": "This assumption has been inherited from the process of how a human asks questions based on their knowledge.", "labels": [], "entities": []}, {"text": "For example, if a person knows that a university is an educational institution, then they can ask a question about its faculty and students.", "labels": [], "entities": []}, {"text": "In this research, our main goal is to generate fact-based questions about a given topic from its associated content information.", "labels": [], "entities": []}, {"text": "We generate questions by exploiting the named entity information and the predicate argument structures of the sentences (along with semantic roles) present in the given body of texts.", "labels": [], "entities": []}, {"text": "The named entities and the semantic role labels are used to identify relevant parts of a sentence in order to form relevant questions about them.", "labels": [], "entities": []}, {"text": "The importance of the generated questions is measured in two steps.", "labels": [], "entities": []}, {"text": "In the first step, we identify whether the question is asking something about the topic or something that is very closely related to the topic.", "labels": [], "entities": []}, {"text": "We call this the measure of topic relevance.", "labels": [], "entities": []}, {"text": "For this purpose, we use Latent Dirichlet Allocation (LDA) to identify the subtopics (which are closely related to the original topic) in the given body of texts and apply the Extended String Subsequence Kernel (ESSK) () to calculate their similarity with the questions.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 25, "end_pos": 58, "type": "METRIC", "confidence": 0.8623901506265005}]}, {"text": "In the second step, we judge the syntactic correctness of each generated question.", "labels": [], "entities": []}, {"text": "We apply the tree kernel functions and re-implement the syntactic tree kernel model according to for computing the syntactic similarity of each question with the associated content information.", "labels": [], "entities": []}, {"text": "We rank the questions by considering their topic relevance and syntactic correctness scores.", "labels": [], "entities": []}, {"text": "Experimental results show the effectiveness of our approach for automatically generating topical questions.", "labels": [], "entities": [{"text": "automatically generating topical questions", "start_pos": 64, "end_pos": 106, "type": "TASK", "confidence": 0.7090265601873398}]}, {"text": "The remainder of the article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the description of our QG system.", "labels": [], "entities": []}, {"text": "Section 4 explains the experiments and shows evaluation results; Section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a methodology derived from and to evaluate the performance of our QG systems.", "labels": [], "entities": []}, {"text": "Three native English-speaking university graduate students judge the quality of the top-ranked 20% questions using two criteria: topic relevance and syntactic correctness.", "labels": [], "entities": []}, {"text": "For topic relevance, the given score is an integer between 1 (very poor) and 5 (very good) and is guided by the consideration of the following aspects: 1.", "labels": [], "entities": [{"text": "topic relevance", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8515217304229736}]}, {"text": "Semantic correctness (i.e., the question is meaningful and related to the topic), 2.", "labels": [], "entities": []}, {"text": "Correctness of question type (i.e., a correct question word is used), and 3.", "labels": [], "entities": []}, {"text": "Referential clarity (i.e., it is clearly possible to understand what the question refers to).", "labels": [], "entities": [{"text": "clarity", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9373427033424377}]}, {"text": "For syntactic correctness, the assigned score is also an integer between 1 (very poor) and 5 (very good).", "labels": [], "entities": [{"text": "syntactic correctness", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7976431846618652}]}, {"text": "Whether a question is grammatically corrector not is checked here.", "labels": [], "entities": []}, {"text": "The judges were asked to read the topics with their associated body of texts and then rate the top-ranked questions generated by different systems.", "labels": [], "entities": []}, {"text": "For each question, we calculate the average of the judges' scores.", "labels": [], "entities": []}, {"text": "The judges were provided with an annotation guideline and sample judgments, according to the methodology derived from and.", "labels": [], "entities": []}, {"text": "The same judges evaluated all the system outputs and they were blind to the system identity when judging.", "labels": [], "entities": []}, {"text": "No guidelines were provided on the relative importance of the various aspects that made the judgment task subjective.", "labels": [], "entities": []}, {"text": "15 A syntactically incorrect question is not useful even if it is relevant to the topic.", "labels": [], "entities": []}, {"text": "This motivated us to give equal importance to topic relevance and syntactic correctness.", "labels": [], "entities": [{"text": "topic relevance", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7070080786943436}]}, {"text": "The parameter w can be tuned to investigate its impact on the system performance.", "labels": [], "entities": []}, {"text": "16 http://www.questiongeneration.org/mediawiki.", "labels": [], "entities": []}, {"text": "17 We use these data to build necessary general purpose rules for our QG model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Topic relevance and syntactic correctness scores.", "labels": [], "entities": []}, {"text": " Table 4  Acceptability of the questions (in %).", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9976320266723633}]}, {"text": " Table 5  Topic relevance and syntactic correctness scores (narrowed focus).", "labels": [], "entities": []}, {"text": " Table 6  Acceptability of the questions in % (narrowed focus).", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9960975050926208}]}, {"text": " Table 8  Judgment scores associated with example questions.", "labels": [], "entities": [{"text": "Judgment", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8323704600334167}]}]}