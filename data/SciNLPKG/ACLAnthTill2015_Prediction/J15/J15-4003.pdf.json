{"title": [{"text": "Inducing Implicit Arguments from Comparable Texts: A Framework and Its Applications", "labels": [], "entities": [{"text": "Inducing Implicit Arguments from Comparable Texts", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8439765771230062}]}], "abstractContent": [{"text": "In this article, we investigate aspects of sentential meaning that are not expressed in local predicate-argument structures.", "labels": [], "entities": []}, {"text": "In particular, we examine instances of semantic arguments that are only inferable from discourse context.", "labels": [], "entities": []}, {"text": "The goal of this work is to automatically acquire and process such instances, which we also refer to as implicit arguments, to improve computational models of language.", "labels": [], "entities": []}, {"text": "As contributions towards this goal, we establish an effective framework for the difficult task of inducing implicit arguments and their antecedents in discourse and empirically demonstrate the importance of modeling this phenomenon in discourse-level tasks.", "labels": [], "entities": []}, {"text": "Our framework builds upon a novel projection approach that allows for the accurate detection of implicit arguments by aligning and comparing predicate-argument structures across pairs of comparable texts.", "labels": [], "entities": []}, {"text": "As part of this framework, we develop a graph-based model for predicate alignment that significantly outperforms previous approaches.", "labels": [], "entities": [{"text": "predicate alignment", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.9383369088172913}]}, {"text": "Based on such alignments, we show that implicit argument instances can be automatically induced and applied to improve a current model of linking implicit arguments in discourse.", "labels": [], "entities": []}, {"text": "We further validate that decisions on argument realization, although being a subtle phenomenon most of the time, can considerably affect the perceived coherence of a text.", "labels": [], "entities": [{"text": "argument realization", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7244988083839417}]}, {"text": "Our experiments reveal that previous models of coherence are notable to predict this impact.", "labels": [], "entities": []}, {"text": "Consequently, we develop a novel coherence model, which learns to accurately predict argument realization based on automatically aligned pairs of implicit and explicit arguments.", "labels": [], "entities": [{"text": "predict argument realization", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.6280588805675507}]}], "introductionContent": [{"text": "The goal of semantic parsing is to automatically process natural language text and map the underlying meaning of text to appropriate representations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7316610962152481}]}, {"text": "Semantic role labeling induces shallow semantic representations, so-called predicate-argument structures, by processing sentences and mapping them to predicates and associated arguments.", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6656681597232819}]}, {"text": "Arguments of these structures can, however, be non-local in natural language text, as shown in Example 1.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our model is to correctly predict the realization type (implicit or explicit) of an argument that maximizes the perceived coherence of the document.", "labels": [], "entities": []}, {"text": "As a proxy for coherence, we use the naturalness ratings given by our annotators.", "labels": [], "entities": []}, {"text": "We evaluate classification performance on the 70 data points in our annotated test set for which clear preferences have been established.", "labels": [], "entities": [{"text": "classification", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.9631513357162476}]}, {"text": "We report results in terms of precision, recall, and F 1 -score per class as well as micro-and macro-averaged F 1 -score across classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9997257590293884}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9996488094329834}, {"text": "F 1 -score", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9916097968816757}, {"text": "F 1 -score", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.958309605717659}]}, {"text": "We compute precision as the fraction of correct classifier decisions divided by the total number of classifications made fora specific class label; recall as the fraction of correct classifier decisions divided by the total number of test items with the specific label; and F 1 as the harmonic mean between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9988752007484436}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9991359114646912}, {"text": "F 1", "start_pos": 274, "end_pos": 277, "type": "METRIC", "confidence": 0.9929091930389404}, {"text": "precision", "start_pos": 307, "end_pos": 316, "type": "METRIC", "confidence": 0.998506486415863}, {"text": "recall", "start_pos": 321, "end_pos": 327, "type": "METRIC", "confidence": 0.9917356371879578}]}, {"text": "For comparison, we apply a couple of coherence models proposed in previous work: the original entity grid model by, a modified version that uses topic models, and an extended version that includes entity-specific features (Elsner and Charniak 2011b); we further apply the discourse-new model by, and the pronoun-based model by.", "labels": [], "entities": []}, {"text": "For all of the aforementioned models, we use their respective implementation provided with the Brown Coherence Toolkit.", "labels": [], "entities": []}, {"text": "14 Note that the toolkit only returns one coherence score for each document.", "labels": [], "entities": []}, {"text": "To use the model scores for predicting argument realization, we use two documents per data point-one that contains the affected argument explicitly and one that does not (implicit argument)-and treat the higher scoring variant as classification output.", "labels": [], "entities": [{"text": "predicting argument realization", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.9091776808102926}]}, {"text": "If both documents achieve the same score, we count the test item neither as correctly nor as incorrectly classified.", "labels": [], "entities": []}, {"text": "Like the applied baseline models, our models do not make use of any manually labeled data for training.", "labels": [], "entities": []}, {"text": "Instead, we utilize the automatically identified instances of explicit and implicit arguments from pairs of comparable texts, which we described in Section 5.", "labels": [], "entities": []}, {"text": "To train our own model, we prepare this data set as follows: Firstly, we remove all data points that were selected for the test set; secondly, we split all pairs of texts into two groups-texts that contain a PAS in which an implicit argument has been identified (IA), and their comparable counterparts, which contain the aligned PAS with an explicit argument (EA).", "labels": [], "entities": [{"text": "EA", "start_pos": 360, "end_pos": 362, "type": "METRIC", "confidence": 0.9774836301803589}]}, {"text": "All texts are labeled according to their group.", "labels": [], "entities": []}, {"text": "For all texts in group EA, we remove the explicit argument from the aligned PAS.", "labels": [], "entities": []}, {"text": "This way, the feature extractor always gets to seethe text and automatic annotations as if the realization decision had not been performed and can thus extract unbiased feature values for the affected entity and argument position.", "labels": [], "entities": []}, {"text": "Given each feature representation, we train a classifier using the default parameters of the LIBSVM package (Chang and Lin 2011).", "labels": [], "entities": [{"text": "LIBSVM package", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.8610662221908569}]}, {"text": "We apply our own model on each data point in the small annotated test set, where we always treat the affected argument, regardless of its actual annotation, as implicit to extract unbiased feature values for classification.", "labels": [], "entities": []}, {"text": "Based on the features described in Section 7.2 and trained on the automatically constructed PAS alignments, our model predicts the realization type of each argument in the given context.", "labels": [], "entities": []}, {"text": "We note that our model has an advantage here because it is specifically designed for this task and trained on corresponding data.", "labels": [], "entities": []}, {"text": "All models compute local coherence ratings based on entity occurrences, however, and should thus be able to predict which realization type coheres best with the given discourse context.", "labels": [], "entities": []}, {"text": "That is, because the input document pairs are identical except for the affected argument position, the coherence scores assigned by each model to pairs of text only differ with respect to the affected entity realization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Statistics on predicates and alignments in the annotated data sets.", "labels": [], "entities": []}, {"text": " Table 6  Results for identifying and linking implicit arguments in the SemEval test set.", "labels": [], "entities": [{"text": "SemEval test set", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.8510122696558634}]}, {"text": " Table 7  Results for identifying and linking implicit arguments using features selected on our full data set  and different combinations of task-specific and automatically induced data for training.", "labels": [], "entities": []}, {"text": " Table 8  Statistics on the collected data and final test set.", "labels": [], "entities": []}, {"text": " Table 9  Results for correctly predicting argument realization. Significant differences from our (full)  model in terms of micro-averaged F 1 -score are marked with asterisks (* p < 0.01).", "labels": [], "entities": [{"text": "predicting argument realization", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.9141743779182434}, {"text": "F 1 -score", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.931721031665802}]}]}