{"title": [{"text": "Large Linguistic Corpus Reduction with SCP Algorithms", "labels": [], "entities": [{"text": "Large Linguistic Corpus Reduction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5435497313737869}]}], "abstractContent": [{"text": "Linguistic corpus design is a critical concern for building rich annotated corpora useful in different domains of applications.", "labels": [], "entities": [{"text": "Linguistic corpus design", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6924668351809183}]}, {"text": "For example, speech technologies such as ASR (Automatic Speech Recognition) or TTS (Text-to-Speech) need a huge amount of speech data to train data-driven models or to produce synthetic speech.", "labels": [], "entities": [{"text": "ASR (Automatic Speech Recognition)", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6980157891909281}]}, {"text": "Collecting data is always related to costs (recording speech, verifying annotations, etc.), and as a rule of thumb, the more data you gather, the more costly your application will be.", "labels": [], "entities": []}, {"text": "Within this context, we present in this article solutions to reduce the amount of linguistic text content while maintaining a sufficient level of linguistic richness required by a model or an application.", "labels": [], "entities": []}, {"text": "This problem can be formalized as a Set Covering Problem (SCP) and we evaluate two algorithmic heuristics applied to design large text corpora in English and French for covering phonological information or POS labels.", "labels": [], "entities": [{"text": "Set Covering Problem (SCP)", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.8756698071956635}]}, {"text": "The first considered algorithm is a standard greedy solution with an agglomerative/spitting strategy and we propose a second algorithm based on Lagrangian relaxation.", "labels": [], "entities": []}, {"text": "The latter approach provides a lower bound to the cost of each covering solution.", "labels": [], "entities": []}, {"text": "This lower bound can be used as a metric to evaluate the quality of a reduced corpus whatever the algorithm applied.", "labels": [], "entities": []}, {"text": "Experiments show that a suboptimal algorithm like a greedy algorithm achieves good results; the cost of its solutions is not so far from the lower bound (about 4.35% for 3-phoneme coverings).", "labels": [], "entities": []}, {"text": "Usually, constraints in SCP are binary; we proposed here a generalization where the constraints on each covering feature can be multi-valued.", "labels": [], "entities": []}], "introductionContent": [{"text": "In automatic speech and language processing, many technologies make extensive use of written or read text sets.", "labels": [], "entities": [{"text": "automatic speech and language processing", "start_pos": 3, "end_pos": 43, "type": "TASK", "confidence": 0.5694145500659943}]}, {"text": "These linguistic corpora area necessity to train models or to extract rules, and the quality of the results strongly depends on a corpus' content.", "labels": [], "entities": []}, {"text": "Often, the reference corpus should provide a maximum diversity of content., and, it turns out that maximizing the text coverage of the learning corpus improves an automatic syllabification based on a neural network.", "labels": [], "entities": []}, {"text": "Similarly, a high quality speech synthesis system based on the selection of speech units requires a rich corpus in terms of diphones, diphones in context, triphones, and prosodic markers.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7487155199050903}]}, {"text": "In particular, shows the importance of a good coverage of diphones and triphones for the intelligibility of a voice produced by a unit selection speech synthesis system.", "labels": [], "entities": []}, {"text": "To cover the best attributes needed fora task, several strategies are then possible.", "labels": [], "entities": []}, {"text": "A first method-very simple-is to collect text randomly, but it soon becomes expensive because of the natural distribution of linguistic events following the Zipf's law.", "labels": [], "entities": []}, {"text": "Very few events are extremely frequent and many events are very rare.", "labels": [], "entities": []}, {"text": "This problem is often made difficult by the fact that many technologies require several variants of the same event (as in a Text-to-Speech [TTS] system using several acoustical versions of the same phonological unit).", "labels": [], "entities": []}, {"text": "Usually, a large volume of data needs to be collected.", "labels": [], "entities": []}, {"text": "However, and depending on the applications, building such corpora is often achieved under a constraint of parsimony.", "labels": [], "entities": []}, {"text": "As an example, fora TTS system, a high-quality synthetic voice generally needs a huge number of speech recordings.", "labels": [], "entities": []}, {"text": "But minimizing the duration of a recording is also a critical point to ensure uniform quality of the voice, to reduce the drudgery of the recording, to reduce the financial cost, or to follow a technical constraint on the amount of collected data for embedded systems.", "labels": [], "entities": []}, {"text": "Moreover, a reduced set tends to limit the need of human implication for checking the data (transcription and annotation).", "labels": [], "entities": []}, {"text": "Similarly, in the natural language processing field (NLP), the adaptation of a generic model to a specific domain often requires new annotated data that illustrate its specificities (as in Candito, Anguiano, and Seddah 2011).", "labels": [], "entities": [{"text": "natural language processing field (NLP)", "start_pos": 18, "end_pos": 57, "type": "TASK", "confidence": 0.7531132442610604}]}, {"text": "However, the creation cost of such data highly depends on the kind of labels used to adapt the model.", "labels": [], "entities": []}, {"text": "In particular, the annotation in syntax trees is really more expensive than in Part-of-Speech (POS) tags.", "labels": [], "entities": []}, {"text": "Then, it could be more efficient to annotate a compact corpus that reflects the phenomena variability than a corpus with a natural distribution of events, which implies many redundancies (see.", "labels": [], "entities": []}, {"text": "Ina machine learning framework, the active learning strategy can be used as an alternative that reduces the manual data annotation effort to design the training corpus without diminishing the quality of the model to train (see.", "labels": [], "entities": []}, {"text": "It consists of building the corpus iteratively by choosing an item according to an external source of information (a user or an experimental measure).", "labels": [], "entities": []}, {"text": "This approach has been applied in NLP, speech recognition, and spoken language understanding (see for instance.", "labels": [], "entities": [{"text": "NLP", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9509488344192505}, {"text": "speech recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8207232654094696}, {"text": "spoken language understanding", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.6539913813273112}]}, {"text": "A second alternative, when no direct quality measure is available, consists of covering a large set of attributes that may impact the final quality (after annotation or recording).", "labels": [], "entities": []}, {"text": "This kind of approach might also be preferred when the final corpus is builtin one batch (for instance, because of out-sourcing or annotator/performer consistency constraints).", "labels": [], "entities": []}, {"text": "A method could bean automatic extraction from a huge text corpus of a minimal sized subset that covers the identified attributes.", "labels": [], "entities": []}, {"text": "This problem is a generalization of the Set-Covering Problem (SCP), which is an NPhard problem, as shown in.", "labels": [], "entities": []}, {"text": "It is then necessary to use heuristics or sub-optimal algorithms fora reasonable computation time.", "labels": [], "entities": []}, {"text": "Moreover, and have shown that the SCP cannot be polynomially approximated with ratio c \u00d7 ln(n) unless P = NP, when c is a constant, and n refers to the size of the universe to cover.", "labels": [], "entities": []}, {"text": "That means that one cannot be certain to obtain a result under this ratio with any polynomial algorithm.", "labels": [], "entities": []}, {"text": "However, the latter complexity results are given for any kind of distribution in the mono-representation case.", "labels": [], "entities": []}, {"text": "One can ask if good multi-represented coverages can be achieved efficiently on data following Zipf's law, which is usual in the domain of NLP.", "labels": [], "entities": []}, {"text": "Within the field of speech processing, the most frequently used strategy is a greedy method based on an agglomeration policy.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7151334583759308}]}, {"text": "This iterative algorithm selects the sentence with the highest score at each iteration.", "labels": [], "entities": []}, {"text": "The score reflects the contribution of the sentence to the covering under construction.", "labels": [], "entities": []}, {"text": "In, this methodology has been applied to build a database of read speech from a text corpus for the evaluation of speech recognition systems using hierarchically organized covering attributes.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.6846832036972046}]}, {"text": "have tested different variants of greedy selection of texts by varying the units to cover (diphones, duration, etc.) and the \"scores\" fora sentence depending on the considered applications.", "labels": [], "entities": [{"text": "duration", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9751527309417725}]}, {"text": "In Tian, Nurminen, and, the learning corpus for an automatic system of syllabification is designed using a greedy approach with the Levenshtein distance as a score function in order to maximize its text diversity.", "labels": [], "entities": []}, {"text": "In, the methodology gives a priority to the rarest categories of allophones.", "labels": [], "entities": []}, {"text": "The latter methodology has been implemented for the definition of the multi-speaker corpus Neologos in Krstulovi\u00b4c.", "labels": [], "entities": [{"text": "Neologos", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.6185749769210815}]}, {"text": "In the article of, the authors constructed a corpus where the distribution of diphonemes/triphonemes matches a uniform distribution.", "labels": [], "entities": []}, {"text": "A greedy algorithm is led by a score function based on the Kullback-Liebler divergence.", "labels": [], "entities": []}, {"text": "A similar method is used in to design a reduced database in accordance with a specific domain distribution.", "labels": [], "entities": []}, {"text": "propose a pair exchange mechanism that apply after a first reverse greedy algorithm-also called spitting greedy-deleting the useless sentences.", "labels": [], "entities": []}, {"text": "In, the covering of \"sandwich\" units (defined to be more adapted to corpus-based speech synthesis) is carried out by generating new sentences in a semi-automatic way.", "labels": [], "entities": [{"text": "corpus-based speech synthesis", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.7619779904683431}]}, {"text": "Candidates are generated using finite state transducers.", "labels": [], "entities": []}, {"text": "The sentences are ordered according to a greedy criterion (their sandwiches richness) and presented to a human evaluator.", "labels": [], "entities": []}, {"text": "This collection of artificial and rich sentences enables an effective reduction of the size of the covering but requires expensive human intervention to obtain semantically correct sentences that will be therefore easier to record.", "labels": [], "entities": []}, {"text": "The results of these previously cited studies are difficult to compare because of the different initial corpora and covering constraints (partial or full covering) and evaluation criteria (the number of gathered sentences, the Kullback divergence, etc.).", "labels": [], "entities": []}, {"text": "In Zhang and Nakamura (2008), a priority policy for the rare units is added into an agglomerative greedy algorithm in order to get a covering of triphoneme classes from a large text corpus in Chinese language.", "labels": [], "entities": []}, {"text": "The results show that this priority policy driven by the score function and the phonetic content of the sentences reduces the covering size compared with a standard agglomerative greedy algorithm.", "labels": [], "entities": []}, {"text": "Similarly, in, several combinations of greedy algorithms (agglomeration, spitting, pair exchange, or priority to rare units) were applied to the construction of a corpus for speech synthesis in French containing at least three representatives of the most frequent diphones.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 174, "end_pos": 190, "type": "TASK", "confidence": 0.7568257749080658}]}, {"text": "Based on this work, the best strategy would be the application of an agglomerative greedy followed by a spitting greedy algorithm.", "labels": [], "entities": []}, {"text": "During the agglomeration phase, the score of a sentence corresponds to the number of its unit instances that remain to be covered normalized by its length.", "labels": [], "entities": []}, {"text": "During the spitting phase, at each iteration, the longest redundant sentence is removed from the covering.", "labels": [], "entities": [{"text": "spitting phase", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9043225347995758}]}, {"text": "This algorithm is called the Agglomeration and Spitting Algorithm (ASA).", "labels": [], "entities": []}, {"text": "As an alternative to a greedy algorithm, which is sub-optimal, solving the SCP using Lagrangian relaxation principles can provide an exact solution for problems of reasonable size.", "labels": [], "entities": []}, {"text": "However, for speech processing, the SCP has several millions of sentences with tens of thousands of covering features.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7906385362148285}]}, {"text": "Considering these practical constraints, adapted a Lagrangian relaxation based algorithm proposed by.", "labels": [], "entities": []}, {"text": "In the context of Italian railways, Caprara, Fischetti, and Toth proposed heuristics to solve scheduling problems and won a competition, called Faster, organized by the Italian Operational Research Society in 1994, ahead of other Lagrangian relaxation heuristics-based algorithms, like.", "labels": [], "entities": []}, {"text": "In, the algorithm takes into account the constraints of multi-representation.", "labels": [], "entities": []}, {"text": "A minimal number of representatives for the same unit maybe required.", "labels": [], "entities": []}, {"text": "The proposed algorithm, called LamSCP -Lagrangian-based Algorithm for Multi-represented SCP -is applied to extract coverings of diphonemes with a mono-or a 5-representation and coverings of triphonemes with mono-representation constraints.", "labels": [], "entities": []}, {"text": "These results are compared with the greedy strategy ASA and are about 5% to 10% better.", "labels": [], "entities": [{"text": "ASA", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.5630410313606262}]}, {"text": "Besides, the LamSCP provides a lower bound for the cost of the optimal covering and allows for evaluating the quality of the results.", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.5889514684677124}]}, {"text": "In, phonological content of diphoneme coverings is studied regarding many parameters.", "labels": [], "entities": []}, {"text": "These coverings are obtained by different algorithms (LamSCP, ASA, greedy based on the Kullback divergence) and some of the coverings are randomly completed to reach a given size (from 20,000 to 30,000 phones).", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9326623678207397}, {"text": "ASA", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.7959417700767517}]}, {"text": "It turns out that the coverings obtained using LamSCP and ASA provide a good representation of short units and the representation of long units mainly depends on the length of the corpus.", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.9469553828239441}, {"text": "ASA", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.8174088001251221}]}, {"text": "In this article, we present in more detail the LamSCP algorithm and its score functions and heuristics that take into account multi-representation constraints.", "labels": [], "entities": []}, {"text": "We deepen the study about the performance of LamSCP for the construction of a phonologically rich corpus according to the size of the search space.", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.7843731641769409}]}, {"text": "We evaluate LamSCP and ASA algorithms on a corpus of sentences in English fora covering of multi-represented diphones, where the minimal number of required unit representatives varies from one to five.", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8921575546264648}, {"text": "ASA", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.7289718985557556}]}, {"text": "We also compare them in the case of very constrained triphoneme coverings in English and French, which represent about 12 times more units to cover.", "labels": [], "entities": []}, {"text": "Additionally, both algorithms are tested to provide multi-represented coverings of POS tags in order to assess their ability to deal with different kinds of linguistic data.", "labels": [], "entities": []}, {"text": "A particular effort has been made on methodology to obtain comparable measures, to study the stability of both algorithms, and to establish confidence intervals for each solution.", "labels": [], "entities": []}, {"text": "This article is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, the SCP framework and the associated notations are introduced.", "labels": [], "entities": []}, {"text": "The ASA algorithm is described in Section 3 and the LamSCP is detailed in Section 4.", "labels": [], "entities": [{"text": "ASA", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6246103048324585}, {"text": "LamSCP", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.6319817304611206}]}, {"text": "The experimental methodology is presented in Section 5 and results are discussed in Section 6.", "labels": [], "entities": []}, {"text": "Before concluding in Section 8, we present experiments in the context of TTS where we evaluate on that task the benefits of a reduction in section 7.", "labels": [], "entities": [{"text": "TTS", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8170618414878845}]}], "datasetContent": [{"text": "We propose a twofold comparison of the ASA and LamSCP algorithms: One part is focused on the covering cost fora large SCP, and the other on the stability of the solutions.", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.8610460758209229}]}, {"text": "Moreover, in order to assess the ability and the behavior of both algorithms to process different linguistic data, a first set of experiments deals with phonological attributes (mainly covering co-occurrences of phonemes) and a second set with grammatical labels (mainly covering co-occurrences of POS labels).", "labels": [], "entities": []}, {"text": "Both attribute types, often involved in automatic linguistic processing, were chosen because their distribution consists of few highly frequent events and numerous rare events.", "labels": [], "entities": []}, {"text": "On the one hand, for TTS tasks, the phonological type covering is a useful preliminary step of the text corpus design before the recording step.", "labels": [], "entities": [{"text": "TTS tasks", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.9232178032398224}]}, {"text": "In order to produce the signal corresponding to a requested sentence, the unit selection engine requires at least one instance of each phone (or 2-phone, depending on the concatenation process).", "labels": [], "entities": []}, {"text": "Because the recording and the post-recording annotation process are expensive tasks, the recording length of such a corpus has to be as short as possible.", "labels": [], "entities": []}, {"text": "On the other hand, in order to train a domain-specific dependency parser, the covering of POS sequences maybe useful for increasing the diversity of syntax patterns.", "labels": [], "entities": []}, {"text": "Because the dependency annotation is a highly expensive task, the adaptation corpus to annotate needs to be as small as possible, containing characteristic examples of the specific lexical variation rather than following the natural distribution.", "labels": [], "entities": []}, {"text": "One can expect that increasing its diversity of POS sequences may lead to more diversity in the syntax trees.", "labels": [], "entities": []}, {"text": "Experiments on covering co-occurrences of phonemes are carried out on two large phonologically annotated text corpora, and consist of covering at least k instances of each phoneme, diphoneme until n-phoneme (i.e., triphoneme if n = 3, diphoneme if n = 2).", "labels": [], "entities": []}, {"text": "The cost c j of the sentence s j is given by its number of phones.", "labels": [], "entities": []}, {"text": "From this point, this kind of SCP is called a \"k-covering of n-phonemes.\"", "labels": [], "entities": [{"text": "SCP", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9479023814201355}]}, {"text": "A first corpus, Gutenberg, is composed of texts in English, mainly extracted from novels and short stories.", "labels": [], "entities": []}, {"text": "This corpus is the production of the Gutenberg Project, presented by, and has been used by to design the speech corpus Arctic.", "labels": [], "entities": []}, {"text": "A second corpus, in French, named Le-Monde, is extracted from articles published in the newspaper Le Monde in 1997.", "labels": [], "entities": [{"text": "Le Monde in 1997", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.8810720145702362}]}, {"text": "summarizes the main features of both corpora.", "labels": [], "entities": []}, {"text": "The phonological annotation of the Gutenberg corpus comes from the Arctic/ Festvox database (see, and the annotation of the Le-Monde corpus is a by-product of the Neologos project, detailed by Krstulovi\u00b4c.", "labels": [], "entities": [{"text": "Arctic/ Festvox database", "start_pos": 67, "end_pos": 91, "type": "DATASET", "confidence": 0.9125999808311462}]}, {"text": "For each corpus, we have collected every phoneme, diphoneme, triphoneme, and their occurrences in each sentence so as to define the set U of units to cover and the matrix A.", "labels": [], "entities": []}, {"text": "A is built by collecting one sentence after the other following the ordering inside the corpus, and one unit after the other inside the sentences.", "labels": [], "entities": []}, {"text": "After this matrix translation, we obtain two description files and two index files.", "labels": [], "entities": []}, {"text": "The first file describes the matrix A and the second one the cost vector C.", "labels": [], "entities": []}, {"text": "Because of the low matrix density, we have chosen a sparse representation to save space and computation time: For instance, the 2-phoneme Gutenberg matrix is about 2.2% dense.", "labels": [], "entities": []}, {"text": "We only store the cells of A that have a non-zero value so as to get a sparse matrix.", "labels": [], "entities": []}, {"text": "The index files are made for the correspondence between the general covering problem and the application domain.", "labels": [], "entities": []}, {"text": "The implementation is made in C.", "labels": [], "entities": []}, {"text": "In terms of software engineering, our algorithms are working on an SCP that does not depend on the application data.", "labels": [], "entities": []}, {"text": "For example, there is no information on what types of units are to be covered.", "labels": [], "entities": []}, {"text": "The algorithms only have the matrix of occurrences A, the cost vector C, and the constraint vector B.", "labels": [], "entities": []}, {"text": "A set of translation files (from application data to SCP and from SCP to application data) is built before each computation.", "labels": [], "entities": []}, {"text": "As a consequence, there is no difficulty in addressing a different set of features to cover on the same or on a different corpus.", "labels": [], "entities": []}, {"text": "To study the achievements of ASA and LamSCP on different types of data, we have also chosen to address the \"k-covering of n-POS\" on the corpus Le-Monde.", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.8822785019874573}]}, {"text": "The grammatical and syntactical analyses are processed by the Synapse development analyzer presented in Synapse (2011).", "labels": [], "entities": [{"text": "Synapse development analyzer presented in Synapse (2011)", "start_pos": 62, "end_pos": 118, "type": "DATASET", "confidence": 0.6356689499484168}]}, {"text": "In order to consider a SCP with a substantial number of required units, a very detailed level of POS tagging has been selected, providing 141 distinct tags after analyzing Le-Monde.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.7173059582710266}]}, {"text": "For example, this level provides tags like \"Determiner male singular Article\" or \"Noun female singular,\" whereas the simplest level gives \"Determiner\" or \"Noun.\"", "labels": [], "entities": []}, {"text": "This latter level of description would have given only nine different POS tags after analyzing Le-Monde.", "labels": [], "entities": []}, {"text": "The main associated statistics are given in.", "labels": [], "entities": []}, {"text": "For these experiments of POS covering, the cost of a sentence is defined as its number of POS occurrences.", "labels": [], "entities": [{"text": "POS covering", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.9432108402252197}]}, {"text": "We used a PC with 2 CPUs (E5320/1.86GHz/4 cores/64bits) and 32 GB RAM for the phonological coverings and the POS coverings were computed using a PC with 8 CPUs (Intel Xeon X7550/2.00Ghz/8 cores/64bits) and 128 GB RAM.", "labels": [], "entities": []}, {"text": "Our implementations do not take advantage of any parallelism.", "labels": [], "entities": []}, {"text": "The following sections detail more precisely the different experiments conducted on French or English.", "labels": [], "entities": []}, {"text": "In the previous sections, different algorithms dealing with corpus reduction were introduced and studied.", "labels": [], "entities": [{"text": "corpus reduction", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.767851859331131}]}, {"text": "The proposed experiments mainly evaluate the effects of these algorithms in terms of corpus reduction but not according to a practical task.", "labels": [], "entities": [{"text": "corpus reduction", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7258226275444031}]}, {"text": "This section proposes an experiment to assess the impact of the corpus reduction on a unit selection speech synthesis system.", "labels": [], "entities": [{"text": "unit selection speech synthesis", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6267262250185013}]}, {"text": "As explained in Section 1, a corpus reduction fora TTS system is a trade-off between minimizing the recording and post-processing time to build the speech corpus and keeping the highest phonological richness of the corpus to ensure the quality of the synthetic speech.", "labels": [], "entities": []}, {"text": "The goal of this experiment is to measure this trade-off by evaluating the quality of the same TTS system fed with different speech corpora uttered by the same speaker.", "labels": [], "entities": []}, {"text": "Note that the intrinsic quality of this system is not the purpose here.", "labels": [], "entities": []}, {"text": "Firstly, a brief presentation of a state-of-the-art unit selection-based TTS system is proposed in Section 7.1.", "labels": [], "entities": [{"text": "TTS", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8217154741287231}]}, {"text": "The linguistic parameters used by the TTS system are detailed because they are linked to the required features in the reduction stage.", "labels": [], "entities": [{"text": "TTS", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.847014307975769}]}, {"text": "In Section 7.2, corpora used in the experiment are introduced.", "labels": [], "entities": []}, {"text": "The attributes to cover and the methodology of evaluation are described in Section 7.3; the results are given and discussed in Section 7.4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Statistics of the studied corpora.", "labels": [], "entities": []}, {"text": " Table 2  Statistics of 1-POS and 2-POS occurrences, corpus Le-Monde. On average, a sentence contains  27.64 POS with a standard deviation of 16.45.", "labels": [], "entities": []}, {"text": " Table 3  Statistics of the solutions of 1-covering of 2-phonemes computed by ASA and LamSCP from  Le-Monde. The last column represents the best lower bounds found by LamSCP.", "labels": [], "entities": [{"text": "ASA", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.6703847050666809}, {"text": "LamSCP", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.9299352169036865}, {"text": "LamSCP", "start_pos": 167, "end_pos": 173, "type": "DATASET", "confidence": 0.9609977602958679}]}, {"text": " Table 4  Experiment 2: Statistics based on 60 instances of a k-covering of 2-phonemes from Gutenberg.", "labels": [], "entities": []}, {"text": " Table 5  Ratios \u03c4 LamSCP and \u03c4 ASA for the k-covering of 2-phonemes from Gutenberg.", "labels": [], "entities": [{"text": "LamSCP", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.8100895285606384}, {"text": "ASA", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.7240696549415588}]}, {"text": " Table 8  Experiment 3: Statistics based on 35 experiments of a 1-covering of 3-phonemes from Gutenberg.", "labels": [], "entities": []}, {"text": " Table 9  Experiment 4: Statistics based on 30 experiments of a 1-covering of 3-phonemes from Le-Monde.", "labels": [], "entities": []}, {"text": " Table 10  Statistics of 50 instances of a k-covering of n-POS from Le-Monde.", "labels": [], "entities": []}, {"text": " Table 11  Characteristics of the two corpora.", "labels": [], "entities": []}, {"text": " Table 12  Statistics about the reduced corpora computed by ASA and LamSCP from Learning corpus.  The last column concerns the best lower bound found by LamSCP.", "labels": [], "entities": [{"text": "ASA", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.6362988948822021}, {"text": "LamSCP", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.9578672051429749}, {"text": "Learning corpus", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.7926939129829407}, {"text": "LamSCP", "start_pos": 153, "end_pos": 159, "type": "DATASET", "confidence": 0.9612285494804382}]}]}