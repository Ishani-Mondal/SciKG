{"title": [{"text": "Efficient Global Learning of Entailment Graphs", "labels": [], "entities": [{"text": "Efficient Global Learning of Entailment Graphs", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.5525669554869334}]}], "abstractContent": [{"text": "Entailment rules between predicates are fundamental to many semantic-inference applications.", "labels": [], "entities": []}, {"text": "Consequently, learning such rules has been an active field of research in recent years.", "labels": [], "entities": []}, {"text": "Methods for learning entailment rules between predicates that take into account dependencies between different rules (e.g., entailment is a transitive relation) have been shown to improve rule quality, but suffer from scalability issues, that is, the number of predicates handled is often quite small.", "labels": [], "entities": []}, {"text": "In this article, we present methods for learning transitive graphs that contain tens of thousands of nodes, where nodes represent predicates and edges correspond to entailment rules (termed entailment graphs).", "labels": [], "entities": []}, {"text": "Our methods are able to scale to a large number of predicates by exploiting structural properties of entailment graphs such as the fact that they exhibit a \"tree-like\" property.", "labels": [], "entities": []}, {"text": "We apply our methods on two data sets and demonstrate that our methods find high-quality solutions faster than methods proposed in the past, and moreover our methods for the first timescale to large graphs containing 20,000 nodes and more than 100,000 edges.", "labels": [], "entities": []}], "introductionContent": [{"text": "Performing textual inference is at the heart of many semantic inference applications, such as Question Answering (QA) and Information Extraction (IE).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.8472894191741943}, {"text": "Information Extraction (IE)", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.8521814107894897}]}, {"text": "A prominent generic Our method contains two main steps.", "labels": [], "entities": []}, {"text": "The first step is based on a sparsity assumption that even if an entailment graph contains many predicates, most of them do not entail one another, and thus we can decompose the graph into smaller components that can be solved more efficiently.", "labels": [], "entities": []}, {"text": "For example, the predicates X parent of Y, X child of Y and X relative of Y are independent from the predicates X works with Y, X boss of Y, and X manages Y, and thus we can consider each one of these two sets separately.", "labels": [], "entities": []}, {"text": "We prove that finding the optimal solution for each of the smaller components results in a global optimal solution for our optimization problem.", "labels": [], "entities": []}, {"text": "The second step proposes a polynomial heuristic approximation algorithm for finding a transitive set of edges in each one of the smaller components.", "labels": [], "entities": []}, {"text": "It is based on a novel modeling assumption that entailment graphs exhibit a \"tree-like\" property, which we term forest reducible.", "labels": [], "entities": []}, {"text": "For example, the graph in is not a tree, because the predicates X related to nausea and X associated with nausea form a cycle.", "labels": [], "entities": []}, {"text": "However, these two predicates are synonymous, and if they were merged into a single node, then the graph would become a tree.", "labels": [], "entities": []}, {"text": "We propose a simple iterative approximation algorithm, wherein each iteration a single node is deleted from the graph and then inserted back in away that improves the objective function value.", "labels": [], "entities": []}, {"text": "We prove that if we impose a constraint that entailment graphs must be forest reducible, then each iteration can be performed in linear time.", "labels": [], "entities": []}, {"text": "This results in an algorithm that can scale to entailment graphs containing tens of thousands of nodes.", "labels": [], "entities": []}, {"text": "We apply our algorithm on two data sets.", "labels": [], "entities": []}, {"text": "The first data set includes medium-sized entailment graphs where predicates are typed, that is, the arguments are restricted to belong to a particular semantic type (for instance, X person parent of Y person ).", "labels": [], "entities": []}, {"text": "We show that using our algorithm, we can substantially improve runtime, suffering from only a slight reduction in the quality of learned entailment graphs.", "labels": [], "entities": []}, {"text": "The second data set includes a much larger graph containing 20,000 untyped nodes, where applying state-of-the-art methods that use an ILP solver is completely impossible.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 134, "end_pos": 144, "type": "TASK", "confidence": 0.6101223975419998}]}, {"text": "We run our algorithm on this data set and demonstrate that we can learn knowledge bases with more than 100,000 entailment rules at a higher precision compared with local learning methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9896503686904907}]}, {"text": "The article is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we survey prior work on the learning of entailment rules.", "labels": [], "entities": []}, {"text": "We first focus on local methods (Section 2.1), that is, methods that handle each pair of predicates independently of other predicates, and then describe global methods (Section 2.2), that is, methods that take into account multiple predicates simultaneously.", "labels": [], "entities": []}, {"text": "Section 3 is the core of the article and describes our main algorithmic contributions.", "labels": [], "entities": []}, {"text": "After formalizing entailment rule learning as a graph optimization problem, we present the two steps of our algorithm.", "labels": [], "entities": []}, {"text": "Section 3.1 describes the first step, in which a large entailment graph is decomposed into smaller components.", "labels": [], "entities": []}, {"text": "Section 3.2 describes the second step, in which we develop an efficient heuristic approximation based on the assumption that entailment graphs are forest reducible.", "labels": [], "entities": []}, {"text": "Section 4 describes experiments on the first data set containing medium-sized entailment graphs with typed predicates.", "labels": [], "entities": []}, {"text": "Section 5 presents an empirical evaluation on a large graph containing 20,000 untyped predicates.", "labels": [], "entities": []}, {"text": "We also perform a qualitative analysis in Section 5.4 to further elucidate the behavior of our algorithm.", "labels": [], "entities": [{"text": "Section 5.4", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9085873365402222}]}, {"text": "Section 6 concludes the article.", "labels": [], "entities": []}, {"text": "This article is based on previous work), but expands over it in multiple directions.", "labels": [], "entities": []}, {"text": "Empirically, we present results on a novel data set (Section 5) that is by orders of magnitude larger than in the past.", "labels": [], "entities": []}, {"text": "Algorithmically, we present the Tree-Node-And-Component-Fix algorithm, which is an extension of the Tree-Node-Fix algorithm presented in and achieves best results in our experimental evaluation.", "labels": [], "entities": []}, {"text": "Theoretically, we provide an NP-hardness proof for the Max-Trans-Forest optimization problem presented in and an ILP formulation for it.", "labels": [], "entities": []}, {"text": "Last, we perform in Section 5.4 an extensive qualitative analysis of the graphs learned by our local and global algorithms from which we draw conclusions for future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this and the next section we empirically evaluate the algorithms presented in Section 3 on two different data sets.", "labels": [], "entities": []}, {"text": "Our first data set, presented in this section, comprises medium-sized graphs for which obtaining an exact solution is possible, and we demonstrate that our approximation methods substantially improve runtime while causing only a small degradation in performance compared with the optimal solution.", "labels": [], "entities": []}, {"text": "The graphs are also particularly suited for global optimization because graph predicates are typed, which substantially reduces their ambiguity.", "labels": [], "entities": [{"text": "global optimization", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8802669048309326}]}, {"text": "The second data set (Section 5) contains a graph with tens of thousands of untyped nodes, where exact inference is completely impossible.", "labels": [], "entities": []}, {"text": "We show that our methods scale to this graph and that transitivity improves performance even when predicates are untyped.", "labels": [], "entities": []}, {"text": "The resulting graph contains more than 100,000 entailment rules that can be utilized in downstream semantic applications.", "labels": [], "entities": []}, {"text": "The input to the algorithms presented in Section 3 is a set of nodes V and a weighting function w : V \u00d7 V \u2192 R.", "labels": [], "entities": []}, {"text": "We describe how those are constructed before presenting an experimental evaluation.", "labels": [], "entities": []}, {"text": "To evaluate performance we use the second half of the data set released by Zeichner, Berant, and Dagan (2012) as a test set.", "labels": [], "entities": [{"text": "second half of the data set released by Zeichner, Berant, and Dagan (2012)", "start_pos": 35, "end_pos": 109, "type": "DATASET", "confidence": 0.9018516715835122}]}, {"text": "This test set contains 3,283 annotated examples, where 1,734 are covered by the 10,000 nodes of our graph (649 positive examples and 1,085 negative examples).", "labels": [], "entities": []}, {"text": "As usual, for each algorithm we compute recall and precision with respect to the gold standard at various points by varying the sparseness parameter \u03bb.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9990984201431274}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9988518953323364}]}, {"text": "Most methods used over typed graphs in Section 4 (Exact-graph, Exact-forest, LPrelax, and GNF) are intractable because they require an ILP solver and our graph does not decompose into components that are small enough.", "labels": [], "entities": []}, {"text": "Therefore, we compare the Notrans local baseline, where transitivity is not used, with the efficient global methods TNF and TNCF.", "labels": [], "entities": [{"text": "Notrans local baseline", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.8834649920463562}]}, {"text": "Recall that in Section 4 we initialized TNF by applying an ILP solver in a sparse configuration on each graph component.", "labels": [], "entities": []}, {"text": "In this experiment the graph is too large to use an ILP solver and so we initialize TNF and TNCF with a simple heuristic we describe next.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.5632466375827789}]}, {"text": "We begin with an empty graph and first sort all pairs of predicates (i, j) for which w ij > 0, according to their weight w.", "labels": [], "entities": []}, {"text": "Then, we go over predicate pairs one-by-one and perform two operations.", "labels": [], "entities": []}, {"text": "First, we verify that inserting (i, j) into the graph does not violate the FRG assumption.", "labels": [], "entities": [{"text": "FRG", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9345259070396423}]}, {"text": "This is done by going overall edges (i, k) \u2208 E and checking that for every k either ( j, k) \u2208 E or (k, j) \u2208 E.", "labels": [], "entities": []}, {"text": "If this is the case, then the edge (i, j) is a valid candidate edge as the resulting reduced graph will remain a directed forest, otherwise adding it will result in i having two parents in the reduced graph, and we do not insert this edge.", "labels": [], "entities": []}, {"text": "Second, we compute the transitive closure T ij , which contains all node pairs that must be edges in the graph in case we insert (i, j), due to transitivity.", "labels": [], "entities": []}, {"text": "We compute the change in the value of the objective function if we were to insert T ij into the graph.", "labels": [], "entities": []}, {"text": "If this change improves the objective function value, we insert T ij into the graph (and so the value of the objective function increases monotonically).", "labels": [], "entities": []}, {"text": "We keep going over the candidate edges from highest score to lowest until the objective function can no longer be improved by inserting a candidate edge.", "labels": [], "entities": []}, {"text": "We term this initialization HTL-FRG, because we scan edges from high-score to low-score and maintain an FRG.", "labels": [], "entities": [{"text": "FRG", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.995377779006958}]}, {"text": "We add this initialization procedure as another baseline.", "labels": [], "entities": []}, {"text": "presents the precision-recall curve of all global algorithms compared with the local classifier for 0.05 \u2264 \u03bb \u2264 2.5.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.997398853302002}]}, {"text": "One evident property is that No-trans reaches higher recall values than the global algorithms, which means that adding a global transitivity constraint prevents adding correct edges that have positive weight, since this would cause the addition of many other edges that have negative weight.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9986124038696289}]}, {"text": "A possible reason for that is predicate ambiguity, where positive weight edges connect connectivity components through an ambiguous predicate.", "labels": [], "entities": []}, {"text": "This suggests that a natural direction for future research is to develop algorithms that do not impose transitivity constraints for rules i \u21d2 j and j \u21d2 k, if each rule refers to a different meaning of j.", "labels": [], "entities": []}, {"text": "One possible direction is to learn latent \"sense\" or \"topic\" variables for each rule (as suggested by Melamud et al. Melamud et al.).", "labels": [], "entities": []}, {"text": "Then, we can use the methods presented in this article  In this section we evaluate our methods on a graph with 20,000 nodes.", "labels": [], "entities": []}, {"text": "Again, we describe how the nodes V and the weighting function ware constructed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5  Recall, precision, F 1 , and the number of learned rules for No-trans and several global algorithms  for parameters of \u03bb for which recall is in the range 0.1-0.25.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9343178868293762}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9995699524879456}, {"text": "F 1", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.995652586221695}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9987599849700928}]}]}