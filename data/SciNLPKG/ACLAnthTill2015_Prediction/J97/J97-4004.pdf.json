{"title": [], "abstractContent": [{"text": "Tokenization is the process of mapping sentences from character strings into strings of words.", "labels": [], "entities": []}, {"text": "This paper sets out to study critical tokenization, a distinctive type of tokenization following the principle of maximum tokenization.", "labels": [], "entities": []}, {"text": "The objective in this paper is to develop its mathematical description and understanding.", "labels": [], "entities": []}, {"text": "The main results are as follows: (1) Critical points are all and only unambiguous toke~ boundaries for any character string on a complete dictionary; (2) Any critically tokenized word string is a minimal element in the partially ordered set of all tokenized word strings with respect to the word string cover relation; (3) Any tokenized string can be reproduced from a critically tokenized word string but not vice versa; (4) Critical tokenization forms the sound mathematical foundation for categorizing tokenization ambiguity into critical and hidden types, a precise mathematical understanding of conventional concepts like combinational and overlapping ambiguities ; (5) Many important maximum tokenization variations, such as forward and backward maximum matching and shortest tokenization, are all true subclasses of critical tokenization.", "labels": [], "entities": []}, {"text": "It is believed that critical tokenization provides a precise mathematical description of the principle of maximum tokenization.", "labels": [], "entities": []}, {"text": "Important implications and practical applications of critical tokenization in effective ambiguity resolution and in efficient tokenization implementation are also carefully examined.", "labels": [], "entities": [{"text": "effective ambiguity resolution", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6581722994645437}, {"text": "tokenization implementation", "start_pos": 126, "end_pos": 153, "type": "TASK", "confidence": 0.9359333217144012}]}], "introductionContent": [{"text": "Words, and tokens in general, are the primary building blocks in almost all linguistic theories (e.g., and language processing systems (e.g.,.", "labels": [], "entities": []}, {"text": "Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing.", "labels": [], "entities": []}, {"text": "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g.,; Sun and T'sou 1995;.", "labels": [], "entities": [{"text": "Chinese sentence tokenization", "start_pos": 128, "end_pos": 157, "type": "TASK", "confidence": 0.5729757944742838}]}, {"text": "The tokenization problem exists in almost all natural languages, including Japanese,, German (, and English, in various media, such as continuous speech and cursive handwriting, and in numerous applications, such as translation, recognition, indexing, and proofreading.", "labels": [], "entities": [{"text": "translation, recognition", "start_pos": 216, "end_pos": 240, "type": "TASK", "confidence": 0.6943603257338206}]}, {"text": "For Chinese, sentence tokenization is still an unsolved problem, which is in part due to its overall complexity but also due to the lack of a good mathematical description and understanding of the problem.", "labels": [], "entities": [{"text": "sentence tokenization", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7112610638141632}]}, {"text": "The theme in this paper is therefore to develop such a mathematical description.", "labels": [], "entities": []}, {"text": "In particular, this paper focuses on critical tokenization 1, a distinctive type of tokenization following the maximum principle.", "labels": [], "entities": []}, {"text": "What is to be established in this paper is the notion of critical tokenization itself, together with its precise descriptions and well-proved properties.", "labels": [], "entities": []}, {"text": "We will prove that critical points are all and only unambiguous token boundaries for any character string on a complete dictionary.", "labels": [], "entities": []}, {"text": "We will show that any critically tokenized word string is a minimal element in the partially ordered set of all tokenized word strings on the word string cover relation.", "labels": [], "entities": []}, {"text": "We will also show that any tokenized string can be reproduced from a critically tokenized word string but not vice versa.", "labels": [], "entities": []}, {"text": "In other words, critical tokenization is the most compact representation of tokenization.", "labels": [], "entities": []}, {"text": "In addition, we will show that critical tokenization forms a sound mathematical foundation for categorizing critical ambiguity and hidden ambiguity in tokenizations, which provides a precise mathematical understanding of conventional concepts like combinational and overlapping ambiguities.", "labels": [], "entities": []}, {"text": "Moreover, we will confirm that some important maximum tokenization variations, such as forward and backward maximum matching and shortest tokenization, are all subclasses of critical tokenization.", "labels": [], "entities": []}, {"text": "Based on a mathematical understanding of tokenization, we reported, in Guo (1997), a series of interesting findings.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9643339514732361}]}, {"text": "For instance, there exists an optimal algorithm that can identify all and only critical points, and thus all unambiguous token boundaries, in time proportional to the input character string length but independent of the size of the tokenization dictionary.", "labels": [], "entities": []}, {"text": "Tested on a representative corpus, about 98% of the critical fragments generated are by themselves desired tokens.", "labels": [], "entities": []}, {"text": "In other words, about 98% close-dictionary tokenization accuracy can be achieved efficiently without disambiguation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9565866589546204}]}, {"text": "Another interesting finding is that, for those critical fragments with critical ambiguities, by replacing the conventionally adopted meaning preservation criterion with the critical tokenization criterion, disagreements among (human) judges on the acceptability of a tokenization basically become non-existent.", "labels": [], "entities": []}, {"text": "Consequently, an objective (human) analysis and annotation of all (critical) tokenizations in a corpus becomes achievable, which in turn leads to some important observations.", "labels": [], "entities": []}, {"text": "For instance, we observed from a Chinese corpus of four million morphemes a very strong tendency to have one tokenization per source.", "labels": [], "entities": []}, {"text": "Naturally, this observation suggests tokenization disambiguation strategies notably different from the mainstream best-path-finding strategy.", "labels": [], "entities": [{"text": "tokenization disambiguation", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.9703246355056763}]}, {"text": "For instance, the simple strategy of tokenization by memorization alone could easily exhibit critical ambiguity resolution accuracy of no less than 90%, which is notably higher than what has been achieved in the literature.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9723515510559082}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.6029461622238159}]}, {"text": "Moreover, it has been observed that critical tokenization can also provide helpful guidance in identifying hidden ambiguities and in determining unregistered (unknown) tokens.", "labels": [], "entities": []}, {"text": "While these are just some of the very primitive findings, they are nevertheless promising and motivate 1 All terms mentioned here will be precisely defined later in this paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}