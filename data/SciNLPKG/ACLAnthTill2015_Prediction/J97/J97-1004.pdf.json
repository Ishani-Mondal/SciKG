{"title": [{"text": "Developing and Empirically Evaluating Robust Explanation Generators: The KNIGHT Experiments", "labels": [], "entities": [{"text": "Developing and Empirically Evaluating Robust Explanation Generators", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.7624955815928323}]}], "abstractContent": [{"text": "To explain complex phenomena, an explanation system must be able to select information from a formal representation of domain knowledge, organize the selected information into multisenten-tial discourse plans, and realize the discourse plans in text.", "labels": [], "entities": []}, {"text": "Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for explanation , empirical results have been limited.", "labels": [], "entities": []}, {"text": "This paper reports on a seven-year effort to empirically study explanation generation from semantically rich, large-scale knowledge bases.", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.854170173406601}]}, {"text": "In particular, it describes KNIGHT, a robust explanation system that constructs multisentential and multi-paragraph explanations from the Biology Knowledge Base, a large-scale knowledge base in the domain of botanical anatomy, physiology, and development.", "labels": [], "entities": [{"text": "Biology Knowledge Base", "start_pos": 138, "end_pos": 160, "type": "DATASET", "confidence": 0.7648771603902181}]}, {"text": "We introduce the Two-Panel evaluation methodology and describe how KNIGHT'S performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system.", "labels": [], "entities": []}, {"text": "In this evaluation, KNIGHT scored within \"half a grade\" of domain experts, and its performance exceeded that of one of the domain experts.", "labels": [], "entities": [{"text": "KNIGHT", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.5362592339515686}]}], "introductionContent": [{"text": "In the course of their daily affairs, scientists explain complex phenomena--both to one another and to lay people--in a manner that facilitates clear communication.", "labels": [], "entities": []}, {"text": "Similarly, physicians, lawyers, and teachers are equally facile at generating explanations in their respective areas of expertise.", "labels": [], "entities": []}, {"text": "In an effort to computationalize this critical ability, research in natural language generation has addressed abroad range of issues in automatically constructing text from formal representations of domain knowledge.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6679763495922089}]}, {"text": "Research on text planning) has developed techniques for determining the content and organization of many genres, and explanation generation) in particular has been the subject of intense investigation.", "labels": [], "entities": [{"text": "text planning", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.8379704654216766}, {"text": "explanation generation", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7402757555246353}]}, {"text": "In addition to exploring a panorama of application domains, the explanation community has begun to assemble these myriad designs into a coherent framework.", "labels": [], "entities": []}, {"text": "As a result, we have begun to see a crystalization of the major components, as well as detailed analyses of their roles in explanation.", "labels": [], "entities": []}, {"text": "Despite this success, empirical results in explanation generation are limited.", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.9548218548297882}]}, {"text": "Although techniques for developing and evaluating robust explanation generation should yield results that are more conclusive than those produced by prototype, \"proof-of-concept\" systems, with only a few notable exceptions, most work has adopted a research methodology in which a proof-of-concept system is constructed and its operation is analyzed on a few examples.", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7294659465551376}]}, {"text": "While isolating one or a small number of problems enables researchers to consider particular issues in detail, it is difficult to gauge the scalability and robustness of a proposed approach.", "labels": [], "entities": []}, {"text": "A critical factor contributing to the dearth of empirical results is the absence of semantically rich, large-scale knowledge bases (KBs).", "labels": [], "entities": []}, {"text": "Knowledge bases housing tens of thousands of different concepts and hundreds of different relations could furnish ample raw materials for empirical study, but no work in explanation generation has been conducted or empirically evaluated in the context of these knowledge bases.", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.7427060306072235}]}, {"text": "To empirically study explanation generation from semantically rich, large-scale knowledge bases, we undertook a seven-year experiment.", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.9193489849567413}]}, {"text": "First, our domain experts (one employed full-time) constructed the Biology Knowledge Base (), a very large structure representing more than 180,000 facts about botanical anatomy, physiology, and development.", "labels": [], "entities": [{"text": "Biology Knowledge Base", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.7654806276162466}]}, {"text": "Second, we designed, implemented, and empirically evaluated KNIGHT, a robust explanation system that extracts information from the Biology Knowledge Base, organizes it, and realizes it in multisentential and multiparagraph expository explanations of complex biological phenomena.", "labels": [], "entities": []}, {"text": "Third, we developed a novel evaluation methodology for gauging the effectiveness of explanation systems and employed this methodology to evaluate KNIGHT.", "labels": [], "entities": [{"text": "KNIGHT", "start_pos": 146, "end_pos": 152, "type": "DATASET", "confidence": 0.5682370662689209}]}, {"text": "This paper describes the lessons learned during the course of the \"KNIGHT experiments.\"", "labels": [], "entities": []}, {"text": "In the spirit of EDGE and PAULINE, which synthesize work in interactive explanation systems and generational pragmatics, respectively, KNIGHT addresses abroad range of issues, all in the context of semantically rich, large-scale knowledge bases: \u2022 Robust Knowledge-Base Access: KNIGHT exploits a library of robust knowledge-base access methods that insulate discourse planners from the idiosyncracies and errors in knowledge bases.", "labels": [], "entities": []}, {"text": "These \"view construction\" methods selectively extract coherent packets of propositions about the structure and function of objects, the changes made to objects by processes, and the temporal attributes and temporal decompositions of processes.", "labels": [], "entities": [{"text": "view construction", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7477333545684814}]}, {"text": "\u2022 Discourse-Knowledge Engineering: Discourse-knowledge engineers, i.e., knowledge engineers who encode discourse knowledge, should be able to inspect and easily modify discourse-planning specifications for rapid iterative refinement.", "labels": [], "entities": []}, {"text": "The Explanation Design Package (EDP) formalism is a convenient, schema-like) programming language for text planning.", "labels": [], "entities": [{"text": "text planning", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.7847329080104828}]}, {"text": "Because the EDP formalism is a hybrid of the declarative and procedural paradigms, discourse-knowledge engineers can easily understand EDPs, modify them, and use them to represent new discourse knowledge.", "labels": [], "entities": []}, {"text": "EDPS have been used by KNIGHT to generate hundreds of expository explanations of biological objects and processes.", "labels": [], "entities": [{"text": "EDPS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7720452547073364}, {"text": "KNIGHT", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8155466318130493}]}, {"text": "\u2022 Explanation Planning: KNIGHT employs a robust explanation planner that selects EDPS and applies them to invoke knowledge-base accessors.", "labels": [], "entities": []}, {"text": "The explanation planner considers the desired length of explanations and the relative importance of subtopics as it constructs explanation plans encoding content and organization.", "labels": [], "entities": []}, {"text": "\u2022 Functional Realization: KNIGHT's functional realization system is built on top of a unification-based surface generator with a large systemic grammar).", "labels": [], "entities": []}, {"text": "To assess KNIGHT'S performance, we developed the Two-Panel evaluation methodology for natural language generation and employed it in the most extensive and rigorous empirical evaluation ever conducted on an explanation system.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6556588411331177}]}, {"text": "In this study, KNIGHT constructed explanations on randomly chosen topics from the Biology Knowledge Base.", "labels": [], "entities": [{"text": "Biology Knowledge Base", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.8233495652675629}]}, {"text": "A panel of domain experts was instructed to produce explanations on these same topics, and both KNIGHT'S explanations and the explanations produced by this panel were submitted to a second panel of domain experts.", "labels": [], "entities": []}, {"text": "The second panel then graded all of the explanations on several dimensions with an A-F scale.", "labels": [], "entities": [{"text": "A-F scale", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.973558247089386}]}, {"text": "KNIGHT scored within approximately half a grade of the domain experts, and its performance exceeded that of one of the domain experts.", "labels": [], "entities": [{"text": "KNIGHT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6692320108413696}]}, {"text": "This paper is structured as follows: The task of explanation generation is characterized and the Biology Knowledge Base is described.", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.9350136518478394}, {"text": "Biology Knowledge Base", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.8401344815889994}]}, {"text": "A brief description of KNIGHT's knowledge-base access methods is followed by (1) a description of the EDP language, (2) KNIGHT'S explanation planner, and (3) an overview of the realization techniques.", "labels": [], "entities": []}, {"text": "The empirical evaluation is then discussed in some detail.", "labels": [], "entities": []}, {"text": "The paper concludes with discussions of related work and future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating the performance of explanation systems is a critical and nontrivial problem.", "labels": [], "entities": []}, {"text": "Although gauging the performance of explanation systems is inherently difficult, five evaluation criteria can be applied.", "labels": [], "entities": []}, {"text": "\u2022 Coherence: A global assessment of the overall quality of the explanations generated by a system.", "labels": [], "entities": []}, {"text": "\u2022 Content: The extent to which the information is adequate and focused.", "labels": [], "entities": []}, {"text": "\u2022 Organization: The extent to which the information is well organized.", "labels": [], "entities": []}, {"text": "\u2022 Writing style: The quality of the prose.", "labels": [], "entities": []}, {"text": "\u2022 Correctness: For scientific explanations, the extent to which the explanations are in accord with the established scientific record.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 2, "end_pos": 13, "type": "METRIC", "confidence": 0.9760277271270752}]}, {"text": "In addition to performing well on the evaluation criteria, if explanation systems are to make the difficult transition from research laboratories to field applications, we want them to exhibit two important properties, both of which significantly affect scalability.", "labels": [], "entities": []}, {"text": "First, these systems' representation of discourse knowledge should be easily inspected and modified.", "labels": [], "entities": []}, {"text": "To develop explanation systems fora broad range of domains, tasks, and question types, discourse-knowledge engineers must be able to create and efficiently debug the discourse knowledge that drives the systems' behavior.", "labels": [], "entities": []}, {"text": "The second property that explanation systems should exhibit is robustness.", "labels": [], "entities": []}, {"text": "Despite the complex and possibly malformed representational structures that an explanation system may encounter in its knowledge base, it should be able to cope with these structures and construct reasonably well-formed explanations.", "labels": [], "entities": []}, {"text": "Traditionally, research projects in explanation generation have not included empirical evaluations.", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.928533673286438}]}, {"text": "Conducting a formal study with a generator has posed difficulties for at least three reasons: the absence of large-scale knowledge bases; the problem of robustness; and the subjective nature of the task.", "labels": [], "entities": []}, {"text": "First, the field of explanation generation has experienced a dearth of \"raw materials.\"", "labels": [], "entities": [{"text": "explanation generation", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9130795001983643}]}, {"text": "The task of an explanation generator is three-fold: to extract information from a knowledge base, to organize this information, and to translate it to natural language.", "labels": [], "entities": []}, {"text": "Unless an explanation generator has access to a sufficiently large knowledge base, the first step--and hence the second and third--cannot be carried out enough times to evaluate the system empirically.", "labels": [], "entities": []}, {"text": "Unfortunately, because of the tremendous cost of construction, large-scale knowledge bases are scarce.", "labels": [], "entities": []}, {"text": "Second, even if large-scale knowledge bases were more plentiful, an explanation generator cannot be evaluated unless it is sufficiently robust to produce many explana-  The Two-Panel evaluation methodology can be used to empirically evaluate natural language generation work.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 242, "end_pos": 269, "type": "TASK", "confidence": 0.6618611315886179}]}, {"text": "We developed this methodology, which involves two pan-els of domain experts, to combat the inherent subjectivity of NLG: although multiple judges will rarely reach a consensus, their collective opinion provides persuasive evidence about the quality of explanations.", "labels": [], "entities": []}, {"text": "To ensure the integrity of the evaluation results, a central stipulation of the methodology is that the following condition be maintained throughout the study: Computer Blindness: None of the participants can be aware that some texts are machine-generated or, for that matter, that a computer is in anyway involved in the study.", "labels": [], "entities": []}, {"text": "The methodology involves four steps: 1.", "labels": [], "entities": []}, {"text": "Generation of explanations by computer.", "labels": [], "entities": [{"text": "Generation of explanations", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8792776664098104}]}, {"text": "Each of these is discussed in turn.", "labels": [], "entities": []}, {"text": "Because KNIGHT's operation is initiated when a user poses a question, the first task was to select the questions it would be asked.", "labels": [], "entities": []}, {"text": "To this end, we combed the Biology Knowledge Base for concepts that could furnish topics for questions.", "labels": [], "entities": [{"text": "Biology Knowledge Base", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.7812657455603281}]}, {"text": "Although the knowledge base focuses on botanical anatomy, physiology, and development, it also contains a substantial amount of information about biological taxons.", "labels": [], "entities": []}, {"text": "Because this latter area is significantly less developed, we ruled out concepts about taxons.", "labels": [], "entities": []}, {"text": "In addition, we ruled out concepts that were too abstract (e.g., Object).", "labels": [], "entities": []}, {"text": "We then requested KNIGHT to generate explanations about the 388 concepts that passed through these filters.", "labels": [], "entities": []}, {"text": "To thoroughly exercise KNIGHT'S organizational abilities, we were most interested in observing its performance on longer explanations.", "labels": [], "entities": [{"text": "KNIGHT'S organizational", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8070762753486633}]}, {"text": "Hence, we eliminated explanations of concepts that were sparsely represented in the knowledge base.", "labels": [], "entities": []}, {"text": "To this end, we passed the 388 explanations through a \"length filter\": explanations that consisted of at least 3 sentences were retained; shorter explanations were disposed of.", "labels": [], "entities": []}, {"text": "17 This produced 87 explanations, of which 48 described objects and 39 described processes.", "labels": [], "entities": []}, {"text": "Finally, to test an equal number of objects and processes, we randomly chose 30 objects and 30 processes.", "labels": [], "entities": []}, {"text": "Two Panels of Domain Experts.", "labels": [], "entities": []}, {"text": "To address the difficult problem of subjectivity, we assembled 12 domain experts, all of whom were Ph.D. students or post-doctoral scientists in biology.", "labels": [], "entities": []}, {"text": "Because we wanted to gauge KNIGHT's performance relative to humans, we assigned each of the experts to one of two panels: the Writing Panel and the Judging Panel.", "labels": [], "entities": [{"text": "KNIGHT", "start_pos": 27, "end_pos": 33, "type": "TASK", "confidence": 0.9148815274238586}]}, {"text": "By securing the services of such a large number of domain experts, we were able to form relatively large panels of 4 writers and 8 judges).", "labels": [], "entities": []}, {"text": "To promote high-quality human-generated explanations, we assigned the 4 most experienced experts to the Writing Panel.", "labels": [], "entities": []}, {"text": "The remaining 8 experts were assigned to the Judging Panel to evaluate explanations.", "labels": [], "entities": []}, {"text": "To minimize the effect of factors that might make it difficult for judges to compare KNIGHT's explanations with those of domain experts, we took three precautions.", "labels": [], "entities": []}, {"text": "First, we attempted to control for the length of explanations.", "labels": [], "entities": []}, {"text": "Although we could not impose hard constraints, we made suggestions about how long atypical explanation might be.", "labels": [], "entities": []}, {"text": "Second, to make the \"level\" of the explanations comparable, we asked writers to compose explanations fora particular audience, freshman biology students.", "labels": [], "entities": []}, {"text": "Third, so that the general topics of discussion would be comparable, we asl<ed writers to focus on anatomy, physiology, and development.", "labels": [], "entities": []}, {"text": "To ensure that the difficulty of the concepts assigned to the writers were the same as those assigned to KNIGHT, the writers were given the task of explaining exactly the same set of concepts that KNIGHT had explained.", "labels": [], "entities": [{"text": "KNIGHT", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.8799856305122375}]}, {"text": "Because we wanted to give writers an opportunity to explain both objects and processes, each writer was given an approximately equal number of objects and processes.", "labels": [], "entities": []}, {"text": "Each of the four writers was given 15 concepts to explain, and each concept was assigned to exactly one writer.", "labels": [], "entities": []}, {"text": "We then transcribed their handwritten explanations and put them and KNIGHT'S explanations into an identical format.", "labels": [], "entities": [{"text": "KNIGHT'S", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.8224026560783386}]}, {"text": "At this point, we had a pool of 120 explanations: 60 of these pertained to objects (30 written by biologists and 30 by KNIGHT), and the other 60 pertained to processes (also 30 written by biologists and 30 by KNIGHT).", "labels": [], "entities": [{"text": "KNIGHT", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.9274165630340576}, {"text": "KNIGHT", "start_pos": 209, "end_pos": 215, "type": "DATASET", "confidence": 0.92447829246521}]}, {"text": "We then submitted the explanations to the panel of eight judges.", "labels": [], "entities": []}, {"text": "The judges were not informed of the source of the explanations, and all of the explanations appeared in the same format.", "labels": [], "entities": []}, {"text": "Each judge was given 15 explanations to evaluate.", "labels": [], "entities": []}, {"text": "Judges were asked to rate the explanations on several dimensions: overall quality and coherence, content, organization, writing style, and correctness.", "labels": [], "entities": []}, {"text": "To provide judges with a familiar rating scale, they were asked to assign letters grades (A, B, C, D, or F) to each explanation on each of the dimensions.", "labels": [], "entities": [{"text": "F", "start_pos": 105, "end_pos": 106, "type": "METRIC", "confidence": 0.9633598327636719}]}, {"text": "Because carefully evaluating multiple dimensions of explanations is a labor-intensive task, time considerations required us to limit the number of explanations submitted to each judge.", "labels": [], "entities": []}, {"text": "Hence, we assigned each judge a set of 15 explanations.", "labels": [], "entities": []}, {"text": "(On average, each judge took an hour to evaluate 15 explanations.)", "labels": [], "entities": []}, {"text": "We assigned explanations to judges using an allocation policy that obeyed the following four constraints: \u2022 System-Human Division: Each judge received explanations that were approximately evenly divided between those that were produced by KNIGHT and those that were produced by biologists.", "labels": [], "entities": [{"text": "KNIGHT", "start_pos": 239, "end_pos": 245, "type": "DATASET", "confidence": 0.8702523708343506}]}, {"text": "\u2022 Object-Process Division: Each judge received explanations that were approximately evenly divided between objects and processes.", "labels": [], "entities": [{"text": "Object-Process Division", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.7612235844135284}]}, {"text": "\u2022 Single-Explanation Restriction: No judge received two explanations of the same concept.", "labels": [], "entities": []}, {"text": "TM \u2022 Multijudge Stipulation: The explanations written by each writer were parceled out to at least two judges, i.e., rather than having one judge evaluate one writer's explanations, that writer's explanations were distributed among multiple judges.", "labels": [], "entities": [{"text": "Multijudge Stipulation", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.8065981566905975}]}, {"text": "It is important to emphasize again that the judges were not made aware of the purpose of the experiment, nor were they told that any of the explanations were computergenerated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4  Differences and significance.", "labels": [], "entities": [{"text": "significance", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9783750176429749}]}]}