{"title": [], "abstractContent": [{"text": "Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research.", "labels": [], "entities": []}, {"text": "To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm.", "labels": [], "entities": []}, {"text": "In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters.", "labels": [], "entities": []}, {"text": "The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995).", "labels": [], "entities": []}, {"text": "To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields.", "labels": [], "entities": []}, {"text": "In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations.", "labels": [], "entities": []}, {"text": "The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Stochastic versions of regular grammars and context-free grammars have received a great deal of attention in computational linguistics for the last several years, and basic techniques of stochastic parsing and parameter estimation have been known for decades.", "labels": [], "entities": [{"text": "stochastic parsing", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.6133005023002625}, {"text": "parameter estimation", "start_pos": 210, "end_pos": 230, "type": "TASK", "confidence": 0.6997570991516113}]}, {"text": "However, regular and context-free grammars are widely deemed linguistically inadequate; standard grammars in computational linguistics are attribute-value (AV) grammars of some variety.", "labels": [], "entities": []}, {"text": "Before the advent of statistical methods, regular and context-free grammars were considered too inexpressive for serious consideration, and even now the reliance on stochastic versions of the less-expressive grammars is often seen as an expedient necessitated by the lack of an adequate stochastic version of attribute-value grammars.", "labels": [], "entities": []}, {"text": "Proposals have been made for extending stochastic models developed for the regular and context-free cases to grammars with constraints.", "labels": [], "entities": []}, {"text": "1 Brew (1995) sketches a probabilistic version of Head-Driven Phrase Structure Grammar (HPSG).", "labels": [], "entities": [{"text": "1 Brew (1995)", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8971522927284241}, {"text": "Head-Driven Phrase Structure Grammar (HPSG)", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.726566629750388}]}, {"text": "He proposes a stochastic process for generating attribute-value structures, that is, directed acyclic graphs (dags).", "labels": [], "entities": []}, {"text": "A dag is generated starting from a single node labeled with the (unique) most general type.", "labels": [], "entities": []}, {"text": "Each type S has a set of maximal subtypes T1 .....", "labels": [], "entities": []}, {"text": "To expand anode labeled S, one chooses a maximal subtype T stochastically.", "labels": [], "entities": []}, {"text": "One then considers equating the current node with other nodes of type T, making a stochastic yes/no de-type X.", "labels": [], "entities": []}, {"text": "In these terms, Brew and Eisele propose estimating parameters as the empirical relative frequency of the corresponding rules.", "labels": [], "entities": []}, {"text": "That is, the weight of the rule X ---+ ~i is obtained by counting the number of times X rewrites as ~i in the training corpus, divided by the total number of times X is rewritten in the training corpus.", "labels": [], "entities": []}, {"text": "For want of a standard term, let us call these estimates Empirical Relative Frequency (ERF) estimates.", "labels": [], "entities": [{"text": "Empirical Relative Frequency (ERF)", "start_pos": 57, "end_pos": 91, "type": "METRIC", "confidence": 0.9405982295672098}]}, {"text": "To deal with incomplete data, both Brew and Eisele appeal to the Expectation-Maximization (EM) algorithm, applied however to ERF rather than maximum-likelihood estimates.", "labels": [], "entities": [{"text": "Expectation-Maximization (EM) algorithm", "start_pos": 65, "end_pos": 104, "type": "METRIC", "confidence": 0.913491940498352}]}, {"text": "Under certain independence conditions, ERF estimates are maximum-likelihood estimates.", "labels": [], "entities": []}, {"text": "Unfortunately, these conditions are violated when there are context dependencies of the sort found in attribute-value grammars, as will be shown below.", "labels": [], "entities": []}, {"text": "As a consequence, applying the ERF method to attribute-value grammars' does not generally yield maximum-likelihood estimates.", "labels": [], "entities": []}, {"text": "This is true whether one uses EM or not--a method that yields the \"wrong\" estimates on complete data does not improve when EM is used to extend the method to incomplete data.", "labels": [], "entities": []}, {"text": "Eisele identifies an important symptom that something is amiss with ERF estimates: the probability distribution over proof trees that one obtains does not agree Abney Stochastic Attribute-Value Grammars with the frequency of proof trees in the training corpus.", "labels": [], "entities": [{"text": "Abney", "start_pos": 161, "end_pos": 166, "type": "METRIC", "confidence": 0.9689173102378845}]}, {"text": "Eisele recognizes that this problem arises only where there are context dependencies.", "labels": [], "entities": []}, {"text": "Fortunately, solutions to the context-dependency problem have been described (and indeed are currently enjoying a surge of interest) in statistics, machine learning, and statistical pattern recognition, particularly image processing.", "labels": [], "entities": [{"text": "statistical pattern recognition", "start_pos": 170, "end_pos": 201, "type": "TASK", "confidence": 0.7277227838834127}, {"text": "image processing", "start_pos": 216, "end_pos": 232, "type": "TASK", "confidence": 0.7925231754779816}]}, {"text": "The models of interest are known as random fields.", "labels": [], "entities": []}, {"text": "Random fields can be seen as a generalization of Markov chains and stochastic branching processes.", "labels": [], "entities": []}, {"text": "Markov chains are stochastic processes corresponding to regular grammars and random branching processes are stochastic processes corresponding to context-free grammars.", "labels": [], "entities": []}, {"text": "The evolution of a Markov chain describes a line, in which each stochastic choice depends only on the state at the immediately preceding time-point.", "labels": [], "entities": []}, {"text": "The evolution of a random branching process describes a tree in which a finite-state process may spawn multiple child processes at the next time-step, but the number of processes and their states depend only on the state of the unique parent process at the preceding time-step.", "labels": [], "entities": []}, {"text": "In particular, stochastic choices are independent of other choices at the same time-step: each process evolves independently.", "labels": [], "entities": []}, {"text": "If we permit re-entrancies, that is, if we permit processes to re-merge, we generally introduce context-sensitivity.", "labels": [], "entities": []}, {"text": "In order to re-merge, processes must be \"in synch,\" which is to say, they cannot evolve incomplete independence of one another.", "labels": [], "entities": []}, {"text": "Random fields area particular class of multidimensional random processes, that is, processes corresponding to probability distributions over an arbitrary graph.", "labels": [], "entities": []}, {"text": "The theory of random fields can be traced back to; indeed, the probability distributions involved are known as Gibbs distributions.", "labels": [], "entities": []}, {"text": "To my knowledge, the first application of random fields to natural language was.", "labels": [], "entities": []}, {"text": "The problem of interest was how to combine a stochastic contextfree grammar with n-gram language models.", "labels": [], "entities": []}, {"text": "In the resulting structures, the probability of choosing a particular word is constrained simultaneously by the syntactic tree in which it appears and the choices of words at then preceding positions.", "labels": [], "entities": []}, {"text": "The contextsensitive constraints introduced by the n-gram model are reflected in re-entrancies in the structure of statistical dependencies, as in Statistical dependencies under the model of.", "labels": [], "entities": []}, {"text": "In this diagram, the choice of label on anode z with parent x and preceding wordy is dependent on the label of x and y, but conditionally independent of the label on any other node.", "labels": [], "entities": []}, {"text": "Della Pietra, Della Pietra, and Lafferty (1995, henceforth, DD&L) also apply random fields to natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.6387999256451925}]}, {"text": "The application they consider is the induction of English orthographic constraints--inducing a grammar of possible English words.", "labels": [], "entities": []}, {"text": "DD&L describe an algorithm called Improved Iterative Scaling (IIS) for selecting informative features of words to construct a random field, and for setting the parameters of the field optimally fora given set of features, to model an empirical word distribution.", "labels": [], "entities": [{"text": "DD&L", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8570024371147156}, {"text": "Improved Iterative Scaling (IIS)", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7905847529570261}]}, {"text": "It is not immediately obvious how to use the IIS algorithm to equip attribute-value grammars with probabilities.", "labels": [], "entities": []}, {"text": "In brief, the difficulty is that the IIS algorithm requires the computation of the expectations, under random fields, of certain functions; in general, computing these expectations involves summing overall configurations (all possible character sequences, in the orthography application), which is not possible when the configuration space is large.", "labels": [], "entities": [{"text": "IIS", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9633892774581909}]}, {"text": "Instead, DD&L use Gibbs sampling to estimate the needed expectations.", "labels": [], "entities": []}, {"text": "Gibbs sampling is possible for the application that DD&L consider.", "labels": [], "entities": []}, {"text": "A prerequisite for Gibbs sampling is that the configuration space be closed under relabeling of graph nodes.", "labels": [], "entities": [{"text": "Gibbs sampling", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.7759634852409363}]}, {"text": "In the orthography application, the configuration space is the set of possible English words, represented as finite linear graphs labeled with ASCII characters.", "labels": [], "entities": []}, {"text": "Every way of changing a label, that is, every substitution of one ASCII character fora different one, yields a possible English word.", "labels": [], "entities": []}, {"text": "By contrast, the set of graphs admitted by an attribute-value grammar G is highly constrained.", "labels": [], "entities": []}, {"text": "If one changes an arbitrary node label in a dag admitted by G, one does not necessarily obtain anew dag admitted by G. Hence, Gibbs sampling is not applicable.", "labels": [], "entities": []}, {"text": "However, I will show that a more general sampling method, the Metropolis-Hastings algorithm, can be used to compute the maximum-likelihood estimate of the parameters of AV grammars.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Computing the divergence of ql from ft.", "labels": [], "entities": []}, {"text": " Table 2  Computing the divergence of q' from ft.", "labels": [], "entities": []}, {"text": " Table 4  Estimating the parameters of G2 using the ERF method.", "labels": [], "entities": [{"text": "G2", "start_pos": 39, "end_pos": 41, "type": "DATASET", "confidence": 0.875059187412262}]}]}