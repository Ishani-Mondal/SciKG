{"title": [], "abstractContent": [{"text": "The sentence is a standard textual unit in natural language processing applications.", "labels": [], "entities": []}, {"text": "In many languages the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.", "labels": [], "entities": []}, {"text": "As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation.", "labels": [], "entities": [{"text": "sentence boundary disambiguation", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.671837588151296}]}, {"text": "The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark.", "labels": [], "entities": []}, {"text": "Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques.", "labels": [], "entities": [{"text": "sentence analysis", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.8231788575649261}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9993206262588501}]}, {"text": "The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on French and German.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen a dramatic increase in the availability of on-line text collections, which are useful in many areas of computational linguistics research.", "labels": [], "entities": [{"text": "computational linguistics research", "start_pos": 126, "end_pos": 160, "type": "TASK", "confidence": 0.7464018960793813}]}, {"text": "One active area of research is the development of algorithms for aligning sentences in parallel corpora.", "labels": [], "entities": []}, {"text": "The success of most natural language processing (NLP) algorithms, including multilingual sentence alignment algorithms, 1 part-of-speech taggers (, and parsers, depends on prior knowledge of the location of sentence boundaries.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7333559691905975}]}, {"text": "Segmenting a text into sentences is a nontrivial task, however, since in English and many other languages the end-of-sentence punctuation marks are ambiguous.", "labels": [], "entities": [{"text": "Segmenting a text into sentences", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8973762512207031}]}, {"text": "2 A period, for example, can denote a decimal point, an abbreviation, the end of a sentence, or even an abbreviation at the end of a sentence.", "labels": [], "entities": []}, {"text": "Exclamation points and question marks can occur within quotation marks or parentheses as well as at the end of a sentence.", "labels": [], "entities": [{"text": "Exclamation points and question marks", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8038722157478333}]}, {"text": "Ellipsis, a series of periods (...), can occur both within sentences and at The existence of punctuation in grammatical subsentences suggests the possibility of a further decomposition of the sentence boundary problem into types of sentence boundaries, one of which would be \"embedded sentence boundary.\"", "labels": [], "entities": []}, {"text": "Such a distinction might be useful for certain applications that analyze the grammatical structure of the sentence.", "labels": [], "entities": []}, {"text": "However, in this work we will only address the simpler problem of determining boundaries between sentences, finding that which Nunberg (1990) calls the \"text-sentence.\"", "labels": [], "entities": []}, {"text": "In examples, the word immediately preceding and the word immediately following a punctuation mark provide important information about its role in the sentence.", "labels": [], "entities": []}, {"text": "However, more context maybe necessary, such as when punctuation occurs in a subsentence within quotation marks or parentheses, as seen in example (2), or when an abbreviation appears at the end of a sentence, as seen in (5a): Examples (5a-b) also show some problems inherent in relying on brittle features, such as capitalization, when determining sentence boundaries.", "labels": [], "entities": []}, {"text": "The initial capital in Saturday does not necessarily indicate that Saturday is the first word in the sentence.", "labels": [], "entities": []}, {"text": "As a more dramatic example, some important kinds of text consist only of upper-case letters, thus thwarting any system that relies on capitalization rules.", "labels": [], "entities": []}, {"text": "Another obstacle to systems that rely on brittle features is that many texts are not well-formed.", "labels": [], "entities": []}, {"text": "One such class of texts are those that are the output of optical character recognition (OCR); typically these texts contain many extraneous or incorrect characters.", "labels": [], "entities": [{"text": "optical character recognition (OCR)", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.780568132797877}]}, {"text": "This article presents an efficient, trainable system for sentence boundary disambiguation that circumvents these obstacles.", "labels": [], "entities": [{"text": "sentence boundary disambiguation", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.7129685282707214}]}, {"text": "The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that determines whether the punctuation mark is a sentence boundary or serves another purpose in the sentence.", "labels": [], "entities": []}, {"text": "Satz is very fast in both training and sentence analysis; training is accomplished in less than one minute on a workstation, and it can process 10,000 sentences per minute.", "labels": [], "entities": [{"text": "sentence analysis", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7748811841011047}]}, {"text": "The combined robustness and accuracy of the system surpasses existing techniques, consistently producing an error rate less than 1.5% on a range of corpora and languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994901418685913}, {"text": "error rate", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9684076011180878}]}, {"text": "It requires only a small lexicon (which can be less than 5,000 words) and a training corpus of 300-500 sentences.", "labels": [], "entities": []}, {"text": "The following sections discuss related work and the criteria used to evaluate such work, describe our system in detail, and present the results of applying the system to a variety of texts.", "labels": [], "entities": []}, {"text": "The transferability of the system from English to other languages is also demonstrated on French and German text.", "labels": [], "entities": []}, {"text": "Finally, the learning-based system is shown to be able to improve the results of a more conventional system on especially difficult cases.", "labels": [], "entities": []}], "datasetContent": [{"text": "An important consideration when discussing related work is the mode of evaluation.", "labels": [], "entities": []}, {"text": "To aid our evaluation, we define a lower bound, an objective score which any reasonable algorithm should be able to match or better.", "labels": [], "entities": []}, {"text": "In our test collections, the ambiguous punctuation mark is used much more often as a sentence boundary marker than for any other purpose.", "labels": [], "entities": []}, {"text": "Therefore, a very simple, successful algorithm is one in which every potential boundary marker is labeled as the end-of-sentence.", "labels": [], "entities": []}, {"text": "Thus, for the task of sentence boundary disambiguation, we define the lower bound of a text collection as the percentage of possible sentence-ending punctuation marks in the text that indeed denote sentence boundaries.", "labels": [], "entities": [{"text": "sentence boundary disambiguation", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6412406265735626}]}, {"text": "Since the use of abbreviations in a text depends on the particular text and text genre, the number of ambiguous punctuation marks, and the corresponding lower bound, will vary dramatically depending on text genre.", "labels": [], "entities": []}, {"text": "For example, report on a Wall Street Journal corpus containing 14,153 periods per million tokens, whereas in the Tagged Brown corpus, the figure is only 10,910 periods per million tokens.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.960738018155098}, {"text": "Tagged Brown corpus", "start_pos": 113, "end_pos": 132, "type": "DATASET", "confidence": 0.642399529616038}]}, {"text": "Liberman and Church also report that 47% of the periods in the WSJ corpus denote abbreviations (thus a lower bound of 53%), compared to only 10% in the Brown corpus (lower bound 90%)).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8364561498165131}, {"text": "Brown corpus", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.890542209148407}]}, {"text": "In contrast, reports lower bound statistics ranging from 54.7% to 92.8% within a corpus of scientific abstracts.", "labels": [], "entities": []}, {"text": "Such a range of lower bound figures suggests the need fora robust approach that can adapt rapidly to different text requirements.", "labels": [], "entities": []}, {"text": "Another useful evaluation technique is the comparison of anew algorithm against a strong baseline algorithm.", "labels": [], "entities": []}, {"text": "The baseline algorithm should perform better than the lower bound and should represent a strong effort or a standard method for solving the problem at hand.", "labels": [], "entities": []}, {"text": "Although sentence boundary disambiguation is an essential preprocessing step of many natural language processing systems, it is a topic rarely addressed in the literature and there are few public-domain programs for performing the segmentation task.", "labels": [], "entities": [{"text": "sentence boundary disambiguation", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.6730049649874369}, {"text": "segmentation task", "start_pos": 231, "end_pos": 248, "type": "TASK", "confidence": 0.8986443877220154}]}, {"text": "For our studies we compared our system against the results of the UNIX STYLE program (Cherry and Vesterman 1991).", "labels": [], "entities": []}, {"text": "3 The STYLE program, which attempts to provide a stylistic profile of writing at the word and sentence level, reports the length and structure for all sentences in a document, thereby indicating the sentence boundaries.", "labels": [], "entities": []}, {"text": "STYLE defines a sentence as a string of words ending in one of: period, exclamation point, question mark, or backslash-period (the latter of which can be used by an author to mark an imperative sentence ending).", "labels": [], "entities": [{"text": "STYLE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9201271533966064}]}, {"text": "The program handles numbers with embedded decimal points and commas and makes use of an abbreviation list with 48 entries.", "labels": [], "entities": []}, {"text": "It also uses the following heuristic: initials cause a sentence break only if the next word begins with a capital letter and is found in a dictionary of function words.", "labels": [], "entities": []}, {"text": "In an evaluation on a sample of 20 documents, the developers of the program found it to incorrectly classify sentence boundaries 204 times out of 3287 possible (an error rate of 6.3%).", "labels": [], "entities": []}, {"text": "We first tested the Satz system using English texts from the Wall Street Journal portion of the ACL/DCI collection (Church and Liberman 1991).", "labels": [], "entities": [{"text": "Wall Street Journal portion of the ACL/DCI collection", "start_pos": 61, "end_pos": 114, "type": "DATASET", "confidence": 0.9576337337493896}]}, {"text": "We constructed a training text of 573 test cases and a cross-validation text of 258 test cases.", "labels": [], "entities": []}, {"text": "15 We then constructed a separate test text consisting of 27,294 test cases, with a lower bound of 75.0%.", "labels": [], "entities": []}, {"text": "The baseline system (UNIX STYLE) achieved an error rate of 8.3% on the sentence boundaries in the test set.", "labels": [], "entities": [{"text": "error rate", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9887215197086334}]}, {"text": "The lexicon and thus the frequency counts used to calculate the descriptor arrays were derived from the Brown corpus (.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.8713364899158478}]}, {"text": "In initial experiments we used the extensive lexicon from the PARTS part-of-speech tagger, which contains 30,000 words.", "labels": [], "entities": [{"text": "PARTS part-of-speech tagger", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.599716325600942}]}, {"text": "We later experimented with a much smaller lexicon, and these results are discussed in Section 4.4.", "labels": [], "entities": []}, {"text": "In Sections 4.1-4.9 we describe the results of our experiments with the Satz system using the neural network as the learning algorithm.", "labels": [], "entities": []}, {"text": "Section 4.10 describes results using decision tree induction.", "labels": [], "entities": [{"text": "decision tree induction", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.7905330856641134}]}], "tableCaptions": [{"text": " Table 1  Results of comparing context sizes.", "labels": [], "entities": []}, {"text": " Table 2  Results of comparing hidden layer sizes (6-context).", "labels": [], "entities": []}, {"text": " Table 4  Results of comparing lexicon size (27,294 potential sentence  boundaries).", "labels": [], "entities": []}, {"text": " Table 5  Results of comparing probabilistic to binary feature inputs (5,000 word lexicon, 27,294 English  test cases).", "labels": [], "entities": []}, {"text": " Table 6  Results of varying the sensitivity thresholds (27,294 test cases, 6-context, 2 hidden units).", "labels": [], "entities": []}, {"text": " Table 10  Summary of best results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9387955069541931}]}]}