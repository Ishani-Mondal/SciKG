{"title": [{"text": "Automatic Rule Induction for Unknown-Word Guessing", "labels": [], "entities": [{"text": "Automatic Rule Induction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5097423394521078}, {"text": "Unknown-Word Guessing", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7071765214204788}]}], "abstractContent": [{"text": "Words unknown to the lexicon present a substantial problem to NLP modules that rely on mor-phosyntactic information, such as part-of-speech taggers or syntactic parsers.", "labels": [], "entities": []}, {"text": "In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments.", "labels": [], "entities": []}, {"text": "The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus.", "labels": [], "entities": []}, {"text": "Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules.", "labels": [], "entities": []}, {"text": "Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Words unknown to the lexicon present a substantial problem to NLP modules (as, for instance, part-of-speech (pos-) taggers) that rely on information about words, such as their part of speech, number, gender, or case.", "labels": [], "entities": []}, {"text": "Taggers assign a single POS-tag to a word-token, provided that it is known what Pos-tags this word can take on in general and the context in which this word was used.", "labels": [], "entities": []}, {"text": "A Pos-tag stands fora unique set of morpho-syntactic features, as exemplified in, and a word can take several Pos-tags, which constitute an ambiguity class or POS-class for this word.", "labels": [], "entities": []}, {"text": "Words with their POs-classes are usually kept in a lexicon.", "labels": [], "entities": [{"text": "POs-classes", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.9051803946495056}]}, {"text": "For every input word-token, the tagger accesses the lexicon, determines possible POS-tags this word can take on, and then chooses the most appropriate one.", "labels": [], "entities": []}, {"text": "However, some domain-specific words or infrequently used morphological variants of general-purpose words can be missing from the lexicon and thus, their POs-classes should be guessed by the system and only then sent to the disambiguation module.", "labels": [], "entities": []}, {"text": "The simplest approach to POS-class guessing is either to assign all possible tags to an unknown word or to assign the most probable one, which is proper singular noun for capitalized words and common singular noun otherwise.", "labels": [], "entities": [{"text": "POS-class guessing", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9586713910102844}]}, {"text": "The appealing feature of these approaches is their extreme simplicity.", "labels": [], "entities": []}, {"text": "Not surprisingly, their performance is quite poor: if a word is assigned all possible tags, the search space for the disambiguation of a single POS-tag increases and makes it fragile; if every unknown word is classified as a noun, there will be no difficulties for disambiguation but accuracy will suffer--such a guess is not reliable enough.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 284, "end_pos": 292, "type": "METRIC", "confidence": 0.9991838335990906}]}, {"text": "To assign capitalized unknown words the category proper noun seems a good heuristic, but may not always work.", "labels": [], "entities": []}, {"text": "As argued in, who proposes a more elaborated heuristic, proposed a simple probabilistic approach to unknown-word guessing: The most frequent open-class tags from the Penn tag set.", "labels": [], "entities": [{"text": "unknown-word guessing", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.6969105750322342}, {"text": "Penn tag set", "start_pos": 166, "end_pos": 178, "type": "DATASET", "confidence": 0.9677592913309733}]}, {"text": "the probability that an unknown word has a particular Pos-tag is estimated from the probability distribution of hapax words (words that occur only once) in the previously seen texts.", "labels": [], "entities": []}, {"text": "1 Whereas such a guesser is more accurate than the naive assignments and easily trainable, the tagging performance on unknown words is reported to be only about 66% correct for English.", "labels": [], "entities": []}, {"text": "2 More advanced word-guessing methods use word features such as leading and trailing word segments to determine possible tags for unknown words.", "labels": [], "entities": []}, {"text": "Such methods can achieve better performance, reaching tagging accuracy of up to 85% on unknown words for English).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.968451201915741}]}, {"text": "The Xerox tagger) comes with a set of rules that assign an unknown word a set of possible pos-tags (i.e., POS-class) on the basis of its ending segment.", "labels": [], "entities": []}, {"text": "We call such rules endingguessing rules because they rely only on ending segments in their predictions.", "labels": [], "entities": [{"text": "endingguessing", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.9565688967704773}]}, {"text": "For example, an ending-guessing rule can predict that a word is a gerund or an adjective if it ends with ing.", "labels": [], "entities": []}, {"text": "The ending-guessing approach was elaborated in, where an unknown word was guessed by using the probability for an unknown word to be of a particular Pos-tag, given its capitalization feature and its ending.) describes a system of rules that uses both ending-guessing and more morphologically motivated rules.", "labels": [], "entities": []}, {"text": "A morphological rule, unlike an ending-guessing rule, uses information about morphologically related words already known to the lexicon in its prediction.", "labels": [], "entities": []}, {"text": "For instance, a morphologically motivated guessing rule can say that a word is an adjective if adding the suffix ly to it will result in a word.", "labels": [], "entities": []}, {"text": "Clearly, ending-guessing rules have wider coverage than morphologically oriented ones, but their predictions can be less accurate.", "labels": [], "entities": []}, {"text": "The major topic in the development of word-Pos guessers is the strategy used for the acquisition of the guessing rules.", "labels": [], "entities": [{"text": "word-Pos guessers", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6705977618694305}]}, {"text": "A rule-based tagger described in was equipped with a set of guessing rules that had been hand-crafted using knowledge of English morphology and intuitions.", "labels": [], "entities": []}, {"text": "A more appealing approach is automatic acquisition of such rules from available lexical resources, since it is usually less labor-intensive and less error-prone.", "labels": [], "entities": []}, {"text": "developed a system for automated learning of morphological word formation rules.", "labels": [], "entities": [{"text": "automated learning of morphological word formation rules", "start_pos": 23, "end_pos": 79, "type": "TASK", "confidence": 0.703380537884576}]}, {"text": "This system divides a string into three regions and infers from training examples their correspondence to underlying morphological features.", "labels": [], "entities": []}, {"text": "describes a guessing component that uses a prespecified list of suffixes (or rather endings) and then statistically learns the 1 A similar idea for estimating lexical prior probabilities for unknown words was suggested in.", "labels": [], "entities": []}, {"text": "The best result was detected for GermanM2% accuracy and the worst result for Italian--50% accuracy.", "labels": [], "entities": [{"text": "GermanM2", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.8280446529388428}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9777155518531799}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9775356650352478}]}, {"text": "predictive properties of those endings from an untagged corpus.", "labels": [], "entities": []}, {"text": "Ina transformation-based learner that learns guessing rules from a pretagged training corpus is outlined: First the unknown words are labeled as common nouns and a list of generic transformations is defined.", "labels": [], "entities": []}, {"text": "Then the learner tries to instantiate the generic transformations with word features observed in the text.", "labels": [], "entities": []}, {"text": "A statistical-based suffix learner is presented in.", "labels": [], "entities": []}, {"text": "From a training corpus, it constructs a suffix tree where every suffix is associated with its information measure to emit a particular pos-tag.", "labels": [], "entities": []}, {"text": "Although the learning process in these systems is fully automated and the accuracy of obtained guessing rules reaches current state-of-the-art levels, for estimation of their parameters they require significant amounts of specially prepared training data--a large training corpus (usually pretagged), training examples, and soon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9975945353507996}]}, {"text": "In this paper, we describe a novel, fully automatic technique for the induction of Pos-class-guessing rules for unknown words.", "labels": [], "entities": []}, {"text": "This technique has been partially outlined in) and, along with a level of accuracy for the induced rules that is higher than any previously quoted, it has an advantage in terms of quantity and simplicity of annotation of data for training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.999529242515564}, {"text": "quantity", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9778919219970703}]}, {"text": "Unlike many other approaches, which implicitly or explicitly assume that the surface manifestations of morpho-syntactic features of unknown words are different from those of general language, we argue that within the same language unknown words obey general morphological regularities.", "labels": [], "entities": []}, {"text": "In our approach, we do not require large amounts of annotated text but employ fully automatic statistical learning using a pre-existing general-purpose lexicon mapped to a particular tag set and word-frequency distribution collected from a raw corpus.", "labels": [], "entities": []}, {"text": "The proposed technique is targeted to the acquisition of both morphological and ending-guessing rules, which then can be applied cascadingly using the most accurate guessing rules first.", "labels": [], "entities": []}, {"text": "The rule induction process is guided by a thorough guessing-rule evaluation methodology that employs precision, recall, and coverage as evaluation metrics.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9103612303733826}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.998852014541626}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9964559674263}, {"text": "coverage", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9905989766120911}]}, {"text": "In the rest of the paper we first introduce the kinds of guessing rules to be induced and then present a semi-unsupervised 3 statistical rule induction technique using data derived from the CELEX lexical database).", "labels": [], "entities": [{"text": "CELEX lexical database", "start_pos": 190, "end_pos": 212, "type": "DATASET", "confidence": 0.9719688892364502}]}, {"text": "Finally we evaluate the induced guessing rules by removing all the hapax words from the lexicon and tagging the Brown Corpus ( by a stochastic tagger and a rule-based tagger.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9398181438446045}]}], "datasetContent": [{"text": "There are two important questions that arise at the rule acquisition stage: how to choose the scoring threshold Os and what the performance of the rule sets produced with different thresholds is.", "labels": [], "entities": [{"text": "rule acquisition", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.9131174385547638}, {"text": "scoring threshold Os", "start_pos": 94, "end_pos": 114, "type": "METRIC", "confidence": 0.5445754726727804}]}, {"text": "The task of assigning a set of POS-tags to a word is actually quite similar to the task of document categorization where a document is assigned a set of descriptors that represent its contents.", "labels": [], "entities": []}, {"text": "There area number of standard parameters) used for measuring performance on this kind of task.", "labels": [], "entities": []}, {"text": "For example, suppose that a word can take on one or more POS-tags from the set of open-class POS-tags: qJ NN NNS RB VB VBD VBG VBN VBZ).", "labels": [], "entities": [{"text": "qJ NN NNS RB VB VBD VBG VBN VBZ", "start_pos": 103, "end_pos": 134, "type": "DATASET", "confidence": 0.5134946472114987}]}, {"text": "To see how well the guesser performs, we can compare the results of the guessing with the Pos-tags known to be true for the Word (i.e., listed in the lexicon).", "labels": [], "entities": []}, {"text": "Let us take, for instance, a lexicon entry [developed (JJ VBD VBN)].", "labels": [], "entities": [{"text": "JJ VBD VBN", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9102583924929301}]}, {"text": "Suppose that the guesser categorized it as [developed (JJ NN RB VBD VBZ)].", "labels": [], "entities": [{"text": "JJ NN RB VBD VBZ", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.808296287059784}]}, {"text": "We can represent this situation as in.", "labels": [], "entities": []}, {"text": "The performance of the guesser can be measured in: \u2022 recall -the percentage of POS-tags correctly assigned by the guesser, i.e., two (jJ VBD) out of three (JJ VBD VBN) or 66%.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9946314096450806}]}, {"text": "100% recall would mean that the guesser had assigned all the correct pos-tags but not necessarily only the correct ones.", "labels": [], "entities": [{"text": "recall", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9995322227478027}]}, {"text": "So, for example, if the guesser had assigned all possible POS-tags to the word its recall would have been 100%.", "labels": [], "entities": [{"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9995442032814026}]}, {"text": "\u2022 precision -the percentage of POS-tags the guesser assigned correctly (JJ VBD) over the total number of POS-tags it assigned to the word (Jl NN RB VBD VBZ), i.e., 2/5 or 40%.", "labels": [], "entities": [{"text": "precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9995310306549072}, {"text": "JJ VBD)", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.6219783922036489}, {"text": "Jl NN RB VBD VBZ)", "start_pos": 139, "end_pos": 156, "type": "METRIC", "confidence": 0.7585360109806061}]}, {"text": "100% precision would mean that the guesser did not assign incorrect POS-tags, although not necessarily all the correct ones were assigned.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9995601773262024}]}, {"text": "So, if the guesser had assigned only (JJ) its precision would have been 100%.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9997559189796448}]}, {"text": "\u2022 coverage -the proportion of words guesser was able to classify, but not necessarily correctly.", "labels": [], "entities": [{"text": "coverage", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9975423812866211}]}, {"text": "So, for example, if we had evaluated a guesser with The interpretation of these percentages is by no means straightforward, as there is no straightforward way of combining these different measures into a single one.", "labels": [], "entities": []}, {"text": "For example, these measures assume that all combinations of POS-tags will be equally hard to disambiguate for the tagger, which is not necessarily the case.", "labels": [], "entities": []}, {"text": "Obviously, the most important measure is recall since we want all possible categories fora word to be guessed.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9991362690925598}]}, {"text": "Precision seems to be slightly less important since the disambiguator should be able to handle additional noise but obviously not in large amounts.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9784985184669495}]}, {"text": "Coverage is a very important measure fora rule set, since a rule set that can guess very accurately but only fora tiny proportion of words is of questionable value.", "labels": [], "entities": []}, {"text": "Thus, we will try to maximize recall first, then coverage, and, finally, precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9836434721946716}, {"text": "coverage", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9981591105461121}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9990346431732178}]}, {"text": "We will measure the aggregate by averaging over measures per word (micro-average), i.e., for every single word from the test collection the precision and recall of the guesses are calculated, and then we average over these values.", "labels": [], "entities": [{"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9989475607872009}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9969070553779602}]}, {"text": "To find the optimal threshold (0s) for the production of a guessing rule set, we generated a number of similar rule sets using different thresholds and evaluated them against the training lexicon and the test lexicon of unseen 17,868 hapax words.", "labels": [], "entities": []}, {"text": "Every word from the two lexicons was guessed by a rule set and the results were compared with the information the word had in the lexicon.", "labels": [], "entities": []}, {"text": "For every application of a rule set to a word, we computed the precision and recall, and then using the total number of guessed words we computed the coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9996973276138306}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9993577599525452}, {"text": "coverage", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9827955365180969}]}, {"text": "We noticed certain regularities in the behavior of the metrics in response to the change of the threshold: recall improves as the threshold increases while coverage drops proportionally.", "labels": [], "entities": [{"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9994526505470276}, {"text": "coverage", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.978781521320343}]}, {"text": "This is not surprising: the higher the threshold, the fewer the inaccurate rules included in the rule set, but at the same time the fewer the words that can be handled.", "labels": [], "entities": []}, {"text": "An interesting behavior is shown by precision: first, it grows proportionally along with the increase of the threshold, but then, at high thresholds, it decreases.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9994639754295349}]}, {"text": "This means that among very confident rules with very high scores, there are many quite general ones.", "labels": [], "entities": []}, {"text": "The best thresholds were obtained in the range of 70-80 points.", "labels": [], "entities": []}, {"text": "displays the metrics for the best-scored (by aggregate of the three metrics on the training and the test samples) rule sets.", "labels": [], "entities": []}, {"text": "As the baseline standard, we took the ending-guessing rule set supplied with the Xerox tagger ().", "labels": [], "entities": [{"text": "Xerox tagger", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.875736266374588}]}, {"text": "When we compared the Xerox ending guesser with the induced ending-guessing rule set (Ending*), we saw that its precision was about 6% poorer and, most importantly, it could handle 6% fewer unknown words.", "labels": [], "entities": [{"text": "Xerox ending guesser", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.665198028087616}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9994537234306335}]}, {"text": "Finally, we measured the performance of the cascading application of the induced rule sets when the morphological guessing rules were applied before the ending-guessing rules (Prefix+Suffix\u00b0+Suffix 1 +Ending -c*).", "labels": [], "entities": [{"text": "Prefix+Suffix\u00b0+Suffix 1 +Ending -c*)", "start_pos": 176, "end_pos": 212, "type": "METRIC", "confidence": 0.7887840081344951}]}, {"text": "We detected that the cascading application of the morphological rule sets together with the ending-guessing rules increases the overall precision of the guessing by about 8%.", "labels": [], "entities": [{"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9994233846664429}]}, {"text": "This made the improvement over the baseline Xerox guesser 13% in precision and 7% in coverage on the test sample.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9996508359909058}, {"text": "coverage", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9991220831871033}]}, {"text": "We evaluated the taggers with the guessing components on all fifteen subcorpora of the Brown Corpus, one after another.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9450657069683075}]}, {"text": "The HMM tagger was trained on the Brown Corpus in such away that the subcorpus used for the evaluation was not seen at the training phase.", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.6428878307342529}, {"text": "Brown Corpus", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9787352681159973}]}, {"text": "All the hapax words and capitalized words with frequency less than 20 were not seen at the training of the cascading guesser.", "labels": [], "entities": []}, {"text": "These words were not used in the training of the tagger either.", "labels": [], "entities": []}, {"text": "This means that neither the HMM tagger nor the cascading guesser had been trained on the texts and words used for evaluation.", "labels": [], "entities": []}, {"text": "We do not know whether the same holds for the Brill tagger and the Brill and Xerox guessers since we took them pretrained.", "labels": [], "entities": [{"text": "Brill tagger", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.7670063078403473}, {"text": "Brill and Xerox guessers", "start_pos": 67, "end_pos": 91, "type": "DATASET", "confidence": 0.6870167702436447}]}, {"text": "For words that the guessing components failed to guess, we applied the standard method of classifying them as common nouns (NN) if they were not capitalized inside a sentence and proper nouns (NNP) otherwise.", "labels": [], "entities": []}, {"text": "When we used the cascading guesser with the Brill tagger we interfaced them on the level of the lexicon: we guessed the unknown words before the tagging and added them to the lexicon listing the most likely tags first as required.", "labels": [], "entities": [{"text": "Brill tagger", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9427475333213806}]}, {"text": "7 Here we want to clarify that we evaluated the overall results of the Brill tagger rather than just its unknown-word tagging component.", "labels": [], "entities": [{"text": "Brill tagger", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.7975351214408875}]}, {"text": "Another point to mention is that, since we included the guessed words in the lexicon, the Brill tagger could use for the transformations all relevant Postags for unknown words.", "labels": [], "entities": []}, {"text": "This is quite different from the output of the original Brill's guesser, which provides only one Pos-tag for an unknown word.", "labels": [], "entities": [{"text": "Brill's guesser", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.8922470013300577}, {"text": "Pos-tag", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9486028552055359}]}, {"text": "In our tagging experiments, we measured the error rate of tagging on unknown 7 We estimated the most likely tags from the training data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.980683296918869}]}, {"text": "Since, arguably, the guessing of proper nouns is easier than is the guessing of other categories, we also measured the error rate for the subcategory of capitalized unknown words separately.", "labels": [], "entities": [{"text": "guessing of proper nouns", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8726170510053635}, {"text": "error rate", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9808276891708374}]}, {"text": "The error rate fora category of words was calculated as follows:  In our experiments the category of unknown proper nouns had a larger share (63-64%) than we expect in real life because all the capitalized words with frequency less than 20 were taken out of the lexicon.", "labels": [], "entities": [{"text": "error rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9565307796001434}]}, {"text": "The cascading guesser also helped to improve the accuracy on unknown proper nouns by about 1% in comparison to Brill's guesser and about 3% in comparison to Xerox's guesser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9989474415779114}]}, {"text": "The cascading guesser outperformed the other two guessers on every subcorpus of the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.911126434803009}]}, {"text": "shows the distribution of the workload and the tagging accuracy among the different rule sets of the cascading guesser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9808551669120789}]}, {"text": "The default assignment of the NN tag to unguessed words performed very poorly, having the error rate of 44%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9932326376438141}]}, {"text": "When we compared this distribution to that of the Xerox guesser we saw that the accuracy of the Xerox guesser itself was only about 6.5% lower than that of the cascading guesser 9 and the fact that it could handle 6% fewer unknown words than the cascading guesser resulted in the increase of incorrect assignments by the default strategy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9994418025016785}]}, {"text": "There were three types of mistaggings on unknown words detected in our experiments.", "labels": [], "entities": []}, {"text": "Mistagging of the first type occurred when a guesser provided a broader POS-class for an unknown word than a lexicon would, and the tagger had difficulties with its disambiguation.", "labels": [], "entities": [{"text": "Mistagging", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9479104280471802}]}, {"text": "This was especially the case with the words that were guessed as noun/adjective (NN JJ) but, in fact, act only as one of them (as do, for example, many hyphenated words).", "labels": [], "entities": []}, {"text": "Another highly ambiguous group is the ing words, which, in general, can act as nouns, adjectives, and gerunds and only direct lexicalization can restrict the search-space, as in the case of the word seeing, which cannot act as an adjective.", "labels": [], "entities": []}, {"text": "The second type of mistagging was caused by incorrect assignments by the guesser.", "labels": [], "entities": [{"text": "mistagging", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.958871066570282}]}, {"text": "Usually this was the case with irregular words such as cattle or data, which were wrongly guessed to be singular nouns (NN) but in fact were plural nouns (NN8).", "labels": [], "entities": []}, {"text": "We also did not include the \"foreign word\" category (FW) in the set of tags to guess, but this did not do too much harm because these words were very infrequent in the texts.", "labels": [], "entities": []}, {"text": "And the third type of mistagging occurred when the word-POS guesser assigned the correct Pos-class to a word but the tagger still disambiguated this class incorrectly.", "labels": [], "entities": [{"text": "mistagging", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9634665250778198}]}, {"text": "This was the most frequent type of error, which accounted for more than 60% of the mistaggings on unknown words.", "labels": [], "entities": [{"text": "error", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9489421844482422}]}], "tableCaptions": [{"text": " Table 4  Results of tagging the unknown words in the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9369579255580902}]}]}