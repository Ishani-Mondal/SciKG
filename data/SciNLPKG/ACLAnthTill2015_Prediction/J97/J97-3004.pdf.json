{"title": [{"text": "An Efficient Implementation of the Head-Corner Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "Gertjan van Noord\" Rijksuniversiteit Groningen This paper describes an efficient and robust implementation of a bidirectional, head-driven parser for constraint-based grammars.", "labels": [], "entities": []}, {"text": "This parser is developed for the OVIS system: a Dutch spoken dialogue system in which information about public transport can be obtained by telephone.", "labels": [], "entities": []}, {"text": "After a review of the motivation for head-driven parsing strategies, and head-corner parsing in particular, a nondeterministic version of the head-corner parser is presented.", "labels": [], "entities": [{"text": "head-driven parsing", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.5630961954593658}, {"text": "head-corner parsing", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7277662456035614}]}, {"text": "A memorization technique is applied to obtain a fast parser.", "labels": [], "entities": []}, {"text": "A goal-weakening technique is introduced, which greatly improves average case efficiency, both in terms of speed and space requirements.", "labels": [], "entities": []}, {"text": "I argue in favor of such a memorization strategy with goal-weakening in comparison with ordinary chart parsers because such a strategy can be applied selectively and therefore enormously reduces the space requirements of the parser, while no practical loss in time-efficiency is observed.", "labels": [], "entities": []}, {"text": "On the contrary, experiments are described in which head-corner and left-corner parsers implemented with selective memorization and goal weakening outperform \"standard\" chart parsers.", "labels": [], "entities": []}, {"text": "The experiments include the grammar of the OV/S system and the Alvey NL Tools grammar.", "labels": [], "entities": [{"text": "Alvey NL Tools grammar", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.8287590742111206}]}, {"text": "Head-corner parsing is a mix of bottom-up and top-down processing.", "labels": [], "entities": [{"text": "Head-corner parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.767328292131424}]}, {"text": "Certain approaches to robust parsing require purely bottom-up processing.", "labels": [], "entities": [{"text": "robust parsing", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.5704484283924103}]}, {"text": "Therefore, it seems that head-corner parsing is unsuitable for such robust parsing techniques.", "labels": [], "entities": [{"text": "head-corner parsing", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7850437462329865}]}, {"text": "However, it is shown how underspecification (which arises very naturally in a logic programming environment) can be used in the head-corner parser to allow such robust parsing techniques.", "labels": [], "entities": []}, {"text": "A particular robust parsing model, implemented in OVIS, is described.", "labels": [], "entities": [{"text": "OVIS", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.8662274479866028}]}], "introductionContent": [], "datasetContent": [{"text": "The OVIS grammar (for Dutch) contains about 1,400 lexical entries (many of which are station and city names) and 66 rules (a substantial fraction of which are concerned with time and date expressions), including 7 epsilon rules.", "labels": [], "entities": [{"text": "OVIS grammar", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8406893014907837}]}, {"text": "The most important epsilon rule is part of a gap-threading implementation of verb-second.", "labels": [], "entities": []}, {"text": "The grammar is documented in detail in van Noord, The input for the parser consists of a test set of 5,000 word-graphs, randomly taken from a corpus of more than 25,000 word-graphs.", "labels": [], "entities": []}, {"text": "These word-graphs are the latest word-graphs that were available to us; they are \"real\" output of the current version of the speech recognizer as developed by our project partners.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7059217840433121}]}, {"text": "In this application, typical 7 Experiments suggest that the load on the machine in fact does influence the timings somewhat.", "labels": [], "entities": [{"text": "timings", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9895243048667908}]}, {"text": "However, the experiments were performed at times when the load of the machine was low.", "labels": [], "entities": []}, {"text": "It is believed, therefore, that no such artifacts are present in the numbers given here.", "labels": [], "entities": []}, {"text": "As a consequence, the typical size of word-graphs is rather small too, as can be seen in.", "labels": [], "entities": []}, {"text": "We report on three different experiments with the OVIS grammar and these wordgraphs.", "labels": [], "entities": [{"text": "OVIS grammar", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8476475775241852}]}, {"text": "In the first experiment, the system runs in best-l-mode: the best path is selected from the word-graph using bigram scores and the acoustic scores (present in the wordgraph).", "labels": [], "entities": []}, {"text": "This best path is then sent to the parser and robustness component.", "labels": [], "entities": []}, {"text": "In the second experiment, the parser is given the utterance as it was actually spoken (to simulate a situation in which speech recognition is perfect).", "labels": [], "entities": []}, {"text": "In the third experiment, the parser takes the full word-graph as its input.", "labels": [], "entities": []}, {"text": "The results are then passed onto the robustness component.", "labels": [], "entities": []}, {"text": "As explained in the previous section on robustness, each of the parsers finds all derivations of the start symbol anywhere in the input (this is the casein each of the OVIS experiments).", "labels": [], "entities": []}, {"text": "For the current version of the OVIS system, parsing on the basis of the best path in the word-graph gives results in terms of word accuracy that are similar to the results obtained with full word-graphs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.8516274690628052}]}, {"text": "Results for concept accuracy are not yet available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9325847029685974}]}, {"text": "Details can be found in van Noord, Bouma, Koeling, and Nederhof (1996)., the CPU-time requirements and the maximum space requirements of the different parsers are listed.", "labels": [], "entities": []}, {"text": "In the table we list, respectively, the total number of milliseconds CPU-time required for all 5,000 word-graphs (timings include lexical lookup, parsing, and the robustness component), the average number of milliseconds per word-graph, and the maximum number of milliseconds fora word-graph.", "labels": [], "entities": []}, {"text": "The final column lists the maximum amount of space requirements (per word-graph, in Kbytes).", "labels": [], "entities": []}, {"text": "8 Total and average CPU-time and maximal space requirements fora test set of 5,000 best paths through word-graphs (OVIS grammar).", "labels": [], "entities": []}, {"text": "Another experiment was carried out for the English grammar of the MiMo2 system.", "labels": [], "entities": [{"text": "MiMo2 system", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.8729313910007477}]}, {"text": "This grammar is a unification-based grammar that is compiled into a DCG.", "labels": [], "entities": []}, {"text": "The grammar contains 525 lexical entries, 63 rules including 13 gaps.", "labels": [], "entities": []}, {"text": "The head-corner relation contains 33 pairs and the lexical head-corner relation contains 18 pairs.", "labels": [], "entities": []}, {"text": "The left-corner parser runs into hidden left-recursion problems on the original grammar, so it uses aversion of the grammar in which left-most gaps are compiled out.", "labels": [], "entities": []}, {"text": "This compiled grammar has 69 rules.", "labels": [], "entities": []}, {"text": "The left-corner relation contains 80 pairs; the lexical left-corner relation contains 62 pairs.", "labels": [], "entities": []}, {"text": "As a result, the left-corner parser only hypothesizes gaps for non-left-most daughters.", "labels": [], "entities": []}, {"text": "Because the grammar never allows gaps as head, the head-corner parser can be optimized in a similar fashion.", "labels": [], "entities": []}, {"text": "Both the left-corner and head-corner parser use a goal-weakening operator that only leaves the functor symbol.", "labels": [], "entities": [{"text": "head-corner parser", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.6007574051618576}]}, {"text": "This simplifies the way in which the goal table is maintained.", "labels": [], "entities": []}, {"text": "For this experiment we have no notion of typical input, but instead made up a set of 25 sentences of various lengths and levels of difficulty, with a total of 338 readings.", "labels": [], "entities": []}, {"text": "In order to be able to complete the experiment, a time-out of 60 seconds of CPU-time was used.", "labels": [], "entities": []}, {"text": "Timings include lexical lookup and parse tree recovery.", "labels": [], "entities": [{"text": "parse tree recovery", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8067516883214315}]}, {"text": "The original parser implemented in the MiMo2 system (a left-corner parser without packing) took 294 seconds of CPU-time to complete the experiment (with three time-outs).", "labels": [], "entities": []}, {"text": "Because the test environment was (only slightly) different, we have indicated the latter results in italics.", "labels": [], "entities": []}, {"text": "Average CPU-time is only given for those parsers that completed each of the sentences within the time limit.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "The bottom-up active chart parser performs better on smaller sentences with a small number of readings.", "labels": [], "entities": [{"text": "bottom-up active chart parser", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6746494323015213}]}, {"text": "For longer and more ambiguous sentences, the head-corner parser is (much) more efficient.", "labels": [], "entities": [{"text": "head-corner parser", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.6219111979007721}]}, {"text": "The other parsers are consistently much less efficient.", "labels": [], "entities": []}, {"text": "A final set of experiments was performed for the Alvey NL Tools grammar, similar to the experiments discussed in.", "labels": [], "entities": [{"text": "Alvey NL Tools grammar", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.8649169057607651}]}, {"text": "For a longer description of the grammar and the test sets we refer the reader to this publication.", "labels": [], "entities": []}, {"text": "The grammar contains 2,363 lexical entries, and 780 rules (8 of which are gaps).", "labels": [], "entities": []}, {"text": "The left-corner relation contains 440 pairs; the lexical left-corner relation Total and average CPU-time and maximum space requirements fora set of 25 sentences (MiMo2 grammar).", "labels": [], "entities": []}, {"text": "Italicized items are offered for cautious comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  The left-most table gives information concerning the  number of transitions per word-graph of the test set for  the OVIS grammar. As can be seen from this table, more  than half of the corpus consists of word-graphs with at  most five transitions. In the right-most table, the number  of words per utterance is given. Many utterances consists  of less than five words.", "labels": [], "entities": [{"text": "OVIS grammar", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.7538483440876007}]}, {"text": " Table 1.  We report on three different experiments with the OVIS grammar and these word- graphs. In the first experiment, the system runs in best-l-mode: the best path is selected  from the word-graph using bigram scores and the acoustic scores (present in the word- graph). This best path is then sent to the parser and robustness component. In the  second experiment, the parser is given the utterance as it was actually spoken (to  simulate a situation in which speech recognition is perfect). In the third experiment,  the parser takes the full word-graph as its input. The results are then passed on to  the robustness component. As explained in the previous section on robustness, each  of the parsers finds all derivations of the start symbol anywhere in the input (this is  the case in each of the OVIS experiments).", "labels": [], "entities": [{"text": "OVIS grammar", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.8651376366615295}, {"text": "speech recognition", "start_pos": 466, "end_pos": 484, "type": "TASK", "confidence": 0.7484584450721741}]}, {"text": " Table 2  Total and average CPU-time and maximal space requirements for a test set of 5,000 best  paths through word-graphs (OVIS grammar).", "labels": [], "entities": []}, {"text": " Table 3  Total and average CPU-time and maximum space requirements for a test set of 5,000  utterances (OVIS grammar).", "labels": [], "entities": []}, {"text": " Table 4  Total and average CPU-time and maximum space requirements for a test set of 5,000  word-graphs (OVIS grammar).", "labels": [], "entities": []}, {"text": " Table 5  Percentage of word-graphs that can be treated within time limit (OVIS grammar).", "labels": [], "entities": []}]}