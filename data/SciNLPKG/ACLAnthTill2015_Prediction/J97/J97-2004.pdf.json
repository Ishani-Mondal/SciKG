{"title": [{"text": "A Class-based Approach to Word Alignment", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7563502490520477}]}], "abstractContent": [{"text": "This paper presents an algorithm capable of identifying the translation for each word in a bilingual corpus.", "labels": [], "entities": [{"text": "identifying the translation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7137448489665985}]}, {"text": "Previously proposed methods rely heavily on word-based statistics.", "labels": [], "entities": []}, {"text": "Under a word-based approach, frequent words with a consistent translation can be aligned at a high rate of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9956011772155762}]}, {"text": "However, words that are less frequent or exhibit diverse translations generally do not have statistically significant evidence for confident alignment, thereby leading to incomplete or incorrect alignments.", "labels": [], "entities": []}, {"text": "The algorithm proposed herein attempts to broaden coverage by exploiting lexico-graphic resources.", "labels": [], "entities": []}, {"text": "To this end, we draw on the two classification systems of words in Longman Lexicon of Contemporary English (LLOCE) and Tongyici Cilin (Synonym Forest, CILIN).", "labels": [], "entities": [{"text": "Longman Lexicon of Contemporary English (LLOCE)", "start_pos": 67, "end_pos": 114, "type": "DATASET", "confidence": 0.9481264054775238}]}, {"text": "Automatically acquired class-based alignment rules are used to compensate for what is lacking in a bilingual dictionary such as the English-Chinese version of the Longman Dictionary of Contemporary English (LecDOCE).", "labels": [], "entities": [{"text": "Longman Dictionary of Contemporary English (LecDOCE)", "start_pos": 163, "end_pos": 215, "type": "DATASET", "confidence": 0.9006990194320679}]}, {"text": "In addition, this alignment method is implemented using LecDOCE examples and their translations for training and testing, while further examples from a technical manual in both English and Chinese are used for an open test.", "labels": [], "entities": []}, {"text": "Quantitative results of the closed and open tests are also summarized.", "labels": [], "entities": []}], "introductionContent": [{"text": "Brown, Cocke, Della Pietra, Della Pietra, Jelinek, Laffert~ Mercer, and advocate a statistical approach to machine translation (MT) and initiate much of the recent interest in bilingual corpora.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.8341259598731995}]}, {"text": "Statistical machine translation (SMT) can be understood as a word-by-word model consisting of two submodels: a language model for generating a source text segment Sand a translation model for mapping S to its translation T.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8232347071170807}]}, {"text": "They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S I T) in the translation model.", "labels": [], "entities": []}, {"text": "For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates, and for other language pairs, including English-German, EnglishChinese, and English-Japanese ().", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9896238446235657}, {"text": "sentence alignment", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7079551517963409}, {"text": "Hansards", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.7296488285064697}]}, {"text": "Alignment at other levels of resolution is obviously useful.", "labels": [], "entities": []}, {"text": "A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation.", "labels": [], "entities": []}, {"text": "Other logical approaches involve aligning parse trees of a sentence and its translation, or simultaneously generating parse trees and alignment arrangements).", "labels": [], "entities": []}, {"text": "In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation, translation assessment and critiquing tools), text generation, bilingual lexicography (, and word-sense disambiguation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7663505375385284}, {"text": "machine-aided translation", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.718687579035759}, {"text": "translation assessment", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.9440008997917175}, {"text": "text generation", "start_pos": 178, "end_pos": 193, "type": "TASK", "confidence": 0.8089125156402588}, {"text": "word-sense disambiguation", "start_pos": 225, "end_pos": 250, "type": "TASK", "confidence": 0.7527450025081635}]}, {"text": "For these applications, we must go one step further from sentence alignment and identify alignment at the word level.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7323500216007233}]}, {"text": "In the process of word alignment, the translation of each source word is identified.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7647197246551514}]}, {"text": "This study concentrates primarily on identifying alignment at the word level fora given sentence and its translation.", "labels": [], "entities": []}, {"text": "In the context of SMT, present a series of five models of Pr(S I T) for word alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.99399334192276}, {"text": "word alignment", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7771622240543365}]}, {"text": "Model 1 assumes that Pr(S ] T) depends only on lexical translation probability (LTP) t(s I t), that is, the probability that the ith word sin S translates into the jth word tin T.", "labels": [], "entities": [{"text": "lexical translation probability (LTP) t", "start_pos": 47, "end_pos": 86, "type": "METRIC", "confidence": 0.6603572964668274}]}, {"text": "The pair of words (s, t), or more precisely (s, t, i,j) since there could be more than one instance of s or t, is called a connection.", "labels": [], "entities": []}, {"text": "Model 2 enhances Model I by considering the dependence of Pr(S I T) on the distortion probability (DP) d(i I J, l, m) where I and mare the respective lengths of Sand T measured in number of words.", "labels": [], "entities": [{"text": "Pr", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9727003574371338}, {"text": "distortion probability (DP) d", "start_pos": 75, "end_pos": 104, "type": "METRIC", "confidence": 0.9671817819277445}]}, {"text": "propose using an adaptive Expectation and Maximization (EM) algorithm to estimate the parameters for LTP and DP from a bilingual corpus.", "labels": [], "entities": [{"text": "adaptive Expectation and Maximization (EM)", "start_pos": 17, "end_pos": 59, "type": "TASK", "confidence": 0.6869975456169674}]}, {"text": "The EM algorithm iterates between two phases to estimate LTP and DP until both functions converge.", "labels": [], "entities": [{"text": "LTP", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.7637044787406921}]}, {"text": "In the expectation phase, the parameters t(s I t) and d) in the SMT model for all possible values of s, t, i, j, I, and mare estimated from the sample of an aligned bilingual corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9877600073814392}]}, {"text": "In the maximization phase, each sentence-translation pair in the corpus is aligned by maximizing the translation probability, Pr(S I T).", "labels": [], "entities": []}, {"text": "They examine the feasibility of aligning the English-French Hansards corpus using the SMT model, on both the sentence level and the word level.", "labels": [], "entities": [{"text": "English-French Hansards corpus", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.6876922249794006}, {"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9196759462356567}]}, {"text": "The SMT model is then tested for the task of machine translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.987581729888916}, {"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8136677742004395}]}, {"text": "The model produces 35 acceptable translations for 73 sentences.", "labels": [], "entities": []}, {"text": "However, to our knowledge, the degree of success of word alignment has not yet been explored.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.8188409209251404}]}, {"text": "observe that reliably distinguishing sentence boundaries fora noisy bilingual text scanned by an OCR device is quite difficult.", "labels": [], "entities": []}, {"text": "In such a circumstance, they recommend aligning words directly without the preprocessing phase of sentence alignment.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7129696905612946}]}, {"text": "Under that proposal, a rough character-by-character alignment is first performed.", "labels": [], "entities": [{"text": "character-by-character alignment", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.6172731220722198}]}, {"text": "Based on the character alignment, words are subsequently aligned based on a modified version of Brown et al.'s Model 2.", "labels": [], "entities": []}, {"text": "The authors report that 60.5% of 65,000 words in a noisy document are correctly aligned.", "labels": [], "entities": [{"text": "correctly", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9731824398040771}]}, {"text": "For 84% of the words, the offset from correct alignment is at most 3.", "labels": [], "entities": [{"text": "offset", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9659913182258606}, {"text": "correct alignment", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.8590430021286011}]}, {"text": "Gale and Church (1991b) present an alternative algorithm that does not estimate and store probabilities for all word pairs to reduce memory requirement and to ensure robustness of probability estimation.", "labels": [], "entities": []}, {"text": "Instead, for each source word s, only a handful of target words strongly associated with s are found and stored.", "labels": [], "entities": []}, {"text": "Such a task is achieved by applying a X2-1ike statistic.", "labels": [], "entities": []}, {"text": "They report that the method produces highly precise (95%) alignment for 61% of the words in the 800 sentences tested.", "labels": [], "entities": []}, {"text": "This paper is motivated by the following observations: First, the above survey dearly reveals that word-based methods offer only limited coverage even after they are trained with an extremely large bilingual corpus.", "labels": [], "entities": []}, {"text": "Second, we believe that for most applications, low coverage is just as serious as low precision.", "labels": [], "entities": [{"text": "coverage", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9104369878768921}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9968361854553223}]}, {"text": "For aligned corpora to be useful for NLP tasks such as machine translation and word-sense disambiguation, a coverage rate higher than 60% is desirable, even at the expense of a slightly lower precision rate.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.797175258398056}, {"text": "word-sense disambiguation", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.7601957023143768}, {"text": "coverage rate", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.9713404178619385}, {"text": "precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.997930645942688}]}, {"text": "This paper presents a word alignment algorithm based on classification in existing thesauri.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7982336580753326}]}, {"text": "The proposed algorithm, called ClassAlign, relies on an automatic procedure to acquire class-based alignment rules; it does not employ word-by-word translation probabilities, nor does it use an iterative EM algorithm for estimating such probabilities.", "labels": [], "entities": []}, {"text": "Experimental results indicate that classification based on existing thesauri is highly effective in broadening coverage while maintaining a high precision rate.", "labels": [], "entities": [{"text": "broadening coverage", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8901996612548828}, {"text": "precision rate", "start_pos": 145, "end_pos": 159, "type": "METRIC", "confidence": 0.9799215197563171}]}, {"text": "The rest of this paper is organized as follows: In Section 2 we briefly discuss the nature of text and translation that justifies a class-based approach.", "labels": [], "entities": []}, {"text": "A set of three algorithms leading to class-based alignment are outlined in Section 3.", "labels": [], "entities": [{"text": "class-based alignment", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6140214055776596}]}, {"text": "The algorithms' effectiveness is demonstrated through examples and their translations in the LecDOCE (Longman Group 1992), a bilingual version of the Longman Dictionary of Contemporary English, as well as sentences from bilingual texts in the LightShip User's Guide.", "labels": [], "entities": [{"text": "LecDOCE (Longman Group 1992)", "start_pos": 93, "end_pos": 121, "type": "DATASET", "confidence": 0.915388822555542}, {"text": "Longman Dictionary of Contemporary English", "start_pos": 150, "end_pos": 192, "type": "DATASET", "confidence": 0.89844229221344}]}, {"text": "The experiments we undertook to assess the performance of these algorithms are the topic of Section 4.", "labels": [], "entities": []}, {"text": "Quantitative experimental results are also summarized.", "labels": [], "entities": []}, {"text": "In Section 5, we analyze the experimental results and consider ways in which the proposed algorithms might be extended and improved.", "labels": [], "entities": []}, {"text": "Concluding remarks are made in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the proposed method's effectiveness, we have implemented the algorithms described in Section 3 and conducted a series of experiments.", "labels": [], "entities": []}, {"text": "Tests are performed on the sentences found in the LecDOCE and a user's manual available in both languages to assess the method's robustness and generality.", "labels": [], "entities": [{"text": "LecDOCE", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9706995487213135}]}, {"text": "The similarities and differences between English and Mandarin texts are briefly reviewed, since our experiments involve the alignment of English-Mandarin parallel corpora.", "labels": [], "entities": []}, {"text": "A general description of the materials used in the experiments follows.", "labels": [], "entities": []}, {"text": "Finally, the success rates are quantitatively evaluated.", "labels": [], "entities": []}, {"text": "The experimental results obtained from the proposed algorithm with respect to word alignment are presented in this section.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7775169312953949}]}, {"text": "Nearly 42,000 example sentences and their translations from the LecDOCE were used as training data, primarily to acquire rules and to determine MLE estimates for the cases of LTP and DP.", "labels": [], "entities": [{"text": "LecDOCE", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9613317847251892}]}, {"text": "The algorithm's performance was evaluated using the two sets of data.", "labels": [], "entities": []}, {"text": "The first three experiments were designed to demonstrate the effectiveness of the naive DictAlign algorithm based on a bilingual MRD.", "labels": [], "entities": []}, {"text": "According to the experimental results, although DictAlign produces high-precision alignment, the coverage for both test sets is below 30%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9904224872589111}]}, {"text": "However, if the thesaurus effect is exploited, the coverage can be increased considerably, at the cost of a decrease of less than 4% in precision.", "labels": [], "entities": [{"text": "coverage", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9755481481552124}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9985719919204712}]}, {"text": "In the fourth experiment, the ClassAlign algorithm is employed to align both sets of test data again.", "labels": [], "entities": []}, {"text": "reveals that the acquired conceptual information compensates for what is lacking in the LecDOCE to yield optimum alignment results.", "labels": [], "entities": [{"text": "LecDOCE", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.9010106921195984}]}, {"text": "The ClassAlign algorithm expands coverage almost twofold to over 80%, while maintaining the same level of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9981451034545898}]}, {"text": "The generality of the approach is evident from the open test's comparably high coverage and precision rates.", "labels": [], "entities": [{"text": "coverage", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9973113536834717}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9985820055007935}]}, {"text": "As shown in, over 80% of the source words in both test sets are connected to a target and over 90% of the connections are true ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4  All connection candidates and DTSim values in (El0, C10).", "labels": [], "entities": []}, {"text": " Table 8  Alignment connections for example (El2, C12).", "labels": [], "entities": []}, {"text": " Table 9  Maximum likelihood estimation (MLE) of LTP.", "labels": [], "entities": [{"text": "Maximum likelihood estimation (MLE)", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.8754901389280955}]}, {"text": " Table 10  Maximum likelihood estimation (MLE) of DE", "labels": [], "entities": [{"text": "Maximum likelihood estimation (MLE)", "start_pos": 11, "end_pos": 46, "type": "METRIC", "confidence": 0.8549724419911703}]}, {"text": " Table 14  The connection candidates (s, t) in (El0, C10) with higher Pr(s, t) values.", "labels": [], "entities": [{"text": "Pr", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9685080051422119}]}]}