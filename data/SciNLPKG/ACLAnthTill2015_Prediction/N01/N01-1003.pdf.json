{"title": [{"text": "SPoT: A Trainable Sentence Planner", "labels": [], "entities": []}], "abstractContent": [{"text": "Sentence planning is a set of interrelated but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences.", "labels": [], "entities": [{"text": "Sentence planning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9121629893779755}, {"text": "sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences", "start_pos": 79, "end_pos": 229, "type": "Description", "confidence": 0.7400089101149485}]}, {"text": "In this paper, we present SPoT, a sentence planner, and anew methodology for automatically training SPoT on the basis of feedback provided by human judges.", "labels": [], "entities": [{"text": "sentence planner", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7169188559055328}]}, {"text": "We reconceptualize the task into two distinct phases.", "labels": [], "entities": []}, {"text": "First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of possible sentence plans fora given text-plan input.", "labels": [], "entities": []}, {"text": "Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan.", "labels": [], "entities": []}, {"text": "The SPR uses ranking rules automatically learned from training data.", "labels": [], "entities": [{"text": "SPR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9289146065711975}]}, {"text": "We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan.", "labels": [], "entities": [{"text": "SPR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9543659090995789}]}], "introductionContent": [{"text": "Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences.", "labels": [], "entities": [{"text": "Sentence planning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9186335206031799}, {"text": "sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences", "start_pos": 80, "end_pos": 218, "type": "Description", "confidence": 0.7035165727138519}]}, {"text": "For example, consider the required capabilities of a sentence planner fora mixed-initiative spoken dialog system for travel planning: (D1) System1: Welcome....", "labels": [], "entities": []}, {"text": "What airport would you like to fly out of?", "labels": [], "entities": []}, {"text": "User2: I need to go to Dallas.", "labels": [], "entities": []}, {"text": "System3: Flying to Dallas.", "labels": [], "entities": []}, {"text": "What departure airport was that?", "labels": [], "entities": []}, {"text": "User4: from Newark on September the 1st.", "labels": [], "entities": [{"text": "Newark on September the 1st", "start_pos": 12, "end_pos": 39, "type": "DATASET", "confidence": 0.9319363236427307}]}, {"text": "System5: What time would you like to travel on September the 1st to Dallas from Newark?", "labels": [], "entities": []}, {"text": "Utterance System1 requests information about the caller's departure airport, but in User2, the caller takes the initiative to provide information about her destination.", "labels": [], "entities": [{"text": "User2", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9467266798019409}]}, {"text": "In System3, the system's goal is to implicitly confirm the destination (because of the possibility of error in the speech recognition component), and request information (for the second time) of the caller's departure airport.", "labels": [], "entities": []}, {"text": "In User4, the caller provides this information but also provides the month and day of travel.", "labels": [], "entities": [{"text": "User4", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9189659357070923}]}, {"text": "Given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and destination cities and the month and day information, as well as to request information about the time of travel.", "labels": [], "entities": []}, {"text": "The system's representation of its communicative goals for utterance System5 is in.", "labels": [], "entities": []}, {"text": "The job of the sentence planner is to decide among the large number of potential realizations of these communicative goals.", "labels": [], "entities": []}, {"text": "Some example alternative realizations are in. implicit-confirm(orig-city:NEWARK) implicit-confirm(dest-city:DALLAS) implicit-confirm(month:9) implicit-confirm(day-number:1) request(depart-time) Figure 1: The text plan (communicative goals) for utterance System5 in dialog D1 Figure 2: Alternative sentence plan realizations for the text plan for utterance System5 in dialog D1.", "labels": [], "entities": []}, {"text": "H = human rating, RB = RankBoost score.", "labels": [], "entities": [{"text": "human rating", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.7716555595397949}, {"text": "RB = RankBoost score", "start_pos": 18, "end_pos": 38, "type": "METRIC", "confidence": 0.7503795474767685}]}, {"text": "In this paper, we present SPoT, for \"Sentence Planner, Trainable\".", "labels": [], "entities": [{"text": "SPoT", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9486885666847229}, {"text": "Sentence Planner", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.9102216958999634}]}, {"text": "We also present anew methodology for automatically training SPoT on the basis of feedback provided by human judges.", "labels": [], "entities": [{"text": "SPoT", "start_pos": 60, "end_pos": 64, "type": "TASK", "confidence": 0.7672252655029297}]}, {"text": "In order to train SPoT, we reconceptualize its task as consisting of two distinct phases.", "labels": [], "entities": []}, {"text": "In the first phase, the sentence-plan-generator (SPG) generates a potentially large sample of possible sentence plans fora given text-plan input.", "labels": [], "entities": []}, {"text": "In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer.", "labels": [], "entities": []}, {"text": "Our primary contribution is a method for training the SPR.", "labels": [], "entities": [{"text": "SPR", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8906084299087524}]}, {"text": "The SPR uses rules automatically learned from training data, using techniques similar to.", "labels": [], "entities": [{"text": "SPR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9388105869293213}]}, {"text": "Our method for training a sentence planner is unique in neither depending on hand-crafted rules, nor on the existence of a text or speech corpus in the domain of the sentence planner obtained from the interaction of a human with a system or another human.", "labels": [], "entities": [{"text": "sentence planner", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7246539294719696}]}, {"text": "We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan.", "labels": [], "entities": [{"text": "SPR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9543659090995789}]}, {"text": "In the remainder of the paper, section 2 describes the sentence planning task in more detail.", "labels": [], "entities": [{"text": "sentence planning task", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8029959599177042}]}, {"text": "We then describe the sentence plan generator (SPG) in section 3, the sentence plan ranker (SPR) in section 4, and the results in section 5.", "labels": [], "entities": [{"text": "sentence plan ranker (SPR)", "start_pos": 69, "end_pos": 95, "type": "METRIC", "confidence": 0.690175419052442}]}], "datasetContent": [{"text": "To train and test the SPR we partitioned the corpus into 5 disjoint folds and performed 5-fold cross-validation, in which at each fold, 80% of the examples were used for training an SPR and the other unseen 20% was used for testing.", "labels": [], "entities": [{"text": "SPR", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9069026112556458}]}, {"text": "This method ensures that every example occurs once in the test set.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the the trained SPR on the test sets of text plans by comparing for each text plan: shows the distributions of scores for the highest ranked sp-tree for each of the 100 text plans, according to the human experts, according to SPoT, and according to random choice.", "labels": [], "entities": []}, {"text": "The human rankings provide a topline for SPoT (since SPoT is choosing among options ranked by the humans, it cannot possibly do better), while the random scores provide a baseline.", "labels": [], "entities": []}, {"text": "The BEST distribution shows that 97% of text plans had at least one sentence plan ranked 4 or better.", "labels": [], "entities": [{"text": "BEST", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9836881160736084}]}, {"text": "The RANDOM distribution approximates the distribution of rankings for all sentence plans for all examples.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7398521304130554}]}, {"text": "Because each text plan is used in some fold of 5-fold cross validation as a test element, we assess the significance of the ranking differences with a paired t-test of SPOT to BEST and SPOT to RANDOM.", "labels": [], "entities": [{"text": "SPOT", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.881698727607727}, {"text": "BEST", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.8085628151893616}, {"text": "RANDOM", "start_pos": 193, "end_pos": 199, "type": "DATASET", "confidence": 0.6017916202545166}]}, {"text": "A paired t-test of SPOT to BEST shows that there are significant differences in performance (\u0090 ).", "labels": [], "entities": [{"text": "SPOT", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.7647280693054199}, {"text": "BEST", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9801178574562073}]}, {"text": "Perfect performance would have meant that there would be no significant difference.", "labels": [], "entities": []}, {"text": "However, the mean of BEST is 4.82 as compared with the mean of SPOT of 4.56, fora mean difference of 0.26 on a scale of 1 to 5.", "labels": [], "entities": [{"text": "BEST", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9965399503707886}, {"text": "SPOT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.7786135673522949}]}, {"text": "This is only a 5% difference in performance.", "labels": [], "entities": []}, {"text": "also shows that the main differences are in the lower half of the distribution of rankings; both distributions have a median of 5.", "labels": [], "entities": []}, {"text": "A paired t-test of SPOT to RANDOM shows that there are also significant differences in performance (\u0090 ).", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.5686375498771667}]}, {"text": "The median of the RANDOM distri- We then examined the rules that SPoT learned in training and the resulting RankBoost scores.", "labels": [], "entities": [{"text": "RANDOM distri-", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.7508313258488973}]}, {"text": "shows, for each alternative sentence plan, the BEST rating used as feedback to RankBoost and the score that RankBoost gave that example when it was in the test set in a fold.", "labels": [], "entities": [{"text": "BEST rating", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9777046144008636}, {"text": "RankBoost", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9594300985336304}]}, {"text": "Recall that RankBoost focuses on learning relative scores, not absolute values, so the scores are normalized to range between 0 and 1.", "labels": [], "entities": []}, {"text": "shows some of the rules that were learned on the training data, that were then applied to the alternative sentence plans in each test set of each fold in order to rank them.", "labels": [], "entities": []}, {"text": "We include only a subset of the rules that had the largest impact on the score of each sp-tree.", "labels": [], "entities": []}, {"text": "We discuss some particular rule examples hereto help the reader understand how SPoT's SPR works, but leave it to the reader to examine the thresholds and feature values in the remainder of the rules and sum the increments and decrements.", "labels": [], "entities": []}, {"text": "Rule (1) in states that an implicit confirmation as the first leaf of the sp-tree leads to a large (.94) increase in the score.", "labels": [], "entities": []}, {"text": "Thus all three of our alternative sptrees accrue this ranking increase.", "labels": [], "entities": []}, {"text": "Rules (2) and (5) state that the occurrence of 2 or more PRONOUN nodes in the DSyntS reduces the ranking by 0.85, and that 3 or more PRONOUN nodes reduces the ranking by an additional 0.34.", "labels": [], "entities": [{"text": "DSyntS", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.927056074142456}]}, {"text": "Alternative 8 is above the threshold for both of these rules; alternative 5 is above the threshold for Rule (2) and alternative 0 is always below the thresholds.", "labels": [], "entities": []}, {"text": "Rule (6) on the other hand increases only the scores of alternatives 0 and 5 by 0.33 since alternative 8 is below the threshold for that feature.", "labels": [], "entities": []}, {"text": "Note also that the quality of the rules in general seems to be high.", "labels": [], "entities": []}, {"text": "Although we provided multiple instantiations of features, some of which included parameters or lexical items that might identify particular discourse contexts, most of the learned rules utilize general properties of the sp-tree and the DSyntS.", "labels": [], "entities": []}, {"text": "This is probably partly due to the fact that we eliminated features that appeared fewer than 10 times in the training data, but also partly due to the fact that boosting algorithms in general appear to be resistant to overfitting the data ().", "labels": [], "entities": []}], "tableCaptions": []}