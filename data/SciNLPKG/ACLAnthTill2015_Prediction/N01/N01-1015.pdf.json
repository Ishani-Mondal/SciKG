{"title": [], "abstractContent": [{"text": "Using finite-state automata for the text analysis component in a text-to-speech system is problematic in several respects: the rewrite rules from which the automata are compiled are difficult to write and maintain, and the resulting automata can become very large and therefore inefficient.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.7333036959171295}]}, {"text": "Converting the knowledge represented explicitly in rewrite rules into a more efficient format is difficult.", "labels": [], "entities": []}, {"text": "We take an indirect route, learning an efficient decision tree representation from data and tapping information contained in existing rewrite rules, which increases performance compared to learning exclusively from a pronunciation lexicon.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text-to-speech (TTS) systems, like any other piece of sophisticated software, suffer from the shortcomings of the traditional software development process.", "labels": [], "entities": []}, {"text": "Highly skilled developers area costly resource, the complexity and sheer size of the code involved are difficult to manage.", "labels": [], "entities": []}, {"text": "A paradigmatic example of this is the letter-to-sound component within the text analysis module of a mature largescale text-to-speech system.", "labels": [], "entities": []}, {"text": "In the system described in) text analysis is performed using finite-state transducers compiled from rewrite rules and other high-level descriptions.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.7622820436954498}]}, {"text": "While the exclusive use of finite-state technology has advantages, it is not without its shortcomings, both technical and stemming from the use of hand-crafted rule sets and how they are represented: 1.", "labels": [], "entities": []}, {"text": "Extensive rule sets need to be constructed by human experts, which is labor-intensive and expensive).", "labels": [], "entities": []}, {"text": "2. Realistic rule sets are difficult to maintain because of complex interactions between serially composed rules.", "labels": [], "entities": []}, {"text": "3. Although rewrite rules can, in principle, be compiled into a huge monolithic transducer that is then very time-efficient, in practice this is not feasible because of the enormous sizes of the resulting machines (cf. the numbers given in and).", "labels": [], "entities": []}, {"text": "4. For reasons of space efficiency, certain computations are deferred until run-time (), with a significant impact on time efficiency.", "labels": [], "entities": []}, {"text": "While there is a clear need for human expert knowledge.), those experts should not have to deal with the performance aspects of the knowledge representation.", "labels": [], "entities": []}, {"text": "Ideally we would like to use a knowledge representation that is both time and space efficient and can be constructed automatically from individually meaningful features supplied by human experts.", "labels": [], "entities": []}, {"text": "For practical reasons we have to be content with methods that address the efficiency issues and can make use of explicitly represented knowledge from legacy systems, so that moving to anew way of building TTS systems does not entail starting over from scratch.", "labels": [], "entities": []}, {"text": "As a case study of how this transition might be achieved we took the letter-to-phoneme rules for French in the TTS system described in) and proceeded to 1.", "labels": [], "entities": [{"text": "TTS system", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.7528890073299408}]}, {"text": "Construct a lexicon using the existing system.", "labels": [], "entities": []}, {"text": "2. Produce an alignment for that lexicon.", "labels": [], "entities": []}, {"text": "3. Convert the aligned lexicon into training instances for an automatically induced classifier.", "labels": [], "entities": []}, {"text": "4. Train and evaluate decision trees.", "labels": [], "entities": []}, {"text": "By running the existing system on a small newspaper corpus (ca.", "labels": [], "entities": []}, {"text": "1M words of newspaper text from Le Monde) and eliminating abbreviations we obtained a lexicon of about 18k words.", "labels": [], "entities": [{"text": "newspaper text from Le Monde", "start_pos": 12, "end_pos": 40, "type": "DATASET", "confidence": 0.6695622801780701}]}, {"text": "This means that the performance of the automatically trained system built from this lexicon is relative to the existing system.", "labels": [], "entities": []}, {"text": "The key steps, aligning the lexicon and building a training set, are described in detail in Sections 2 and 3 below.", "labels": [], "entities": []}, {"text": "Our choice of decision trees was motivated by their following desirable properties: 1.", "labels": [], "entities": []}, {"text": "Space and time efficiency, provided the feature functions can be represented and computed efficiently, which they can be in our case.", "labels": [], "entities": []}, {"text": "3. Symbolic representation that can easily be inspected and converted.", "labels": [], "entities": []}, {"text": "The first property addresses the efficiency requirements stated above: if every feature function can be computed in time O(f ), where the function f does not involve the height of the decision tree h, then the classification function represented by the decision tree can be computed in time O(\u03bbn. h \u00d7 f (n)) = O(f ) if feature values can be mapped to child nodes inconstant time, e. g. through hashing; and similarly for space.", "labels": [], "entities": [{"text": "O", "start_pos": 310, "end_pos": 311, "type": "METRIC", "confidence": 0.9469608068466187}]}, {"text": "The other properties justify the use of decision trees as a knowledge representation format.", "labels": [], "entities": []}, {"text": "In particular, decision trees can be converted into implicational rules that an expert could inspect and can in principle be compiled back into finite-state machines, although that would re-introduce the original efficiency problems.", "labels": [], "entities": []}, {"text": "On the other hand, finite-state transducers have the advantage of being invertible, which can be exploited e. g. for testing hand-crafted rule sets.", "labels": [], "entities": []}, {"text": "We use a standard decision tree learner), since we believe that it would be premature to investigate the implications of different choices of machine learning algorithms while the fundamental question of what any such algorithm should use as training data is still open.", "labels": [], "entities": []}, {"text": "This topic is explored further in Section 5.", "labels": [], "entities": []}, {"text": "Related work is discussed in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We delineated a 90%/10% split of the lexicon and performed the alignment using a probability distribution with coefficients \u03bb 1 = 0, \u03bb 2 = 0.9, and \u03bb 3 = 0.1, i. e., no information from the rewrite rules was used and the empirical probabilities derived from the lexicon were smoothed slightly.", "labels": [], "entities": []}, {"text": "The value for \u03bb 3 was determined empirically after several trial runs on a held-out portion.", "labels": [], "entities": []}, {"text": "We then generated training instances as described in the previous section, and set aside the 10% we had earmarked earlier for testing purposes.", "labels": [], "entities": []}, {"text": "We ran C4.5 on the remaining portion of the data, using the held out 10% for testing.: Performance relative to context size, alignment based on lexicon crease, however, when we repeated the above procedure with different coefficients \u03bb.", "labels": [], "entities": []}, {"text": "This time we set \u03bb 1 = 0.9, \u03bb 2 = 0.09, and \u03bb 3 = 0.01.", "labels": [], "entities": []}, {"text": "These particular values were again determined empirically.", "labels": [], "entities": []}, {"text": "The important thing to note is that the information from the rewrite rules is now dominant, as compared to before when it was completely absent.", "labels": [], "entities": []}, {"text": "The effect this had on performance is summarized in for three letters of context.", "labels": [], "entities": []}, {"text": "As before, classification accuracy is given on a per-symbol basis; average accuracy per word is around 85%.", "labels": [], "entities": [{"text": "classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9478669166564941}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9574909806251526}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.984919548034668}]}, {"text": "Notice that the size of the tree decreases as a result of a better alignment.", "labels": [], "entities": []}, {"text": "These figures are all relative to our existing system.", "labels": [], "entities": []}, {"text": "What is most important to us are the vast improvements in efficiency: the decision trees take up less than 10% of the space of the original letter-tophoneme component, which weighs in at 6.7 MB total with composition deferred until runtime, since off-line composition would have resulted in an impractically large machine.", "labels": [], "entities": []}, {"text": "The size of the original component could be reduced through the use of compression techniques, which would lead to an additional run-time overhead.", "labels": [], "entities": []}, {"text": "Classification speed of the decision trees is on the order of several thousand letters per second (depending on platform details), which is many times faster than the existing system.", "labels": [], "entities": []}, {"text": "The exact details of a speed comparison depend heavily on platform issues and what one considers to be the average case, but a conservative estimate places the speedup at a factor of 20 or more.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance relative to context size, align- ment based on lexicon", "labels": [], "entities": []}, {"text": " Table 2: Performance relative to alignment quality  (context size: 3)", "labels": [], "entities": []}]}