{"title": [{"text": "A Structured Language Model based on Context-Sensitive Probabilistic Left-Corner Parsing", "labels": [], "entities": [{"text": "Context-Sensitive Probabilistic Left-Corner Parsing", "start_pos": 37, "end_pos": 88, "type": "TASK", "confidence": 0.5938167423009872}]}], "abstractContent": [{"text": "Recent contributions to statistical language model-ing for speech recognition have shown that prob-abilistically parsing a partial word sequence aids the prediction of the next word, leading to \"struc-tured\" language models that have the potential to outperform n-grams.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.730610728263855}, {"text": "prob-abilistically parsing a partial word sequence", "start_pos": 94, "end_pos": 144, "type": "TASK", "confidence": 0.7510553002357483}]}, {"text": "Existing approaches to struc-tured language modeling construct nodes in the partial parse tree after all of the underlying words have been predicted.", "labels": [], "entities": [{"text": "struc-tured language modeling", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6534542838732401}]}, {"text": "This paper presents a different approach , based on probabilistic left-corner grammar (PLCG) parsing, that extends a partial parse both from the bottom up and from the top down, leading to a more focused and more accurate, though somewhat less robust, search of the parse space.", "labels": [], "entities": [{"text": "probabilistic left-corner grammar (PLCG) parsing", "start_pos": 52, "end_pos": 100, "type": "TASK", "confidence": 0.6169546714850834}]}, {"text": "At the core of our new structured language model is a fast context-sensitive and lexicalized PLCG parsing algorithm that uses dynamic programming.", "labels": [], "entities": [{"text": "PLCG parsing", "start_pos": 93, "end_pos": 105, "type": "TASK", "confidence": 0.7334617227315903}]}, {"text": "Preliminary perplexity and word-accuracy results appear to be competitive with previous ones, while speed is increased.", "labels": [], "entities": [{"text": "speed", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9981411695480347}]}], "introductionContent": [], "datasetContent": [{"text": "The main target application of our research into LM is speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8521084785461426}]}, {"text": "We performed N-best list rescoring experiments on the DARPA WSJ Nov '92 evaluation test set, non-verbalized punctuation.", "labels": [], "entities": [{"text": "DARPA WSJ Nov '92 evaluation test set", "start_pos": 54, "end_pos": 91, "type": "DATASET", "confidence": 0.9337128177285194}]}, {"text": "The N-best lists were obtained from the L&H Voice Xpress v4 speech recognizer using the standard trigram model included in the test suite (20k open vocabulary, no punctuation).", "labels": [], "entities": [{"text": "L&H Voice Xpress v4 speech recognizer", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.864121101796627}]}, {"text": "In we report word-recognition error rates (WER) after rescoring using Chelba-Jelinek and PLCG-based models.", "labels": [], "entities": [{"text": "word-recognition error rates (WER)", "start_pos": 13, "end_pos": 47, "type": "METRIC", "confidence": 0.8086066047350565}]}, {"text": "Both DI and GT smoothing methods yielded very comparable results.", "labels": [], "entities": [{"text": "GT smoothing", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.6542392373085022}]}, {"text": "Due to technical limitations, all the models except the baseline trigram were trimmed by ignoring highestorder events that occurred only once.", "labels": [], "entities": []}, {"text": "The best PLCG-based SLM trained on the BWC train set (f) performs worse than the official word trigram (a).", "labels": [], "entities": [{"text": "PLCG-based SLM", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.4554111510515213}, {"text": "BWC train set", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9950019915898641}]}, {"text": "However, since the BWC does not completely cover the complete WSJ0 LM train material and slightly differs in tokenization, it is more fair to compare with the performance of a word trigram trained on the BWC train set (b).", "labels": [], "entities": [{"text": "BWC", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9359219074249268}, {"text": "WSJ0 LM train material", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.9289385229349136}, {"text": "BWC train set", "start_pos": 204, "end_pos": 217, "type": "DATASET", "confidence": 0.9921170473098755}]}, {"text": "Results (g) and (h) show that the PLCG-based SLM lowers WER with 4% relative when used in combination with the baseline models.", "labels": [], "entities": [{"text": "WER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9974390268325806}]}, {"text": "A comparable result was obtained with the Chelba-Jelinek SLM (results (d) and (e)).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word trigram (baseline) and PTB model per-", "labels": [], "entities": [{"text": "PTB", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.832375705242157}]}, {"text": " Table 2: WER results (%) after 100-best list rescoring", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9637019038200378}, {"text": "rescoring", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.38572973012924194}]}]}