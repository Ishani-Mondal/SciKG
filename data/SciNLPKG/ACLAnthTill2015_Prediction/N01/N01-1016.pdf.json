{"title": [{"text": "Edit Detection and Parsing for Transcribed Speech", "labels": [], "entities": [{"text": "Edit Detection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7975795567035675}, {"text": "Transcribed Speech", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.841371476650238}]}], "abstractContent": [{"text": "We present a simple architecture for parsing transcribed speech in which an edited-word detector first removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words.", "labels": [], "entities": [{"text": "parsing transcribed speech", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.8771336873372396}]}, {"text": "The edit detector achieves a misclassification rate on edited words of 2.2%.", "labels": [], "entities": [{"text": "misclassification rate", "start_pos": 29, "end_pos": 51, "type": "METRIC", "confidence": 0.9450049698352814}]}, {"text": "(The NULL-model, which marks everything as not edited, has an error rate of 5.9%.)", "labels": [], "entities": [{"text": "NULL-model", "start_pos": 5, "end_pos": 15, "type": "DATASET", "confidence": 0.8420159220695496}, {"text": "error rate", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9879210293292999}]}, {"text": "To evaluate our parsing results we introduce anew evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of EDITED nodes.", "labels": [], "entities": []}, {"text": "By this metric the parser achieves 85.3% precision and 86.5% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9986094236373901}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9991934895515442}]}], "introductionContent": [{"text": "While significant effort has been expended on the parsing of written text, parsing speech has received relatively little attention.", "labels": [], "entities": [{"text": "parsing of written text", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.9100625365972519}, {"text": "parsing speech", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.9143908619880676}]}, {"text": "The comparative neglect of speech (or transcribed speech) is understandable, since parsing transcribed speech presents several problems absent in regular text: \"um\"s and \"ah\"s (or more formally, filled pauses), frequent use of parentheticals (e.g., \"you know\"), ungrammatical constructions, and speech repairs (e.g., \"Why didn't he, why didn't she stay home?\").", "labels": [], "entities": []}, {"text": "In this paper we present and evaluate a simple two-pass architecture for handling the problems of parsing transcribed speech.", "labels": [], "entities": [{"text": "parsing transcribed speech", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.901615838209788}]}, {"text": "The first pass tries to identify which of the words in the string are edited (\"why didn't he,\" in the above example).", "labels": [], "entities": []}, {"text": "These words are removed from the string given to the second pass, an already existing statistical parser trained on a transcribed speech * This research was supported in part by NSF grant LIS SBR 9720368 and by corpus.", "labels": [], "entities": [{"text": "NSF grant LIS SBR 9720368", "start_pos": 178, "end_pos": 203, "type": "DATASET", "confidence": 0.7590978741645813}]}, {"text": "(In particular, all of the research in this paper was performed on the parsed \"Switchboard\" corpus as provided by the Linguistic Data Consortium.)", "labels": [], "entities": [{"text": "Switchboard\" corpus", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.6040957371393839}, {"text": "Linguistic Data Consortium", "start_pos": 118, "end_pos": 144, "type": "DATASET", "confidence": 0.7433054049809774}]}, {"text": "This architecture is based upon a fundamental assumption: that the semantic and pragmatic content of an utterance is based solely on the unedited words in the word sequence.", "labels": [], "entities": []}, {"text": "This assumption is not completely true.", "labels": [], "entities": []}, {"text": "For example, Core and Schubert point to counterexamples such as \"have the engine take the oranges to Elmira, um, I mean, take them to Corning\" where the antecedent of \"them\" is found in the EDITED words.", "labels": [], "entities": [{"text": "Elmira", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.956917405128479}]}, {"text": "However, we believe that the assumption is so close to true that the number of errors introduced by this assumption is small compared to the total number of errors made by the system.", "labels": [], "entities": []}, {"text": "In order to evaluate the parser's output we compare it with the gold-standard parse trees.", "labels": [], "entities": []}, {"text": "For this purpose a very simple third pass is added to the architecture: the hypothesized edited words are inserted into the parser output (see Section 3 for details).", "labels": [], "entities": []}, {"text": "To the degree that our fundamental assumption holds, a \"real\" application would ignore this last step.", "labels": [], "entities": []}, {"text": "This architecture has several things to recommend it.", "labels": [], "entities": []}, {"text": "First, it allows us to treat the editing problem as a pre-process, keeping the parser unchanged.", "labels": [], "entities": []}, {"text": "Second, the major clues in detecting edited words in transcribed speech seem to be relatively shallow phenomena, such as repeated word and part-of-speech sequences.", "labels": [], "entities": []}, {"text": "The kind of information that a parser would add, e.g., the node dominating the EDITED node, seems much less critical.", "labels": [], "entities": []}, {"text": "Note that of the major problems associated with transcribed speech, we choose to deal with only one of them, speech repairs, in a special fashion.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.7033029347658157}]}, {"text": "Our reasoning here is based upon what one might and might not expect from a secondpass statistical parser.", "labels": [], "entities": []}, {"text": "For example, ungrammaticality in some sense is relative, so if the training corpus contains the same kind of ungrammatical examples as the testing corpus, one would not expect ungrammaticality itself to be a showstopper.", "labels": [], "entities": []}, {"text": "Furthermore, the best statistical parsers do not use grammatical rules, but rather define probability distributions overall possible rules.", "labels": [], "entities": []}, {"text": "Similarly, parentheticals and filled pauses exist in the newspaper text these parsers currently handle, albeit at a much lower rate.", "labels": [], "entities": []}, {"text": "Thus there is no particular reason to expect these constructions to have a major impact.", "labels": [], "entities": []}, {"text": "1 This leaves speech repairs as the one major phenomenon not present in written text that might pose a major problem for our parser.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7263792753219604}]}, {"text": "It is for that reason that we have chosen to handle it separately.", "labels": [], "entities": []}, {"text": "The organization of this paper follows the architecture just described.", "labels": [], "entities": []}, {"text": "Section 2 describes the first pass.", "labels": [], "entities": []}, {"text": "We present therein a boosting model for learning to detect edited nodes (Sections 2.1 -2.2) and an evaluation of the model as a stand-alone edit detector (Section 2.3).", "labels": [], "entities": []}, {"text": "Section 3 describes the parser.", "labels": [], "entities": []}, {"text": "Since the parser is that already reported in, this section simply describes the parsing metrics used (Section 3.1), the details of the experimental setup (Section 3.2), and the results (Section 3.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "For the purposes of this research the Switchboard corpus, as distributed by the Linguistic Data Consortium, was divided into four sections and the word immediately following the interregnum also appears in a (different) rough copy, then we say that the interregnum word token appears in a rough copy.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.751613587141037}, {"text": "Linguistic Data Consortium", "start_pos": 80, "end_pos": 106, "type": "DATASET", "confidence": 0.736874908208847}]}, {"text": "This permits us to approximate the Switchboard annotation convention of annotating interregna as EDITED if they appear in iterated edits.", "labels": [], "entities": [{"text": "EDITED", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.8994389176368713}]}, {"text": "The training subcorpus consists of all files in the directories 2 and 3 of the parsed/merged Switchboard corpus.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.8358877897262573}]}, {"text": "Directory 4 is split into three approximately equal-size sections.", "labels": [], "entities": []}, {"text": "(Note that the files are not consecutively numbered.)", "labels": [], "entities": []}, {"text": "The first of these (files sw4004.mrg to sw4153.mrg) is the testing corpus.", "labels": [], "entities": []}, {"text": "All edit detection and parsing results reported herein are from this subcorpus.", "labels": [], "entities": [{"text": "edit detection and parsing", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6887669712305069}]}, {"text": "The files sw4154.mrg to sw4483.mrg are reserved for future use.", "labels": [], "entities": []}, {"text": "The files sw4519.mrg to sw4936.mrg are the development corpus.", "labels": [], "entities": []}, {"text": "In the complete corpus three parse trees were sufficiently ill formed in that our tree-reader failed to read them.", "labels": [], "entities": []}, {"text": "These trees received trivial modifications to allow them to be read, e.g., adding the missing extra set of parentheses around the complete tree.", "labels": [], "entities": []}, {"text": "We trained our classifier on the parsed data files in the training and development sections, and evaluated the classifer on the test section.", "labels": [], "entities": []}, {"text": "Section 3 evaluates the parser's output in conjunction with this classifier; this section focuses on the classifier's performance at the individual word token level.", "labels": [], "entities": []}, {"text": "In our complete application, the classifier uses a bitag tagger to assign each word a POS tag.", "labels": [], "entities": []}, {"text": "Like all such taggers, our tagger has a nonnegligible error rate, and these tagging could conceivably affect the performance of the classifier.", "labels": [], "entities": [{"text": "error rate", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9278175532817841}]}, {"text": "To determine if this is the case, we report classifier performance when trained both on \"Gold Tags\" (the tags assigned by the human annotators of the Switchboard corpus) and on \"Machine Tags\" (the tags assigned by our bitag tagger).", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 150, "end_pos": 168, "type": "DATASET", "confidence": 0.8890946507453918}]}, {"text": "We compare these results to a baseline \"null\" classifier, which never identifies a word as EDITED.", "labels": [], "entities": []}, {"text": "Our basic measure of performance is the word misclassification rate (see Section 2.1).", "labels": [], "entities": [{"text": "misclassification rate", "start_pos": 45, "end_pos": 67, "type": "METRIC", "confidence": 0.8249888122081757}]}, {"text": "However, we also report precision and recall scores for EDITED words alone.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9996360540390015}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9988296627998352}]}, {"text": "All words are assigned one of the two possible labels, EDITED or not.", "labels": [], "entities": [{"text": "EDITED", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9691981673240662}]}, {"text": "However, in our evaluation we report the accuracy of only words other than punctuation and filled pauses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9993232488632202}]}, {"text": "Our logic here is much the same as that in the statistical parsing community which ignores the location of punctuation for purposes of evaluation on the grounds that its placement is entirely conventional.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7829587459564209}]}, {"text": "The same can be said for filled pauses in the switchboard corpus.", "labels": [], "entities": [{"text": "switchboard corpus", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.8025836646556854}]}, {"text": "Our results are given in  The parser described in was trained on the Switchboard training corpus as specified in section 2.1.", "labels": [], "entities": [{"text": "Switchboard training corpus", "start_pos": 69, "end_pos": 96, "type": "DATASET", "confidence": 0.7629818121592203}]}, {"text": "The input to the training algorithm was the gold standard parses minus all EDITED nodes and their children.", "labels": [], "entities": []}, {"text": "We tested on the Switchboard testing subcorpus (again as specified in Section 2.1).", "labels": [], "entities": [{"text": "Switchboard testing subcorpus", "start_pos": 17, "end_pos": 46, "type": "DATASET", "confidence": 0.8667444586753845}]}, {"text": "All parsing results reported herein are from all sentences of length less than or equal to 100 words and punctuation.", "labels": [], "entities": []}, {"text": "When parsing the test corpus we carried out the following operations: 1.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9789437055587769}]}, {"text": "create the simplified gold standard parse by removing non-terminal children of an EDITED node and merging consecutive EDITED nodes.", "labels": [], "entities": []}, {"text": "2. remove from the sentence to be fed to the parser all words marked as edited by an edit detector (see below).", "labels": [], "entities": []}, {"text": "3. parse the resulting sentence.", "labels": [], "entities": []}, {"text": "4. add to the resulting parse EDITED nodes containing the non-terminal symbols removed in step 2.", "labels": [], "entities": []}, {"text": "The nodes are added as high as possible (though the definition of equivalence from Section 3.1 should make the placement of this node largely irrelevant).", "labels": [], "entities": []}, {"text": "5. evaluate the parse from step 4 against the simplified gold standard parse from step 1.", "labels": [], "entities": []}, {"text": "We ran the parser in three experimental situations, each using a different edit detector in step 2.", "labels": [], "entities": []}, {"text": "In the first of the experiments (labeled \"Gold Edits\") the \"edit detector\" was simply the simplified gold standard itself.", "labels": [], "entities": []}, {"text": "This was to see how well the parser would do it if had perfect information about the edit locations.", "labels": [], "entities": []}, {"text": "In the second experiment (labeled \"Gold Tags\"), the edit detector was the one described in Section 2 trained and tested on the part-ofspeech tags as specified in the gold standard trees.", "labels": [], "entities": [{"text": "gold standard trees", "start_pos": 166, "end_pos": 185, "type": "DATASET", "confidence": 0.7515586018562317}]}, {"text": "Note that the parser was not given the gold standard part-of-speech tags.", "labels": [], "entities": []}, {"text": "We were interested in contrasting the results of this experiment with that of the third experiment to gauge what improvement one could expect from using a more sophisticated tagger as input to the edit detector.", "labels": [], "entities": []}, {"text": "In the third experiment (\"Machine Tags\") we used the edit detector based upon the machine generated tags.", "labels": [], "entities": []}, {"text": "The results of the experiments are given in.", "labels": [], "entities": []}, {"text": "The last line in the figure indicates the performance of this parser when trained and tested on Wall Street Journal text.", "labels": [], "entities": [{"text": "Wall Street Journal text", "start_pos": 96, "end_pos": 120, "type": "DATASET", "confidence": 0.9779393374919891}]}, {"text": "It is the \"Machine Tags\" results that we consider the \"true\" capability of the detector/parser combination: 85.3% precision and 86.5% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9992555975914001}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9991341233253479}]}], "tableCaptions": [{"text": " Table 1: Conditioning variables used in the EDITED classifier.", "labels": [], "entities": [{"text": "EDITED classifier", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.75563845038414}]}, {"text": " Table 2: Performance of the \"null\" classifier (which never marks a word as EDITED) and boosting  classifiers trained on \"Gold Tags\" and \"Machine Tags\".", "labels": [], "entities": [{"text": "EDITED", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.8973585367202759}]}, {"text": " Table 3: Results of Switchboard parsing, sentence length \u2264 100.", "labels": [], "entities": [{"text": "Switchboard parsing", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.9392144083976746}]}]}