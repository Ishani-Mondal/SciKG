{"title": [], "abstractContent": [{"text": "We present a lexicon-based approach to extracting sentiment from text.", "labels": [], "entities": [{"text": "extracting sentiment from text", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.8824394196271896}]}, {"text": "The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensification and negation.", "labels": [], "entities": []}, {"text": "SO-CAL is applied to the polarity classification task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.", "labels": [], "entities": [{"text": "polarity classification task", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.8021034995714823}]}, {"text": "We show that SO-CAL's performance is consistent across domains and on completely unseen data.", "labels": [], "entities": [{"text": "SO-CAL", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.9464425444602966}]}, {"text": "Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.", "labels": [], "entities": [{"text": "dictionary creation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8700048327445984}, {"text": "consistency", "start_pos": 119, "end_pos": 130, "type": "METRIC", "confidence": 0.9781792759895325}]}], "introductionContent": [{"text": "Semantic orientation (SO) is a measure of subjectivity and opinion in text.", "labels": [], "entities": [{"text": "Semantic orientation (SO)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8321613430976867}]}, {"text": "It usually captures an evaluative factor (positive or negative) and potency or strength (degree to which the word, phrase, sentence, or document in question is positive or negative) towards a subject topic, person, or idea.", "labels": [], "entities": []}, {"text": "When used in the analysis of public opinion, such as the automated interpretation of on-line product reviews, semantic orientation can be extremely helpful in marketing, measures of popularity and success, and compiling reviews.", "labels": [], "entities": []}, {"text": "The analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (, subjectivity, opinion mining (, analysis of stance (, appraisal (Martin and White 2005), point of view, evidentiality, and a few others, without expanding into neighboring disciplines and the study of emotion and affect.", "labels": [], "entities": [{"text": "automatic extraction of semantic orientation", "start_pos": 17, "end_pos": 61, "type": "TASK", "confidence": 0.675738525390625}, {"text": "sentiment analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.9298272728919983}, {"text": "opinion mining", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.7300156354904175}]}, {"text": "In this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9576112031936646}]}, {"text": "Our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.", "labels": [], "entities": []}, {"text": "There exist two main approaches to the problem of extracting sentiment automatically.", "labels": [], "entities": [{"text": "extracting sentiment automatically", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.928982655207316}]}, {"text": "The lexicon-based approach involves calculating orientation fora document from the semantic orientation of words or phrases in the document).", "labels": [], "entities": []}, {"text": "The text classification approach involves building classifiers from labeled instances of texts or sentences, essentially a supervised classification task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8149052858352661}]}, {"text": "The latter approach could also be described as a statistical or machine-learning approach.", "labels": [], "entities": []}, {"text": "We follow the first method, in which we use dictionaries of words annotated with the word's semantic orientation, or polarity.", "labels": [], "entities": []}, {"text": "Dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also, or automatically, using seed words to expand the list of words (.", "labels": [], "entities": []}, {"text": "Much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text ().", "labels": [], "entities": []}, {"text": "First, a list of adjectives and corresponding SO values is compiled into a dictionary.", "labels": [], "entities": []}, {"text": "Then, for any given text, all adjectives are extracted and annotated with their SO value, using the dictionary scores.", "labels": [], "entities": [{"text": "SO", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9715611934661865}]}, {"text": "The SO scores are in turn aggregated into a single score for the text.", "labels": [], "entities": []}, {"text": "The majority of the statistical text classification research builds Support Vector Machine classifiers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams).", "labels": [], "entities": [{"text": "statistical text classification", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.708436131477356}, {"text": "Support Vector Machine classifiers", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.4887012466788292}]}, {"text": "Classifiers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (Chaovalit and Zhou 2005;.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.993908166885376}, {"text": "detecting the polarity of a text", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.8268954952557882}]}, {"text": "However, although such classifiers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classifier is used in a different domain (Aue and Gamon; see also the discussion about domain specificity in Pang and Lee).", "labels": [], "entities": []}, {"text": "Consider, for example, an experiment using the Polarity Dataset, a corpus containing 2,000 movie reviews, in which Brooke (2009) extracted the 100 most positive and negative unigram features from an SVM classifier that reached 85.1% accuracy.", "labels": [], "entities": [{"text": "Polarity Dataset", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.6513465046882629}, {"text": "accuracy", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9885818362236023}]}, {"text": "Many of these features were quite predictable: worst, waste, unfortunately, and mess are among the most negative, whereas memorable, wonderful, laughs, and enjoyed are all highly positive.", "labels": [], "entities": [{"text": "mess", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9700673818588257}]}, {"text": "Other features are domain-specific and somewhat inexplicable: If the writer, director, plot, or script are mentioned, the review is likely to be disfavorable towards the movie, whereas the mention of performances, the ending, or even flaws, indicates a good movie.", "labels": [], "entities": []}, {"text": "Closedclass function words appear frequently; for instance, as, yet, with, and both are all extremely positive, whereas since, have, though, and those have negative weight.", "labels": [], "entities": []}, {"text": "Names also figure prominently, a problem noted by other researchers).", "labels": [], "entities": []}, {"text": "Perhaps most telling is the inclusion of unigrams like 2, video, tv, and series in the list of negative words.", "labels": [], "entities": []}, {"text": "The polarity of these words actually makes some sense in context: Sequels and movies adapted from video games or TV series do tend to be less well-received than the average movie.", "labels": [], "entities": []}, {"text": "However, these real-world facts are not the sort of knowledge a sentiment classifier ought to be learning; within the domain of movie reviews such facts are prejudicial, and in other domains (e.g., video games or TV shows) they are either irrelevant or a source of noise.", "labels": [], "entities": []}, {"text": "Another area where the lexicon-based model might be preferable to a classifier model is in simulating the effect of linguistic context.", "labels": [], "entities": []}, {"text": "On reading any document, it becomes apparent that aspects of the local context of a word need to betaken into account in SO assessment, such as negation (e.g., not good) and intensification (e.g., very good), aspects that named contextual valence shifters.", "labels": [], "entities": [{"text": "SO assessment", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.934717983007431}]}, {"text": "Research by concentrated on implementing those insights.", "labels": [], "entities": []}, {"text": "They dealt with negation and intensification by creating separate features, namely, the appearance of good might be either good (no modification) not good (negated good), int good (intensified good), or dim good (diminished good).", "labels": [], "entities": []}, {"text": "The classifier, however, cannot determine that these four types of good are in anyway related, and so in order to train accurately there must be enough examples of all four in the training corpus.", "labels": [], "entities": []}, {"text": "Moreover, we show in Section 2.4 that expanding the scope to two-word phrases does not deal with negation adequately, as it is often a long-distance phenomenon.", "labels": [], "entities": [{"text": "negation", "start_pos": 97, "end_pos": 105, "type": "TASK", "confidence": 0.9660322666168213}]}, {"text": "Recent work has begun to address this issue.", "labels": [], "entities": []}, {"text": "For instance, present a classifier that treats negation from a compositional point of view by first calculating polarity of terms independently, and then applying inference rules to arrive at a combined polarity score.", "labels": [], "entities": []}, {"text": "As we shall see in Section 2, our lexicon-based model handles negation and intensification in away that generalizes to all words that have a semantic orientation value.", "labels": [], "entities": []}, {"text": "A middle ground exists, however, with semi-supervised approaches to the problem., for instance, use semi-supervised methods to build domainindependent polarity classifiers.", "labels": [], "entities": []}, {"text": "Read and Carroll built different classifiers and show that they are more robust across domains.", "labels": [], "entities": []}, {"text": "Their classifiers are, in effect, dictionarybased, differing only in the methodology used to build the dictionary.", "labels": [], "entities": []}, {"text": "use co-training to incorporate labeled and unlabeled examples, also making use of a distinction between sentences with a first person subject and with other subjects.", "labels": [], "entities": []}, {"text": "Other hybrid methods include those of,,,, or. uses co-training in a method that uses English labeled data and an English classifier to learn a classifier for Chinese.", "labels": [], "entities": []}, {"text": "In our approach, we seek methods that operate at a deep level of analysis, incorporating semantic orientation of individual words and contextual valence shifters, yet do not aim at a full linguistic analysis (one that involves analysis of word senses or argument structure), although further work in that direction is possible.", "labels": [], "entities": []}, {"text": "In this article, starting in Section 2, we describe the Semantic Orientation CALculator (SO-CAL) that we have developed over the last few years.", "labels": [], "entities": [{"text": "Semantic Orientation CALculator", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6968677242596945}]}, {"text": "We first extract sentiment-bearing words (including adjectives, verbs, nouns, and adverbs), and use them to calculate semantic orientation, taking into account valence shifters (intensifiers, downtoners, negation, and irrealis markers).", "labels": [], "entities": []}, {"text": "We show that this lexicon-based method performs well, and that it is robust across domains and texts.", "labels": [], "entities": []}, {"text": "One of the criticisms raised against lexicon-based methods is that the dictionaries are unreliable, as they are either built automatically or hand-ranked by humans (.", "labels": [], "entities": []}, {"text": "In Section 3, we present the results of several experiments that show that our dictionaries are robust and reliable, both against other existing dictionaries, and as compared to values assigned by humans (through the use of the Mechanical Turk interface).", "labels": [], "entities": []}, {"text": "Section 4 provides comparisons to other work, and Section 5 conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the performance of all of SO-CAL's features, we used the following data sets:  Reference to domain portability, in this article and in other work, is usually limited to portability across different types of reviews.", "labels": [], "entities": []}, {"text": "This section shows that SO-CAL's performance is maintained across domains other than reviews, and across different types of text structures.", "labels": [], "entities": [{"text": "SO-CAL", "start_pos": 24, "end_pos": 30, "type": "TASK", "confidence": 0.9723830819129944}]}, {"text": "Even though SO-CAL was primarily designed to determine the sentiment of texts roughly a paragraph or longer, the evaluations reported in this section demonstrate comparable performance when applied to shorter texts such as headlines and sentences extracted from news and blogs.", "labels": [], "entities": []}, {"text": "We tested SO-CAL with four different data sets: the Multi-Perspective Question Answering (MPQA) corpus, version 2.0 (Wiebe, Wilson, and Cardie 2005); a collection of MySpace.com comments from Mike Thelwall (Prabowo and Thelwall 2009); a set of news and blog posts from Alina Andreevskaia (; and a set of headlines from Rada Mihalcea and Carlo Strappavara (.", "labels": [], "entities": [{"text": "Multi-Perspective Question Answering (MPQA)", "start_pos": 52, "end_pos": 95, "type": "TASK", "confidence": 0.7704585840304693}]}, {"text": "The first set of data is the MPQA corpus (version 2.0), a collection of news articles and other documents (texts from the American National Corpus and other sources) annotated for opinions and other private states (beliefs, emotions, speculation, etc.).", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9708616137504578}, {"text": "American National Corpus and other sources) annotated for opinions and other private states (beliefs, emotions, speculation, etc.)", "start_pos": 122, "end_pos": 252, "type": "Description", "confidence": 0.7659138143062592}]}, {"text": "We We thank them all for sharing their data with us. extracted all the sentences that contained subjective positive and negative expressions, in all levels of intensity (low, medium, high, and extreme).", "labels": [], "entities": []}, {"text": "The extracted set contains 663 positive and 1,211 negative sentences.", "labels": [], "entities": []}, {"text": "The data from Mike Thelwall consists of comments posted on MySpace.com.", "labels": [], "entities": [{"text": "MySpace.com", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9890016317367554}]}, {"text": "The annotation is done on a 1 to 5 scale, where 1 indicates \"no emotion.\"", "labels": [], "entities": []}, {"text": "As a consequence, we focused on the comments with scores of 4 and 5.", "labels": [], "entities": []}, {"text": "Because each comment had both a positive and negative label, we labeled \"positive\" those with a higher positive score and vice versa for negative labels, and excluded comments with the same score for both (i.e., neutral).", "labels": [], "entities": []}, {"text": "This yielded a total of 83 comments (59 positive, 24 negative).", "labels": [], "entities": []}, {"text": "The data from Alina Andreevskaia consist of individual sentences from both news and blogs, annotated according to whether they are negative, positive, or neutral.", "labels": [], "entities": []}, {"text": "We used only the negative and positive sentences (788 from news, and 802 from blogs, equally divided between positive and negative).", "labels": [], "entities": []}, {"text": "The Affective Text data from Rada Mihalcea and Carlo Strappavara was used in the 2007 SemEval task.", "labels": [], "entities": [{"text": "Affective Text data", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7086944480737051}, {"text": "SemEval task", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.7126153707504272}]}, {"text": "It contains 1,000 news headlines annotated with a range between \u2212100 (very negative) and 100 (very positive).", "labels": [], "entities": []}, {"text": "We excluded six headlines that had been labeled as 0 (therefore neutral), yielding 468 positive and 526 negative headlines.", "labels": [], "entities": []}, {"text": "In addition to the full evaluation, also propose a coarse evaluation, where headlines with scores \u2212100 to \u221250 are classified as negative, and those 50 to 100 as positive.", "labels": [], "entities": [{"text": "coarse", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9359068274497986}]}, {"text": "Excluding the headlines in the middle gives us 155 positive headlines and 255 negative ones.", "labels": [], "entities": []}, {"text": "shows the results of the evaluation.", "labels": [], "entities": []}, {"text": "Included in the table is a baseline for each data set, assigning polarity to the most frequent class for the data.", "labels": [], "entities": []}, {"text": "These data sets include much smaller spans of text than are found in consumer reviews, with some sentences or headlines not containing any words from the SO-CAL dictionaries.", "labels": [], "entities": []}, {"text": "This ranged from about 21% of the total in the MySpace comments to 54% in the headlines.", "labels": [], "entities": [{"text": "MySpace comments", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9436648488044739}]}, {"text": "Two approaches were used in this cross-domain evaluation when SO-CAL encountered texts for which it found no words in its dictionaries (SO-empty texts).", "labels": [], "entities": []}, {"text": "First, the backoff method involves using the most frequent polarity for the corpus (or positive, when they are equal), and assigning that polarity to all SO-empty texts.", "labels": [], "entities": []}, {"text": "This method provides results that can be directly compared to other results on these data sets, although, like the baseline, it assumes some knowledge about the polarity balance of the corpus.", "labels": [], "entities": []}, {"text": "The figures in the first section of suggest robust performance as compared to a mostfrequent-class baseline, including modest improvement over the relevant cross-domain results of. also use the headlines data, and obtain a polarity classification accuracy of 77.94% below our results excluding empty.", "labels": [], "entities": [{"text": "headlines data", "start_pos": 194, "end_pos": 208, "type": "DATASET", "confidence": 0.8828894793987274}, {"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.5099154710769653}]}, {"text": "21 By default, SO-CAL assigns a zero to such texts, which is usually interpreted to mean that the text is neither positive nor negative.", "labels": [], "entities": []}, {"text": "However, in a task where we know a priori that all texts are either positive or negative, this can be a poor strategy, because we will get all of these empty texts wrong: When there area significant number of empty texts, performance can be worse than guessing.", "labels": [], "entities": []}, {"text": "Note that the problem of how to interpret empty texts is not a major issue for the full text reviews where we typically apply SO-CAL, because there are very few of them; for instance, out of the 2,400 texts in the Camera corpus, only 4 were assigned a zero by SO-CAL.", "labels": [], "entities": [{"text": "Camera corpus", "start_pos": 214, "end_pos": 227, "type": "DATASET", "confidence": 0.7857474386692047}]}, {"text": "Guessing the polarity or removing those four texts entirely has no effect on the accuracy reported in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9994314312934875}]}, {"text": "22 Their ensemble classifier had 73.3% accuracy in news, but only 70.9% in blogs, and their performance in the Polarity Dataset was 62.1%, or over 14% lower than ours.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9992282390594482}, {"text": "Polarity Dataset", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.7635598182678223}]}, {"text": "23 Our results are not comparable to those of on the MySpace comments, as they classify the comments on a 1-5 scale (obtaining average accuracy of 60.6% and 72.8% in positive and negative comments, respectively), whereas we have a much simpler two-point scale (positive or negative).", "labels": [], "entities": [{"text": "MySpace comments", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9489607810974121}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9701210856437683}]}, {"text": "The second method used in evaluating SO-CAL on SO-empty texts is to only classify texts for which it has direct evidence to make a judgment.", "labels": [], "entities": [{"text": "SO-CAL", "start_pos": 37, "end_pos": 43, "type": "TASK", "confidence": 0.9280945658683777}]}, {"text": "Thus, we exclude such SO-empty texts from the evaluation.", "labels": [], "entities": []}, {"text": "The second part of shows the results of this evaluation.", "labels": [], "entities": []}, {"text": "The results are strikingly similar to the performance we saw on full review texts, with most attaining a minimum of 75-80% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9976230263710022}]}, {"text": "Although missing vocabulary (domain-specific or otherwise) undoubtedly plays a role, the results provide strong evidence that relative text size is the primary cause of SO-empty texts in these data sets.", "labels": [], "entities": [{"text": "SO-empty texts", "start_pos": 169, "end_pos": 183, "type": "TASK", "confidence": 0.9106898307800293}]}, {"text": "When the SO-empty texts are removed, the results are entirely comparable to those that we saw in the previous section.", "labels": [], "entities": []}, {"text": "Although sentence-level polarity detection is a more difficult task, and not one that SO-CAL was specifically designed for, the system has performed well on this task, here, and in related work (; Brooke and Hurst 2009).", "labels": [], "entities": [{"text": "sentence-level polarity detection", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.8136188586552938}]}, {"text": "Because we asked six Turkers to provide responses for each word, we can also calculate average percentage of pairwise agreement (the number of pairs of Turkers who agreed, divided by the total number of possible pairings), which for this task was 67.7%, well above chance but also far from perfect agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.6087333559989929}]}, {"text": "Note that we are not trying to establish reliability in the traditional sense.", "labels": [], "entities": [{"text": "reliability", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.983041524887085}]}, {"text": "Our method depends on a certain amount of disagreement; if it were simply the case that some \u22121 words were judged by all rankers as neutral (or positive), the best explanation for that fact would be errors in individual SO values.", "labels": [], "entities": []}, {"text": "If Turkers, however, generally agree about SO 5/\u22125 words, but generally disagree about SO 1/\u22121 words, this \"unreliability\" actually reflects the SO scale.", "labels": [], "entities": []}, {"text": "This is indeed the pattern we see in the data: Average pairwise agreement is 60.1% for 1/\u22121 words, but 98.2% for 5/\u22125 (see for similar results in a study of inter-annotator agreement in adjective dictionaries).", "labels": [], "entities": [{"text": "Average pairwise agreement", "start_pos": 47, "end_pos": 73, "type": "METRIC", "confidence": 0.7346298694610596}]}, {"text": "Interestingly, although we expected relatively equal numbers of positive and negative judgments at SO = 0, that was not the result.", "labels": [], "entities": [{"text": "SO", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9115265011787415}]}, {"text": "Instead, words with SO = 0 were sometimes interpreted as positive, but almost never interpreted as negative.", "labels": [], "entities": [{"text": "SO", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9637110233306885}]}, {"text": "This is mostly likely attributable to the default status of positive, and the marked character of negative expression; neutral description might betaken as being vaguely positive, but it would not be mistaken for negative expression.", "labels": [], "entities": []}, {"text": "For the word-pair task, we categorize the distribution data by the difference in SO between the two words, putting negative and positive words in separate tables.", "labels": [], "entities": [{"text": "SO", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9854815602302551}]}, {"text": "For instance, a \u22124 difference with negative adjectives means the result of comparing a \u22121 word to a \u22125 word, and a +2 difference corresponds to comparing a \u22125 with a negative \u22123, a \u22124 with a \u22122, or a \u22123 with a \u22121.", "labels": [], "entities": []}, {"text": "(We always compare words with the same sign, i.e., negative to negative.)", "labels": [], "entities": []}, {"text": "In early testing, we found that Turkers almost completely ignored the same category, and so we took steps (changing the instructions and the order of presentation) to try to counteract this effect.", "labels": [], "entities": []}, {"text": "Still, the same designation was underused.", "labels": [], "entities": []}, {"text": "There area number of possible explanations for this, all of which probably have some merit.", "labels": [], "entities": []}, {"text": "One is that our scale is far too coarse-grained, that we are collapsing distinguishable words into a single classification.", "labels": [], "entities": []}, {"text": "The trade-off here is with ease of ranking; if we provided, for instance, 20 SO values instead of 10, it would be more difficult to provide confident rankings, and would probably yield little in the way of tangible benefits.", "labels": [], "entities": []}, {"text": "Another potential confounding factor is that words within the same SO category often vary considerably on some other dimension, and it is not natural to think of them as being equivalent.", "labels": [], "entities": []}, {"text": "For instance, we judged savory, lush, and jolly to be equally positive, but they are applied to very different kinds of things, and so are not easily compared.", "labels": [], "entities": []}, {"text": "And, of course, even assuming our 10-point scale, there are words in our dictionary that do not belong together in the same category; our focus here is on the big picture, but we can use this data to identify words which are problematic and improve the dictionary in the next iteration.", "labels": [], "entities": []}, {"text": "The results in for the adjective word pair task are otherwise very encouraging.", "labels": [], "entities": [{"text": "adjective word pair task", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6763418167829514}]}, {"text": "Unlike the single word task, we see a clear linear pattern that covers the entire SO spectrum (though, again, there is noise).", "labels": [], "entities": []}, {"text": "At SO value difference = 0, same reaches a maximum, and positive and negative judgments are almost evenly distributed.", "labels": [], "entities": [{"text": "SO value difference", "start_pos": 3, "end_pos": 22, "type": "METRIC", "confidence": 0.9495518406232198}]}, {"text": "The average pairwise agreement on this task was somewhat lower, 60.0% 25 Note also that another drawback with pairwise agreement is that agreement does not change linearly with respect to the number of dissenters.", "labels": [], "entities": []}, {"text": "For example, in the six-rater task, a single disagreement drops agreement from 100% to 66.7%; a second disagreement drops the score to 40% if different than the first agreement, or 46.7% if the same.", "labels": [], "entities": [{"text": "agreement", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9615457653999329}]}, {"text": "26 This is the opposite result from the impressions reported by, where, in an evaluation of comments for eBay sellers, neutral comments were perceived as close to negative.", "labels": [], "entities": []}, {"text": "27 Other evidence that suggests making our scale more fine-grained is unlikely to help: When two words were difficult to distinguish, we often saw three different answers across the six Turkers.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 186, "end_pos": 193, "type": "DATASET", "confidence": 0.9231098890304565}]}, {"text": "For example, for \u22123 SO words fat and ignorant, three Turkers judged them the same, two judged ignorant as stronger, and one judged fat as stronger.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Examples of words in the noun and verb dictionaries.", "labels": [], "entities": []}, {"text": " Table 2  Examples from the adverb dictionary.", "labels": [], "entities": []}, {"text": " Table 4  Comparison of performance using different dictionaries.", "labels": [], "entities": []}, {"text": " Table 5  Performance of SO-CAL using various options.", "labels": [], "entities": [{"text": "SO-CAL", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.8807344436645508}]}, {"text": " Table 6  Performance across review types and on positive and negative reviews.", "labels": [], "entities": []}, {"text": " Table 7  Performance of SO-CAL in other domains.", "labels": [], "entities": []}, {"text": " Table 8  Distribution percentages for the negative/negated positive SO comparison task.", "labels": [], "entities": [{"text": "SO comparison task", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9255175590515137}]}, {"text": " Table 9  Comparison of performance of different dictionaries derived from SentiWordNet.", "labels": [], "entities": []}]}