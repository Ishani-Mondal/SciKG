{"title": [{"text": "Towards Automatic Error Analysis of Machine Translation Output", "labels": [], "entities": [{"text": "Automatic Error Analysis of Machine Translation Output", "start_pos": 8, "end_pos": 62, "type": "TASK", "confidence": 0.801226087978908}]}], "abstractContent": [{"text": "Evaluation and error analysis of machine translation output are important but difficult tasks.", "labels": [], "entities": [{"text": "error analysis of machine translation output", "start_pos": 15, "end_pos": 59, "type": "TASK", "confidence": 0.7349102844794592}]}, {"text": "In this article, we propose a framework for automatic error analysis and classification based on the identification of actual erroneous words using the algorithms for computation of Word Error Rate (WER) and Position-independent word Error Rate (PER), which is just a very first step towards development of automatic evaluation measures that provide more specific information of certain translation problems.", "labels": [], "entities": [{"text": "automatic error analysis and classification", "start_pos": 44, "end_pos": 87, "type": "TASK", "confidence": 0.6502402007579804}, {"text": "Word Error Rate (WER)", "start_pos": 182, "end_pos": 203, "type": "METRIC", "confidence": 0.8566586573918661}, {"text": "Position-independent word Error Rate (PER)", "start_pos": 208, "end_pos": 250, "type": "METRIC", "confidence": 0.7349003723689488}]}, {"text": "The proposed approach enables the use of various types of linguistic knowledge in order to classify translation errors in many different ways.", "labels": [], "entities": [{"text": "classify translation errors", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.7849852641423544}]}, {"text": "This work focuses on one possible setup , namely, on five error categories: inflectional errors, errors due to wrong word order, missing words, extra words, and incorrect lexical choices.", "labels": [], "entities": []}, {"text": "For each of the categories, we analyze the contribution of various POS classes.", "labels": [], "entities": []}, {"text": "We compared the results of automatic error analysis with the results of human error analysis in order to investigate two possible applications: estimating the contribution of each error type in a given translation output in order to identify the main sources of errors fora given translation system, and comparing different translation outputs using the introduced error categories in order to obtain more information about advantages and disadvantages of different systems and possibilites for improvements, as well as about advantages and disadvantages of applied methods for improvements.", "labels": [], "entities": [{"text": "automatic error analysis", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6477373739083608}]}, {"text": "We used Arabic-English Newswire and Broadcast News and Chinese-English Newswire outputs created in the framework of the GALE project, several Spanish and English European Parliament outputs generated during the TC-Star project, and three German-English outputs generated in the framework of the fourth Machine Translation Workshop.", "labels": [], "entities": [{"text": "GALE", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.7815545797348022}, {"text": "Machine Translation Workshop", "start_pos": 302, "end_pos": 330, "type": "TASK", "confidence": 0.8248120148976644}]}, {"text": "We show that our results correlate very well with the results of a human error analysis , and that all our metrics except the extra words reflect well the differences between different versions of the same translation system as well as the differences between different translation systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The evaluation of machine translation output is an important and at the same time difficult task for the progress of the field.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7506337761878967}]}, {"text": "Because there is no unique reference translation fora text (as for example in speech recognition), automatic measures are hard to define.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7863553464412689}]}, {"text": "Human evaluation, although of course providing (at least in principle) the most reliable judgments, is costly and time consuming.", "labels": [], "entities": []}, {"text": "A great deal of effort has been spent on finding measures that correlate well with human judgments when determining which one of a set of translation systems is the best (be it different versions of the same system in the development phase or a set of \"competing\" systems, as for example in a machine translation evaluation).", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 293, "end_pos": 323, "type": "TASK", "confidence": 0.8331431150436401}]}, {"text": "However, most of the work has been focused just on best-worst decisions, namely, finding a ranking between different machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.6750021725893021}]}, {"text": "Although this is useful information and helps in the continuous improvement of machine translation (MT) systems, MT researches often would find it helpful to have additional information about their systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8410548567771912}, {"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9712603092193604}]}, {"text": "What are the strengths of their systems?", "labels": [], "entities": []}, {"text": "Where do they make errors?", "labels": [], "entities": []}, {"text": "Does a particular modification improve some aspect of the system, although perhaps it does not improve the overall score in terms of one of the standard measures?", "labels": [], "entities": []}, {"text": "Does a worseranked system outperform a best-ranked one in any aspect?", "labels": [], "entities": []}, {"text": "Hardly any systematic work has been done in this direction and developers must resort to looking at the translation outputs in order to obtain an insight of the actual problems of their systems.", "labels": [], "entities": []}, {"text": "A framework for human error analysis and error classification has been proposed by, but as every human evaluation, this is also a difficult and timeconsuming task.", "labels": [], "entities": [{"text": "human error analysis", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.6466657320658366}, {"text": "error classification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.6709409952163696}]}, {"text": "This article presents a framework for automatic analysis and classification of errors in a machine translation output which is just a very first step in this direction.", "labels": [], "entities": [{"text": "automatic analysis and classification of errors in a machine translation output", "start_pos": 38, "end_pos": 117, "type": "TASK", "confidence": 0.6573907922614705}]}, {"text": "The basic idea is to extend the standard error rates using linguistic knowledge.", "labels": [], "entities": [{"text": "standard error rates", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.7204771637916565}]}, {"text": "The first step is the identification of the actual erroneous words using the algorithms for the calculation of Word Error Rate (WER) and Position-independent word Error Rate (PER).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 111, "end_pos": 132, "type": "METRIC", "confidence": 0.9000859459241232}, {"text": "Position-independent word Error Rate (PER)", "start_pos": 137, "end_pos": 179, "type": "METRIC", "confidence": 0.8317366157259259}]}, {"text": "The extracted erroneous words can then be used in combination with different types of linguistic knowledge, such as base forms, Part-of-Speech (POS) tags, Name Entity (NE) tags, compound words, suffixes, prefixes, and soon, in order to obtain various details about the nature of actual errors, for example, error categories (e.g., morphological errors, reordering errors, missing words), contribution of different word classes (e.g., POS, NE), and so forth.", "labels": [], "entities": []}, {"text": "The focus of this work is the definition of the following error categories: r inflectional errors r reordering errors r missing words r extra words r incorrect lexical choices and the comparison of the results of automatic error analysis with those obtained by human error analysis for these categories.", "labels": [], "entities": []}, {"text": "Each error category can be further classified according to POS tags (e.g., inflectional errors of verbs, missing pronouns).", "labels": [], "entities": []}, {"text": "The translation outputs used for the comparison of human and automatic error analysis were produced in the frameworks of the GALE 1 project, the TC-STAR 2 project, and the fourth Workshop on Statistical Machine Translation 3 (WMT09).", "labels": [], "entities": [{"text": "comparison of human and automatic error analysis", "start_pos": 37, "end_pos": 85, "type": "TASK", "confidence": 0.6165053290980202}, {"text": "GALE 1 project", "start_pos": 125, "end_pos": 139, "type": "DATASET", "confidence": 0.6104022264480591}, {"text": "Statistical Machine Translation 3 (WMT09)", "start_pos": 191, "end_pos": 232, "type": "TASK", "confidence": 0.7851690607411521}]}, {"text": "The comparison with human error analysis is done considering two possible applications: estimating the contribution of each error category in a particular translation output, and comparing different translation outputs using these categories.", "labels": [], "entities": [{"text": "human error analysis", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.6163615385691324}]}, {"text": "In addition, we show how the new error measures can be used to get more information about the differences between translation systems trained on different source and target languages, between different training set-ups fora same phrase-based translation system, as well as between different translation systems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 6  Results (raw error counts) of human (left) and automatic (right) error analysis for the  GALE corpora.", "labels": [], "entities": [{"text": "GALE corpora", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8478061258792877}]}, {"text": " Table 8  Results (raw error counts) of human (left) and automatic (right) error analysis for the GALE  corpora: Distribution of different error types over basic POS classes.", "labels": [], "entities": [{"text": "GALE  corpora", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.8260533511638641}]}, {"text": " Table 9  Correlation coefficients for the GALE corpora: Spearman rank \u03c1 (left column) and Pearson r  (right column) coefficient.", "labels": [], "entities": [{"text": "GALE corpora", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8046175539493561}, {"text": "Spearman rank \u03c1", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.9312749107678732}, {"text": "Pearson r  (right column) coefficient", "start_pos": 91, "end_pos": 128, "type": "METRIC", "confidence": 0.9403799261365619}]}, {"text": " Table 10  Results (raw error counts) of human (left) and automatic (right) error analysis for the TC-STAR  corpora.", "labels": [], "entities": [{"text": "TC-STAR  corpora", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.8514224290847778}]}, {"text": " Table 11  Results (raw error counts) of human (left) and automatic (right) error analysis for TC-STAR  corpora -inflectional details: tense (Vten) and person (Vper) of verbs, gender (Agen), and  number (Anum) of adjectives.", "labels": [], "entities": [{"text": "TC-STAR  corpora -inflectional details", "start_pos": 95, "end_pos": 133, "type": "TASK", "confidence": 0.6745551586151123}, {"text": "number (Anum)", "start_pos": 196, "end_pos": 209, "type": "METRIC", "confidence": 0.8430056422948837}]}, {"text": " Table 12  Correlation coefficients for the TC-STAR corpora: Spearman rank \u03c1 (left column) and Pearson r  (right column).", "labels": [], "entities": [{"text": "TC-STAR corpora", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.875556230545044}, {"text": "Spearman rank \u03c1", "start_pos": 61, "end_pos": 76, "type": "METRIC", "confidence": 0.9697336753209432}, {"text": "Pearson r", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.989141196012497}]}, {"text": " Table 13  Results (raw error counts) of human (left) and automatic (right) error analysis for six different  Spanish-to-English TC-STAR systems; Spearman (left) and Pearson (right) correlation coefficients  for each translation output across error categories (last column) and for each error category  across different translation outputs (last row).", "labels": [], "entities": [{"text": "Pearson (right) correlation", "start_pos": 166, "end_pos": 193, "type": "METRIC", "confidence": 0.8263957500457764}]}, {"text": " Table 14  Results (raw error counts) of human (left) and automatic (right) error analysis for three different  German-to-English WMT systems; Spearman (left) and Pearson (right) correlation coefficients  for each translation output across error categories (last column) and for each error category  across different translation outputs (last row).", "labels": [], "entities": [{"text": "Pearson (right) correlation", "start_pos": 163, "end_pos": 190, "type": "METRIC", "confidence": 0.8139573335647583}]}, {"text": " Table 16  Spearman (above) and Pearson (below) correlation coefficients for each POS class within one  error category across six different TC-STAR and three different WMT09 English translation  outputs. The coefficients are omitted for the cases when both error analyses detected zero errors.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9314607381820679}, {"text": "correlation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9843533635139465}, {"text": "TC-STAR", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9297242164611816}, {"text": "WMT09 English translation  outputs", "start_pos": 168, "end_pos": 202, "type": "DATASET", "confidence": 0.845257580280304}]}, {"text": " Table 17  Error categories -novel error rates (raw error counts in five error categories normalized over  reference length) compared with standard word error rates WER, PER, and TER. The BLEU score  is also shown as illustration.", "labels": [], "entities": [{"text": "WER", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.6252707242965698}, {"text": "PER", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.9169634580612183}, {"text": "TER", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.9938981533050537}, {"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9989078044891357}]}, {"text": " Table 18  Error categories for different source and target languages: examples of German-to-English,  French-to-English, Spanish-to-English, English-to-German, English-to-French, and  English-to-Spanish WMT09 translation outputs.", "labels": [], "entities": [{"text": "Error", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9645475149154663}, {"text": "WMT09 translation", "start_pos": 204, "end_pos": 221, "type": "TASK", "confidence": 0.5580889284610748}]}, {"text": " Table 19  Effects of local POS-based reorderings on reordering error rates for the Spanish-English  translation for reordered sentences and for the rest: overall RER, RER of nouns and adjectives,  and RER of verbs.", "labels": [], "entities": [{"text": "RER", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9969295859336853}, {"text": "RER", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.9920456409454346}, {"text": "RER", "start_pos": 202, "end_pos": 205, "type": "METRIC", "confidence": 0.9968665242195129}]}, {"text": " Table 20  Effects of long range POS-based reorderings on reordering error rates and missing word error  rates for the German-English translation for reordered sentences and for the rest.", "labels": [], "entities": [{"text": "missing word error  rates", "start_pos": 85, "end_pos": 110, "type": "METRIC", "confidence": 0.7363897636532784}]}, {"text": " Table 21  Error categories for different Spanish-to-English and English-to-Spanish TC-STAR translation  systems.", "labels": [], "entities": [{"text": "Error", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9644302129745483}, {"text": "TC-STAR translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8253332376480103}]}, {"text": " Table 23  Inflectional error rates of verbs, nouns, and adjectives in English outputs produced by four  WMT09 translation systems: the best ranked Google, the worst ranked Geneva, and two medium  ranked systems (Limsi (statistical) and rbmt3 (rule-based)).", "labels": [], "entities": [{"text": "WMT09 translation", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.6948277354240417}, {"text": "Geneva", "start_pos": 173, "end_pos": 179, "type": "DATASET", "confidence": 0.9184631705284119}]}]}