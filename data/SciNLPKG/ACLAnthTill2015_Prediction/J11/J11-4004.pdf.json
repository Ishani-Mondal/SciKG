{"title": [{"text": "What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation", "labels": [], "entities": [{"text": "Meta-Analytic Investigation", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7505304515361786}]}], "abstractContent": [{"text": "Recent discussions of annotator agreement have mostly centered around its calculation and interpretation , and the correct choice of indices.", "labels": [], "entities": [{"text": "calculation and interpretation", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.7671710054079691}]}, {"text": "Although these discussions are important, they only consider the \"back-end\" of the story, namely, what to do once the data are collected.", "labels": [], "entities": []}, {"text": "Just as important in our opinion is to know how agreement is reached in the first place and what factors influence coder agreement as part of the annotation processor setting, as this knowledge can provide concrete guidelines for the planning and setup of annotation projects.", "labels": [], "entities": []}, {"text": "To investigate whether there are factors that consistently impact annotator agreement we conducted a meta-analytic investigation of annotation studies reporting agreement percentages.", "labels": [], "entities": []}, {"text": "Our meta-analysis synthesized factors reported in 96 annotation studies from three domains (word-sense disambiguation, prosodic transcriptions, and phonetic transcriptions) and was based on a total of 346 agreement indices.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.714207261800766}]}, {"text": "Our analysis identified seven factors that influence reported agreement values: annotation domain, number of categories in a coding scheme, number of annotators in a project, whether annotators received training, the intensity of annotator training, the annotation purpose, and the method used for the calculation of percentage agreements.", "labels": [], "entities": []}, {"text": "Based on our results we develop practical recommendations for the assessment, interpretation, calculation, and reporting of coder agreement.", "labels": [], "entities": [{"text": "assessment, interpretation, calculation, and reporting of coder agreement", "start_pos": 66, "end_pos": 139, "type": "TASK", "confidence": 0.6951188011602922}]}, {"text": "We also briefly discuss theoretical implications for the concept of annotation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen a growing emphasis in the field of computational linguistics to provide empirical evidence that manual annotations satisfy at least minimal standards of data quality.", "labels": [], "entities": []}, {"text": "This trend is indicated by a rising number of publications on methodological and statistical issues of how to measure and interpret inter-annotator agreement as well as a rising number of reports on annotation quality and schema problems and potential reasons for lower agreement rates.", "labels": [], "entities": []}, {"text": "In the same regard, knowing which annotator characteristics are conducive to consistent and reliable work enables us to select annotators that meet these criteria.", "labels": [], "entities": []}, {"text": "Unfortunately, despite a wealth of annotation studies and practical experiences, we still lack a clear picture on what influences inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Although individual studies have found potential factors, it is unclear whether these findings are applicable only to the specific projector task in which they were observed, or whether they represent systematic influences on annotator agreement over a wide range of (or even all) situations.", "labels": [], "entities": []}, {"text": "The main question guiding our study in this article was therefore whether we are able to identify factors that impact coder agreement in systematic ways across multiple studies and multiple domains.", "labels": [], "entities": []}, {"text": "Trying to propose a general framework of factors influencing annotator agreement is challenging, as manual annotations are required in avast range of tasks from POStagging to prosodic transcriptions, word-sense disambiguation, the classification of text genres, or the identification of gestures or emotions.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 200, "end_pos": 225, "type": "TASK", "confidence": 0.7365101873874664}, {"text": "classification of text genres", "start_pos": 231, "end_pos": 260, "type": "TASK", "confidence": 0.8578111082315445}, {"text": "identification of gestures or emotions", "start_pos": 269, "end_pos": 307, "type": "TASK", "confidence": 0.8572195053100586}]}, {"text": "Moreover, each annotation project has its specific, idiosyncratic features.", "labels": [], "entities": []}, {"text": "If factors such as annotator expertise or scheme granularity are important influences in one specific project, how can we be sure that this is also the casein other projects or other areas?", "labels": [], "entities": []}, {"text": "The best approach to answer this question is to review and synthesize information from a wide variety of studies in the same field.", "labels": [], "entities": []}, {"text": "In this article we used a metaanalytic approach to synthesize existing information on inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Meta-analysis is a statistical method that combines and compares results of independent studies to obtain an overview of the respective research field.", "labels": [], "entities": []}, {"text": "It is further used to identify factors with a (statistically) significant impact on the outcomes of individual studies.", "labels": [], "entities": []}, {"text": "Our investigation followed two aims: (1) to test whether there are factors that consistently influence annotator agreement, while abstracting from the specifics of individual annotation projects and settings; and (2) to get a first indication of how far these influencing factors are generalizable across different annotation domains.", "labels": [], "entities": []}, {"text": "This second aspect is of considerable practical importance.", "labels": [], "entities": []}, {"text": "If influencing factors are comparable across domains, findings from one area can be applied to other areas.", "labels": [], "entities": []}, {"text": "If not, each domain has to consider factors that impact coder agreement individually.", "labels": [], "entities": []}, {"text": "This study contributes to the existing literature in that it approaches the question of inter-annotator agreement at the stage when decisions about procedures and settings can still influence the end results.", "labels": [], "entities": []}, {"text": "It thus adds to earlier discussions by emphasizing the importance of the earliest stages in the annotation process before and when annotations are made.", "labels": [], "entities": []}, {"text": "It does so by taking an objective, statistical approach to reanalyze and synthesize findings of existing empirical studies.", "labels": [], "entities": []}, {"text": "In the next section, we give a short introduction to the meta-analytic method and describe the specific procedure followed in the present study.", "labels": [], "entities": []}, {"text": "This is followed by the presentation of our findings and a discussion of the practical implications for annotation projects.", "labels": [], "entities": []}, {"text": "Based on our results, we formulate recommendations for the planning, execution, and reporting of annotations.", "labels": [], "entities": []}, {"text": "Further, we briefly outline the broader theoretical implications and open questions for the concept of manual annotation quality.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Number of included studies and indices in the three domains.", "labels": [], "entities": []}, {"text": " Table 3  Results of moderator analyses for count variables (based on the complete data set).", "labels": [], "entities": []}, {"text": " Table 4  Results of moderator analyses for categorical variables (based on the complete data set).", "labels": [], "entities": []}, {"text": " Table 5  Comparison of study characteristics in the three domains (percentage of studies, excluding  language).", "labels": [], "entities": []}]}