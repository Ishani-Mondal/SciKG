{"title": [{"text": "Annotating and Learning Event Durations in Text", "labels": [], "entities": [{"text": "Learning Event Durations", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6422814627488455}]}], "abstractContent": [{"text": "This article presents our work on constructing a corpus of news articles in which events are annotated for estimated bounds on their duration, and automatically learning from this corpus.", "labels": [], "entities": []}, {"text": "We describe the annotation guidelines, the event classes we categorized to reduce gross discrepancies in inter-annotator judgments, and our use of normal distributions to model vague and implicit temporal information and to measure inter-annotator agreement for these event duration distributions.", "labels": [], "entities": []}, {"text": "We then show that machine learning techniques applied to this data can produce coarse-grained event duration information automatically, considerably outperforming a baseline and approaching human performance.", "labels": [], "entities": []}, {"text": "The methods described here should be applicable to other kinds of vague but substantive information in texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Consider the sentence from a news article: George W. Bush met with Vladimir Putin in Moscow.", "labels": [], "entities": []}, {"text": "How long did the meeting last?", "labels": [], "entities": []}, {"text": "Our first inclination is to say we have no idea.", "labels": [], "entities": []}, {"text": "But in fact we do have some idea.", "labels": [], "entities": []}, {"text": "We know the meeting lasted more than ten seconds and less than one year.", "labels": [], "entities": []}, {"text": "As we guess narrower and narrower bounds, our chances of being correct go down, but if we are correct, the utility of the information goes up.", "labels": [], "entities": []}, {"text": "Just how accurately can we make duration judgments like this?", "labels": [], "entities": []}, {"text": "How much agreement can we expect among people?", "labels": [], "entities": []}, {"text": "Will it be possible to extract this kind of information from text automatically?", "labels": [], "entities": []}, {"text": "Sometimes we are explicitly told the duration of events, as in \"a five-day meeting\" and \"I have lived here for three years.\"", "labels": [], "entities": []}, {"text": "But more often, such phrases are missing, and present-day natural language applications simply have to proceed without them.", "labels": [], "entities": []}, {"text": "There has been a great deal of work on formalizing temporal information) and on temporal anchoring and event ordering in text;).", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7127989381551743}, {"text": "event ordering", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7044639736413956}]}, {"text": "The uncertainty of temporal durations has been recognized as one of the most significant issues for temporal reasoning.", "labels": [], "entities": [{"text": "temporal reasoning", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.6961268484592438}]}, {"text": "point out byway of example that we have to know how long a battery remains charged to decide when to replace it or to predict the effects of actions which refer to the battery charge as a precondition.", "labels": [], "entities": []}, {"text": "Yet to our knowledge, there has been no serious published empirical effort to model and learn the vague and implicit duration information in natural language, and to perform reasoning over this information.", "labels": [], "entities": []}, {"text": "Cyc has some fuzzy duration information, although it is not generally available; discusses the issue for less than a page; there has been work in fuzzy logic on representing and reasoning with imprecise durations (.", "labels": [], "entities": []}, {"text": "But none of these efforts make an attempt to collect human judgments on such durations or to extract them automatically from text.", "labels": [], "entities": []}, {"text": "Nevertheless, people have little trouble exploiting temporal information implicitly encoded in the descriptions of events, relying on their knowledge of the range of usual durations of types of events.", "labels": [], "entities": []}, {"text": "This hitherto largely unexploited information is part of our commonsense knowledge.", "labels": [], "entities": []}, {"text": "We can estimate roughly how long events of different types last and roughly how long situations of various sorts persist.", "labels": [], "entities": []}, {"text": "We know that government policies typically last somewhere between one and ten years, and weather conditions fairly reliably persist between three hours and one day.", "labels": [], "entities": []}, {"text": "We are often able to decide whether two events overlap or are in sequence by accessing this information.", "labels": [], "entities": []}, {"text": "We know that if a war started yesterday, we can be pretty sure it is still going on today.", "labels": [], "entities": []}, {"text": "If a hurricane started last year, we can be sure it is over by now.", "labels": [], "entities": []}, {"text": "This article describes an exploration into how this information can be captured automatically.", "labels": [], "entities": []}, {"text": "Our results can have a significant impact on computational linguistics applications like event anchoring and ordering in text (, event coreference (, question answering (, and other intelligent systems that would benefit from such temporal commonsense knowledge, for example, temporal reasoning (.", "labels": [], "entities": [{"text": "event anchoring and ordering in text", "start_pos": 89, "end_pos": 125, "type": "TASK", "confidence": 0.7573658873637518}, {"text": "event coreference", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.7288226783275604}, {"text": "question answering", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7962587773799896}]}, {"text": "Our goal is to be able to extract this implicit event duration information from text automatically, and to that end we first annotated the events in news articles with bounds on their durations.", "labels": [], "entities": []}, {"text": "The corpus that we have annotated currently contains all 48 non-Wall-Street-Journal (non-WSJ) news articles (2,132 event instances), as well as 10 WSJ articles (156 event instances), from the TimeBank corpus annotated in TimeML ().", "labels": [], "entities": [{"text": "TimeBank corpus annotated in TimeML", "start_pos": 192, "end_pos": 227, "type": "DATASET", "confidence": 0.9209290027618409}]}, {"text": "The non-WSJ articles (mainly political and disaster news) include both print and broadcast news that are from a variety of news sources, such as ABC, AP, CNN, and VOA.", "labels": [], "entities": [{"text": "VOA", "start_pos": 163, "end_pos": 166, "type": "DATASET", "confidence": 0.9540654420852661}]}, {"text": "All the annotated data have already been integrated into the TimeBank corpus.", "labels": [], "entities": [{"text": "TimeBank corpus", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.9789281189441681}]}, {"text": "This article is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe our annotation guidelines, including the annotation strategy and assumptions, and the representative event classes we have categorized to minimize discrepant judgments between annotators.", "labels": [], "entities": []}, {"text": "The method for measuring inter-annotator agreement when the judgments are intervals on a scale is described in Section 3.", "labels": [], "entities": []}, {"text": "We will discuss how to integrate our event duration annotations to TimeML in Section 4.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9181329607963562}]}, {"text": "In Section 5 we show that machine learning techniques applied to the annotated data considerably outperform a baseline and approach human performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to see how effective our guidelines are, we conducted experiments to compare the inter-annotator agreement before and after annotators read the guidelines.", "labels": [], "entities": []}, {"text": "The data for the evaluation was split into two sets.", "labels": [], "entities": []}, {"text": "The first set contained 13 articles (521 events, 1,563 annotated durations) which were all political and disaster news stories from ABC, APW, CNN, PRI, and VOA.", "labels": [], "entities": [{"text": "APW", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.7515171766281128}, {"text": "VOA", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.9590175747871399}]}, {"text": "The annotators annotated independently  before reading the guidelines.", "labels": [], "entities": []}, {"text": "The annotators were only given short instructions on what to annotate and one sample article with annotations.", "labels": [], "entities": []}, {"text": "The second set (test set) contained 5 articles (125 events, 375 annotated durations) that were also political and disaster news stories from the same news sources.", "labels": [], "entities": []}, {"text": "The annotators annotated independently after reading the guidelines.", "labels": [], "entities": []}, {"text": "The comparison is shown in.", "labels": [], "entities": []}, {"text": "Agreement is measured by the area of overlap in two distributions and is thus a number between 0 and 1.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9767395257949829}]}, {"text": "The graphs show the answer to the question \"If we set the threshold for agreement at x, counting everything above x as agreement, what is the percentage y of inter-annotator agreement?\"", "labels": [], "entities": []}, {"text": "The horizontal axis represents the overlap thresholds, and the vertical axis represents the agreement percentage, that is, the percentage of annotated durations that agree forgiven overlap thresholds.", "labels": [], "entities": [{"text": "agreement percentage", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.9485072791576385}]}, {"text": "There are three lines in the graph.", "labels": [], "entities": []}, {"text": "The top one (with circles) represents the after-guidelines agreement; the middle one (with triangles) represents the beforeguidelines agreement; and the lowest one (with squares) represents the expected or baseline agreement.", "labels": [], "entities": []}, {"text": "This graph shows that, for example, if we define agreement to be a 10% overlap or better (an overlap threshold of 0.1), we can get 0.8 agreement after reading the guidelines, 0.72 agreement before reading the guidelines, and 0.36 expected agreement with only the knowledge of the global distribution.", "labels": [], "entities": []}, {"text": "From this graph, we can see that our guidelines are indeed effective in improving the inter-annotator agreement.", "labels": [], "entities": []}, {"text": "shows more detailed experimental results.", "labels": [], "entities": []}, {"text": "For each overlap threshold, it shows the expected or baseline agreement, the before-guidelines agreement, and the after-guidelines agreement, as well as the kappa statistic computed from the after- guidelines agreement (P(A)) and the expected or baseline agreement (P(E)).", "labels": [], "entities": []}, {"text": "The agreement actually gets marginally worse when the agreement criteria is very stringent (i.e., overlap \u2265 0.9), which indicates there really is no consensus at that level of agreement.", "labels": [], "entities": [{"text": "overlap", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9668563604354858}]}, {"text": "The overall agreement is relatively low.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9859992861747742}]}, {"text": "Thus in this article, we mainly focus on learning coarse-grained event durations with much higher inter-annotator agreement.", "labels": [], "entities": []}, {"text": "See Sections 5.2 and 5.3 for more details.", "labels": [], "entities": []}, {"text": "After the two corpora are integrated, it would be useful to evaluate how consistent the temporal relationship annotations are originally in TimeBank and in the newly integrated event duration annotations.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 140, "end_pos": 148, "type": "DATASET", "confidence": 0.9510613083839417}]}, {"text": "Because not all the events in TimeBank are anchored exactly on a timeline, for this consistency evaluation we have only evaluated the \"includes\" / \" is included\" TLINK relationship: If event A includes event B, the duration of event A should be no shorter than the duration of event B.", "labels": [], "entities": [{"text": "TLINK", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.992381751537323}]}, {"text": "Because the event duration annotation is a range, there are three possible relationships between the two duration annotations for event A [a  We call the case (i) strictly compatible, (ii) softly compatible, and (iii) incompatible.", "labels": [], "entities": []}, {"text": "For this consistency evaluation, one article was randomly picked from each news source, and for each \"include\" TLINK relationship in the article, one of the three compatibility labels is assigned based on their definitions.", "labels": [], "entities": []}, {"text": "The result shows that out of a total of 116 \"include\" relationships, 59.5% are strictly compatible, 19.8% are softly compatible, and 20.7% are incompatible.", "labels": [], "entities": []}, {"text": "We can merge the first two categories as compatible, which accounts for 79.3%.", "labels": [], "entities": []}, {"text": "Most of the incompatible cases are due to different event interpretations and guidelines for the two corpora.", "labels": [], "entities": []}, {"text": "For example, TimeBank bounds most of the I-State events to the article time, whereas our annotations usually give them a much longer durationfor example, the event \"appear\" in \"Everyone appears to believe that somehow Cuba is going to change,\" and the event \"hope\" in \"The quarantine hopes to staunch the flow of Iraqi oil.\"", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9201474785804749}]}, {"text": "Aspectual events are another class of events that cause many incompatible cases, including those where multiple interpretations are possible, for example, in \"This is quite an extraordinary story unfolding here,\" the event \"unfolding\" can be interpreted as either the start of the unfolding (TimeBank), or the entire process of the unfolding (duration annotation); Sometimes even when both corpora agree on the interpretation of the event, they may not agree on its duration, for example, in \"But with the taskforce investigation just getting underway, officials have been careful not to draw any firm conclusions,\" it is clear that the \"getting\" event is the start of the investigation, but should it last momentarily (TimeBank) or fora couple of weeks (duration annotation)?", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 292, "end_pos": 300, "type": "DATASET", "confidence": 0.717752993106842}, {"text": "TimeBank", "start_pos": 720, "end_pos": 728, "type": "DATASET", "confidence": 0.7418027520179749}, {"text": "duration", "start_pos": 755, "end_pos": 763, "type": "METRIC", "confidence": 0.9635697603225708}]}, {"text": "Despite the difficulty with this event class, there exists some clear cases, for example, in \"A new Essex County task force began delving Thursday into the slayings of 14 black women over the last five years in the Newark area,\" it is correct to bound \"began\" to Thursday, whereas the event \"delving\" should last much longer.", "labels": [], "entities": []}, {"text": "Three supervised learning algorithms were evaluated for our binary classification task, namely, Support Vector Machines (SVM)), Naive Bayes (NB) (, and Decision Trees (C4.5) (Quinlan 1993).", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.7937748233477274}]}, {"text": "The Weka (Witten and Frank 2005) machine learning package was used for the implementation of these learning algorithms.", "labels": [], "entities": [{"text": "Weka (Witten and Frank 2005) machine learning package", "start_pos": 4, "end_pos": 57, "type": "DATASET", "confidence": 0.7863997757434845}]}, {"text": "Linear kernel is used for SVM in our experiments.", "labels": [], "entities": [{"text": "SVM", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9106264710426331}]}, {"text": "Each event instance has a total of 18 feature values, as described in Section 5.1, for the event only condition, and 30 feature values for the local context condition, when n = 2.", "labels": [], "entities": []}, {"text": "For SVM and C4.5, all features are converted into binary features (6,665 and 12,502 features).", "labels": [], "entities": []}, {"text": "Ten-fold cross validation was used to train the learning models, which were then tested on the unseen held-out test set, and the performance (including the precision, recall, and F-score 4 for each class) of the three learning algorithms is shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9996547698974609}, {"text": "recall", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9977508187294006}, {"text": "F-score 4", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9872402250766754}]}, {"text": "The significant measure is overall precision, and this is shown for the three algorithms in, together with human agreement (the upper bound of the learning task) and the baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9963765740394592}]}, {"text": "We can see that among the three learning algorithms, SVM achieves the best F-score for each class and also the best overall precision (76.6%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9988967180252075}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9973843693733215}]}, {"text": "Compared with the baseline (59.0%) and human agreement (87.7%), this level of performance is very encouraging, especially as the learning is from such limited training data.", "labels": [], "entities": []}, {"text": "The best performing learning algorithm, SVM, was then used to examine the utility of combinations of four different feature sets (i.e., event, local context, syntactic, and WordNet hypernym features).", "labels": [], "entities": []}, {"text": "The detailed comparison is shown in.", "labels": [], "entities": []}, {"text": "We can see that most of the performance comes from event word or phrase itself.", "labels": [], "entities": []}, {"text": "A significant improvement above that is due to the addition of information about the subject and object.", "labels": [], "entities": []}, {"text": "Local context does not help and in fact may hurt, and hypernym information also does not seem to help.", "labels": [], "entities": []}, {"text": "It is of interest that the most important information is that from the predicate and arguments describing the event, as our linguistic intuitions would lead us to expect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Inter-annotator agreement with different overlap thresholds.", "labels": [], "entities": []}, {"text": " Table 5  Inter-annotator agreement for binary event durations.", "labels": [], "entities": []}, {"text": " Table 6  Test performance of three algorithms.", "labels": [], "entities": []}, {"text": " Table 7  Feature evaluation with different feature sets using SVM.", "labels": [], "entities": []}, {"text": " Table 8  Learning performance for each event class.", "labels": [], "entities": []}, {"text": " Table 9  Test performance on WSJ data.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9165189862251282}]}]}