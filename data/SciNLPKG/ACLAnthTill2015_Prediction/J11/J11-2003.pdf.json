{"title": [{"text": "Learning to Rank Answers to Non-Factoid Questions from Web Collections", "labels": [], "entities": [{"text": "Learning to Rank Answers to Non-Factoid Questions", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7332701895918164}]}], "abstractContent": [{"text": "This work investigates the use of linguistically motivated features to improve search, in particular for ranking answers to non-factoid questions.", "labels": [], "entities": []}, {"text": "We show that it is possible to exploit existing large collections of question-answer pairs (from online social Question Answering sites) to extract such features and train ranking models which combine them effectively.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.6999784260988235}]}, {"text": "We investigate a wide range of feature types, some exploiting natural language processing such as coarse word sense disambiguation, named-entity identification, syntactic parsing, and semantic role labeling.", "labels": [], "entities": [{"text": "coarse word sense disambiguation", "start_pos": 98, "end_pos": 130, "type": "TASK", "confidence": 0.603096142411232}, {"text": "named-entity identification", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.7191465944051743}, {"text": "syntactic parsing", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7276295572519302}, {"text": "semantic role labeling", "start_pos": 184, "end_pos": 206, "type": "TASK", "confidence": 0.6463422874609629}]}, {"text": "Our experiments demonstrate that linguistic features, in combination, yield considerable improvements inaccuracy.", "labels": [], "entities": []}, {"text": "Depending on the system settings we measure relative improvements of 14% to 21% in Mean Reciprocal Rank and Precision@1, providing one of the most compelling evidence to date that complex linguistic features such as word senses and semantic roles can have a significant impact on large-scale information retrieval tasks.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank", "start_pos": 83, "end_pos": 103, "type": "METRIC", "confidence": 0.9322680830955505}, {"text": "Precision@1", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9454152782758077}, {"text": "information retrieval tasks", "start_pos": 292, "end_pos": 319, "type": "TASK", "confidence": 0.7847018341223398}]}], "introductionContent": [{"text": "The problem of Question Answering (QA) has received considerable attention in the past few years.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8785957634449005}]}, {"text": "Nevertheless, most of the work has focused on the task of factoid QA, where questions match short answers, usually in the form of named or numerical entities.", "labels": [], "entities": [{"text": "factoid QA", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.7962726056575775}]}, {"text": "Thanks to international evaluations organized by conferences such as the Text REtrieval Conference (TREC) and the Cross Language Evaluation Forum (CLEF) Workshop, annotated corpora of questions and answers have become available for several languages, which has facilitated the development of robust machine learning models for the task.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.804135337471962}]}, {"text": "The situation is different once one moves beyond the task of factoid QA.", "labels": [], "entities": [{"text": "factoid QA", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.765772134065628}]}, {"text": "Comparatively little research has focused on QA models for non-factoid questions such as causation, manner, or reason questions.", "labels": [], "entities": []}, {"text": "Because virtually no training data is available for this problem, most automated systems train either on small hand-annotated corpora built in-house ( or on question-answer pairs harvested from Frequently Asked Questions (FAQ) lists or similar resources).", "labels": [], "entities": []}, {"text": "None of these situations is ideal: The cost of building the training corpus in the former setup is high; in the latter scenario the data tend to be domain-specific, hence unsuitable for the learning of opendomain models, and for drawing general conclusions about the underlying scientific problems.", "labels": [], "entities": []}, {"text": "On the other hand, recent years have seen an explosion of user-generated content (or social media).", "labels": [], "entities": []}, {"text": "Of particular interest in our context are community-driven questionanswering sites, such as Yahoo!", "labels": [], "entities": []}, {"text": "Answers, where users answer questions posed by other users and best answers are selected manually either by the asker or by all the participants in the thread.", "labels": [], "entities": []}, {"text": "The data generated by these sites have significant advantages over other Web resources: (a) they have a high growth rate and they are already abundant; (b) they cover a large number of topics, hence they offer a better approximation of opendomain content; and (c) they are available for many languages.", "labels": [], "entities": []}, {"text": "Community QA sites, similar to FAQs, provide a large number of question-answer pairs.", "labels": [], "entities": [{"text": "FAQs", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.6448806524276733}]}, {"text": "Nevertheless, these data have a significant drawback: they have high variance of quality (i.e., questions and answers range from very informative to completely irrelevant or even abusive).", "labels": [], "entities": []}, {"text": "shows some examples of both high and low quality content from the Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.8799684345722198}]}, {"text": "In this article we investigate two important aspects of non-factoid QA:", "labels": [], "entities": [{"text": "non-factoid QA", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.4874916076660156}]}], "datasetContent": [{"text": "We used several measures to evaluate our models.", "labels": [], "entities": []}, {"text": "Recall that we are using an initial retrieval engine to select a pool of N answer candidates), which are then reranked.", "labels": [], "entities": []}, {"text": "This couples the performance of the initial retrieval engine and the re-rankers.", "labels": [], "entities": []}, {"text": "We tried to de-couple them in our performance measures, as follows.", "labels": [], "entities": []}, {"text": "We note that if the initial retrieval engine does not rank the correct answer in the pool of top N results, it is impossible for any re-ranker to do well.", "labels": [], "entities": []}, {"text": "We therefore follow the approach of and define performance measures only with respect to the subset of pools which contain the correct answer fora given N.", "labels": [], "entities": []}, {"text": "This complicates slightly the typical notions of recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9980955719947815}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9959765076637268}]}, {"text": "Let us call Q the set of all queries in the collection and Q N the subset of queries for which the retrieved answer pool of size N contains the correct answer.", "labels": [], "entities": []}, {"text": "We will then use the following performance measure definitions: Retrieval Recall@N: The usual recall definition: |Q| . This is equal for all re-rankers.", "labels": [], "entities": [{"text": "Retrieval Recall", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.8501871228218079}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9909195899963379}]}, {"text": "Re-ranking Precision@1: Average Precision@1 over the Q N set, where the Precision@1 of a query is defined as 1 if the correct answer is re-ranked into the first position, 0 otherwise.", "labels": [], "entities": [{"text": "Average Precision@1", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.9280522912740707}]}, {"text": "Re-ranking MRR: MRR over the Q N set, where the reciprocal rank is the inverse of the rank of the correct answer.", "labels": [], "entities": [{"text": "MRR", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.7927186489105225}]}, {"text": "Note that as N gets larger, Q N grows in size, increasing the Retrieval Recall@N but also increasing the difficulty of the task for the re-ranker, and therefore decreasing Reranking Precision@1 and Re-ranking MRR.", "labels": [], "entities": [{"text": "Retrieval Recall@N", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.8470704257488251}, {"text": "Reranking Precision@1", "start_pos": 172, "end_pos": 193, "type": "METRIC", "confidence": 0.7477582544088364}, {"text": "MRR", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.4537942409515381}]}, {"text": "During training of the FMIX re-ranker, the presentation of the training instances is randomized, which defines a randomized training protocol producing different models with each permutation of the data.", "labels": [], "entities": [{"text": "FMIX re-ranker", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9144533276557922}]}, {"text": "We exploit this property to estimate the variance on the experimental results by reporting the average performance of 10 different models, together with an estimate of the standard deviation.", "labels": [], "entities": []}, {"text": "The initial retrieval engine used to select the pool of candidate answers is the BM25 score as described earlier.", "labels": [], "entities": [{"text": "BM25 score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9136267602443695}]}, {"text": "This is also our baseline re-ranker.", "labels": [], "entities": []}, {"text": "We will compare this to the FMIX re-ranker using all features or using subsets of features.", "labels": [], "entities": [{"text": "FMIX re-ranker", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8882976174354553}]}, {"text": "show the results obtained using FMIX and the baseline for increasing values of N.", "labels": [], "entities": [{"text": "FMIX", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8949763774871826}]}, {"text": "We report results for Perceptron and SVM-rank using the optimal feature set for each (we discuss feature selection in the next sub-section).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Re-ranking evaluation for Perceptron and SVM-rank. Improvement indicates relative  improvement over the baseline.", "labels": [], "entities": [{"text": "Improvement", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9849423170089722}]}, {"text": " Table 4  Summary of the model selection process using Perceptron.", "labels": [], "entities": []}, {"text": " Table 5  Summary of the model selection process using SVM-rank.", "labels": [], "entities": [{"text": "SVM-rank", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.8390661478042603}]}, {"text": " Table 7  Percentage of questions in the test set that are improved/worsened/unchanged after re-ranking.  This experiment used the Perceptron model.", "labels": [], "entities": []}, {"text": " Table 8  Examples of questions improved by our re-ranking model. URLs were replaced with <URL> in  answer texts. Some non-relevant text was replaced with <...> to save space. The remaining text  maintains the original capitalization and spelling. Non-stop question terms are emphasized in  the answers.", "labels": [], "entities": []}, {"text": " Table 10  Examples of questions in each error class. The corresponding error class is listed on the left side  of the question text. We list the answer ranked at the top position by FMIX only where relevant  (e.g., the ALSO GOOD category). URLs were replaced with <URL> in answer texts. Some  non-relevant text was replaced with <...> to save space. The remaining text maintains the  original capitalization and spelling.", "labels": [], "entities": [{"text": "FMIX", "start_pos": 183, "end_pos": 187, "type": "DATASET", "confidence": 0.9195147752761841}, {"text": "ALSO GOOD", "start_pos": 220, "end_pos": 229, "type": "METRIC", "confidence": 0.5973989069461823}]}]}