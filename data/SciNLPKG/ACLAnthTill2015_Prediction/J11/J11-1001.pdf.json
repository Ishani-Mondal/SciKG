{"title": [{"text": "Squibs Nouveau-ROUGE: A Novelty Metric for Update Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.7588016390800476}]}], "abstractContent": [{"text": "An update summary should provide a fluent summarization of new information on a time-evolving topic, assuming that the reader has already reviewed older documents or summaries.", "labels": [], "entities": [{"text": "summarization of new information on a time-evolving topic", "start_pos": 42, "end_pos": 99, "type": "TASK", "confidence": 0.8035026863217354}]}, {"text": "In 2007 and 2008, an annual summarization evaluation included an update summarization task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9811733365058899}]}, {"text": "Several participating systems produced update summaries indistinguishable from human-generated summaries when measured using ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.8344150185585022}]}, {"text": "However, no machine system performed near human-level performance in manual evaluations such as pyramid and overall responsiveness scoring.", "labels": [], "entities": [{"text": "pyramid", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.9226464033126831}]}, {"text": "We present a metric called Nouveau-ROUGE that improves correlation with manual evaluation metrics and can be used to predict both the pyramid score and overall responsiveness for update summaries.", "labels": [], "entities": []}, {"text": "Nouveau-ROUGE can serve as a less expensive surrogate for manual evaluations when comparing existing systems and when developing new ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Update summaries focus on what is new relative to a previous body of information.", "labels": [], "entities": []}, {"text": "They pose new challenges both to algorithm developers and to evaluation of summaries.", "labels": [], "entities": [{"text": "evaluation of summaries", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.700916608174642}]}, {"text": "In 2007, DUC (Document Understanding Conference) introduced an update summarization task, repeated in 2008 for TAC (Text Analysis Conference).", "labels": [], "entities": [{"text": "DUC (Document Understanding Conference)", "start_pos": 9, "end_pos": 48, "type": "TASK", "confidence": 0.6862042546272278}, {"text": "TAC (Text Analysis Conference)", "start_pos": 111, "end_pos": 141, "type": "TASK", "confidence": 0.6012587000926336}]}, {"text": "This task consisted of producing a multi-document summary fora set of articles on a single topic, followed by one (2008) or two (2007) multi-document summaries for sets of articles on the same topic published at later dates.", "labels": [], "entities": []}, {"text": "The goal was to generate a good first summary, along with update(s) that contained new content and minimized redundancy.", "labels": [], "entities": []}, {"text": "The modifier manual is used to identify evaluations, and the corresponding scores, produced by humans.", "labels": [], "entities": []}, {"text": "The modifier automatic is used to identify evaluations, and the corresponding scores, produced by machines.", "labels": [], "entities": []}, {"text": "Similarly, human-generated and machine-generated will be used to distinguish between summaries created by humans and those generated by machine systems, respectively.", "labels": [], "entities": []}, {"text": "Because we are working with update summarization, there is a minimum of two summaries fora set of documents.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.6003178954124451}]}, {"text": "The first summary for the document set is called the original (Task A) summary and a later summary is called an update (Task B) summary.", "labels": [], "entities": []}, {"text": "Several machine summarizing systems produced update summaries that were statistically indistinguishable from human-generated summaries, as measured by the ROUGE metrics, the standard metrics for automatic evaluation of summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9309689998626709}]}, {"text": "However, none of these machine systems performed near human levels in overall responsiveness or pyramid evaluation, the currently used manual evaluation metrics.", "labels": [], "entities": []}, {"text": "We define the metric gap (or gap) as the distance between a prediction of a manual score, based on automatic scores, and the observed manual score.", "labels": [], "entities": [{"text": "metric gap (or gap)", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.8930333058039347}]}, {"text": "The purpose of our work is to investigate and mitigate this metric gap by introducing an automatic evaluation that is a better predictor of manual evaluation.", "labels": [], "entities": []}, {"text": "Reducing the metric gap is important for two reasons.", "labels": [], "entities": []}, {"text": "First, the gap severely limits the usefulness of automatic evaluation and forces the use of much more expensive manual evaluation for comparing existing systems.", "labels": [], "entities": [{"text": "automatic evaluation", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.6323665082454681}]}, {"text": "More importantly, however, this gap is a severe handicap to research on new update summarization methods because it makes it difficult to evaluate new ideas and compare them with existing methods.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.6342005133628845}]}, {"text": "In Section 2, we analyze the results of the TAC 2008 summarization task, demonstrating the large gap between ROUGE automatic metrics and manual evaluation of update summaries.", "labels": [], "entities": [{"text": "TAC 2008 summarization task", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6330912932753563}, {"text": "ROUGE", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.9360230565071106}]}, {"text": "In Section 3, we modify ROUGE to produce scores that correlate significantly better with manual evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9917200803756714}]}, {"text": "We evaluate our new metric on TAC 2008 data in Section 4, demonstrating its superiority as a predictor of manual evaluations.", "labels": [], "entities": [{"text": "TAC 2008 data", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9686535795529684}]}], "datasetContent": [{"text": "TAC 2008 presented 48 20-document sets, with 10 documents in each of two subsets, A and B.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9594697654247284}]}, {"text": "Subset B documents were more recent.", "labels": [], "entities": []}, {"text": "Original summaries were generated for the A subsets and update summaries were then produced for the B subsets.", "labels": [], "entities": []}, {"text": "In TAC 2008, ROUGE was used for automatic evaluation.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8190434575080872}, {"text": "ROUGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9929924011230469}]}, {"text": "ROUGE (Lin and Hovy 2000) compares any summary to any other (typically human-generated) summary using a recall-oriented approach.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9386247992515564}, {"text": "recall-oriented", "start_pos": 104, "end_pos": 119, "type": "METRIC", "confidence": 0.9360132813453674}]}, {"text": "ROUGE-1 and -2 are based on unigrams and bigrams, respectively; ROUGE-SU4 uses bigrams with a maximum skip distance of 4 between bigrams; ROUGE-BE (Hovy, Lin, and Zhou 2005) is an n-gram approach based on basic elements, computed via parsing or automatic entity recognition.", "labels": [], "entities": []}, {"text": "ROUGE-2, ROUGE-SU4, and ROUGE-BE were the official automatic metrics for TAC 2008, used to compare machine-generated summaries to human-generated summaries, and to compare human-generated summaries to each other using a jackknife approach.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9000074863433838}, {"text": "ROUGE-SU4", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.8593870997428894}, {"text": "ROUGE-BE", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9683729410171509}, {"text": "TAC 2008", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.6814415156841278}]}, {"text": "In addition to the three official metrics, we include ROUGE-1 in our study as it is often competitive with the official metrics.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9969668984413147}]}, {"text": "Three manual evaluation metrics were used in TAC 2008: pyramid, overall responsiveness, and linguistic quality (not considered in our work).", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.734923392534256}, {"text": "pyramid", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9652916193008423}]}, {"text": "The pyramid method) is a content-based metric for which human annotators mark content units in the human-generated summaries.", "labels": [], "entities": []}, {"text": "The content units are collected across a set of human-generated summaries fora topic, and a weight is computed based on how many human-generated summaries include this content unit.", "labels": [], "entities": []}, {"text": "TAC 2008 also used a manual overall responsiveness score.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9574756324291229}, {"text": "overall responsiveness score", "start_pos": 28, "end_pos": 56, "type": "METRIC", "confidence": 0.7466093202431997}]}, {"text": "After evaluating data from, NIST decided that this score, which evaluates summary usefulness including linguistic quality, is a reliable and stable manual evaluation.", "labels": [], "entities": [{"text": "NIST", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.946937620639801}]}, {"text": "We analyzed the three official TAC 2008 automatic evaluation scores to see how well they predict the manual evaluation metrics of overall responsiveness and pyramid score.", "labels": [], "entities": [{"text": "TAC 2008 automatic evaluation scores", "start_pos": 31, "end_pos": 67, "type": "DATASET", "confidence": 0.8459590554237366}, {"text": "responsiveness", "start_pos": 138, "end_pos": 152, "type": "METRIC", "confidence": 0.8010365962982178}, {"text": "pyramid score", "start_pos": 157, "end_pos": 170, "type": "METRIC", "confidence": 0.9765140116214752}]}, {"text": "shows scatter plots of responsiveness and pyramid scores vs. the three official ROUGE measures for the TAC 2008 update task.", "labels": [], "entities": [{"text": "pyramid scores", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.9548417925834656}, {"text": "ROUGE", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9900330901145935}, {"text": "TAC 2008 update task", "start_pos": 103, "end_pos": 123, "type": "DATASET", "confidence": 0.7749671787023544}]}, {"text": "Each solid data point represents the average score fora human summarizer over 24 document sets, and a dashed line marks the minimum; each open data point represents the average score fora machine system.", "labels": [], "entities": []}, {"text": "We also show robust linear least squares fits to the data as well as the Pearson correlation coefficients between ROUGE-BE, -2, and -SU4 and the manual-evaluation scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 73, "end_pos": 92, "type": "METRIC", "confidence": 0.9514584541320801}, {"text": "ROUGE-BE", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9932370781898499}]}, {"text": "Surprisingly, these correlations are higher for the update task than for the original summarization task (data not shown); nevertheless, the gap between the lines' predictions and the scores for the human-generated summaries is larger.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9014392197132111}]}, {"text": "We report correlation coefficients only for the machine-generated summaries.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9558005332946777}]}, {"text": "Scores for the human-generated summaries are distributed differently, and correlation for the set of human-generated summaries is often not significant due to the small number of human summarizers.", "labels": [], "entities": [{"text": "correlation", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.982953667640686}]}, {"text": "The correlation coefficients in show that the automatic metrics do well in predicting responsiveness and pyramid scoring for machine-generated summaries.", "labels": [], "entities": [{"text": "pyramid scoring", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.6850692629814148}]}, {"text": "In contrast, the scores for human-generated summaries far exceed the predictions, with a large gap between predicted and actual scores.", "labels": [], "entities": []}, {"text": "As maybe expected, ROUGE is more highly correlated with the pyramid evaluation, which is a pure content evaluation score, whereas the responsiveness score also reflects linguistic quality.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9975367784500122}]}, {"text": "We more formally define the metric gap to be the absolute value of the difference between a manual evaluation score and our prediction of it based only on automatic evaluation scores.", "labels": [], "entities": []}, {"text": "A number of TAC 2008 machine systems performed within statistical confidence of human performance in the automatic evaluation metrics, but no system performed near human performance in the manual evaluations.", "labels": [], "entities": [{"text": "TAC 2008 machine", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8257546424865723}]}, {"text": "This has also been observed in previous summarization evaluations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9878441095352173}]}, {"text": "Progress has been made in closing this metric gap but it persists, especially for update summaries.", "labels": [], "entities": []}, {"text": "A good update summary must contain essential information but focus on new information.", "labels": [], "entities": []}, {"text": "When a machine-generated update summary is good, it is similar to the human-generated update summaries.", "labels": [], "entities": []}, {"text": "This is assessed quite well by a ROUGE score.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9981464147567749}]}, {"text": "But the machine-generated update summary should also be different from the human-generated original summaries, and we need an automatic metric to assess this difference, or lack of redundancy.", "labels": [], "entities": []}, {"text": "We suggest using a ROUGE score to measure similarity, and thus redundancy, between a given original summary and an update summary: A high ROUGE score indicates high redundancy.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.959179162979126}, {"text": "similarity", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9797236919403076}, {"text": "ROUGE", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9777966141700745}]}, {"text": "To  Given this evidence, we propose predicting manual scores for update summaries by using two ROUGE scores, R (AB) i and R (BB) i (i = 1, 2, SU4, ...), in a three-parameter model called Nouveau-ROUGE: We determine the \u03b1 parameters) using robust linear regression on the TAC 2008 evaluation data so that the Nouveau-ROUGE score Ni best predicts the manual scores of responsiveness and pyramid performance.", "labels": [], "entities": [{"text": "R (BB) i", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.7324846982955933}, {"text": "TAC 2008 evaluation data", "start_pos": 271, "end_pos": 295, "type": "DATASET", "confidence": 0.9330525100231171}]}, {"text": "Nouveau-ROUGE could be used by researchers to predict how anew system would compare with the TAC 2008 systems in overall responsiveness and pyramid scoring, a comparison that up to now has been impossible.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.8290104568004608}]}, {"text": "Correlation scores between automatic and manual scores have traditionally been used as a measure of the effectiveness of automatic evaluation as a surrogate for manual evaluation.", "labels": [], "entities": []}, {"text": "Pearson correlation coefficients, shown in, were computed for the scores for the held-back subset of summaries.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8668249547481537}]}, {"text": "Correlation is indeed higher for the Nouveau-ROUGE scores than for any of the ROUGE scores.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9880896210670471}, {"text": "ROUGE", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.8391768336296082}]}, {"text": "shows that ROUGE-BE and ROUGE-1 predictions of both responsiveness and pyramid scores are inferior to the Nouveau-ROUGE-BE predictions.", "labels": [], "entities": [{"text": "ROUGE-BE", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9693892598152161}, {"text": "ROUGE-1", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9783170819282532}]}, {"text": "Plots for N 2 and N SU4 are omitted due to space restrictions, but performance improvement relative to ROUGE is greater than that for N BE and less than that for N 1 .  To show that our results are not due to a lucky partitioning of the data, we used bootstrapping (Efron and Tibshirani 1993), a resampling method, which allows us to compute our statistical confidence in the results.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9879150390625}, {"text": "BE", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9408791065216064}]}, {"text": "This model assumes that observed data (scores for the 58 systems) characterize all data.", "labels": [], "entities": []}, {"text": "Given this model, the proper sampling method is to choose subsets with replacement.", "labels": [], "entities": []}, {"text": "We chose 58 systems (with replacement) and used half to determine the Nouveau-ROUGE parameters and half to test the model.", "labels": [], "entities": []}, {"text": "We repeated this process 1,000 times.", "labels": [], "entities": []}, {"text": "gives the correlation coefficients (for the tested-half of the data) for all four ROUGE metrics with each of the manual evaluations when comparing the machine-generated summaries with the human-generated summaries.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9646931290626526}, {"text": "ROUGE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9209875464439392}]}, {"text": "Data in the columns labeled \"p-value\" result from a Mann-Whitney Utest for equal medians of R (BB) i and Ni of the distributions of correlations returned by the bootstrapping procedure.", "labels": [], "entities": []}, {"text": "Because all p-values are small, we can conclude that the differences between the R (BB) i and Ni correlation scores are statistically significant for all variants of ROUGE.", "labels": [], "entities": [{"text": "R (BB) i and Ni correlation scores", "start_pos": 81, "end_pos": 115, "type": "METRIC", "confidence": 0.8036712209383646}]}, {"text": "gives the median gap on the TAC 2008 data for predicting responsiveness and pyramid scores.", "labels": [], "entities": [{"text": "TAC 2008 data", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9129726688067118}]}, {"text": "Recall that the gap is the absolute value of the difference between the manual score and the prediction of it.", "labels": [], "entities": []}, {"text": "The median gap is always smaller for Nouveau-ROUGE than for ROUGE; in fact, the gap is smaller on every trial.", "labels": [], "entities": [{"text": "median gap", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9211645126342773}, {"text": "ROUGE", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.8574375510215759}]}, {"text": "We used the Wilcox sign test to test the significance of this observation.", "labels": [], "entities": [{"text": "Wilcox sign test", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.8238874475161234}]}, {"text": "The null hypothesis is that the differences in the gaps between ROUGE and Nouveau-ROUGE has median 0.", "labels": [], "entities": []}, {"text": "The p-values from the Wilcox test indicate that the null hypothesis is true with probability 128 \u2248 7.812 \u00d7 10 \u22123 , so it can be safely rejected.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  TAC 2008: Nouveau-ROUGE \u03b1-parameters.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.7315236330032349}]}, {"text": " Table 3  Correlation scores for TAC 2008 human evaluations.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9737334847450256}, {"text": "TAC 2008 human evaluations", "start_pos": 33, "end_pos": 59, "type": "DATASET", "confidence": 0.7212522178888321}]}, {"text": " Table 4  TAC 2008: Median Pearson correlation coefficients for automatic vs. manual evaluations.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.6609645038843155}, {"text": "Pearson correlation coefficients", "start_pos": 27, "end_pos": 59, "type": "METRIC", "confidence": 0.8720898826917013}]}, {"text": " Table 5  Narrowing the TAC 2008 metric gap.", "labels": [], "entities": [{"text": "TAC 2008 metric gap", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.8732023388147354}]}]}