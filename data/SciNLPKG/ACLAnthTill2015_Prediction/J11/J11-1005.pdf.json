{"title": [{"text": "Syntactic Processing Using the Generalized Perceptron and Beam Search", "labels": [], "entities": [{"text": "Syntactic Processing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9539949893951416}]}], "abstractContent": [{"text": "We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model, trained by the generalized perceptron together with a generic beam-search decoder.", "labels": [], "entities": []}, {"text": "We apply the framework to word segmentation, joint segmentation and POS-tagging, dependency parsing, and phrase-structure parsing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7726320922374725}, {"text": "dependency parsing", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8169764280319214}, {"text": "phrase-structure parsing", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.7541694045066833}]}, {"text": "Both components of the framework are conceptually and computationally very simple.", "labels": [], "entities": []}, {"text": "The beam-search decoder only requires the syntactic processing task to be broken into a sequence of decisions, such that, at each stage in the process, the decoder is able to consider the top-n candidates and generate all possibilities for the next stage.", "labels": [], "entities": []}, {"text": "Once the decoder has been defined, it is applied to the training data, using trivial updates according to the generalized perceptron to induce a model.", "labels": [], "entities": []}, {"text": "This simple framework performs surprisingly well, giving accuracy results competitive with the state-of-the-art on all the tasks we consider.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9992451667785645}]}, {"text": "The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives, including log-linear and large-margin training algorithms and dynamic-programming for decoding.", "labels": [], "entities": []}, {"text": "Moreover, the framework offers the freedom to define arbitrary features which can make alternative training and decoding algorithms prohibitively slow.", "labels": [], "entities": []}, {"text": "We discuss how the general framework is applied to each of the problems studied in this article, making comparisons with alternative learning and decoding algorithms.", "labels": [], "entities": []}, {"text": "We also show how the comparability of candidates considered by the beam is an important factor in the performance.", "labels": [], "entities": []}, {"text": "We argue that the conceptual and computational simplicity of the framework, together with its language-independent nature, make it a competitive choice fora range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this article we study a range of syntactic processing tasks using a general framework for structural prediction that consists of the generalized perceptron) and beam-search.", "labels": [], "entities": [{"text": "structural prediction", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7222714424133301}]}, {"text": "We show that the framework, which is conceptually and computationally simple, is practically effective for structural prediction problems that can be turned into an incremental process, allowing accuracies competitive with the state-of-the-art to be achieved for all the problems we consider.", "labels": [], "entities": [{"text": "structural prediction", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.7751617133617401}]}, {"text": "The framework is extremely flexible and easily adapted to each task.", "labels": [], "entities": []}, {"text": "One advantage of beam-search is that it does not impose any requirements on the structure of the problem, for example, the optimal sub-problem property required for dynamicprogramming, and can easily accommodate non-local features.", "labels": [], "entities": []}, {"text": "The generalized perceptron is equally flexible, relying only on a decoder for each problem and using a trivial online update procedure for each training example.", "labels": [], "entities": []}, {"text": "An advantage of the linear perceptron models we use is that they are global models, assigning a score to a complete hypothesis for each problem rather than assigning scores to parts which are then combined under statistical independence assumptions.", "labels": [], "entities": []}, {"text": "Here we are following a recent line of work applying global discriminative models to tagging and wide-coverage parsing problems.", "labels": [], "entities": [{"text": "tagging", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9646168351173401}, {"text": "wide-coverage parsing", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.646763876080513}]}, {"text": "The flexibility of our framework leads to competitive accuracies for each of the tasks we consider.", "labels": [], "entities": []}, {"text": "For word segmentation, we show how the framework can accommodate a word-based approach, rather than the standard and more restrictive character-based tagging approaches.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8039854764938354}]}, {"text": "For POS-tagging, we consider joint segmentation and showing that a single beam-search decoder can be used to achieve a significant accuracy boost over the pipeline baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9966961145401001}]}, {"text": "For Chinese and English dependency parsing, we show how both graph-based and transition-based algorithms can be implemented as beam-search, and then combine the two approaches into a single model which outperforms both in isolation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7524386942386627}]}, {"text": "Finally, for Chinese phrase-structure parsing, we describe a global model fora shift-reduce parsing algorithm, in contrast to current deterministic approaches which use only local models at each step of the parsing process.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6824345141649246}]}, {"text": "For all these tasks we present results competitive with the best results in the literature.", "labels": [], "entities": []}, {"text": "In Section 2 we describe our general framework of the generic beam-search algorithm and the generalized perceptron.", "labels": [], "entities": []}, {"text": "Then in the subsequent sections we describe each task in turn, based on conference papers including, presented in our single coherent framework.", "labels": [], "entities": []}, {"text": "We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.", "labels": [], "entities": []}, {"text": "For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy.", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.9192081093788147}, {"text": "accuracy", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9966048002243042}]}, {"text": "For the joint segmentation and POS-tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work (Zhang and Clark 2008a), while being more than an order of magnitude faster.", "labels": [], "entities": []}, {"text": "In Section 7 we provide further discussion of the framework based on the studies of the individual tasks.", "labels": [], "entities": []}, {"text": "We present the main advantages of the framework, and give an analysis of the main reasons for the high speeds and accuracies achieved.", "labels": [], "entities": []}, {"text": "We also discuss how this framework can be applied to a potential new task, and show that the comparability of candidates in the incremental process is an important factor to consider.", "labels": [], "entities": []}, {"text": "In summary, we study a general framework for incremental structural prediction, showing how the framework can be tailored to a range of syntactic processing problems to produce results competitive with the state-of-the-art.", "labels": [], "entities": [{"text": "incremental structural prediction", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.6761991381645203}]}, {"text": "The conceptual and computational simplicity of the framework, together with its language-independent nature, make it a competitive choice that should be considered for comparison by developers of alternative approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data.", "labels": [], "entities": [{"text": "Chinese Treebank 2", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9717259407043457}, {"text": "Chinese Treebank 5 data", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9751375764608383}]}, {"text": "Standard data preparation was performed before the experiments: Empty terminal nodes were removed; any non-terminal nodes with no children were removed; any unary X \u2192 X nodes resulting from the previous steps were collapsed into one X node.", "labels": [], "entities": []}, {"text": "For all experiments, we used the EVALB tool 5 for evaluation, and used labeled recall (LR), labeled precision (LP) and F1 score (which is the harmonic mean of LR and LP) to measure parsing accuracy.", "labels": [], "entities": [{"text": "EVALB tool 5", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.7819374998410543}, {"text": "recall (LR)", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.912662923336029}, {"text": "labeled precision (LP)", "start_pos": 92, "end_pos": 114, "type": "METRIC", "confidence": 0.833685576915741}, {"text": "F1 score", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9908238351345062}, {"text": "parsing", "start_pos": 181, "end_pos": 188, "type": "TASK", "confidence": 0.9660949110984802}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.814399242401123}]}, {"text": "Our development data consists of 150K words in 4, 798 sentences.", "labels": [], "entities": []}, {"text": "Eighty percent (80%) of the data were randomly chosen as the development training data, and the rest were used as the development test data.", "labels": [], "entities": []}, {"text": "Our development tests were mainly used to decide the size of the beam, the number of training iterations, and to observe the effect of early update.", "labels": [], "entities": []}, {"text": "shows the accuracy curves for joint segmentation and POS-tagging by the number of training iterations, using different beam sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999386191368103}, {"text": "joint segmentation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7541820704936981}]}, {"text": "With the size of the beam increasing from 1 to 32, the accuracies generally increase, although the amount of increase becomes small when the size of the beam becomes 16.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9969636797904968}]}, {"text": "After the tenth iteration, abeam size of 32 does not always give better accuracies than abeam size of 16.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9873639941215515}]}, {"text": "We therefore chose 16 as the size of the beam for our system.", "labels": [], "entities": []}, {"text": "The also shows that the accuracy increases with an increased number of training iterations, but the amount of increase becomes small after the 25th iteration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996979236602783}]}, {"text": "We chose 29 as the number of iterations to train our system.", "labels": [], "entities": []}, {"text": "The effect of early update: We compare the accuracies by early update and normal perceptron training.", "labels": [], "entities": [{"text": "early update", "start_pos": 14, "end_pos": 26, "type": "METRIC", "confidence": 0.9002324938774109}, {"text": "accuracies", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9813452363014221}]}, {"text": "In the normal perceptron training case, the system reached the best performance at the 22nd iteration, with a segmentation F-score of 90.58% and joint  F-score of 83.38%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9165776968002319}, {"text": "joint", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.9407991170883179}, {"text": "F-score", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.815848708152771}]}, {"text": "When using early update, the algorithm reached the best accuracy at the 30th training iteration, obtaining a segmentation F-score of 91.14% and a joint F-score of 84.06%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995145797729492}, {"text": "segmentation", "start_pos": 109, "end_pos": 121, "type": "TASK", "confidence": 0.942187488079071}, {"text": "F-score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9077560305595398}, {"text": "F-score", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.9991495609283447}]}, {"text": "We used Penn Treebank 3 for our experiments, which was separated into the training, development, and test sets in the same way as, shown in.", "labels": [], "entities": [{"text": "Penn Treebank 3", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.992146631081899}]}, {"text": "Bracketed sentences from the Treebank were translated into dependency structures using the head-finding rules from.", "labels": [], "entities": []}, {"text": "Before parsing, POS-tags are assigned to the input sentence using our baseline POS-tagger of, which can be seen as the perceptron tagger of Collins with beam-search.", "labels": [], "entities": []}, {"text": "Like McDonald, Crammer, and , we evaluated the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent.", "labels": [], "entities": [{"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.944917619228363}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.952997624874115}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9990088939666748}]}, {"text": "A set of development tests, including the convergence of the perceptron, can be found in.", "labels": [], "entities": [{"text": "convergence", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9854527115821838}]}, {"text": "In this article, we report only the final test accuracies and a set of additional speed/accuracy tradeoff results.", "labels": [], "entities": [{"text": "speed", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9923574328422546}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.7786099910736084}]}, {"text": "The accuracies of our transition-based and combined parsers on English data are shown together with other systems in, and MSTParser with templates 1-6 from    See text for details.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 122, "end_pos": 131, "type": "DATASET", "confidence": 0.9416491389274597}]}, {"text": "We used the Penn Chinese Treebank 5 for our experiments.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.9443653970956802}]}, {"text": "Following Duan, Zhao, and Xu (2007), we split the corpus into training, development, and test data as shown in.", "labels": [], "entities": []}, {"text": "We used a set of head-finding rules to turn the bracketed sentences into dependency structures, and they can be found in.", "labels": [], "entities": []}, {"text": "Like Duan, Zhao, and, we used gold-standard POS-tags for the input.", "labels": [], "entities": []}, {"text": "The parsing accuracy was evaluated by the percentage of non-root words that have been assigned the correct head, the percentage of correctly identified root words, and the percentage of complete matches, all excluding punctuation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9565775394439697}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9648296236991882}]}, {"text": "The accuracies are shown in.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9948207139968872}]}, {"text": "Rows Transition and Combined show our models in the same way as for the English experiments from Section 5.2.", "labels": [], "entities": [{"text": "Combined", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9783598184585571}, {"text": "English experiments from Section 5.2", "start_pos": 72, "end_pos": 108, "type": "DATASET", "confidence": 0.7865586042404175}]}, {"text": "Row Duan 2007 represents the transition-based model from Duan, Zhao, and, which applied beam-search to the deterministic model from.", "labels": [], "entities": [{"text": "Row Duan 2007", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7845589319864908}]}, {"text": "Row Huang 2010 represents the model of, which applies dynamicprogramming packing to transition-based parsing.", "labels": [], "entities": [{"text": "Row Huang 2010", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7782673339049021}]}, {"text": "The observations were similar to the English tests.", "labels": [], "entities": []}, {"text": "The combined parser outperformed the transition-based parsers.", "labels": [], "entities": []}, {"text": "It gave the best accuracy we are aware of for supervised dependency parsing using the CTB.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9988766312599182}, {"text": "dependency parsing", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7436271011829376}, {"text": "CTB", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9385347962379456}]}, {"text": "One last question we investigate for this article is the overall performance when the parser is pipelined with a POS-tagger, or with the joint segmentation and POS-tagging algorithm in Section 4, forming a complete pipeline for Chinese inputs.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "For these experiments, we tune the pipelined POS-tagger and the joint segmentor and POS-tagger on the CTB5 corpus in, using the development test data to decide the number of training iterations and reporting the final test accuracy.", "labels": [], "entities": [{"text": "CTB5 corpus", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9836449027061462}, {"text": "accuracy", "start_pos": 223, "end_pos": 231, "type": "METRIC", "confidence": 0.9580099582672119}]}, {"text": "The overall accuracy is calculated in F-score: Defining nc as the number of output words that have been correctly segmented and assigned the correctly segmented headword, no as the number of words in the output, and nr the number of words in the reference, precision is p = nc/no and recall is r = nc/nr. When pipelined with a pure POS-tagger using gold-standard segmentation, the pipelined system gave 93.89% POS accuracy and 81.21% joint tagging and parsing F-score for non-root words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9991357922554016}, {"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9956246018409729}, {"text": "precision", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9991728663444519}, {"text": "recall", "start_pos": 284, "end_pos": 290, "type": "METRIC", "confidence": 0.9990823268890381}, {"text": "accuracy", "start_pos": 414, "end_pos": 422, "type": "METRIC", "confidence": 0.8305507898330688}, {"text": "F-score", "start_pos": 460, "end_pos": 467, "type": "METRIC", "confidence": 0.9759638905525208}]}, {"text": "When combined with the joint segmentation and POS-tagging system, the segmentation F-score, joint segmentation and POS-tagging F-score were 95.00% and 90.17%, respectively, and the joint segmentation and parsing F-score for non-root words (excluding punctuations) was 75.09%, where the corresponding accuracy with gold-standard segmented and POStagged input was 86.21%, as shown in.", "labels": [], "entities": [{"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.8740426898002625}, {"text": "POS-tagging F-score", "start_pos": 115, "end_pos": 134, "type": "METRIC", "confidence": 0.77995765209198}, {"text": "F-score", "start_pos": 212, "end_pos": 219, "type": "METRIC", "confidence": 0.7070154547691345}, {"text": "accuracy", "start_pos": 300, "end_pos": 308, "type": "METRIC", "confidence": 0.9985600113868713}]}], "tableCaptions": [{"text": " Table 3  Training, development, and test data for word segmentation on CTB5.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7639505565166473}, {"text": "CTB5", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9438961744308472}]}, {"text": " Table 4. We follow the format from", "labels": [], "entities": []}, {"text": " Table 4  The accuracies of various word segmentors over the first SIGHAN bakeoff data.", "labels": [], "entities": [{"text": "SIGHAN bakeoff data", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.722601056098938}]}, {"text": " Table 5  The accuracies of various word segmentors over the second SIGHAN bakeoff data.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9549052715301514}, {"text": "SIGHAN bakeoff data", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.746666411558787}]}, {"text": " Table 6  Comparison between three different decoders for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7586206495761871}]}, {"text": " Table 8  The accuracies of joint segmentation and POS-tagging by 10-fold cross validation.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9699522852897644}, {"text": "joint segmentation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6975806355476379}]}, {"text": " Table 9  The speeds of joint word segmentation and POS-tagging by 10-fold cross validation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6750258803367615}]}, {"text": " Table 10  The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold  cross validation using CTB.", "labels": [], "entities": []}, {"text": " Table 11  Training, development, and test data from CTB5 for joint word segmentation and POS-tagging.", "labels": [], "entities": [{"text": "CTB5", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.9460020661354065}, {"text": "word segmentation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.6804444044828415}]}, {"text": " Table 12  Accuracy comparisons between various joint segmentors and POS-taggers on CTB5.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.969900906085968}, {"text": "CTB5", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.9691469073295593}]}, {"text": " Table 15  The training, development, and test data for English dependency parsing.", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6283240417639414}]}, {"text": " Table 16  Accuracy comparisons between various dependency parsers on English data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9817920327186584}]}, {"text": " Table 17  Training, development, and test data for Chinese dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.77474445104599}]}, {"text": " Table 18  Test accuracies of various dependency parsers on CTB5 data.", "labels": [], "entities": [{"text": "CTB5 data", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.974025547504425}]}, {"text": " Table 19  The combined segmentation, POS-tagging, and dependency parsing F-scores using different  pipelined systems.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.9782273769378662}, {"text": "dependency parsing F-scores", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6468298733234406}]}, {"text": " Table 21  The standard split of CTB2 data for phrase-structure parsing.", "labels": [], "entities": [{"text": "CTB2 data", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9285803139209747}, {"text": "phrase-structure parsing", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8306691348552704}]}, {"text": " Table 22  Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags.", "labels": [], "entities": [{"text": "CTB2", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9576800465583801}]}, {"text": " Table 23  Accuracies of various phrase-structure parsers on CTB2 with automatically assigned tags.", "labels": [], "entities": [{"text": "CTB2", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9482936263084412}]}, {"text": " Table 24  Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically  assigned POS-tags.", "labels": [], "entities": [{"text": "CTB5", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9764580130577087}]}, {"text": " Table 25  Comparison of dependency accuracies between phrase-structure parsing and dependency  parsing using CTB5 data.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.7464891672134399}, {"text": "dependency  parsing", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7638550996780396}, {"text": "CTB5 data", "start_pos": 110, "end_pos": 119, "type": "DATASET", "confidence": 0.9722244739532471}]}]}