{"title": [{"text": "Opinion Word Expansion and Target Extraction through Double Propagation", "labels": [], "entities": [{"text": "Opinion Word Expansion", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7209492723147074}, {"text": "Target Extraction", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6597327142953873}]}], "abstractContent": [{"text": "Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems.", "labels": [], "entities": [{"text": "Analysis of opinions", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.894179900487264}, {"text": "opinion mining or sentiment analysis", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.8021037817001343}]}, {"text": "In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.", "labels": [], "entities": [{"text": "opinion lexicon expansion", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7708272933959961}, {"text": "opinion target extraction", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.6250487665335337}]}, {"text": "Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.", "labels": [], "entities": []}, {"text": "To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.", "labels": [], "entities": []}, {"text": "These relations can be identified using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.", "labels": [], "entities": []}, {"text": "This proposed method is based on bootstrapping.", "labels": [], "entities": []}, {"text": "We call it double propagation as it propagates information between opinion words and targets.", "labels": [], "entities": [{"text": "double propagation", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.808767557144165}]}, {"text": "A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.", "labels": [], "entities": []}, {"text": "Thus, the method is semi-supervised due to the use of opinion word seeds.", "labels": [], "entities": []}, {"text": "In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.", "labels": [], "entities": []}, {"text": "The results show that our approach outperforms these existing methods significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Opinion mining (or sentiment analysis) has attracted a great deal of attention from researchers of natural language processing and data mining in the past few years due to many challenging research problems and practical applications.", "labels": [], "entities": [{"text": "Opinion mining (or sentiment analysis)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.736313670873642}, {"text": "natural language processing and data mining", "start_pos": 99, "end_pos": 142, "type": "TASK", "confidence": 0.6418786446253458}]}, {"text": "Two fundamental problems in opinion mining are opinion lexicon expansion and opinion target extraction).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8673765063285828}, {"text": "opinion lexicon expansion", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.7489746610323588}, {"text": "opinion target extraction", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.6291520595550537}]}, {"text": "An opinion lexicon is a list of opinion words such as good, excellent, poor, and bad which are used to indicate positive or negative sentiments.", "labels": [], "entities": []}, {"text": "It forms the foundation of many opinion mining tasks, for example, sentence) and document) sentiment classification, and feature-based opinion summarization (.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.802234023809433}, {"text": "sentence) and document) sentiment classification", "start_pos": 67, "end_pos": 115, "type": "TASK", "confidence": 0.6774407199450901}, {"text": "opinion summarization", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.6359027326107025}]}, {"text": "Although there are several opinion lexicons publicly available, it is hard, if not impossible, to maintain a universal opinion lexicon to coverall domains as opinion expressions vary significantly from domain to domain.", "labels": [], "entities": []}, {"text": "A word can be positive in one domain but has no opinion or even negative opinion in another domain.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary to expand a known opinion lexicon for applications in different domains using text corpora from the corresponding domains.", "labels": [], "entities": []}, {"text": "Opinion targets are topics on which opinions are expressed.", "labels": [], "entities": []}, {"text": "They are important because without knowing the targets, the opinions expressed in a sentence or document are of limited use.", "labels": [], "entities": []}, {"text": "For example, in the opinion sentence I am not happy with the battery life of this phone, battery life is the target of the opinion.", "labels": [], "entities": []}, {"text": "If we do not know that, this opinion is of little value.", "labels": [], "entities": []}, {"text": "Although several researchers have studied the opinion lexicon expansion and opinion target extraction (also known as topic, feature, or aspect extraction) problems, their algorithms either need additional and external resources or impose strong constraints and are of limited success.", "labels": [], "entities": [{"text": "opinion lexicon expansion", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.67173699537913}, {"text": "opinion target extraction (also known as topic, feature, or aspect extraction)", "start_pos": 76, "end_pos": 154, "type": "TASK", "confidence": 0.5926559329032898}]}, {"text": "Detailed discussions of existing works will be given in Section 2.", "labels": [], "entities": []}, {"text": "In this article, we propose a novel propagation based method to solve the opinion lexicon expansion and target extraction problems simultaneously.", "labels": [], "entities": [{"text": "opinion lexicon expansion", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.6301499009132385}, {"text": "target extraction", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.749189555644989}]}, {"text": "Our approach differs from existing approaches in that it requires no additional resources except an initial seed opinion lexicon, which is readily available.", "labels": [], "entities": []}, {"text": "Thus, it can be seen as a semi-supervised method due to the use of the seeds.", "labels": [], "entities": []}, {"text": "It is based on the observation that there are natural relations between opinion words and targets due to the fact that opinion words are used to modify targets.", "labels": [], "entities": []}, {"text": "Furthermore, we find that opinion words and targets themselves have relations in opinionated expressions too.", "labels": [], "entities": []}, {"text": "These relations can be identified via a dependency parser based on the dependency grammar, and then exploited to perform the extraction tasks.", "labels": [], "entities": []}, {"text": "The basic idea of our approach is to extract opinion words (or targets) iteratively using known and extracted (in previous iterations) opinion words and targets through the identification of syntactic relations.", "labels": [], "entities": []}, {"text": "The identification of the relations is the key to the extractions.", "labels": [], "entities": []}, {"text": "As our approach propagates information back and forth between opinion words and targets, we call it double propagation.", "labels": [], "entities": [{"text": "double propagation", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7738582491874695}]}, {"text": "Opinion word sentiment or polarity assignment (positive, negative, or neutral) and noisy target pruning methods are also designed to refine the initially extracted results.", "labels": [], "entities": []}, {"text": "In evaluation, we compare our approach with several state-of-the-art existing approaches in opinion lexicon expansion (or opinion word extraction) and target (or feature/topic) extraction.", "labels": [], "entities": [{"text": "opinion lexicon expansion (or opinion word extraction)", "start_pos": 92, "end_pos": 146, "type": "TASK", "confidence": 0.6367497344811758}, {"text": "target (or feature/topic) extraction", "start_pos": 151, "end_pos": 187, "type": "TASK", "confidence": 0.7512651458382607}]}, {"text": "The results show that our approach outperforms these existing approaches significantly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present the experimental results on opinion lexicon expansion and target extraction.", "labels": [], "entities": [{"text": "opinion lexicon expansion", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.8308020035425822}, {"text": "target extraction", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7202504128217697}]}, {"text": "We use the customer review collection 3 from Hu and as the testing data.", "labels": [], "entities": [{"text": "customer review collection 3 from Hu", "start_pos": 11, "end_pos": 47, "type": "DATASET", "confidence": 0.7078520357608795}]}, {"text": "The collection contains five review data sets: two on two digital cameras, one on a DVD player, one on an mp3 player, and one on a cellphone.", "labels": [], "entities": []}, {"text": "The detailed information of each review data set is shown in.", "labels": [], "entities": [{"text": "review data set", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.8224881887435913}]}, {"text": "The targets (i.e., product features) in these reviews are already labeled.", "labels": [], "entities": []}, {"text": "Although the opinion expressed on each target in each sentence is also labeled, the polarities (or orientations) of opinion words are not labeled.", "labels": [], "entities": []}, {"text": "In our experiments, we manually labeled the opinion words and their polarities.", "labels": [], "entities": []}, {"text": "The seed opinion lexicon is also provided by, which contains 654 positive and 1,098 negative opinion words.", "labels": [], "entities": []}, {"text": "For the comparison of our approach in opinion lexicon expansion, we implemented the approach in referred to as KN06 hereafter).", "labels": [], "entities": [{"text": "opinion lexicon expansion", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8029216527938843}, {"text": "KN06", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.8679817914962769}]}, {"text": "Details about this approach were given in Section 2.", "labels": [], "entities": []}, {"text": "We only considered adjectives as the candidates in our experiments because our method is only concerned with adjective opinion words.", "labels": [], "entities": []}, {"text": "As propagation is not performed in KN06, we also implemented a nonpropagation version of our approach, in which opinion words are only extracted by the seed words and targets which are extracted by both the seeds and extracted opinion words.", "labels": [], "entities": [{"text": "KN06", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9409147500991821}]}, {"text": "Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique (Lafferty, McCallum, and Pereira 2001) for extraction, which is a popular information extraction method and has been successfully used in labeling tasks such as POS tagging and Named Entity Recognition (Finkel, Grenager, and Manning 2005).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 288, "end_pos": 310, "type": "TASK", "confidence": 0.7804438769817352}, {"text": "POS tagging", "start_pos": 375, "end_pos": 386, "type": "TASK", "confidence": 0.8630192279815674}, {"text": "Named Entity Recognition", "start_pos": 391, "end_pos": 415, "type": "TASK", "confidence": 0.6529872020085653}]}, {"text": "The well-known toolkit CRF++ 4 is employed.", "labels": [], "entities": []}, {"text": "We consider two kinds of processing windows, one using the whole sentence (CRF); the other using words between any pair of adjective and noun (CRF-D).", "labels": [], "entities": []}, {"text": "In the first case, we designed seven labels for training, product features, nonfeature nouns, opinion adjectives, non-opinion adjectives, verbs, prepositions/conjunctions, and others.", "labels": [], "entities": []}, {"text": "In the second case, we took advantage of the relations on the shortest dependency path between the two words and used them as labels.", "labels": [], "entities": []}, {"text": "In this way, CRF is made to capture long range dependencies between words.", "labels": [], "entities": [{"text": "CRF", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9045791029930115}]}, {"text": "For both cases, we use the default parameter settings in CRF++.", "labels": [], "entities": [{"text": "CRF++", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9300158321857452}]}, {"text": "To train CRF for the extraction task, we use one data set for training and the remaining four sets for testing.", "labels": [], "entities": []}, {"text": "Consequently, we have five runs.", "labels": [], "entities": []}, {"text": "The average results are reported here.", "labels": [], "entities": []}, {"text": "In the set-up of our approaches and KN06, to examine the accuracy in extracting opinion words with different numbers of seeds we divide the initial opinion lexicon into 10 subsets, each with roughly the same number of words.", "labels": [], "entities": [{"text": "KN06", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8881052732467651}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9969106316566467}]}, {"text": "We call these lists of opinion words the 10p lists.", "labels": [], "entities": []}, {"text": "These ten 10p lists are combined to produce 20p, 50p, and 80p lists which mean 20%, 50%, and 80% of the original set (1,752 opinion words), respectively.", "labels": [], "entities": []}, {"text": "The experiments using four kinds of seed lists are performed separately.", "labels": [], "entities": []}, {"text": "Note that all metrics (precision, recall, and F-score) are computed on the newly extracted opinion words.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9992913007736206}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9976928234100342}, {"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.999053418636322}]}, {"text": "This is an important point because only the new extractions are meaningful.", "labels": [], "entities": []}, {"text": "Using all the extracted words to compute precision and recall is not appropriate as they can include many words that are already in the seed list or the labeled training set in the case of CRF.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9987396597862244}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9983596205711365}]}, {"text": "For performance evaluation on opinion target extraction, we compare our work (and also the non-propagation version, i.e., extracting targets using only the opinion words) with those in and, which also considered only explicit noun targets and experimented with the same data sets.", "labels": [], "entities": [{"text": "opinion target extraction", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6635135312875112}]}, {"text": "Details of both approaches have been described in Section 2.", "labels": [], "entities": []}, {"text": "Additionally, we experimented with the popular topic modeling algorithm PLSA (Hofmann 1999), using a public domain program, and CRF (CRF-D) using the toolkit CRF++.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7549533843994141}]}, {"text": "The parameters and training set-ups were set the same as in the opinion word extraction experiment.", "labels": [], "entities": [{"text": "opinion word extraction", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.6643234093983968}]}, {"text": "Note in this set of experiments, all our initial opinion words were used.", "labels": [], "entities": []}, {"text": "In PLSA, the maximum number of iterations was set to 500.", "labels": [], "entities": []}, {"text": "As PLSA only clusters words of the same rough topic together but does not perform fine-grained target extraction directly, we computed the precision, recall, and F-score results by combining the top M nouns of each cluster together as the extracted targets by PLSA.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.99958735704422}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.996597945690155}, {"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9993300437927246}]}, {"text": "The value of M and the number of clusters were chosen empirically.", "labels": [], "entities": []}, {"text": "We set M as 10, 20, and 30 and number of clusters as, and 50.", "labels": [], "entities": [{"text": "M", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.8486828804016113}]}, {"text": "We used the best results based on F-scores as the final results of each data set for PLSA.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9834454655647278}, {"text": "PLSA", "start_pos": 85, "end_pos": 89, "type": "TASK", "confidence": 0.6453795433044434}]}, {"text": "For the five data sets, the best results for the DVD player were gained at M = 10 with the number of clusters being 30, and those for the remaining four data sets were all gained at M = 20 with the number of clusters being 10., and 5 show the precision, recall, and F-score results respectively of our propagation approach (Prop-dep), the nonpropagation version (noProp-dep), Hu, Popescu, PLSA, and CRF (CRF-D).", "labels": [], "entities": [{"text": "precision", "start_pos": 243, "end_pos": 252, "type": "METRIC", "confidence": 0.999420166015625}, {"text": "recall", "start_pos": 254, "end_pos": 260, "type": "METRIC", "confidence": 0.9972043633460999}, {"text": "F-score", "start_pos": 266, "end_pos": 273, "type": "METRIC", "confidence": 0.9989973902702332}]}], "tableCaptions": [{"text": " Table 2  Detailed information of the five review data sets.", "labels": [], "entities": [{"text": "review data sets", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.8183702627817789}]}, {"text": " Table 3  Precisions of our propagation approach (Prop-dep), the non-propagation version (noProp-dep),  Hu, Popescu, PLSA, CRF, and CRF-D.", "labels": [], "entities": []}, {"text": " Table 4  Recalls of our propagation approach (Prop-dep), the non-propagation version (noProp-dep), Hu,  Popescu, PLSA, CRF, and CRF-D.", "labels": [], "entities": []}, {"text": " Table 5  F-scores of our propagation approach (Prop-dep), the non-propagation version (noProp-dep),  Hu, Popescu, PLSA, CRF, and CRF-D.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9908663630485535}]}]}