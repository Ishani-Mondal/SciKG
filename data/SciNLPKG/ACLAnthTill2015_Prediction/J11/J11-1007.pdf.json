{"title": [], "abstractContent": [{"text": "There has been a rapid increase in the volume of research on data-driven dependency parsers in the past five years.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6821189522743225}]}, {"text": "This increase has been driven by the availability of treebanks in a wide variety of languages-due in large part to the CoNLL shared tasks-as well as the straightforward mechanisms by which dependency theories of syntax can encode complex phenomena in free word order languages.", "labels": [], "entities": []}, {"text": "In this article, our aim is to take a step back and analyze the progress that has been made through an analysis of the two predominant paradigms for data-driven dependency parsing, which are often called graph-based and transition-based dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.7174021750688553}, {"text": "transition-based dependency parsing", "start_pos": 220, "end_pos": 255, "type": "TASK", "confidence": 0.6412467559178671}]}, {"text": "Our analysis covers both theoretical and empirical aspects and sheds light on the kinds of errors each type of parser makes and how they relate to theoretical expectations.", "labels": [], "entities": []}, {"text": "Using these observations, we present an integrated system based on a stacking learning framework and show that such a system can learn to overcome the shortcomings of each non-integrated system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic dependency representations have along history in descriptive and theoretical linguistics and many formal models have been advanced, most notably Word Grammar, Meaning-Text Theory, Functional Generative Description, and Constraint Dependency Grammar).", "labels": [], "entities": [{"text": "Syntactic dependency representations", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7543747425079346}, {"text": "Word Grammar", "start_pos": 155, "end_pos": 167, "type": "TASK", "confidence": 0.7265203297138214}, {"text": "Functional Generative Description", "start_pos": 190, "end_pos": 223, "type": "TASK", "confidence": 0.6186736226081848}]}, {"text": "Common to all theories is the notion of directed syntactic dependencies between the words of a sentence, an example of which is given in for the sentence A hearing is scheduled on the issue today, which has been extracted from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 231, "end_pos": 244, "type": "DATASET", "confidence": 0.9938076734542847}]}, {"text": "A dependency graph of a sentence represents each word and its syntactic modifiers through labeled directed arcs, where each arc label comes from some finite set representing possible syntactic roles.", "labels": [], "entities": []}, {"text": "Returning to our example in, we can see multiple instances of labeled dependency relations such as the one from the finite verb is to hearing labeled SBJ indicating that hearing is the head of the syntactic subject of the finite verb.", "labels": [], "entities": []}, {"text": "An artificial word has been inserted at the beginning of the sentence that will always serve as the single root of the graph and is primarily a means to simplify computation.", "labels": [], "entities": []}, {"text": "Syntactic dependency graphs have recently gained a wide interest in the computational linguistics community and have been successfully employed for many problems ranging from machine translation) to ontology construction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.8004877269268036}, {"text": "ontology construction", "start_pos": 199, "end_pos": 220, "type": "TASK", "confidence": 0.8822104930877686}]}, {"text": "A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to long-distance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order.", "labels": [], "entities": []}, {"text": "This is undoubtedly one of the reasons for the emergence of dependency parsers fora wide range of languages).", "labels": [], "entities": []}, {"text": "Thus, the example in contains an instance of a discontinuous construction through the subgraph rooted at the word hearing.", "labels": [], "entities": []}, {"text": "Specifically, the dependency arc from hearing to on spans the words is and scheduled, which are not nodes in this subgraph.", "labels": [], "entities": []}, {"text": "An arc of this kind is said to be non-projective.", "labels": [], "entities": []}, {"text": "In this article we focus on a common paradigm called data-driven dependency parsing, which encompasses parsing systems that learn to produce dependency graphs for sentences from a corpus of sentences annotated with dependency graphs.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7015586793422699}]}, {"text": "The advantage of such models is that they are easily ported to any domain or language in which annotated resources exist.", "labels": [], "entities": []}, {"text": "Many data-driven parsing systems are grammarless, in that they do not assume the existence of a grammar that defines permissible sentences of the language.", "labels": [], "entities": []}, {"text": "Instead, the goal of most data-driven parsing systems is to discriminate good parses from bad fora given sentence, regardless of its grammaticality.", "labels": [], "entities": []}, {"text": "Alternatively, one can view such systems as parsers fora grammar that induces the language of all strings.", "labels": [], "entities": []}, {"text": "The rise of statistical methods in natural language processing coupled with the availability of dependency annotated corpora for multiple languages-most notably from the 2006 and 2007 CoNLL shared tasks)-has led to a boom in research on data-driven dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 249, "end_pos": 267, "type": "TASK", "confidence": 0.6740357726812363}]}, {"text": "Making sense of this work is a challenging problem, but an important one if the field is to continue to make advances.", "labels": [], "entities": []}, {"text": "Of the many important questions to be asked, three are perhaps most crucial at this stage in the development of parsers: 1.", "labels": [], "entities": []}, {"text": "How can we formally categorize the different approaches to data-driven dependency parsing?", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7272978723049164}]}], "datasetContent": [{"text": "The experiments presented in this article are all based on data from the CoNLL-X shared task).", "labels": [], "entities": []}, {"text": "In this section we first describe the task and the resources created there and then describe how MSTParser and MaltParser were trained for the task, including feature representations and learning algorithms.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.9180945754051208}]}, {"text": "In this section, we present an experimental evaluation of the two guided models followed by a comparative error analysis including both the base models and the guided models.", "labels": [], "entities": []}, {"text": "The data sets used in these experiments are identical to those used in Section 4.", "labels": [], "entities": []}, {"text": "The guided models were trained according to the scheme explained in Section 5, with two-fold cross-validation when parsing the training data with the guide parsers.", "labels": [], "entities": []}, {"text": "Preliminary experiments suggested that cross-validation with more folds had a negligible impact on the results.", "labels": [], "entities": []}, {"text": "Models are evaluated by their labeled attachment score on the test set using the evaluation software from the CoNLL-X shared task with default settings.", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 110, "end_pos": 129, "type": "DATASET", "confidence": 0.8064238429069519}]}, {"text": "Statistical significance was assessed using Dan Bikel's randomized parsing evaluation comparator with the default setting of 10,000 iterations.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.5665873885154724}]}, {"text": "shows the results, for each language and on average, for the two base models (MST, Malt) and for the two guided models (MST Malt , Malt MST ).", "labels": [], "entities": [{"text": "MST Malt", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9063313603401184}]}, {"text": "We also give oracle combination scores based on both by taking the best graph or the best set of arcs relative to the gold standard, as discussed in Section 4.4.", "labels": [], "entities": []}, {"text": "First of all, we see that both guided models show a consistent increase inaccuracy compared to their base model, even though the extent of the improvement varies across languages from about half a percentage point (Malt MST on Chinese) up to almost four percentage points (Malt MST on Slovene).", "labels": [], "entities": []}, {"text": "11 It is thus quite clear that both models have the capacity to learn from features generated by the other model.", "labels": [], "entities": []}, {"text": "However, it is also clear that the graph-based MST model shows a somewhat larger improvement, both on average and for all languages except Czech, German, Portuguese, and Slovene.", "labels": [], "entities": [{"text": "MST", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9319921731948853}]}, {"text": "Finally, given that the two base models had the best performance for these data sets at the CoNLL-X shared task, the guided models achieve a substantial improvement of the state of the art.", "labels": [], "entities": []}, {"text": "Although there is no statistically significant difference between the two base models, they are both outperformed by Malt MST (p < 0.0001), which in turn has significantly lower accuracy than MST Malt (p < 0.0005).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9990098476409912}]}, {"text": "An extension to the models described so far would be to iteratively integrate the two parsers in the spirit of pipeline iteration.", "labels": [], "entities": []}, {"text": "For example, one could start with a Malt model, use it to train a guided MST Malt model, then use that as the guide to train a Malt MST Malt model, and so forth.", "labels": [], "entities": [{"text": "MST Malt model", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.7575674454371134}, {"text": "Malt MST Malt model", "start_pos": 127, "end_pos": 146, "type": "DATASET", "confidence": 0.8011743873357773}]}, {"text": "We ran such experiments, but found that accuracy did not increase significantly and in some cases decreased slightly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9995693564414978}]}, {"text": "This was true regardless of which parser began the iterative process.", "labels": [], "entities": []}, {"text": "In retrospect, this result is not surprising.", "labels": [], "entities": []}, {"text": "Because the initial integration effectively incorporates knowledge from both parsing systems, there is little to be gained by adding additional parsers in the chain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Labeled parsing accuracy for top-scoring systems at CoNLL-X (Buchholz and Marsi 2006).", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.92283695936203}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9739726185798645}, {"text": "CoNLL-X", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9382229447364807}]}, {"text": " Table 2  Data sets. Tok = number of tokens (\u00d71000); Sen = number of sentences (\u00d71000); T/S = tokens per  sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speech  tags; PoS = number of (fine-grained) part-of-speech tags; MSF = number of morphosyntactic  features (split into atoms); Dep = number of dependency types; NPT = proportion of  non-projective dependencies/tokens (%); NPS = proportion of non-projective dependency  graphs/sentences (%).", "labels": [], "entities": []}, {"text": " Table 6  Labeled attachment scores for base parsers and guided parsers (improvement in percentage  points).", "labels": [], "entities": [{"text": "attachment", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9371966123580933}]}, {"text": " Table 7  Accuracy relative to dependent part of speech (improvement in percentage points).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986431002616882}]}]}