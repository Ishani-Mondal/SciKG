{"title": [{"text": "Learning and Evaluation of Dialogue Strategies for New Applications: Empirical Methods for Optimization from Small Data Sets", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew data-driven methodology for simulation-based dialogue strategy learning, which allows us to address several problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and determining a data-driven reward function.", "labels": [], "entities": [{"text": "dialogue strategy learning", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.728225181500117}]}, {"text": "In addition, we evaluate the result with real users, and explore how results transfer between simulated and real interactions.", "labels": [], "entities": []}, {"text": "We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is \"bootstrapped\" from small amounts of Wizard-of-Oz (WOZ) data.", "labels": [], "entities": []}, {"text": "This use of WOZ data allows data-driven development of optimal strategies for domains where no working prototype is available.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.7196495234966278}]}, {"text": "Using simulation-based RL allows us to find optimal policies which are not (necessarily) present in the original data.", "labels": [], "entities": []}, {"text": "Our results show that simulation-based RL significantly outperforms the average (human wizard) strategy as learned from the data by using Supervised Learning.", "labels": [], "entities": [{"text": "RL", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9246299266815186}]}, {"text": "The bootstrapped RL-based policy gains on average 50 times more reward when tested in simulation, and almost 18 times more reward when interacting with real users.", "labels": [], "entities": []}, {"text": "Users also subjectively rate the RL-based policy on average 10% higher.", "labels": [], "entities": [{"text": "RL-based", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.7482677698135376}]}, {"text": "We also show that results from simulated interaction do transfer to interaction with real users, and we explicitly evaluate the stability of the data-driven reward function.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical learning approaches, such as Reinforcement Learning (RL), for Spoken Dialogue Systems offer several potential advantages over the standard rule-based handcoding approach to dialogue systems development: a data-driven development cycle, provably optimal action policies, a precise mathematical model for action selection, possibilities for generalization to unseen states, and automatic optimization of competing trade-offs in the objective function.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7260855436325073}]}, {"text": "See,, and for an introduction to dialogue strategy learning.", "labels": [], "entities": [{"text": "dialogue strategy learning", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.8930131991704305}]}, {"text": "One of the major limitations of this approach is that it relies on a large quantity of data being available.", "labels": [], "entities": []}, {"text": "In cases when a fixed data set is used for learning (e.g.,, the optimal policy can only be discovered when it is present in the data set.", "labels": [], "entities": []}, {"text": "(Note, by a policy being \"present in a data set\" we mean that the set of state-action mappings which define the policy is contained in that data set.", "labels": [], "entities": []}, {"text": "When a policy is not present in a data set, either some states covered by the policy are not seen at all in that data, or the actions chosen by the policy in some states are different to those seen in the data.)", "labels": [], "entities": []}, {"text": "To overcome this problem, simulated learning environments are being used to explore optimal policies which were previously unseen in the data (e.g., Eckert,).", "labels": [], "entities": []}, {"text": "However, several aspects of the components of this simulated environment are usually hand-crafted, and thus limit the scope of policy learning.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.8285303711891174}]}, {"text": "In particular, the optimization (or reward) function is often manually set).", "labels": [], "entities": []}, {"text": "In order to build simulation components from real data, annotated in-domain dialogue corpora have to be available which explore a range of dialogue management decisions.", "labels": [], "entities": []}, {"text": "Collecting dialogue data without a working prototype is problematic, leaving the developer with a classic \"chicken-or-egg\" problem.", "labels": [], "entities": []}, {"text": "We therefore propose to learn dialogue strategies using simulation-based RL, where the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data.", "labels": [], "entities": []}, {"text": "Ina WOZ experiment, a hidden human operator, the so-called \"wizard,\" simulates (partly or completely) the behavior of the application, while subjects are left in the belief that they are interacting with areal system.", "labels": [], "entities": []}, {"text": "In contrast to preceding work, our approach enables strategy learning in domains where no prior system is available.", "labels": [], "entities": [{"text": "strategy learning", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.9272832870483398}]}, {"text": "Optimized learned strategies are then available from the first moment of on-line operation, and handcrafting of dialogue strategies is avoided.", "labels": [], "entities": []}, {"text": "This independence from large amounts of in-domain dialogue data allows researchers to apply RL to new application areas beyond the scope of existing dialogue systems.", "labels": [], "entities": [{"text": "RL", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9604304432868958}]}, {"text": "We call this method \"bootstrapping.\"", "labels": [], "entities": []}, {"text": "In addition, our work is the first using a data-driven simulated environment.", "labels": [], "entities": []}, {"text": "Previous approaches to simulation-based dialogue strategy learning usually handcraft some of their components.", "labels": [], "entities": [{"text": "dialogue strategy learning", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.8103522260983785}]}, {"text": "Of course, some human effort is needed in developing the WOZ environment and annotating the collected data, although automatic dialogue annotation could be applied ().", "labels": [], "entities": []}, {"text": "The alternative-collecting data using hand-coded dialogue strategies-would still require annotation of the user actions, and has the disadvantage of constraining the system policies explored in the collected data.", "labels": [], "entities": []}, {"text": "Therefore, WOZ data allows exploration of a range of possible strategies, as intuitively generated by the wizards, in contrast to using an initial system which can only explore a pre-defined range of options.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7857195138931274}]}, {"text": "However, WOZ experiments usually only produce a limited amount of data, and the optimal policy is not likely to be present in the original small data set.", "labels": [], "entities": [{"text": "WOZ", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.7892453074455261}]}, {"text": "Our method shows how to use these data to build a simulated environment in which optimal policies can be discovered.", "labels": [], "entities": []}, {"text": "We show this advantage by comparing RL-based strategy against a supervised strategy which captures average human wizard performance on the dialogue task.", "labels": [], "entities": []}, {"text": "This comparison allows us to measure relative improvement over the training data.", "labels": [], "entities": []}, {"text": "The use of WOZ data has earlier been proposed in the context of RL.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7279379516839981}, {"text": "RL", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.962312638759613}]}, {"text": "use WOZ data to discover the state and action space for the design of a Markov Decision Process (MDP).", "labels": [], "entities": []}, {"text": "use WOZ data to build a simulated user and noise model for simulation-based RL.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7688473463058472}, {"text": "RL", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.894404411315918}]}, {"text": "Although both studies show promising first results, their simulated environments still contain many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 225, "end_pos": 233, "type": "DATASET", "confidence": 0.8083545863628387}]}, {"text": "propose to \"bootstrap\" with a simulated user which is entirely hand-crafted.", "labels": [], "entities": []}, {"text": "In the following we propose what is currently the most strongly data-driven approach to these problems.", "labels": [], "entities": []}, {"text": "We also show that the resulting policy performs well for real users.", "labels": [], "entities": []}, {"text": "In particular we propose a five-step procedure (see):", "labels": [], "entities": []}], "datasetContent": [{"text": "Six people played the role of an intelligent interface (the \"wizards\").", "labels": [], "entities": []}, {"text": "The wizards were able to speak freely and display search results on the screen by clicking on precomputed templates.", "labels": [], "entities": []}, {"text": "Wizards' outputs were not restricted, in order to explore the different ways they intuitively chose to present search results.", "labels": [], "entities": []}, {"text": "Wizard's utterances were immediately transcribed and played back to the user with Text-To-Speech.", "labels": [], "entities": []}, {"text": "Twenty-one subjects (11 women, 10 men) were given a set of predefined tasks to perform, as well as a primary driving task, using a driving simulator.", "labels": [], "entities": []}, {"text": "The users were able to speak, as well as make selections on the screen.", "labels": [], "entities": []}, {"text": "The experiment proceeded as follows.", "labels": [], "entities": []}, {"text": "First the wizards were trained to use the database interface and they were also given general instructions about how to interact with the user.", "labels": [], "entities": []}, {"text": "Training took 45 minutes, including five example tasks.", "labels": [], "entities": []}, {"text": "After the user arrived s/he was introduced to the driving simulator and had to perform a short test drive.", "labels": [], "entities": []}, {"text": "The users solved two sets of tasks with two tasks in each.", "labels": [], "entities": []}, {"text": "After each task the user filled out a task-specific questionnaire, in which they indicated perceived task success and satisfaction on a five-point Likert scale.", "labels": [], "entities": []}, {"text": "Finally, the user was interviewed by the experiment leader following a questionnaire containing questions similar to the PARADISE study, including questions on task ease, timing, multimodal and verbal presentation, as well as future use of such systems.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8150242567062378}]}, {"text": "All subjects reported that they were convinced that they were interacting with areal system.", "labels": [], "entities": []}, {"text": "To approximate speech recognition errors we used a tool that randomly deletes parts of the transcribed utterances.", "labels": [], "entities": [{"text": "speech recognition errors", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.7455476522445679}]}, {"text": "Due to the fact that humans are able to make sense of even heavily corrupted input, this method not only covers non-understandings, but wizards also built up their own hypotheses about what the user really said, which can lead to misunderstandings.", "labels": [], "entities": []}, {"text": "The word deletion rate varied: 20% of the utterances were weakly corrupted (= deletion rate of 20%), and 20% were strongly corrupted (= deletion rate of 50%).", "labels": [], "entities": [{"text": "word deletion", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.6385644823312759}]}, {"text": "In 60% of the cases the wizard saw the transcribed speech uncorrupted.", "labels": [], "entities": []}, {"text": "Example (1) illustrates the kind of corrupted utterances the wizard had to deal with.", "labels": [], "entities": []}, {"text": "There are some shortcomings of this technique, which we discuss in.", "labels": [], "entities": []}, {"text": "However, the data are useful for our purposes because our main interest here is in multimodal presentation strategies (in the presence of some input noise).", "labels": [], "entities": []}, {"text": "Other studies have specifically targeted the Dialogue Management question of how to handle ASR input noise (e.g.,).", "labels": [], "entities": [{"text": "Dialogue Management", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8716922998428345}, {"text": "ASR input noise", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.895929237206777}]}, {"text": "This metric measures the divergence between distributions P and Q in a context with M responses.", "labels": [], "entities": []}, {"text": "Ideally, the KL divergence between two similar distributions is close to zero.", "labels": [], "entities": []}, {"text": "KL allows us to compare the raw probabilities as observed in the original data with the probabilities generated by our user simulation models.", "labels": [], "entities": [{"text": "KL", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8307474851608276}]}, {"text": "We then compare the KL results of the cluster-based and the smoothed user simulation against a random model and a majority baseline (see).", "labels": [], "entities": []}, {"text": "The random model is constructed by assigning equal frequency to all four actions, whereas the majority baseline always predicts the most frequent action in one context.", "labels": [], "entities": []}, {"text": "The comparison against the random baseline tests the hypothesis that our user simulations are more consistent with the observed data than random behavior.", "labels": [], "entities": []}, {"text": "The majority baseline represents the hypothesis that our user simulation explores a significantly wider range of behavior than the most frequent user action.", "labels": [], "entities": []}, {"text": "The user simulation models have a small divergence from the original data suggesting that they are good simulations for training and testing policies.", "labels": [], "entities": []}, {"text": "The smoothed and the cluster-based model gain on average five times lower KL scores than the baselines.", "labels": [], "entities": [{"text": "KL scores", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9570552110671997}]}, {"text": "We therefore conclude that both simulations show consistent (i.e., better than random) as well as varying (i.e., better than the majority decision) behavior.", "labels": [], "entities": []}, {"text": "As mentioned previously, we want user simulations for policy training to allow more exploration, whereas for testing we want user simulations which are more realistic.", "labels": [], "entities": []}, {"text": "We therefore choose to test with the smoothed model because its low KL score shows that it is closest to the data, and we use the cluster-based simulation for training.", "labels": [], "entities": [{"text": "KL score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9737636744976044}]}, {"text": "Note that the KL divergence only measures consistency with respect to specific dialogue contexts.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.8225108683109283}]}, {"text": "However, user simulations also need to be coherent with respect to the dialogue history and the current user goal.", "labels": [], "entities": []}, {"text": "We therefore model the user's goal (i.e., the song s/he is looking for) similar to \"agenda-based user models\"(.", "labels": [], "entities": []}, {"text": "The user goal corresponds to a database entry, which is randomly chosen in the beginning of each dialogue.", "labels": [], "entities": []}, {"text": "Every time the user simulation generates a speech act, the corresponding value is chosen from the goal record, dependent on the slot value the system was asking for.", "labels": [], "entities": []}, {"text": "For the user tests the RL policy is ported to a working ISU-based dialogue system via table look-up (see table in Appendix E) , which indicates the action with the highest expected reward for each state (cf..", "labels": [], "entities": [{"text": "RL", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9479706287384033}]}, {"text": "The supervised average wizard baseline is implemented using standard threshold-based update rules.", "labels": [], "entities": []}, {"text": "The experimen-.06(\u00b151.5)*** *** Significant difference between SL and RL at p < .001 (with standard deviation \u00b1).", "labels": [], "entities": [{"text": "SL", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9228218197822571}, {"text": "RL", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.7248644828796387}]}, {"text": "tal conditions are similar to the WOZ study, that is, we ask the users to solve similar tasks, and use similar questionnaires.", "labels": [], "entities": [{"text": "WOZ study", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.8597177565097809}]}, {"text": "Furthermore, we decided to use typed user input rather than ASR.", "labels": [], "entities": []}, {"text": "The use of text input allows us to target the experiments to the dialogue management decisions on presentation strategies, and prevents ASR quality from interfering with the experimental results, especially because subjective user scores are highly sensitive to ASR noise).", "labels": [], "entities": [{"text": "ASR", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.9769765734672546}]}, {"text": "Both RL and SL wizard policies are trained to handle noisy conditions, so that they usually confirm user input, which makes dialogues longer but more reliable.", "labels": [], "entities": []}, {"text": "The lack of noise in this experiment means that confirmation happens more than is strictly required (although there are still text input spelling mistakes), but the information presentation decisions are not affected.", "labels": [], "entities": [{"text": "confirmation", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.9215623736381531}]}, {"text": "Seventeen subjects (8 women, 9 men) are given a set of 12 (6\u00d72) predefined, ordered tasks, which they solve by interaction with the RL-based and the SL-based average wizard system in a cyclic order.", "labels": [], "entities": []}, {"text": "As a secondary task users are asked to count certain objects in a driving simulation.", "labels": [], "entities": []}, {"text": "In total, 204 dialogues with 1,115 turns are gathered in this set-up.", "labels": [], "entities": []}, {"text": "See also Rieser (2008).", "labels": [], "entities": []}, {"text": "We introduce a final phase where we meta-evaluate the whole framework.", "labels": [], "entities": []}, {"text": "This final step is necessary because WOZ experiments only simulate HCI.", "labels": [], "entities": []}, {"text": "We therefore need to show that a strategy bootstrapped from WOZ data indeed transfers to real HCI.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8447948396205902}]}, {"text": "We first show that the results for simulated and real interaction are compatible (Section 6.1).", "labels": [], "entities": []}, {"text": "We also meta-evaluate the reward function, showing that it is a stable, accurate estimate for real users' preferences (Section 6.2).", "labels": [], "entities": []}, {"text": "We propose anew method for meta-evaluation of the reward (or \"objective\") function.", "labels": [], "entities": []}, {"text": "One major advantage of RL-based dialogue strategy development is that the dialogue strategy can be automatically trained and evaluated using the same objective function).", "labels": [], "entities": [{"text": "RL-based dialogue strategy development", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.9516909718513489}]}, {"text": "Despite its central aspect for RL, quality assurance for objective functions has received little attention so far.", "labels": [], "entities": [{"text": "RL", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9831218123435974}]}, {"text": "In fact, as noted in Section 3.6, the reward function is one of the most hand-coded aspects of RL).", "labels": [], "entities": [{"text": "RL", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9080488681793213}]}, {"text": "Here, we bring together two strands of research for evaluating the reward function: One strand uses Reinforcement Learning to automatically optimize dialogue strategies (e.g.,; the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR-ADISE framework), and meta-evaluation of dialogue metrics (e.g.,.", "labels": [], "entities": []}, {"text": "Clearly, automatic optimization and evaluation of dialogue policies, as well as quality control of the objective function, are closely interrelated problems: How can we make sure that we optimize a system according to real users' preferences?", "labels": [], "entities": []}, {"text": "In Section 3.6 we constructed a data-driven objective function using the PARADISE framework, and used it for automatic dialogue strategy optimization, following work by.", "labels": [], "entities": [{"text": "PARADISE framework", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.7614093720912933}, {"text": "dialogue strategy optimization", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.7522549629211426}]}, {"text": "However, it is not clear how reliable such a predictive model is, that is, whether it indeed estimates real user preferences.", "labels": [], "entities": []}, {"text": "The models obtained with PARADISE often fit the data poorly.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.7395159602165222}]}, {"text": "It is also not clear how general they are across different systems and user groups).", "labels": [], "entities": []}, {"text": "Furthermore, it is not clear how they perform when being used for automatic strategy optimization within the RL framework.", "labels": [], "entities": [{"text": "automatic strategy optimization", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6628236472606659}]}, {"text": "In the following we evaluate different aspects of the reward function.", "labels": [], "entities": []}, {"text": "In Section 6.2.1 we test the model stability in a test-retest comparison across different user populations and data sets.", "labels": [], "entities": []}, {"text": "In Section 6.2.2 we measure its prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.8793563842773438}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9773595929145813}]}], "tableCaptions": [{"text": " Table 5  User action types and frequencies as annotated in the data.", "labels": [], "entities": []}, {"text": " Table 7  Kullback-Leibler divergence scores for the different user simulations.", "labels": [], "entities": []}, {"text": " Table 11  Prediction accuracy for models within (1-3) and across (4-5) data sets.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.8488971590995789}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8883526921272278}]}, {"text": " Table 1  Inter-annotator agreement on user act with \u03ba = .632.", "labels": [], "entities": []}]}