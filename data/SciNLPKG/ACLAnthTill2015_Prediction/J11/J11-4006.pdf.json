{"title": [{"text": "Parsing Noun Phrases in the Penn Treebank", "labels": [], "entities": [{"text": "Parsing Noun Phrases", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8337073723475138}, {"text": "Penn Treebank", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9735715687274933}]}], "abstractContent": [{"text": "Noun phrases (NPs) area crucial part of natural language, and can have a very complex structure.", "labels": [], "entities": []}, {"text": "However, this NP structure is largely ignored by the statistical parsing field, as the most widely used corpus is not annotated with it.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7419164478778839}]}, {"text": "This lack of gold-standard data has restricted previous efforts to parse NPs, making it impossible to perform the supervised experiments that have achieved high performance in so many Natural Language Processing (NLP) tasks.", "labels": [], "entities": [{"text": "parse NPs", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.8495949804782867}]}, {"text": "We comprehensively solve this problem by manually annotating NP structure for the entire Wall Street Journal section of the Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 89, "end_pos": 137, "type": "DATASET", "confidence": 0.9548715054988861}]}, {"text": "The inter-annotator agreement scores that we attain dispel the belief that the task is too difficult, and demonstrate that consistent NP annotation is possible.", "labels": [], "entities": []}, {"text": "Our gold-standard NP data is now available for use in all parsers.", "labels": [], "entities": [{"text": "NP data", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.6427352577447891}]}, {"text": "We experiment with this new data, applying the Collins (2003) parsing model, and find that its recovery of NP structure is significantly worse than its overall performance.", "labels": [], "entities": [{"text": "recovery", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9563161134719849}]}, {"text": "The parser's F-score is up to 5.69% lower than a baseline that uses deterministic rules.", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9989822506904602}]}, {"text": "Through much experimentation , we determine that this result is primarily caused by alack of lexical information.", "labels": [], "entities": []}, {"text": "To solve this problem we construct a wide-coverage, large-scale NP Bracketing system.", "labels": [], "entities": [{"text": "NP Bracketing", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.7540661096572876}]}, {"text": "With our Penn Treebank data set, which is orders of magnitude larger than those used previously, we build a supervised model that achieves excellent results.", "labels": [], "entities": [{"text": "Penn Treebank data set", "start_pos": 9, "end_pos": 31, "type": "DATASET", "confidence": 0.994244709610939}]}, {"text": "Our model performs at 93.8% F-score on the simple NP task that most previous work has undertaken, and extends to bracket longer, more complex NPs that are rarely dealt within the literature.", "labels": [], "entities": [{"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9996228218078613}]}, {"text": "We attain 89.14% F-score on this much more difficult task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9924094676971436}]}, {"text": "Finally, we implement a post-processing module that brackets NPs identified by the Bikel (2004) parser.", "labels": [], "entities": []}, {"text": "Our NP Bracketing model includes a wide variety of features that provide the lexical information that was missing during the parser experiments, and as a result, we outperform the parser's F-score by 9.04%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 189, "end_pos": 196, "type": "METRIC", "confidence": 0.9936933517456055}]}, {"text": "These experiments demonstrate the utility of the corpus, and show that many NLP applications can now make use of NP structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "The parsing of noun phrases (NPs) involves the same difficulties as parsing in general.", "labels": [], "entities": [{"text": "parsing of noun phrases (NPs)", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.9039553659302848}]}, {"text": "NPs contain structural ambiguities, just as other constituent types do, and resolving these ambiguities is required for their proper interpretation.", "labels": [], "entities": []}, {"text": "Despite this, statistical methods for parsing have not achieved high performance until now.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9899847507476807}]}, {"text": "Many Natural Language Processing (NLP) systems specifically require the information carried within NPs.", "labels": [], "entities": []}, {"text": "Question Answering (QA) systems need to supply an NP as the answer to many types of factoid questions, often using a parser to identify candidate NPs to return to the user.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8407275021076203}]}, {"text": "If the parser cannot recover NP structure then the correct candidate may never be found, even if the correct dominating noun phrase has been found.", "labels": [], "entities": []}, {"text": "As an example, consider the following extract: . .", "labels": [], "entities": []}, {"text": "as crude oil prices rose by 50%, a result of the.", "labels": [], "entities": []}], "datasetContent": [{"text": "These figures supply a more detailed picture of how performance has changed, showing that although the new brackets make parsing marginally more difficult overall (by about 0.5% in F-score), accuracy on the original structure is only negligibly worse.", "labels": [], "entities": [{"text": "parsing", "start_pos": 121, "end_pos": 128, "type": "TASK", "confidence": 0.97903972864151}, {"text": "F-score", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9968075752258301}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9990260601043701}]}, {"text": "The new NML and JJP brackets are the cause of the performance drop, with an F-score more than 20% lower than the overall This demonstrates the difficulty of parsing NPs.", "labels": [], "entities": [{"text": "NML", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.9323522448539734}, {"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9956142902374268}, {"text": "parsing NPs", "start_pos": 157, "end_pos": 168, "type": "TASK", "confidence": 0.8278369903564453}]}, {"text": "The all-brackets result actually compares well to the original Penn Treebank model, as the latter is not recovering or being evaluated on NP structure and as such, has a much easier task.", "labels": [], "entities": [{"text": "Penn Treebank model", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9847800135612488}]}, {"text": "However the parser's performance on NML and JJP brackets is surprisingly poor.", "labels": [], "entities": [{"text": "NML", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8879452347755432}, {"text": "JJP brackets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.7743556201457977}]}, {"text": "Indeed, the figure of 67.44% is more than 5% lower than the baseline established using the annotation tool's suggestions (see).", "labels": [], "entities": []}, {"text": "The suggestions were in part based on NER information that the parser does not possess, but we would still expect the parser to outperform a set of deterministic rules.", "labels": [], "entities": []}, {"text": "The rest of this section will describe a number of attempts to improve the parser's performance by altering the data being used and the parser model itself.", "labels": [], "entities": []}, {"text": "With our new data set of simple NPs, we began running experiments similar to those carried out in the literature (.", "labels": [], "entities": []}, {"text": "Refer back to Section 2.3 fora reminder of the models typically used for this task.", "labels": [], "entities": []}, {"text": "We implemented both an adjacency and dependency model, and three different association measures: the raw bigram count, the bigram probability, and \u03c7 2 . Raw bigram count = count(w i , w j ) ( 7 )  The complex NP results are evaluated using several measures.", "labels": [], "entities": []}, {"text": "Firstly, matching brackets is the standard Parseval evaluation method (.", "labels": [], "entities": []}, {"text": "Secondly, because our annotation only marks left-branching structure explicitly (see Section 3), we can also report implicit matching brackets, where we automatically insert the implicit right-branching brackets for evaluation purposes.", "labels": [], "entities": []}, {"text": "This takes into account fully right-branching NPs, which contribute no score using the harsher, explicit matching brackets evaluation.", "labels": [], "entities": []}, {"text": "For example, a baseline of always choosing right branching will score 0.0, as no explicit brackets are needed.", "labels": [], "entities": []}, {"text": "We also measure exact NP match, which measures the percentage of complex NPs that are entirely correct, and the model's performance on the three word NPs that are processed during Barker's algorithm.", "labels": [], "entities": [{"text": "exact NP match", "start_pos": 16, "end_pos": 30, "type": "METRIC", "confidence": 0.8371476531028748}]}, {"text": "We only report accuracy for implicit brackets, as there is a set number of brackets dependent on the length of the word, and so precision and recall are always equal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9992306232452393}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9995933175086975}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9994745850563049}]}, {"text": "Finally, note that the three-word NPs are different for each model, as the next three word NP to bracket depends on the decisions made previously for this complex NP.", "labels": [], "entities": []}, {"text": "Consequently, the numbers for this measure are not directly comparable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Agreement between annotators, before and after discussion and revision. Two evaluations are  shown: matched brackets of the newly added NML and JJP nodes, and automatically generated  dependencies for all words in the NP.", "labels": [], "entities": []}, {"text": " Table 3  Agreement with DepBank. Two evaluations are shown: over-all dependencies, and where all  dependencies in an NP must be correct. The bottom two rows exclude NPs where no NML or  JJP annotation was added.", "labels": [], "entities": []}, {"text": " Table 5  Suggestion rule performance. The middle group shows a subtractive analysis, removing  individual suggestion groups from the All row. The final two rows are on specific sections;  all other figures are calculated over the entire corpus.", "labels": [], "entities": []}, {"text": " Table 6  Performance achieved with the Bikel (2004) parser, initial results on development set.", "labels": [], "entities": [{"text": "Bikel (2004) parser", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.8760664701461792}]}, {"text": " Table 7  Performance achieved with the Bikel (2004) parser and relabeled brackets. The DIFF column  compares against the initial results in Table 6.", "labels": [], "entities": [{"text": "Bikel (2004) parser", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.8590768575668335}, {"text": "DIFF", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.8591158390045166}]}, {"text": " Table 8  Performance achieved with the Bikel (2004) parser and correct head-finding rules. The DIFF  column compares against the initial results in Table 6.", "labels": [], "entities": [{"text": "Bikel (2004) parser", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.7615901827812195}, {"text": "DIFF", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9206846356391907}]}, {"text": " Table 9  Performance achieved with the Bikel (2004) parser and the base-NP model off in three different  ways: (1) No NP submodel, (2) No NPB nodes, (3) No NPB nodes when the NP is dominating a  NML. DIFF is again comparing against the initial results in Table 6.", "labels": [], "entities": []}, {"text": " Table 10  Performance achieved with the Bikel (2004) parser and explicit right-branching structure. The  DIFF column compares against the initial results in Table 6.", "labels": [], "entities": [{"text": "DIFF", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.827692985534668}]}, {"text": " Table 11  Performance achieved with the Bikel (2004) parser, final results on the test set. The suggestion  baseline is comparable to the NML and JJP brackets only figures, as are the Original PTB and  Original structure figures.", "labels": [], "entities": [{"text": "Bikel (2004) parser", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.755138611793518}, {"text": "NML", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.9425873756408691}, {"text": "Original PTB", "start_pos": 185, "end_pos": 197, "type": "DATASET", "confidence": 0.6801930069923401}]}, {"text": " Table 12  Error analysis for the Bikel (2004) parser on the development set, showing how many times the  error occurred (#), the percentage of total errors (%), and how many of the errors were false  positives (FP) or false negatives (FN). If a cross (\u00d7) is in the final column then the example  shows the error being made. On the other hand, if the example is marked with a tick (  \u221a  ) then it  is demonstrating the correct bracketing.", "labels": [], "entities": [{"text": "errors were false  positives (FP) or false negatives (FN)", "start_pos": 182, "end_pos": 239, "type": "METRIC", "confidence": 0.8362305256036612}]}, {"text": " Table 13  Comparison showing the sizes of various NP bracketing corpora.", "labels": [], "entities": [{"text": "NP bracketing corpora", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.8031338453292847}]}, {"text": " Table 14  The most common bracketings of POS tag sequences in our complex NP corpus.", "labels": [], "entities": [{"text": "NP corpus", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.7254372835159302}]}, {"text": " Table 15  Unsupervised results for the simple NPs in Lauer's data set.", "labels": [], "entities": [{"text": "NPs in Lauer's data set", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.7572307934363683}]}, {"text": " Table 16  Unsupervised results for the simple NPs in the Penn Treebank data set.", "labels": [], "entities": [{"text": "Penn Treebank data set", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.9957074820995331}]}, {"text": " Table 19  Comparing unsupervised approaches to a supervised model on the development data of the  Penn Treebank simple NP corpus. The last two results groups show a subtractive analysis,  removing individual feature groups from the All features model.", "labels": [], "entities": [{"text": "Penn Treebank simple NP corpus", "start_pos": 99, "end_pos": 129, "type": "DATASET", "confidence": 0.982195007801056}]}, {"text": " Table 20  Test set results for the Penn Treebank simple NP corpus comparing the best supervised and  unsupervised models.", "labels": [], "entities": [{"text": "Penn Treebank simple NP corpus", "start_pos": 36, "end_pos": 66, "type": "DATASET", "confidence": 0.9795457482337951}]}, {"text": " Table 21. The supervised methods significantly outperform  the unsupervised methods, with a matched brackets F-score comparable to the Bikel  (2004) parser's overall performance. We carry out a subtractive analysis of the feature  types, and find that both context feature groups, as well as the semantic, border, POS tag  rule, and parser features all have a negative impact on performance.", "labels": [], "entities": [{"text": "F-score", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9505986571311951}]}, {"text": " Table 22  Results with Bikel (2004) parsed complex NPs over four evaluation measures: (1) matched  brackets (precision, recall, and F-score), (2) accuracy after including all implicit right-branching  brackets, (3) exact NP match, and (4) accuracy over the simple NPs that were bracketed. The third  group of results shows a subtractive analysis, removing individual feature groups from the All  features model. The negative feature groups are removed for the Best results. The final three  rows are calculated over the test set, rather than the development set as in all earlier experiments.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9986911416053772}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9849462509155273}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9947585463523865}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9993426203727722}, {"text": "exact NP match", "start_pos": 216, "end_pos": 230, "type": "METRIC", "confidence": 0.8879185517628988}, {"text": "accuracy", "start_pos": 240, "end_pos": 248, "type": "METRIC", "confidence": 0.9988338351249695}]}, {"text": " Table 23  Performance comparison of suggestion baseline, parser, and the NP bracketer post-processing  the parser's output on development data.", "labels": [], "entities": []}, {"text": " Table 24  Performance comparison of suggestion baseline, parser, and the NP bracketer post-processing  the parser's output on test data.", "labels": [], "entities": []}]}