{"title": [{"text": "Information Status Distinctions and Referring Expressions: An Empirical Study of References to People in News Summaries", "labels": [], "entities": [{"text": "Information Status Distinctions", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6728999813397726}, {"text": "References to People in News Summaries", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.7735826273759207}]}], "abstractContent": [{"text": "Although there has been much theoretical work on using various information status distinctions to explain the form of references in written text, there have been few studies that attempt to automatically learn these distinctions for generating references in the context of computer-regenerated text.", "labels": [], "entities": []}, {"text": "In this article, we present a model for generating references to people in news summaries that incorporates insights from both theory and a corpus analysis of human written summaries.", "labels": [], "entities": [{"text": "generating references to people in news summaries", "start_pos": 40, "end_pos": 89, "type": "TASK", "confidence": 0.6047218569687435}]}, {"text": "In particular, our model captures how two properties of a person referred to in the summary-familiarity to the reader and global salience in the news story-affect the content and form of the initial reference to that person in a summary.", "labels": [], "entities": []}, {"text": "We demonstrate that these two distinctions can be learned from atypical input for multi-document summarization and that they can be used to make regeneration decisions that improve the quality of extractive summaries.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 82, "end_pos": 110, "type": "TASK", "confidence": 0.5342805236577988}]}], "introductionContent": [{"text": "News reports, and consequently news summaries, contain frequent references to the people who participate in the reported events.", "labels": [], "entities": []}, {"text": "Generating referring expressions to people in news summaries is a complex task, however, especially in a multi-document summarization setting where different documents can refer to the same person in different ways.", "labels": [], "entities": [{"text": "Generating referring expressions to people in news summaries", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.8997738435864449}, {"text": "multi-document summarization", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.611211434006691}]}, {"text": "One issue is that the generator has to work with textual input as opposed to closed-domain semantic representations.", "labels": [], "entities": []}, {"text": "More importantly, generating references to people involves issues not generally considered in the referring expression literature.", "labels": [], "entities": []}, {"text": "People have names that usually distinguish them from others in context, and as such, the framework of generating descriptions that rule out distractors (e.g., the body of research building on Dale, including the recent shared tasks based on the TUNA corpus) is not appropriate.", "labels": [], "entities": [{"text": "TUNA corpus", "start_pos": 245, "end_pos": 256, "type": "DATASET", "confidence": 0.9214374721050262}]}, {"text": "Recent GREC challenges have, however, focused on references to named entities, and we compare that body of work to ours in Section 2.2.", "labels": [], "entities": [{"text": "GREC", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.8410860896110535}]}, {"text": "In our domain of summarizing news articles, the aim of generating references to people is to introduce them to the listener and relate them to the story.", "labels": [], "entities": [{"text": "summarizing news articles", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.9386076927185059}]}, {"text": "How much description is needed depends on many factors, including the knowledge the speaker expects the listener to have, the context of the discourse, and the importance of the person to the narrative.", "labels": [], "entities": []}, {"text": "In this article, we explore these three information status distinctions in the context of generating references to people in multi-document summaries of newswire: 1.", "labels": [], "entities": []}, {"text": "discourse-new vs. discourse-old: whether a reference to a person is a first or subsequent mention of that person is purely a property of the text.", "labels": [], "entities": []}], "datasetContent": [{"text": "There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts.", "labels": [], "entities": []}, {"text": "Recent evaluation exercises such as the TUNA challenge therefore consider metrics other than length of a reference, such as humanness and the time taken by hearers to identify the referent.", "labels": [], "entities": [{"text": "TUNA challenge", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.7025498449802399}]}, {"text": "Ina similar vein, examine how similar references produced by well-known algorithms are to human-produced references, and examine differences inhuman behavior when generating referring expressions.", "labels": [], "entities": []}, {"text": "There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop . Recently, several corpora marked for various information status aspects have been made available.", "labels": [], "entities": []}, {"text": "Subsequent studies concerned with predicting givenness status), narrow focus, and rheme and theme distinctions have not been used for generation or summarization tasks.", "labels": [], "entities": [{"text": "generation or summarization tasks", "start_pos": 134, "end_pos": 167, "type": "TASK", "confidence": 0.7992702722549438}]}, {"text": "Current efforts in the language generation community aim at providing a corpus and evaluation task (the GREC challenge) to address just this issue).", "labels": [], "entities": [{"text": "GREC", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.5108963251113892}]}, {"text": "The GREC-2.0 corpus, extracted from Wikipedia articles and annotated for the task of referring expression generation for both first and subsequent mentions of the main subject of the article, consists of 2,000 texts in five different domains (cities, countries, rivers, people, and mountains).", "labels": [], "entities": [{"text": "GREC-2.0 corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8080986738204956}, {"text": "referring expression generation", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.666220635175705}]}, {"text": "The more recent GREC-People corpus consists of 1,000 texts in just one domain (people) but references to all people mentioned in a text have been annotated.", "labels": [], "entities": [{"text": "GREC-People corpus", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.8962962925434113}]}, {"text": "The GREC challenges require systems to pick the most appropriate reference in context from a list of all references in the document and several defaults, including pronouns, common-noun references, elided reference, and standardized versions of names.", "labels": [], "entities": []}, {"text": "By selecting encyclopedic articles about specific referents, this corpus contains large numbers of subsequent references, and in general, the emphasis has been to model the form of subsequent references to named entities in longer texts.", "labels": [], "entities": []}, {"text": "Discourse-new vs. discourse-old is the only information status distinction that participating systems model, with other features derived from lexical and syntactic context; for instance, consider subjecthood, parallelism, recency, and ambiguity.", "labels": [], "entities": []}, {"text": "To evaluate the impact of the rewrite rules on the overall summary, Algorithm 1 was used to rewrite 11 summaries chosen at random from the DUC 2001 and 2002 summaries that contained at least one reference to a person.", "labels": [], "entities": [{"text": "DUC 2001 and 2002 summaries", "start_pos": 139, "end_pos": 166, "type": "DATASET", "confidence": 0.9585418820381164}]}, {"text": "Four human judges (graduate students at Columbia University, two in computational linguistics and two in other areas) were then given the pairs of the original summary and its rewritten variant without being explicitly told which is which.", "labels": [], "entities": []}, {"text": "They were asked to read the summaries and decide if they prefer one text over the other or if they are equal.", "labels": [], "entities": []}, {"text": "They were also asked to give free-form comments on what they would change themselves.", "labels": [], "entities": []}, {"text": "The distribution of the 44 preferences is as follows: 39 (89%) preferences were for the rewritten form, 4 (9%) were for the original, and there was no preference for the remaining 1 (2%).", "labels": [], "entities": []}, {"text": "This preference for the rewritten form is significant (z-test; p < 0.01).", "labels": [], "entities": []}, {"text": "In two out of the four cases where the assessor preferred the original version, they commented that the reason for the preference was that the original version exhibited more variation.", "labels": [], "entities": []}, {"text": "This observation indicates that the rule for strictly using surname at discourse-old references is too rigid and most probably will need modification in cases where a person is mentioned more often.", "labels": [], "entities": []}, {"text": "Having created labeled data for classifying people as hearer-new or hearer-old and as major or minor characters, we now proceed to learn these distinctions in a supervised framework.", "labels": [], "entities": []}, {"text": "For our experiments, we used the WEKA machine learning toolkit and obtained the best results for hearer-old/hearer-new using a support vector machine (Sequential Minimal Optimization algorithm, with default parameters) and for major/minor, a tree-based classifier (J48, with WEKA parameters: \"J48 -U -M 4\").", "labels": [], "entities": [{"text": "WEKA machine learning toolkit", "start_pos": 33, "end_pos": 62, "type": "DATASET", "confidence": 0.7725083529949188}, {"text": "WEKA", "start_pos": 275, "end_pos": 279, "type": "DATASET", "confidence": 0.7999041676521301}, {"text": "J48 -U -M 4", "start_pos": 293, "end_pos": 304, "type": "DATASET", "confidence": 0.7393995920817057}]}, {"text": "We now discuss what features we used for our two classification tasks (see the list of features in).", "labels": [], "entities": []}, {"text": "Our hypothesis is that features capturing the frequency and syntactic and lexical forms of references are sufficient to infer the desired distinctions.", "labels": [], "entities": []}, {"text": "The frequency features are likely to give a good indication of the global salience of a person in the document set.", "labels": [], "entities": []}, {"text": "Pronominalization indicates that an entity was particularly salient at a specific point of the discourse, as has been widely discussed in attentional status and centering literature (.", "labels": [], "entities": []}, {"text": "Modified noun phrases (with apposition, relative clauses, or pre-modification) can also signal different information status; for instance, we expect post-modification to be more prevalent for characters who are less familiar.", "labels": [], "entities": []}, {"text": "For our lexical features, we used two months worth of news articles collected over the Web (and independent of the DUC collection) to collect unigram and bigram lexical models of discourse-new references of people.", "labels": [], "entities": [{"text": "DUC collection", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.9633329212665558}]}, {"text": "The names themselves were removed from the discourse-new reference noun phrases and the counts were collected over the pre-modifiers only.", "labels": [], "entities": []}, {"text": "One of the lexical features we used is whether a person's description contains any of the 20 most frequent description words from our Web corpus.", "labels": [], "entities": []}, {"text": "We reasoned that these frequent descriptors may signal importance; the full list is: president, former, spokesman, sen, dr, chief, coach, attorney, minister, director, gov, rep, leader, secretary, rev, judge, US, general, manager, chairman We also used features based on the overall likelihood of a person's description using the bigram model from our Web corpus.", "labels": [], "entities": []}, {"text": "These features can help indicate whether a person has a role or affiliation that is important.", "labels": [], "entities": []}, {"text": "In the experiments reported subsequently, all our features are derived exclusively from the input documents, and we do not derive any features from the summaries.", "labels": [], "entities": []}, {"text": "We performed 20-fold cross validation for both classification tasks.", "labels": [], "entities": []}, {"text": "We evaluated our algorithm using 14 news clusters from the Google News world news section.", "labels": [], "entities": [{"text": "Google News world news section", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.8456161975860595}]}, {"text": "We selected these clusters in the order they were presented on the Google News site; we excluded three clusters that we deemed too similar to already selected Rewrite rules for discourse-new references: IF the person's affiliation has already been mentioned AND is the most salient organization in the discourse at the point where the reference needs to be generated THEN EXCLUDE affiliation ELSE INCLUDE Affiliation iv. Use machine learner described in Section 5.2 to decide whether to include post-modification 2.", "labels": [], "entities": []}, {"text": "ELSE IF The name is not the head of the noun phrase it appears in, THEN it is not rewritten Rewrite rules for discourse-old references: 1.", "labels": [], "entities": [{"text": "ELSE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9831799268722534}, {"text": "THEN", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9882128834724426}]}, {"text": "Use surname only, EXCLUDE all pre-modifiers and post-modifiers Algorithm 4: Generating references to people in news summaries.", "labels": [], "entities": [{"text": "EXCLUDE", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.8907265663146973}, {"text": "Generating references to people in news summaries", "start_pos": 76, "end_pos": 125, "type": "TASK", "confidence": 0.8087440643991742}]}, {"text": "We used the first 10 articles in each cluster to create our 14 document sets.", "labels": [], "entities": []}, {"text": "We then used the freely available extractive summarizer MEAD () to generate 200 word summaries for each document set.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.8990066051483154}]}, {"text": "These extractive summaries were automatically rewritten according to Algorithm 4, as described subsequently.", "labels": [], "entities": []}, {"text": "Our evaluation compares the extractive and rewritten summaries.", "labels": [], "entities": []}, {"text": "Our reference rewrite module operates on parse trees obtained using the Stanford Parser (.", "labels": [], "entities": []}, {"text": "For each person automatically identified using the techniques described in Section 4.1.1, we matched every mention of their surname in the parse trees of MEAD summary sentences.", "labels": [], "entities": [{"text": "MEAD summary sentences", "start_pos": 154, "end_pos": 176, "type": "DATASET", "confidence": 0.7373424768447876}]}, {"text": "We then replaced the enclosing NP (includes all pre-and post-modifying constituents) with anew NP generated using Algorithm 4.", "labels": [], "entities": []}, {"text": "The regenerated summary was produced automatically, without any manual correction of parses, semantic analyses, or information status classifications.", "labels": [], "entities": []}, {"text": "We now enumerate implementation details not covered in Section 4.1.1: 1.", "labels": [], "entities": []}, {"text": "Although our algorithm determines when to include role and affiliation attributes, it doesn't inform us as to which ones to include; for instance, a politician might have a constituency, party, or nationality affiliation.", "labels": [], "entities": []}, {"text": "Our implementation selects the most frequently mentioned role (e.g., prime minister) in references to that person in the document set, and then selects the affiliation associated with that role (e.g., British).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Likelihood of reference forms for discourse-new and discourse-old references in DUC  multi-document track news clusters.", "labels": [], "entities": [{"text": "DUC  multi-document track news clusters", "start_pos": 90, "end_pos": 129, "type": "DATASET", "confidence": 0.8708092451095581}]}, {"text": " Table 3  Cross-validation Accuracy and P/R/F results for hearer-old vs. hearer-new predictions (258 data  points). The improvement in accuracy of SMO over the baseline is statistically significant (z-test;  p < 0.01).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9539225697517395}, {"text": "P/R/F", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.8880431771278381}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9991621971130371}, {"text": "SMO", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9628904461860657}]}, {"text": " Table 4  Cross-validation Accuracy and P/R/F results for major vs. minor predictions (4,184 data points).  The improvement in accuracy of J48 over the baseline is statistically significant (z-test; p < 0.01).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.949802815914154}, {"text": "P/R/F", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.8752593636512757}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9993646740913391}]}, {"text": " Table 5  J48 Recall results and human agreement for major vs. minor classifications.", "labels": [], "entities": [{"text": "J48", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6991280913352966}, {"text": "Recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.7850130796432495}, {"text": "agreement", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.8753360509872437}]}, {"text": " Table 6  Accuracy, precision, and recall for Newsblaster data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9880121946334839}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9995797276496887}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9997346997261047}, {"text": "Newsblaster data", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9751943051815033}]}, {"text": " Table 8  Summary of the accuracy of Algorithm 4 for specific regeneration decisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995686411857605}]}, {"text": " Table 9  Results for the evaluation for rewritten summaries.", "labels": [], "entities": []}]}