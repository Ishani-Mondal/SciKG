{"title": [{"text": "A New Unsupervised Approach to Word Segmentation", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7330581545829773}]}], "abstractContent": [{"text": "This article proposes ESA, anew unsupervised approach to word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.747518390417099}]}, {"text": "ESA is an iterative process consisting of three phases: Evaluation, Selection, and Adjustment.", "labels": [], "entities": []}, {"text": "In Evaluation , both the certainty and uncertainty of character sequence co-occurrence in corpora are considered as statistical evidence supporting goodness measurement.", "labels": [], "entities": [{"text": "certainty", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9966899156570435}]}, {"text": "Additionally, the statistical data of character sequences with various lengths become comparable with each other by using a simple process called Balancing.", "labels": [], "entities": [{"text": "Balancing", "start_pos": 146, "end_pos": 155, "type": "TASK", "confidence": 0.9283148646354675}]}, {"text": "In Selection, a local maximum strategy is adopted without thresholds, and the strategy can be implemented with dynamic programming.", "labels": [], "entities": []}, {"text": "In Adjustment, apart of the statistical data is updated to improve successive results.", "labels": [], "entities": [{"text": "Adjustment", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.9440036416053772}]}, {"text": "In our experiment, ESA was evaluated on the SIGHAN Bakeoff-2 data set.", "labels": [], "entities": [{"text": "ESA", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7061038613319397}, {"text": "SIGHAN Bakeoff-2 data set", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.9397813528776169}]}, {"text": "The results suggest that ESA is effective on Chinese corpora.", "labels": [], "entities": []}, {"text": "It is noteworthy that the F-measures of the results are basically monotone increasing and can rapidly converge to relatively high values.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9980195760726929}]}, {"text": "Furthermore, empirical formulae based on the results can be used to predict the parameter in ESA to avoid parameter estimation that is usually time-consuming.", "labels": [], "entities": [{"text": "ESA", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.682811439037323}]}], "introductionContent": [{"text": "Word segmentation is an important task in natural language processing (NLP) for languages without word delimiters (e.g., Chinese).", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7055616080760956}, {"text": "natural language processing (NLP)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.8341913819313049}]}, {"text": "To date, most existing approaches to Chinese word segmentation (CWS) are supervised.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.7781730542580286}]}, {"text": "Although supervised approaches reach higher accuracy than unsupervised ones in many cases, they involve much more human effort.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9971746206283569}]}, {"text": "Furthermore, unsupervised approaches are more adaptive to relatively unfamiliar languages for which we do not have enough linguistic knowledge.", "labels": [], "entities": []}, {"text": "In addition, unsupervised approaches can cooperate with supervised ones to overcome drawbacks of both.", "labels": [], "entities": []}, {"text": "Since introduced mutual information (MI) to word segmentation, some researchers have conducted research on unsupervised approaches to word segmentation.", "labels": [], "entities": [{"text": "mutual information (MI)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7030348658561707}, {"text": "word segmentation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7414778918027878}, {"text": "word segmentation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7554925978183746}]}, {"text": "proposed an unsupervised approach based on an improved expectation maximum (EM) learning algorithm and a pruning algorithm based on MI.", "labels": [], "entities": []}, {"text": "Their approach outperforms softcounting) that is also based on EM and MI.", "labels": [], "entities": []}, {"text": "Non-parametric Bayesian techniques-for example, the Pitman-Yor process (PYP, a generalization of the Dirichlet process, DP), hierarchical DP (HDP) (), hierarchical PYP (HPYP)), and hierarchical HPYP (HHPYP)-have been introduced to word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 231, "end_pos": 248, "type": "TASK", "confidence": 0.7548658847808838}]}, {"text": "proposed a novel unsupervised approach based on HPYP, and evaluated it on apart of SIGHAN Bakeoff-2 data set.", "labels": [], "entities": [{"text": "HPYP", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.796576976776123}, {"text": "SIGHAN Bakeoff-2 data set", "start_pos": 83, "end_pos": 108, "type": "DATASET", "confidence": 0.9270142316818237}]}, {"text": "Their evaluation results suggested that their approach outperformed the previous ones.", "labels": [], "entities": []}, {"text": "Some approaches, such as TONGO ( and Voting Experts, are based on relatively simple ideas.", "labels": [], "entities": [{"text": "TONGO", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.3955170810222626}]}, {"text": "In most cases, an unsupervised approach can be viewed as a kind of goodness measurement to find boundaries between words or filter words from candidates or both.", "labels": [], "entities": []}, {"text": "There are four goodness algorithms reviewed by.", "labels": [], "entities": []}, {"text": "The algorithms, including Description Length Gain (DLG), Accessor Variety (, and Branching Entropy (), were evaluated on SIGHAN Bakeoff-3 data set).", "labels": [], "entities": [{"text": "SIGHAN Bakeoff-3 data set", "start_pos": 121, "end_pos": 146, "type": "DATASET", "confidence": 0.893472746014595}]}, {"text": "In this article, we propose ESA, anew unsupervised approach to word segmentation, and demonstrate its effectiveness on Chinese corpora.", "labels": [], "entities": [{"text": "ESA", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.7951157689094543}, {"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7516369819641113}]}, {"text": "The approach was motivated by the following considerations: 1.", "labels": [], "entities": []}, {"text": "In contrast to the semi-supervised or supervised approaches, we want to find an approach which produces acceptable results under harsh conditions.", "labels": [], "entities": []}, {"text": "The harsh conditions are lack of prior knowledge, namely, no lexicons, annotated corpora, or linguistic rules.", "labels": [], "entities": []}, {"text": "The acceptability involves comparison with the gold standards, which usually means the manually segmented results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation is the phase that gives a character sequence or a pair of adjacent subsequences a goodness value according to statistical information.", "labels": [], "entities": []}, {"text": "There are three issues to be settled: 1.", "labels": [], "entities": []}, {"text": "What are the character sequence and the pair of adjacent subsequences that can be evaluated?", "labels": [], "entities": []}, {"text": "The experiment uses the SIGHAN Bakeoff-2 data set that is publicly available on the SIGHAN Web site (www.sighan.org).", "labels": [], "entities": [{"text": "SIGHAN Bakeoff-2 data set", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.8213652521371841}, {"text": "SIGHAN Web site", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.8472099900245667}]}, {"text": "We tested ESA with various settings.", "labels": [], "entities": [{"text": "ESA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5267755389213562}]}, {"text": "In this section, we describe the test settings, report experimental results, and discuss those results.", "labels": [], "entities": []}, {"text": "According to the experiment, we establish the empirical formulae to predict the exponent in LRV.", "labels": [], "entities": [{"text": "LRV", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9253260493278503}]}], "tableCaptions": [{"text": " Table 1  The scales of corpora.", "labels": [], "entities": []}, {"text": " Table 2  The results of setting 1 (Punctuation and other encoding information are not used; the maximum  length is 30).", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9722270369529724}]}, {"text": " Table 3  The results of setting 2 (Punctuation and other encoding information are not used; the maximum  length is 10).", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.968330442905426}]}, {"text": " Table 4  The results of setting 3 (Punctuation is used; the maximum length is 30).", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9808340668678284}]}, {"text": " Table 5  The results of setting 4 (Punctuation and other encoding information are used; the maximum  length is 30).", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9442945122718811}]}, {"text": " Table 6  The results brought by different maximum lengths.", "labels": [], "entities": []}, {"text": " Table 7  The empirical formulae for the prediction (linear model).", "labels": [], "entities": [{"text": "prediction", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.9582236409187317}]}, {"text": " Table 8  The comparison between NPYLM and ESA.", "labels": [], "entities": [{"text": "NPYLM", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8791424036026001}, {"text": "ESA", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8817009925842285}]}, {"text": " Table 9  The comparison between DLG, AV, BE, and ESA.", "labels": [], "entities": [{"text": "BE", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9901907444000244}]}]}