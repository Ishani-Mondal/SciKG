{"title": [], "abstractContent": [{"text": "In this paper we present an approach to automatic authorship attribution dealing with real-world (or unrestricted) text.", "labels": [], "entities": [{"text": "automatic authorship attribution", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.6005387902259827}]}, {"text": "Our method is based on the computational analysis of the input text using a text-processing tool.", "labels": [], "entities": []}, {"text": "Besides the style markers relevant to the output of this tool we also use analysis-dependent style markers, that is, measures that represent the way in which the text has been processed.", "labels": [], "entities": []}, {"text": "No word frequency counts, nor other lexically-based measures are taken into account.", "labels": [], "entities": []}, {"text": "We show that the proposed set of style markers is able to distinguish texts of various authors of a weekly newspaper using multiple regression.", "labels": [], "entities": []}, {"text": "All the experiments we present were performed using real-world text downloaded from the World Wide Web.", "labels": [], "entities": []}, {"text": "Our approach is easily trainable and fully-automated requiring no manual text preprocessing nor sampling.", "labels": [], "entities": []}], "introductionContent": [{"text": "The vast majority of the attempts to computerassisted authorship attribution has been focused on literary texts.", "labels": [], "entities": [{"text": "computerassisted authorship attribution", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.6005823612213135}]}, {"text": "In particular, a lot of attention has been paid to the establishment of the authorship of anonymous or doubtful texts.", "labels": [], "entities": []}, {"text": "A typical paradigm is the case of the Federalist papers twelve of which are of disputed authorship.", "labels": [], "entities": [{"text": "Federalist papers", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.7261350750923157}]}, {"text": "Moreover, the lack of a generic and formal definition of the idiosyncratic style of an author has led to the employment of statistical methods (e.g., discriminant analysis, principal components, etc.).", "labels": [], "entities": []}, {"text": "Nowadays, the wealth of text available in the World Wide Web in electronic form fora wide variety of genres and languages, as well as the development of reliable text-processing tools open the way for the solution of the authorship attribution problem as regards real-world text.", "labels": [], "entities": []}, {"text": "The most important approaches to authorship attribution involve lexically based measures.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8531235456466675}]}, {"text": "A lot of style markers have been proposed for measuring the richness of the vocabulary used by the author.", "labels": [], "entities": []}, {"text": "For example, the type-token ratio, the hapax legomena (i.e., once-occurring words), the hapax dislegomena (i.e., twiceoccurring words), etc.", "labels": [], "entities": []}, {"text": "There are also functions that make use of these measures such as Yule's K), Honore's R, etc.", "labels": [], "entities": [{"text": "Yule's K)", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.867768406867981}, {"text": "Honore's R", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.7412630915641785}]}, {"text": "A review of this metrics can be found in.", "labels": [], "entities": []}, {"text": "In ( five vocabulary richness functions were used in the framework of a multivariate statistical analysis of the Federalist papers and a principal components analysis was performed.", "labels": [], "entities": [{"text": "Federalist papers", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.8911888003349304}]}, {"text": "All the disputed papers lie in the side of James Madison (rather than Alexander Hamilton) in the space of the first two principal components.", "labels": [], "entities": []}, {"text": "However, such measures require the development of large lexicons with specialized information in order to detect the various forms of the lexical units that constitute an author's vocabulary.", "labels": [], "entities": []}, {"text": "For languages with a rich morphology, i.e. Modem Greek, this is an important shortcoming.", "labels": [], "entities": []}, {"text": "Instead of counting how many words occur certain number of times, proposed the use of a set of common function (or context-free) word frequencies in the sample text.", "labels": [], "entities": []}, {"text": "This method combined with a principal components analysis achieved remarkable results when applied to a wide variety of authors.", "labels": [], "entities": []}, {"text": "On the other hand, a lot of effort is required regarding the selection of the most appropriate set of words that best distinguish a given set of authors.", "labels": [], "entities": []}, {"text": "Moreover, all the lexicallybased style markers are highly author and language dependent.", "labels": [], "entities": []}, {"text": "The results of a work using such measures, therefore, cannot be applied to a different group of authors nor another language.", "labels": [], "entities": []}, {"text": "In order to avoid the problems of lexicallybased measures, proposed the use of syntax-based ones.", "labels": [], "entities": []}, {"text": "This approach is based on the frequencies of the rewrite rules as they appear in a syntactically annotated corpus.", "labels": [], "entities": []}, {"text": "Both high-frequent and low-frequent rewrite rules give accuracy results comparable to lexically-based methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9988515377044678}]}, {"text": "However, the computational analysis is considered as a significant limitation of this method since the required syntactic annotation scheme is very complicated and current text-processing tools are not capable of providing automatically such information, especially in the case of unrestricted text.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no computational system for the automatic detection of authorship dealing with real-world text.", "labels": [], "entities": [{"text": "automatic detection of authorship dealing with real-world text", "start_pos": 71, "end_pos": 133, "type": "TASK", "confidence": 0.8244275897741318}]}, {"text": "In thispaper, we present an approach to this problem.", "labels": [], "entities": []}, {"text": "In particular, our aim is the discrimination between the texts of various authors of a Modem Greek weekly newspaper.", "labels": [], "entities": [{"text": "Modem Greek weekly newspaper", "start_pos": 87, "end_pos": 115, "type": "DATASET", "confidence": 0.9076650142669678}]}, {"text": "We use an already existing text processing tool able to detect sentence and chunk boundaries in unrestricted text for the extraction of style markers.", "labels": [], "entities": []}, {"text": "Instead of trying to minimize the computational analysis of the text, we attempt to take advantage of this procedure.", "labels": [], "entities": []}, {"text": "In particular, we use a set of analysis-level style markers, i.e., measures that represent the way in which the text has been processed by the tool.", "labels": [], "entities": []}, {"text": "For example, a useful measure is the percentage of the sample text remaining unanalyzed after the automatic processing.", "labels": [], "entities": []}, {"text": "In other words, we attempt to adapt the set of the style markers to the method used by the sentence and chunk detector in order to analyze the sample text.", "labels": [], "entities": []}, {"text": "The statistical technique of multiple regression is, then, used for extracting a linear combination of the values of the style markers that manages to distinguish the different authors.", "labels": [], "entities": []}, {"text": "The experiments we present, for both author identification and author verification tasks, were performed using real-world text downloaded from the World Wide Web.", "labels": [], "entities": [{"text": "author identification", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8558696210384369}, {"text": "author verification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7205563187599182}]}, {"text": "Our approach is easily trainable and fully automated requiring no manual text preprocessing nor sampling.", "labels": [], "entities": []}, {"text": "A brief description of the extraction of the style markers is given in section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the composition of the corpus of realworld text used in the experiments.", "labels": [], "entities": []}, {"text": "The training procedure is given in section 4 while section 5 comprises analytical experimental results.", "labels": [], "entities": []}, {"text": "Finally, in section 6 some conclusions are drawn and future work directions are given.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. The Corpus Consisting of Texts Taken from the Weekly Newspaper TO BHMA.", "labels": [], "entities": [{"text": "The Corpus Consisting of Texts Taken from the Weekly Newspaper TO BHMA", "start_pos": 10, "end_pos": 80, "type": "DATASET", "confidence": 0.6645330861210823}]}, {"text": " Table 2. Statistics of the Regression Functions.", "labels": [], "entities": []}, {"text": " Table 3. Confusion Matrix of the Author Identification Experiment.", "labels": [], "entities": [{"text": "Author Identification", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.6879969239234924}]}, {"text": " Table 4. Author Verification Results  \"threshold=R/2).", "labels": [], "entities": [{"text": "R/2", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9442153771718343}]}]}