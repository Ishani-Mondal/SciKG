{"title": [{"text": "New Models for Improving Supertag Disambiguation", "labels": [], "entities": [{"text": "Improving Supertag Disambiguation", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.8078823884328207}]}], "abstractContent": [{"text": "In previous work, supertag disambigua-tion has been presented as a robust, partial parsing technique.", "labels": [], "entities": []}, {"text": "In this paper we present two approaches: contextual models, which exploit a variety of features in order to improve supertag performance , and class-based models, which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.989080011844635}]}], "introductionContent": [{"text": "Many natural language applications are beginning to exploit some underlying structure of the language. and use structure-based language models in the context of speech applications. and use phrasal information in information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 213, "end_pos": 235, "type": "TASK", "confidence": 0.818274050951004}]}, {"text": "uses dependency information in a machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7146004140377045}]}, {"text": "The need to impose structure leads to the need to have robust parsers.", "labels": [], "entities": []}, {"text": "There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as,, and) and Statistical Parsing (such as,, and).", "labels": [], "entities": [{"text": "Statistical Parsing", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8527767658233643}]}, {"text": "Srinivas (1997a) has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques.", "labels": [], "entities": []}, {"text": "The idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (Supertags) that impose complex constraints in a local context.", "labels": [], "entities": []}, {"text": "Supertag disambiguation is resolved \"Supported by NSF grants ~SBR-9710411 and ~GER-9354869 by using statistical distributions of supertag cooccurrences collected from a corpus of parses.", "labels": [], "entities": [{"text": "Supertag disambiguation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8531041145324707}, {"text": "GER-9354869", "start_pos": 79, "end_pos": 90, "type": "DATASET", "confidence": 0.8263577818870544}]}, {"text": "It results in a representation that is effectively a parse (almost parse).", "labels": [], "entities": []}, {"text": "Supertagging has been found useful fora number of applications.", "labels": [], "entities": [{"text": "Supertagging", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8951002359390259}]}, {"text": "For instance, it can be used to speedup conventional chart parsers because it reduces the ambiguity which a parser must face, as described in. has shown that supertagging maybe employed in information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 189, "end_pos": 210, "type": "TASK", "confidence": 0.8138412237167358}]}, {"text": "Furthermore, given a sentence aligned parallel corpus of two languages and almost parse information for the sentences of one of the languages, one can rapidly develop a grammar for the other language using supertagging, as suggested by.", "labels": [], "entities": []}, {"text": "In contrast to the aforementioned work in supertag disambiguation, where the objective was to provide a-direct comparison between trigram models for part-of-speech tagging and supertagging, in this paper our goal is to improve the performance of supertagging using local techniques which avoid full parsing.", "labels": [], "entities": [{"text": "supertag disambiguation", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.9419040083885193}, {"text": "part-of-speech tagging", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.7052152454853058}]}, {"text": "These supertag disambiguation models can be grouped into contextual models and class based models.", "labels": [], "entities": []}, {"text": "Contextual models use different features in frameworks that exploit the information those features provide in order to achieve higher accuracies in supertagging.", "labels": [], "entities": []}, {"text": "For class based models, supertags are first grouped into clusters and words are tagged with clusters of supertags.", "labels": [], "entities": []}, {"text": "We develop several automated clustering techniques.", "labels": [], "entities": []}, {"text": "We then demonstrate that with a slight increase in supertag ambiguity that supertagging accuracy can be substantially improved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9940470457077026}]}, {"text": "The layout of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly review the task of supertagging and the results from previous work.", "labels": [], "entities": [{"text": "supertagging", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9685490131378174}]}, {"text": "In Section 3, we explore contextual models.", "labels": [], "entities": []}, {"text": "In Section 4, we outline various class based approaches.", "labels": [], "entities": []}, {"text": "Ideas for future work are presented in Section 5.", "labels": [], "entities": []}, {"text": "Lastly, we v Proceedings of EACL '99 present our conclusions in Section 6.", "labels": [], "entities": [{"text": "Proceedings of EACL '99", "start_pos": 13, "end_pos": 36, "type": "DATASET", "confidence": 0.9442934393882751}]}], "datasetContent": [{"text": "We also compare various models' performance on base-NP detection and PP attachment disambiguation.", "labels": [], "entities": [{"text": "base-NP detection", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7248168289661407}, {"text": "PP attachment disambiguation", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.789144237836202}]}, {"text": "The results will underscore the adroitness of the classifier combination model in using both local and long distance features.", "labels": [], "entities": []}, {"text": "They will also show that, depending on the ultimate application, one model maybe more appropriate than another model.", "labels": [], "entities": []}, {"text": "A base-NP is a non-recursive NP structure whose detection is useful in many applications, such as information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8921831846237183}]}, {"text": "We extend our supertagging models to perform this task in a fashion similar to that described in.", "labels": [], "entities": []}, {"text": "Selected models have been trained on 200K words.", "labels": [], "entities": []}, {"text": "Subsequently, after a model has supertagged the test corpus, a procedure detects base-NPs by scanning for appropriate sequences of supertags.", "labels": [], "entities": []}, {"text": "Results for base-NP detection are shown in.", "labels": [], "entities": []}, {"text": "Note that the mixed model performs nearly as well as the trigram model.", "labels": [], "entities": []}, {"text": "Note also that the head trigram model is outperformed by the other models.", "labels": [], "entities": []}, {"text": "We suspect that unlike the trigram model, the head model does not perform the accurate modeling of local context which is important for base-NP detection.", "labels": [], "entities": [{"text": "base-NP detection", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7365959286689758}]}, {"text": "In contrast, information about long distance dependencies are more important for the the PP attachment task.", "labels": [], "entities": [{"text": "PP attachment task", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8933919072151184}]}, {"text": "In this task, a model must decide whether a PP attaches at the NP or the VP level.", "labels": [], "entities": []}, {"text": "This corresponds to a choice between two PP supertags: one associated with NP attachment, and another associated with VP attachment.", "labels": [], "entities": []}, {"text": "The trigram model, head trigram model, 3-gram mixed model, and classifier combination model perform at accuracies of 78.53%, 79.56%, 80.16%, and 82.10%, respectively, on the PP at-: Some contextual models' results on base-NP chunking tachment task.", "labels": [], "entities": []}, {"text": "As maybe expected, the trigram model performs the worst on this task, presumably because it is restricted to considering purely local information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: In the 3-gram mixed model, previous con- ditioning context and the current supertag deter- ministically establish the next conditioning con-", "labels": [], "entities": []}, {"text": " Table 1. Intuitively, the mixed  model is like the trigram model except that a mod- ifier tag is discarded from the conditioning context  when it has found an object of modification. The  mixed model achieves an accuracy of 91.79%, a  significant improvement over both the head tri- gram model's and the trigram model's accuracies,  p < 0.05. Furthermore, this mixed model is com- putationally more efficient as well as more accu- rate than the 5-gram model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9993768334388733}, {"text": "accu- rate", "start_pos": 426, "end_pos": 436, "type": "METRIC", "confidence": 0.9551049073537191}]}, {"text": " Table 3: Accuracies of Single Classifiers and Pairwise Combination of Classifiers.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9884823560714722}]}, {"text": " Table 4: Some contextual models' results on base- NP chunking", "labels": [], "entities": [{"text": "base- NP chunking", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6851658672094345}]}]}