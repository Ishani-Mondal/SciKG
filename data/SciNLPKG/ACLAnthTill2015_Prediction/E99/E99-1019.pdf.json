{"title": [{"text": "Exploring the Use of Linguistic Features in Domain and Genre Classification", "labels": [], "entities": [{"text": "Domain and Genre Classification", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6234128177165985}]}], "abstractContent": [{"text": "The central questions are: How useful is information about part-of-speech frequency for text categorisation?", "labels": [], "entities": [{"text": "text categorisation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.8431667387485504}]}, {"text": "Is it feasible to limit word features to content words for text classifications?", "labels": [], "entities": []}, {"text": "This is examined for 5 domain and 4 genre classification tasks using LIMAS, the Ger-man equivalent of the Brown corpus.", "labels": [], "entities": [{"text": "genre classification", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7099571526050568}, {"text": "Brown corpus", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.8236921429634094}]}, {"text": "Because LIMAS is too heterogeneous, neither question can be answered reliably for any of the tasks.", "labels": [], "entities": [{"text": "LIMAS", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.6530033946037292}]}, {"text": "However, the results suggest that both questions have to be examined separately for each task at hand, because in some cases, the additional information can indeed improve performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The greater the amounts of text people can access and have to process, the more important efficient methods for text categorisation become.", "labels": [], "entities": [{"text": "text categorisation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8085657954216003}]}, {"text": "So far, most research has concentrated on contentbased categories.", "labels": [], "entities": []}, {"text": "But determining the genre of a text can also be very important, for example when having to distinguish an EU press release on the introduction of the euro from a newspaper commentary on the same topic.", "labels": [], "entities": []}, {"text": "The results of e.g. () indicate that for good content classification, we basically need a vector which contains the most relevant words of the text.", "labels": [], "entities": [{"text": "content classification", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7604262232780457}]}, {"text": "Using n-grams hardly yields significant improvements, because the dimension of the document representation space increases exponentially.", "labels": [], "entities": []}, {"text": "But do wordbased vectors also work well for genre detection?", "labels": [], "entities": [{"text": "genre detection", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.9312807023525238}]}, {"text": "Or do we need additional linguistically motivated features to capture the different styles of writing associated with different genres?", "labels": [], "entities": []}, {"text": "In this paper, we present a pilot study based on a set of easily computable linguistic features, namely the frequency of part-of-speech (POS) tags, and a corpus of German, LIMAS, which contains a wide range of different genres.", "labels": [], "entities": []}, {"text": "LIMAS is described briefly in Sac.", "labels": [], "entities": [{"text": "LIMAS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.43586835265159607}, {"text": "Sac.", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9373496472835541}]}, {"text": "3, while sections 2 and 4 motivate the choice of features.", "labels": [], "entities": []}, {"text": "The text categorisation experiments are described in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our categorisation experiments, we chose a relational k-nearest-neighbour (k-NN) classifier, RIBL), and two feature-based k-NN algorithms, learning vector quantisation (LVQ, (), and IBLI(-IG) (.", "labels": [], "entities": [{"text": "RIBL", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.8509546518325806}, {"text": "IBLI", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.91895592212677}]}, {"text": "The reason for choosing k-NN-based approaches is that this algorithm has been very successful in text categorisation (Yang, 1997).", "labels": [], "entities": [{"text": "text categorisation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8251390159130096}]}, {"text": "We first ran the experiments on the LPEcorpus, which had mainly exploratory character, then on the complete corpus.", "labels": [], "entities": [{"text": "LPEcorpus", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.9336740970611572}, {"text": "complete corpus", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.8001101911067963}]}, {"text": "In the LPE-experiments, we distinguished six feature sets: CW, CWPOS, CWPP, WS, WS-POS, and WSPP, where CW stands for content word lemmata, WS for all lemmata, POS for POS information, and PP for POS and punctuation information.", "labels": [], "entities": []}, {"text": "In the CL-experiments, we did not control for the potential contribution of punctuation features to the results, but on the type of lemma from which the features were derived.", "labels": [], "entities": []}, {"text": "We again explored 6 feature sets, CW, CWPOS, WS, WSPOS, FW, and FWPOS, where FW stands for function word lemmata.", "labels": [], "entities": [{"text": "FW", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9377663731575012}, {"text": "FWPOS", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8830719590187073}]}, {"text": "Punctuation was included in conditions WS, WSPOS, FW, and FWPOS, but not in CW and CWPOS.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9812522530555725}, {"text": "WSPOS", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.5424915552139282}, {"text": "FW", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.7044793367385864}, {"text": "FWPOS", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.8027679324150085}, {"text": "CW and CWPOS", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.6605413556098938}]}, {"text": "In addition to feature type, we also varied the length of the feature vectors.", "labels": [], "entities": []}, {"text": "In the following subsections, we outline our general method for feature selection and evaluation and give a brief description of the algorithms used.", "labels": [], "entities": [{"text": "feature selection and evaluation", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.7202830463647842}]}, {"text": "We then report on the results of the two suites of experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents n in each category  and classification accuracy acc. if each document  is judged not to belong to that category.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9909035563468933}]}, {"text": " Table 2: Test set performance averaged over all  runs for each task and for the best combination of  feature set and number of features, precision and  recall having equal weight.", "labels": [], "entities": [{"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9995809197425842}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9995173215866089}]}, {"text": " Table 3: Average LVQ results (precision) for cate- gories H and S, 50 codebook vectors, even initial- ization.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9967722296714783}]}]}