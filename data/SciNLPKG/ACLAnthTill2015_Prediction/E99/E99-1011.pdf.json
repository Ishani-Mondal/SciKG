{"title": [{"text": "The TIPSTER SUMMAC Text Summarization Evaluation", "labels": [], "entities": [{"text": "TIPSTER SUMMAC Text Summarization", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.591386616230011}]}], "abstractContent": [{"text": "The TIPSTER Text Summarization Evaluation (SUMMAC) has established definitively that automatic text summa-rization is very effective in relevance assessment tasks.", "labels": [], "entities": [{"text": "TIPSTER Text Summarization", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.5797402858734131}]}, {"text": "Summaries as short as 17% of full text length sped up decision-making by almost a factor of 2 with no statistically significant degradation in F-score accuracy.", "labels": [], "entities": [{"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9827991127967834}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.6643823981285095}]}, {"text": "SUMMAC has also introduced anew intrinsic method for automated evaluation of informative summaries .", "labels": [], "entities": [{"text": "SUMMAC", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8466668725013733}]}], "introductionContent": [{"text": "In May 1998, the U.S. government completed the TIPSTER Text Summarization Evaluation (SUMMAC), which was the first large-scale, developer-independent evaluation of automatic text summarization systems.", "labels": [], "entities": [{"text": "TIPSTER Text Summarization Evaluation (SUMMAC)", "start_pos": 47, "end_pos": 93, "type": "TASK", "confidence": 0.7604958968503135}, {"text": "text summarization", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.6475938260555267}]}, {"text": "The goals of the SUMMAC evaluation were to judge individual summarization systems in terms of their usefulness in specific summarization tasks and to gain a better understanding of the issues involved in building and evaluating such systems.", "labels": [], "entities": [{"text": "SUMMAC", "start_pos": 17, "end_pos": 23, "type": "TASK", "confidence": 0.9052839279174805}, {"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9562399387359619}]}], "datasetContent": [{"text": "Methods for evaluating text summarization can be broadly classified into two categories.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.720275416970253}]}, {"text": "The first, an intrinsic (or normative) evaluation, judges the quality of the summary directly based on analysis in terms of some set of norms.", "labels": [], "entities": []}, {"text": "This can involve user judgments of fluency of the summary (,, coverage of stipulated \"key/essential ideas\" in the source (Paice 1990), (, or similarity to an \"ideal\" summary, e.g.,, ().", "labels": [], "entities": []}, {"text": "The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish.", "labels": [], "entities": []}, {"text": "There can be a large number of generic and topic-related abstracts that could summarize a given document.", "labels": [], "entities": []}, {"text": "Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g.,,), although judges may agree more on the most important sentences to include).", "labels": [], "entities": []}, {"text": "The second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the completion of some other task.", "labels": [], "entities": []}, {"text": "There have been a number of extrinsic evaluations, including question-answering and comprehension tasks, e.g.,), as welt as tasks which measure the impact of summarization on determining the relevance of a document to a topic (Mani and Bloedorn 1997),),),).", "labels": [], "entities": []}, {"text": "In meeting the evaluation goals, the main question to be answered was whether summarization saved time in relevance assessment, without impairing accuracy.: Participant Summarization Features.", "labels": [], "entities": [{"text": "accuracy.", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9974722266197205}]}, {"text": "tf: term frequency; loc: location; disc:discourse (e.g., use of discourse model); coref: coreference; co-occ: co-occurrence; syn: synonyms.", "labels": [], "entities": []}, {"text": "X and Y are distinct categories other than Noneof-the-above, represented as None.", "labels": [], "entities": []}, {"text": "The topics we chose were a subset of the 20 adhoc TREC topics selected.", "labels": [], "entities": []}, {"text": "For each topic, 30 relevant documents from the adhoc task corpus were chosen as the source texts for topic-related summarization.", "labels": [], "entities": []}, {"text": "The principal tasks of each evaluator (one evaluator per topic, 3 in all) were to prepare the questions and answer keys and to score the 4Dropping two outlier assessors in the categorization task -the fastest and the slowest -resulted in the pairwise and three-way agreement going up to 69.3% and 54.0% respectively, making the agreement comparable with the adhoc task.: Percentage of decisions subjects agreed on when viewing full-text (consistency tasks).", "labels": [], "entities": []}, {"text": "To construct the answer key, each evaluator marked off any passages in the text that provided an answer to a question (example shown in).", "labels": [], "entities": []}, {"text": "The summaries generated by the participants (who were given the topics and the documents to be summarized, but not the questions) were scored against the answer key.", "labels": [], "entities": []}, {"text": "The evaluators used a common set of guidelines for writing questions, creating answer keys, and scoring summaries that were intended to minimize variability across evaluators in the methods used s.", "labels": [], "entities": []}, {"text": "Eight of the adhoc participants also submitted summaries for the Q&A evaluation.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8857435782750448}]}, {"text": "Thirty summaries per topic were scored against the answer keys.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Adhoc Time and Accuracy by Condition. TP, FP, FN, TN are expressed as percentage of  totals observed in all four categories. All time differences are significant except between B and S1  (HSD=9.8). All F-score differences are significant, except between F (Full-Text) and $2 (HSD=.10).  Precision (P) differences aren't significant. All Recall (R) differences between conditions are significant,  except between F and $2 (HSD=.12). \"SD\" = standard deviation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.981484055519104}, {"text": "Precision (P) differences", "start_pos": 297, "end_pos": 322, "type": "METRIC", "confidence": 0.9070799112319946}, {"text": "Recall (R) differences", "start_pos": 347, "end_pos": 369, "type": "METRIC", "confidence": 0.9461300611495972}]}, {"text": " Table 5: Categorization Time and Accuracy by Condition. Here TP, FP, FN, TN are expressed as  percentage of totals in all four categories. All time differences are significant except between F and  $2, and between B and S1 (HSD=15.6).Only the F-score of B is significantly less than the others  (HSD=.09). Precision (P) and Recall (R) of B is significantly less than the others: HSD(Precision)--.11;  HSD(Recall)-.11.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9850724339485168}, {"text": "F-score", "start_pos": 244, "end_pos": 251, "type": "METRIC", "confidence": 0.9778900742530823}, {"text": "Precision (P)", "start_pos": 307, "end_pos": 320, "type": "METRIC", "confidence": 0.9246721714735031}, {"text": "Recall (R)", "start_pos": 325, "end_pos": 335, "type": "METRIC", "confidence": 0.9606628268957138}, {"text": "HSD(Precision)--.11", "start_pos": 380, "end_pos": 399, "type": "METRIC", "confidence": 0.8215143918991089}, {"text": "HSD(Recall)-.11", "start_pos": 402, "end_pos": 417, "type": "METRIC", "confidence": 0.8770199894905091}]}, {"text": " Table 8: Adhoc Accuracy: Participant Groups tbr  $2 summaries. Groups I and III are significantly  different in F-score (albeit with a small effect size).  Accuracy differences within groups and between  Group II and the others are not significant.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9137481451034546}, {"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9957209229469299}, {"text": "Accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9952520132064819}]}, {"text": " Table 6: Adhoc Accuracy by Participant. For variable-length: Precision (P) differences aren't signifi- cant; CGI/CMU and Cornell/SabIR are significantly different from SRA, NTU, and ISI in Recall (R)  (HSD=0.17) and from ISI in F-score (HSD=0.13). For fixed-length, no significant differences on any of  the measures.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.942410871386528}, {"text": "Cornell/SabIR", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.7903371850649515}, {"text": "Recall (R)", "start_pos": 190, "end_pos": 200, "type": "METRIC", "confidence": 0.9412586390972137}, {"text": "F-score", "start_pos": 229, "end_pos": 236, "type": "METRIC", "confidence": 0.9876673221588135}]}, {"text": " Table 7: Categorization Accuracy by Participant. No significant differences on any of the measures.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9887143969535828}]}, {"text": " Table 9: Percentage of decisions subjects agreed on when viewing full-text (consistency tasks).", "labels": [], "entities": []}]}