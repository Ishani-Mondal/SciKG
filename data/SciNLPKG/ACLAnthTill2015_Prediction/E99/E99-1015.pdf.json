{"title": [{"text": "An annotation scheme for discourse-level argumentation in research articles", "labels": [], "entities": []}], "abstractContent": [{"text": "In order to build robust automatic abstracting systems, there is a need for better training resources than are currently available.", "labels": [], "entities": []}, {"text": "In this paper, we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way.", "labels": [], "entities": []}, {"text": "The seven categories of the scheme are based on rhetorical moves of argumentation.", "labels": [], "entities": []}, {"text": "Our experimental results show that the scheme is stable, reproducible and intuitive to use.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current approaches to automatic summarization cannot create coherent, flexible automatic summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.863289475440979}]}, {"text": "Sentence selection techniques (e.g.) produce extracts which can be incoherent and which, because of the generality of the methodology, can give under-informative results; fact extraction techniques (e.g. are tailored to particular domains, but have not really scaled up from restricted texts and restricted domains to larger domains and unrestricted text.", "labels": [], "entities": [{"text": "Sentence selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8833814561367035}, {"text": "fact extraction", "start_pos": 171, "end_pos": 186, "type": "TASK", "confidence": 0.7600299715995789}]}, {"text": "Sp~irck argues that taking into account the structure of a text will help when summarizing the text.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.9825323820114136}]}, {"text": "The problem with sentence selection is that it relies on extracting sentences out of context, but the meaning of extracted material tends to depend on wherein the text the extracted sentence was found.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7607566118240356}]}, {"text": "However, sentence selection still has the distinct advantage of robustness.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8216196894645691}]}, {"text": "We think sentence selection could be improved substantially if the global rhetorical context of the extracted material was taken into account more.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7955244481563568}]}, {"text": "Marcu (1997) makes a similar point based on rhetorical relations as defined by Rhetorical Structure Theory).", "labels": [], "entities": []}, {"text": "In contrast to this approach, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to local RST-type moves.", "labels": [], "entities": []}, {"text": "For example, sentences which describe weaknesses of previous approaches can provide a good characterization of the scientific articles in which they occur, since they are likely to also be a description of the problem that paper is intending to solve.", "labels": [], "entities": []}, {"text": "Take a sentence like \"Un]ortunately, this work does not solve problem X\": if X is a shortcoming in someone else's work, this usually means that the current paper will try to solve X.", "labels": [], "entities": []}, {"text": "Sentence extraction methods can locate sentences like these, e.g. using a cue phrase method.", "labels": [], "entities": [{"text": "Sentence extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.928302139043808}]}, {"text": "But a very similar-looking sentence can play a completely different argumentative role in a scientific text: when it occurs in the section \"Future Work\", it might refer to a minor weakness in the work presented in the source paper (i.e. of the author's own solution).", "labels": [], "entities": []}, {"text": "In that case, the sentence is not a good characterization of the paper.", "labels": [], "entities": []}, {"text": "Our approach to automatic text summarization is to find important sentences in a source text by determining their most likely argumentative role.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.568548580010732}]}, {"text": "In order to create an automatic process to do so, either by symbolic or machine learning techniques, we need training material: a collection of texts (in this case, scientific articles) where each sentence is annotated with information about the argumentative role that sentence plays in the paper.", "labels": [], "entities": []}, {"text": "Currently, no such resource is available.", "labels": [], "entities": []}, {"text": "We developed an annotation scheme as a starting point for building up such a resource, which we will describe in section 2.", "labels": [], "entities": []}, {"text": "In section 3, we use content analysis techniques to test the annotation scheme's reliability.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our annotation scheme is based on the intuition that its categories provide an adequate and intuitive description of scientific texts.", "labels": [], "entities": []}, {"text": "But this intuition alone is not enough of a justification: we believe that our claims, like claims about any other descriptive account of textual interpretation, should be substantiated by demonstrating that other humans can apply this interpretation consistently to actual texts.", "labels": [], "entities": []}, {"text": "Study I and II were designed to find out if the two versions of the annotation scheme (basic vs. full) can be learned by human coders with a significant amount of training.", "labels": [], "entities": []}, {"text": "We are interested in two formal properties of the annotation scheme: stability and reproducibility.", "labels": [], "entities": []}, {"text": "Stability, the extent to which one annotator will produce the same classifications at different times, is important because an instable annotation scheme can never be reproducible.", "labels": [], "entities": []}, {"text": "Reproducibility, the extent to which different annotators will produce the same classifications, is important because it measures the consistency of shared understandings (or meaning) held between annotators.", "labels": [], "entities": [{"text": "Reproducibility", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9172409772872925}]}, {"text": "We use the Kappa coefficient K ( to measure stability and reproducibility among k annotators on N items: In our experiment, the items are sentences.", "labels": [], "entities": []}, {"text": "Kappa is a better measurement of agreement than raw percentage agreement) because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7889704704284668}]}, {"text": "No matter how many items or annotators, or how the categories are distributed, K--0 when there is no agreement other than what would be expected by chance, and K=I when agreement is perfect.", "labels": [], "entities": [{"text": "K--0", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.88006059328715}]}, {"text": "We expect high random agreement for our annotation scheme because so many sentences fall into the OWN category.", "labels": [], "entities": [{"text": "agreement", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.8491050004959106}]}, {"text": "Studies I and II will determine how far we can trust in the human-annotated training material for both learning and evaluation of the automatic method.", "labels": [], "entities": []}, {"text": "The outcome of Study II (full annotation scheme) is crucial to the task, as some of the categories specific to the full annotation scheme (particularly AIM) add considerable value to the information contained in the training material.", "labels": [], "entities": []}, {"text": "Study III tries to answer the question whether the considerable training effort used in Studies I and II can be reduced.", "labels": [], "entities": []}, {"text": "If it were the case that coders with hardly any task-specific training can produce similar results to highly trained coders, the training material could be acquired in a more efficient way.", "labels": [], "entities": []}, {"text": "A positive outcome of Study III would also strengthen claims about the intuitivity of the category definitions.", "labels": [], "entities": []}, {"text": "We chose papers that had been presented at COL-ING, ANLP or ACL conferences (including student sessions), or ACL-sponsored workshops, and been put onto the archive between April 1994 and April 1995.", "labels": [], "entities": [{"text": "COL-ING", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9061119556427002}, {"text": "ANLP or ACL conferences", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.5751807019114494}]}], "tableCaptions": []}