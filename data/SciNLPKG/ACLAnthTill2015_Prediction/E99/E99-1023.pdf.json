{"title": [], "abstractContent": [{"text": "Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9809024333953857}, {"text": "information extraction", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.8019298613071442}, {"text": "information retrieval", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.8326096832752228}]}, {"text": "(l~mshaw and Marcus, 1995) have introduced a \"convenient\" data representation for chunking by converting it to a tagging task.", "labels": [], "entities": []}, {"text": "In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks.", "labels": [], "entities": [{"text": "recognizing noun phrase chunks", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.7945375740528107}]}, {"text": "We will show that the the data representation choice has a minor influence on chunking performance.", "labels": [], "entities": [{"text": "chunking", "start_pos": 78, "end_pos": 86, "type": "TASK", "confidence": 0.9606367945671082}]}, {"text": "However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results fora standard data set.", "labels": [], "entities": []}], "introductionContent": [{"text": "The text corpus tasks parsing, information extraction and information retrieval can benefit from dividing sentences in chunks of words.) describe an error-driven transformation-based learning (TBL) method for finding NP chunks in texts.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7754811346530914}, {"text": "information retrieval", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.7676160335540771}]}, {"text": "NP chunks (or baseNPs) are non-overlapping, non-recursive noun phrases.", "labels": [], "entities": []}, {"text": "In their experiments they have modeled chunk recognition as a tagging task: words that are inside a baseNP were marked I, words outside a baseNP received an 0 tag and a special tag B was used for the first word inside a baseNP immediately following another baseNP.", "labels": [], "entities": [{"text": "chunk recognition", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9578096866607666}]}, {"text": "A text example: where all the chunkinitial words receive the same start tag (analogous to the B tag) while the remainder of the words in the chunk are paired with a different tag.", "labels": [], "entities": []}, {"text": "In the Ratnaparkhi representation equal noun phrases receive the same tag sequence regardless of the context in which they appear.", "labels": [], "entities": []}, {"text": "The data representation choice might influence the performance of chunking systems.", "labels": [], "entities": []}, {"text": "In this paper we discuss how large this influence is.", "labels": [], "entities": []}, {"text": "Therefore we will compare seven different data representation formats for the baseNP recognition task.", "labels": [], "entities": [{"text": "baseNP recognition task", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.848046064376831}]}, {"text": "We are particularly interested in finding out whether with one of the representation formats the best reported results for this task can be improved.", "labels": [], "entities": []}, {"text": "The second section of this paper presents the general setup of the experiments.", "labels": [], "entities": []}, {"text": "The results Can be found in the third section.", "labels": [], "entities": []}, {"text": "In the fourth section we will describe some related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present and explain the data representation formats and the machine learning algorithm that we have used.", "labels": [], "entities": []}, {"text": "In the final part we describe the feature representation used in our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results first experiment series: the best F~=I scores for different left (L) and right (R)  word/POS tag pair context sizes for the seven representation formats using 5-fold cross-validation on  section 15 of the WSJ corpus.", "labels": [], "entities": [{"text": "F~=I", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9540699124336243}, {"text": "WSJ corpus", "start_pos": 223, "end_pos": 233, "type": "DATASET", "confidence": 0.97794970870018}]}, {"text": " Table 3: Results second experiment series: the best F~=I scores for different left (L) and right (R)  chunk tag context sizes for the seven representation formats using 5-fold cross-validation on section 15  of the WSJ corpus.", "labels": [], "entities": [{"text": "F~=I", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9567861159642538}, {"text": "WSJ corpus", "start_pos": 216, "end_pos": 226, "type": "DATASET", "confidence": 0.9785732626914978}]}, {"text": " Table 4: Results third experiment series: the best F~=I scores for different combinations of chunk tag  context sizes for the seven representation formats using 5-fold cross-validation on section 15 of the WSJ  corpus.", "labels": [], "entities": [{"text": "F~=I", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9573394060134888}, {"text": "WSJ  corpus", "start_pos": 207, "end_pos": 218, "type": "DATASET", "confidence": 0.9765490293502808}]}, {"text": " Table 5: Results fourth experiment series: the best FZ=I scores for different combinations of left and  right classification tag context sizes for the seven representation formats using 5-fold cross-validation  on section 15 of the WSJ corpus obtained with IBI-Ic parameter k=3. IOB1 is the best representation  format but the differences with the results of the other formats are not significant.", "labels": [], "entities": [{"text": "FZ=I scores", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9516623318195343}, {"text": "WSJ corpus", "start_pos": 233, "end_pos": 243, "type": "DATASET", "confidence": 0.9746545553207397}]}, {"text": " Table 6: The F~=I scores for the (Ramshaw and Marcus, 1995) test set after training with their  training data set. The data was processed with the optimal input feature combinations found in the  fourth experiment series. The accuracy rate contains the fraction of chunk tags that was correct. The  other three rates regard baseNP recognition. The bottom part of the table shows some other reported  results with this data set. With all but two formats IBI-IG achieves better FZ=l rates than the best  published result in (Ramshaw", "labels": [], "entities": [{"text": "F~=I", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9616490006446838}, {"text": "Ramshaw and Marcus, 1995) test set", "start_pos": 35, "end_pos": 69, "type": "DATASET", "confidence": 0.7666145637631416}, {"text": "accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9986018538475037}, {"text": "baseNP recognition", "start_pos": 325, "end_pos": 343, "type": "TASK", "confidence": 0.6699803471565247}, {"text": "FZ=l", "start_pos": 477, "end_pos": 481, "type": "METRIC", "confidence": 0.9383729696273804}]}]}