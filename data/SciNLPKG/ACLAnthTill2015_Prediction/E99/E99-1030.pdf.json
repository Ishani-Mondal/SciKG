{"title": [{"text": "The Development of Lexical Resources for Information Extraction from Text Combining WordNet and Dewey Decimal Classification*", "labels": [], "entities": [{"text": "Information Extraction from Text Combining WordNet", "start_pos": 41, "end_pos": 91, "type": "TASK", "confidence": 0.7681712706883749}, {"text": "Dewey Decimal Classification", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.523938407500585}]}], "abstractContent": [{"text": "Lexicon definition is one of the main bottlenecks in the development of new applications in the field of Information Extraction from text.", "labels": [], "entities": [{"text": "Lexicon definition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8421251475811005}, {"text": "Information Extraction from text", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.8433252722024918}]}, {"text": "Generic resources (e.g., lexical databases) are promising for reducing the cost of specific lexica definition , but they introduce lexical ambiguity.", "labels": [], "entities": [{"text": "specific lexica definition", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.6167473991711935}]}, {"text": "This paper proposes a methodology for building application-specific lex-ica by using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9660870432853699}]}, {"text": "Lexical ambiguity is kept under control by marking synsets in WordNet with field labels taken from the Dewey Decimal Classification.", "labels": [], "entities": [{"text": "Lexical ambiguity", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8372920453548431}]}], "introductionContent": [{"text": "One of the current issues in Information Extraction (IE) is efficient transportability, as the cost of new applications is one of the factors limiting the market.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.8624379336833954}]}, {"text": "The lexicon definition process is currently one of the main bottlenecks in producing applications.", "labels": [], "entities": [{"text": "lexicon definition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.716649204492569}]}, {"text": "As a matter of fact the necessary lexicon for an average application is generally large (hundreds to thousands of words) and most lexical information is not transportable across domains.", "labels": [], "entities": []}, {"text": "The problem of lexicon transport is worsened by the growing degree of lexicalization of IE systems: nowadays several successful systems adopt lexical rules at many levels.", "labels": [], "entities": []}, {"text": "The IE research mainstream focused essentially on the definition of lexica starting from a corpus sample) with the implicit assumption that a corpus provided for an application is representative of the whole applica-*This work was carried on at ITC-IRST as part of the author's dissertation for the degree in Philosophy (University of Turin, supervisor: Carla Bazzanella).", "labels": [], "entities": [{"text": "ITC-IRST", "start_pos": 245, "end_pos": 253, "type": "DATASET", "confidence": 0.9263986945152283}]}, {"text": "The author wants to thank her supervisor at ITC-IRST, Fabio Ciravegna, for his constant help.", "labels": [], "entities": [{"text": "ITC-IRST", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.8746486306190491}]}, {"text": "Alberto Lavelli provided valuable comments to the paper.", "labels": [], "entities": []}, {"text": "Unfortunately one of the current trends in IE is the progressive reduction of the size of training corpora: e.g., from the 1,000 texts of the MUC-5) to the 100 texts in.", "labels": [], "entities": [{"text": "IE", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9929497241973877}, {"text": "MUC-5", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.9515222907066345}]}, {"text": "When the corpus size is limited, the assumption of lexical representativeness of the sample corpus may not hold any longer, and the problem of producing a representative lexicon starting from the corpus lexicon arises.", "labels": [], "entities": []}, {"text": "Generic resources are interesting as they contain (among others) most of the terms necessary for an IE application.", "labels": [], "entities": [{"text": "IE application", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.9109102189540863}]}, {"text": "Nevertheless up to now the use of generic resources within IE system has been limited for two main reasons.", "labels": [], "entities": []}, {"text": "First the information associated to each term is often not detailed enough for describing the relations necessary fora IE lexicon; secondly the presence of a large amount of lexical polysemy.", "labels": [], "entities": []}, {"text": "In this paper we propose a methodology for semi-automatically developing the relevant part of a lexicon (foreground lexicon) for IE applications by using both a small corpus and WordNet.", "labels": [], "entities": [{"text": "IE", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.967846155166626}, {"text": "WordNet", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.9449320435523987}]}], "datasetContent": [], "tableCaptions": []}