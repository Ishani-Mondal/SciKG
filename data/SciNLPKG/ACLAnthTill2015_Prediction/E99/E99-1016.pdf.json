{"title": [], "abstractContent": [{"text": "This paper presents anew approach to partial parsing of context-free structures.", "labels": [], "entities": [{"text": "partial parsing of context-free structures", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.7054914355278015}]}, {"text": "The approach is based on Markov Models.", "labels": [], "entities": []}, {"text": "Each layer of the resulting structure is represented by its own Markov Model, and output of a lower layer is passed as input to the next higher layer.", "labels": [], "entities": []}, {"text": "An empirical evaluation of the method yields very good results for NP/PP chunking of German newspaper texts.", "labels": [], "entities": [{"text": "NP/PP chunking of German newspaper texts", "start_pos": 67, "end_pos": 107, "type": "TASK", "confidence": 0.7569836378097534}]}], "introductionContent": [{"text": "Partial parsing, often referred to as chunking, is used as a pre-processing step before deep analysis or as shallow processing for applications like information retrieval, messsage extraction and text summarization.", "labels": [], "entities": [{"text": "Partial parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7955912947654724}, {"text": "information retrieval", "start_pos": 149, "end_pos": 170, "type": "TASK", "confidence": 0.7843854129314423}, {"text": "messsage extraction", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.8198084235191345}, {"text": "text summarization", "start_pos": 196, "end_pos": 214, "type": "TASK", "confidence": 0.7765988409519196}]}, {"text": "Chunking concentrates on constructs that can be recognized with a high degree of certainty.", "labels": [], "entities": []}, {"text": "For several applications, this type of information with high accuracy is more valuable than deep analysis with lower accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9955352544784546}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9889504313468933}]}, {"text": "We will present anew approach to partial parsing that uses Markov Models.", "labels": [], "entities": [{"text": "partial parsing", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.6137124001979828}]}, {"text": "The presented models are extensions of the part-of-speech tagging technique and are capable of emitting structure.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7263367176055908}]}, {"text": "They utilize context-free grammar rules and add left-to-right transitional context information.", "labels": [], "entities": []}, {"text": "This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (.", "labels": [], "entities": [{"text": "NEGRA corpus of German newspaper texts", "start_pos": 73, "end_pos": 111, "type": "DATASET", "confidence": 0.9403467377026876}]}, {"text": "Part-of-speech tagging is the assignment of syntactic categories (tags) to words that occur in the processed text.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7512246370315552}, {"text": "assignment of syntactic categories (tags) to words that occur in the processed text", "start_pos": 30, "end_pos": 113, "type": "TASK", "confidence": 0.7560784896214803}]}, {"text": "Among others, this task is efficiently solved with Markov Models.", "labels": [], "entities": []}, {"text": "States of a Markov Model represent syntactic categories (or tuples of syntactic categories), and outputs represent words and punctuation.", "labels": [], "entities": []}, {"text": "This technique of statistical part-of-speech tagging operates very successfully, and usually accuracy rates between 96 and 97% are reported for new, unseen text.", "labels": [], "entities": [{"text": "statistical part-of-speech tagging", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.5910218556722006}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9990134239196777}]}, {"text": "showed that the technique of statistical tagging can be shifted to the next level of syntactic processing and is capable of assigning grammatical functions.", "labels": [], "entities": [{"text": "statistical tagging", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7769315540790558}]}, {"text": "These are functions like subject, direct object, head, etc.", "labels": [], "entities": []}, {"text": "They mark the function of a child node within its parent phrase.", "labels": [], "entities": []}, {"text": "shows an example sentence and its structure.", "labels": [], "entities": []}, {"text": "The terminal sequence is complemented by tags.", "labels": [], "entities": []}, {"text": "Non-terminal nodes are labeled with phrase categories, edges are labeled with grammatical functions (NEGRA tagset).", "labels": [], "entities": []}, {"text": "In this paper, we will show that Markov Models are not restricted to the labeling task (i.e., the assignment of part-of-speech labels, phrase labels, or labels for grammatical functions), but are also capable of generating structural elements.", "labels": [], "entities": []}, {"text": "We will use cascades of Markov Models.", "labels": [], "entities": []}, {"text": "Starting with the part-of-speech layer, each layer of the resulting structure is represented by its own Markov Model.", "labels": [], "entities": []}, {"text": "A lower layer passes its output as input to the next higher layer.", "labels": [], "entities": []}, {"text": "The output of a layer can be ambiguous and it is complemented by a probability distribution for the alternatives.", "labels": [], "entities": []}, {"text": "This type of parsing is inspired by finite state cascades which are presented by several authors.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9627414345741272}]}, {"text": "CASS) is a partial parser that recognizes non-recursive basic phrases (chunks) with finite state transducers.", "labels": [], "entities": []}, {"text": "Each transducer emits a single best analysis (a longest match) that serves as input for the transducer at the next higher level.", "labels": [], "entities": []}, {"text": "CASS needs a special grammar for which rules are manually coded.", "labels": [], "entities": [{"text": "CASS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5609156489372253}]}, {"text": "Each layer creates a particular subset of phrase types.", "labels": [], "entities": []}, {"text": "FASTUS) is heavily based on pattern matching.", "labels": [], "entities": [{"text": "FASTUS)", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6726586520671844}, {"text": "pattern matching", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7337980568408966}]}, {"text": "Each pattern is associated with one or more trigger words.", "labels": [], "entities": []}, {"text": "It uses a series of non-deterministic finite-state transducers to build chunks; the output of one transducer is passed 'A large amount of money and work was raised by the involved organizations': Example sentence and annotation.", "labels": [], "entities": []}, {"text": "The structure consists of terminal nodes (words and their parts-of-speech), non-terminal nodes (phrases) and edges (labeled with grammatical functions).", "labels": [], "entities": []}, {"text": "as input to the next transducer.", "labels": [], "entities": []}, {"text": "uses the fix point of a finite-state transducer.", "labels": [], "entities": []}, {"text": "The transducer is iteratively applied to its own output until it remains identical to the input.", "labels": [], "entities": []}, {"text": "The method is successfully used for efficient processing with large grammars.) present an approach to chunking based on a mixture of finite state and context-free techniques.", "labels": [], "entities": []}, {"text": "They use NP rules of a pruned treebank grammar.", "labels": [], "entities": []}, {"text": "For processing, each point of a text is matched against the treebank rules and the longest match is chosen.", "labels": [], "entities": []}, {"text": "Cascades of automata and transducers can also be found in speech processing, see e.g. ().", "labels": [], "entities": []}, {"text": "Contrary to finite-state transducers, Cascaded Markov Models exploit probabilities when processing layers of a syntactic structure.", "labels": [], "entities": []}, {"text": "They do not generate longest matches but most-probable sequences.", "labels": [], "entities": []}, {"text": "Furthermore, a higher layer sees different alternatives and their probabilities for the same span.", "labels": [], "entities": []}, {"text": "It can choose a lower ranked alternative if it fits better into the context of the higher layer.", "labels": [], "entities": []}, {"text": "An additional advantage is that Cascaded Markov Models do not need a \"stratified\" grammar (i.e., each layer encodes a disjoint subset of phrases).", "labels": [], "entities": []}, {"text": "Instead the system can be immediately trained on existing treebank data.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 addresses the encoding of parsing processes as Markov Models.", "labels": [], "entities": [{"text": "encoding of parsing processes", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.7016731053590775}]}, {"text": "Section 3 presents Cascaded Markov Models.", "labels": [], "entities": []}, {"text": "Section 4 reports on the evaluation of Cascaded Markov Models using treebank data.", "labels": [], "entities": []}, {"text": "Finally, section 5 will give conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports on results of experiments with Cascaded Markov Models.", "labels": [], "entities": []}, {"text": "We evaluate chunking precision and recall, i.e., the recognition of kernel NPs and PPs.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9613029956817627}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9993788003921509}]}, {"text": "These exclude prenominal adverbs and postnominal PPs and relative clauses, but include all other prenominal modifiers, which can be fairly complex adjective phrases in German.", "labels": [], "entities": []}, {"text": "shows an example of a complex NP and the output of the parsing process.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9692859649658203}]}, {"text": "For our experiments, we use the NEGRA corpus (.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9413515329360962}]}, {"text": "It consists of German newspaper texts (Frankfurter Rundschau) that are annotated with predicate-argument structures.", "labels": [], "entities": [{"text": "Frankfurter Rundschau)", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.8760121464729309}]}, {"text": "We extracted all structures for NPs, PPs, APs, AVPs (i.e., we mainly excluded sentences, VPs and coordinations).", "labels": [], "entities": []}, {"text": "The version of the corpus used contains 17,000 sentences (300,000 tokens).", "labels": [], "entities": []}, {"text": "The corpus was divided into training part (90%) and test part (10%).", "labels": [], "entities": []}, {"text": "Experiments were repeated 10 times, results were averaged.", "labels": [], "entities": []}, {"text": "Cross-evaluation was done in order to obtain more reliable performance estimates than by just one test run.", "labels": [], "entities": []}, {"text": "Input of the process is a sequence of words (divided into sentences), output are part-of-speech tags and structures like the one indicated in. presents results of the chunking task using Cascaded Markov Models for different numbers of layers.", "labels": [], "entities": []}, {"text": "2 Percentages are slightly below those presented by).", "labels": [], "entities": []}, {"text": "But 2The figure indicates unlabeled recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9948443174362183}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9988192915916443}]}, {"text": "Differences to labeled recall/precision are small, since the number of different non-terminal categories is very restricted.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9804607033729553}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.842573344707489}]}, {"text": "they started with correctly tagged data, so our task is harder since it includes the process of partof-speech tagging.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.798418402671814}]}, {"text": "Recall increases with the number of layers.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9913614392280579}]}, {"text": "It ranges from 54.0% for 1 layer to 84.8% for 9 layers.", "labels": [], "entities": []}, {"text": "This could be expected, because the number of layers determines the number of phrases that can be parsed by the model.", "labels": [], "entities": []}, {"text": "The additional line for \"topline recall\" indicates the percentage of phrases that can be parsed by Cascaded Markov Models with the given number of layers.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9499498605728149}]}, {"text": "All nodes that belong to higher layers cannot be recognized.", "labels": [], "entities": []}, {"text": "Precision slightly decreases with the number of layers.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9636414647102356}]}, {"text": "It ranges from 91.4% for 1 layer to 88.3% for 9 layers.", "labels": [], "entities": []}, {"text": "The F-score is a weighted combination of recall Rand precision P and defined as follows: /3 is a parameter encoding the importance of recall and precision.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9934711456298828}, {"text": "recall Rand precision P", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.9487964659929276}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9975038170814514}, {"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9880746006965637}]}, {"text": "Using an equal weight for both (/3 = 1), the maximum F-score is reached for 7 layers (F =86.5%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9992617964744568}, {"text": "F", "start_pos": 86, "end_pos": 87, "type": "METRIC", "confidence": 0.9963842630386353}]}, {"text": "The part-of-speech tagging accuracy slightly increases with the number of Markov Model layers (bottom line in).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7446343898773193}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9782634377479553}]}, {"text": "This can be explained by top-down decisions of Cascaded Markov Models.", "labels": [], "entities": []}, {"text": "A model at a higher layer can select a tag with a lower probability if this increases the probability at that layer.", "labels": [], "entities": []}, {"text": "Thereby some errors made at lower layers can be corrected.", "labels": [], "entities": []}, {"text": "This leads to the increase of up to 0.3% inaccuracy.", "labels": [], "entities": []}, {"text": "Results for chunking Penn Treebank data were previously presented by several authors 'the dismissal of the federation from several areas that was intended by the government'   because they processed a different language and generated only one layer of structure (the chunk boundaries), while our algorithm also generates the internal structure of chunks.", "labels": [], "entities": [{"text": "chunking", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.9058839082717896}, {"text": "Penn Treebank data", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.9290206829706827}]}, {"text": "But generally, Cascaded Markov Models can be reduced to generating just one layer and can be trained on Penn Treebank data.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 104, "end_pos": 122, "type": "DATASET", "confidence": 0.9861024419466654}]}], "tableCaptions": []}