{"title": [{"text": "GENEVAL: A Proposal for Shared-task Evaluation in NLG", "labels": [], "entities": [{"text": "Shared-task Evaluation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.878718763589859}]}], "abstractContent": [{"text": "We propose to organise a series of shared-task NLG events, where participants are asked to build systems with similar in-put/output functionalities, and these systems are evaluated with a range of different evaluation techniques.", "labels": [], "entities": []}, {"text": "The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "There is along history of shared task initiatives in NLP, of which the best known is perhaps MUC; others include TREC, PARSE-VAL, SENSEVAL, and the range of shared tasks organised by CoNLL.", "labels": [], "entities": [{"text": "MUC", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.6907753348350525}, {"text": "TREC", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9140247106552124}, {"text": "PARSE-VAL", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.699756383895874}, {"text": "CoNLL", "start_pos": 183, "end_pos": 188, "type": "DATASET", "confidence": 0.9313225150108337}]}, {"text": "Such exercises are now common inmost areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8586649000644684}, {"text": "information extraction", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.8013285994529724}]}, {"text": "One of the best-known comparative studies of evaluation techniques was by who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9730246663093567}, {"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8171375095844269}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.997962236404419}]}, {"text": "Several other studies of this type have been carried out in the MT and Summarisation communities.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9399287104606628}, {"text": "Summarisation", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9310715198516846}]}, {"text": "The first comparison of NLG evaluation techniques which we are aware of is by.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8564720451831818}]}, {"text": "The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.965410848458608}]}, {"text": "They used linear regression to suggest a combination of the corpus-based metrics which they be-lieve is a better predictor of human judgements than any of the individual metrics.", "labels": [], "entities": []}, {"text": "In our work (, we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts.", "labels": [], "entities": []}, {"text": "We then analysed how well the corpus-based evaluations correlated with the human-based evaluations.", "labels": [], "entities": []}, {"text": "Amongst other things, we concluded that BLEU-type metrics work reasonably well when comparing statistical NLG systems, but less well when comparing statistical NLG systems to knowledge-based NLG systems.", "labels": [], "entities": [{"text": "BLEU-type", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9981118440628052}]}, {"text": "We worked in this domain because of the availability of the SumTime corpus (, which contains both numerical weather prediction data (i.e., inputs to NLG) and human written forecast texts (i.e., target outputs from NLG).", "labels": [], "entities": [{"text": "SumTime corpus", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.7708578407764435}]}, {"text": "We are not aware of any other NLG-related corpora which contain a large number of texts and corresponding input data sets, and are freely available to the research community.", "labels": [], "entities": []}, {"text": "The final step is to evaluate the systems and components submitted to the workshop.", "labels": [], "entities": []}, {"text": "As the main Corpus num texts num ref (*) text size main NLG challenges Weather statements 3000 300 1-2 sentences content det, lex choice, aggregation Statistical summaries 1000 100 paragraph above plus surface realisation Nurses' reports 200 50 several paras above plus text structuring/layout (*) In addition to the main corpus, we will also gather texts which will be used as reference texts for corpus-based evaluations; 'num ref' is the number of such texts.", "labels": [], "entities": []}, {"text": "These texts will not be released.", "labels": [], "entities": []}, {"text": "Corpus-based evaluations: We will develop new, linguistically grounded evaluation metrics, and compare these to existing metrics including BLEU, NIST, and string-edit distance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9754760265350342}]}, {"text": "We will also investigate how sensitive different metrics are to size and make-up of the reference corpus.", "labels": [], "entities": []}, {"text": "Human-based preference judgements: We will investigate different experimental designs and methods for overcoming respondent bias (e.g. what is known as 'central tendency bias', where some respondents avoid judgements at either end of a scale).", "labels": [], "entities": []}, {"text": "As we showed previously () that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results.", "labels": [], "entities": []}, {"text": "This depends on the domain, but e.g. in the nurse-report domain we could use the methodology of (), who showed medical professionals the texts, asked them to make a treatment decision, and then rated the correctness of the suggested treatments.", "labels": [], "entities": []}, {"text": "As well as recommendations about the appropriateness of existing evaluation techniques, we hope the above experiments will allow us to suggest new evaluation techniques for NLG.", "labels": [], "entities": []}], "tableCaptions": []}