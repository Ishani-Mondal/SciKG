{"title": [{"text": "Learning Information Status of Discourse Entities", "labels": [], "entities": [{"text": "Learning Information Status of Discourse Entities", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.5214738001426061}]}], "abstractContent": [{"text": "In this paper we address the issue of automatically assigning information status to discourse entities.", "labels": [], "entities": []}, {"text": "Using an annotated corpus of conversational English and exploiting morpho-syntactic and lexical features, we train a decision tree to classify entities introduced by noun phrases as old, mediated , or new.", "labels": [], "entities": []}, {"text": "We compare its performance with hand-crafted rules that are mainly based on morpho-syntactic features and closely relate to the guidelines that had been used for the manual annotation.", "labels": [], "entities": []}, {"text": "The decision tree model achieves an overall accuracy of 79.5%, significantly outperform-ing the hand-crafted algorithm (64.4%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.999643087387085}]}, {"text": "We also experiment with binary classifications by collapsing in turn two of the three target classes into one and retraining the model.", "labels": [], "entities": []}, {"text": "The highest accuracy achieved on binary classification is 93.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999634861946106}, {"text": "binary classification", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7557773888111115}]}], "introductionContent": [{"text": "Information structure is the way a speaker or writer organises known and new information in text or dialogue.", "labels": [], "entities": [{"text": "Information structure", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7624227702617645}]}, {"text": "Information structure has been the subject of numerous and very diverse linguistic studies, for instance), thus also yielding a wide range of terms and definitions (see (Vallduv\u00ed, * The work reported in this paper was carried out while the author was a research fellow at the Institute for Communicating and Collaborative Systems of the University of Edinburgh, United Kingdom, and was supported by a Scottish Enterprise Edinburgh-Stanford Link grant (265000-3102-R36766). 1992;) fora discussion).", "labels": [], "entities": [{"text": "Scottish Enterprise Edinburgh-Stanford Link grant", "start_pos": 401, "end_pos": 450, "type": "DATASET", "confidence": 0.8631549835205078}]}, {"text": "In the present study, we adopt the term \"Information Status\", following the definition employed for the annotation of the corpus we use for our experiments ).", "labels": [], "entities": []}, {"text": "Information status describes to which degree a discourse entity is available to the hearer, in terms of the speaker's assumptions about the hearer's knowledge and beliefs.", "labels": [], "entities": []}, {"text": "Although there is a fine line in the distinction between Information Status and Information Structure, it is fair to say that whereas the latter models wider discourse coherence, the former focuses mainly on the local level of discourse entities.", "labels": [], "entities": []}, {"text": "Section 2 provides more details on how this notion is encoded in our corpus.", "labels": [], "entities": []}, {"text": "Information status has generated large interest among researchers because of its complex interaction with other linguistic phenomena, thus affecting several Natural Language Processing tasks.", "labels": [], "entities": [{"text": "Information status", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.854270875453949}, {"text": "Natural Language Processing tasks", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.695334255695343}]}, {"text": "Since it correlates with word order and pitch accent, for instance, incorporating knowledge on information status would be helpful for natural language generation, and in particular text-tospeech systems.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 135, "end_pos": 162, "type": "TASK", "confidence": 0.6839898228645325}]}, {"text": "St\u00f6ber and colleagues, for example, ascribe to the lack of such information the lower performance of text-to-speech compared to concept-to-speech generation, where such knowledge could be made directly available to the system ().", "labels": [], "entities": [{"text": "concept-to-speech generation", "start_pos": 128, "end_pos": 156, "type": "TASK", "confidence": 0.7321124970912933}]}, {"text": "Another area where information status can play an important role is anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.862909734249115}]}, {"text": "A major obstacle in the resolution of definite noun phrases with full lexical heads is that only a small proportion of them is actually anaphoric (ca. 30%).", "labels": [], "entities": [{"text": "resolution of definite noun phrases with full lexical heads", "start_pos": 24, "end_pos": 83, "type": "TASK", "confidence": 0.8522860474056668}]}, {"text": "Therefore, in the absence of anaphoricity information, a resolution system will try to find an antecedent also for non-anaphoric definite noun phrases, thus severely affecting performance.", "labels": [], "entities": []}, {"text": "There has been recent interest in determining anaphoricity before performing anaphora resolution (, but results have not been entirely satisfactory.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.700400173664093}]}, {"text": "Given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination.", "labels": [], "entities": [{"text": "identification of information status", "start_pos": 85, "end_pos": 121, "type": "TASK", "confidence": 0.8716076165437698}, {"text": "anaphoricity determination", "start_pos": 136, "end_pos": 162, "type": "TASK", "confidence": 0.8637737929821014}]}, {"text": "have recently shown that learning information structure with high accuracy is feasible for Czech.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9948312640190125}]}, {"text": "However, there are yet no studies that explore such a task for English.", "labels": [], "entities": []}, {"text": "Exploiting an existing annotated corpus, in this paper we report experiments on learning a model for the automatic identification of information status in English.", "labels": [], "entities": [{"text": "identification of information status in English", "start_pos": 115, "end_pos": 162, "type": "TASK", "confidence": 0.8166913290818533}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Per class performance of hand-crafted  rules on the development and evaluation sets", "labels": [], "entities": []}, {"text": " Table 3: Distribution of information status over  NP types in the evaluation set", "labels": [], "entities": []}, {"text": " Table 5: Per class performance of C4.5 on the de- velopment and evaluation sets", "labels": [], "entities": []}, {"text": " Table 6: the highest proportion of mistakes is  due to 1,453 new instances classified as mediated.  Also significant is the wrong assignment of me- diated tags to old entities. Such behaviour of the  classifier is to be expected, given the 'in-between'  nature of mediated entities.", "labels": [], "entities": []}, {"text": " Table 6: Confusion matrix for evaluation set.  C=Classifier tag; G=Gold tag  C \u2192  G \u2193", "labels": [], "entities": []}, {"text": " Table 7: Overview of accuracy for hand-crafted  rules and C4.5 on three-way and binary classifica- tions on development and evaluation sets", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9993903636932373}]}, {"text": " Table 8: Performance of leave-one-out and single- feature classifiers on three-way classification", "labels": [], "entities": []}]}