{"title": [{"text": "What are the Productive Units of Natural Language Grammar? A DOP Approach to the Automatic Identification of Constructions", "labels": [], "entities": [{"text": "Automatic Identification of Constructions", "start_pos": 81, "end_pos": 122, "type": "TASK", "confidence": 0.7555118352174759}]}], "abstractContent": [{"text": "We explore a novel computational approach to identifying \"constructions\" or \"multi-word expressions\" (MWEs) in an annotated corpus.", "labels": [], "entities": [{"text": "identifying \"constructions\" or \"multi-word expressions\" (MWEs) in an annotated corpus", "start_pos": 45, "end_pos": 130, "type": "TASK", "confidence": 0.722386347129941}]}, {"text": "In this approach, MWEs have no special status, but emerge in a general procedure for finding the best statistical grammar to describe the training corpus.", "labels": [], "entities": []}, {"text": "The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing.", "labels": [], "entities": [{"text": "stochastic tree substitution grammars (STSGs)", "start_pos": 50, "end_pos": 95, "type": "TASK", "confidence": 0.7490204913275582}]}, {"text": "We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank.", "labels": [], "entities": []}, {"text": "We report quantitative results on the ATIS corpus of phrase-structure annotated sentences , and give examples of the MWEs extracted from this corpus.", "labels": [], "entities": [{"text": "ATIS corpus of phrase-structure annotated sentences", "start_pos": 38, "end_pos": 89, "type": "DATASET", "confidence": 0.924168070157369}]}], "introductionContent": [{"text": "Many current theories of language use and acquisition assume that language users store and use much larger fragments of language than the single words and rules of combination of traditional linguistic models.", "labels": [], "entities": [{"text": "language use and acquisition", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6758935153484344}]}, {"text": "Such fragments are often called constructions, and the theories that assign them a central role \"construction grammar\", among others).", "labels": [], "entities": []}, {"text": "For construction grammarians, multi-word expressions (MWEs) such as idioms, collocations, fixed expressions and compound verbs and nouns, are not so much exceptions to the rule, but rather extreme cases that reveal some fundamental properties of natural language.", "labels": [], "entities": []}, {"text": "In the construction grammar tradition, cooccurrence statistics from corpora have often been used as evidence for hypothesized constructions.", "labels": [], "entities": []}, {"text": "However, such statistics are typically gathered on a case-by-case basis, and no reliable procedure exists to automatically identify constructions.", "labels": [], "entities": []}, {"text": "In contrast, in computational linguistics, many automatic procedures are studied for identifying) -with varying success -but here they are treated as exceptions: identifying multi-word expressions is a pre-processing step, where typically adjacent words are grouped together after which the usual procedures for syntactic or semantic analysis can be applied.", "labels": [], "entities": [{"text": "syntactic or semantic analysis", "start_pos": 312, "end_pos": 342, "type": "TASK", "confidence": 0.6141204759478569}]}, {"text": "In this paper I explore an alternative formal and computational approach, where multi-word constructions have no special status, but emerge in a general procedure to find the best statistical grammar to describe a training corpus.", "labels": [], "entities": []}, {"text": "Crucially, I use a formalism known as \"Stochastic Tree Substitution Grammars\" (henceforth, STSGs), which can represent single words, contiguous and noncontiguous MWEs, context-free rules or complete parse trees in a unified representation.", "labels": [], "entities": []}, {"text": "My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7408447265625}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.986473798751831}, {"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9835365414619446}, {"text": "Penn Tree Bank", "start_pos": 190, "end_pos": 204, "type": "DATASET", "confidence": 0.9967015782992045}]}, {"text": "DOP, first proposed in), can be seen as an early formalization and combination of ideas from construction grammar and statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.775140643119812}]}, {"text": "Its key innovations were (i) the proposal to use fragments of trees from a tree bank as the symbolic backbone; (ii) the proposal to allow, in principle, trees of arbitrary size and shape as the elementary units of combination; (iii) the proposal to use the occurrence and co-occurrence frequencies as the basis for structural disambiguation in parsing.", "labels": [], "entities": []}, {"text": "The model I develop in this paper is true to these general DOP ideals, although it differs in important respects from the many DOP implementations that have been studied since its first inception, and many others).", "labels": [], "entities": []}, {"text": "The crucial difference is in the estimation procedure for choosing the weights of the STSG based on observed frequencies in a corpus.", "labels": [], "entities": [{"text": "estimation", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9024341106414795}]}, {"text": "Existing DOP models converge to STSGs that either (i) give all subtrees of the observed trees nonzero weights, or (ii) give only the largest possible fragments nonzero weights).", "labels": [], "entities": []}, {"text": "The model in this paper, in contrast, aims at finding the smallest set of productive units that explain the occurrences and co-occurrences in a corpus.", "labels": [], "entities": []}, {"text": "Large subtrees only receive non-zero weights, if they occur more frequently than can be expected on the basis of the weights of smaller subtrees.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Parseval scores of DOP1 and push-n-pull  on the same 462-116 random train-testset split of a  treebank derived from the ATIS3 corpus (we empha- size that all trees, also those of the test-set, were con- verted to Chomsky Normal Form, whereby unary  production and traces were removed and top-nodes  relabeled \"TOP\". These results are thus not compa- rable to previous methods evaluated on the ATIS3  corpus.) EM is \"exact match\".", "labels": [], "entities": [{"text": "Parseval", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9518388509750366}, {"text": "DOP1", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9497716426849365}, {"text": "ATIS3 corpus", "start_pos": 130, "end_pos": 142, "type": "DATASET", "confidence": 0.9697783291339874}, {"text": "TOP", "start_pos": 320, "end_pos": 323, "type": "METRIC", "confidence": 0.91631019115448}, {"text": "ATIS3  corpus", "start_pos": 403, "end_pos": 416, "type": "DATASET", "confidence": 0.9731860756874084}, {"text": "EM", "start_pos": 419, "end_pos": 421, "type": "METRIC", "confidence": 0.8871124386787415}, {"text": "exact match", "start_pos": 426, "end_pos": 437, "type": "METRIC", "confidence": 0.9534133970737457}]}, {"text": " Table 2: Parseval scores using a p-n-p induced  STSG on the same treebank as in table 1, using a  different random 525-53 train-testset split. Shown  are results were only elementary trees with scores  higher than 0.3 and 0.1 respectively are used.", "labels": [], "entities": []}]}