{"title": [{"text": "BRUJA: Question Classification for Spanish. Using Machine Translation and an English Classifier", "labels": [], "entities": [{"text": "BRUJA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9396737217903137}, {"text": "Question Classification", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.6371661573648453}]}], "abstractContent": [{"text": "Question Classification is an important task in Question Answering Systems.", "labels": [], "entities": [{"text": "Question Classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8003964424133301}, {"text": "Question Answering Systems", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.8658276398976644}]}, {"text": "This paper presents a Spanish Question Classi-fier based on machine learning, automatic online translators and different language features.", "labels": [], "entities": [{"text": "Spanish Question Classi-fier", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.815703292687734}]}, {"text": "Our system works with Eng-lish collections and bilingual questions (English/Spanish).", "labels": [], "entities": []}, {"text": "We have tested two Spanish-English online translators to identify the lost of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9988910555839539}]}, {"text": "We have made experiments using lexical, syntactic and semantic features to test which ones made a better performance.", "labels": [], "entities": []}, {"text": "The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9991336464881897}]}, {"text": "Our conclusion about the features is that a lexical, syntactic and semantic features combination obtains the best result.", "labels": [], "entities": []}], "introductionContent": [{"text": "A Question Answering (QA) system seeks and shows the user an accurate and concise answer, given a free-form question, and using a large text data collection.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.8362381994724274}]}, {"text": "The use of Cross Language Information Retrieval Systems (CLIR) is growing, and also the application of these ones into other general systems, such as Question Answering or Question Classification.", "labels": [], "entities": [{"text": "Cross Language Information Retrieval Systems (CLIR)", "start_pos": 11, "end_pos": 62, "type": "TASK", "confidence": 0.6627360917627811}, {"text": "Question Answering", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7970191836357117}, {"text": "Question Classification", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.7275583744049072}]}, {"text": "A CLIR system is an Information Retrieval System that works with collections in several languages, and extract relevant documents or passages.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.727952316403389}]}, {"text": "We have proposed a Multilingual Question Answering System (BRUJA -in Spanish \"Busqueda de Respuestas University of Jaen\") that works with collections in several languages.", "labels": [], "entities": [{"text": "Multilingual Question Answering", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6150836447874705}, {"text": "BRUJA", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.8892208337783813}, {"text": "Busqueda de Respuestas University of Jaen", "start_pos": 78, "end_pos": 119, "type": "DATASET", "confidence": 0.6198252588510513}]}, {"text": "Since there are several languages, tasks such as obtaining relevant documents and extracting the answer could be accomplished in two ways: using NPL tools and resources for each language or fora pivot language only (English) and translating to the pivot language the rest of the relevant information when it is required.", "labels": [], "entities": []}, {"text": "Because of the translation step, the second approach is less accurate but more practical since we need only NPL resources for English.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9745883941650391}]}, {"text": "The central question is the noise, because of the translation process, is too high in order to use this approach in spite of their practical advantages.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9601391553878784}]}, {"text": "The first step of this system is a Question Classifier (QC).", "labels": [], "entities": [{"text": "Question Classifier (QC)", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6864167749881744}]}, {"text": "Given a query, a question classification module obtains the class of such question.", "labels": [], "entities": [{"text": "question classification", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7112263292074203}]}, {"text": "This information is useful for the extraction of the answer.", "labels": [], "entities": []}, {"text": "For example, given the query \"Where is Madrid?, the QA system expects a location entity as answer type.", "labels": [], "entities": []}, {"text": "The proposed QA module works with questions in several languages, translates them into English using different online translators, and obtains the type of questions and some features, such as the focus, the keywords or the context.", "labels": [], "entities": []}, {"text": "In this work we aim to find out whether a multilingual QC module is possible by using translation tools and English as pivot language or not.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are made using some public datasets available by USC (), UIUC and TREC 3 as training and test collections.", "labels": [], "entities": [{"text": "USC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9843968749046326}, {"text": "UIUC", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9635483026504517}]}, {"text": "These datasets have been labeled manually by UIUC group by means of the following general and detailed categories: ABBR: abbreviation, expansion.", "labels": [], "entities": [{"text": "UIUC group", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9180617928504944}, {"text": "ABBR", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9822461605072021}, {"text": "abbreviation", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.9845634698867798}, {"text": "expansion", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9328798055648804}]}, {"text": "DESC: definition, description, manner, reason.", "labels": [], "entities": [{"text": "DESC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.405977725982666}]}, {"text": "ENTY: animal, body, color, creation, currency, disease/medical, event, food, instrument, language, letter, other, plant, product, religion, sport, substance, symbol, technique, term, vehicle, word.", "labels": [], "entities": [{"text": "ENTY: animal, body, color, creation, currency, disease/medical, event, food, instrument, language, letter, other, plant, product, religion, sport, substance, symbol, technique, term, vehicle, word", "start_pos": 0, "end_pos": 196, "type": "Description", "confidence": 0.8655434732741498}]}, {"text": "HUM: description, group, individual, title.", "labels": [], "entities": [{"text": "HUM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.49718189239501953}]}, {"text": "LOC: city, country, mountain, other, state.", "labels": [], "entities": [{"text": "LOC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.445865273475647}]}, {"text": "NUM: code, count, date, distance, money, order, other, percent, period, speed, temperature, size, weight.", "labels": [], "entities": [{"text": "count", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9192020893096924}]}, {"text": "For instance the question \"What does NATO mean?\" is an ABBR (abbreviation) category, \"What is a receptionist?\" is a DESC (definition) category or \"When did George Bush born?\" is a NUM (numeric) category.", "labels": [], "entities": [{"text": "ABBR", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8945210576057434}]}, {"text": "The training data area set of 5500 questions and the test data area set of 500 questions.", "labels": [], "entities": []}, {"text": "All questions were labelled for the 10th conference CrossLanguage Evaluation Forum of Question Answering (CLEF-QA).", "labels": [], "entities": [{"text": "CrossLanguage Evaluation Forum of Question Answering (CLEF-QA)", "start_pos": 52, "end_pos": 114, "type": "TASK", "confidence": 0.6005923284424676}]}, {"text": "The same dataset has been used in other investigations, such as in ().", "labels": [], "entities": []}, {"text": "The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in.", "labels": [], "entities": []}, {"text": "Likewise, the distribution of categories of these 5500 training questions is showed in.", "labels": [], "entities": []}, {"text": "The distribution of the 500 test questions, with respect to its interrogative pronoun or the initial word, is showed in, and the distribution of categories of these 500 test questions is showed in.", "labels": [], "entities": []}, {"text": "In our experiments we try to identify the general category.", "labels": [], "entities": []}, {"text": "Our proposal is to try a detailed classification later.", "labels": [], "entities": []}, {"text": "We have used the Accuracy as a general measure and the Precision of each category as a detailed measure.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.984093189239502}, {"text": "Precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9872065782546997}]}, {"text": "Other measure used is the F-score, defined as the harmonic mean of precision and recall).", "labels": [], "entities": [{"text": "F-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9984878301620483}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9724775552749634}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9916688203811646}]}, {"text": "It is a commonly used metric to summarize precision and recall in one measure.", "labels": [], "entities": [{"text": "summarize", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.9839270710945129}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9652659296989441}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.997837245464325}]}], "tableCaptions": [{"text": " Table 1: Training questions distribution according  with its interrogative pronoun", "labels": [], "entities": []}, {"text": " Table 2: Training questions distribution according  with its general category.", "labels": [], "entities": []}, {"text": " Table 3: Test questions distribution according with  its interrogative pronoun.", "labels": [], "entities": []}, {"text": " Table 4: Test questions distribution according with  its general category.", "labels": [], "entities": []}, {"text": " Table 5: Results in terms of Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994044303894043}]}, {"text": " Table 6: Results in terms of F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9980499744415283}]}, {"text": " Table 7: Detailed results for each category, using  the combination lexsemsin6 and the original Eng- lish questions and the translated questions by us- ing Prompt", "labels": [], "entities": []}]}