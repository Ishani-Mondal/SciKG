{"title": [{"text": "Measuring Aboutness of an Entity in a Text", "labels": [], "entities": []}], "abstractContent": [{"text": "In many information retrieval and selection tasks it is valuable to score how much a text is about a certain entity and to compute how much the text discusses the entity with respect to a certain viewpoint.", "labels": [], "entities": [{"text": "information retrieval and selection", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.7470806837081909}]}, {"text": "In this paper we are interested in giving an aboutness score to a text, when the input query is a person name and we want to measure the aboutness with respect to the biographical data of that person.", "labels": [], "entities": []}, {"text": "We present a graph-based algorithm and compare its results with other approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many information processing tasks one is interested in measuring how much a text or passage is about a certain entity.", "labels": [], "entities": []}, {"text": "This is called aboutness or topical relevance.", "labels": [], "entities": []}, {"text": "Simple word counts of the entity term often give only a rough estimation of aboutness.", "labels": [], "entities": []}, {"text": "The true frequency of the entity might be hidden by coreferents.", "labels": [], "entities": []}, {"text": "Two entities are considered as coreferents when they both refer to the same entity in the situation described in the text (e.g., in the sentences: \"Dan Quayle met his wife in college.", "labels": [], "entities": []}, {"text": "The Indiana senator married her shortly after he finished his studies\": \"his\", \"Indiana senator\" and \"he\" all corefer to \"Dan Quayle\").", "labels": [], "entities": [{"text": "Indiana senator", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.763912171125412}]}, {"text": "If we want to score the aboutness of an entity with respect to a certain viewpoint, the aboutness is also obfuscated by the referents that refer to the chosen viewpoint and in which context the entity is mentioned.", "labels": [], "entities": []}, {"text": "In the example \"Dan Quayle ran for presidency\", \"presidency\" can be considered as a referent for \"Dan Quayle\".", "labels": [], "entities": []}, {"text": "Because, coreferents and referents can be depicted in a graphical representation of the discourse content, it seems interesting to exploit this graph structure in order to compute aboutness.", "labels": [], "entities": []}, {"text": "This approach is inspired by studies in cognitive science on text comprehension.", "labels": [], "entities": []}, {"text": "When humans read a text, they make many inferences about and link information that is found in the text, a behavior that influences aboutness assessment.", "labels": [], "entities": []}, {"text": "Automated aboutness computation has many applications such as text indexing, summarization, and text linking.", "labels": [], "entities": [{"text": "Automated aboutness computation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5962051848570505}, {"text": "text indexing", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.7729054391384125}, {"text": "summarization", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9870895743370056}, {"text": "text linking", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.8196326792240143}]}, {"text": "We focus on estimating the aboutness score of a text given an input query in the form of a person proper name.", "labels": [], "entities": []}, {"text": "The score should reflect how much the text deals with biographical information about the person.", "labels": [], "entities": []}, {"text": "We present an algorithm based on eigenvector analysis of the link matrix of the discourse graph built by the noun phrase coreferents and referents.", "labels": [], "entities": []}, {"text": "We test the approach with a small set of documents, which we rank by decreasing aboutness of the input entity.", "labels": [], "entities": []}, {"text": "We compare the results with results obtained by traditional approaches such as a normalized term frequency (possibly corrected by coreference resolution and augmented with other referent information).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.8739237189292908}]}, {"text": "Although the results on a small test set do not pretend to give firm evidence on the validity of our approach, our contribution lies in the reflection of using graph based document representations of discourse content and exploiting this structure in content recognition.", "labels": [], "entities": [{"text": "content recognition", "start_pos": 251, "end_pos": 270, "type": "TASK", "confidence": 0.7452471852302551}]}], "datasetContent": [{"text": "For learning person related words we used a training corpus consisting of biographical texts of persons obtained from the Web (from http://www.biography.com) and biographical and non-biographical texts from DUC-2002 and DUC-2003.", "labels": [], "entities": [{"text": "DUC-2002", "start_pos": 207, "end_pos": 215, "type": "DATASET", "confidence": 0.9696034789085388}, {"text": "DUC-2003", "start_pos": 220, "end_pos": 228, "type": "DATASET", "confidence": 0.920272707939148}]}, {"text": "For considering a term as biography-related, we set a likelihood ratio threshold such that the hypothesis of independence can be rejected with a significance level of less than 0.0025, assuring that the selected terms are really biography-related.", "labels": [], "entities": []}, {"text": "In order to evaluate the aboutness computation, we considered five input queries consisting of a proper person name phrase (\"Dan Quayle\" (D), \"Hillary Clinton\" (H), \"Napoleon\" (N), \"Sadam Hussein\" (S) and \"Sharon Stone\" (ST)) and downloaded for each of the queries 5 texts from the Web (each text contains minimally once an exact match with the input query).", "labels": [], "entities": []}, {"text": "Two persons were asked to rank the texts according to relevancy, if they were searching biographical information on the input person (100% agreement was obtained).", "labels": [], "entities": []}, {"text": "Two aspects are important in determining relevancy: a text should really and almost exclusively contain biographical information of the input person in order not to lose time with other information.", "labels": [], "entities": []}, {"text": "For each query, at least one of the texts is a biographical text and one of the texts only marginally mentions the person in question.", "labels": [], "entities": []}, {"text": "All texts except for the biography texts speak about other persons, and pronouns are abundantly used.", "labels": [], "entities": []}, {"text": "The \"Hillary Clinton\" texts do not contain many other persons except for Hillary, in contrast with the \"Dan Quayle\", \"Napoleon\" and \"Sadam Hussein\" texts.", "labels": [], "entities": [{"text": "Hillary Clinton\" texts", "start_pos": 5, "end_pos": 27, "type": "DATASET", "confidence": 0.7211371958255768}, {"text": "Napoleon\" and \"Sadam Hussein\" texts", "start_pos": 118, "end_pos": 153, "type": "DATASET", "confidence": 0.7316948212683201}]}, {"text": "The \"Hillary Clinton\" texts are in general quite relevant for this first lady.", "labels": [], "entities": [{"text": "Hillary Clinton\" texts", "start_pos": 5, "end_pos": 27, "type": "DATASET", "confidence": 0.6254504323005676}]}, {"text": "For \"Napoleon\" there is one biographical text on Napoleon's surgeon that mentions Napoleon only marginally.", "labels": [], "entities": []}, {"text": "The \"Dan Quayle\" texts contain a lot of direct speech.", "labels": [], "entities": [{"text": "Dan Quayle\" texts", "start_pos": 5, "end_pos": 22, "type": "DATASET", "confidence": 0.872153252363205}]}, {"text": "For \"Sharon Stone\" 4 out of the 5 texts described a movie in which this actress played a role, thus being only marginally relevant fora demand of biographical data of the actress.", "labels": [], "entities": []}, {"text": "Then we ranked the texts based on the TF, TFCOREF, TFREF, TFCOREFREF and LM scores and computed the congruence of each ranking (R x ) with the manual ranking (R m ).", "labels": [], "entities": [{"text": "TFREF", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.7784154415130615}, {"text": "TFCOREFREF", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.8593795299530029}]}, {"text": "We used the following measure of similarity of the rankings: where n is the number of items in the 2 rankings and r x,i and r m,i denote the position of the ith item in Rx and R m. respectively.", "labels": [], "entities": [{"text": "similarity", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9859021902084351}]}], "tableCaptions": []}