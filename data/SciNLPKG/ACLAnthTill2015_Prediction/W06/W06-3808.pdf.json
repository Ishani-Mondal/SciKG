{"title": [{"text": "Seeing stars when there aren't many stars: Graph-based semi-supervised learning for sentiment categorization", "labels": [], "entities": [{"text": "sentiment categorization", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.965505987405777}]}], "abstractContent": [{"text": "We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9242244064807892}, {"text": "rating inference", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.9001879394054413}]}, {"text": "Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., \"4 stars\"), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text.", "labels": [], "entities": []}, {"text": "In particular, we are interested in the situation where labeled data is scarce.", "labels": [], "entities": []}, {"text": "We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance.", "labels": [], "entities": []}, {"text": "We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task.", "labels": [], "entities": []}, {"text": "We then solve an optimization problem to obtain a smooth rating function over the whole graph.", "labels": [], "entities": []}, {"text": "When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9489185810089111}]}], "introductionContent": [{"text": "Sentiment analysis of text documents has received considerable attention recently ().", "labels": [], "entities": [{"text": "Sentiment analysis of text documents", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.9473777413368225}]}, {"text": "Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9468252956867218}]}, {"text": "In particular Pang and Lee proposed the rating-inference problem.", "labels": [], "entities": []}, {"text": "Rating inference is harder than binary positive / negative opinion classification.", "labels": [], "entities": [{"text": "Rating inference", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8980131149291992}, {"text": "binary positive / negative opinion classification", "start_pos": 32, "end_pos": 81, "type": "TASK", "confidence": 0.6596434116363525}]}, {"text": "The goal is to infer a numerical rating from reviews, for example the number of \"stars\" that a critic gave to a movie.", "labels": [], "entities": []}, {"text": "Pang and Lee showed that supervised machine learning techniques (classification and regression) work well for rating inference with large amounts of training data.", "labels": [], "entities": [{"text": "rating inference", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.953125387430191}]}, {"text": "However, review documents often do not come with numerical ratings.", "labels": [], "entities": []}, {"text": "We call such documents unlabeled data.", "labels": [], "entities": []}, {"text": "Standard supervised machine learning algorithms cannot learn from unlabeled data.", "labels": [], "entities": []}, {"text": "Assigning labels can be a slow and expensive process because manual inspection and domain expertise are needed.", "labels": [], "entities": [{"text": "Assigning labels", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9406402409076691}]}, {"text": "Often only a small portion of the documents can be labeled within resource constraints, so most documents remain unlabeled.", "labels": [], "entities": []}, {"text": "Supervised learning algorithms trained on small labeled sets suffer in performance.", "labels": [], "entities": []}, {"text": "Can one use the unlabeled reviews to improve rating-inference?", "labels": [], "entities": []}, {"text": "suggested that doing so should be useful.", "labels": [], "entities": []}, {"text": "We demonstrate that the answer is 'Yes.'", "labels": [], "entities": []}, {"text": "Our approach is graph-based semi-supervised learning.", "labels": [], "entities": []}, {"text": "Semi-supervised learning is an active research area in machine learning.", "labels": [], "entities": []}, {"text": "It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (.", "labels": [], "entities": []}, {"text": "This paper contains three contributions: \u2022 We present a novel adaptation of graph-based semi-supervised learning ( to the sentiment analysis domain, extending past supervised learning work by; \u2022 We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3; \u2022 We show the benefit of semi-supervised learning for rating inference with extensive experimental results in section 4.", "labels": [], "entities": [{"text": "sentiment analysis domain", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.8465529680252075}, {"text": "rating inference", "start_pos": 410, "end_pos": 426, "type": "TASK", "confidence": 0.7990150451660156}]}], "datasetContent": [{"text": "We performed experiments using the movie review documents and accompanying 4-class (C = {0, 1, 2, 3}) labels found in the \"scale dataset v1.0\" available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in (Pang and).", "labels": [], "entities": [{"text": "movie review documents", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.6408187051614126}, {"text": "Pang", "start_pos": 233, "end_pos": 237, "type": "DATASET", "confidence": 0.9255110025405884}]}, {"text": "We chose 4-class instead of 3-class labeling because it is harder.", "labels": [], "entities": []}, {"text": "The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents.", "labels": [], "entities": []}, {"text": "We ran experiments individually for each author.", "labels": [], "entities": []}, {"text": "Each document is represented as a {0, 1} word-presence vector, normalized to sum to 1.", "labels": [], "entities": []}, {"text": "We systematically vary labeled set size |L| \u2208 {0.9n, 800, 400, 200, 100, 50, 25, 12, 6} to observe the effect of semi-supervised learning.", "labels": [], "entities": []}, {"text": "|L| = 0.9n is included to match 10-fold cross validation used by).", "labels": [], "entities": []}, {"text": "For each |L| we run 20 trials where we randomly split the corpus into labeled and test (unlabeled) sets.", "labels": [], "entities": []}, {"text": "We ensure that all four classes are represented in each labeled set.", "labels": [], "entities": []}, {"text": "The same random splits are used for all methods, allowing paired t-tests for statistical significance.", "labels": [], "entities": []}, {"text": "All reported results are average test set accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9662240147590637}]}, {"text": "We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in (Pang and).", "labels": [], "entities": [{"text": "Pang", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.9204825758934021}]}], "tableCaptions": [{"text": " Table 1: Accuracy using shared (c = 0.2, \u03b1 = 1.5)  vs. author-specific parameters, with |L| = 0.9n.", "labels": [], "entities": []}, {"text": " Table 2: 20-trial average unlabeled set accuracy for each author across different labeled set sizes and meth- ods. In each row, we list in bold the best result and any results that cannot be distinguished from it with a  paired t-test at the 0.05 level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9323049783706665}]}]}