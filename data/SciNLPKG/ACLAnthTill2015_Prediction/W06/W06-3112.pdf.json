{"title": [{"text": "Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9362038671970367}]}], "abstractContent": [{"text": "In this paper we present a novel method for deriving paraphrases during automatic MT evaluation using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9539008438587189}, {"text": "word and phrase alignment", "start_pos": 183, "end_pos": 208, "type": "TASK", "confidence": 0.5881374850869179}]}, {"text": "Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation.", "labels": [], "entities": [{"text": "word and phrase alignment", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6115621030330658}]}, {"text": "The method produces lexical and low-level syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with a variety of automatic MT evaluation system.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 196, "end_pos": 209, "type": "TASK", "confidence": 0.9173164963722229}]}], "introductionContent": [{"text": "Since their appearance, BLEU () and NIST) have been the standard tools used for evaluating the quality of machine translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9977133274078369}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7800667583942413}]}, {"text": "They both score candidate translations on the basis of the number of n-grams it shares with one or more reference translations provided.", "labels": [], "entities": []}, {"text": "Such automatic measures are indispensable in the development of machine translation systems, because they allow the developers to conduct frequent, cost-effective, and fast evaluations of their evolving models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7913993000984192}]}, {"text": "These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any divergence from them.", "labels": [], "entities": []}, {"text": "In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical choices and syntactic structure it contains, even though perfectly legitimate, are not present in at least one of the references.", "labels": [], "entities": []}, {"text": "Necessarily, this score would not reflect a much more favourable human judgment that such a translation would receive.", "labels": [], "entities": []}, {"text": "The limitations of string comparison are the reason why it is advisable to provide multiple references fora candidate translation in the BLEU-or NIST-based evaluation in the first place.", "labels": [], "entities": [{"text": "string comparison", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6820494830608368}, {"text": "BLEU-or", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.9160106778144836}, {"text": "NIST-based", "start_pos": 145, "end_pos": 155, "type": "DATASET", "confidence": 0.6813853979110718}]}, {"text": "While ( argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9910502433776855}, {"text": "NIST", "start_pos": 173, "end_pos": 177, "type": "DATASET", "confidence": 0.8726739287376404}]}, {"text": "On the other hand, in practice even a number of references do not capture the whole potential variability of the translation.", "labels": [], "entities": []}, {"text": "Moreover, often it is the case that multiple references are not available or are too difficult and expensive to produce: when designing a statistical machine translation system, the need for large amounts of training data limits the researcher to collections of parallel corpora like Europarl (, which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, often exceeds the resources available.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.6426764329274496}, {"text": "Europarl", "start_pos": 284, "end_pos": 292, "type": "DATASET", "confidence": 0.9637682437896729}]}, {"text": "Therefore, it would be desirable to find away to automatically generate legitimate translation alternatives not present in the reference(s) already available.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel method that automatically derives paraphrases using only the source and reference texts involved in for the evaluation of French-to-English Europarl translations produced by two MT systems: statistical phrase-based Pharaoh) and rulebased Logomedia.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 174, "end_pos": 182, "type": "DATASET", "confidence": 0.7438108325004578}]}, {"text": "In using what is in fact a miniature bilingual corpus our approach differs from the mainstream paraphrase generation based on monolingual resources.", "labels": [], "entities": []}, {"text": "We show that paraphrases produced in this way are more relevant to the task of evaluating machine translation than the use of external lexical knowledge resources like thesauri or WordNet 2 , in that our paraphrases contain both lexical equivalents and low-level syntactic variants, and in that, as a side-effect, evaluation bitextderived paraphrasing naturally yields domainspecific paraphrases.", "labels": [], "entities": [{"text": "evaluating machine translation", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.6250571807225546}]}, {"text": "The paraphrases generated from the evaluation bitext are added to the existing reference sentences, in effect creating multiple references and resulting in a higher score for the candidate translation.", "labels": [], "entities": []}, {"text": "Our hypothesis, confirmed by the experiments in this paper, is that the scores raised by additional references produced in this way will correlate better with human judgment than the original scores.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 describes related work; Section 3 describes our method and presents examples of derived paraphrases; Section 4 presents the results of the comparison between the BLUE and NIST scores fora single-reference translation and the same translation using the paraphrases automatically generated from the bitext, as well as the correlations between the scores and human judgment; Section 5 discusses ongoing work; Section 6 concludes.", "labels": [], "entities": [{"text": "BLUE and NIST scores", "start_pos": 225, "end_pos": 245, "type": "DATASET", "confidence": 0.5975976139307022}]}], "datasetContent": [{"text": "The insensitivity of BLEU and NIST to perfectly legitimate variation has been raised, among others, in), but the criticism is widespread.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.920259416103363}, {"text": "NIST", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8960332870483398}]}, {"text": "Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level), a problem also noted by  and.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.985403835773468}]}, {"text": "A side effect of this phenomenon is that BLEU is less reliable for smaller data sets, so the advantage it provides in the speed of evaluation is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test data set in order to obtain a reliable score for their system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9978389143943787}]}, {"text": "Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.6760885715484619}]}, {"text": "Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String ( or Translation Error Rate ().", "labels": [], "entities": [{"text": "word reordering", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7214109599590302}, {"text": "Maximum Matching String", "start_pos": 68, "end_pos": 91, "type": "METRIC", "confidence": 0.7788885037104288}, {"text": "Translation Error Rate", "start_pos": 97, "end_pos": 119, "type": "METRIC", "confidence": 0.5962074001630148}]}, {"text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (), which employs aversion of edit distance for word substitution and reordering; METEOR (, which uses stemming and WordNet synonymy; and a linear regression model developed by), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.", "labels": [], "entities": [{"text": "word substitution", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7558336555957794}, {"text": "METEOR", "start_pos": 210, "end_pos": 216, "type": "METRIC", "confidence": 0.9138669967651367}, {"text": "proper name matching", "start_pos": 407, "end_pos": 427, "type": "TASK", "confidence": 0.6520707408587137}]}, {"text": "A closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like WordNet, verb class databases, and extensive text preparation: stemming, tagging, etc.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 229, "end_pos": 236, "type": "DATASET", "confidence": 0.9494724273681641}]}, {"text": "The advantage of our method is that it produces relevant paraphrases with nothing more than the evaluation bitext and a widely available word and phrase alignment software, and therefore can be used with any existing evaluation metric.", "labels": [], "entities": [{"text": "word and phrase alignment", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.59072045981884}]}, {"text": "For our experiment, we used two test sets, each consisting of 2000 sentences, drawn randomly from the test section of the Europarl parallel corpus.", "labels": [], "entities": [{"text": "Europarl parallel corpus", "start_pos": 122, "end_pos": 146, "type": "DATASET", "confidence": 0.9628079732259115}]}, {"text": "The source language was French and the target language was English.", "labels": [], "entities": []}, {"text": "One of the test sets was translated by Pharaoh trained on 156,000 French-English sentence pairs.", "labels": [], "entities": []}, {"text": "The other test set was translated by Logomedia, a commercially available rule-based MT system.", "labels": [], "entities": [{"text": "Logomedia", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9101598858833313}, {"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.8884633779525757}]}, {"text": "Each test set consisted therefore of three files: the French source file, the English translation file, and the English reference file.", "labels": [], "entities": [{"text": "French source file", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.7987247904141744}]}, {"text": "Each translation was evaluated by the BLEU and NIST metrics first with the single reference, then with the multiple references for each sentence using the paraphrases automatically generated from the source-reference mini corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9681563377380371}, {"text": "NIST", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.8616894483566284}]}, {"text": "A subset of a 100 sentences was randomly extracted from each test set and evaluated by two independent human judges with respect to accuracy and fluency; the human scores were then compared to the BLEU and NIST scores for the single-reference and the automatically generated multiple-reference files.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9986698627471924}, {"text": "BLEU", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9948983788490295}, {"text": "NIST", "start_pos": 206, "end_pos": 210, "type": "DATASET", "confidence": 0.8948515057563782}]}], "tableCaptions": [{"text": " Table 1. Comparison of single-reference and multi- reference scores for test set PH and test set LM", "labels": [], "entities": []}, {"text": " Table 2. Pearson's correlation between human  judgment and single-reference and multiple- reference BLEU, smoothed BLEU, and NIST for  subset PH (of test set PH)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.861668586730957}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.7897298336029053}]}, {"text": " Table 3. It is  easy to see that Logomedia's translation received a  higher mean score (on a scale 0 to 5) from the hu- man judges and with less variance than Pharaoh.", "labels": [], "entities": [{"text": "variance", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9434924125671387}]}, {"text": " Table 3. Human judgment mean scores and coeffi- cients of variation for Subset PH and Subset LM", "labels": [], "entities": []}, {"text": " Table 6. Semantic accuracy of paraphrases", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9882584810256958}, {"text": "paraphrases", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.5747973322868347}]}]}