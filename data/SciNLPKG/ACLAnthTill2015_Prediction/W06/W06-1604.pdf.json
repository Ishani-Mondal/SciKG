{"title": [{"text": "Detecting Parser Errors Using Web-based Semantic Filters", "labels": [], "entities": [{"text": "Detecting Parser Errors", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.840308944384257}]}], "abstractContent": [{"text": "NLP systems for tasks such as question answering and information extraction typically rely on statistical parsers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9045895934104919}, {"text": "information extraction", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7898381948471069}]}, {"text": "But the efficacy of such parsers can be surprisingly low, particularly for sentences drawn from heterogeneous corpora such as the Web.", "labels": [], "entities": []}, {"text": "We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences, which can be detected automatically using semantic information obtained from the Web.", "labels": [], "entities": []}, {"text": "Based on this observation, we introduce Web-based semantic filtering-a novel, domain-independent method for automatically detecting and discarding incorrect parses.", "labels": [], "entities": [{"text": "Web-based semantic filtering-a", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6057689885298411}]}, {"text": "We measure the effectiveness of our filtering system, called WOODWARD, on two test collections.", "labels": [], "entities": []}, {"text": "On a set of TREC questions, it reduces error by 67%.", "labels": [], "entities": [{"text": "error", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9989821314811707}]}, {"text": "On a set of more complex Penn Treebank sentences, the reduction in error rate was 20%.", "labels": [], "entities": [{"text": "Penn Treebank sentences", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.9838336308797201}, {"text": "error rate", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.969986617565155}]}], "introductionContent": [{"text": "Semantic processing of text in applications such as question answering or information extraction frequently relies on statistical parsers.", "labels": [], "entities": [{"text": "Semantic processing of text", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8951705098152161}, {"text": "question answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8825314342975616}, {"text": "information extraction", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7924777865409851}]}, {"text": "Unfortunately, the efficacy of state-of-the-art parsers can be disappointingly low.", "labels": [], "entities": []}, {"text": "For example, we found that the Collins parser correctly parsed just 42% of the list and factoid questions from TREC 2004 (that is, 42% of the parses had 100% precision and 100% recall on labeled constituents).", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 111, "end_pos": 120, "type": "DATASET", "confidence": 0.7710684537887573}, {"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9950562715530396}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9974173307418823}]}, {"text": "Similarly, this parser produced 45% correct parses on a subset of 100 sentences from section 23 of the Penn Treebank.", "labels": [], "entities": [{"text": "section 23 of the Penn Treebank", "start_pos": 85, "end_pos": 116, "type": "DATASET", "confidence": 0.7216456731160482}]}, {"text": "Although statistical parsers continue to improve their efficacy overtime, progress is slow, particularly for Web applications where training the parsers on a \"representative\" corpus of handtagged sentences is not an option.", "labels": [], "entities": []}, {"text": "Because of the heterogeneous nature of text on the Web, such a corpus would be exceedingly difficult to generate.", "labels": [], "entities": []}, {"text": "In response, this paper investigates the possibility of detecting parser errors by using semantic information obtained from the Web.", "labels": [], "entities": []}, {"text": "Our fundamental hypothesis is that incorrect parses often result in wildly implausible semantic interpretations of sentences, which can be detected automatically in certain circumstances.", "labels": [], "entities": []}, {"text": "Consider, for example, the following sentence from the Wall Street Journal: \"That compares with per-share earnings from continuing operations of 69 cents.\"", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.9637758334477743}]}, {"text": "The Collins parser yields a parse that attaches \"of 69 cents\" to \"operations,\" rather than \"earnings.\"", "labels": [], "entities": [{"text": "Collins", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.938008189201355}]}, {"text": "By computing the mutual information between \"operations\" and \"cents\" on the Web, we can detect that this attachment is unlikely to be correct.", "labels": [], "entities": []}, {"text": "Our WOODWARD system detects parser errors as follows.", "labels": [], "entities": []}, {"text": "First, it maps the tree produced by a parser to a relational conjunction (RC), a logicbased representation language that we describe in Section 2.1.", "labels": [], "entities": []}, {"text": "Second, WOODWARD employs four distinct methods for analyzing whether a conjunct in the RC is likely to be \"reasonable\" as described in Section 2.", "labels": [], "entities": [{"text": "WOODWARD", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.6502033472061157}]}, {"text": "Our approach makes several assumptions.", "labels": [], "entities": []}, {"text": "First, if the sentence is absurd to begin with, then a correct parse could be deemed incorrect.", "labels": [], "entities": []}, {"text": "Second, we require a corpus whose content overlaps at least in part with the content of the sentences to be parsed.", "labels": [], "entities": []}, {"text": "Otherwise, much of our semantic analysis is impossible.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8179659247398376}]}, {"text": "In applications such as Web-based question answering, these assumptions are quite natural.", "labels": [], "entities": [{"text": "question answering", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8025309145450592}]}, {"text": "The questions are about topics that are covered extensively on the Web, and we can assume that most questions link verbs to nouns in reasonable combinations.", "labels": [], "entities": []}, {"text": "Likewise, when using parsing for information extraction, we would expect our assumptions to hold as well.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8398612439632416}]}, {"text": "Our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We introduce Web-based semantic filteringa novel, domain-independent method for detecting and discarding incorrect parses.", "labels": [], "entities": [{"text": "Web-based semantic filteringa", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.5521070659160614}]}, {"text": "2. We describe four techniques for analyzing relational conjuncts using semantic information obtained from the Web, and assess their efficacy both separately and in combination.", "labels": [], "entities": []}, {"text": "3. We find that WOODWARD can filter good parses from bad on TREC 2004 questions fora reduction of 67% in error rate.", "labels": [], "entities": [{"text": "WOODWARD", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.6869066953659058}, {"text": "TREC 2004 questions", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.942885677019755}, {"text": "error rate", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9703613519668579}]}, {"text": "On a harder set of sentences from the Penn Treebank, the reduction in error rate is 20%.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9942134916782379}, {"text": "error rate", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9809965789318085}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We give an overview of related work in Section 1.1.", "labels": [], "entities": []}, {"text": "Section 2 describes semantic filtering, including our RC representation and the four Webbased filters that constitute the WOODWARD system.", "labels": [], "entities": [{"text": "semantic filtering", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7926964163780212}]}, {"text": "Section 3 presents our experiments and results, and section 4 concludes and gives ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we tested the ability of WOOD-WARD to detect bad parses.", "labels": [], "entities": [{"text": "detect bad parses", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.6461029450098673}]}, {"text": "Our experiments proceeded as follows: we parsed a set of sentences, ran the semantic interpreter on them, and labeled each parse and each relation in the resulting RCs for correctness.", "labels": [], "entities": []}, {"text": "We then extracted all of the necessary information from the Web and TextRunner.", "labels": [], "entities": [{"text": "TextRunner", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9500570893287659}]}, {"text": "We divided the sentences into a training and test set, and trained the filters on the labeled RCs from the training sentences.", "labels": [], "entities": []}, {"text": "Finally, we ran each of the filters and WOODWARD on the test set to predict which parses were correct.", "labels": [], "entities": [{"text": "WOODWARD", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9861410856246948}]}, {"text": "We report the results below, but first we describe our datasets and tools in more detail.", "labels": [], "entities": []}, {"text": "Because question-answering is a key application, we began with data from the TREC questionanswering track.", "labels": [], "entities": [{"text": "TREC questionanswering track", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.8760155836741129}]}, {"text": "We split the data into a training set of 61 questions (all of the TREC 2002 and TREC 2003 questions), and a test set of 55 questions (all list and factoid questions from TREC 2004).", "labels": [], "entities": [{"text": "TREC 2002 and TREC 2003 questions", "start_pos": 66, "end_pos": 99, "type": "DATASET", "confidence": 0.8822548985481262}]}, {"text": "We preprocessed the questions to remove parentheticals (this affected 3 training questions and 1 test question).", "labels": [], "entities": []}, {"text": "We removed 12 test questions because the Collins parser did not parse them as questions, and that error was too easy to detect.", "labels": [], "entities": [{"text": "Collins", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.939224362373352}]}, {"text": "25 training questions had the same error, but we left them in to provide more training data.", "labels": [], "entities": []}, {"text": "We used the Penn Treebank as our second data set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.9900681674480438}]}, {"text": "Training sentences were taken from section 22, and test sentences from section 23.", "labels": [], "entities": []}, {"text": "Because PBF is time-consuming, we took a subset of 100 sentences from each section to expedite our experiments.", "labels": [], "entities": [{"text": "PBF", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.49901989102363586}]}, {"text": "We extracted from each section the first 100 sentences that did not contain conjunctions, and for which all of the errors, if any, were contained in preposition and verb relations.", "labels": [], "entities": []}, {"text": "For our parser, we used Bikel's implementation of the Collins parsing model, trained on sections 2-21 of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9877864420413971}]}, {"text": "We only use the topranked parse for each sentence.", "labels": [], "entities": []}, {"text": "For the TREC data only, we first POS-tagged each question using Ratnaparkhi's MXPOST tagger.", "labels": [], "entities": [{"text": "TREC data", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.6809944957494736}, {"text": "Ratnaparkhi's MXPOST tagger", "start_pos": 64, "end_pos": 91, "type": "DATASET", "confidence": 0.8389373868703842}]}, {"text": "We judged each of the TREC parses manually for correctness, but scored the Treebank parses automatically.", "labels": [], "entities": [{"text": "TREC parses", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.5179102867841721}, {"text": "Treebank parses", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.8160025477409363}]}], "tableCaptions": [{"text": " Table 1: Accuracy of the filters on three relation types in the TREC 2004 questions and WSJ data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9916914701461792}, {"text": "TREC 2004 questions", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.9282255172729492}, {"text": "WSJ data", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8641901314258575}]}, {"text": " Table 2: Performance of WOODWARD on different data sets. Parser efficacy reports the percentage of sentences that", "labels": [], "entities": [{"text": "Parser efficacy", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.9429033994674683}]}]}