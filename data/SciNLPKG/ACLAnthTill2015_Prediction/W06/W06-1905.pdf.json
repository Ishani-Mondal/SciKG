{"title": [{"text": "Keyword Translation Accuracy and Cross-Lingual Question Answering in Chinese and Japanese", "labels": [], "entities": [{"text": "Keyword Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7057497799396515}, {"text": "Accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.7871199250221252}, {"text": "Cross-Lingual Question Answering", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.616419384876887}]}], "abstractContent": [{"text": "In this paper, we describe the extension of an existing monolingual QA system for English-to-Chinese and English-to-Japanese cross-lingual question answering (CLQA).", "labels": [], "entities": [{"text": "cross-lingual question answering (CLQA)", "start_pos": 125, "end_pos": 164, "type": "TASK", "confidence": 0.7088566025098165}]}, {"text": "We also attempt to characterize the influence of translation on CLQA performance through experimental evaluation and analysis.", "labels": [], "entities": []}, {"text": "The paper also describes some language-specific issues for keyword translation in CLQA.", "labels": [], "entities": [{"text": "keyword translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8225048184394836}]}], "introductionContent": [{"text": "The JAVELIN system is a modular, extensible architecture for building question-answering (QA) systems).", "labels": [], "entities": []}, {"text": "Since the JAVELIN architecture is language-independent, we extended the original English version of JAVELIN for cross-language question answering (CLQA) in Chinese and Japanese.", "labels": [], "entities": [{"text": "cross-language question answering (CLQA)", "start_pos": 112, "end_pos": 152, "type": "TASK", "confidence": 0.7654711405436198}]}, {"text": "The same overall architecture was used for both systems, allowing us to compare the performance of the two systems.", "labels": [], "entities": []}, {"text": "In this paper, we describe how we extended the monolingual system for CLQA (see Section 3).", "labels": [], "entities": []}, {"text": "Keyword translation is a crucial element of the system; we describe our translation module in Section 3.2.", "labels": [], "entities": [{"text": "Keyword translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8276984989643097}]}, {"text": "In Section 4, we evaluate the end-to-end CLQA systems using three different translation methods.", "labels": [], "entities": []}, {"text": "Language-specific translation issues are discussed in Section 5.", "labels": [], "entities": [{"text": "Language-specific translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5424393564462662}]}], "datasetContent": [{"text": "To evaluate the effect of translation accuracy on the overall performance of the CLQA system, we conducted several experiments using different translation methods.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9333377480506897}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7718480825424194}]}, {"text": "Three different runs were carried out for both the E-C and E-J systems, using the same 200-question test set and the document corpora provided by the NTCIR CLQA task.", "labels": [], "entities": [{"text": "NTCIR CLQA task", "start_pos": 150, "end_pos": 165, "type": "DATASET", "confidence": 0.8777510126431783}]}, {"text": "The first run was a fully automatic run using the original translation module in the CLQA system; the result is exactly same as the one we submitted to NTCIR5 CLQA.", "labels": [], "entities": [{"text": "NTCIR5 CLQA", "start_pos": 152, "end_pos": 163, "type": "DATASET", "confidence": 0.9135514795780182}]}, {"text": "For the second run, we manually translated the keywords that were selected by the Question Analyzer module.", "labels": [], "entities": []}, {"text": "This translation was done by looking at only the selected keywords, but not the original question.", "labels": [], "entities": [{"text": "translation", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9756812453269958}]}, {"text": "For both E-C and E-J tasks, the NTCIR organizers provided the translations for the English questions, which we assume are the goldstandard translations.", "labels": [], "entities": [{"text": "NTCIR", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8725506067276001}]}, {"text": "Taking advantage of this resource, in the third run we simply looked up the corresponding term for each English keyword from the gold-standard translation of the question.", "labels": [], "entities": []}, {"text": "The results for these runs are shown in Table 7 and 8 below.", "labels": [], "entities": []}, {"text": "We found that in the NTCIR task, the supported/correct document set was not complete.", "labels": [], "entities": [{"text": "NTCIR task", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.4900875985622406}]}, {"text": "Some answers judged as unsupported were indeed well supported, but the supporting document did not appear in NTCIR's correct document set.", "labels": [], "entities": [{"text": "NTCIR's correct document set", "start_pos": 109, "end_pos": 137, "type": "DATASET", "confidence": 0.9430403232574462}]}, {"text": "Therefore, we think the Top1+U column is more informative for this evaluation.", "labels": [], "entities": []}, {"text": "From and 8, it is obvious that the overall performance increases as translation accuracy", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. E-C Keyword Translation", "labels": [], "entities": [{"text": "Keyword Translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7040076553821564}]}, {"text": " Table 2. Translation Pair Page Counts", "labels": [], "entities": [{"text": "Translation Pair", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6181720495223999}]}, {"text": " Table 4. Individual Term Page Counts", "labels": [], "entities": [{"text": "Individual Term Page", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.863679051399231}]}, {"text": " Table 5. Target Language Query Page Counts", "labels": [], "entities": []}, {"text": " Table 6. Translation Score, Language Model  Score, and Overall Score", "labels": [], "entities": [{"text": "Overall Score", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.895329475402832}]}, {"text": " Table 7. Effect of Translation (E-C)", "labels": [], "entities": []}, {"text": " Table 8. Effect of Translation (E-J)", "labels": [], "entities": [{"text": "Translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.7155146598815918}]}]}