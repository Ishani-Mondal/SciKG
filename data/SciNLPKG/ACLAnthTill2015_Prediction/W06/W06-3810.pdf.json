{"title": [{"text": "Graph-based Generalized Latent Semantic Analysis for Document Representation", "labels": [], "entities": [{"text": "Generalized Latent Semantic Analysis", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.7506976425647736}, {"text": "Document Representation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8195454478263855}]}], "abstractContent": [{"text": "Document indexing and representation of term-document relations are very important for document clustering and retrieval.", "labels": [], "entities": [{"text": "Document indexing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8684606552124023}, {"text": "document clustering", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.6857729703187943}]}, {"text": "In this paper, we combine a graph-based dimensionality reduction method with a corpus-based association measure within the Generalized Latent Semantic Analysis framework.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6814486980438232}, {"text": "Generalized Latent Semantic Analysis", "start_pos": 123, "end_pos": 159, "type": "TASK", "confidence": 0.7191532254219055}]}, {"text": "We evaluate the graph-based GLSA on the document clustering task.", "labels": [], "entities": [{"text": "document clustering task", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8020229736963908}]}], "introductionContent": [{"text": "Document indexing and representation of termdocument relations are very important issues for document clustering and retrieval.", "labels": [], "entities": [{"text": "Document indexing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8955715298652649}, {"text": "document clustering", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.684816911816597}]}, {"text": "Although the vocabulary space is very large, content bearing words are often combined into semantic classes that contain synonyms and semantically related words.", "labels": [], "entities": []}, {"text": "Hence there has been a considerable interest in lowdimensional term and document representations.", "labels": [], "entities": [{"text": "lowdimensional term and document representations", "start_pos": 48, "end_pos": 96, "type": "TASK", "confidence": 0.566717404127121}]}, {"text": "Latent Semantic Analysis (LSA)) is one of the best known dimensionality reduction algorithms.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA))", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7213177482287089}, {"text": "dimensionality reduction", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.736064076423645}]}, {"text": "The dimensions of the LSA vector space can be interpreted as latent semantic concepts.", "labels": [], "entities": []}, {"text": "The cosine similarity between the LSA document vectors corresponds to documents' similarity in the input space.", "labels": [], "entities": []}, {"text": "LSA preserves the documents similarities which are based on the inner products of the input bag-of-word documents and it preserves these similarities globally.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.72319096326828}]}, {"text": "More recently, a number of graph-based dimensionality reduction techniques were successfully applied to document clustering and retrieval).", "labels": [], "entities": [{"text": "graph-based dimensionality reduction", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.6802883942921957}, {"text": "document clustering", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7343465387821198}]}, {"text": "The main advantage of the graph-based approaches over LSA is the notion of locality.", "labels": [], "entities": []}, {"text": "Laplacian Eigenmaps Embedding and Locality Preserving Indexing (LPI) () discover the local structure of the term and document space and compute a semantic subspace with a stronger discriminative power.", "labels": [], "entities": [{"text": "Locality Preserving Indexing (LPI)", "start_pos": 34, "end_pos": 68, "type": "METRIC", "confidence": 0.7524530390898386}]}, {"text": "Laplacian Eigenmaps Embedding and LPI preserve the input similarities only locally, because this information is most reliable.", "labels": [], "entities": []}, {"text": "Laplacian Eigenmaps Embedding does not provide a fold-in procedure for unseen documents.", "labels": [], "entities": []}, {"text": "LPI is a linear approximation to Laplacian Eigenmaps Embedding that eliminates this problem.", "labels": [], "entities": [{"text": "LPI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7510189414024353}]}, {"text": "Similar to LSA, the input similarities to LPI are based on the inner products of the bag-of-word documents.", "labels": [], "entities": []}, {"text": "Laplacian Eigenmaps Embedding can use any kind of similarity in the original space.", "labels": [], "entities": []}, {"text": "Generalized Latent Semantic Analysis (GLSA) () is a framework for computing semantically motivated term and document vectors.", "labels": [], "entities": [{"text": "Generalized Latent Semantic Analysis (GLSA)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7269302308559418}]}, {"text": "It extends the LSA approach by focusing on term vectors instead of the dual document-term representation.", "labels": [], "entities": []}, {"text": "GLSA requires a measure of semantic association between terms and a method of dimensionality reduction.", "labels": [], "entities": [{"text": "GLSA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6945332884788513}]}, {"text": "In this paper, we use GLSA with point-wise mutual information as a term association measure.", "labels": [], "entities": []}, {"text": "We introduce the notion of locality into this framework and propose to use Laplacian Eigenmaps Embedding as a dimensionality reduction algorithm.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.6785942018032074}]}, {"text": "We evaluate the importance of locality for document representation in document clustering experiments.", "labels": [], "entities": [{"text": "document representation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.711182713508606}, {"text": "document clustering", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.6719005852937698}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Sec-tion 2 contains the outline of the graph-based GLSA algorithm.", "labels": [], "entities": []}, {"text": "Section 3 presents our experiments, followed by conclusion in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a document clustering experiment for the Reuters-21578 collection.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.676860049366951}, {"text": "Reuters-21578 collection", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.9680189490318298}]}, {"text": "To collect the cooccurrence statistics for the similarities matrix S we used a subset of the English Gigaword collection (LDC), containing New York Times articles labeled as \"story\".", "labels": [], "entities": [{"text": "English Gigaword collection (LDC)", "start_pos": 93, "end_pos": 126, "type": "DATASET", "confidence": 0.9035423497358958}]}, {"text": "We had 1,119,364 documents with 771,451 terms.", "labels": [], "entities": []}, {"text": "We used the Lemur toolkit 1 to tokenize and index all document collections used in our experiments, with stemming and a list of stop words.", "labels": [], "entities": []}, {"text": "Since Locality Preserving Indexing algorithm (LPI) is most related to the graph-based GLSA L , we ran experiments similar to those reported in).", "labels": [], "entities": [{"text": "Locality Preserving Indexing", "start_pos": 6, "end_pos": 34, "type": "TASK", "confidence": 0.7555186847845713}]}, {"text": "We computed the GLSA document vectors for the 20 largest categories from the Reuters-21578 document collection.", "labels": [], "entities": [{"text": "Reuters-21578 document collection", "start_pos": 77, "end_pos": 110, "type": "DATASET", "confidence": 0.9817410310109457}]}, {"text": "We had 8564 documents and 7173 terms.", "labels": [], "entities": []}, {"text": "We used the same list of 30 TREC words as in () which are listed in table 1 2 . For each word on this list, we generated a cluster as a subset of Reuters documents that contained this word.", "labels": [], "entities": [{"text": "Reuters documents", "start_pos": 146, "end_pos": 163, "type": "DATASET", "confidence": 0.9142968058586121}]}, {"text": "Clusters are not disjoint and contain documents from different Reuters categories.", "labels": [], "entities": []}, {"text": "We computed GLSA, GLSA L , LSA and LPI representations.", "labels": [], "entities": []}, {"text": "We report the results fork = 5 for the k nearest neighbors graph for LPI and Laplacian Embedding, and binary weights for the adjacency matrix.", "labels": [], "entities": [{"text": "fork", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.985201895236969}]}, {"text": "We report results for 300 embedding dimensions for GLSA, LPI and LSA and 500 dimensions for GLSA L . We evaluate these representations in terms of how well the cosine similarity between the document vectors within each cluster corresponds to the true semantic similarity.", "labels": [], "entities": []}, {"text": "We expect documents from the same Reuters category to have higher similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9925230145454407}]}, {"text": "For each cluster we computed all pair-wise document similarities.", "labels": [], "entities": []}, {"text": "All pair-wise similarities were sorted in decreasing order.", "labels": [], "entities": []}, {"text": "The term \"inter-pair\" describes a pair of documents that have the same label.", "labels": [], "entities": []}, {"text": "For the k th inter-pair, we computed precision at k as: where p j refers to the j th inter-pair.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9991284012794495}]}, {"text": "The average of the precision values for each of the inter-pairs was used as the average precision for the particular document cluster.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9986369013786316}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9272176027297974}]}, {"text": "The first column shows the words according to which document clusters were generated and the entropy of the category distribution within that cluster.", "labels": [], "entities": []}, {"text": "The baseline was to use the tf document vectors.", "labels": [], "entities": []}, {"text": "We report results for GLSA, GLSA L , LSA and LPI.", "labels": [], "entities": [{"text": "GLSA", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.4694473445415497}, {"text": "GLSA L", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.7489092648029327}]}, {"text": "The LSA and LPI computations were based solely on the Reuters collection.", "labels": [], "entities": [{"text": "LSA", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7604191303253174}, {"text": "Reuters collection", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.9813964366912842}]}, {"text": "For GLSA and GLSA L we used the term associations computed for the Gigaword collection, as described above.", "labels": [], "entities": [{"text": "Gigaword collection", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.902880847454071}]}, {"text": "Therefore, the similarities that are preserved are quite different.", "labels": [], "entities": []}, {"text": "For LSA and LPI they reflect the term distribution specific for the Reuters collection whereas for GLSA they are more general.", "labels": [], "entities": [{"text": "Reuters collection", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.9191868901252747}]}, {"text": "By paired 2-tailed t-test, at p \u2264 0.05, GLSA outperformed all other approaches.", "labels": [], "entities": [{"text": "GLSA", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.6835055947303772}]}, {"text": "There was no significant difference in performance of GLSA L , LSA and the baseline.", "labels": [], "entities": [{"text": "GLSA L", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.7994582951068878}, {"text": "LSA", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.895167350769043}]}, {"text": "Disappointingly, we could not achieve good performance with LPI.", "labels": [], "entities": [{"text": "LPI", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9491488337516785}]}, {"text": "Its performance varies over clusters similar to that of other approaches but the average is significantly lower.", "labels": [], "entities": []}, {"text": "We would like to stress that the comparison of our results to those presented in () are only suggestive since () applied LPI to each cluster separately and used PCA as preprocessing.", "labels": [], "entities": []}, {"text": "We computed the LPI representation for the full collection and did not use PCA.: Average inter-pairs accuracy.", "labels": [], "entities": [{"text": "PCA.", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.7617201209068298}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9801939725875854}]}, {"text": "The inter-pair accuracy depended on the categories distribution within clusters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9835668802261353}]}, {"text": "For more homogeneous clusters, e.g. \"loss\", all methods (except LPI) achieve similar precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9961792230606079}]}, {"text": "For less homogeneous clusters, e.g. \"national\", \"industrial\", \"bank\", GLSA and LSA outperformed the tf document vectors more significantly.", "labels": [], "entities": [{"text": "GLSA", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.8365763425827026}]}], "tableCaptions": [{"text": " Table 1: Average inter-pairs accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9975503087043762}]}]}