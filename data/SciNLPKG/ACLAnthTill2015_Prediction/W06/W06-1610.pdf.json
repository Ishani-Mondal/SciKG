{"title": [{"text": "Re-evaluating Machine Translation Results with Paraphrase Support", "labels": [], "entities": [{"text": "Re-evaluating Machine Translation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.793234388033549}]}], "abstractContent": [{"text": "In this paper, we present ParaEval, an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations.", "labels": [], "entities": [{"text": "machine translation evaluations", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.8139310876528422}]}, {"text": "Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching.", "labels": [], "entities": [{"text": "lexical identity matching", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.6606718003749847}]}, {"text": "ParaEval addresses three important issues: support for para-phrase/synonym matching, recall measurement , and correlation with human judgments.", "labels": [], "entities": [{"text": "para-phrase/synonym matching", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.6405149400234222}, {"text": "recall measurement", "start_pos": 85, "end_pos": 103, "type": "METRIC", "confidence": 0.9267414510250092}]}, {"text": "We show that ParaEval correlates significantly better than BLEU with human assessment in measurements for both fluency and adequacy.", "labels": [], "entities": [{"text": "ParaEval", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8987057209014893}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9983370304107666}]}], "introductionContent": [{"text": "The introduction of automated evaluation procedures, such as BLEU () for machine translation (MT) and ROUGE ( for summarization, have prompted much progress and development in both of these areas of research in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9988646507263184}, {"text": "machine translation (MT)", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.8445882081985474}, {"text": "ROUGE", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.995806097984314}, {"text": "summarization", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9574536681175232}, {"text": "Natural Language Processing (NLP)", "start_pos": 211, "end_pos": 244, "type": "TASK", "confidence": 0.704854408899943}]}, {"text": "Both evaluation tasks employ a comparison strategy for comparing textual units from machine-generated and gold-standard texts.", "labels": [], "entities": []}, {"text": "Ideally, this comparison process would be performed manually, because of humans' abilities to infer, paraphrase, and use world knowledge to relate differently worded pieces of equivalent information.", "labels": [], "entities": []}, {"text": "However, manual evaluations are time consuming and expensive, thus making them a bottleneck in system development cycles.", "labels": [], "entities": []}, {"text": "BLEU measures how close machine-generated translations are to professional human translations, and ROUGE does the same with respect to summaries.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9884677529335022}, {"text": "ROUGE", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9966819882392883}]}, {"text": "Both methods incorporate the comparison of a system-produced text to one or more corresponding reference texts.", "labels": [], "entities": []}, {"text": "The closeness between texts is measured by the computation of a numeric score based on n-gram co-occurrence statistics.", "labels": [], "entities": []}, {"text": "Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses ).", "labels": [], "entities": [{"text": "MT", "start_pos": 182, "end_pos": 184, "type": "TASK", "confidence": 0.9972785115242004}, {"text": "summarization", "start_pos": 189, "end_pos": 202, "type": "TASK", "confidence": 0.9389933347702026}]}, {"text": "Text comparisons in MT and summarization evaluations are performed at different text granularity levels.", "labels": [], "entities": [{"text": "Text comparisons", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6865800768136978}, {"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9774352312088013}, {"text": "summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9810394048690796}]}, {"text": "Since most of the phrase-based, syntax-based, and rule-based MT systems translate one sentence at a time, the text comparison in the evaluation process is also performed at the single-sentence level.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9100058674812317}]}, {"text": "In summarization evaluations, there is no sentence-to-sentence correspondence between summary pairs-essentially a multi-sentence-to-multi-sentence comparison, making it more difficult and requiring a completely different implementation for matching strategies.", "labels": [], "entities": [{"text": "summarization evaluations", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.931783139705658}]}, {"text": "In this paper, we focus on the intricacies involved in evaluating MT results and address two prominent problems associated with the BLEU-esque metrics, namely their lack of support for paraphrase matching and the absence of recall scoring.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9922361969947815}, {"text": "BLEU-esque", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9864391088485718}, {"text": "paraphrase matching", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.868989109992981}, {"text": "recall scoring", "start_pos": 224, "end_pos": 238, "type": "METRIC", "confidence": 0.9759345352649689}]}, {"text": "Our solution, ParaEval, utilizes a large collection of paraphrases acquired through an unsupervised process-identifying phrase sets that have the same translation in another language-using state-of-the-art statistical MT word alignment and phrase extraction methods.", "labels": [], "entities": [{"text": "MT word alignment", "start_pos": 218, "end_pos": 235, "type": "TASK", "confidence": 0.9006460706392924}, {"text": "phrase extraction", "start_pos": 240, "end_pos": 257, "type": "TASK", "confidence": 0.7696884274482727}]}, {"text": "This collection facilitates paraphrase matching, additionally coupled with lexical identity matching which is designed for comparing text/sentence fragments that are not consumed by paraphrase matching.", "labels": [], "entities": [{"text": "paraphrase matching", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9295406639575958}, {"text": "lexical identity matching", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7124241789182028}, {"text": "paraphrase matching", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7323125749826431}]}, {"text": "We adopt a unigram counting strategy for contents matched between sentences from peer and reference translations.", "labels": [], "entities": []}, {"text": "This unweighted scoring scheme, for both precision and recall computations, allows us to directly examine both the power and limitations of ParaEval.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9975751042366028}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.975385844707489}]}, {"text": "We show that ParaEval is a more stable and reliable comparison mechanism than BLEU, in both fluency and adequacy rankings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9974871873855591}]}, {"text": "This paper is organized in the following way: Section 2 shows an overview on BLEU and lexical identity n-gram statistics; we describe ParaEval's implementation in detail in Section 3; the evaluation of ParaEval is shown in Section 4; recall computation is discussed in Section 5; in Section 6, we discuss the differences between BLEU and ParaEval when the numbers of reference translations change; and we conclude and discuss future work in Section 7.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9132972955703735}, {"text": "BLEU", "start_pos": 329, "end_pos": 333, "type": "METRIC", "confidence": 0.755972146987915}]}], "datasetContent": [{"text": "We adopt a two-tier matching strategy for MT evaluation in ParaEval.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9668588042259216}]}, {"text": "At the top tier, a paraphrase match is performed on system-translated sentences and corresponding reference sentences.", "labels": [], "entities": []}, {"text": "Then, unigram matching is performed on the words not matched by paraphrases.", "labels": [], "entities": [{"text": "unigram matching", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.6964461356401443}]}, {"text": "Precision is measured as the ratio of the total number of words matched to the total number of words in the peer translation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9475535154342651}]}, {"text": "Running our system on the example in, the paraphrase-matching phase consumes the words marked in grey and aligns \"have been\" and \"to be\", \"completed\" and \"fully\", \"to date\" and \"up till now\", and \"sequence\" and \"sequenced\".", "labels": [], "entities": [{"text": "sequence", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9687138795852661}]}, {"text": "The subsequent unigram-matching aligns words based on lexical identity.", "labels": [], "entities": []}, {"text": "We maintain the computation of modified unigram precision, defined by the BLEU-esque Philosophy, in principle.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.8165599703788757}, {"text": "BLEU-esque", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9858161211013794}]}, {"text": "In addition to clipping individual candidate words with their corresponding maximum reference counts (only for words not matched by paraphrases), we clip candidate paraphrases by their maximum reference paraphrase counts.", "labels": [], "entities": []}, {"text": "So two completely different phrases in a reference sentence can be counted as two occurrences of one phrase.", "labels": [], "entities": []}, {"text": "For example in, candidate phrases \"blown up\" and \"bombing\" matched with three phrases from the references, namely \"bombing\" and two instances of \"explosion\".", "labels": [], "entities": []}, {"text": "Treating these two candidate phrases as one (paraphrase match), we can see its clip is 2 (from Ref 1, where \"bombing\" and \"explosion\" are counted as two occurrences of a single phrase).", "labels": [], "entities": [{"text": "Ref 1", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.7203360795974731}]}, {"text": "The only word that was matched by its lexical identity is \"was\".", "labels": [], "entities": []}, {"text": "The modified unigram precision calculated by our method is 4/5, whereas BLEU gives 2/5.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8934444189071655}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.998913049697876}]}], "tableCaptions": [{"text": " Table 1. Ranking correlations with human  assessments.", "labels": [], "entities": []}, {"text": " Table 2. ParaEval's recall ranking correlation.", "labels": [], "entities": [{"text": "recall ranking correlation", "start_pos": 21, "end_pos": 47, "type": "METRIC", "confidence": 0.9275428056716919}]}, {"text": " Table 3. ParaEval's correlation (precision)  while using only single references.", "labels": [], "entities": [{"text": "correlation", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9720087051391602}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.8389151692390442}]}, {"text": " Table 4. Differences among reference  translations (raw ParaEval precision  scores).", "labels": [], "entities": [{"text": "ParaEval precision  scores", "start_pos": 57, "end_pos": 83, "type": "METRIC", "confidence": 0.6833557486534119}]}, {"text": " Table 6. Incremental implementation of  BLEU and the correlation behavior at the  three steps: MUP, GM, and BP-BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9952362179756165}, {"text": "MUP", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.5849583745002747}, {"text": "GM", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.8894103765487671}, {"text": "BP-BLEU", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.7942623496055603}]}, {"text": " Table 5. BLEU's correlating behavior with  multi-and single-reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9974580407142639}]}]}