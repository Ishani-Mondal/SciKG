{"title": [{"text": "Improving \"Email Speech Acts\" Analysis via N-gram Selection", "labels": [], "entities": [{"text": "Improving \"Email Speech Acts\" Analysis", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7802546407495227}]}], "abstractContent": [{"text": "In email conversational analysis, it is often useful to trace the the intents behind each message exchange.", "labels": [], "entities": [{"text": "email conversational analysis", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.701206793387731}]}, {"text": "In this paper, we consider classification of email messages as to whether or not they contain certain intents or email-acts, such as \"pro-pose a meeting\" or \"commit to a task\".", "labels": [], "entities": []}, {"text": "We demonstrate that exploiting the con-textual information in the messages can noticeably improve email-act classification.", "labels": [], "entities": [{"text": "email-act classification", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6864419728517532}]}, {"text": "More specifically, we describe a combination of n-gram sequence features with careful message preprocessing that is highly effective for this task.", "labels": [], "entities": []}, {"text": "Compared to a previous study (Cohen et al., 2004), this representation reduces the classification error rates by 26.4% on average.", "labels": [], "entities": [{"text": "classification error rates", "start_pos": 83, "end_pos": 109, "type": "METRIC", "confidence": 0.7979976634184519}]}, {"text": "Finally , we introduce Ciranda: anew open source toolkit for email speech act prediction .", "labels": [], "entities": [{"text": "email speech act prediction", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6682998090982437}]}], "introductionContent": [{"text": "One important use of work-related email is negotiating and delegating shared tasks and subtasks.", "labels": [], "entities": [{"text": "negotiating and delegating shared tasks and subtasks", "start_pos": 43, "end_pos": 95, "type": "TASK", "confidence": 0.759498051234654}]}, {"text": "To provide intelligent email automated assistance, it is desirable to be able to automatically detect the intent of an email message-for example, to determine if the email contains a request, a commitment by the sender to perform some task, or an amendment to an earlier proposal.", "labels": [], "entities": []}, {"text": "Successfully adding such a semantic layer to email communication is still a challenge to current email clients.", "labels": [], "entities": []}, {"text": "Ina previous work,  used text classification methods to detect \"email speech acts\".", "labels": [], "entities": [{"text": "text classification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6872151494026184}]}, {"text": "Based on the ideas from Speech Act Theory and guided by analysis of several email corpora, they defined a set of \"email acts\" (e.g., Request, Deliver, Propose, Commit) and then classified emails as containing or not a specific act.", "labels": [], "entities": [{"text": "Speech Act Theory", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6746348341306051}, {"text": "Request, Deliver, Propose, Commit)", "start_pos": 133, "end_pos": 167, "type": "TASK", "confidence": 0.6381940431892872}]}, {"text": "showed that machine learning algorithms can learn the proposed email-act categories reasonably well.", "labels": [], "entities": []}, {"text": "It was also shown that there is an acceptable level of human agreement over the categories.", "labels": [], "entities": []}, {"text": "A method for accurate classification of email into such categories would have many potential applications.", "labels": [], "entities": [{"text": "accurate classification of email", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.7534708082675934}]}, {"text": "For instance, it could be used to help users track the status of ongoing joint activities, improving task delegation and coordination.", "labels": [], "entities": [{"text": "task delegation", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.669367253780365}]}, {"text": "Email speech acts could also be used to iteratively learn user's tasks in a desktop environment).", "labels": [], "entities": []}, {"text": "Email acts classification could also be applied to predict hierarchy positions in structured organizations or email-centered teams; predicting leadership positions can be useful to analyze behavior in teams without an explicitly assigned leader.", "labels": [], "entities": [{"text": "Email acts classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8450289169947306}]}, {"text": "By using only single words as features,  disregarded a very important linguistic aspect of the speech act inference task: the textual context.", "labels": [], "entities": []}, {"text": "For instance, the specific sequence of tokens \"Can you give me\" can be more informative to detect a Request act than the words \"can\", \"you\", \"give\" and \"me\" separately.", "labels": [], "entities": [{"text": "Request act", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.8136257231235504}]}, {"text": "Similarly, the word sequence \"I will call you\" maybe a much stronger indication of a Commit act than the four words separately.", "labels": [], "entities": []}, {"text": "More generally, because so many specific sequence of words (or n-grams) are inherently associated with the intent of an email message, one would expect that exploiting this linguistic aspect of the messages would improve email-act classification.", "labels": [], "entities": [{"text": "email-act classification", "start_pos": 221, "end_pos": 245, "type": "TASK", "confidence": 0.6839393675327301}]}, {"text": "In the current work we exploit the linguistic aspects of the problem by a careful combination of ngram feature extraction and message preprocessing.", "labels": [], "entities": [{"text": "ngram feature extraction", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.6382370889186859}]}, {"text": "After preprocessing the messages to detect entities, punctuation, pronouns, dates and times, we generate anew feature set by extracting all possible term sequences with a length of 1, 2, 3, 4 or 5 tokens.", "labels": [], "entities": []}, {"text": "Using this n-gram based representation in classification experiments, we obtained a relative average drop of 26.4% in error rate when compared to the original  paper.", "labels": [], "entities": [{"text": "error rate", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9887522459030151}]}, {"text": "Also, ranking the most \"meaningful\" n-grams based on Information Gain score) revealed an impressive agreement with the linguistic intuition behind the email speech acts.", "labels": [], "entities": []}, {"text": "We finalize this work introducing Ciranda: an open source package for Email Speech Act prediction.", "labels": [], "entities": [{"text": "Email Speech Act prediction", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.8471667915582657}]}, {"text": "Among other features, Ciranda provides an easy interface for feature extraction and feature selection, outputs the prediction confidence, and allows retraining using several learning algorithms.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7105642557144165}, {"text": "feature selection", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.6637815088033676}]}], "datasetContent": [{"text": "Here we describe how the classification experiments on the email speech acts dataset were carried out.", "labels": [], "entities": [{"text": "email speech acts dataset", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.5744378492236137}]}, {"text": "Using all n-gram features, we performed 5-fold crossvalidation tests over the 1716 email messages.", "labels": [], "entities": [{"text": "1716 email messages", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.8754008611043295}]}, {"text": "Linear SVM 1 was used as classifier.", "labels": [], "entities": [{"text": "Linear SVM 1", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7352060476938883}]}, {"text": "Results are illustrated in. shows the test error rate of four different experiments (bars) for all email acts.", "labels": [], "entities": [{"text": "test error rate", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.6779105265935262}]}, {"text": "The first bar denotes the error rate obtained by ) in a 5-fold crossvalidation experiment, also using linear SVM.", "labels": [], "entities": [{"text": "error rate", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9537787437438965}]}, {"text": "Their dataset had 1354 email messages, and only 1-gram features were extracted.", "labels": [], "entities": []}, {"text": "The second bar illustrates the error rate obtained using only 1-gram features with additional data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9695957005023956}]}, {"text": "In this case, we used 1716 email messages.", "labels": [], "entities": []}, {"text": "The third bar represents the the same as the second bar (1-1 We used the LIBSVM implementation) with default parameters.", "labels": [], "entities": []}, {"text": "gram features with 1716 messages), with the difference that the emails went through the preprocessing procedure previously described.", "labels": [], "entities": []}, {"text": "The fourth bar shows the error rate when all 1-gram, 2-gram and 3-gram features are used and the 1716 messages go through the preprocessing procedure.", "labels": [], "entities": [{"text": "error rate", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9733642637729645}]}, {"text": "The last bar illustrates the error rate when all n-gram features (i.e., 1g+2g+3g+4g+5g) are used in addition to preprocessing in all 1716 messages.", "labels": [], "entities": [{"text": "error rate", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9710816740989685}]}, {"text": "In all acts, a consistent improvement in 1-gram performance is observed when more data is added, i.e., a drop in error rate from the first to the second bar.", "labels": [], "entities": [{"text": "error rate", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9800747632980347}]}, {"text": "Therefore, we can conclude that  could have obtained better results if they had used more labeled data.", "labels": [], "entities": []}, {"text": "A comparison between the second and third bars reveals the extent to which preprocessing seems to help classification based on 1-grams only.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.9590157866477966}, {"text": "classification", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.9624090194702148}]}, {"text": "As we can see, no significant performance difference can be observed: for most acts the relative difference is: Error Rate 5-fold Crossvalidation Experiment very small, and in one or maybe two acts some small improvement can be noticed.", "labels": [], "entities": [{"text": "Error Rate 5-fold Crossvalidation", "start_pos": 112, "end_pos": 145, "type": "METRIC", "confidence": 0.8866432011127472}]}, {"text": "A much larger performance improvement can be seen between the fourth and third bars.", "labels": [], "entities": []}, {"text": "This reflects the power of the contextual features: using all 1-grams, 2-grams and 3-grams is considerably more powerful than using only 1-gram features.", "labels": [], "entities": []}, {"text": "This significant difference can be observed in all acts.", "labels": [], "entities": []}, {"text": "Compared to the original values from ), we observed a relative error rate drop of 24.7% in the Request act, 33.3% in the Commit act, 23.7% for the Deliver act, 38.3% for the Propose act, 9.2% for Meeting and 29.1% in the dData act.", "labels": [], "entities": [{"text": "error rate", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9818965792655945}]}, {"text": "In average, a relative improvement of 26.4% in error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.964080274105072}]}, {"text": "We also considered adding the 4-gram and 5-gram features to the best system.", "labels": [], "entities": []}, {"text": "As pictured in the last bar of, this addition did not seem to improve the performance and, in some cases, even a small increase in error rate was observed.", "labels": [], "entities": [{"text": "error rate", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.988844245672226}]}, {"text": "We believe this was caused by the insufficient amount of labeled data in these tests; and the 4-gram and 5-gram features are likely to improve the performance of this system if more labeled data becomes available.", "labels": [], "entities": []}, {"text": "Precision versus recall curves of the Request act classification task are illustrated in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9842775464057922}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9658240675926208}, {"text": "Request act classification task", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.9214223325252533}]}, {"text": "The curve on the top shows the Request act performance when the preprocessing step cues and n-grams proposed in Section 4 are applied.", "labels": [], "entities": [{"text": "Request act", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.48082391917705536}]}, {"text": "For the bottom curve, only 1g features were used.", "labels": [], "entities": []}, {"text": "These two curves correspond to the second bar (bottom curve) and forth bar (top curve) in. clearly shows that both recall and precision are improved by using the contextual features.", "labels": [], "entities": [{"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9984464049339294}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9992713332176208}]}, {"text": "To summarize, these results confirm the intuition that contextual information (n-grams) can be very effective in the task of email speech act classification.", "labels": [], "entities": [{"text": "email speech act classification", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.7702331244945526}]}], "tableCaptions": []}