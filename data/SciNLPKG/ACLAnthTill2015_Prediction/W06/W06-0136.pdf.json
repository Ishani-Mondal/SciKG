{"title": [{"text": "N-gram Based Two-Step Algorithm for Word Segmentation", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7585249543190002}]}], "abstractContent": [{"text": "This paper describes an n-gram based reinforcement approach to the closed track of word segmentation in the third Chinese word segmentation bakeoff.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7065193951129913}, {"text": "Chinese word segmentation bakeoff", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.6908968314528465}]}, {"text": "Character n-gram features of unigram, bigram, and trigram are extracted from the training corpus and its frequencies are counted.", "labels": [], "entities": []}, {"text": "We investigated a step-by-step methodology by using the n-gram statistics.", "labels": [], "entities": []}, {"text": "In the first step, relatively definite segmentations are fixed by the tight threshold value.", "labels": [], "entities": []}, {"text": "The remaining tags are decided by considering the left or right space tags that are already fixed in the first step.", "labels": [], "entities": []}, {"text": "Definite and loose segmenta-tion are performed simply based on the bigram and trigram statistics.", "labels": [], "entities": []}, {"text": "In order to overcome the data sparseness problem of bigram data, unigram is used for the smoothing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word segmentation has been one of the very important problems in the Chinese language processing.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.703901544213295}, {"text": "Chinese language processing", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.599635640780131}]}, {"text": "It is a necessary in the information retrieval system for the Korean language).", "labels": [], "entities": []}, {"text": "Though Korean words are separated by white spaces, many web users often do not set a space in a sentence when they write a query at the search engine.", "labels": [], "entities": []}, {"text": "Another necessity of automatic word segmentation is the index term extraction from a sentence that includes word spacing errors.", "labels": [], "entities": [{"text": "automatic word segmentation", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6668122410774231}, {"text": "index term extraction from a sentence that includes word spacing", "start_pos": 56, "end_pos": 120, "type": "TASK", "confidence": 0.6806374073028565}]}, {"text": "The motivation of this research is to investigate a practical word segmentation system for the Korean language.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7509920299053192}]}, {"text": "While we develop the system, we found that ngram-based algorithm was exactly applicable to the Chinese word segmentation and we have participated the bakeoff ().", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.6732718348503113}, {"text": "bakeoff", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.7764289975166321}]}, {"text": "The bakeoff result is not satisfiable, but it is acceptable because our method is language independent that does not consider the characteristics of the Chinese language.", "labels": [], "entities": []}, {"text": "We do not use any language dependent features except the average length of Chinese words.", "labels": [], "entities": []}, {"text": "Another advantage of our approach is that it can express the ambiguous word boundaries that are error-prone.", "labels": [], "entities": []}, {"text": "So, there area good possibility of improving the performance if language dependent functionalities are added such as proper name, numeric expression recognizer, and the postprocessing of single character words.", "labels": [], "entities": [{"text": "numeric expression recognizer", "start_pos": 130, "end_pos": 159, "type": "TASK", "confidence": 0.6028088927268982}]}], "datasetContent": [{"text": "We evaluated our system in the closed task on all four corpora.", "labels": [], "entities": []}, {"text": "shows the final results in bakeoff 2006.", "labels": [], "entities": [{"text": "bakeoff 2006", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.8643293976783752}]}, {"text": "We expect that R oov will be improved if any unknown word processing is performed.", "labels": [], "entities": [{"text": "R", "start_pos": 15, "end_pos": 16, "type": "METRIC", "confidence": 0.9907913208007812}]}, {"text": "R iv can also be improved if lexicon is applied to correct the segmentation errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The number of n-gram features", "labels": [], "entities": []}, {"text": " Table 2. Strong and weak threshold values 6", "labels": [], "entities": []}, {"text": " Table 3. Final results in bakeoff 2006", "labels": [], "entities": []}, {"text": " Table 4. The number of space positions", "labels": [], "entities": []}, {"text": " Table 7. Modified space tags by error correction", "labels": [], "entities": [{"text": "Modified space tags", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8300777872403463}]}, {"text": " Table 8. Results before error correction", "labels": [], "entities": [{"text": "error correction", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7917076051235199}]}]}