{"title": [{"text": "Graph-based Word Clustering using a Web Search Engine", "labels": [], "entities": []}], "abstractContent": [{"text": "Word clustering is important for automatic thesaurus construction, text classification, and word sense disambiguation.", "labels": [], "entities": [{"text": "Word clustering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7207569926977158}, {"text": "automatic thesaurus construction", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.658854196468989}, {"text": "text classification", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7869246900081635}, {"text": "word sense disambiguation", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.711695154507955}]}, {"text": "Recently, several studies have reported using the web as a corpus.", "labels": [], "entities": []}, {"text": "This paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web counts.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7862825095653534}]}, {"text": "Each pair of words is queried to a search engine, which produces a co-occurrence matrix.", "labels": [], "entities": []}, {"text": "By calculating the similarity of words, a word co-occurrence graph is obtained.", "labels": [], "entities": []}, {"text": "A new kind of graph clustering algorithm called New-man clustering is applied for efficiently identifying word clusters.", "labels": [], "entities": [{"text": "New-man clustering", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.6552392095327377}]}, {"text": "Evaluations are made on two sets of word groups derived from a web directory and WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9722708463668823}]}], "introductionContent": [{"text": "The web is a good source of linguistic information for several natural language techniques such as question answering, language modeling, and multilingual lexicon acquisition.", "labels": [], "entities": [{"text": "question answering", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8625085055828094}, {"text": "language modeling", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7102458477020264}, {"text": "multilingual lexicon acquisition", "start_pos": 142, "end_pos": 174, "type": "TASK", "confidence": 0.6536104381084442}]}, {"text": "Numerous studies have examined the use of the web as a corpus.", "labels": [], "entities": []}, {"text": "Web-based models perform especially well against the sparse data problem: Statistical techniques perform poorly when the words are rarely used.", "labels": [], "entities": []}, {"text": "For example, F. use the web to obtain frequencies for unseen bigrams in a given corpus.", "labels": [], "entities": []}, {"text": "They count for adjective-noun, noun-noun, and verb-object bigrams by querying a search engine, and demonstrate that web frequencies (web counts) correlate with frequencies from a carefully edited corpus such as the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 215, "end_pos": 244, "type": "DATASET", "confidence": 0.9680077234903971}]}, {"text": "Aside from counting bigrams, various tasks are attainable using webbased models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and soon ().", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.9312520623207092}, {"text": "adjective ordering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7272316217422485}, {"text": "compound noun bracketing", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.582523117462794}, {"text": "countability detection", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.6785055100917816}, {"text": "soon", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9736020565032959}]}, {"text": "For some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a standard large corpus; the web yields better counts than the BNC.", "labels": [], "entities": []}, {"text": "The web is an excellent source of information on new words.", "labels": [], "entities": []}, {"text": "Therefore, automatic thesaurus construction) offers great potential for various useful NLP applications.", "labels": [], "entities": [{"text": "automatic thesaurus construction", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.6502893269062042}]}, {"text": "Several studies have addressed the extraction of hypernyms and hyponyms from the web ().", "labels": [], "entities": []}, {"text": "P. presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI).", "labels": [], "entities": [{"text": "recognize synonyms", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8020139634609222}, {"text": "pointwise mutual information (PMI)", "start_pos": 84, "end_pos": 118, "type": "METRIC", "confidence": 0.723088264465332}]}, {"text": "For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets.", "labels": [], "entities": [{"text": "automatic thesaurus construction", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.6561805506547292}, {"text": "word clustering", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7563720941543579}]}, {"text": "It also contributes to word sense disambiguation ( and text classification () because the dimensionality is reduced efficiently.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7480177084604899}, {"text": "text classification", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.81373730301857}]}, {"text": "This paper presents an unsupervised algorithm for word clustering based on a word similarity measure by web counts.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.777739554643631}]}, {"text": "Given a set of words, the algorithm clusters the words into groups so that the similar words are in the same cluster.", "labels": [], "entities": []}, {"text": "Each pair of words is queried to a search engine, which results in a co-occurrence matrix.", "labels": [], "entities": []}, {"text": "By calculating the similarity of words, a word co-occurrence graph is created.", "labels": [], "entities": []}, {"text": "Then, anew kind of graph clustering algorithm, called Newman clustering, is applied.", "labels": [], "entities": [{"text": "graph clustering", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.752428412437439}, {"text": "Newman clustering", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6731309741735458}]}, {"text": "Newman clustering emphasizes betweenness of an edge and identifies densely connected subgraphs.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first attempt to obtain word groups using web counts.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: \u2022 A new algorithm for word clustering is described.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8026217818260193}]}, {"text": "It has few parameters and thus is easy to implement as a baseline method.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate the algorithm on two sets of word groups derived from a web directory and WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.9682407975196838}]}, {"text": "The chi-square measure and Newman clustering are both used in our algorithm, they are revealed to outperform PMI and hierarchical clustering.", "labels": [], "entities": []}, {"text": "We target Japanese words in this paper.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: We overview the related studies in the next section.", "labels": [], "entities": []}, {"text": "Our proposed algorithm is described in Section 3.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 explain evaluations and advance discussion.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two sets of word groups are used for the evaluation: one is derived from documents on a web directory; another is from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9687879681587219}]}, {"text": "We first evaluate the co-1.", "labels": [], "entities": []}, {"text": "Input A set of words is given.", "labels": [], "entities": []}, {"text": "The number of words is denoted as n.", "labels": [], "entities": []}, {"text": "2. Obtain frequencies Put a query for each pair of words to a search engine, and obtain a cooccurrence matrix.", "labels": [], "entities": []}, {"text": "Then calculate the chi-square matrix (alternatively a PMI matrix, or a Jaccard matrix.)", "labels": [], "entities": []}, {"text": "3. Make a graph Set anode for each word, and an edge to a pair of nodes whose \u03c7 2 value is above a threshold.", "labels": [], "entities": []}, {"text": "A word co-occurrence graph is created using PMI, Jaccard, and chi-square measures.", "labels": [], "entities": [{"text": "PMI", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.7075004577636719}]}, {"text": "The threshold is determined so that the network density d thre is 0.3.", "labels": [], "entities": []}, {"text": "Then, we apply clustering to obtain nine clusters; n c = 9.", "labels": [], "entities": []}, {"text": "Finally, we compare the resultant clusters with the correct categories.", "labels": [], "entities": []}, {"text": "Clustering results for DMOZ-J sets are shown in.", "labels": [], "entities": [{"text": "DMOZ-J sets", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.8650578856468201}]}, {"text": "Newman clustering produces higher precision and recall.", "labels": [], "entities": [{"text": "Newman clustering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.5513956695795059}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9995726943016052}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9994643330574036}]}, {"text": "Especially, the combination of chi-square and Newman is the best in our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Web counts for each word.", "labels": [], "entities": [{"text": "Web counts", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9222925901412964}]}, {"text": " Table 2: Co-occurrence matrix by web counts.", "labels": [], "entities": []}, {"text": " Table 3: A matrix of pointwise mutual information.", "labels": [], "entities": []}, {"text": " Table 4: A matrix of chi-square values.", "labels": [], "entities": []}, {"text": " Table 7: Precision for DMOZ-J set.  PMI Jaccard  \u03c7 2  Mean 0.415 0.402 0.537  Min 0.396 0.376 0.493  Max 0.447 0.424 0.569  SD  0.020 0.020 0.032", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9759530425071716}, {"text": "PMI Jaccard  \u03c7 2  Mean 0.415 0.402 0.537  Min 0.396 0.376 0.493  Max 0.447 0.424 0.569  SD  0.020 0.020 0.032", "start_pos": 37, "end_pos": 146, "type": "METRIC", "confidence": 0.880047969520092}]}, {"text": " Table 8: Precision of WordNet set.  PMI Jaccard  \u03c7 2  Mean 0.549 0.484 0.584  Min 0.473 0.415 0.498  Max 0.593 0.503 0.656  SD  0.037 0.027 0.048", "labels": [], "entities": [{"text": "WordNet set", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.8935054838657379}, {"text": "PMI Jaccard  \u03c7 2  Mean 0.549 0.484 0.584  Min 0.473 0.415 0.498  Max 0.593 0.503 0.656  SD  0.037 0.027 0.048", "start_pos": 37, "end_pos": 146, "type": "METRIC", "confidence": 0.8028309226036072}]}, {"text": " Table 9: Precision, recall and the F-measure for  each clustering.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985141158103943}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996291399002075}, {"text": "F-measure", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9984816908836365}]}]}