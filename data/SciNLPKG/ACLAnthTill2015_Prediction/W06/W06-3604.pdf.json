{"title": [], "abstractContent": [{"text": "We present a classification-based word prediction model based on IGTREE, a decision-tree induction algorithm with favorable scaling abilities and a functional equivalence to n-gram models with back-off smoothing.", "labels": [], "entities": [{"text": "classification-based word prediction", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.6887242496013641}, {"text": "IGTREE", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.7593405246734619}]}, {"text": "Through a first series of experiments, in which we train on Reuters newswire text and test either on the same type of data or on general or fictional text, we demonstrate that the system exhibits log-linear increases in prediction accuracy with increasing numbers of training examples.", "labels": [], "entities": [{"text": "Reuters newswire text", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.9553781946500143}, {"text": "prediction", "start_pos": 220, "end_pos": 230, "type": "TASK", "confidence": 0.9348598718643188}, {"text": "accuracy", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.924043595790863}]}, {"text": "Trained on 30 million words of newswire text, prediction accuracies range between 12.6% on fictional text and 42.2% on newswire text.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.6855137348175049}]}, {"text": "Ina second series of experiments we compare all-words prediction with confusable prediction, i.e., the same task, but specialized to predicting among limited sets of words.", "labels": [], "entities": [{"text": "predicting among limited sets of words", "start_pos": 133, "end_pos": 171, "type": "TASK", "confidence": 0.8433755536874136}]}, {"text": "Con-fusable prediction yields high accuracies on nine example confusable sets in all genres of text.", "labels": [], "entities": [{"text": "Con-fusable prediction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6002107411623001}, {"text": "accuracies", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9902077317237854}]}, {"text": "The confusable approach outperforms the all-words-prediction approach , but with more data the difference decreases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word prediction is an intriguing language engineering semi-product.", "labels": [], "entities": [{"text": "Word prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7993516325950623}]}, {"text": "Arguably it is the \"archetypical prediction problem in natural language processing\").", "labels": [], "entities": []}, {"text": "It is usually not an engineering end in itself to predict the next word in a sequence, or fill in a blanked-out word in a sequence.", "labels": [], "entities": []}, {"text": "Yet, it could bean asset in higher-level proofing or authoring tools, e.g. to be able to automatically discern among confusables and thereby to detect confusable errors).", "labels": [], "entities": []}, {"text": "It could alleviate problems with lowfrequency and unknown words in natural language processing and information retrieval, by replacing them with likely and higher-frequency alternatives that carry similar information.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7399307787418365}]}, {"text": "And also, since the task of word prediction is a direct interpretation of language modeling, a word prediction system could provide useful information for to be used in speech recognition systems.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.800058126449585}, {"text": "word prediction", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7327824085950851}, {"text": "speech recognition", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7388324439525604}]}, {"text": "A unique aspect of the word prediction task, as compared to most other tasks in natural language processing, is that real-world examples abound in large amounts.", "labels": [], "entities": [{"text": "word prediction task", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8597760995229086}]}, {"text": "Any digitized text can be used as training material fora word prediction system capable of learning from examples, and nowadays gigascale and terascale document collections are available for research purposes.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.8241285383701324}]}, {"text": "A specific type of word prediction is confusable prediction, i.e., learn to predict among limited sets of confusable words such as to/two/too and there/their/they're ().", "labels": [], "entities": [{"text": "word prediction", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7111731171607971}, {"text": "confusable prediction", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7442125976085663}]}, {"text": "Having trained a confusable predictor on occurrences of words within a confusable set, it can be applied to any new occurrence of a word from the set; if its prediction based on the context deviates from the word actually present, then this word might be a confusable error, and the classifier's prediction might be its correction.", "labels": [], "entities": []}, {"text": "Confusable prediction and correction is a strong asset in proofing tools.", "labels": [], "entities": [{"text": "Confusable prediction and correction", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5490952432155609}, {"text": "proofing", "start_pos": 58, "end_pos": 66, "type": "TASK", "confidence": 0.9728144407272339}]}, {"text": "In this paper we generalize the word prediction task to predicting any word in context.", "labels": [], "entities": [{"text": "word prediction task", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8101333777109782}, {"text": "predicting any word in context", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.8368723154067993}]}, {"text": "This is basically the task of a generic language model.", "labels": [], "entities": []}, {"text": "An explicit choice for the particular study on \"all-words\" prediction is to encode context only by words, and not by any higher-level linguistic non-terminals which have been investigated in related work on word prediction ().", "labels": [], "entities": [{"text": "all-words\" prediction", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7365601658821106}, {"text": "word prediction", "start_pos": 207, "end_pos": 222, "type": "TASK", "confidence": 0.7612683773040771}]}, {"text": "This choice leaves open the question how the same tasks can be learned from examples when non-terminal symbols are taken into account as well.", "labels": [], "entities": []}, {"text": "The choice for our algorithm, a decision-tree approximation of k-nearest-neigbor (k-NN) based or memory-based learning, is motivated by the fact that, as we describe later in this paper, this particular algorithm can scale up to predicting tens of thousands of words, while simultaneously being able to scale up to tens of millions of examples as training material, predicting words at useful rates of hundreds to thousands of words per second.", "labels": [], "entities": []}, {"text": "Another motivation for our choice is that our decision-tree approximation of k-nearest neighbor classification is functionally equivalent to back-off smoothing; not only does it share its performance capacities with n-gram models with back-off smoothing, it also shares its scaling abilities with these models, while being able to handle large values of n.", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe what data we selected for our experiments, and we provide an overview of the experimental methodology used throughout the experiments, including a description of the IGTREE algorithm central to our study.", "labels": [], "entities": [{"text": "IGTREE", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.6715201735496521}]}, {"text": "In Section 3 the results of the word prediction experiments are presented, and the subsequent Section 4 contains the experimental results of the experiments on confusables.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.8100349307060242}]}, {"text": "We briefly relate our work to earlier work that inspired the current study in Section 5.", "labels": [], "entities": []}, {"text": "The results are discussed, and conclusions are drawn in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we identify the textual corpora used.", "labels": [], "entities": []}, {"text": "We then describe the general experimental setup of learning curve experiments, and the IGTREE decisiontree induction algorithm used throughout all experiments.", "labels": [], "entities": [{"text": "IGTREE decisiontree induction", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.57403364777565}]}, {"text": "All experiments described in this article take the form of learning curve experiments (), in which a sequence of training sets is generated with increasing size, where each size training set is used to train a model for word prediction, which is subsequently tested on a held-out test set -which is fixed throughout the whole learning curve experiment.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 220, "end_pos": 235, "type": "TASK", "confidence": 0.8102804720401764}]}, {"text": "Training set sizes are exponentially grown, as earlier studies have shown that at a linear scale, performance effects tend to decrease in size, but that when measured with exponentially growing training sets, near-constant (i.e. log-linear) improvements are observed ().", "labels": [], "entities": []}, {"text": "We create incrementally-sized training sets for the word prediction task on the basis of the TRAIN-REUTERS set.", "labels": [], "entities": [{"text": "word prediction task", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8893318176269531}, {"text": "TRAIN-REUTERS", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.9622454047203064}]}, {"text": "Each training subset is created backward from the point at which the final 100,000-word REUTERS set starts.", "labels": [], "entities": []}, {"text": "The increments are exponential with base number 10, and for every power of 10 we cutoff training sets at n times that power, where n = 1, 2, 3, . .", "labels": [], "entities": []}, {"text": "The actual examples to learn from are created by windowing overall sequences of tokens.", "labels": [], "entities": []}, {"text": "We encode examples by taking a left context window spanning seven tokens, and aright context also spanning seven tokens.", "labels": [], "entities": []}, {"text": "Thus, the task is represented by a growing number of examples, each characterized by 14 positional features carrying tokens as values, and one class label representing the word to be predicted.", "labels": [], "entities": []}, {"text": "The choice for 14 is intended to cover at least the superficially most important positional features.", "labels": [], "entities": []}, {"text": "We assume that a word more distant than seven positions left or right of a focus word will almost never be more informative for the task than any of the words within this scope.", "labels": [], "entities": []}, {"text": "The word prediction accuracy learning curves computed on the three test sets, and trained on increasing portions of TRAIN-REUTERS, are displayed in.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7619397640228271}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.768427312374115}, {"text": "TRAIN-REUTERS", "start_pos": 116, "end_pos": 129, "type": "METRIC", "confidence": 0.8857856392860413}]}, {"text": "The best accuracy observed is 42.2% with 30 million training examples, on REUTERS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995546936988831}, {"text": "REUTERS", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.7843736410140991}]}, {"text": "Apparently, training and testing on the same type of data yields markedly higher prediction accuracies than testing on a different-type corpus.", "labels": [], "entities": []}, {"text": "Accuracies on BROWN are slightly higher than on ALICE, but the difference is small; at 30 million training examples, the accuracy on ALICE is 12.6%, and on BROWN 15.8%.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9940223097801208}, {"text": "BROWN", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.6964811682701111}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9995170831680298}, {"text": "BROWN", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.8820046782493591}]}, {"text": "A second observation is that all three learning curves are progressing upward with more training examples, and roughly at a constant log-linear rate.", "labels": [], "entities": []}, {"text": "When estimating the rates after about 50,000 examples (before which the curves appear to be more volatile), with every tenfold increase of the number of training examples the prediction accuracy on REUTERS increases by a constant rate of about 8%, while the increases on ALICE and BROWN are both about 2% at every tenfold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9850890040397644}, {"text": "REUTERS", "start_pos": 198, "end_pos": 205, "type": "METRIC", "confidence": 0.8455116748809814}, {"text": "ALICE", "start_pos": 271, "end_pos": 276, "type": "METRIC", "confidence": 0.8754358291625977}, {"text": "BROWN", "start_pos": 281, "end_pos": 286, "type": "METRIC", "confidence": 0.9128351211547852}]}], "tableCaptions": [{"text": " Table 1: Training and test set sources, genres, sizes  in terms of numbers of tokens, and unigram and bi- gram coverage (%) of the training set on the test sets.", "labels": [], "entities": [{"text": "unigram and bi- gram coverage", "start_pos": 91, "end_pos": 120, "type": "METRIC", "confidence": 0.7086718330780665}]}, {"text": " Table 2: Disambiguation scores on nine confusable  set, attained by the all-words prediction classifier  trained on 30 million examples of TRAIN-REUTERS,  and by confusable experts on the same training set.  The second column displays the number of exam- ples of each confusable set in the 30-million word  training set; the list is ordered on this column.", "labels": [], "entities": []}]}