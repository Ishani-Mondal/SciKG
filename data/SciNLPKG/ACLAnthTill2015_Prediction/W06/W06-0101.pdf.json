{"title": [{"text": "Improving Context Vector Models by Feature Clustering for Auto- matic Thesaurus Construction", "labels": [], "entities": [{"text": "Improving Context Vector Models", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9019261747598648}]}], "abstractContent": [{"text": "Thesauruses are useful resources for NLP; however, manual construction of thesaurus is time consuming and suffers low coverage.", "labels": [], "entities": [{"text": "NLP", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9366218447685242}, {"text": "coverage", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9579392671585083}]}, {"text": "Automatic thesaurus construction is developed to solve the problem.", "labels": [], "entities": [{"text": "thesaurus construction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8101316392421722}]}, {"text": "Conventional way to automatically construct thesaurus is by finding similar words based on context vector models and then organizing similar words into thesaurus structure.", "labels": [], "entities": []}, {"text": "But the context vector methods suffer from the problems of vast feature dimensions and data sparse-ness.", "labels": [], "entities": []}, {"text": "Latent Semantic Index (LSI) was commonly used to overcome the problems.", "labels": [], "entities": [{"text": "Latent Semantic Index (LSI)", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.9071906705697378}]}, {"text": "In this paper, we propose a feature clustering method to overcome the same problems.", "labels": [], "entities": [{"text": "feature clustering", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7248054444789886}]}, {"text": "The experimental results show that it performs better than the LSI models and do enhance contextual information for infrequent words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Thesaurus is one of the most useful linguistic resources.", "labels": [], "entities": [{"text": "Thesaurus", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.873170018196106}]}, {"text": "It provides information more than just synonyms., it also builds up relations between synonym sets, such as hyponym, hypernym.", "labels": [], "entities": []}, {"text": "There are two Chinese thesauruses and Hownet . Cilin provides synonym sets with simple hierarchical structure.", "labels": [], "entities": [{"text": "Hownet", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.961631178855896}]}, {"text": "Hownet uses some primitive senses to describe word meanings.", "labels": [], "entities": [{"text": "Hownet", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9104049801826477}]}, {"text": "The common primitive senses provide additional relations between words implicitly.", "labels": [], "entities": []}, {"text": "However, many words occurred in contemporary news corpora are not covered by Chinese thesauruses.", "labels": [], "entities": []}, {"text": "Therefore, we intend to create a thesaurus based on contemporary news corpora.", "labels": [], "entities": []}, {"text": "The common steps to automatically construct a thesaurus include a) contextual information extraction, b) finding synonym words and c) organizing synonym words into a thesaurus.", "labels": [], "entities": [{"text": "contextual information extraction", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.6263733406861623}]}, {"text": "The approach is based upon the fact that word meaning lays on its contextual behavior.", "labels": [], "entities": []}, {"text": "If words act similarly in context, they may share the same meaning.", "labels": [], "entities": []}, {"text": "However, the method can only handle frequent words rather than infrequent ones.", "labels": [], "entities": []}, {"text": "In fact most of vocabularies occur infrequently, one has to discover extend information to overcome the data sparseness problem.", "labels": [], "entities": []}, {"text": "We will introduce the conventional approaches for automatic thesaurus construction in section 2.", "labels": [], "entities": [{"text": "automatic thesaurus construction", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.6402418812115988}]}, {"text": "Follow a discussion about the problems and solutions of context vector models in section 3.", "labels": [], "entities": []}, {"text": "In section 4, we use two performance evaluation metrics, i.e. discrimination and nonlinear interpolated precision, to evaluate our proposed method.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.7366505265235901}]}], "datasetContent": [{"text": "To evaluate the performance of the feature clustering method, we had prepared two sets of testing data with high and low frequency words respectively.", "labels": [], "entities": [{"text": "feature clustering", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7632054686546326}]}, {"text": "We want to seethe effects of feature reduction and feature extension for both frequent and infrequent words.", "labels": [], "entities": [{"text": "feature reduction", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.6967408657073975}, {"text": "feature extension", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7089858651161194}]}, {"text": "The context vectors were derived from a 10 year news corpus from The Central News Agency.", "labels": [], "entities": [{"text": "The Central News Agency", "start_pos": 65, "end_pos": 88, "type": "DATASET", "confidence": 0.8043433278799057}]}, {"text": "It contains nearly 33 million sentences, 234 million word tokens, and we extracted 186 million syntactic relations from this corpus.", "labels": [], "entities": []}, {"text": "Due to the low reliability of infrequent data, only the relation triples (w, r, c), which occurs more than 3 times and POS of wand c must be noun or verb, are used.", "labels": [], "entities": [{"text": "POS", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9743005633354187}]}, {"text": "It results that nearly 30,000 high frequent nouns and verbs are used as the contextual features.", "labels": [], "entities": []}, {"text": "And with feature clustering 2 , the contextual dimensions were reduced from 30,988 literal words to 12,032 semantic classes.", "labels": [], "entities": []}, {"text": "In selecting testing data, we consider the words that occur more than 200 times as high frequent words and the frequencies range from 40 to 200 as low frequent words.", "labels": [], "entities": []}], "tableCaptions": []}