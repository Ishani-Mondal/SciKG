{"title": [{"text": "Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines", "labels": [], "entities": []}], "abstractContent": [{"text": "The end-to-end performance of natural language processing systems for compound tasks, such as question answering and textual entailment, is often hampered by use of a greedy 1-best pipeline architecture , which causes errors to propagate and compound at each stage.", "labels": [], "entities": [{"text": "question answering and textual entailment", "start_pos": 94, "end_pos": 135, "type": "TASK", "confidence": 0.6846195876598358}]}, {"text": "We present a novel architecture, which models these pipelines as Bayesian networks, with each low level task corresponding to a variable in the network, and then we perform approximate inference to find the best labeling.", "labels": [], "entities": []}, {"text": "Our approach is extremely simple to apply but gains the benefits of sampling the entire distribution over labels at each stage in the pipeline.", "labels": [], "entities": []}, {"text": "We apply our method to two tasks-semantic role labeling and recognizing textual entailment-and achieve useful performance gains from the superior pipeline architecture.", "labels": [], "entities": [{"text": "recognizing textual entailment-and", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.7181368370850881}]}], "introductionContent": [{"text": "Almost any system for natural language understanding must recover hidden linguistic structure at many different levels: parts of speech, syntactic dependencies, named entities, etc.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6966588099797567}]}, {"text": "For example, modern semantic role labeling (SRL) systems use the parse of the sentence, and question answering requires question type classification, parsing, named entity tagging, semantic role labeling, and often other tasks, many of which are dependent on one another and must be pipelined together.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8126223186651865}, {"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8188664317131042}, {"text": "question type classification", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.6337155401706696}, {"text": "parsing", "start_pos": 150, "end_pos": 157, "type": "TASK", "confidence": 0.9487952589988708}, {"text": "named entity tagging", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.6674619118372599}, {"text": "semantic role labeling", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.6432160139083862}]}, {"text": "Pipelined systems are ubiquitous in NLP: in addition to the above examples, commonly parsers and named entity recognizers use part of speech tags and chunking information, and also word segmentation for languages such as Chinese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 181, "end_pos": 198, "type": "TASK", "confidence": 0.7249013334512711}]}, {"text": "Almost no NLP task is truly standalone.", "labels": [], "entities": []}, {"text": "Most current systems for higher-level, aggregate NLP tasks employ a simple 1-best feed forward architecture: they greedily take the best output at each stage in the pipeline and pass it onto the next stage.", "labels": [], "entities": []}, {"text": "This is the simplest architecture to build (particularly if reusing existing component systems), but errors are frequently made during this pipeline of annotations, and when a system is given incorrectly labeled input it is much harder for that system to do its task correctly.", "labels": [], "entities": []}, {"text": "For example, when doing semantic role labeling, if no syntactic constituent of the parse actually corresponds to a given semantic role, then that semantic role will almost certainly be misidentified.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6223742266496023}]}, {"text": "It is therefore disappointing, but not surprising, that F-measures on SRL drop more than 10% when switching from gold parses to automatic parses (for instance, from 91.2 to 80.0 for the joint model of).", "labels": [], "entities": [{"text": "F-measures", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9402456879615784}, {"text": "SRL", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.6484368443489075}]}, {"text": "A common improvement on this architecture is to pass k-best lists between processing stages, for example ().", "labels": [], "entities": []}, {"text": "Passing on a k-best list gives useful improvements (e.g., in), but efficiently enumerating k-best lists often requires very substantial cognitive and engineering effort, e.g., in.", "labels": [], "entities": []}, {"text": "At the other extreme, one can maintain the entire space of representations (and their probabilities) at each level, and use this full distribution to calculate the full distribution at the next level.", "labels": [], "entities": []}, {"text": "If restricting oneself to weighted finite state transducers (WFSTs), a framework applicable to a number of NLP applications (as outlined in), a pipeline can be compressed down into a single WFST, giving outputs equivalent to propagating the entire distribution through the pipeline.", "labels": [], "entities": []}, {"text": "In the worst case there is an exponential space cost, but in many relevant cases composition is in practice quite practical.", "labels": [], "entities": []}, {"text": "Outside of WFSTs, maintaining entire probability distributions is usually infeasible in NLP, because for most intermediate tasks, such as parsing and named entity recognition, there is an exponential number of possible labelings.", "labels": [], "entities": [{"text": "parsing", "start_pos": 138, "end_pos": 145, "type": "TASK", "confidence": 0.9765031337738037}, {"text": "named entity recognition", "start_pos": 150, "end_pos": 174, "type": "TASK", "confidence": 0.5393902361392975}]}, {"text": "Nevertheless, for some models, such as most parsing models, these exponential labelings can be compactly represented in a packed form, e.g.,, and subsequent stages can be reengineered to work over these packed representations, e.g.,).", "labels": [], "entities": []}, {"text": "However, doing this normally also involves a very high cognitive and engineering effort, and in practice this solution is infrequently adopted.", "labels": [], "entities": []}, {"text": "Moreover, in some cases, a subsequent module is incompatible with the packed representation of a previous module and an exponential amount of work is nevertheless required within this architecture.", "labels": [], "entities": []}, {"text": "Here we present an attractive middle ground in dealing with linguistic pipelines.", "labels": [], "entities": []}, {"text": "Rather than only using the 1 or k most likely labelings at each stage, we would indeed like to take into account all possible labelings and their probabilities, but we would like to be able to do so without a lot of thinking or engineering.", "labels": [], "entities": []}, {"text": "We propose that this can be achieved by use of approximate inference.", "labels": [], "entities": []}, {"text": "The form of approximate inference we use is very simple: at each stage in the pipeline, we draw a sample from the distribution of labels, conditioned on the samples drawn at previous stages.", "labels": [], "entities": []}, {"text": "We repeat this many times, and then use the samples from the last stage, which corresponds to the ultimate, higher-level task, to form a majority vote classifier.", "labels": [], "entities": []}, {"text": "As the number of samples increases, this method will approximate the complete distribution.", "labels": [], "entities": []}, {"text": "Use of the method is normally a simple modification to an existing piece of code, and the method is general.", "labels": [], "entities": []}, {"text": "It can be applied not only to all pipelines, but to multi-stage algorithms which are not pipelines as well.", "labels": [], "entities": []}, {"text": "We apply our method to two problems: semantic role labeling and recognizing textual entailment.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.764366885026296}, {"text": "recognizing textual entailment", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7838193575541178}]}, {"text": "For semantic role labeling we use a two stage pipeline which parses the input sentence, and for recognizing textual entailment we use a three stage pipeline which tags the sentence with named entities and then parses it before passing it to the entailment decider.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6848653753598531}]}], "datasetContent": [{"text": "In our experiments we compare the greedy pipelined approach with our sampling pipeline approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for semantic role labeling task. The sampled  numbers are averaged over several runs, as discussed.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8240167647600174}]}]}