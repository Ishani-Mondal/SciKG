{"title": [{"text": "Get out the vote: Determining support or opposition from Congressional floor-debate transcripts", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.", "labels": [], "entities": []}, {"text": "To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.", "labels": [], "entities": []}, {"text": "We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation.", "labels": [], "entities": []}], "introductionContent": [{"text": "One ought to recognize that the present political chaos is connected with the decay of language, and that one can probably bring about some improvement by starting at the verbal end.", "labels": [], "entities": []}, {"text": "-Orwell, \"Politics and the English language\" We have entered an era where very large amounts of politically oriented text are now available online.", "labels": [], "entities": [{"text": "Orwell", "start_pos": 1, "end_pos": 7, "type": "DATASET", "confidence": 0.9805348515510559}]}, {"text": "This includes both official documents, such as the full text of laws and the proceedings of legislative bodies, and unofficial documents, such as postings on weblogs (blogs) devoted to politics.", "labels": [], "entities": []}, {"text": "In some sense, the availability of such data is simply a manifestation of a general trend of \"everybody putting their records on the Internet\".", "labels": [], "entities": []}, {"text": "The online accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially society-changing effect.", "labels": [], "entities": []}, {"text": "In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the \"electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process\", may \"[alter] the citizen-government relationship\").", "labels": [], "entities": [{"text": "electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process", "start_pos": 211, "end_pos": 329, "type": "TASK", "confidence": 0.7711581885814667}]}, {"text": "Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics 2 , or at least on political journalism . Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process.", "labels": [], "entities": []}, {"text": "Evaluative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text.", "labels": [], "entities": []}, {"text": "People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S.) bills often reach several hundred pages in length ().", "labels": [], "entities": []}, {"text": "Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist.plicitly solicited in the eRulemaking scenario.", "labels": [], "entities": []}, {"text": "In the analysis of evaluative language, it is fundamentally necessary to determine whether the author/speaker supports or disapproves of the topic of discussion.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each \"speech\" (continuous single-speaker segment of text) represents support for or opposition to a proposed piece of legislation.", "labels": [], "entities": []}, {"text": "Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.", "labels": [], "entities": []}, {"text": "Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes,,, and; see for an active bibliography).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.9437807500362396}]}, {"text": "In particular, since we treat each individual speech within a debate as a single \"document\", we are considering aversion of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents.", "labels": [], "entities": []}, {"text": "Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.", "labels": [], "entities": []}, {"text": "A few others incorporate various measures of inter-document similarity between the texts to be labeled ().", "labels": [], "entities": []}, {"text": "Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.", "labels": [], "entities": []}, {"text": "For example, we may find textual evidence of a high likelihood of agreement be- Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.", "labels": [], "entities": []}, {"text": "For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.", "labels": [], "entities": []}, {"text": "Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.", "labels": [], "entities": []}, {"text": "tween two speakers, such as explicit assertions (\"I second that!\") or quotation of messages in emails or postings (see but cf.).", "labels": [], "entities": [{"text": "quotation of messages in emails or postings", "start_pos": 70, "end_pos": 113, "type": "TASK", "confidence": 0.863928130694798}]}, {"text": "Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.", "labels": [], "entities": []}, {"text": "Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually.", "labels": [], "entities": []}, {"text": "Intuition suggests that this is true of the data with which we experiment, for several reasons.", "labels": [], "entities": []}, {"text": "First, U.S. congressional debates contain very rich language and cover an extremely wide variety of topics, ranging from flag burning to international policy to the federal budget.", "labels": [], "entities": [{"text": "flag burning", "start_pos": 121, "end_pos": 133, "type": "TASK", "confidence": 0.7182296365499496}]}, {"text": "Debates are also subject to digressions, some fairly natural and others less so (e.g., \"Why are we discussing this bill when the plight of my constituents regarding this other issue is being ignored?\")", "labels": [], "entities": []}, {"text": "Second, an important characteristic of persuasive language is that speakers may spend more time presenting evidence in support of their positions (or attacking the evidence presented by others) than directly stating their attitudes.", "labels": [], "entities": []}, {"text": "An extreme example will illustrate the problems involved.", "labels": [], "entities": []}, {"text": "Consider a speech that describes the U.S. flag as deeply inspirational, and thus contains only positive language.", "labels": [], "entities": []}, {"text": "If the bill under discussion is a proposed flag-burning ban, then the speech is supportive; but if the bill under discussion is aimed at rescinding an existing flag-burning ban, the speech may represent opposition to the legislation.", "labels": [], "entities": []}, {"text": "Given the current state of the art in sentiment analysis, it is doubtful that one could determine the (probably topic-specific) relationship between presented evidence and speaker opinion.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9622747600078583}]}, {"text": "Qualitative summary of results The above difficulties underscore the importance of enhancing standard classification techniques with new information sources that promise to improve accuracy, such as inter-document relationships between the documents to be labeled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9975457787513733}]}, {"text": "In this paper, we demonstrate that the incorporation of agreement modeling can provide substantial improvements over the application of support vector machines (SVMs) in isolation, which represents the state of the art in the individual classification of documents.", "labels": [], "entities": [{"text": "agreement modeling", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7170253992080688}]}, {"text": "The enhanced accuracies are obtained via a fairly primitive automatically-acquired \"agreement detector\" and a conceptually simple method for integrating isolated-document and agreement-based information.", "labels": [], "entities": []}, {"text": "We thus view our results as demonstrating the potentially large benefits of exploiting sentiment-related discourse-segment relationships in sentiment-analysis tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents experiments testing the utility of using speech-segment relationships, evaluating against a number of baselines.", "labels": [], "entities": []}, {"text": "All reported results use values for the free parameter \u03b1 derived via tuning on the development set.", "labels": [], "entities": []}, {"text": "In the tables, boldface indicates the development-and test-set results for the development-set-optimal parameter settings, as one would make algorithmic choices based on development-set performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Agreement-classifier accuracy, in per- cent. \"Amdmts\"=\"speech segments containing the  word 'amendment'\". Recall that boldface indi- cates results for development-set-optimal settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9847941994667053}]}, {"text": " Table 4: Segment-based speech-segment classifi- cation accuracy, in percent.", "labels": [], "entities": [{"text": "Segment-based speech-segment classifi", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.7467437982559204}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.695817232131958}]}, {"text": " Table 5: Speaker-based speech-segment classifica- tion accuracy, in percent. Here, the initial SVM is  run on the concatenation of all of a given speaker's  speech segments, but the results are computed  over speech segments (not speakers), so that they  can be compared to those in", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9383859038352966}]}]}