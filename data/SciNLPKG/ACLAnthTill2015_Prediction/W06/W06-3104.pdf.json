{"title": [{"text": "Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7422520220279694}]}, {"text": "We present anew model of the translation process: quasi-synchronous grammar (QG).", "labels": [], "entities": [{"text": "translation process", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.9014824628829956}]}, {"text": "Given a source-language parse tree T 1 , a QG defines a monolingual grammar that generates translations of T 1.", "labels": [], "entities": []}, {"text": "The trees T 2 allowed by this monolingual grammar are inspired by pieces of substruc-ture in T 1 and aligned to T 1 at those points.", "labels": [], "entities": []}, {"text": "We describe experiments learning quasi-synchronous context-free grammars from bitext.", "labels": [], "entities": []}, {"text": "As with other monolingual language models, we evaluate the cross-entropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence.", "labels": [], "entities": []}, {"text": "When evaluated on a word alignment task, QG matches standard baselines.", "labels": [], "entities": [{"text": "word alignment task", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8087441126505533}]}], "introductionContent": [], "datasetContent": [{"text": "We claim that for modeling human-translated bitext, it is better to project syntax only loosely.", "labels": [], "entities": []}, {"text": "To evaluate this claim, we train quasi-synchronous dependency grammars that allow progressively more divergence from monotonic tree alignment.", "labels": [], "entities": []}, {"text": "We evaluate these models on cross-entropy over held-out data and on error rate in a word-alignment task.", "labels": [], "entities": [{"text": "error rate", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9437460005283356}]}, {"text": "One might doubt the use of dependency trees for alignment, since found that constituency trees aligned better.", "labels": [], "entities": [{"text": "alignment", "start_pos": 48, "end_pos": 57, "type": "TASK", "confidence": 0.9801738262176514}]}, {"text": "That experiment, however, aligned only the 1-best parse trees.", "labels": [], "entities": []}, {"text": "We too will consider only the 1-best source tree T 1 , but in constrast to Gildea, we will search for the target tree T 2 that aligns best with T 1 . Finding T 2 and the alignment is simply a matter of parsing S 2 with the QG derived from T 1 .", "labels": [], "entities": [{"text": "Gildea", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.9476597309112549}]}], "tableCaptions": [{"text": " Table 1: Cross-entropy on held-out data with different depen-", "labels": [], "entities": []}, {"text": " Table 2: Alignment error rate (%) with different dependency", "labels": [], "entities": [{"text": "Alignment error rate", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.798415740331014}]}]}