{"title": [], "abstractContent": [{"text": "In this paper, we describe methods for building and evaluation of limited domain question-answering characters.", "labels": [], "entities": []}, {"text": "Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate.", "labels": [], "entities": [{"text": "text classification", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8160511553287506}, {"text": "language-model based retrieval", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.6356678605079651}, {"text": "cross-language information retrieval", "start_pos": 143, "end_pos": 179, "type": "TASK", "confidence": 0.6878300408522288}]}, {"text": "We also evaluated the effect of speech recognition errors on performance with users, finding that retrieval is robust until recognition reaches over 50% WER.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7018172740936279}, {"text": "WER", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9952210783958435}]}], "introductionContent": [{"text": "In the recent Hollywood movie \"iRobot\" set in 2035 the main character played by Will Smith is running an investigation into the death of an old friend.", "labels": [], "entities": []}, {"text": "The detective finds a small device that projects a holographic image of the deceased.", "labels": [], "entities": []}, {"text": "The device delivers a recorded message and responds to questions by playing back prerecorded answers.", "labels": [], "entities": []}, {"text": "We are developing virtual characters with similar capabilities.", "labels": [], "entities": []}, {"text": "Our target applications for these virtual characters are training, education, and entertainment.", "labels": [], "entities": []}, {"text": "For use in education, such a character should be able to deliver a message to the student on a specific topic.", "labels": [], "entities": []}, {"text": "It also should be able to support a basic spoken dialog on the subject of the message, e.g., answer questions about the message topic and give additional explanations.", "labels": [], "entities": []}, {"text": "For example, consider a student learning about an event in a virtual world.", "labels": [], "entities": []}, {"text": "Lets say there is a small circus in a small town and someone has released all the animals from circus.", "labels": [], "entities": []}, {"text": "A young student plays a role of a reporter to find out who caused this local havoc.", "labels": [], "entities": []}, {"text": "She is out to interrogate a number of witnesses represented by the virtual characters.", "labels": [], "entities": []}, {"text": "It is reasonable to expect that each conversation is going to be focused solely on the event of interest and the characters may refuse to talk about anything else.", "labels": [], "entities": []}, {"text": "Each witness may have a particular and very narrow view into an aspect of the event, and the student's success would depend on what sort of questions she asks and to which character she addresses them.", "labels": [], "entities": []}, {"text": "Automatic question answering (QA) has been studied extensively in recent years.", "labels": [], "entities": [{"text": "Automatic question answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8478934466838837}]}, {"text": "For example, there is a significant body of research done in the context of the QA track at the Text REtrieval Conference (TREC).", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 96, "end_pos": 128, "type": "TASK", "confidence": 0.7719270636638006}]}, {"text": "In contrast to the TREC scenario where both questions and answers are based on facts and the goal is to provide the most relevant answer, we focus the answer's appropriateness.", "labels": [], "entities": []}, {"text": "In our example about an investigation, an evasive, misleading, or an \"honestly\" wrong answer from a witness character would be appropriate but might not be relevant.", "labels": [], "entities": []}, {"text": "We try to highlight that distinction by talking about QA characters as opposed to QA systems or agents.", "labels": [], "entities": []}, {"text": "We expect that atypical simulation would contain quite a few QA characters.", "labels": [], "entities": []}, {"text": "We also expect those characters to have a natural spoken language interaction with the student.", "labels": [], "entities": []}, {"text": "Our technical requirements for such a QA character is that it should be able to understand spoken language.", "labels": [], "entities": []}, {"text": "It should be robust to disfluencies in conversational English.", "labels": [], "entities": []}, {"text": "It should be relatively fast, easy, and inexpensive to construct without the need for extensive domain knowledge and dialog management design expertise.", "labels": [], "entities": []}, {"text": "In this paper we describe a QA character by the name of SGT Blackwell who was originally designed to serve as an information kiosk at an army conference (see Appendix C fora photograph of the system) (?).", "labels": [], "entities": []}, {"text": "We have used SGT Blackwell to develop our technology for automatic answer selection, conversation management, and system integration.", "labels": [], "entities": [{"text": "SGT Blackwell", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.8134527206420898}, {"text": "answer selection", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.8449580073356628}, {"text": "conversation management", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.8464827537536621}, {"text": "system integration", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7525053322315216}]}, {"text": "We are presently using this technology to create other QA characters.", "labels": [], "entities": []}, {"text": "In the next section we outline the SGT Blackwell system setup.", "labels": [], "entities": [{"text": "SGT Blackwell", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.5824761986732483}]}, {"text": "In Section 3 we discuss the answer selection problem and consider three different algorithms: Support Vector Machines classifier (SVM), Language Model retrieval (LM), and Cross-lingual Language Model (CLM) retrieval.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.9361120164394379}, {"text": "Language Model retrieval (LM)", "start_pos": 136, "end_pos": 165, "type": "TASK", "confidence": 0.7267077465852102}, {"text": "Cross-lingual Language Model (CLM) retrieval", "start_pos": 171, "end_pos": 215, "type": "TASK", "confidence": 0.5996536229337964}]}, {"text": "We present the results of off-line experiments showing that the CLM method performs significantly better than the other two techniques in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 describes a user study of the system that uses the CLM approach for answer selection.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.9306063652038574}]}, {"text": "Our results show that the approach is very robust to deviations in wording from expected answers, and speech recognition errors.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.6719598919153214}]}, {"text": "Finally, we summarize our results and outline some directions for future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of three different algorithms for answer selection on SGT Blackwell data. Each  performance number is given in percentages.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.9466834962368011}, {"text": "SGT Blackwell data", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9465571443239847}]}, {"text": " Table 2: Comparison of three different algorithms for answer selection on 7 additional QA characters.  The table shows the number of answers and the number of questions collected for each character. The  accuracy and the improvement over the baseline numbers are given in percentages.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.8755252957344055}, {"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9994352459907532}]}]}