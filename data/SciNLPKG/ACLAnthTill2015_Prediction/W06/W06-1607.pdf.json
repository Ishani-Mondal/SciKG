{"title": [{"text": "Phrasetable Smoothing for Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrasetable Smoothing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8072969913482666}, {"text": "Statistical Machine Translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.8610644737879435}]}], "abstractContent": [{"text": "We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings.", "labels": [], "entities": [{"text": "Statistical MT", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.5875588357448578}]}, {"text": "We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used.", "labels": [], "entities": []}, {"text": "The best smoothing techniques yield consistent gains of approximately 1% (abso-lute) according to the BLEU metric.", "labels": [], "entities": [{"text": "abso-lute", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9785858392715454}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9952157735824585}]}], "introductionContent": [{"text": "Smoothing is an important technique in statistical NLP, used to deal with perennial data sparseness and empirical distributions that overfit the training corpus.", "labels": [], "entities": [{"text": "statistical NLP", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7286282181739807}]}, {"text": "Surprisingly, however, it is rarely mentioned in statistical Machine Translation.", "labels": [], "entities": [{"text": "statistical Machine Translation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.6605272789796194}]}, {"text": "In particular, state-of-the-art phrase-based SMT relies on a phrasetable-a large set of ngram pairs over the source and target languages, along with their translation probabilities.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8051084876060486}]}, {"text": "This table, which may contain tens of millions of entries, and phrases of up to ten words or more, is an excellent candidate for smoothing.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 129, "end_pos": 138, "type": "TASK", "confidence": 0.9817055463790894}]}, {"text": "Yet very few publications describe phrasetable smoothing techniques in detail.", "labels": [], "entities": [{"text": "phrasetable smoothing", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.9148710072040558}]}, {"text": "In this paper, we provide the first systematic study of smoothing methods for phrase-based SMT.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.600025087594986}]}, {"text": "Although we introduce a few new ideas, most methods described here were devised by others; the main purpose of this paper is not to invent new methods, but to compare methods.", "labels": [], "entities": []}, {"text": "In experiments over many language pairs, we show that smoothing yields small but consistent gains in translation performance.", "labels": [], "entities": []}, {"text": "We feel that this paper only scratches the surface: many other combinations of phrasetable smoothing techniques remain to be tested.", "labels": [], "entities": [{"text": "phrasetable smoothing", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7528436481952667}]}, {"text": "We define a phrasetable as a set of source phrases (ngrams) \u02dc sand their translations\u02dcttranslations\u02dc translations\u02dct, along with associated translation probabilities p(\u02dc s|\u02dcts|\u02dc s|\u02dct) and p( \u02dc t|\u02dcst|\u02dcs).", "labels": [], "entities": []}, {"text": "These conditional distributions are derived from the joint frequencies c(\u02dc s, \u02dc t) of source/target phrase pairs observed in a word-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "Traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities (, eg, p(\u02dc s|\u02dcts|\u02dc s|\u02dct) = c(\u02dc s, \u02dc t)/ \u02dc s c(\u02dc s, \u02dc t) (since the estimation problems for p(\u02dc s|\u02dcts|\u02dc s|\u02dct) and p( \u02dc t|\u02dcst|\u02dcs) are symmetrical, we will usually refer only to p(\u02dc s|\u02dcts|\u02dc s|\u02dct) for brevity).", "labels": [], "entities": []}, {"text": "The most obvious example of the overfitting this causes can be seen in phrase pairs whose constituent phrases occur only once in the corpus.", "labels": [], "entities": []}, {"text": "These are assigned conditional probabilities of 1, higher than the estimated probabilities of pairs for which much more evidence exists, in the typical case where the latter have constituents that cooccur occasionally with other phrases.", "labels": [], "entities": []}, {"text": "During decoding, overlapping phrase pairs are indirect competition, so estimation biases such as this one in favour of infrequent pairs have the potential to significantly degrade translation quality.", "labels": [], "entities": []}, {"text": "An excellent discussion of smoothing techniques developed for ngram language models (LMs) maybe found in).", "labels": [], "entities": []}, {"text": "Phrasetable smoothing differs from ngram LM smoothing in the following ways: \u2022 Probabilities of individual unseen events are not important.", "labels": [], "entities": [{"text": "Phrasetable smoothing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8783720433712006}, {"text": "ngram LM smoothing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6526618997255961}]}, {"text": "Because the decoder only proposes phrase translations that are in the phrasetable (ie, that have non-zero count), it never requires estimates for pairs\u02dcspairs\u02dc pairs\u02dcs, \u02dc t having c(\u02dc s, \u02dc t) = 0. 1 However, probability mass is reserved for the set of unseen translations, implying that probability mass is subtracted from the seen translations.", "labels": [], "entities": []}, {"text": "\u2022 There is no obvious lower-order distribution for backoff.", "labels": [], "entities": []}, {"text": "One of the most important techniques in ngram LM smoothing is to combine estimates made using the previous n \u2212 1 words with those using only the previous n\u2212i words, for i = 2 . .", "labels": [], "entities": [{"text": "ngram LM smoothing", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.5950434108575186}]}, {"text": ". n. This relies on the fact that closer words are more informative, which has no direct analog in phrasetable smoothing.", "labels": [], "entities": [{"text": "phrasetable smoothing", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7772259712219238}]}, {"text": "\u2022 The predicted objects are word sequences (in another language).", "labels": [], "entities": []}, {"text": "This contrasts to LM smoothing where they are single words, and are thus less amenable to decomposition for smoothing purposes.", "labels": [], "entities": [{"text": "LM smoothing", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7757386565208435}]}, {"text": "We propose various ways of dealing with these special features of the phrasetable smoothing problem, and give evaluations of their performance within a phrase-based SMT system.", "labels": [], "entities": [{"text": "phrasetable smoothing problem", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.854577918847402}, {"text": "SMT", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.8124013543128967}]}, {"text": "The paper is structured as follows: section 2 gives a brief description of our phrase-based SMT system; section 3 presents the smoothing techniques used; section 4 reviews previous work; section 5 gives experimental results; and section 6 concludes and discusses future work.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.8461532592773438}]}], "datasetContent": [{"text": "We carried out experiments in two different settings: broad-coverage ones across six European language pairs using selected smoothing techniques and relatively small training corpora; and Chinese to English experiments using all implemented smoothing techniques and large training corpora.", "labels": [], "entities": []}, {"text": "For the black-box techniques, the smoothed phrase table replaced the original relative-frequency (RF) phrase table.", "labels": [], "entities": []}, {"text": "For the glass-box techniques, a phrase table (either the original RF phrase table or its replacement after black-box smoothing) was interpolated in loglinear fashion with the smoothing glass-box distribution, with weights set to maximize BLEU on a development corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.9987626075744629}]}, {"text": "To estimate the significance of the results across different methods, we used 1000-fold pairwise bootstrap resampling at the 95% confidence level.", "labels": [], "entities": []}, {"text": "In order to measure the benefit of phrasetable smoothing for relatively small corpora, we used the data made available for the WMT06 shared task).", "labels": [], "entities": [{"text": "phrasetable smoothing", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8637910783290863}, {"text": "WMT06 shared task", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.8030413389205933}]}, {"text": "This exercise is conducted openly with access to all needed resources and is thus ideal for benchmarking statistical phrasebased translation systems on a number of language pairs.", "labels": [], "entities": [{"text": "benchmarking statistical phrasebased translation", "start_pos": 92, "end_pos": 140, "type": "TASK", "confidence": 0.6691756993532181}]}, {"text": "The WMT06 corpus is based on sentences extracted from the proceedings of the European Parliament.", "labels": [], "entities": [{"text": "WMT06 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9015367031097412}]}, {"text": "Separate sentence-aligned parallel corpora of about 700,000 sentences (about 150MB) are provided for the three language pairs having one of French, Spanish and German with English.", "labels": [], "entities": []}, {"text": "SRILM language models based on the same source are also provided for each of the four languages.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8858559131622314}]}, {"text": "We used the provided 2000-sentence devsets for tuning loglinear parameters, and tested on the 3064-sentence test sets.", "labels": [], "entities": []}, {"text": "Results are shown in table 1 for relativefrequency (RF), Good-Turing (GT), Kneser-Ney with 1 (KN1) and 3 (KN3) discount coefficients; and loglinear combinations of both RF and KN3 phrasetables with Zens-Ney-IBM1 (ZN-IBM1) smoothed phrasetables (these combinations are denoted RF+ZN-IBM1 and KN3+ZN-IBM1).", "labels": [], "entities": []}, {"text": "It is apparent from table 1 that any kind of phrase table smoothing is better than using none; the minimum improvement is 0.45 BLEU, and the difference between RF and all other methods is statistically significant.", "labels": [], "entities": [{"text": "phrase table smoothing", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7157114744186401}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9993942975997925}]}, {"text": "Also, KneserNey smoothing gives a statistically significant improvement over GT smoothing, with a minimum gain of 0.30 BLEU.", "labels": [], "entities": [{"text": "KneserNey smoothing", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.5920712947845459}, {"text": "GT smoothing", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.7071979343891144}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9983296990394592}]}, {"text": "Using more discounting coefficients does not appear to help.", "labels": [], "entities": []}, {"text": "Smoothing relative frequencies with an additional Zens-Ney phrasetable gives about the same gain as KneserNey smoothing on its own.", "labels": [], "entities": []}, {"text": "However, combining Kneser-Ney with Zens-Ney gives a clear gain over any other method (statistically significant for all language pairs except en\u2192es and en\u2192de) demonstrating that these approaches are complementary.", "labels": [], "entities": []}, {"text": "To test the effects of smoothing with larger corpora, we ran a set of experiments for Chinese-English translation using the corpora distributed for the NIST MT05 evaluation (www.nist.gov/speech/tests/mt).", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.5854656398296356}, {"text": "NIST MT05 evaluation", "start_pos": 152, "end_pos": 172, "type": "DATASET", "confidence": 0.8988496859868368}]}, {"text": "These are summarized in table 2.", "labels": [], "entities": []}, {"text": "Due to the large size of the out-of-domain UN corpus, we trained one phrasetable on it, and another on all other parallel corpora (smoothing was applied to both).", "labels": [], "entities": []}, {"text": "We also used a subset of the English Gigaword corpus to augment the LM training material.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.8162375688552856}]}, {"text": "As with the broad-coverage experiments, all of the black-box smoothing techniques do significantly better than the RF baseline.", "labels": [], "entities": []}, {"text": "However, GT appears to work better in the large-corpus setting: it is statistically indistinguishable from KN3, and both these methods are significantly better than all other fixeddiscount variants, among which there is little difference.", "labels": [], "entities": []}, {"text": "Not surprisingly, the two glass-box methods, ZN-IBM1 and KOM-IBM1, do poorly when used on their own.", "labels": [], "entities": []}, {"text": "However, in combination with another phrasetable, they yield the best results, obtained by RF+ZN-IBM1 and GT+KOM-IBM1, which are statistically indistinguishable.", "labels": [], "entities": [{"text": "RF+ZN-IBM1", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.8376824855804443}]}, {"text": "In constrast to the situation in the broad-coverage setting, these are not significantly better than the best black-box method (GT) on its own, although  A striking difference between the broadcoverage setting and the Chinese-English setting is that in the former it appears to be beneficial to apply KN3 smoothing to the phrasetable that gets combined with the best glass-box phrasetable (ZN), whereas in the latter setting it does not.", "labels": [], "entities": []}, {"text": "To test whether this was due to corpus size (as the broad-coverage corpora are around 10% of those for Chinese-English), we calculated ChineseEnglish learning curves for the RF+ZN-IBM1 and KN3-ZN-IBM1 methods, shown in.", "labels": [], "entities": []}, {"text": "The results are somewhat inconclusive: although the KN3+ZN-IBM1 curve is perhaps slightly flatter, the most obvious characteristic is that this method appears to be highly sensitive to the particular corpus sample used.", "labels": [], "entities": []}], "tableCaptions": []}