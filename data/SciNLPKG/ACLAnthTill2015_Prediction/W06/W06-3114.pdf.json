{"title": [{"text": "Manual and Automatic Evaluation of Machine Translation between European Languages", "labels": [], "entities": [{"text": "Machine Translation between European Languages", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.8210692048072815}]}], "abstractContent": [{"text": "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to En-glish and back.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7378551661968231}]}, {"text": "Evaluation was done automatically using the BLEU score and manually on fluency and adequacy.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9530665278434753}]}, {"text": "For the 2006 NAACL/HLT Workshop on Machine Translation, we organized a shared task to evaluate machine translation performance.", "labels": [], "entities": [{"text": "NAACL/HLT Workshop on Machine Translation", "start_pos": 13, "end_pos": 54, "type": "TASK", "confidence": 0.7650917172431946}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.785457044839859}]}, {"text": "14 teams from 11 institutions participated, ranging from commercial companies, industrial research labs to individual graduate students.", "labels": [], "entities": []}, {"text": "The motivation for such a competition is to establish baseline performance numbers for defined training scenarios and test sets.", "labels": [], "entities": []}, {"text": "We assembled various forms of data and resources: a baseline MT system, language models, prepared training and test sets, resulting in actual machine translation output from several state-of-the-art systems and manual evaluations.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9742447733879089}, {"text": "machine translation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7172446548938751}]}, {"text": "All this is available at the workshop website 1.", "labels": [], "entities": []}, {"text": "The shared task is a follow-up to the one we organized in the previous year, at a similar venue (Koehn and Monz, 2005).", "labels": [], "entities": []}, {"text": "As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training.", "labels": [], "entities": [{"text": "translation of European languages", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.8968993872404099}, {"text": "Europarl corpus", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9880446791648865}]}, {"text": "Again, most systems that participated could be categorized as statistical phrase-based systems.", "labels": [], "entities": []}, {"text": "While there is now a number of competitions-DARPA/NIST (Li, 2005), IWSLT (Eck and Hori, 2005), TC-Star-this one focuses on text translation between various Euro-pean languages.", "labels": [], "entities": [{"text": "NIST (Li, 2005)", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8891175091266632}, {"text": "IWSLT", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.8618490695953369}, {"text": "text translation between various Euro-pean languages", "start_pos": 123, "end_pos": 175, "type": "TASK", "confidence": 0.7930915455023447}]}, {"text": "This year's shared task changed in some aspects from last year's: \u2022 We carried out a manual evaluation in addition to the automatic scoring.", "labels": [], "entities": []}, {"text": "Manual evaluation 1 http://www.statmt.org/wmt06/ was done by the participants.", "labels": [], "entities": []}, {"text": "This revealed interesting clues about the properties of automatic and manual scoring.", "labels": [], "entities": []}, {"text": "\u2022 We evaluated translation from English, in addition to into English.", "labels": [], "entities": [{"text": "translation from English", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.8811872601509094}]}, {"text": "English was again paired with German, French, and Spanish.", "labels": [], "entities": []}, {"text": "We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.", "labels": [], "entities": []}, {"text": "\u2022 We included an out-of-domain test set.", "labels": [], "entities": []}, {"text": "This allows us to compare machine translation performance in-domain and out-of-domain.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.774863064289093}]}, {"text": "1 Evaluation Framework The evaluation framework for the shared task is similar to the one used in last year's shared task.", "labels": [], "entities": []}, {"text": "Training and testing is based on the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9958536028862}]}, {"text": "Figure 1 provides some statistics about this corpus.", "labels": [], "entities": []}, {"text": "1.1 Baseline system To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9685987830162048}]}, {"text": "To summarize, we provided: \u2022 sentence-aligned, tokenized training corpus \u2022 a development and development test set \u2022 trained language models for each language \u2022 the phrase-based MT decoder Pharaoh \u2022 a training script to build models for Pharaoh The performance of the baseline system is similar to the best submissions in last year's shared task.", "labels": [], "entities": []}, {"text": "We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer.", "labels": [], "entities": []}, {"text": "102 Training corpus Spanish \u2194 English French \u2194 English German \u2194 English Sentences 730,740 688,031 751,088 Foreign words 15,676,710 15,323,737 15,256,793 English words 15,222,105 13,808,104 16,052,269 Distinct foreign words 102,886 80,349 195,291 Distinct English words 64,123 61,627 65,889", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The evaluation framework for the shared task is similar to the one used in last year's shared task.", "labels": [], "entities": []}, {"text": "Training and testing is based on the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9958536028862}]}, {"text": "Figure 1 provides some statistics about this corpus.", "labels": [], "entities": []}, {"text": "For the automatic evaluation, we used BLEU, since it is the most established metric in the field.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9988994598388672}]}, {"text": "The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9937011003494263}]}, {"text": "It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence.", "labels": [], "entities": []}, {"text": "The BLEU score has been shown to correlate well with human judgement, when statistical ma-chine translation systems are compared.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9718184471130371}]}, {"text": "However, a recent study ), pointed out that this correlation may not always be strong.", "labels": [], "entities": []}, {"text": "They demonstrated this with the comparison of statistical systems against (a) manually post-edited MT output, and (b) a rule-based commercial system.", "labels": [], "entities": [{"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9076231718063354}]}, {"text": "The development of automatic scoring methods is an open field of research.", "labels": [], "entities": []}, {"text": "It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation.", "labels": [], "entities": []}, {"text": "At the very least, we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.", "labels": [], "entities": []}, {"text": "While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7340444624423981}, {"text": "BLEU", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.955524206161499}]}, {"text": "Many human evaluation metrics have been proposed.", "labels": [], "entities": []}, {"text": "Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7716512978076935}]}, {"text": "The main disadvantage of manual evaluation is that it is time-consuming and thus too expensive to do frequently.", "labels": [], "entities": []}, {"text": "In this shared task, we were also confronted with this problem, and since we had no funding for paying human judgements, we asked participants in the evaluation to share the burden.", "labels": [], "entities": []}, {"text": "Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.", "labels": [], "entities": []}, {"text": "This is the first time that we organized a large-scale manual evaluation.", "labels": [], "entities": []}, {"text": "While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns.", "labels": [], "entities": []}, {"text": "For instance, in the recent IWSLT evaluation, first fluency annotations were solicited (while withholding the source sentence), and then adequacy annotations.", "labels": [], "entities": [{"text": "IWSLT evaluation", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.8139817416667938}]}, {"text": "Almost all annotators reported difficulties in maintaining a consistent standard for fluency and adequacy judgements, but nevertheless most did not explicitly move towards a ranking-based evaluation.", "labels": [], "entities": []}, {"text": "Almost all annotators expressed their preference to move to a ranking-based evaluation in the future.", "labels": [], "entities": []}, {"text": "A few pointed out that adequacy should be broken up into two criteria: (a) are all source words covered?", "labels": [], "entities": []}, {"text": "(b) does the translation have the same meaning, including connotations?", "labels": [], "entities": []}, {"text": "Annotators suggested that long sentences are almost impossible to judge.", "labels": [], "entities": []}, {"text": "Since all long sentence translation are somewhat muddled, even a contrastive evaluation between systems was difficult.", "labels": [], "entities": [{"text": "long sentence translation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.819456160068512}]}, {"text": "A few annotators suggested to breakup long sentences into clauses and evaluate these separately.", "labels": [], "entities": []}, {"text": "Not every annotator was fluent in both the source and the target language.", "labels": [], "entities": []}, {"text": "While it is essential to be fluent in the target language, it is not strictly necessary to know the source language, if a reference translation was given.", "labels": [], "entities": []}, {"text": "However, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate -due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.6730664968490601}]}, {"text": "Lack of correct reference translations was pointed out as a short-coming of our evaluation.", "labels": [], "entities": []}, {"text": "One annotator suggested that this was the case for as much as 10% of our test sentences.", "labels": [], "entities": []}, {"text": "Annotators argued for the importance of having correct and even multiple references.", "labels": [], "entities": []}, {"text": "It was also proposed to allow annotators to skip sentences that they are unable to judge.", "labels": [], "entities": []}], "tableCaptions": []}