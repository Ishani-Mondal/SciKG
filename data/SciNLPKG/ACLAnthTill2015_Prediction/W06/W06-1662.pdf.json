{"title": [{"text": "Sentence Ordering with Manifold-based Classification in Multi-Document Summarization", "labels": [], "entities": [{"text": "Sentence Ordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9437790215015411}, {"text": "Summarization", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.7349776029586792}]}], "abstractContent": [{"text": "In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8029456734657288}, {"text": "sentence classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7527489066123962}]}, {"text": "The classification is based on the manifold structure underlying sentences, addressing the problem of limited labeled data.", "labels": [], "entities": []}, {"text": "The historical ordering helps to ensure topic continuity and avoid topic bias.", "labels": [], "entities": [{"text": "historical ordering", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6421898156404495}, {"text": "topic continuity", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.6903008222579956}]}, {"text": "Experiments demonstrate that the method is effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence ordering has been a concern in text planning and concept-to-text generation ().", "labels": [], "entities": [{"text": "Sentence ordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9339543879032135}, {"text": "text planning", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.793912261724472}, {"text": "concept-to-text generation", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7703788578510284}]}, {"text": "Recently, it has also drawn attention in multi-document summarization ().", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.6878896355628967}]}, {"text": "Since summary sentences generally come from different sources in multi-document summarization, an optimal ordering is crucial to make summaries coherent and readable.", "labels": [], "entities": [{"text": "summaries coherent", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.8950464725494385}]}, {"text": "In general, the strategies for sentence ordering in multi-document summarization fall in two categories.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7153530269861221}]}, {"text": "One is chronological ordering (), which is based on time-related features of the documents.", "labels": [], "entities": [{"text": "chronological ordering", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8584756851196289}]}, {"text": "However, such temporal features maybe not available in all cases.", "labels": [], "entities": []}, {"text": "Furthermore, temporal inference in texts is still a problem, in spite of some progress in automatic disambiguation of temporal information).", "labels": [], "entities": [{"text": "temporal inference", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6807934194803238}]}, {"text": "Another strategy is majority ordering (MO) (;), in which each summary sentence is mapped to a theme, i.e., a set of similar sentences in the documents, and the order of these sentences determines that for summary sentences.", "labels": [], "entities": [{"text": "majority ordering (MO", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.6062490865588188}]}, {"text": "To do that, a directed theme graph is built, in which if a theme A occurs behind another theme B in a document, B is linked to A no matter how faraway they are located.", "labels": [], "entities": []}, {"text": "However, this may lead to wrong theme correlations, since B's occurrence may rely on a third theme C and have nothing to do with A.", "labels": [], "entities": [{"text": "B's occurrence", "start_pos": 58, "end_pos": 72, "type": "METRIC", "confidence": 0.7027575572331747}]}, {"text": "In addition, when outputting theme orders, MO uses a kind of heuristic that chooses a theme based on its in-out edge difference in the directed theme graph.", "labels": [], "entities": [{"text": "MO", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.7235931158065796}]}, {"text": "This may cause topic disruption, since the next choice may have no link with previous choices.", "labels": [], "entities": []}, {"text": "proposed a probabilistic ordering (PO) method for text structuring, which can be adapted to majority ordering if the training texts are those documents to be summarized.", "labels": [], "entities": [{"text": "text structuring", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.76019287109375}, {"text": "majority ordering", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.6925702095031738}]}, {"text": "The primary evidence for the ordering are informative features of sentences, including words and their grammatical dependence relations, which needs reliable parsing of the text.", "labels": [], "entities": []}, {"text": "Unlike in MO, selection of the next sentence here is based on the most recent one.", "labels": [], "entities": [{"text": "MO", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.961879312992096}]}, {"text": "However, this may lead to topic bias: i.e. too many sentences on the same topic.", "labels": [], "entities": []}, {"text": "In this paper, we propose a historical ordering (HO) strategy, in which the selection of the next sentence is based on the whole history of selection, not just the most recent choice.", "labels": [], "entities": [{"text": "historical ordering (HO)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6948845863342286}]}, {"text": "This strategy helps to ensure continuity of topics but to avoid topic bias at the same time.", "labels": [], "entities": []}, {"text": "To do that, we need to map summary sentences to those in documents.", "labels": [], "entities": []}, {"text": "We formalize this as a kind of classification problem, with summary sentences as class labels.", "labels": [], "entities": []}, {"text": "Since there are very few (only one) labeled examples for each class, we adopt a kind of semi-supervised classification method, which makes use of the manifold structure underlying the sentences to do the classification.", "labels": [], "entities": []}, {"text": "A common assumption behind this learning paradigm is that the manifold structure among the data, revealed by higher density, provides a global comparison between data points.", "labels": [], "entities": []}, {"text": "Under such an assumption, even one labeled example is enough for classification, if only the structure is determined.", "labels": [], "entities": [{"text": "classification", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.9765539169311523}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we give an overview of the proposed method.", "labels": [], "entities": []}, {"text": "In section 3~5, we talk about the method including sentence networks, classification and ordering.", "labels": [], "entities": []}, {"text": "In section 6, we present experiments and evaluations.", "labels": [], "entities": []}, {"text": "Finally in section 7, we give some conclusions and future work.", "labels": [], "entities": []}, {"text": "gives the overall structure of the proposed method, which includes three modules: construction of sentence networks, sentence classification and sentence ordering.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.7725136578083038}, {"text": "sentence ordering", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.7142582386732101}]}, {"text": "The first step is to build a sentence neighborhood network with weights on edges, which can serve as the basis fora Markov random walk ().", "labels": [], "entities": []}, {"text": "The neighborhood is based on similarity between sentences, and weights on edges can be seen as transition probabilities for the random walk.", "labels": [], "entities": []}, {"text": "From this network, we can derive new representations for sentences.", "labels": [], "entities": []}, {"text": "The second step is to make a classification of sentences, with each summary sentence as a class label.", "labels": [], "entities": []}, {"text": "Since only one labeled example exists for each class, we use a semi-supervised method based on a Markov random walk to reveal the manifold structure for the classification.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed method in this paper consists of two main steps: sentence classification and sentence ordering.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.7494674623012543}, {"text": "sentence ordering", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7446997463703156}]}, {"text": "For classification, we used pointwise entropy () to measure the quality of the classification result due to lack of enough labeled data.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9707652926445007}]}, {"text": "For a n\u00d7m matrix M, whose row vectors are normalized as 1, its pointwise entropy is defined in 9).", "labels": [], "entities": []}, {"text": "Intuitively, if M i,j is close to 0 or 1, E(M) tends towards 0, which corresponds to clearer distinctions between classes; otherwise E(M) tends towards 1, which means there are no clear boundaries between classes.", "labels": [], "entities": []}, {"text": "For comparison between different matrices, E(M) needs to be averaged over n\u00d7m.", "labels": [], "entities": []}, {"text": "For sentence ordering, we used Kendall's \u03c4 coefficient, as defined in 10), where, N I is number of inversions of consecutive sentences needed to transform output of the algorithm to manual summaries.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8103142082691193}, {"text": "Kendall's \u03c4 coefficient", "start_pos": 31, "end_pos": 54, "type": "METRIC", "confidence": 0.6636933237314224}]}, {"text": "The measure ranges from -1 for inverse ranks to +1 for identical ranks, and can also be seen as a kind of edit similarity between two ranks: smaller values for lower similarity, and bigger values for higher similarity.", "labels": [], "entities": []}, {"text": "For sentence classification, we need to estimate the parameter t.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8291659951210022}]}, {"text": "We randomly chose 5 document clusters and one manual summary from the four.", "labels": [], "entities": []}, {"text": "shows the change of the average margin overall the top 40% sentences in a cluster with t varying from 3 to 25.", "labels": [], "entities": []}, {"text": "indicates that the average margin changes with t for each cluster and the values oft maximizing the margin are different for different clusters.", "labels": [], "entities": []}, {"text": "For the 5 clusters, the estimated t is 16, 8, 14, 12 and 21 respectively.", "labels": [], "entities": []}, {"text": "So we need to estimate the best t for each cluster.", "labels": [], "entities": []}, {"text": "After estimation oft, EM was used to estimate the membership probabilities.", "labels": [], "entities": [{"text": "EM", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9812643527984619}]}, {"text": "gives the average pointwise entropy for top 10% to top 100% sentences in each cluster, where sentences were ordered by their membership probabilities.", "labels": [], "entities": []}, {"text": "The values were averaged over 20 runs, and for each run, 10 document clusters and one summary were randomly selected, and the entropy was averaged over the summaries., the column E_Semi shows entropies of the semi-supervised classification.", "labels": [], "entities": []}, {"text": "It indicates that the entropy increases as more sentences are considered.", "labels": [], "entities": []}, {"text": "This is not surprising since the sentences are ordered by their membership probabilities in a cluster, which can be seen as a kind of measure for closeness between sentences and cluster centroids, and the boundaries between clusters become dim with more sentences considered.", "labels": [], "entities": []}, {"text": "We used the same classification results to test the performance of our ordering algorithm HO as well as MO and PO.", "labels": [], "entities": [{"text": "MO", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9784067273139954}, {"text": "PO", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9626443386077881}]}, {"text": "indicates that the combination of sentences harms the performance.", "labels": [], "entities": []}, {"text": "To see why, we checked the classification results, and found that the pointwise entropies for two and three sentence combinations (for the top 40% sentence in each cluster) increase 12.4% and 18.2% respectively.", "labels": [], "entities": []}, {"text": "This means that the cluster structure becomes less clear with two or three sentence combinations, which would lead to less similar sentences being clustered with summary sentences.", "labels": [], "entities": []}, {"text": "This result also suggests that if the summary sentence subsumes multiple sentences in the documents, they tend to be not consecutive.", "labels": [], "entities": []}, {"text": "shows change of \u03c4 scores with different number of sentences used for ordering, where x axis denotes top (1-x)*100% sentences in each cluster.", "labels": [], "entities": []}, {"text": "The score was averaged over 20 runs, and for each run, 10 summaries were randomly selected and evaluated.", "labels": [], "entities": []}, {"text": "indicates that with fewer sentences (x >=0.7) used for ordering, the performance decreases.", "labels": [], "entities": []}, {"text": "The reason maybe that with fewer and fewer sentences used, the result is deficient training data for the ordering.", "labels": [], "entities": []}, {"text": "On the other hand, with more sentences used (x <0.6), the performance also decreases.", "labels": [], "entities": []}, {"text": "The reason maybe that as more sentences are used, the noisy sentences could dominate the ordering.", "labels": [], "entities": []}, {"text": "That's why we considered only the top 40% sentences in each cluster as training data for sentence reordering here.", "labels": [], "entities": [{"text": "sentence reordering", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7158718109130859}]}, {"text": "As an example, the following is a summary fora cluster of documents about Central American storms, in which the ordering is given manually.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Entropy of classification result", "labels": [], "entities": [{"text": "Entropy of classification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6482886175314585}]}]}