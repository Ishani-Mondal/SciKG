{"title": [{"text": "Another Evaluation of Anaphora Resolution Algorithms and a Comparison with GETARUNS' Knowledge Rich Approach", "labels": [], "entities": [{"text": "Anaphora Resolution Algorithms", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.7654500305652618}, {"text": "GETARUNS", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.5168596506118774}]}], "abstractContent": [{"text": "In this paper we will present an evaluation of current state-of-the-art algorithms for Anaphora Resolution based on a segment of Susanne corpus (itself a portion of Brown", "labels": [], "entities": [{"text": "Anaphora Resolution", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.9469294250011444}, {"text": "Susanne corpus", "start_pos": 129, "end_pos": 143, "type": "DATASET", "confidence": 0.9661949872970581}]}], "introductionContent": [{"text": "The problem of anaphora resolution (hence AR) looms more and more as a prominent one in unrestricted text processing due to the need to recover semantically consistent information inmost current NLP applications.", "labels": [], "entities": [{"text": "anaphora resolution (hence AR)", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6358855466047922}]}, {"text": "This problem does not lend itself easily to a statistical approach so that rule-based approaches seem the only viable solution.", "labels": [], "entities": []}, {"text": "We present anew evaluation of three state-of-the-art algorithms for anaphora resolution -GuiTAR, JavaRAP, MARS -on the basis of a portion of Susan Corpus (derived from Brown Corpus) a much richer testbed than the ones previously used for evaluation, and in any case a much more comparable source with such texts as newspaper articles and stories.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7517852485179901}, {"text": "GuiTAR", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.7493226528167725}, {"text": "MARS", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9876734614372253}, {"text": "Susan Corpus (derived from Brown Corpus", "start_pos": 141, "end_pos": 180, "type": "DATASET", "confidence": 0.9286589026451111}]}, {"text": "Texts used previously ranged from scientific manuals to descriptive scientific texts and were generally poor on pronouns and rich on nominal descriptions.", "labels": [], "entities": []}, {"text": "Two of the algorithmsGuiTAR and JavaRAP -use Charniak's parser output, which contributes to the homogeneity of the type of knowledge passed to the resolution procedure.", "labels": [], "entities": []}, {"text": "MARS, on the contrary, uses a more sophisticated input, the one provided by Connexor FDG-parser.", "labels": [], "entities": [{"text": "MARS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8632491230964661}, {"text": "Connexor FDG-parser", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.775801032781601}]}, {"text": "The algorithms will then be compared to our system, GETARUNS, which incorporated an AR algorithm at the end of a pipeline of interconnected modules that instantiate standard architectures for NLP.", "labels": [], "entities": [{"text": "GETARUNS", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8636406660079956}]}, {"text": "The version of the algorithm presented here is a newly elaborated one, and is devoted to unrestricted text processing.", "labels": [], "entities": []}, {"text": "It is an upgraded version from the one discussed in) and tries to incorporate as much as possible of the more sophisticated version implemented in the complete GETARUN (see.", "labels": [], "entities": [{"text": "GETARUN", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.883858859539032}]}, {"text": "The paper is organized as follows: in section 2 below we briefly discuss architectures and criteria for AR of the three algorithms evaluated.", "labels": [], "entities": [{"text": "AR", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.5259649157524109}]}, {"text": "In section 3 we present our system.", "labels": [], "entities": []}, {"text": "Section 4 is dedicated to a compared evaluation and a general discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating anaphora resolution systems calls fora reformulation of the usual parameters of Precision and Recall as introduced in IR/IE field: in that case, there are two levels that are used as valuable results; a first stage where systems are measured for their capacity to retrieve/extract relevant items from the corpus/web (coverage-recall).", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7373183071613312}, {"text": "Precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.8171429634094238}, {"text": "Recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8194974660873413}, {"text": "IR/IE", "start_pos": 129, "end_pos": 134, "type": "TASK", "confidence": 0.6844241817792257}]}, {"text": "Then a second stage follows in which systems are evaluated for their capacity to match the content of the query (accuracy-precision).", "labels": [], "entities": [{"text": "accuracy-precision", "start_pos": 113, "end_pos": 131, "type": "METRIC", "confidence": 0.9987396597862244}]}, {"text": "In the field of IR/IE items to be matched are usually constituted by words/phrases and pattern-matching procedures are the norm.", "labels": [], "entities": [{"text": "IR/IE items", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.903174951672554}]}, {"text": "However, for AR systems this is not sufficient and NLP heavy techniques are used to get valuable results.", "labels": [], "entities": []}, {"text": "As Mitkov also notes, this phase jeopardizes the capacity of AR systems to reach satisfactory accuracy scores simply because of its intrinsic weakness: none of the off-the-shelf parsers currently available overcomes 90% accuracy.", "labels": [], "entities": [{"text": "accuracy scores", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.9732767045497894}, {"text": "accuracy", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9928203225135803}]}, {"text": "To clarify these issues, we present here below two Tables: in the first one we report data related to the vexed question of whether pleonastic \"it\" should be regarded as part of the task of anaphora resolution or rather part of a separate classification task -as suggested in a number of papers by Mitkov.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.7549470663070679}]}, {"text": "In the former case, they should contribute to the overall anaphora resolution evaluation metrics; in the latter case they should be compute separately as a case of classification overall occurrences of \"it\" in the current dataset and discarded from the overall count.", "labels": [], "entities": [{"text": "anaphora resolution evaluation", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7607086102167765}]}, {"text": "Even though we don't agree fully with Mitkov's position, we find it useful to deal with \"it\" separate, due to its high inherent ambiguity.", "labels": [], "entities": []}, {"text": "Besides, it is true that the AR task is not like any Information Retrieval task.", "labels": [], "entities": [{"text": "AR", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.7418079376220703}, {"text": "Information Retrieval task", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.824634850025177}]}, {"text": "In below we reported figures for \"it\" in order to evaluate the three algorithms in relation to the classification task.", "labels": [], "entities": []}, {"text": "Then in. we report general data where we computed the two types of accuracy reported in the literature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9989529848098755}]}, {"text": "In we split results for \"it\" into Wrong Reference vs Wrong Classification: following Mitkov, in case we only computed anaphora related cases and disregarded those cases of \"it\" which were wrongly classified as expletives.", "labels": [], "entities": []}, {"text": "Expletive \"it\" present in the text are 189: so at first we computed coverage and accuracy with the usual formula that we report below.", "labels": [], "entities": [{"text": "coverage", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9983301758766174}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9992720484733582}]}, {"text": "Then we subtracted wrongly classified cases from the number of total \"it\" found in one case (following Mitkov who claims that wrongly classified \"it\" found by the system should not count; in another case, this number is subtracted from the total number of \"it\" to be found in the text.", "labels": [], "entities": []}, {"text": "Only for MARS we then computed different measures of Coverage and Accuracy.", "labels": [], "entities": [{"text": "MARS", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8053837418556213}, {"text": "Coverage", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8919873237609863}, {"text": "Accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.996769905090332}]}, {"text": "If we regard this approach worth pursuing, we come up with two Adjusted Accuracy measures which are related to the revised total numbers of anaphors by the two subtractions indicated above.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.7120496034622192}]}, {"text": "We computed manually all third person pronominal expressions and came up with a figure 982 which is only confirmed by one of the three systems considered: JavaRAP.", "labels": [], "entities": [{"text": "JavaRAP", "start_pos": 155, "end_pos": 162, "type": "DATASET", "confidence": 0.8894234299659729}]}, {"text": "Pronouns considered are the following one, lowercase and uppercase included: Possessives -his, its, her, hers, their, theirs Personals -he, she, it, they, him, her, it, them (where \"it\" and \"her\" have to be disambiguated) Reflexives -himself, itself, herself, themselves There are 16 different wordforms.", "labels": [], "entities": []}, {"text": "As can be seen from the table below, apart from JavaRAP, none of the other systems considered comes close to 100% coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9869961738586426}]}, {"text": "Computing general measures for Precision and Recall we have three quantities (see also Poesio & Kabadjov): \u2022 total number of anaphors present in the text; \u2022 anaphors identified by the system; \u2022 correctly resolved anaphors.", "labels": [], "entities": [{"text": "Precision and Recall", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.5921556750933329}]}, {"text": "Formulas related to Accuracy/Success Rate or Precision are as follows: Accuracy1 = number of successfully resolved anaphors/number of all anaphors; Accuracy2 = number of successfully resolved anaphors/number of anaphors found (attempted to be resolved).", "labels": [], "entities": [{"text": "Accuracy/Success Rate", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.7967153787612915}, {"text": "Precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.80101478099823}, {"text": "Accuracy1", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9990432858467102}, {"text": "Accuracy2", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9965776801109314}]}, {"text": "Recall -which should correspond to Coverage -we come up with formula: R= number of anaphors found /number of all anaphors to be resolved (present in the text).", "labels": [], "entities": [{"text": "Coverage", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.5799668431282043}, {"text": "R", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.97254478931427}]}, {"text": "Finally the formula for F-measure is as follows: 2*P*R/(P+R) where P is chosen as Accuracy 2.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9958006739616394}]}, {"text": "In absolute terms best accuracy figures have been obtained by GETARUNS, followed by JavaRAP.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993942975997925}, {"text": "GETARUNS", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.4977726340293884}, {"text": "JavaRAP", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.903727650642395}]}, {"text": "So it is still thanks to the classic Recall formula that this result stands out clearly.", "labels": [], "entities": []}, {"text": "We also produced another table which can however only be worked out for our system, which uses a distributed approach.", "labels": [], "entities": []}, {"text": "We managed to separate pronominal expressions in relation to their contribution at the different levels of anaphora resolution considered: clause level, utterance level, discourse level.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8100579977035522}]}, {"text": "At clause level, only those pronouns which must be bound locally are checked, as is the case with reflexive pronouns, possessives, some cases of expletive 'it': both arguments and adjuncts may contribute the appropriate antecedent.", "labels": [], "entities": []}, {"text": "At utterance level, in case the sentence is complex or there is more than one clause, also personal subject/object pronouns maybe bound (if only preferentially so).", "labels": [], "entities": []}, {"text": "Eventually, those pronouns which do not find an antecedent are regarded discourse level pronouns.", "labels": [], "entities": []}, {"text": "We collapsed under CLAUSE all pronouns bound at clause and utterance level; DISCOURSE contains only sentence external pronouns.", "labels": [], "entities": []}, {"text": "Expletives have been computed in a separate column.", "labels": [], "entities": [{"text": "Expletives", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8970239162445068}]}, {"text": "As can be noticed easily, the highest percentage of pronouns found is at Clause level: this is not however the best performance of the system, which on the contrary performs better at discourse level.", "labels": [], "entities": []}, {"text": "Expletives contribute by far the highest correct result.", "labels": [], "entities": [{"text": "correct result", "start_pos": 41, "end_pos": 55, "type": "METRIC", "confidence": 0.9712886810302734}]}, {"text": "We also found correctly 47 'there' expletives and 6 correctly classified pronominal 'there' which however have been left unbound.", "labels": [], "entities": []}, {"text": "The system also found 48 occurrences of deictic discourse bound \"this\" and \"that\", which corresponds to the full coverage.", "labels": [], "entities": []}, {"text": "Finally, nominal expressions: the History List (HL) has been incremented up to 2243 new entities.", "labels": [], "entities": [{"text": "History List (HL)", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8643003463745117}]}, {"text": "The system identified 2773 entities from the HL by matching their linguistic description.", "labels": [], "entities": []}, {"text": "The overall number of resolution actions taken by the Discourse Level algorithm is 1861: this includes both cases of nominal and pronominal expressions.", "labels": [], "entities": []}, {"text": "However, since only 366 can be pronouns, the remaining 1500 resolution actions have been carried out on nominal expressions present in the HL.", "labels": [], "entities": []}, {"text": "If we compare these results to the ones computed by GuiTAR, which assign semantic indices to NamedEntities disregarding their status of anaphora, we can see that the whole text is made up of 12731 NEs.", "labels": [], "entities": [{"text": "GuiTAR", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.8714818358421326}]}, {"text": "GuiTAR finds 1585 cases of identity relations between a NE and an antecedent.", "labels": [], "entities": [{"text": "GuiTAR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8547694087028503}]}, {"text": "However, GuiTAR introduces always new indices and creates local antecedent-referring expression chains rather than repeating the same index of the chain head.", "labels": [], "entities": []}, {"text": "In this way, it is difficult if not impossible to compute how many times the text corefers/cospecifies to the same referring expressions.", "labels": [], "entities": []}, {"text": "On the contrary, in our case, this can be easily computed by counting how many times the same semantic index is being repeated in a \"resolution\" or \"identity\" action of the anaphora resolution algorithm.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.7483178377151489}]}, {"text": "For instance, the Jury is coreferred/cospecified 12 times; Price Daniel also 12 times and soon.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Expletive \"it\" compared results  MARS  JavaRAP  GuiTAR  GETARUNS  Coverage  163 (86.2%)  188 (99.5%)  188 (99.5%)  171 (91%)  Accuracy 1  63 (33.3%)  73 (38.6%)  75 (39.7%)  87 (46 %)  Wrong Classification  44  163-44=119  189-44=145", "labels": [], "entities": [{"text": "MARS  JavaRAP  GuiTAR  GETARUNS  Coverage  163 (86.2%)  188 (99.5%)  188 (99.5%)  171 (91%)  Accuracy 1", "start_pos": 43, "end_pos": 146, "type": "METRIC", "confidence": 0.7456571809623552}, {"text": "Wrong Classification  44  163-44", "start_pos": 195, "end_pos": 227, "type": "DATASET", "confidence": 0.7996278703212738}]}, {"text": " Table 3. GETARUNS pronouns collapsed at structural level  CLAUSE  DISCOURSE  EXPLETIVES  TOTALS  Pronouns found  410  366  109  885  Correct  266  222  67  555  Errors made  144  144  42  330", "labels": [], "entities": [{"text": "GETARUNS pronouns", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5566013157367706}, {"text": "CLAUSE  DISCOURSE  EXPLETIVES  TOTALS", "start_pos": 59, "end_pos": 96, "type": "METRIC", "confidence": 0.7444040477275848}]}]}