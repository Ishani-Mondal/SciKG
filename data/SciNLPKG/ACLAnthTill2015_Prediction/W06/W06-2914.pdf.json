{"title": [{"text": "Word Distributions for Thematic Segmentation in a Support Vector Machine Approach", "labels": [], "entities": [{"text": "Thematic Segmentation", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.9037280678749084}]}], "abstractContent": [{"text": "We investigate the appropriateness of using a technique based on support vector machines for identifying thematic structure of text streams.", "labels": [], "entities": []}, {"text": "The thematic seg-mentation task is modeled as a binary-classification problem, where the different classes correspond to the presence or the absence of a thematic boundary.", "labels": [], "entities": []}, {"text": "Experiments are conducted with this approach by using features based on word distributions through text.", "labels": [], "entities": []}, {"text": "We provide empirical evidence that our approach is robust , by showing good performance on three different data sets.", "labels": [], "entities": []}, {"text": "In particular , substantial improvement is obtained over previously published results of word-distribution based systems when evaluation is done on a corpus of recorded and transcribed multi-party dialogs.", "labels": [], "entities": []}], "introductionContent": [{"text": ") distinguishes between \"local-level topics (of sentences, utterances and short discourse segments)\" and \"discourse topics (of more extended stretches of discourse)\".", "labels": [], "entities": []}, {"text": "1) points out that \"discourse-level topics are one of the most elusive and intractable notions in semantics\".", "labels": [], "entities": []}, {"text": "Despite this difficulty in giving a rigorous definition of discourse topic, the task of discourse/dialogue segmentation into thematic episodes can be described by invoking an \"intuitive notion of topic\".", "labels": [], "entities": []}, {"text": "Thematic segmentation also relates to several notions such as speaker's intention, topic flow and cohesion.", "labels": [], "entities": [{"text": "Thematic segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8407781422138214}]}, {"text": "In order to find out if thematic segment identification is a feasible task, previous state-of-the-art works appeal to experiments, in which several human subjects are asked to mark thematic segment boundaries based on their intuition and a minimal set of instructions.", "labels": [], "entities": [{"text": "thematic segment identification", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.8297562599182129}]}, {"text": "In this manner, previous studies, e.g. (, obtained a level of inter-annotator agreement that is statistically significant.", "labels": [], "entities": []}, {"text": "Automatic thematic segmentation (TS), i.e. the segmentation of a text stream into topically coherent segments, is an important component in applications dealing with large document collections such as information retrieval and document browsing.", "labels": [], "entities": [{"text": "Automatic thematic segmentation (TS)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.81654225786527}, {"text": "information retrieval", "start_pos": 201, "end_pos": 222, "type": "TASK", "confidence": 0.7748645842075348}]}, {"text": "Other tasks that could benefit from the thematic textual structure include anaphora resolution, automatic summarisation and discourse understanding.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7468509376049042}, {"text": "summarisation", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.892772376537323}, {"text": "discourse understanding", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.7142734378576279}]}, {"text": "The work presented here tackles the problem of TS by adopting a supervised learning approach for capturing linear document structure of nonoverlapping thematic episodes.", "labels": [], "entities": [{"text": "TS", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9941269159317017}]}, {"text": "A prerequisite for the input data to our system is that texts are divided into sentences or utterances.", "labels": [], "entities": []}, {"text": "Each boundary between two consecutive utterances is a potential thematic segmentation point and therefore, we model the TS task as a binary-classification problem, where each utterance should be classified as marking the presence or the absence of a topic shift in the discourse/dialogue based only on observations of patterns in vocabulary use.", "labels": [], "entities": [{"text": "TS task", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.933207243680954}]}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "The next section summarizes previous techniques, describes how our method relates to them and presents the motivations fora support vector approach.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 present our approach in adopting support vector learning for thematic segmentation.", "labels": [], "entities": []}, {"text": "Section 5 outlines the empirical methodology and describes the data used in this study.", "labels": [], "entities": []}, {"text": "Section 6 presents and discusses the evaluation results.", "labels": [], "entities": []}, {"text": "The paper closes with Section 7, which briefly summarizes this work and offers some conclusions and future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "(1999) underlined that the standard evaluation metrics of precision and recall are inadequate for thematic segmentation, namely by the fact that these metrics did not account for how faraway a hypothesized boundary (i.e. a boundary found by the automatic procedure) is from the reference boundary.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9992515444755554}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9983458518981934}]}, {"text": "On the other hand, for instance, an algorithm that places a boundary just one utterance away from the reference boundary should be penalized less than an algorithm that places a boundary ten (or more) utterances away from the reference boundary.", "labels": [], "entities": []}, {"text": "Hence the use of two other evaluation metrics is favored in thematic segmentation: the P k metric () and the WindowDiff error metric).", "labels": [], "entities": [{"text": "WindowDiff error metric", "start_pos": 109, "end_pos": 132, "type": "METRIC", "confidence": 0.5889539817969004}]}, {"text": "In con-   trast to precision and recall, these metrics allow fora slight vagueness in where the hypothesized thematic boundaries are placed and capture \"the notion of nearness in a principled way, gently penalizing algorithms that hypothesize boundaries that aren't quite right, and scaling down with the algorithm's degradation\").", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9985994696617126}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9936843514442444}]}, {"text": "That is, computing both P k and WindowDiff metrics involves the use of a fixed-size (i.e. having a fixed number of either words or utterances) window that is moved step by step over the data.", "labels": [], "entities": []}, {"text": "At each step, P k and WindowDiff are basically increased (each metric in a slightly different way) if the hypothesized boundaries and the reference boundaries are not within the same window.", "labels": [], "entities": []}, {"text": "During the model selection phase, we used precision and recall in order to measure the system's error rate.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9995250701904297}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9993112087249756}]}, {"text": "This was motivated by the fact that posing the TS task as a classification problem leads to a loss of the sequential nature of the data, which is an inconvenient in computing the P k and WindowDiff measures.", "labels": [], "entities": [{"text": "TS task", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.8461648523807526}]}, {"text": "However, during the final testing phase of our system, as well as for the evaluation of the previous systems, we use both the P k and the WindowDiff error metric.", "labels": [], "entities": [{"text": "WindowDiff error metric", "start_pos": 138, "end_pos": 161, "type": "METRIC", "confidence": 0.5875105758508047}]}, {"text": "The relatively small size of our datasets does not allow for dividing our test set into multiple sub-test sets for applying statistical significance tests.", "labels": [], "entities": []}, {"text": "This would be desirable in order to indicate whether the differences in system error rates are statistically significant over different data sets.", "labels": [], "entities": []}, {"text": "Nevertheless, we believe that measuring differences in error rates obtained on the test set is indicative of the relative performance.", "labels": [], "entities": []}, {"text": "Thus, the experimental results shown in this paper should be considered as illustrative rather than exhaustive.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The percentage of non-zero features per ex- ample.", "labels": [], "entities": []}, {"text": " Table 2: The optimal settings found for the SVM model, using the RBF kernel.", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.8154816925525665}]}]}