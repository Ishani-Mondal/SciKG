{"title": [{"text": "Unsupervised Discovery of a Statistical Verb Lexicon", "labels": [], "entities": [{"text": "Statistical Verb Lexicon", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.48813843727111816}]}], "abstractContent": [{"text": "This paper demonstrates how unsupervised techniques can be used to learn models of deep linguistic structure.", "labels": [], "entities": []}, {"text": "Determining the semantic roles of a verb's dependents is an important step in natural language understanding.", "labels": [], "entities": [{"text": "Determining the semantic roles of a verb's dependents", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.6750005980332693}, {"text": "natural language understanding", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6371847490469614}]}, {"text": "We present a method for learning models of verb argument patterns directly from unannotated text.", "labels": [], "entities": []}, {"text": "The learned models are similar to existing verb lexicons such as VerbNet and PropBank, but additionally include statistics about the linkings used by each verb.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9373248219490051}]}, {"text": "The method is based on a structured probabilistic model of the domain , and unsupervised learning is performed with the EM algorithm.", "labels": [], "entities": []}, {"text": "The learned models can also be used discriminatively as semantic role labelers, and when evaluated relative to the PropBank annotation , the best learned model reduces 28% of the error between an informed baseline and an oracle upper bound.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9066128134727478}]}], "introductionContent": [{"text": "An important source of ambiguity that must be resolved by any natural language understanding system is the mapping between syntactic dependents of a predicate and the semantic roles 1 that they each express.", "labels": [], "entities": []}, {"text": "The ambiguity stems from the fact that each predicate can allow several alternate mappings, or linkings, 2 between its semantic roles and their syntactic realization.", "labels": [], "entities": []}, {"text": "For example, the verb increase can be used in two ways: (1) The Fed increased interest rates.", "labels": [], "entities": []}, {"text": "(2) Interest rates increased yesterday.", "labels": [], "entities": []}, {"text": "The instances have apparently similar surface syntax: they both have a subject and a noun phrase directly following the verb.", "labels": [], "entities": []}, {"text": "However, while the subject of increase expresses the agent role in the first, it instead expresses the patient role in the second.", "labels": [], "entities": []}, {"text": "Pairs of linkings such as this allowed by a single predicate are often called diathesis alternations.", "labels": [], "entities": []}, {"text": "The current state-of-the-art approach to resolving this ambiguity is to use discriminative classifiers, trained on hand-tagged data, to classify the semantic role of each dependent.", "labels": [], "entities": []}, {"text": "A drawback of this approach is that even a relatively large training corpus exhibits considerable sparsity of evidence.", "labels": [], "entities": []}, {"text": "The two main handtagged corpora are PropBank ( and FrameNet (, the former of which currently has broader coverage.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9498627185821533}]}, {"text": "However, even PropBank, which is based on the 1M word WSJ section of the Penn Treebank, is insufficient in quantity and genre to exhibit many things.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9231841564178467}, {"text": "1M word WSJ section of the Penn Treebank", "start_pos": 46, "end_pos": 86, "type": "DATASET", "confidence": 0.7816671542823315}]}, {"text": "A perfectly common verb like flap occurs only twice, across all morphological forms.", "labels": [], "entities": []}, {"text": "The first example is an adjectival use (flapping wings), and the second is a rare intransitive use with an agent argument and a path (ducks flapping over Washington).", "labels": [], "entities": []}, {"text": "From this data, one cannot learn the basic alternation pattern for flap: the bird flapped its wings vs. the wings flapped.", "labels": [], "entities": [{"text": "flap", "start_pos": 67, "end_pos": 71, "type": "TASK", "confidence": 0.9445300102233887}]}, {"text": "We propose to address the challenge of data sparsity by learning models of verb behavior directly from raw unannotated text, of which there is plenty.", "labels": [], "entities": []}, {"text": "This has the added advantage of being easily extendible to novel text genres and languages, and the possibility of shedding light on the question of human language acquisition.", "labels": [], "entities": [{"text": "human language acquisition", "start_pos": 149, "end_pos": 175, "type": "TASK", "confidence": 0.6959305008252462}]}, {"text": "The models learned by our unsupervised approach provide anew broad-coverage lexical resource which gives statistics about verb behavior, information that may prove useful in other language processing tasks, such as parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 215, "end_pos": 222, "type": "TASK", "confidence": 0.9619725942611694}]}, {"text": "Moreover, they maybe used discriminatively to label novel verb instances for semantic role.", "labels": [], "entities": []}, {"text": "Thus we evaluate them both in terms of the verb alternations that they learn and their accuracy as semantic role labelers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9989680051803589}]}, {"text": "This work bears some similarity to the substantial literature on automatic subcategorization frame acquisition (see, e.g.,,).", "labels": [], "entities": [{"text": "subcategorization frame acquisition", "start_pos": 75, "end_pos": 110, "type": "TASK", "confidence": 0.63633926709493}]}, {"text": "However, that research is focused on acquiring verbs' syntactic behavior, and we are focused on the acquisition of verbs' linking behavior.", "labels": [], "entities": []}, {"text": "More relevant is the work of McCarthy and: The set of syntactic relations we use, where n \u2208 {1, 2, 3} and x is a preposition., which used a statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as Lapata (1999), which searched a large corpus to find evidence of two particular verb alternations.", "labels": [], "entities": []}, {"text": "There has also been some work on both clustering and supervised classification of verbs based on their alternation behavior).", "labels": [], "entities": [{"text": "clustering", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.960839569568634}]}, {"text": "Finally, perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.6305161913235983}]}, {"text": "However, we believe this is the first system to simultaneously discover verb roles and verb linking patterns from unsupervised data using a unified probabilistic model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our models with verb instances extracted from three parsed corpora: (1) the Wall Street Journal section of the Penn Treebank (PTB), which was parsed by human annotators, (2) the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text (BLLIP), which was parsed automatically by the Charniak parser, and (3) the Gigaword corpus of raw newswire text (GW), which we parsed ourselves with the Stanford parser.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank (PTB)", "start_pos": 85, "end_pos": 139, "type": "DATASET", "confidence": 0.9304033030163158}, {"text": "Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text (BLLIP)", "start_pos": 187, "end_pos": 284, "type": "DATASET", "confidence": 0.6524303376674652}, {"text": "Gigaword corpus of raw newswire text (GW)", "start_pos": 353, "end_pos": 394, "type": "DATASET", "confidence": 0.899895515706804}]}, {"text": "In all cases, when training a model,  we specify a set of target verb types (e.g., the ones in the test set), and build a training set by adding a fixed number of instances of each verb type from the PTB, BLLIP, and GW data sets, in that order.", "labels": [], "entities": [{"text": "PTB", "start_pos": 200, "end_pos": 203, "type": "DATASET", "confidence": 0.8192455172538757}, {"text": "BLLIP", "start_pos": 205, "end_pos": 210, "type": "METRIC", "confidence": 0.8350347280502319}, {"text": "GW data sets", "start_pos": 216, "end_pos": 228, "type": "DATASET", "confidence": 0.9182458321253458}]}, {"text": "For the semantic role labeling evaluation, we use our system to label the dependents of unseen verb instances for semantic role.", "labels": [], "entities": [{"text": "semantic role labeling evaluation", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.7145857512950897}]}, {"text": "We use the sentences in PTB section 23 for testing, and PTB section 24 for development.", "labels": [], "entities": [{"text": "PTB section 23", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9434179663658142}, {"text": "PTB section 24", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9285930196444193}]}, {"text": "The development set consists of 2507 verb instances and 833 different verb types, and the test set consists of 4269 verb instances and 1099 different verb types.", "labels": [], "entities": []}, {"text": "Free parameters were tuned on the development set, and the test set was only used for final experiments.", "labels": [], "entities": []}, {"text": "Because we do not observe the gold standard semantic roles at training time, we must choose an alignment between the guessed labels and the gold labels.", "labels": [], "entities": []}, {"text": "We do so optimistically, by choosing the gold label for each guessed label which maximizes the number of correct guesses.", "labels": [], "entities": []}, {"text": "This is a well known approach to evaluation in unsupervised learning: when it is used to compute accuracy, the resulting metric is sometimes called cluster purity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9963819980621338}]}, {"text": "While this amounts to \"peeking\" at the answers before evaluation, the amount of human knowledge that is given to the system is small: it corresponds to the effort required to hand assign a \"name\" to each label that the system proposes.", "labels": [], "entities": []}, {"text": "As is customary, we divide the problem into two subtasks: identification (ID) and classification (CL).", "labels": [], "entities": []}, {"text": "In the identification task, we identify the set of constituents which fill some role fora target verb: in our system we use simple rules to extract dependents of the target verb and their grammatical relations.", "labels": [], "entities": []}, {"text": "In the classification task, the identified constituents are labeled for their semantic role by the learned probabilistic model.", "labels": [], "entities": []}, {"text": "We report results on two variants of the basic classification task: coarse roles, in which all of the adjunct roles are collapsed to a single ARGM role (, and core roles, in which we evaluate performance on the core semantic roles only (thus collapsing the ARGM and unlabeled categories).", "labels": [], "entities": []}, {"text": "We do not report results on the all roles task, since our current model does not distinguish between different types of adjunct roles.", "labels": [], "entities": []}, {"text": "For each task we report precision, recall, and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9975034594535828}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9996726512908936}, {"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9997250437736511}]}], "tableCaptions": [{"text": " Table 3: Summary of results on labeling verb instances  in PropBank Section 23 and Section 24 for semantic role.  Learned results are averaged over 5 runs.", "labels": [], "entities": [{"text": "labeling verb instances", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.8568808237711588}]}]}