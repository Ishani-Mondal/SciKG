{"title": [{"text": "Lexical Reference: a Semantic Matching Subtask", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic lexical matching is a prominent subtask within text understanding applications.", "labels": [], "entities": [{"text": "Semantic lexical matching", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8213341434796652}, {"text": "text understanding", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.791401743888855}]}, {"text": "Yet, it is rarely evaluated in a direct manner.", "labels": [], "entities": []}, {"text": "This paper proposes a definition for lexical reference which captures the common goals of lexical matching.", "labels": [], "entities": []}, {"text": "Based on this definition we created and analyzed a test dataset that was utilized to directly evaluate, compare and improve lexical matching models.", "labels": [], "entities": []}, {"text": "We suggest that such decomposition of the global semantic matching task is critical in order to fully understand and improve individual components.", "labels": [], "entities": [{"text": "global semantic matching task", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7091977521777153}]}], "introductionContent": [{"text": "A fundamental task for text understanding applications is to identify semantically equivalent pieces of text.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8392927348613739}]}, {"text": "For example, Question Answering (QA) systems need to match corresponding parts in the question and in the answer passage, even though such parts maybe expressed in different terms.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8766794085502625}]}, {"text": "Summarization systems need to recognize (redundant) semantically matching parts in multiple sentences that are phrased differently.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.964383065700531}]}, {"text": "Other applications, such as information extraction and retrieval, face pretty much the same semantic matching task.", "labels": [], "entities": [{"text": "information extraction and retrieval", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.7384061962366104}, {"text": "semantic matching", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7247106432914734}]}, {"text": "The degree of semantic matching found is typically factored into systems' scoring and ranking mechanisms.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.6952511072158813}]}, {"text": "The recently proposed framework of textual entailment) attempts to formulate the generic semantic matching problem in an application independent manner.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7214415371417999}, {"text": "generic semantic matching", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.5993103981018066}]}, {"text": "The most commonly implemented semantic matching component addresses the lexical level.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7060989886522293}]}, {"text": "At this level the goal is to identify whether the meaning of a lexical item of one text is expressed also within the other text.", "labels": [], "entities": []}, {"text": "Typically, lexical matching models measure the degree of literal lexical overlap, augmented with lexical substitution criteria based on resources such as Wordnet or the output of statistical similarity methods (see Section 2).", "labels": [], "entities": [{"text": "lexical matching", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7128062844276428}, {"text": "Wordnet", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9842996597290039}]}, {"text": "Many systems apply semantic matching only at the lexical level, which is used to approximate the overall degree of semantic matching between texts.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7139309793710709}]}, {"text": "Other systems incorporate lexical matching as a component within more complex models that examine matching at higher syntactic and semantic levels.", "labels": [], "entities": []}, {"text": "While lexical matching models are so prominent within semantic systems they are rarely evaluated in a direct manner.", "labels": [], "entities": []}, {"text": "Typically, improvements to a lexical matching model are evaluated by their marginal contribution to overall system performance.", "labels": [], "entities": []}, {"text": "Yet, such global and indirect evaluation does not indicate the absolute performance of the model relative to the sheer lexical matching task for which it was designed.", "labels": [], "entities": []}, {"text": "Furthermore, the indirect application-dependent evaluation mode does not facilitate improving lexical matching models in an application dependent manner, and does not allow proper comparison of such models which were developed (and evaluated) by different researchers within different systems.", "labels": [], "entities": []}, {"text": "This paper proposes a generic definition for the lexical matching task, which we term lexical reference.", "labels": [], "entities": [{"text": "lexical matching task", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7834118803342184}]}, {"text": "This definition is application independent and enables annotating test datasets that evaluate directly lexical matching models.", "labels": [], "entities": []}, {"text": "Consequently, we created a dataset annotated for lexical reference, using a sample of sentence pairs (texthypothesis) from the 1st Recognising Textual Entailment dataset.", "labels": [], "entities": [{"text": "1st Recognising Textual Entailment dataset", "start_pos": 127, "end_pos": 169, "type": "DATASET", "confidence": 0.6364071249961853}]}, {"text": "Further analysis identified sev-eral sub-types of lexical reference, pointing at the many interesting cases where lexical reference is derived from a complete context rather than from a particular matching lexical item.", "labels": [], "entities": []}, {"text": "Next, we used the lexical reference dataset to evaluate and compare several state-of-the-art approaches for lexical matching.", "labels": [], "entities": [{"text": "lexical matching", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.725114107131958}]}, {"text": "Having a direct evaluation task enabled us to capture the actual performance level of these models, to reveal their relative strengths and weaknesses, and even to construct a simple combination of two models that outperforms all the original ones.", "labels": [], "entities": []}, {"text": "Overall, we suggest that it is essential to decompose global semantic matching and textual entailment tasks into proper subtasks, like lexical reference.", "labels": [], "entities": [{"text": "global semantic matching", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.6239983141422272}]}, {"text": "Such decomposition is needed in order to fully understand the behavior of individual system components and to guide their future improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created a lexical reference dataset derived from the RTE-1 development set by randomly choosing 400 out of the 567 text-hypothesis examples.", "labels": [], "entities": [{"text": "RTE-1 development set", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.9247950911521912}]}, {"text": "We then created sentence-word examples for all content words in the hypotheses which do not appear in the corresponding sentence and are not a morphological derivation of a word in it (since a simple morphologic module could easily identify these cases).", "labels": [], "entities": []}, {"text": "This resulted in a total of 708 lexical reference examples.", "labels": [], "entities": []}, {"text": "Two annotators annotated these examples as described in the next section.", "labels": [], "entities": []}, {"text": "Taking the same approach as of the RTE-1 dataset creation (), we limited our experiments to the resulting 580 examples that the two annotators agreed upon 3 .  Ina similar manner to (; Vanderwende et al., 2005) we investigated the relationship between lexical reference and textual entailment.", "labels": [], "entities": [{"text": "RTE-1 dataset creation", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.8397718469301859}]}, {"text": "We checked the performance of a textual entailment system which relies solely on an ideal lexical reference component which makes no mistakes and asserts that a hypothesis is entailed from a text if and only if all content words in the hypothesis are referred in the text.", "labels": [], "entities": []}, {"text": "Based on the lexical reference dataset annotations, such an \"ideal\" system would obtain an accuracy of 74% on the corresponding subset of the textual entailment task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9995328187942505}]}, {"text": "The corresponding precision is 68% and a recall of 82%.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9994495511054993}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9982178807258606}]}, {"text": "This is significantly higher than the results of the best performing systems that participated in the challenge on the RTE-1 test set.", "labels": [], "entities": [{"text": "RTE-1 test set", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9588214755058289}]}, {"text": "This suggests that lexical reference is a valuable subtask for entailment.", "labels": [], "entities": []}, {"text": "Interestingly, a similar entailment system based on a lexical reference component which doesn't account for the contextual lexical reference (i.e. all Context annotations are regarded as false) would achieve an accuracy of only 63% with 41% precision and a recall of 63%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.999427318572998}, {"text": "precision", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.9992750287055969}, {"text": "recall", "start_pos": 257, "end_pos": 263, "type": "METRIC", "confidence": 0.9997616410255432}]}, {"text": "This suggests that lexical reference in general and contextual entailment in particular, play an important (though not sufficient) role in entailment recognition.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.9553305804729462}]}, {"text": "Further, we wanted to investigate the validity of the assumption that for entailment relationship to holdall content words in the hypothesis must be referred by the text.", "labels": [], "entities": []}, {"text": "We examined the examples in our dataset which were derived from texthypothesis pairs that were annotated as true (entailing) in the RTE dataset.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9140458703041077}]}, {"text": "Out of 257 such examples only 34 were annotated as false by both annotators.", "labels": [], "entities": []}, {"text": "lists a few such examples in which entailment at whole holds, however, there exists a word in the hypothesis (highlighted in the table) which is not lexically referenced by the text.", "labels": [], "entities": []}, {"text": "In many cases, the target word was part of anon compositional compound in the hypothesis, and therefore should not be expected to be referenced by the text (see examples 1-2).", "labels": [], "entities": []}, {"text": "This finding indicates that the basic assumption is a reasonable approximation for entailment.", "labels": [], "entities": []}, {"text": "We could not have revealed this fact without the dataset for the subtask of lexical reference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: inter-annotator confusion matrix for the  auxiliary annotation.", "labels": [], "entities": []}, {"text": " Table 3: A sample from the lexical reference dataset along with the Bayesian model's score", "labels": [], "entities": []}, {"text": " Table 6: Breakdown of recall of correctly identi- fied example types at an overall system's recall of  25%. Disagreement refers to examples for which  the annotators did not agree on the subcategory an- notation (word vs. phrase/context).", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9101630449295044}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9985960125923157}]}]}