{"title": [{"text": "Robust Parsing: More with Less", "labels": [], "entities": [{"text": "Robust Parsing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8826250731945038}]}], "abstractContent": [{"text": "Covering as many phenomena as possible is a traditional goal of parser development, but the broader a grammar is made, the blunter it may become, as rare constructions influence the behaviour on simple sentences that were already solved correctly.", "labels": [], "entities": [{"text": "parser development", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8644971549510956}]}, {"text": "We observe the effects of intentionally removing support for specific constructions from a broad-coverage grammar of German.", "labels": [], "entities": []}, {"text": "We show that accuracy of analysing sentences from the NEGRA corpus can be improved not only for sentences that do not need the extra coverage, but even when including those that do.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.999271810054779}, {"text": "NEGRA corpus", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9417658448219299}]}], "introductionContent": [{"text": "Traditionally, broad coverage has always been considered to be a desirable property of a grammar: the more linguistic phenomena are treated properly by the grammar, the better results can be expected when applying it to unrestricted text (c.f.", "labels": [], "entities": []}, {"text": "With the advent of empirical methods and the corresponding evaluation metrics, however, this view changed considerably.) was among the first who noted that the relationship between coverage and statistical parsing quality is a more complex one.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.6613069772720337}]}, {"text": "Adding new rules to the grammar, i.e. increasing its coverage, does not only allow the parser to deal with more phenomena, hence more sentences; at the same time it opens up new possibilities for abusing the newly introduced rules to mis-analyse constructions which were already treated properly before.", "labels": [], "entities": []}, {"text": "As a consequence, a net reduction in parsing quality might be observed for simple statistical reasons, since the gain usually is obtained for relatively rare phenomena, while the adverse effects might well affect frequent ones.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9818770885467529}]}, {"text": "( uses this observation to argue in favour of stochastic models which attempt to choose the optimal structural interpretation instead of only providing a list of equally probable alternatives.", "labels": [], "entities": []}, {"text": "However, using such an optimization procedure is not necessarily a sufficient precondition to completely rule out the effect.", "labels": [], "entities": []}, {"text": "Compared to traditional handwritten grammars, successful stochastic models like) open up an even greater space of alternatives for the parser and accordingly offer a great deal of opportunities to construct odd structural descriptions from them.", "labels": [], "entities": []}, {"text": "Whether the guidance of the stochastic model can really prevent the parser from making use of these unwanted opportunities so far remains unclear.", "labels": [], "entities": []}, {"text": "In the following we make a first attempt to quantify the consequences that different degrees of coverage have for the output quality of a wide-coverage parser.", "labels": [], "entities": []}, {"text": "For this purpose we use a Weighted Constraint Dependency Grammar (WCDG), which covers even relatively rare syntactic phenomena of German and performs reliably across a wide variety of different text genres ().", "labels": [], "entities": []}, {"text": "By combining hand-written rules with an optimization procedure for hypothesis selection, such a parser makes it possible to successively exclude certain rare phenomena from the coverage of the grammar and to study the impact of these modifications on its output quality", "labels": [], "entities": []}], "datasetContent": [{"text": "In our first experiment, we analysed 10,000 sentences of online newscast texts both with the normal grammar and with the 21 rare phenomena explicitly excluded.", "labels": [], "entities": []}, {"text": "As usual for dependency parsers, we measure the parsing quality by computing the structural accuracy (the ratio of correct subordinations to all subordinations) and labelled accuracy (the ratio of all correct subordinations that also bear the correct label to all subordinations).", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.784587025642395}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.8684509992599487}, {"text": "labelled accuracy", "start_pos": 165, "end_pos": 182, "type": "METRIC", "confidence": 0.8221477568149567}]}, {"text": "Note that the WCDG parser always establishes exactly one subordination for each word of a sentence, so that no distinction between precision and recall arises.", "labels": [], "entities": [{"text": "WCDG parser", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.608937531709671}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9987466335296631}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9890040755271912}]}, {"text": "Also, the grammar is written in such away that even if a necessary phenomenon is removed, the parser will at least find some analysis, so that the coverage is always 100%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.966132402420044}]}, {"text": "As expected, those 'rare' sentences in which at least one of these constructions does actually occur are analyzed less accurately than before: structural and labelled accuracy drop by about 2 percent points (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9729388952255249}]}, {"text": "However, the other sentences receive slightly better analyses, and since they are in the great majority, the overall effect is an increase in parsing quality.", "labels": [], "entities": [{"text": "parsing", "start_pos": 142, "end_pos": 149, "type": "TASK", "confidence": 0.9579591751098633}]}, {"text": "Note also that the 'rare' sentences appear to be more difficult to analyze in the first place.", "labels": [], "entities": []}, {"text": "Grammar: Normal Reduced Online newscasts rare 87.6%/85.2% 85.8%/85.8% normal 91.0%/89.8% 91.4%/90.4% overall (10,000) 91.0%/89.4% 91.3%/89.7% NEGRA corpus rare 85.5%/83.7% 84.0%/81.4% normal 91.2%/89.3% 91.5%/89.7% overall 90.5%/88.6% 90.6%/88.7%: Structural and labelled accuracy when parsing the same text with reduced coverage.", "labels": [], "entities": [{"text": "NEGRA corpus rare 85.5", "start_pos": 142, "end_pos": 164, "type": "DATASET", "confidence": 0.9213301837444305}, {"text": "accuracy", "start_pos": 272, "end_pos": 280, "type": "METRIC", "confidence": 0.9700351357460022}]}, {"text": "The net gain inaccuracy might be due to plugged leaks (misleading structures that used to be found are rejected in favor of correct structures) or to focussing (structures that were preferred but missed through search errors are now found).", "labels": [], "entities": [{"text": "focussing", "start_pos": 150, "end_pos": 159, "type": "TASK", "confidence": 0.9788105487823486}]}, {"text": "A point in case of the latter explanation is the fact that the average runtime decreases by 10% with the reduced grammar.", "labels": [], "entities": []}, {"text": "Also, if we consider only those sentences on which the local search originally exceeded the time limit of 500 sand therefore had to be interrupted, the accuracy rises from 85.2%/83.0% to 86.5%/84.4%, i.e. even more pronounced than overall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9996048808097839}]}, {"text": "For comparison with previous work and to investigate corpus-specific effects, we repeated the experiment with the test set of the NEGRA corpus as defined by.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 130, "end_pos": 142, "type": "DATASET", "confidence": 0.9468067586421967}]}, {"text": "For that purpose the NE-GRA annotations were automatically transformed to dependency trees with the freely available tool DEPSY ().", "labels": [], "entities": [{"text": "DEPSY", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.5940508246421814}]}, {"text": "Some manual corrections were made to its output to conform to the annotation guidelines of the WCDG of German; altogether, 1% of all words had their regents changed for this purpose.", "labels": [], "entities": []}, {"text": "shows that the proportion of sentences with rare phenomena is somewhat higher in the NEGRA sentences, and consequently the net gain in parsing accuracy is smaller; apparently the advantage of reducing the problem size is almost cancelled by the disadvantage of losing necessary coverage.", "labels": [], "entities": [{"text": "NEGRA sentences", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9221735298633575}, {"text": "parsing", "start_pos": 135, "end_pos": 142, "type": "TASK", "confidence": 0.9774386286735535}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9507894515991211}]}, {"text": "To test this theory, we then reduced the coverage of the grammar in smaller steps.", "labels": [], "entities": []}, {"text": "Since constraints allow us to switch off each of the 21 rare phenomena individually, we can test whether the effects of reducing coverage are merely due to the smaller number of alternatives to consider or whether some constructions affect the parser more than others, if allowed.", "labels": [], "entities": []}, {"text": "We first took the first 3,000 sentences of the NEGRA corpus as a training set and counted how often each construction actually occurs there and in the test set.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.883303314447403}]}, {"text": "shows that the two parts of the corpus, while different, seem similar enough that statistics obtained  on the one could be useful for processing the other.", "labels": [], "entities": []}, {"text": "The test set was then parsed again with the coverage successively reduced in several steps: first, all constructions were removed that never occur in the training set, then those which occur less than 10 times or 100 times respectively were also removed.", "labels": [], "entities": []}, {"text": "We also performed the opposite experiment, first removing support for the least rare phenomena and only then for the really rare ones.", "labels": [], "entities": []}, {"text": "Phenomena structural labelled removed accuracy accuracy none 90.5% 88.6% = 0 90.5% 88.7% < 10 90.6% 88.8% < 100 90.7% 88.6% >= 100 90.5% 88.6% >= 10 90.4% 88.5% > 0 90.5% 88.6% all 90.6% 88.7%: Parsing with coverage reduced stepwise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9796027541160583}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.7835912704467773}, {"text": "Parsing", "start_pos": 194, "end_pos": 201, "type": "TASK", "confidence": 0.9342913627624512}, {"text": "coverage", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9869137406349182}]}, {"text": "shows the results of parsing the test set in this way (the first and last lines are repetitions from Table 3).", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9806188941001892}]}, {"text": "The resulting effects are very small, but they do suggest that removing coverage for the very rare constructions is somewhat more profitable: the first three new experiments tend to yield better accuracy than the original grammar, while in the last three it tends to drop.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9976029992103577}]}, {"text": "The previous experiment used only counts from the treebank annotations to determine how rare a phenomenon is supposed to be, but it might also be important how rare the parser actually assumes it to be.", "labels": [], "entities": []}, {"text": "The fact that a particular construction never occurs in a corpus does not prevent the parser from using it in its analyses, perhaps more often than another construction that is much more common in the annotations.", "labels": [], "entities": []}, {"text": "In other words, we should measure how much each construction actually leaks.", "labels": [], "entities": []}, {"text": "To this end, we parsed the training set with the original grammar and grouped all 21 phenomena into three classes: A: Phenomena that are predicted much more often than they are annotated B: Phenomena that are predicted roughly the right number of times C: Phenomena that are predicted less often than annotated (or in fact not at all).", "labels": [], "entities": []}, {"text": "'Much more often' here means 'by a factor of two or more'; constructions which were never predicted or annotated at all were grouped into class C.", "labels": [], "entities": []}, {"text": "There are different reasons why a phenomenon might leak more or less.", "labels": [], "entities": []}, {"text": "Some constructions depend on particular combinations of word forms in the input; for instance, an auxiliary flip can only be predicted when the finite verb does in fact precede the full verb (phenomenon 12 in), so that covering it should not change the behaviour of the system much.", "labels": [], "entities": []}, {"text": "But most sentences contain more than one noun phrase which the parser might possibly misrepresent as a non-projective extraposition (phenomenon 1).", "labels": [], "entities": []}, {"text": "Also, some rare phenomena are dispreferred more than others even when they are allowed.", "labels": [], "entities": []}, {"text": "We did not investigate these reasons in detail.", "labels": [], "entities": []}, {"text": "Phenomena structural labelled removed accuracy accuracy none 90.5% 88.6% A (1,3,4,6-10,13,16,18-21) 90.9% 89.0% B 90.4% 88.5% C 90.4% 88.6% 1-21 90.6% 88.7%: Parsing with coverage reduced by increasing leakage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9590272903442383}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.8531333804130554}, {"text": "A", "start_pos": 73, "end_pos": 74, "type": "METRIC", "confidence": 0.9427019953727722}, {"text": "Parsing", "start_pos": 158, "end_pos": 165, "type": "TASK", "confidence": 0.8548967242240906}, {"text": "coverage", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.994909942150116}]}, {"text": "shows an interesting asymmetry: of our 21 constructions, 14 regularly leak into sentences where they have no place, while 4 work more or less as designed.", "labels": [], "entities": []}, {"text": "Only 3 are predicted too seldom.", "labels": [], "entities": []}, {"text": "This is consistent with our earlier interpretation that most added coverage is in fact unhelpful when judging a parser solely by its empirical accuracy on a corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9585710763931274}]}, {"text": "Accordingly, it is in fact more helpful to judge constructions by their observed tendency to leak than just by their annotated frequency: the first experiment (A) yields the highest accuracy for the newspaper text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9987689852714539}]}, {"text": "Conversely, removing those constructions which actually work largely as intended (B) reduces even the overall accuracy, and not just the accuracy on 'rare' sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9993271827697754}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9988629817962646}]}, {"text": "The third class contains only three very rare phenomena, and removing them from the grammar does not influence parsing very much at all.", "labels": [], "entities": []}, {"text": "Note that this result was obtained although the distribution of the phenomena differs between parser predictions on the training set and the test set; had we classified them according to their behaviour on the test set itself, the class A would have contained only 9 items (of which 7 overlap with the classification actually used).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Frequency of phenomena by text type.", "labels": [], "entities": []}, {"text": " Table 3: Structural and labelled accuracy when parsing  the same text with reduced coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9742077589035034}, {"text": "parsing  the same text", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.8516292124986649}, {"text": "coverage", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9655616283416748}]}, {"text": " Table 4: Comparison of training and test set.", "labels": [], "entities": []}]}