{"title": [{"text": "Random Indexing using Statistical Weight Functions", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6980980634689331}]}], "abstractContent": [{"text": "Random Indexing is a vector space technique that provides an efficient and scal-able approximation to distributional similarity problems.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6886018961668015}]}, {"text": "We present experiments showing Random Indexing to be poor at handling large volumes of data and evaluate the use of weighting functions for improving the performance of Random Indexing.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.709613025188446}, {"text": "Random Indexing", "start_pos": 169, "end_pos": 184, "type": "TASK", "confidence": 0.6884917318820953}]}, {"text": "We find that Random Index is robust for small data sets, but performance degrades because of the influence high frequency attributes in large data sets.", "labels": [], "entities": [{"text": "Random Index", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.7992856502532959}]}, {"text": "The use of appropriate weight functions improves this significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Synonymy relations between words have been used to inform many Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "While these relations can be extracted from manually created resources such as thesauri (e.g. Roget's Thesaurus) and lexical databases (e.g., it is often beneficial to extract these relationships from a corpus representative of the task.", "labels": [], "entities": []}, {"text": "Manually created resources are expensive and time-consuming to create, and tend to suffer from problems of bias, inconsistency, and limited coverage.", "labels": [], "entities": []}, {"text": "These problems may result in an inappropriate vocabulary, where some terms are not present or an unbalanced set of synonyms.", "labels": [], "entities": []}, {"text": "Ina medical context it is more likely that administration will refer to the giving of medicine than to paperwork, whereas in a business context the converse is more likely.", "labels": [], "entities": []}, {"text": "The most common method for automatically creating these resources uses distributional similarity and is based on the distributional hypothesis that similar words appear in similar contexts.", "labels": [], "entities": []}, {"text": "Terms are described by collating information about their occurrence in a corpus into vectors.", "labels": [], "entities": []}, {"text": "These context vectors are then compared for similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9749318361282349}]}, {"text": "Existing approaches differ primarily in their definition of context, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the context vectors representing each term.", "labels": [], "entities": []}, {"text": "In this paper, we analyse the use of Random Indexing () for semantic similarity measurement.", "labels": [], "entities": [{"text": "semantic similarity measurement", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.8493275046348572}]}, {"text": "Random Indexing is an approximation technique proposed as an alternative to Latent Semantic Analysis).", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6824983209371567}, {"text": "Latent Semantic Analysis", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.5747401018937429}]}, {"text": "Random Indexing is more scalable and allows for the incremental learning of context information.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6408499628305435}]}, {"text": "found that dramatically increasing the volume of raw input data for distributional similarity tasks increases the accuracy of synonyms extracted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9989011287689209}]}, {"text": "Random Indexing performs poorly on these volumes of data.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.755458652973175}]}, {"text": "Noting that in many NLP tasks, including distributional similarity, statistical weighting is used to improve performance, we modify the Random Indexing algorithm to allow for weighted contexts.", "labels": [], "entities": []}, {"text": "We test the performance of the original and our modified system using existing evaluation metrics.", "labels": [], "entities": []}, {"text": "We further evaluate against bilingual lexicon extraction using distributional similarity).", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6277698377768198}]}, {"text": "The paper concludes with a more detailed analysis of Random Indexing in terms of both task and corpus composition.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.8100208640098572}]}, {"text": "We find that Random Index is robust for small corpora, but larger corpora require that the contexts be weighted to maintain accuracy.", "labels": [], "entities": [{"text": "Random Index", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.881966769695282}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9969937801361084}]}], "datasetContent": [{"text": "For the experiments extracting synonymy relations, high quality contexts were extracted from the non-speech portion of the British National Corpus (BNC) as described above.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 123, "end_pos": 152, "type": "DATASET", "confidence": 0.970777690410614}]}, {"text": "This represents 90% of the BNC, or 90 million words.", "labels": [], "entities": [{"text": "BNC", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.9003036618232727}]}, {"text": "Comparisons between low frequency terms are less accurate than between high frequency terms as there is less evidence describing them).", "labels": [], "entities": []}, {"text": "This is compounded in randomised vector techniques because the randomised nature of the representation means that a low frequency term may have a similar context vector to a high frequency term while not sharing many contexts.", "labels": [], "entities": []}, {"text": "A frequency cut-off of 100 was found to balance this inaccuracy with the reduction in vocabulary size.", "labels": [], "entities": []}, {"text": "This reduces the original 246,046 word vocabulary to 14,862 words.", "labels": [], "entities": []}, {"text": "Experiments showed d = 1000 and \ud97b\udf59 = 10 to provide a balance between speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9969819188117981}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9974272847175598}]}, {"text": "Low quality contexts were extracted from portions of the entire of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.9423226714134216}]}, {"text": "These formed corpora of 100,000, 500,000, 1 million, 5 million, 10 million, 50 million and 100 million words, chosen from random documents.", "labels": [], "entities": []}, {"text": "This allowed us test the effect of both corpus size and context quality.", "labels": [], "entities": []}, {"text": "This produced vocabularies of between 10,380 and 522,163 words in size.", "labels": [], "entities": []}, {"text": "Because of the size of the smallest corpora meant that a high cutoff would remove to many terms fora fair test, a cutoff of 5 was applied.", "labels": [], "entities": []}, {"text": "The values d = 1000 and \ud97b\udf59 = 6 were used.", "labels": [], "entities": []}, {"text": "For our experiments in bilingual lexicon acquisition we follow.", "labels": [], "entities": [{"text": "bilingual lexicon acquisition", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7083160281181335}]}, {"text": "We use the Spanish-Swedish and the EnglishGerman portions of the Europarl corpora ().", "labels": [], "entities": [{"text": "Europarl corpora", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9740716218948364}]}, {"text": "1 These consist of 37,379 aligned paragraphs in Spanish-Swedish and 45,556 in EnglishGerman.", "labels": [], "entities": []}, {"text": "The text was lemmatised using Connexor Machinese producing vocabularies of 42,671 terms of Spanish, 100,891 terms of Swedish, 40,181 terms of English and 70,384 terms of German.", "labels": [], "entities": []}, {"text": "We used = 600 and \ud97b\udf59 = 6 and apply a frequency cutoff of 100.", "labels": [], "entities": []}, {"text": "The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold standard.", "labels": [], "entities": []}, {"text": "To reduce the problem of limited coverage, our evaluation of the extraction of synonyms combines three electronic thesauri: the Macquarie, Roget's and Moby thesauri.", "labels": [], "entities": [{"text": "extraction of synonyms", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8051461776097616}, {"text": "Macquarie, Roget's and Moby thesauri", "start_pos": 128, "end_pos": 164, "type": "DATASET", "confidence": 0.8542760951178414}]}, {"text": "We follow and use two performance measures: direct matches (DIRECT) and inverse rank (INVR).", "labels": [], "entities": [{"text": "inverse rank (INVR)", "start_pos": 72, "end_pos": 91, "type": "METRIC", "confidence": 0.9172580361366272}]}, {"text": "DIRECT is the number of returned synonyms found in the gold standard.", "labels": [], "entities": [{"text": "DIRECT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9493550062179565}]}, {"text": "INVR is the sum of the inverse rank of each matching synonym, e.g. matches at ranks 3, 5 and 28 give an inverse rank score of 1 3 + 1 5 + 1 28 . With at most 100 matching synonyms, the maximum INVR is 5.187.", "labels": [], "entities": [{"text": "INVR", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9090912938117981}, {"text": "INVR", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9944889545440674}]}, {"text": "This more fine grained as it incorporates the both the number of matches and their ranking.", "labels": [], "entities": []}, {"text": "The same 300 single word nouns were used for evaluation as used by   onyms.", "labels": [], "entities": []}, {"text": "For each of these terms, the closest 100 terms and their similarity scores were extracted.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 57, "end_pos": 74, "type": "METRIC", "confidence": 0.9563612937927246}]}, {"text": "For the evaluation of bilingual lexicon acquisition we use two online lexical resources used by as gold standards: Lexin's online Swedish-Spanish lexicon and TU Chemnitz' online English-German dictionary.", "labels": [], "entities": [{"text": "bilingual lexicon acquisition", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6528221567471822}, {"text": "Lexin's online Swedish-Spanish lexicon", "start_pos": 115, "end_pos": 153, "type": "DATASET", "confidence": 0.8417072892189026}]}, {"text": "Each of the elements in a compound or multi-word expression is treated as a potential translation.", "labels": [], "entities": []}, {"text": "The German abblendlicht (low beam light) is treated as a translation candidate for low, beam and light separately.", "labels": [], "entities": []}, {"text": "Low coverage is more of problem than in our thesaurus task as we have not used combined resources.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9916799664497375}]}, {"text": "There are an average of 19 translations for each of the 3,403 Spanish terms and 197 translations for each of the 4,468 English terms.", "labels": [], "entities": []}, {"text": "The English-German translation count is skewed by the presence of connectives in multi-word expressions, such as of and on, producing mistranslations.", "labels": [], "entities": [{"text": "count", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.8996995687484741}]}, {"text": "provide good commentary on the evaluation of this task.", "labels": [], "entities": []}, {"text": "Spanish and English are used as the source languages.", "labels": [], "entities": []}, {"text": "The 200 closest terms in the target language are found for all terms in both the source vocabulary and the gold-standards.", "labels": [], "entities": []}, {"text": "We measure the DIRECT score and INVR as above.", "labels": [], "entities": [{"text": "DIRECT score", "start_pos": 15, "end_pos": 27, "type": "METRIC", "confidence": 0.9480577707290649}, {"text": "INVR", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9943892359733582}]}, {"text": "In addition we measure the precision of the closest translation candidate, as used in: Evaluation of bilingual lexicon extraction There was a large variance in the effectiveness of the other weights and most proved to be detrimental to Random Indexing.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9987812638282776}, {"text": "bilingual lexicon extraction", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.6383388737837473}, {"text": "Random Indexing", "start_pos": 236, "end_pos": 251, "type": "TASK", "confidence": 0.7908909022808075}]}, {"text": "TF-IDF was the worst, reducing the DIRECT score to 0.30 and the INVR to 0.07.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8778128623962402}, {"text": "DIRECT score", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.9549426138401031}, {"text": "INVR", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.7451833486557007}]}, {"text": "TF-IDF \u2020, which is a log-weighted alternative to TF-IDF, produced very good results.", "labels": [], "entities": [{"text": "TF-IDF \u2020", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.5495642125606537}]}], "tableCaptions": [{"text": " Table 4: Evaluation of bilingual lexicon extraction", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of Random Indexing using a  very large corpus", "labels": [], "entities": [{"text": "Evaluation of Random Indexing", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.593585193157196}]}]}