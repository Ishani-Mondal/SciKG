{"title": [{"text": "User-Centered Evaluation of Interactive Question Answering Systems", "labels": [], "entities": [{"text": "User-Centered Evaluation of Interactive Question Answering", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.5281249980131785}]}], "abstractContent": [{"text": "We describe a large-scale evaluation of four interactive question answering system with real users.", "labels": [], "entities": [{"text": "four interactive question answering", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.6961592435836792}]}, {"text": "The purpose of the evaluation was to develop evaluation methods and metrics for interactive QA systems.", "labels": [], "entities": []}, {"text": "We present our evaluation method as a case study, and discuss the design and administration of the evaluation components and the effectiveness of several evaluation techniques with respect to their validity and discriminatory power.", "labels": [], "entities": [{"text": "validity", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9545239806175232}]}, {"text": "Our goal is to provide a roadmap to others for conducting evaluations of their own systems, and to put forward a research agenda for interactive QA evaluation.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 145, "end_pos": 158, "type": "TASK", "confidence": 0.9010785818099976}]}], "introductionContent": [{"text": "There is substantial literature on the evaluation of systems in the context of real users and/or realistic problems.", "labels": [], "entities": []}, {"text": "The overall design issues were presented by in a classic paper.", "labels": [], "entities": []}, {"text": "Other authors who have contributed substantially to the discussion include.", "labels": [], "entities": []}, {"text": "The basic change in viewpoint required, in the study of interactive systems with real users, is that one cannot follow the Cranfield Model, in which specific items (whether documents, or snippets of information) are known to be \"good,\" so that measures can be based on the count of such items (e.g., precision and recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 300, "end_pos": 309, "type": "METRIC", "confidence": 0.9994062185287476}, {"text": "recall", "start_pos": 314, "end_pos": 320, "type": "METRIC", "confidence": 0.9874399900436401}]}, {"text": "Instead, one must develop methods and metrics that are sensitive to individual users, tasks and contexts, and robust enough to allow for valid and reliable comparisons across systems.", "labels": [], "entities": []}, {"text": "Most evaluations of QA systems have been conducted as part of the QA Track at TREC.", "labels": [], "entities": [{"text": "TREC", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.5276949405670166}]}, {"text": "They are system-oriented rather than user-oriented, with a focus on evaluating techniques for answer extraction, rather than interaction and use.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.8671523332595825}]}, {"text": "In this paper, we consider an interactive system to be a system that supports at least one exchange between the user and system.", "labels": [], "entities": []}, {"text": "Further, an interactive system is a system that allows the user full or partial control over content and action.", "labels": [], "entities": []}, {"text": "While factoid QA plays a role in analytical QA, analytical QA also includes other activities, such as comparison and synthesis, and demands much richer interactions between the system, the information, and the user.", "labels": [], "entities": [{"text": "comparison and synthesis", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.7262851794560751}]}, {"text": "Thus different evaluation measures are needed for analytical QA systems than for those supporting factoid QA.", "labels": [], "entities": []}, {"text": "Emerging work in the QA community is addressing user interaction with factoid-based QA systems and other more complex QA tasks), but developing robust evaluation methods and metrics for interactive, analytical QA systems in realistic settings with target users and tasks remains an unresolved research problem.", "labels": [], "entities": []}, {"text": "We describe a large-scale evaluation of four interactive QA systems with target users, completing target tasks.", "labels": [], "entities": []}, {"text": "Here we present the evaluation method and design decisions for each aspect of the study as a case study.", "labels": [], "entities": []}, {"text": "The goal of this paper is to identify key issues in the design of evaluations of interactive QA systems and help others construct their own evaluations.", "labels": [], "entities": []}, {"text": "While systems participating in this evaluation received individual feedback about the performances of their systems, the purpose of the project was not to compare a series of systems and declare a 'winner.'", "labels": [], "entities": []}, {"text": "In this paper we focus on the method and results of that method, rather than the performance of anyone system.", "labels": [], "entities": []}, {"text": "In section 2, we describe our evaluation approach, the evaluation environment, systems studied, subjects, corpus and scenarios, and experimental design.", "labels": [], "entities": []}, {"text": "In Section 3 we report our instruments and other data collection techniques.", "labels": [], "entities": []}, {"text": "In Section 4 we discuss our evaluation methods, and present key findings regarding the effectiveness of the various evaluation techniques.", "labels": [], "entities": []}, {"text": "We conclude by considering future research directions for interactive QA evaluation.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.8948554396629333}]}], "datasetContent": [{"text": "This evaluation was conducted as a two-week workshop.", "labels": [], "entities": []}, {"text": "The workshop mode gives analysts an opportunity to fully interact with all four systems, complete time-intensive tasks similar to their normal work tasks and lets us evaluate a range of methods and metrics.", "labels": [], "entities": []}, {"text": "The researchers spent approximately 3 weeks onsite preparing and administering the workshop.", "labels": [], "entities": []}, {"text": "Intelligence analysts, the study participants, spent 2 weeks onsite.", "labels": [], "entities": []}, {"text": "The evaluation employed 8 analysts, 8 scenarios in the chemical/biological WMD domain, and 4 systems -3 QA systems and a Google 1 baseline system.", "labels": [], "entities": []}, {"text": "Each analyst used each system to analyze 2 scenarios and wrote a pseudoreport containing enough structure and content for it to be judged by peer analysts.", "labels": [], "entities": []}, {"text": "During the planning stage, we generated hypotheses about interactive QA systems to guide development of methods and metrics for measuring system effectiveness.", "labels": [], "entities": []}, {"text": "Fifteen hypotheses were selected, of which 13 were operationalized.", "labels": [], "entities": []}, {"text": "Example hypotheses are presented in A good interactive QA system should \u2026 1 Support information gathering with lower cognitive workload 2 Assist analysts in exploring more paths/hypotheses 3 Enable analysts to produce higher quality reports 4 Provide useful suggestions to the analyst 5 Provide analysts with more good surprises than bad: Example hypotheses  The experiment was done at the Pacific Northwest National Laboratory (PNNL) in Richland, WA.", "labels": [], "entities": [{"text": "information gathering", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7451531887054443}]}, {"text": "We used one room with support servers, four rooms with two copies of one system in each and a conference room seating 20, for general meetings, focus group discussions, meetings among observers, meetings among developers, etc.", "labels": [], "entities": []}, {"text": "The evaluation workshop included four, two-day blocks.", "labels": [], "entities": []}, {"text": "In each block, a pair of analysts was assigned to each room, and a single observer was assigned to the pair of analysts.", "labels": [], "entities": []}, {"text": "Analysts used the two machines in each room to work independently during the block.", "labels": [], "entities": []}, {"text": "After each block, analysts and observers rotated to different system rooms, so that analysts were paired together only once and observers observed different analysts during each block.", "labels": [], "entities": []}, {"text": "The goal in using designed experiments is to minimize the second-order interactions, so that estimates of the main effects can be obtained from a much smaller set of observations than is required fora full factorial design.", "labels": [], "entities": []}, {"text": "For instance, one might imagine potential interaction effects of system and scenario (some systems might be better for certain scenarios); system and analysts (some analysts might adapt more quickly to a system); and analyst and scenario (some analysts might be more expert for certain scenarios).", "labels": [], "entities": []}, {"text": "To control these potential interactions, we used a modified Greco-Latin 4x4 design.", "labels": [], "entities": []}, {"text": "This design ensured that each analyst was observed by each of the four observers, and used each of the four systems.", "labels": [], "entities": []}, {"text": "This design also ensured that each system was, for some analyst, the first, second, third or last to be encountered, and that no analyst did the same pair of scenarios twice.", "labels": [], "entities": []}, {"text": "Analyst pairings were unique across blocks.", "labels": [], "entities": [{"text": "Analyst pairings", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8474811911582947}]}, {"text": "Following standard practice, analysts and scenarios were ran-domly assigned codenames (e.g. A1, and Scenario A), and systems were randomly assigned to the rows of.", "labels": [], "entities": [{"text": "A1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9874274730682373}]}, {"text": "Although observers were simply rotated across the system rows, the assignment of human individuals to code number was random.", "labels": [], "entities": []}, {"text": "The last component of each block was Cross Evaluation).", "labels": [], "entities": [{"text": "Cross Evaluation", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7601818144321442}]}, {"text": "Each analyst reviewed (using a paper copy) all seven reports prepared for each scenario in the block (14 total reports).", "labels": [], "entities": []}, {"text": "Analysts used an online tool to rate each report according to 7 criteria using 5-point scales.", "labels": [], "entities": []}, {"text": "After analysts completed independent ratings of each report according to the 7 criteria, they were asked to sort the stack of reports into rank order, placing the best report at the top of the pile.", "labels": [], "entities": []}, {"text": "Analysts were then asked to use a pen to write the appropriate rank number at the top of each report, and to use an online tool to enter their report rankings.", "labels": [], "entities": []}, {"text": "The criteria that the analysts used for evaluating reports were: (1) covers the important ground; (2) avoids the irrelevant materials; (3) avoids redundant information; (4) includes selective information; (5) is well organized; (6) reads clearly and easily; and (7) overall rating.", "labels": [], "entities": []}, {"text": "After the Cross Evaluation, focus groups of four analysts were formed to discuss the results of the Cross Evaluation.", "labels": [], "entities": [{"text": "Cross Evaluation", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.7455445826053619}, {"text": "Cross Evaluation", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.738684892654419}]}, {"text": "These focus groups had two purposes: to develop a consensus ranking of the seven reports for each scenario, and to elicit the aspects, or dimensions, which led each analyst to rank a report high or low in overall quality.", "labels": [], "entities": []}, {"text": "These discussions were taped and an observer took notes during the discussion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Most effective methods for gathering  data about example hypotheses (see", "labels": [], "entities": []}]}