{"title": [], "abstractContent": [{"text": "This paper introduces a semi-supervised learning framework for creating training material, namely active annotation.", "labels": [], "entities": []}, {"text": "The main intuition is that an unsupervised method is used to initially annotate imperfectly the data and then the errors made are detected automatically and corrected by a human annotator.", "labels": [], "entities": []}, {"text": "We applied active annotation to named entity recognition in the biomedical domain and encouraging results were obtained.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6224312782287598}]}, {"text": "The main advantages over the popular active learning framework are that no seed annotated data is needed and that the reusability of the data is maintained.", "labels": [], "entities": []}, {"text": "In addition to the framework, an efficient uncertainty estimation for Hidden Markov Models is presented .", "labels": [], "entities": []}], "introductionContent": [{"text": "Training material is always an issue when applying machine learning to deal with information extraction tasks.", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.8234806259473165}]}, {"text": "It is generally accepted that increasing the amount of training data used improves performance.", "labels": [], "entities": []}, {"text": "However, training material comes at a cost, since it requires annotation.", "labels": [], "entities": []}, {"text": "As a consequence, when adapting existing methods and techniques to anew domain, researchers and users are faced with the problem of absence of annotated material that could be used for training.", "labels": [], "entities": []}, {"text": "A good example is the biomedical domain, which has attracted the attention of the NLP community relatively recently).", "labels": [], "entities": []}, {"text": "Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.9573367834091187}]}, {"text": "Avery popular and well investigated framework in order to cope with the lack of training material is the active learning framework (.", "labels": [], "entities": []}, {"text": "It has been applied to various NLP/IE tasks, including named entity recognition) and parse selection () with rather impressive results in reducing the amount of annotated training data.", "labels": [], "entities": [{"text": "NLP/IE tasks", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.5812200382351875}, {"text": "named entity recognition", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6420562366644541}, {"text": "parse selection", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.9789364039897919}]}, {"text": "However, some criticism of active learning has been expressed recently, concerning the reusability of the data ().", "labels": [], "entities": []}, {"text": "This paper presents a framework in order to deal with the lack of training data for NLP tasks.", "labels": [], "entities": []}, {"text": "The intuition behind it is that annotated training data is produced by applying an (imperfect) unsupervised method, and then the errors inserted in the annotation are detected automatically and reannotated by a human annotator.", "labels": [], "entities": []}, {"text": "The main difference compared to active learning is that instead of selecting unlabeled instances for annotation, possible erroneous instances are selected for checking and correction if they are indeed erroneous.", "labels": [], "entities": []}, {"text": "We will refer to this framework as \"active annotation\" in the rest of the paper.", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the software and the dataset used.", "labels": [], "entities": []}, {"text": "Section 3 explores the effect of errors in the training data and motivates the active annotation framework.", "labels": [], "entities": []}, {"text": "In Section 4 we describe the framework in detail, while Section 5 presents a method for estimating uncertainty for HMMs.", "labels": [], "entities": []}, {"text": "Section 6 presents results from applying the active annotation.", "labels": [], "entities": []}, {"text": "Section 7 compares the proposed framework to active learning and Section 8 attempts an analysis of its performance.", "labels": [], "entities": []}, {"text": "Finally, Section 9 suggests some future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data used in the experiments that follow are taken from the BioNLP 2004 named entity recognition shared task ().", "labels": [], "entities": [{"text": "BioNLP 2004 named entity recognition shared task", "start_pos": 64, "end_pos": 112, "type": "TASK", "confidence": 0.8444595677512032}]}, {"text": "The text passages have been annotated with five classes of entities, \"DNA\", \"RNA\", \"protein\", \"cell type\" and \"cell line\".", "labels": [], "entities": []}, {"text": "In our experiments, following the example of, we simplified the annotation to one entity class, namely \"gene\", which includes the DNA, RNA and protein classes.", "labels": [], "entities": []}, {"text": "In order to evaluate the performance on the task, we used the evaluation script supplied with the data, which computes the F-score (F 1 = 2 * P recision * Recall P recision+Recall ) for each entity class.", "labels": [], "entities": [{"text": "F-score", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9910412430763245}, {"text": "Recall P recision+Recall )", "start_pos": 155, "end_pos": 181, "type": "METRIC", "confidence": 0.715503066778183}]}, {"text": "It must be noted that all tokens of an entity must be recognized correctly in order to count as a correct prediction.", "labels": [], "entities": []}, {"text": "A partially recognized entity counts both as a precision and recall error.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9993144273757935}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.995572566986084}]}, {"text": "In all the experiments that follow, the official split of the data in training and testing was maintained.", "labels": [], "entities": []}, {"text": "The named entity recognition system used in our experiments is the open source NLP toolkit Lingpipe 1 . The named entity recognition module is an HMM model using Witten-Bell smoothing.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6473105549812317}, {"text": "NLP toolkit Lingpipe 1", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.8443233370780945}, {"text": "named entity recognition", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.6645393470923106}]}, {"text": "In our experiments, using the data mentioned earlier it achieved 70.06% F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9992740750312805}]}, {"text": "In this section we present results from applying active annotation to biomedical named entity recognition.", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 70, "end_pos": 105, "type": "TASK", "confidence": 0.6492667943239212}]}, {"text": "Using the noise models described in Section 3, we corrupted the training data and then using Lingpipe as the supervised learner we applied the algorithm of.", "labels": [], "entities": [{"text": "Lingpipe", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.970538318157196}]}, {"text": "The batch of tokens selected to be checked in each round was 2000 tokens.", "labels": [], "entities": []}, {"text": "As a baseline for comparison we used random selection of tokens to be checked.", "labels": [], "entities": []}, {"text": "The results for various noise models and levels are presented in the graphs of.", "labels": [], "entities": []}, {"text": "In each of these graphs, the performance of Lingpipe trained on the partially corrected material (F-ling) is plotted against the number of checked instances, under the label \"entropy\".", "labels": [], "entities": [{"text": "F-ling)", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9477556347846985}]}, {"text": "In all the experiments, active annotation significantly outperformed random selection, with the exception of 50% Random, where the high level of noise (the F-score of the hypothetical tagger that provided the initial data was 0.1) affected the initial judgements of the query module on which instances should be checked.", "labels": [], "entities": [{"text": "Random", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9642651081085205}, {"text": "F-score", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9918521046638489}]}, {"text": "After having checked some portion of the dataset though, active annotation started outperforming random selection.", "labels": [], "entities": []}, {"text": "In the graphs for the 10% Random, 20% LowRecall WholeEntities and 50% LowRecall noise models, under the label \"margin\", appear the performance curves obtained using the uncertainty estimation of.", "labels": [], "entities": []}, {"text": "Even though active annotation using this method performs better than random selection, active annotation using conditional entropy performs significantly better.", "labels": [], "entities": []}, {"text": "These results provide evidence of the theoretical advantages of conditional entropy described earlier.", "labels": [], "entities": []}, {"text": "We also ran experiments using pure uncertainty based sampling (i.e. without checking the consistency of the labels) on selecting instances to be checked.", "labels": [], "entities": []}, {"text": "The performance curves appear under the label \"uncertainty\" for the 20% LowRecall, 50% Random and 20% LowPrecision noise models.", "labels": [], "entities": [{"text": "Random", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9817628860473633}]}, {"text": "The uncertainty was estimated using the method described in Section 5.", "labels": [], "entities": [{"text": "uncertainty", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9922463297843933}]}, {"text": "As expected, uncertainty based sampling performed reasonably well, better than random selection but worse than using labelling consistency, except for the initial stage of 20% LowPrecision.", "labels": [], "entities": []}], "tableCaptions": []}