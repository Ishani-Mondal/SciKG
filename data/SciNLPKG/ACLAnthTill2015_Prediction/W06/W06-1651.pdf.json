{"title": [{"text": "Joint Extraction of Entities and Relations for Opinion Recognition", "labels": [], "entities": [{"text": "Opinion Recognition", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.736554354429245}]}], "abstractContent": [{"text": "We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.", "labels": [], "entities": [{"text": "opinion recognition and analysis", "start_pos": 92, "end_pos": 124, "type": "TASK", "confidence": 0.77203369140625}]}, {"text": "We identify two types of opinion-related entities-expressions of opinions and sources of opinions-along with the linking relation that exists between them.", "labels": [], "entities": []}, {"text": "Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities.", "labels": [], "entities": [{"text": "opinion recognition task", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7582279443740845}, {"text": "relation extraction", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.8228746354579926}]}, {"text": "Performance further improves when a semantic role labeling system is incorporated.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6220610638459524}]}, {"text": "The resulting system achieves F-measures of 79 and 69 for entity and relation extraction , respectively, improving substantially over prior results in the area.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9992314577102661}, {"text": "relation extraction", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.6801702529191971}]}], "introductionContent": [{"text": "Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and,,).", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.779679149389267}, {"text": "Mooney", "start_pos": 147, "end_pos": 153, "type": "DATASET", "confidence": 0.9393700361251831}]}, {"text": "Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (), opinion-oriented question answering (), and opinion-oriented summarization (e.g.,).", "labels": [], "entities": [{"text": "opinion-oriented information extraction", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.6634544432163239}, {"text": "product reputation tracking", "start_pos": 156, "end_pos": 183, "type": "TASK", "confidence": 0.6754175623257955}, {"text": "opinion-oriented question answering", "start_pos": 188, "end_pos": 223, "type": "TASK", "confidence": 0.605196088552475}, {"text": "opinion-oriented summarization", "start_pos": 232, "end_pos": 262, "type": "TASK", "confidence": 0.4789028763771057}]}, {"text": "Moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. and), to determine the polarity and strength of opinion expressions (e.g. ), and to recognize propositional opinions and their sources (e.g.) with reasonable accuracy.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7262849509716034}, {"text": "accuracy", "start_pos": 311, "end_pos": 319, "type": "METRIC", "confidence": 0.9951512813568115}]}, {"text": "To date, however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them.", "labels": [], "entities": []}, {"text": "Without progress on the joint extraction of opinion entities and their relations, the capabilities of opinionbased applications will remain limited.", "labels": [], "entities": []}, {"text": "Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g., ).", "labels": [], "entities": [{"text": "joint classification", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.6889001727104187}]}, {"text": "Moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently -for semantic role labeling (e.g.), information extraction (e.g. ), and sequence tagging (e.g.).", "labels": [], "entities": [{"text": "joint extraction task", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.7913817167282104}, {"text": "semantic role labeling", "start_pos": 276, "end_pos": 298, "type": "TASK", "confidence": 0.6400249302387238}, {"text": "information extraction", "start_pos": 307, "end_pos": 329, "type": "TASK", "confidence": 0.8305919170379639}, {"text": "sequence tagging", "start_pos": 343, "end_pos": 359, "type": "TASK", "confidence": 0.7369525730609894}]}, {"text": "In this paper, we present a global inference approach ( ) to the extraction of opinion-related entities and relations.", "labels": [], "entities": [{"text": "extraction of opinion-related entities and relations", "start_pos": 65, "end_pos": 117, "type": "TASK", "confidence": 0.7751525541146597}]}, {"text": "In particular, we aim to identify two types of entities (i.e. spans of text): entities that express opinions and entities that denote sources of opinions.", "labels": [], "entities": []}, {"text": "More specifically, we use the term opinion expression to denote all direct expressions of subjectivity including opinions, emotions, beliefs, sentiment, etc., as well as all speech expressions that introduce subjective propositions; and use the term source to denote the person or entity (e.g. a re-port) that holds the opinion.", "labels": [], "entities": []}, {"text": "In addition, we aim to identify the relations between opinion expression entities and source entities.", "labels": [], "entities": []}, {"text": "That is, fora given opinion expression O i and source entity S j , we determine whether the relation L i,j def = (S j expresses O i ) obtains, i.e. whether S j is the source of opinion expression O i . We refer to this particular relation as the link relation in the rest of the paper.", "labels": [], "entities": []}, {"text": "Consider, for example, the following sentences: S1.", "labels": [], "entities": []}, {"text": "(1) intends (1) to curb the increase in harmful gas emissions and is counting on (1) the goodwill ...", "labels": [], "entities": []}, {"text": "The underlined phrases above are opinion expressions and phrases marked with square brackets are source entities.", "labels": [], "entities": []}, {"text": "The numeric superscripts on entities indicate link relations: a source entity and an opinion expression with the same number satisfy the link relation.", "labels": [], "entities": []}, {"text": "For instance, the source entity \"Bush\" and the opinion expression \"intends\" satisfy the link relation, and so do \"Bush\" and \"counting on.\"", "labels": [], "entities": []}, {"text": "Notice that a sentence may contain more than one link relation, and link relations are not one-to-one mappings between sources and opinions.", "labels": [], "entities": []}, {"text": "Also, the pair of entities in a link relation may not be the closest entities to each other, as is the casein the second sentence, between \"questioning\" and \"the Islamic Republic of Iran.\"", "labels": [], "entities": []}, {"text": "We expect the extraction of opinion relations to be critical for many opinion-oriented NLP applications.", "labels": [], "entities": [{"text": "extraction of opinion relations", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.8299833834171295}]}, {"text": "For instance, consider the following question that might be given to a question-answering system: \u2022 What is the Imam's opinion toward the Islamic Republic of Iran?", "labels": [], "entities": []}, {"text": "Without in-depth opinion analysis, the questionanswering system might mistake example S2 as relevant to the query, even though S2 exhibits the opinion of the Islamic Republic of Iran toward Imam, not the other way around.", "labels": [], "entities": []}, {"text": "Inspired by , we model our task as global, constraint-based inference over separately trained entity and relation classifiers.", "labels": [], "entities": []}, {"text": "In particular, we develop three base classifiers: two sequence-tagging classifiers for the extraction See  for additional details. of opinion expressions and sources, and a binary classifier to identify the link relation.", "labels": [], "entities": []}, {"text": "The global inference procedure is implemented via integer linear programming (ILP) to produce an optimal and coherent extraction of entities and relations.", "labels": [], "entities": []}, {"text": "Because many (60%) opinion-source relations appear as predicate-argument relations, where the predicate is a verb, we also hypothesize that semantic role labeling (SRL) will be very useful for our task.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.7648201684157053}]}, {"text": "We present two baseline methods for the joint opinion-source recognition task that use a state-of-the-art SRL system (), and describe two additional methods for incorporating SRL into our ILP-based system.", "labels": [], "entities": [{"text": "joint opinion-source recognition task", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.699713222682476}, {"text": "SRL", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.9724386930465698}]}, {"text": "Our experiments show that the global inference approach not only improves relation extraction over the base classifier, but does the same for individual entity extractions.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.777797520160675}]}, {"text": "For source extraction in particular, our system achieves an F-measure of 78.1, significantly outperforming previous results in this area (), which obtained an F-measure of 69.4 on the same corpus.", "labels": [], "entities": [{"text": "source extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7987745404243469}, {"text": "F-measure", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9987940788269043}, {"text": "F-measure", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9946820139884949}]}, {"text": "In addition, we achieve an F-measure of 68.9 for link relation identification and 82.0 for opinion expression extraction; for the latter task, our system achieves human-level performance.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9990237951278687}, {"text": "link relation identification", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.763764500617981}, {"text": "opinion expression extraction", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.6989645262559255}]}], "datasetContent": [{"text": "We evaluate our system using the NRRC MultiPerspective Question Answering (MPQA) corpus that contains 535 newswire articles that are manually annotated for opinion-related information.", "labels": [], "entities": [{"text": "NRRC MultiPerspective Question Answering (MPQA) corpus", "start_pos": 33, "end_pos": 87, "type": "DATASET", "confidence": 0.823491059243679}]}, {"text": "In particular, our gold standard opinion entities correspond to direct subjective expression annotations and subjective speech event annotations (i.e. speech events that introduce opinions) in the MPQA corpus ).", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 197, "end_pos": 208, "type": "DATASET", "confidence": 0.9570718109607697}]}, {"text": "Gold standard source entities and link relations can be extracted from the agent attribute associated with each opinion entity.", "labels": [], "entities": []}, {"text": "We use 135 documents as a development set and report 10-fold cross validation results on the remaining 400 documents in all experiments below.", "labels": [], "entities": []}, {"text": "We evaluate entity and link extraction using both an overlap and exact matching scheme.", "labels": [], "entities": [{"text": "entity and link extraction", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.6562564820051193}]}, {"text": "Because the exact start and endpoints of the manual annotations are somewhat arbitrary, the overlap scheme is more reasonable for our task ( ).", "labels": [], "entities": [{"text": "overlap", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9530505537986755}]}, {"text": "We report results according to both matching schemes, but focus our discussion on results obtained using overlap matching.", "labels": [], "entities": []}, {"text": "We use the Mallet 14 implementation of CRFs.", "labels": [], "entities": [{"text": "Mallet 14 implementation", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9575208028157552}]}, {"text": "For brevity, we will refer to the opinion extraction classifier as CRF-OP, the source extraction classifier as CRF-SRC, and the link relation classifier as CRF-LINK.", "labels": [], "entities": [{"text": "opinion extraction classifier", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.7821577390034994}]}, {"text": "For ILP, we use Matlab, which produced the optimal assignment in a matter of few seconds for each sentence.", "labels": [], "entities": []}, {"text": "The weight adjustment constants defined for ILP are based on the development data.", "labels": [], "entities": [{"text": "weight adjustment constants", "start_pos": 4, "end_pos": 31, "type": "METRIC", "confidence": 0.880208154519399}]}, {"text": "15 The link-nearest baselines For baselines, we first consider a link-nearest heuristic: for each opinion entity extracted by CRF-OP, the linknearest heuristic creates a link relation with the closest source entity extracted by CRF-SRC.", "labels": [], "entities": [{"text": "CRF-SRC", "start_pos": 228, "end_pos": 235, "type": "DATASET", "confidence": 0.9568502902984619}]}, {"text": "Recall that CRF-SRC and CRF-OP extract entities from n-best sequences.", "labels": [], "entities": []}, {"text": "We test the link-nearest heuristic with n = {1, 2, 10} where larger n will boost recall at the cost of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.999053418636322}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.998754620552063}]}, {"text": "Results for the Overlap Match Exact Match r(%) p(%) f(%) r(%) p(%) f(%) NEAREST-1 51.: Relation extraction performance NEAREST-n : link-nearest heuristic w/ n-best SRL : all V-A0 frames from SRL SRL+CRF-OP : all V-A0 filtered by CRF-OP ILP-n : ILP applied to n-best sequences link-nearest heuristic on the full source-expressesopinion relation extraction task are shown in the first three rows of table 2.", "labels": [], "entities": [{"text": "NEAREST-1", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.7890857458114624}, {"text": "Relation extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8449822068214417}, {"text": "relation extraction task", "start_pos": 335, "end_pos": 359, "type": "TASK", "confidence": 0.789718508720398}]}, {"text": "NEAREST-1 performs the best in overlap-match F-measure, reaching 59.9.", "labels": [], "entities": [{"text": "NEAREST-1", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.48726630210876465}, {"text": "F-measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8301818370819092}]}, {"text": "NEAREST-10 has higher recall (66.3%), but the precision is really low (20.9%).", "labels": [], "entities": [{"text": "NEAREST-10", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8448023200035095}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9997913241386414}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9997150301933289}]}, {"text": "Performance of the opinion and source entity classifiers will be discussed in Section 8.", "labels": [], "entities": []}, {"text": "SRL baselines Next, we consider two baselines that use a state-of-the-art SRL system).", "labels": [], "entities": [{"text": "SRL baselines", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8592429161071777}]}, {"text": "In many link relations, the opinion expression entity is a verb phrase and the source entity is in an agent argument position.", "labels": [], "entities": []}, {"text": "Hence our second baseline, SRL, extracts all verb(V)-agent(A0) frames from the output of the SRL system and provides an upper bound on recall (59.7%) for systems that use SRL in isolation for our task.", "labels": [], "entities": [{"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.999165415763855}]}, {"text": "A more sophisticated baseline, SRL+CRF-OP, extracts only those V-A0 frames whose verb overlaps with entities extracted by the opinion expression extractor, CRF-OP.", "labels": [], "entities": []}, {"text": "As shown in table 2, filtering out V-A0 frames that are incompatible with the opinion extractor boosts precision to 83.2%, but the F-measure (58.9) is lower than that of NEAREST-1.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9997487664222717}, {"text": "F-measure", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9995812773704529}, {"text": "NEAREST-1", "start_pos": 170, "end_pos": 179, "type": "DATASET", "confidence": 0.8801820874214172}]}], "tableCaptions": [{"text": " Table 2: Relation extraction performance", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9574389159679413}]}, {"text": " Table 3: Relation extraction with ILP and SRL", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7802001535892487}, {"text": "SRL", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.5065616965293884}]}, {"text": " Table 4: Entity extraction performance (by overlap-matching)", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8384365439414978}]}, {"text": " Table 5: Relation extraction with ILP weight ad- justment. (All cases using ILP+SRL-f -10)", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8652784526348114}, {"text": "SRL-f -10", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.7820504109064738}]}]}