{"title": [{"text": "LoLo: A System based on Terminology for Multilingual Extraction", "labels": [], "entities": []}], "abstractContent": [{"text": "An unsupervised learning method, based on corpus linguistics and special language terminology, is described that can extract time-varying information from text streams.", "labels": [], "entities": []}, {"text": "The method is shown to be 'language-independent' in that its use leads to sets of regular-expressions that can be used to extract the information in typologically distinct languages like Eng-lish and Arabic.", "labels": [], "entities": []}, {"text": "The method uses the information related to the distribution of N-grams, for automatically extracting 'meaning bearing' patterns of usage in a training corpus.", "labels": [], "entities": []}, {"text": "The analysis of an Eng-lish news wire corpus (1,720,142 tokens) and Arabic news wire corpus (1,720,154 tokens) show encouraging results.", "labels": [], "entities": [{"text": "Eng-lish news wire corpus", "start_pos": 19, "end_pos": 44, "type": "DATASET", "confidence": 0.8750568628311157}, {"text": "Arabic news wire corpus", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.6338712051510811}]}], "introductionContent": [{"text": "One of the recent trends in (adaptive) IE has been motivated by the empirical argument that annotated corpora, either annotated automatically or annotated manually, can provide sufficient information for creating the knowledge base of an IE system ().", "labels": [], "entities": [{"text": "IE", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9007327556610107}]}, {"text": "Another equally important trend is to use manually selected seed patterns to initiate learning: In turn, active-training methods use seed patterns to learn new related patterns from unannotated corpora.", "labels": [], "entities": []}, {"text": "Many of the adaptive IE systems rely on the existing part-of-speech (POS) taggers (Debnath and) and/or syntactic parsers) for analysing and annotating text corpora.", "labels": [], "entities": [{"text": "IE", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.8771330118179321}]}, {"text": "The use of corpora in IE, especially adaptive IE, should, in principle, alleviate the need for manually creating the rules for information extraction.", "labels": [], "entities": [{"text": "IE", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9675923585891724}, {"text": "information extraction", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.7868949472904205}]}, {"text": "The successful use of POS/syntactic taggers is dependent on the availability of the knowledge of (natural) language used by the authors of documents in a given corpus.", "labels": [], "entities": [{"text": "POS/syntactic taggers", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7006823271512985}]}, {"text": "There is a wealth of POS taggers and parsers available for English language, as it has been the most widely used language in computational linguistics.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.7244223356246948}]}, {"text": "However, this is not the case for strategically important languages like Arabic and Chinese; to start with, in Chinese one does not have the luxury of separating word-tokens by a white space and in Arabic complex rules are required to identify morphemes compared to English.", "labels": [], "entities": []}, {"text": "The development of segmentation programs in these languages has certainly helped ().", "labels": [], "entities": []}, {"text": "More work is needed in understanding these languages such that the knowledge thus derived can be used to power taggers and parsers.", "labels": [], "entities": []}, {"text": "Typically, IE systems are used to analyse news wire corpora, telephone conversations, and more recently in bio-informatics.", "labels": [], "entities": [{"text": "IE", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9575647115707397}]}, {"text": "The first two systems deal with language of everyday communications -the general language-whereas bioinformatics deals with a specialist domain and has its own 'special language'.", "labels": [], "entities": []}, {"text": "English special languages, for example languages of law, commerce, finance, science & technology, each have a limited vocabulary and idiosyncratic syntactic structures when compared with English used in an everyday context.", "labels": [], "entities": []}, {"text": "The same is true of German, French, Russian, Chinese, Arabic or Hindi.", "labels": [], "entities": []}, {"text": "It appears that few works, if any, take advantage of the properties of special language to build IE systems.", "labels": [], "entities": []}, {"text": "Our objective is to use methods and techniques of IE in the automatic analysis of specialist news that streams in such away that information extracted at an earlier period of time maybe contradicted or reinforced by information extracted at a later time.", "labels": [], "entities": [{"text": "IE", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9840365052223206}]}, {"text": "The impact of news on financial and commodity markets is of consider-able import and is often called sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.9337677657604218}]}, {"text": "The prefix 'sentiment' is used to distinguish this kind of analysis from the more quantitative analysis of assets (called fundamental analysis) and that of price movements (called technical analysis).", "labels": [], "entities": []}, {"text": "There is a great deal of discussion in financial economics, econometrics, and in the newly emergent discipline of investor psychology about the impact of 'good' and 'bad' news on the behaviour of both investors and brokers.", "labels": [], "entities": []}, {"text": "Three Nobel Prizes have been awarded on the impact of market (trader and investor) sentiment on the value of shares, currencies, derivatives and other financial instruments.", "labels": [], "entities": [{"text": "Nobel", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9483881592750549}]}, {"text": "Financial news, in addition to e-mails and blogs, has contributed to the catastrophic failures of major trading institutions.", "labels": [], "entities": []}, {"text": "One of the key proponents of news impact analysis is the Economics Nobel Laureate Robert Engle who has written about asymmetry of information in a market -the brokers have more knowledge than any given individual, rumours have different impact on different actors in the market.", "labels": [], "entities": [{"text": "news impact analysis", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7482251723607382}, {"text": "Economics Nobel Laureate Robert Engle", "start_pos": 57, "end_pos": 94, "type": "DATASET", "confidence": 0.6468892812728881}]}, {"text": "Engle's statistical analysis suggests that the 'bad' news has longer lasting effect than 'good' news.", "labels": [], "entities": []}, {"text": "Usually, sentiment analysis is carried out using news proxies which include dates/times and the names of agencies releasing key items of financial data) or data like the age of a firm, its number of initial public offerings, return on investment, etc.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9571253955364227}]}, {"text": "These proxies are then regressed with share, currency or commodity prices.", "labels": [], "entities": []}, {"text": "News impact analysis is moving into its next phase where the text of news is analysed albeit to a limited extent.", "labels": [], "entities": [{"text": "News impact analysis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7468128005663554}]}, {"text": "The analysis sometimes looks at the frequency distribution of pre-specified keywords -directional metaphors like rose/fall, up/down, health metaphors like anaemic/healthy and animal metaphors like bullish/bearish.", "labels": [], "entities": []}, {"text": "A system is trained to correlate and to learn the changes in distribution of the prescribed metaphorical keywords, together with names of organisations, to the changes in the value of financial instruments (.", "labels": [], "entities": []}, {"text": "We are attempting to create a languageinformed framework for news impact analysis using techniques of corpus linguistics and special language analysis.", "labels": [], "entities": [{"text": "news impact analysis", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.825915257136027}, {"text": "special language analysis", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.7199885845184326}]}, {"text": "The purpose is to automatically extract patterns from a corpus of domain specific texts without prescribing the metaphorical keywords and organisation names.", "labels": [], "entities": []}, {"text": "This, we believe, can be achieved by looking at the lexical signature of a specialist domain and extracting collocational patterns of the individual items of the lexical signature.", "labels": [], "entities": []}, {"text": "The lexical signature includes key vocabulary items of the domain and names of people, places and things in the domain.", "labels": [], "entities": []}, {"text": "There are instances in the part-of-speech tagging literature and in IE where a corpus is used and words within a grammatical category help to extract rules and patterns comprising essential information about a domain or topic.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.760680615901947}, {"text": "IE", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9506241679191589}]}, {"text": "Brill, Wilks and Yangarber induce grammars of a universal kind: we focus on inducing a local grammar that deals with the patterning of the items in the signature.", "labels": [], "entities": []}, {"text": "Note that in all these cases of grammar induction the intuition of the grammar builder plays a critical part whether it be in the choice of syntactic transformation rules, or in choosing sense taggers and implicitly semantic rules, or in choosing user supplied seed patterns.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7123119533061981}]}, {"text": "Most of the work in grammar induction is focussed on English or typologically similar languages.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.823653370141983}]}, {"text": "We have deliberately chosen typologically different languages (English and Arabic) to evaluate the extent to which our method of 'grammar induction' is language independent.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7517486810684204}]}, {"text": "We describe a method for building domain specific IE systems: the patterns used to extract domain specific information are the N-gram collocation patterns of domain specific terms.", "labels": [], "entities": []}, {"text": "The patterns are extracted from un-annotated domainspecific text corpora.", "labels": [], "entities": []}, {"text": "We show how one can analyse the N-gram patterns and render them as regular expressions.", "labels": [], "entities": []}, {"text": "The thesaurus used to identify domain specific words is itself constructed automatically from a (training) special-language corpus.", "labels": [], "entities": []}, {"text": "The frequency distribution of domain specific terms in a special language corpus shows characteristic differences from the distribution of the same terms in a general language corpus.", "labels": [], "entities": []}, {"text": "There is little or no difference in the distribution of the so-called grammatical or closed class terms in a special and a general language corpus.", "labels": [], "entities": []}, {"text": "Furthermore, amongst the domain specific terms, a few tend to dominate the frequency distribution -the so-called lexical signature of a domain.", "labels": [], "entities": []}, {"text": "These signature terms are used as nucleates for compound terms in a domain.", "labels": [], "entities": []}, {"text": "The occurrence of the signature terms, either on their own or in a compound or a phrase, is equally idiosyncratic in that these dominant single or compound terms co-occur more frequently with one set of words than with others.", "labels": [], "entities": []}, {"text": "The behaviour of signature terms appears to be governed by a grammar that is local to the specialism and is not elsewhere in the general language; local grammar is used in general language for telling times and dates in metaphorical expressions, and in the lexicography for describing the language of definitions of lemmas in a lexicon ().", "labels": [], "entities": []}, {"text": "The local grammar approach, rooted in the lexical signature of a given domain can be used to extract 'sentiment' bearing sentences in financial markets) or in the description of work in a scientific laboratory).", "labels": [], "entities": []}, {"text": "We introduce a system that can help in building domain specific IE systems in English and languages that are typologically distinct from English, specifically Arabic.", "labels": [], "entities": [{"text": "IE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9102140069007874}]}, {"text": "The development of LoLo was inspired by Engle's pioneering work in econometrics where news impact analysis is regarded as critical to the analysis of market movement: however much of the work in financial economics relates to the correlation of the timings of news announcements rather than the content of the news stream ().", "labels": [], "entities": []}, {"text": "LoLo can manage a corpus and extract key terms.", "labels": [], "entities": []}, {"text": "Given the keyword list, the system then identifies collocates and selects significant collocates on well defined statistical criterion.", "labels": [], "entities": []}, {"text": "Finally, local grammar rules are identified and an IE system is created.", "labels": [], "entities": []}, {"text": "LoLo has been used to build a local grammar to extract 'sentiment' or key (changing) market events in English and in Arabic from unseen texts.", "labels": [], "entities": [{"text": "extract 'sentiment' or key (changing) market events", "start_pos": 47, "end_pos": 98, "type": "TASK", "confidence": 0.7453118508512323}]}, {"text": "The system can help visualise the distribution of extracted patterns synchronised with the movement of financial markets.", "labels": [], "entities": []}, {"text": "IE systems need to be adaptive, as the specialisms in particular and the world in general is changing rapidly and this change is usually reflected in language use.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9782123565673828}]}, {"text": "There is an equally important need to build cross language IE systems as information maybe in different languages.", "labels": [], "entities": []}, {"text": "The lexically-motivated approach we describe in this paper responds to the need for an adaptive, cross domain and cross language IE systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have argued that a method that is focused on frequency at the lexical level(s) of linguistic description -single words, compounds, and Ngrams-will perhaps lead to patterns that are idiosyncratic of a specialist domain without recourse to a thesaurus.", "labels": [], "entities": []}, {"text": "There area number of linguistic methods -that focus on syntactic and semantic level of description which might be of equal or better use.", "labels": [], "entities": []}, {"text": "In order to show the effectiveness of our method we apply it to sentiment analysis -an analysis that attempts to extract qualitative opinion expressed about a range of human and natural artefacts -films, cars, financial instruments for instance.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9448757171630859}]}, {"text": "Broadly speaking, sentiments in financial markets relate to the 'rise' and 'fall' of financial instruments (shares, currencies, commodities and energy prices): inextricably these sentiments relate to change in the prices of the instruments.", "labels": [], "entities": []}, {"text": "In both English and Arabic, we have found that percent or equivalent is a keyword and trigrams and longer N-grams embedded with this keyword relate to metaphorical movement words-up, down, rise, fall.", "labels": [], "entities": []}, {"text": "However, in English this association is further contextualised with other keywords -shares, stocks-and in Arabic the contextualisation is with shares and the principal commodity of many Arab states economies -oil.", "labels": [], "entities": []}, {"text": "Our system 'discovered' both by following a lexical level of linguistic description.", "labels": [], "entities": []}, {"text": "For each of the two languages of interest to us, we have created 1.72 million token corpora.", "labels": [], "entities": []}, {"text": "Each corpus was then divided into two (roughly) equal sized sub corpora: training corpus and testing corpus; the testing corpus is sub-divided into two testing corpora Test and Test.", "labels": [], "entities": []}, {"text": "First, we extract patterns from the Training Corpus using the discover local grammar algorithm) and also from Test . Next, the Training 1 and Test 1 corpora are merged and patterns extracted from the merged corpus.", "labels": [], "entities": [{"text": "Training Corpus", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8933894336223602}]}, {"text": "The intuition we have is that as the size of the corpus is increased the patterns extracted from a smaller sized corpus will be elaborated: some of the patterns that are idiosyncratic of the smaller sized corpus will become statistically insignificant and hence will be ignored.", "labels": [], "entities": []}, {"text": "The conventional way of testing would have been to see how many patterns discovered in the training corpus are found in the testing corpora; we are quantifying these results currently.", "labels": [], "entities": []}, {"text": "In the following we describe an initial test of our method after introducing LoLo.", "labels": [], "entities": []}, {"text": "We have used the Rules Editor and the Information Extractor to evaluate the patterns on a corpus comprising 2408 texts and 858,650 tokens created by merging Test and Test 2 corpora.", "labels": [], "entities": []}, {"text": "The Arabic evaluation corpus comprised 5118 texts and 860,134 tokens.", "labels": [], "entities": [{"text": "Arabic evaluation corpus", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.5494306981563568}]}, {"text": "The N-gram pattern extractor (where N > 4) showed considerable promise in that who or what went up/or down was unambiguously extracted from the English test corpus using patterns generated through the training corpus.", "labels": [], "entities": [{"text": "N-gram pattern extractor", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6026534338792165}, {"text": "English test corpus", "start_pos": 144, "end_pos": 163, "type": "DATASET", "confidence": 0.7630715171496073}]}, {"text": "Initial results show high precision with the longer N-grams in English and Arabic..", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.999405026435852}]}, {"text": "Patterns with high precision (English).", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.99769127368927}]}, {"text": "However, some patterns return many extracted information that require trimming.", "labels": [], "entities": [{"text": "trimming", "start_pos": 70, "end_pos": 78, "type": "TASK", "confidence": 0.95444655418396}]}, {"text": "For example many organizations names are extracted in Arabic using the pattern shown in table 21 but they usually have the word by-a-ratio (be-nesba, ) attached at the end resulting in low precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9949760437011719}]}, {"text": "Because we have used the same training thresholds for English and Arabic, the patterns in Arabic appeared without the motion words.", "labels": [], "entities": []}, {"text": "However the system can extract these words along with the org/instrument/index names because they appear frequently as slots in the patterns.", "labels": [], "entities": []}, {"text": "The N-gram patterns (when N \u2264 4) show poor results in that either such patterns found in the training corpus are not found in the test corpus, or the patterns retrieved from test corpora are at semantic variance with the same pattern in the training corpus.", "labels": [], "entities": []}, {"text": "This suggests that there is an optimal length of individual patterns in our local grammar.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Downward collocates of percent in a  corpus of 431,850 words.", "labels": [], "entities": []}, {"text": " Table 2. Upward collocates of percent in a cor- pus of 431,850 words.", "labels": [], "entities": []}, {"text": " Table 3. Trigrams of percent.", "labels": [], "entities": [{"text": "Trigrams", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9524247646331787}]}, {"text": " Table 4. Trigrams of percent with omitted low  frequency words (denoted as * for multiple to- kens and # for a single token).", "labels": [], "entities": []}, {"text": " Table 5. Some of top patterns of percent (<s>  identifies a sentence boundary).", "labels": [], "entities": []}, {"text": " Table 6. Downward collocates of percent (al- meaa, \u202b.)\u0627\u202c", "labels": [], "entities": []}, {"text": " Table 7. Upward collocates of percent (almeaa,", "labels": [], "entities": []}, {"text": " Table 8. Trigrams of percent (almeaa, \u202b.)\u0627\u202c", "labels": [], "entities": [{"text": "Trigrams", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9572574496269226}]}, {"text": " Table 10. Training and testing corpora used in  our experiments.", "labels": [], "entities": []}, {"text": " Table 13. Patterns of percent extracted from  Training 1 corpus.", "labels": [], "entities": [{"text": "Patterns", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.962151825428009}, {"text": "Training 1 corpus", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8610302209854126}]}, {"text": " Table 14. Patterns of percent extracted from  Test 1 corpus found as sub-patterns in Training 1 .", "labels": [], "entities": [{"text": "Patterns", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9508070349693298}, {"text": "Test 1 corpus", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7050648828347524}]}, {"text": " Table 16. Similar to the English  corpora the Test 1 Arabic corpus has most of the  trigrams in the Training 1 Corpus and some larger  N-grams(Table 17).", "labels": [], "entities": [{"text": "Test 1 Arabic corpus", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.8563346713781357}, {"text": "Training 1 Corpus", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.8339362939198812}]}, {"text": " Table 16. Patterns of percent (almeaa, \u202b)\u0627\u202c ex- tracted from Training 1 Arabic corpus.", "labels": [], "entities": [{"text": "Patterns", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9649707078933716}, {"text": "Training 1 Arabic corpus", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.683469757437706}]}]}