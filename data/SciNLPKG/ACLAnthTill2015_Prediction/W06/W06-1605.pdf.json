{"title": [{"text": "Distributional Measures of Concept-Distance: A Task-oriented Evaluation", "labels": [], "entities": [{"text": "Distributional Measures", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8566939830780029}]}], "abstractContent": [{"text": "We propose a framework to derive the distance between concepts from distribu-tional measures of word co-occurrences.", "labels": [], "entities": []}, {"text": "We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept-concept matrix roughly .01% the size of that created by existing measures.", "labels": [], "entities": []}, {"text": "We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors.", "labels": [], "entities": [{"text": "correcting real-word spelling errors", "start_pos": 197, "end_pos": 233, "type": "TASK", "confidence": 0.9057717025279999}]}, {"text": "In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath out-performs the best distributional concept-distance measures.", "labels": [], "entities": []}, {"text": "1 Semantic and distributional measures Measures of distance of meaning are of two kinds.", "labels": [], "entities": [{"text": "Measures of distance of meaning", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.8585192203521729}]}, {"text": "The first kind, which we will refer to as semantic measures, rely on the structure of a resource such as WordNet or, in some cases, a semantic network, and hence they measure the distance between the concepts or word-senses that the nodes of the resource represent.", "labels": [], "entities": []}, {"text": "Examples include the measure for MeSH proposed by Rada et al.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.638542652130127}]}, {"text": "(1989) and those for WordNet proposed by Leacock and Chodorow (1998) and Jiang and Conrath (1997).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9287847876548767}]}, {"text": "(Some of the more successful measures, such as Jiang-Conrath, also use information content derived from word frequency.)", "labels": [], "entities": []}, {"text": "Typically, these measures rely on an extensive hierarchy of hyponymy relationships for nouns.", "labels": [], "entities": []}, {"text": "Therefore, these measures are expected to perform poorly when used to estimate distance between senses of part-of-speech pairs other than noun-noun, not just because the WordNet hierarchies for other parts of speech are less well developed, but also because the hierarchies for the different parts of speech are not well connected.", "labels": [], "entities": []}, {"text": "The second kind of measures, which we will refer to as distributional measures, are inspired by the maxim \"You shall know a word by the company it keeps\" (Firth, 1957).", "labels": [], "entities": []}, {"text": "These measures rely simply on raw text, and hence are much less resource-hungry than the semantic measures; but they measure the distance between words rather than word-senses or concepts.", "labels": [], "entities": []}, {"text": "In these measures, two words are considered close if they occur in similar contexts.", "labels": [], "entities": []}, {"text": "The context (or \"company\") of a target word is represented by its distributional profile (DP), which lists the strength of association between the target and each of the lexical, syntactic, and/or semantic units that co-occur with it.", "labels": [], "entities": [{"text": "distributional profile (DP)", "start_pos": 66, "end_pos": 93, "type": "METRIC", "confidence": 0.7632165908813476}]}, {"text": "Commonly used measures of strength of association are conditional probability (0 to 1) and pointwise mutual information (\u221e to \u221e) 1.", "labels": [], "entities": []}, {"text": "Commonly used units of co-occurrence with the target are other words, and so we speak of the lexical dis-tributional profile of a word (lexical DPW).", "labels": [], "entities": []}, {"text": "The co-occurring words maybe all those in a predetermined window around the target, or maybe restricted to those that have a certain syntactic (e.g., verb-object) or semantic (e.g., agent-theme) relation with the target word.", "labels": [], "entities": []}, {"text": "We will refer to the former kind of DPs as relation-free.", "labels": [], "entities": []}, {"text": "Usually in 1 In our experiments, we set negative PMI values to 0, because Church and Hanks (1990), in their seminal paper on word association ratio, show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To evaluate the distributional concept-distance measures, we used them in the tasks of ranking word pairs in order of their semantic distance and of correcting real-word spelling errors, and compared our results to those that we obtained on the same tasks with distributional word-distance measures and those that obtained with WordNet-based semantic measures.", "labels": [], "entities": [{"text": "correcting real-word spelling errors", "start_pos": 149, "end_pos": 185, "type": "TASK", "confidence": 0.7857428938150406}]}, {"text": "The distributional concept-distance measures used a bootstrapped WCCM created from the BNC and the Macquarie Thesaurus.", "labels": [], "entities": [{"text": "BNC", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9809565544128418}, {"text": "Macquarie Thesaurus", "start_pos": 99, "end_pos": 118, "type": "DATASET", "confidence": 0.9694357812404633}]}, {"text": "The word-distance measures used a word-word co-occurrence matrix created from the BNC alone.", "labels": [], "entities": [{"text": "BNC", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8400259017944336}]}, {"text": "The BNC was not lemmatized, part of speech tagged, or chunked.", "labels": [], "entities": [{"text": "BNC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8683959245681763}]}, {"text": "The vocabulary was restricted to the words present in the thesaurus (about 98,000 word types) both to provide a level evaluation platform and to keep the matrix to a manageable size.", "labels": [], "entities": []}, {"text": "Co-occurrence counts less than 5 were reset to 0, and words that co-occurred with more than 2000 other words were stoplisted (543 in all).", "labels": [], "entities": []}, {"text": "We used ASD cp (\u03b1 \ud97b\udf59 0\ud97b\udf5999), Cos cp , JSD cp , and Lin pmi 4 to populate corresponding concept-concept distance matrices and word-word distance matrices.", "labels": [], "entities": [{"text": "JSD cp", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.8626779913902283}]}, {"text": "Applications that require distance values will enjoy a run-time benefit if the distances are precomputed.", "labels": [], "entities": []}, {"text": "While it is easy to completely populate the concept-concept co-occurrence matrix, completely populating the word-word distance matrix is a non-trivial task because of memory and time constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation of distributional measures  with human ranking.  Best results for each  measure-type are shown in boldface.", "labels": [], "entities": []}, {"text": " Table 4: Real-word error correction using distributional word-distance (Distrib word ), distributional  concept-distance (Distrib concept ), and Hirst and Budanitsky's (2005) results using WordNet-based  concept-distance measures (WNet concept ). Best results for each measure-type are shown in boldface.", "labels": [], "entities": [{"text": "Real-word error correction", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6856934229532877}]}]}