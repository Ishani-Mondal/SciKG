{"title": [{"text": "Enriching a formal ontology with a thesaurus: an application in the cultural heritage domain", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a pattern-based method to automatically enrich a core ontology with the definitions of a domain glossary.", "labels": [], "entities": []}, {"text": "We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology.", "labels": [], "entities": [{"text": "CIDOC CRM core ontology", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.9516874998807907}]}, {"text": "To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others.", "labels": [], "entities": [{"text": "AAT art and architecture glossary", "start_pos": 60, "end_pos": 93, "type": "DATASET", "confidence": 0.758498239517212}, {"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9425866007804871}]}], "introductionContent": [{"text": "Large-scale, automatic semantic annotation of web documents based on well established domain ontologies would allow various Semantic Web applications to emerge and gain acceptance.", "labels": [], "entities": []}, {"text": "Wide coverage ontologies are indeed available for general-purpose domains (e.g. WordNet, CYC, SUMO 1 ), however semantic annotation in unconstrained areas seems still out of reach for state of art systems.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9502439498901367}]}, {"text": "Domain-specific ontologies are preferable since they limit the domain and make the applications feasible.", "labels": [], "entities": []}, {"text": "Furthermore, real-world applications (e.g tourism, cultural heritage, ecommerce) are dominated by the requirements of the related web communities, who began to believe in the benefits deriving from the application of Semantic Web techniques.", "labels": [], "entities": []}, {"text": "These communities are interested in extracting from texts specific types of information, rather than general-purpose relations.", "labels": [], "entities": []}, {"text": "Accordingly, they produced remarkable efforts to conceptualize their competence domain through the definition of a core ontology 2 . 1 WordNet: http://wordnet.princeton.edu, CYC: http://www.opencyc.org, SUMO: http://www.ontologyportal.org 2 a core ontology is a very basic ontology consisting only of the minimal concepts relations and axioms Relevant examples are in the area of enterprise modeling) () and cultural heritage.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9594194293022156}, {"text": "CYC", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.9532097578048706}]}, {"text": "Core ontologies are indeed a necessary starting point to model in a principled way the basic concepts, relations and axioms of a given domain.", "labels": [], "entities": []}, {"text": "But in order for an ontology to be really usable in applications, it is necessary to enrich the core structure with the thousands of concepts and instances that \"make\" the domain.", "labels": [], "entities": []}, {"text": "In this paper we present a methodology to automatically annotate a glossary G with the semantic relations of an existing core ontology O.", "labels": [], "entities": []}, {"text": "Glosses are then converted into formal concepts, used to enrich O.", "labels": [], "entities": []}, {"text": "The annotation of glossary definitions is performed using regular expressions, a widely adopted text mining approach.", "labels": [], "entities": []}, {"text": "However, while in the literature regular expressions seek mostly for patterns at the lexical and part of speech level, we defined more complex expressions enriched with syntactic and semantic constraints.", "labels": [], "entities": []}, {"text": "A word sense disambiguation algorithm, SSI, is used to automatically replace the high level semantic constraints specified in the core ontology with finegrained sense restrictions, using the sense inventory of a general purpose lexicalized ontology, WordNet.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.6742842694123586}, {"text": "WordNet", "start_pos": 250, "end_pos": 257, "type": "DATASET", "confidence": 0.9403821229934692}]}, {"text": "We experimented our methodology in the cultural heritage domain, since for this domain several well-established resources are available, like the CIDOC-CRM core ontology, the AAT art and architecture thesaurus, and others.", "labels": [], "entities": [{"text": "CIDOC-CRM core ontology", "start_pos": 146, "end_pos": 169, "type": "DATASET", "confidence": 0.8482675949732462}, {"text": "AAT art and architecture thesaurus", "start_pos": 175, "end_pos": 209, "type": "TASK", "confidence": 0.6065556287765503}]}, {"text": "The paper is organized as follows: in Section 2 we briefly present the CIDOC and the other resources used in this work.", "labels": [], "entities": [{"text": "CIDOC", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9371405839920044}]}, {"text": "In Section 3 we describe in detail the ontology enrichment algorithm.", "labels": [], "entities": [{"text": "ontology enrichment", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8200165927410126}]}, {"text": "Finally, in Section 4 we provide a performance evaluation on a subset of CIDOC required to understand the other concepts in the domain.", "labels": [], "entities": []}, {"text": "properties and a sub-tree of the AAT thesaurus.", "labels": [], "entities": [{"text": "AAT thesaurus", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8637703657150269}]}, {"text": "Related literature is examined in Section 5.", "labels": [], "entities": [{"text": "Related literature", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.725964292883873}]}], "datasetContent": [{"text": "Since the CIDOC-CRM model formalizes a large number of fine-grained properties (precisely, 141), we selected a subset of properties for our experiments (reported in).", "labels": [], "entities": []}, {"text": "We wrote a relation checker for each property in the By applying the checkers in cascade to a gloss G, a set of annotations is produced.", "labels": [], "entities": []}, {"text": "The following is an example of an annotated gloss for the term \"vedute\": Refers to detailed, largely factual topographical views, especially <has-time-span>18th-century</has-time-span> Italian paintings, drawings, or prints of cities.", "labels": [], "entities": []}, {"text": "The first vedute probably were <carried-out-by>painted by northern European artists</carriedout-by> who worked <has former-or-current-location>in Italy</has former-or-current-location><has-time-span>in the 16th century</has-time-span>.", "labels": [], "entities": []}, {"text": "The term refers more generally to any painting, drawing or print <depicts>representing a landscape or town view</depicts> that is largely topographical in conception.", "labels": [], "entities": []}, {"text": "shows a more comprehensive graph representation of the outcome for the concepts vedute#1 and maest\u00e0#1 (see the gloss in Section 2.2).", "labels": [], "entities": []}, {"text": "To evaluate the methodology described in Section 3 we considered 814 glosses from the Visual Works sub-tree of the AAT thesaurus, containing a total of 27,925 words.", "labels": [], "entities": [{"text": "AAT thesaurus", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.8100076615810394}]}, {"text": "The authors wrote the relation checkers by tuning them on a subset of 122 glosses, and tested their generality on the remaining 692.", "labels": [], "entities": [{"text": "relation checkers", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8113537132740021}]}, {"text": "The test set was manually tagged with the subset of the CIDOC-CRM properties shown in by two annotators with adjudication (requiring a careful comparison of the two sets of annotations).", "labels": [], "entities": []}, {"text": "We performed two experiments: in the first, we evaluated the gloss annotation task, in the second the property instance extraction task, i.e. the ability to identify the appropriate domain and range of a property instance.", "labels": [], "entities": [{"text": "property instance extraction", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.6471451818943024}]}, {"text": "In the case of the gloss annotation task, for evaluating each piece of information we adopted the measures of \"labeled\" precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9883847236633301}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9984690546989441}]}, {"text": "These measures are commonly used to evaluate parse trees obtained by a parser and allow the rewarding of good partial results.", "labels": [], "entities": []}, {"text": "Given a property R, labeled precision is the number of words annotated correctly with Rover the number of words annotated automatically with R, while labeled recall is the number of words annotated correctly with Rover the total number of words manually annotated with R. shows the results obtained by applying the checkers to tag the test set (containing a total number of 1,328 distinct annotations and 5,965 annotated words).", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8104920387268066}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.93167644739151}]}, {"text": "Note that here we are evaluating the ability of the system to assign the correct tag to every word in a gloss fragment f, according to the appropriate relation checker.", "labels": [], "entities": []}, {"text": "We choose to evaluate the tag assigned to single words rather than to a whole phrase, because each misalignment would count as a mistake even if the most part of a phrase was tagged correctly by the automatic annotator.", "labels": [], "entities": []}, {"text": "The second experiment consisted in the evaluation of the property instances extracted.", "labels": [], "entities": []}, {"text": "Starting from 1,328 manually annotated fragments of 692 glosses, the checkers extracted an overall number of 1,101 property instances.", "labels": [], "entities": []}, {"text": "We randomly selected a subset of 160 glosses for evaluation, from which we manually extracted 344 property instances.", "labels": [], "entities": []}, {"text": "Two aspects of the property instance extraction task had to be assessed: \uf0a7 the extraction of the appropriate range words in a gloss, fora given property instance \uf0a7 the precision and recall in the extraction of the appropriate concepts for both domain and range of the property instance.", "labels": [], "entities": [{"text": "property instance extraction task", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.7320389747619629}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.999481737613678}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9989833235740662}]}, {"text": "An overall number of 233 property instances were automatically collected by the checkers, out of which 203 were correct with respect to the first assessment (87.12% precision (203/233), 59.01% recall (203/344)).", "labels": [], "entities": [{"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9893459677696228}, {"text": "recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9991914629936218}]}, {"text": "In this example, ground pigment refers to crayons (not to pastels).", "labels": [], "entities": []}, {"text": "The evaluation of the semantic correctness of the domain and range of the property instances extracted led to the final figures of 81.11% (189/233) precision and 54.94% (189/344) recall, due to 9 errors in the choice of Ct as a domain for an instance R(C t , C w ) and 5 errors in the semantic disambiguation of range words w not appearing in AAT, but encoded in WordNet (as described in the last part of Section 3).", "labels": [], "entities": [{"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9964007139205933}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9988730549812317}, {"text": "WordNet", "start_pos": 363, "end_pos": 370, "type": "DATASET", "confidence": 0.9582334160804749}]}, {"text": "A final experiment was performed to evaluate the generality of the approach presented in this paper.", "labels": [], "entities": []}, {"text": "As already remarked, the same procedure used for annotating the glosses of a thesaurus can be used to annotate web documents.", "labels": [], "entities": []}, {"text": "Our objective in this third experiment was to: \uf0a7 Evaluate the ability of the system to annotate fragments of web documents with CIDOC relations \uf0a7 Evaluate the domain dependency of the relation checkers, by letting the system annotate documents not in the cultural heritage domain.", "labels": [], "entities": []}], "tableCaptions": []}