{"title": [], "abstractContent": [{"text": "Event detection and recognition is a complex task consisting of multiple sub-tasks of varying difficulty.", "labels": [], "entities": [{"text": "Event detection and recognition", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8198252022266388}]}, {"text": "In this paper, we present a simple, modular approach to event extraction that allows us to experiment with a variety of machine learning methods for these sub-tasks, as well as to evaluate the impact on performance these sub-tasks have on the overall task.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7679470181465149}]}], "introductionContent": [{"text": "Events are undeniably temporal entities, but they also possess a rich non-temporal structure that is important for intelligent information access systems (information retrieval, question answering, summarization, etc.).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.7122589647769928}, {"text": "question answering", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.8039977550506592}, {"text": "summarization", "start_pos": 198, "end_pos": 211, "type": "TASK", "confidence": 0.8802891373634338}]}, {"text": "Without information about what happened, where, and to whom, temporal information about an event may not be very useful.", "labels": [], "entities": []}, {"text": "In the available annotated corpora geared toward information extraction, we see two models of events, emphasizing these different aspects.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8145045042037964}]}, {"text": "On the one hand, there is the TimeML model, in which an event is a word that points to anode in a network of temporal relations.", "labels": [], "entities": []}, {"text": "On the other hand, there is the ACE model, in which an event is a complex structure, relating arguments that are themselves complex structures, but with only ancillary temporal information (in the form of temporal arguments, which are only noted when explicitly given).", "labels": [], "entities": []}, {"text": "In the TimeML model, every event is annotated, because every event takes part in the temporal network.", "labels": [], "entities": []}, {"text": "In the ACE model, only \"interesting\" events (events that fall into one of 34 predefined categories) are annotated.", "labels": [], "entities": []}, {"text": "The task of automatically extracting ACE events is more complex than extracting TimeML events (in line with the increased complexity of ACE events), involving detection of event anchors, assignment of an array of attributes, identification of arguments and assignment of roles, and determination of event coreference.", "labels": [], "entities": [{"text": "determination of event coreference", "start_pos": 282, "end_pos": 316, "type": "TASK", "confidence": 0.7317180633544922}]}, {"text": "In this paper, we present a modular system for ACE event detection and recognition.", "labels": [], "entities": [{"text": "ACE event detection and recognition", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.8013055443763732}]}, {"text": "Our focus is on the difficulty and importance of each sub-task of the extraction task.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9671226143836975}]}, {"text": "To this end, we isolate and perform experiments on each stage, as well as evaluating the contribution of each stage to the overall task.", "labels": [], "entities": []}, {"text": "In the next section, we describe events in the ACE program in more detail.", "labels": [], "entities": [{"text": "ACE program", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.8043682873249054}]}, {"text": "In section 3, we provide an overview of our approach and some information about our corpus.", "labels": [], "entities": []}, {"text": "In sections 4 through 7, we describe our experiments for each of the subtasks of event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7800363898277283}]}, {"text": "In section 8, we compare the contribution of each stage to the overall task, and in section 9, we conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "Looking at the attribute sub-tasks, the effects on ACE value are even smaller.", "labels": [], "entities": [{"text": "ACE value", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.8717037439346313}]}, {"text": "Using the learned attribute classifiers (with ground truth anchors and arguments) results in 4.8% point loss in value from ground truth attributes (1 vs. 5) and only a 0.5% point gain in value from majority class attributes (4 vs. 5).", "labels": [], "entities": []}, {"text": "With learned anchors and arguments, the learned attribute classifiers result in a 0.4% loss in value from even majority class attributes (3 vs. 7).", "labels": [], "entities": []}, {"text": "Arguments clearly have the greatest impact on ACE value (which is unsurprising, given that arguments are weighted heavily in event value).", "labels": [], "entities": [{"text": "ACE value", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.8463735282421112}]}, {"text": "Using ground truth anchors and attributes, learned arguments result in a loss of value of 35.6% points from ground truth arguments (1 vs. 2).", "labels": [], "entities": []}, {"text": "When the learned coreference classifier is used, the loss in value from ground truth arguments to learned arguments is even greater (42.5%, 8 vs. 10).", "labels": [], "entities": []}, {"text": "Anchor identification also has a large impact on ACE value.", "labels": [], "entities": [{"text": "Anchor identification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7861104905605316}, {"text": "ACE", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.6884984374046326}]}, {"text": "Without coreference but with learned arguments and attributes, the difference between using ground truth anchors and learned anchors is 22.2% points (6 vs. 7).", "labels": [], "entities": []}, {"text": "With coreference, the difference is still 21.0% points.", "labels": [], "entities": []}, {"text": "Overall, using the best learned classifiers for the various subtasks, we achieve an ACE value score of 22.3%, which falls within the range of scores for the 2005 diagnostic event extraction task (19.7%-32.7%).", "labels": [], "entities": [{"text": "ACE value score", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.9798610011736552}, {"text": "diagnostic event extraction task", "start_pos": 162, "end_pos": 194, "type": "TASK", "confidence": 0.8313443809747696}]}, {"text": "Note, though, that these scores are not really comparable, since they involve training on the full training set and testing on a separate set of documents (as noted above, the 2005 ACE testing data is not available for further experimentation, so we are using 90% of the original training data for training/development and", "labels": [], "entities": [{"text": "ACE testing data", "start_pos": 181, "end_pos": 197, "type": "DATASET", "confidence": 0.8461828430493673}]}], "tableCaptions": [{"text": " Table 1: Results for anchor detection and classifi- cation", "labels": [], "entities": [{"text": "anchor detection", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.9578447341918945}]}, {"text": " Table 2: Results for anchor detection (i.e., binary  classification of anchor instances)", "labels": [], "entities": [{"text": "anchor detection", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.9440666735172272}]}, {"text": " Table 4: Results for arguments", "labels": [], "entities": []}, {"text": " Table 5: Results for Time-* arguments", "labels": [], "entities": []}]}