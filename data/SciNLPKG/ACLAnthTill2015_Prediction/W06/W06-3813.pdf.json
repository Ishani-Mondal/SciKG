{"title": [{"text": "Matching Syntactic-Semantic Graphs for Semantic Relation Assignment", "labels": [], "entities": [{"text": "Semantic Relation Assignment", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8302852908770243}]}], "abstractContent": [{"text": "We present a graph-matching algorithm for semantic relation assignment.", "labels": [], "entities": [{"text": "semantic relation assignment", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.7772032817204794}]}, {"text": "The algorithm is part of an interactive text analysis system.", "labels": [], "entities": []}, {"text": "The system automatically extracts pairs of syntactic units from a text and assigns a semantic relation to each pair.", "labels": [], "entities": []}, {"text": "This is an incremental learning algorithm , in which previously processed pairs and user feedback guide the process.", "labels": [], "entities": []}, {"text": "After each assignment, the system adds to its database a syntactic-semantic graph centered on the main element of each pair of units.", "labels": [], "entities": []}, {"text": "A graph consists of the main unit and all syntactic units with which it is syntactically connected.", "labels": [], "entities": []}, {"text": "An edge contains information both about syntax and about semantic relations for use in further processing.", "labels": [], "entities": []}, {"text": "Syntactic-semantic graph matching is used to produce a list of candidate assignments for 63.75% of the pairs analysed, and in 57% of situations the correct relations is one of the system's suggestions; in 19.6% of situations it suggests only the correct relation.", "labels": [], "entities": [{"text": "Syntactic-semantic graph matching", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6859115560849508}]}], "introductionContent": [{"text": "When analysing texts, it is essential to see how elements of meaning are interconnected.", "labels": [], "entities": []}, {"text": "This is an old idea.", "labels": [], "entities": []}, {"text": "The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 . He was a grammarian who analysed Sanskrit.", "labels": [], "entities": []}, {"text": "The idea resurfaced forcefully at several points in the more recent history of linguistic research.", "labels": [], "entities": []}, {"text": "Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic The sources date his work variously between the 5 th and 7 th century.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6485376556714376}, {"text": "semantic parsing", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7234885692596436}]}, {"text": "Graph-like structures area natural way of organising one's impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes.", "labels": [], "entities": []}, {"text": "In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning).", "labels": [], "entities": []}, {"text": "Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations.Tesn\u00ec ere, who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.", "labels": [], "entities": [{"text": "grouping of verb arguments into actants and circumstances", "start_pos": 152, "end_pos": 209, "type": "TASK", "confidence": 0.7902099788188934}]}, {"text": "This idea was expanded to include nouns and their modifiers through verb nominalizations.", "labels": [], "entities": []}, {"text": "We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser.", "labels": [], "entities": []}, {"text": "Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers.", "labels": [], "entities": []}, {"text": "For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun).", "labels": [], "entities": []}, {"text": "It then tries to find among the previously processed instances another main element with a matching syntactic graph.", "labels": [], "entities": []}, {"text": "If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.", "labels": [], "entities": []}, {"text": "We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.", "labels": [], "entities": []}, {"text": "The list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent (.", "labels": [], "entities": []}, {"text": "Section 2 overviews research in semantic relation analysis.", "labels": [], "entities": [{"text": "semantic relation analysis", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.8973332047462463}]}, {"text": "Section 3 describes the text we used in ex-periments, and the semantic relation list.", "labels": [], "entities": []}, {"text": "Section 4 looks in detail at the graph-matching heuristic.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental setting and shows how often the heuristic was used when processing the input text.", "labels": [], "entities": []}, {"text": "We show in detail our findings about syntactic levels (how often graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9986518025398254}]}, {"text": "Discussion and conclusions appear in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The system processes the 513 sentences interactively.", "labels": [], "entities": []}, {"text": "It begins by running the DIPETT parser.", "labels": [], "entities": [{"text": "DIPETT", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8246623277664185}]}, {"text": "Next, it extracts syntactic units (clauses, phrases, words) and pairs them up according to the information in the parse tree.", "labels": [], "entities": []}, {"text": "Each unit is represented by its headword.", "labels": [], "entities": []}, {"text": "Next, the system checks if the same pair of word lemmas has already been processed, to propose the same relation(s) to the user as options.", "labels": [], "entities": []}, {"text": "If not, the system builds a graph centered on the headword, and proceeds with the matching on previously encountered instances, as described in section 4.", "labels": [], "entities": []}, {"text": "When a set of candidates has been found, the system goes through a dialogue with the user.", "labels": [], "entities": []}, {"text": "The system generated 2020 pairs from the 513 sentences.", "labels": [], "entities": []}, {"text": "The experiment was run in 5 interactive sessions of approximately 3 hours each.", "labels": [], "entities": []}, {"text": "The total net processing time was 6 hours, 42 minutes and 52 seconds 4 . While it would have been instructive to run the system several times with different users, it was not feasible.", "labels": [], "entities": []}, {"text": "The experiment was run once, with two cooperating users.", "labels": [], "entities": []}, {"text": "They were instructed on the set of semantic relations, and told how the system works.", "labels": [], "entities": []}, {"text": "They discussed the semantic relation assignment and, once agreed, compared the system's suggestion with their decision.", "labels": [], "entities": [{"text": "semantic relation assignment", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.640919049580892}]}, {"text": "DIPETT did not produce a complete parse for all sentences.", "labels": [], "entities": [{"text": "DIPETT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9509851932525635}]}, {"text": "When a complete parse (correct or incorrect) was not possible, DIPETT produced fragmentary parses.", "labels": [], "entities": []}, {"text": "The semantic analyser extracted units even from tree fragments, although sometimes the fragments were too small to accommodate any pairs.", "labels": [], "entities": []}, {"text": "Of the 513 input sentences, 441 had a parse tree that allowed the system to extract pairs.", "labels": [], "entities": []}, {"text": "Of 2020 pairs generated, the users discarded 545 in the dialogue step.", "labels": [], "entities": []}, {"text": "An example of an erroneous pair comes from the sentence: Tiny clouds drift across like feathers on parade.", "labels": [], "entities": []}, {"text": "The semantic analyser produced the pair (drift,parade), because of a parsing error: parade was parsed as a complement of drift, instead of a post-modifier for feathers.", "labels": [], "entities": []}, {"text": "The correct pairing (feather,parade) is missing, because it cannot be inferred from the parse tree.", "labels": [], "entities": []}, {"text": "gives a summary of the processing statistics.", "labels": [], "entities": []}, {"text": "We observe that graph-matching was used to process a clear majority of the total pairs extracted -63.25% (933/1475) , leaving the remaining 36.75% for the other two heuristics and for cases where no suggestion could be made.", "labels": [], "entities": []}, {"text": "In 57.02% of the situations when graph-matching was used, the system's suggestion contained the correct answer (user's action was either accept or choose), and in 19.61% of the situations a single correct semantic relation was proposed (user action was accept).", "labels": [], "entities": []}, {"text": "When the system presents multiple suggestions to the user, including the correct one, the average number of suggestions is 3.75.", "labels": [], "entities": []}, {"text": "The small number of suggestions shows that the system does not simply add to the list relations that it has previously encountered, but it learns from past experience and graphmatching helps it make good selections.", "labels": [], "entities": []}, {"text": "plots the difference between the number of examples for which the system gives the correct answer (possibly among other suggestions) and the number of examples when the user must supply the correct relation, from the first example processed until the end of the experiment.", "labels": [], "entities": []}, {"text": "We observe a steady increase in the number of correctly processed examples.", "labels": [], "entities": []}, {"text": "Our system does not differentiate between syntactic levels, but based on the structures of the syntactic units in each pair we can decide which syntactic level it pertains to.", "labels": [], "entities": []}, {"text": "For a more in-depth analysis, we have separated the results for each syntactic level,: Difference between the number of situations in which the user accepts or chooses from the system's suggestions, and when it must supply the correct relation and present them for comparison in.", "labels": [], "entities": []}, {"text": "We observe that the intra-clause level -verbs and their arguments -makes the best use of graphmatching, with the curve showing the cumulative number of situations in which the system makes correct predictions becoming steeper as more text is processed.", "labels": [], "entities": []}, {"text": "At the same time, the curve that plots the cumulative number of cases in which the user has to supply a correct answer begins to level off.", "labels": [], "entities": []}, {"text": "As expected, at the noun-phrase level where the syntactic structures are very simple, often consisting of only the noun and its modifier (without even a connective), the graph-matching algorithm does not help as much.", "labels": [], "entities": []}, {"text": "At the inter-clause level the heuristic helps, as shown by the marginally higher curve for cumulative accept/choose user actions, compared to supply actions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of semantic analysis", "labels": [], "entities": [{"text": "Summary of semantic analysis", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6817414164543152}]}]}