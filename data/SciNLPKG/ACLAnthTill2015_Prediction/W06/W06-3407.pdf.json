{"title": [], "abstractContent": [{"text": "We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial and final contributions.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7198493927717209}]}, {"text": "Our evaluation shows that this hybrid approach outperforms state-of-the-art algorithms even when applied to loosely structured, spontaneous dialogue.", "labels": [], "entities": []}, {"text": "Further analysis reveals that using dialogue exchanges versus dialogue contributions improves topic segmentation quality.", "labels": [], "entities": [{"text": "topic segmentation quality", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.7628764510154724}]}], "introductionContent": [{"text": "In this paper we explore the problem of topic segmentation of dialogue.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7476134896278381}]}, {"text": "Use of topic-based models of dialogue has played a role in information retrieval (), information extraction, and summarization, just to name a few applications.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.7823180854320526}, {"text": "information extraction", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.8913460969924927}, {"text": "summarization", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.993536651134491}]}, {"text": "However, most previous work on automatic topic segmentation has focused primarily on segmentation of expository text.", "labels": [], "entities": [{"text": "automatic topic segmentation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.603684534629186}]}, {"text": "This paper presents a survey of the state-of-the-art in topic segmentation technology.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7252480089664459}]}, {"text": "Using the definition of topic segment from (Passonneau and Litman, 1993) applied to two different dialogue corpora, we present an evaluation including a detailed error analysis, illustrating why approaches designed for expository text do not generalize well to dialogue.", "labels": [], "entities": []}, {"text": "We first demonstrate a significant advantage of our hybrid, supervised learning approach called Museli, a multi-source evidence integration approach, over competing algorithms.", "labels": [], "entities": []}, {"text": "We then extend the basic Museli algorithm by introducing an intermediate level of analysis based on Sinclair and Coulthard's notion of a dialogue exchange.", "labels": [], "entities": []}, {"text": "We show that both our baseline and Museli approaches obtain a significant improvement when using perfect, handlabeled dialogue exchanges, typically in the order of 2-3 contributions, as the atomic discourse unit in comparison to using the contribution as the unit of analysis.", "labels": [], "entities": []}, {"text": "We further evaluate our success towards automatic classification of exchange boundaries using the same Museli framework.", "labels": [], "entities": [{"text": "Museli framework", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.9146623909473419}]}], "datasetContent": [{"text": "We evaluate Museli in comparison to the best performing state-of-the-art approaches, demonstrating that our hybrid Museli approach outperforms all of these approaches on two different dialogue corpora by a statistically significant margin (p < .01), in one case reducing the probability of error, as measured by P k), to about 10%.", "labels": [], "entities": []}, {"text": "We used two different dialogue corpora from the educational domain for our evaluation.", "labels": [], "entities": []}, {"text": "Both corpora constitute of dialogues between a student and a tutor (speakers with asymmetric roles) and both were collected via chat software.", "labels": [], "entities": []}, {"text": "The first corpus, which we call the Olney & Cai corpus, is a set of dialogues selected randomly from the same corpus Olney and Cai obtained their corpus from).", "labels": [], "entities": [{"text": "Olney & Cai corpus", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.8622779846191406}]}, {"text": "The dialogues discuss problems related to Newton's Three Laws of Motion.", "labels": [], "entities": []}, {"text": "The second corpus, the Thermo corpus, is a locally collected corpus of thermodynamics tutoring dialogues, in which tutor-student pairs work together to solve an optimization task.", "labels": [], "entities": [{"text": "Thermo corpus", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.7506240606307983}]}, {"text": "shows corpus statistics from both corpora.", "labels": [], "entities": []}, {"text": "Both corpora seem adequate for attempting to harness systematic differences in how speakers with asymmetric roles may initiate or close topic segments.", "labels": [], "entities": []}, {"text": "The Thermo corpus is particularly appropriate for addressing the research question of how to automatically segment natural, spontaneous dialogue.", "labels": [], "entities": [{"text": "Thermo corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8784479796886444}, {"text": "segment natural, spontaneous dialogue", "start_pos": 107, "end_pos": 144, "type": "TASK", "confidence": 0.7076358556747436}]}, {"text": "The exploratory task is more loosely structured than many task-oriented domains investigated in the dialogue community, such as flight reservation or meeting scheduling.", "labels": [], "entities": [{"text": "flight reservation", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.8036419153213501}, {"text": "meeting scheduling", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7094099223613739}]}, {"text": "Students can interrupt with questions and tutors can digress in anyway they feel may benefit the completion of the task.", "labels": [], "entities": []}, {"text": "In the Olney and Cai corpus, the same 10 physics problems are addressed in each session and the interaction is almost exclusively a tutor initiation followed by student response, evident from the nearly equal number of student and tutor contributions.", "labels": [], "entities": [{"text": "Olney and Cai corpus", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.8941837251186371}]}, {"text": "To show the value of dialogue exchanges in topic segmentation, in this section we re-formulate our problem from classifying contributions into NEW_TOPIC and SAME_TOPIC to classifying exchange initial contributions into NEW_TOPIC and SAME_TOPIC.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7322398871183395}, {"text": "NEW_TOPIC", "start_pos": 143, "end_pos": 152, "type": "DATASET", "confidence": 0.851244330406189}, {"text": "NEW_TOPIC", "start_pos": 219, "end_pos": 228, "type": "DATASET", "confidence": 0.8399837811787924}]}, {"text": "For all algorithms, we consider only predictions that coincide with handcoded exchange initial contributions.", "labels": [], "entities": []}, {"text": "We show that, except for our own Museli approach, using exchange boundaries improves segmentation quality across all algorithms (p < .05) when compared to their respective counterparts that ignore exchanges.", "labels": [], "entities": []}, {"text": "Using exchanges gives the Museli approach a significant advantage based on F-measure (p < .05), but only a marginally significant advantage based on P k.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9981346130371094}]}, {"text": "These results confirm our intuition that what gives our Museli approach an advantage over baseline algorithms is its ability to harness the lexical, syntactic, and phrasal cues that mark shifts in topic.", "labels": [], "entities": []}, {"text": "Given that shift-in-topic correlates highly with shift-in-exchange, these features are discriminatory in both respects.", "labels": [], "entities": []}, {"text": "Of the degenerate strategies in section 5.2, only ALL lends itself to our reformulation of the topic segmentation problem.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.6836050599813461}]}, {"text": "For the ALL heuristic, we classify all exchange initial contributions into NEW_TOPIC.", "labels": [], "entities": [{"text": "NEW_TOPIC", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.6314173936843872}]}, {"text": "This degenerate heuristic alone produces better results than all algorithms classifying utterances).", "labels": [], "entities": []}, {"text": "In our implementation of TextTiling (TT) with exchanges, we only consider predictions on contributions that coincide with exchange initial contributions, while ignoring predictions made on contributions that do not introduce anew exchange.", "labels": [], "entities": []}, {"text": "Consistent with our evaluation methodology from Section 5, we optimized the window size using the entire corpus and found an optimal window size of 13 contributions.", "labels": [], "entities": []}, {"text": "Without exchanges, the optimal window size was 6 contributions.", "labels": [], "entities": []}, {"text": "The higher optimal window-size hints to the possibility that by using exchange initial contributions an approach based on lexical cohesion may broaden its horizon without losing precision..", "labels": [], "entities": [{"text": "precision.", "start_pos": 178, "end_pos": 188, "type": "METRIC", "confidence": 0.9953839182853699}]}, {"text": "Results using perfect exchange boundaries In this version of B&L, we use exchanges to build the initial clusters (states) and the final HMM.", "labels": [], "entities": []}, {"text": "B&L with exchanges significantly improves over B&L with contributions, in terms of both P k and F-measure (p < .005) and significantly improves over our ALL heuristic (where all exchange initial contributions introduce anew topic) in terms of P k (p < .0005).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9611238241195679}]}, {"text": "Thus, its use of exchanges goes beyond merely narrowing the space of possible NEW_TOPIC contributions: it also uses these more coarse-grained discourse units to build a more thematically-motivated topic model.", "labels": [], "entities": []}, {"text": "Foltz's and Olney and Cai's (Ortho) approach both use an LSA space trained on the dialogue corpus.", "labels": [], "entities": []}, {"text": "Instead of training the LSA space with individual contributions, we train the LSA space using exchanges.", "labels": [], "entities": []}, {"text": "We hope that by training the space with more contentful text units LSA might capture more topically-meaningful semantic relations.", "labels": [], "entities": []}, {"text": "In addition, only exchange initial contributions where used for the logistic regression training phase.", "labels": [], "entities": []}, {"text": "Thus, we aim to learn the regression equation that best discriminates between exchange initial contributions that introduce a topic and those that do not.", "labels": [], "entities": []}, {"text": "Both Foltz and Ortho improve over their non exchange counterparts, but neither improves over the ALL heuristic by a significant margin.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Evaluation Corpora Statistics", "labels": [], "entities": []}, {"text": " Table 3. Results on both corpora", "labels": [], "entities": []}, {"text": " Table 4. Results using perfect exchange boundaries", "labels": [], "entities": []}]}