{"title": [{"text": "Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output", "labels": [], "entities": [{"text": "Automatic Error Analysis of Statistical Machine Translation Output", "start_pos": 33, "end_pos": 99, "type": "TASK", "confidence": 0.8085471019148827}]}], "abstractContent": [{"text": "Evaluation of machine translation output is an important but difficult task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7476041615009308}]}, {"text": "Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 101, "end_pos": 122, "type": "METRIC", "confidence": 0.8678022821744283}, {"text": "Position Independent Word Error Rate (PER)", "start_pos": 124, "end_pos": 166, "type": "METRIC", "confidence": 0.750626876950264}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9912264943122864}]}, {"text": "However, these measures do not give any details about the nature of translation errors.", "labels": [], "entities": [{"text": "translation errors", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8465731739997864}]}, {"text": "Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts.", "labels": [], "entities": []}, {"text": "On the other hand, human evaluation is a time consuming and expensive task.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.5893789529800415}]}, {"text": "In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements.", "labels": [], "entities": [{"text": "WER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.6870752573013306}, {"text": "PER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9831523895263672}]}], "introductionContent": [{"text": "The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.8526006639003754}]}, {"text": "Automatic evaluation is preferred because human evaluation is a time consuming and expensive task.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6896659731864929}]}, {"text": "A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system.", "labels": [], "entities": []}, {"text": "The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score) and the NIST score).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 25, "end_pos": 46, "type": "METRIC", "confidence": 0.8486465712388357}, {"text": "Position Independent Word Error Rate (PER)", "start_pos": 48, "end_pos": 90, "type": "METRIC", "confidence": 0.8357648625969887}, {"text": "BLEU score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.985346108675003}, {"text": "NIST score", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.7867460250854492}]}, {"text": "However, none of these measures give any details about the nature of translation errors.", "labels": [], "entities": [{"text": "translation errors", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8549536168575287}]}, {"text": "A relationship between these error measures and the actual errors in the translation outputs is not easy to find.", "labels": [], "entities": []}, {"text": "Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9481671452522278}]}, {"text": "A framework for human error analysis and error classification has been proposed in), but like human evaluation, this is also a time consuming task.", "labels": [], "entities": [{"text": "human error analysis", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.6360097428162893}, {"text": "error classification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.6747236996889114}]}, {"text": "The goal of this work is to present a framework for automatic error analysis of machine translation output based on morpho-syntactic information.", "labels": [], "entities": [{"text": "automatic error analysis of machine translation output", "start_pos": 52, "end_pos": 106, "type": "TASK", "confidence": 0.7208550018923623}]}], "datasetContent": [{"text": "We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the translation errors.", "labels": [], "entities": [{"text": "WER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9954454898834229}, {"text": "PER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9923884272575378}]}, {"text": "We investigate two types of potential problems for the translation with the Spanish-English language pair: \u2022 syntactic differences between the two languages considering nouns and adjectives \u2022 inflections in the Spanish language considering mainly verbs, adjectives and nouns As any other automatic evaluation measures, these novel measures will be far from perfect.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9700887799263}]}, {"text": "Possible POS-tagging errors may introduce additional noise.", "labels": [], "entities": []}, {"text": "However, we expect this noise to be sufficiently small and the new measures to be able to give sufficiently clear ideas about particular errors.", "labels": [], "entities": []}, {"text": "The translation experiments have been done in both translation directions on both sizes of the corpus.", "labels": [], "entities": []}, {"text": "In order to examine improvements of the baseline system, anew system with POS-based word reorderings of nouns and adjectives as proposed in) is also analysed.", "labels": [], "entities": [{"text": "POS-based word reorderings of nouns and adjectives", "start_pos": 74, "end_pos": 124, "type": "TASK", "confidence": 0.7394269449370248}]}, {"text": "Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round.", "labels": [], "entities": []}, {"text": "Therefore, local reorderings of nouns and ad- jective groups in the source language have been applied.", "labels": [], "entities": []}, {"text": "If the source language is Spanish, each noun is moved behind the corresponding adjective group.", "labels": [], "entities": []}, {"text": "If the source language is English, each adjective group is moved behind the corresponding noun.", "labels": [], "entities": []}, {"text": "An adverb followed by an adjective (e.g. \"more important\") or two adjectives with a coordinate conjunction in between (e.g. \"economic and political\") are treated as an adjective group.", "labels": [], "entities": []}, {"text": "Standard translation results are presented in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics for the Spanish-English  EPPS task (running words include punctuation  marks)", "labels": [], "entities": [{"text": "Spanish-English  EPPS task", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.4334595302740733}]}, {"text": " Table 2: Translation Results [%]", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.875907838344574}]}, {"text": " Table 3: Relative difference between PER and  WER [%] for different word classes", "labels": [], "entities": [{"text": "Relative difference", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9684597849845886}, {"text": "PER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.7286558747291565}, {"text": "WER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9260757565498352}]}, {"text": " Table 4: PER [%] for different word classes", "labels": [], "entities": [{"text": "PER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9935302138328552}]}, {"text": " Table 5: Relative difference between PER of base  forms and PER of full forms [%] for the Spanish  output", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9919445514678955}, {"text": "PER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.978879988193512}, {"text": "PER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9913609623908997}]}]}