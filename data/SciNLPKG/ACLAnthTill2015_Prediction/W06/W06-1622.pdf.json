{"title": [{"text": "Semantic Role Labeling via Instance-Based Learning", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7807146509488424}]}], "abstractContent": [{"text": "This paper demonstrates two methods to improve the performance of instance-based learning (IBL) algorithms for the problem of Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.7883723725875219}]}, {"text": "Two IBL algorithms are utilized: k-Nearest Neighbor (kNN), and Priority Maximum Likelihood (PML) with a modified back-off combination method.", "labels": [], "entities": [{"text": "Priority Maximum Likelihood (PML)", "start_pos": 63, "end_pos": 96, "type": "METRIC", "confidence": 0.8264833390712738}]}, {"text": "The experimental data are the WSJ23 and Brown Corpus test sets from the CoNLL-2005 Shared Task.", "labels": [], "entities": [{"text": "WSJ23", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9467191696166992}, {"text": "Brown Corpus test sets", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.9512372016906738}, {"text": "CoNLL-2005 Shared Task", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.8532713452974955}]}, {"text": "It is shown that applying the Tree-Based Predicate-Argument Recognition Algorithm (PARA) to the data as a preprocessing stage allows kNN and PML to deliver F 1 : 68.61 and 71.02 respectively on the WSJ23, and F 1 : 56.96 and 60.55 on the Brown Corpus; an increase of 8.28 in F 1 measurement over the most recent published PML results for this problem (Palmer et al., 2005).", "labels": [], "entities": [{"text": "F 1", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9796259999275208}, {"text": "WSJ23", "start_pos": 198, "end_pos": 203, "type": "DATASET", "confidence": 0.9869136214256287}, {"text": "F 1", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.9814779162406921}, {"text": "Brown Corpus", "start_pos": 238, "end_pos": 250, "type": "DATASET", "confidence": 0.9860781729221344}, {"text": "F 1 measurement", "start_pos": 275, "end_pos": 290, "type": "METRIC", "confidence": 0.948379377524058}]}, {"text": "Training times for IBL algorithms are very much faster than for other widely used techniques for SRL (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of PARA yield testing and processing speeds of around 1.0 second per sentence for kNN and 0.9 second per sentence for PML respectively, suggesting that IBL could be a more practical way to perform SRL for NLP applications where it is employed; such as real-time Machine Translation or Automatic Speech Recognition.", "labels": [], "entities": [{"text": "SRL", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9933872818946838}, {"text": "PARA", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.8350566029548645}, {"text": "SRL", "start_pos": 391, "end_pos": 394, "type": "TASK", "confidence": 0.993980884552002}, {"text": "Machine Translation", "start_pos": 456, "end_pos": 475, "type": "TASK", "confidence": 0.6859928369522095}, {"text": "Automatic Speech Recognition", "start_pos": 479, "end_pos": 507, "type": "TASK", "confidence": 0.5988453427950541}]}], "introductionContent": [{"text": "The proceedings from CoNLL2004 and CoNLL2005 detail a wide variety of approaches to Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "CoNLL2004", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.9318835735321045}, {"text": "CoNLL2005", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.9220516085624695}, {"text": "Semantic Role Labeling (SRL)", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.8375405271848043}]}, {"text": "Many research efforts utilize machine learning (ML) approaches; such as support vector machines), perceptrons (), the SNoW learning architecture (), EMbased clustering (), transformation-based learning), memory-based learning, and inductive learning (.", "labels": [], "entities": []}, {"text": "This paper compares two instance-based learning approaches, kNN and PML.", "labels": [], "entities": []}, {"text": "The PML method used here utilizes a modification of the backoff lattice method used by to use a set of basic features-specifically, the features employed for learning in this paper are Predicate (pr), Voice (vo), Phrase Type (pt), Distance (di), Head Word (hw), Path (pa), Preposition in a PP (pp), and an \"Actor\" heuristic.", "labels": [], "entities": []}, {"text": "The general approach presented here is an example of memory-based learning.", "labels": [], "entities": []}, {"text": "Many existing SRL systems are also memory-based (), implemented using TilMBL software (http://ilk.kub.nl/software.html) with advanced methods such as Feature Weighting, and so forth.", "labels": [], "entities": [{"text": "SRL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9682396054267883}, {"text": "Feature Weighting", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.6820845752954483}]}, {"text": "This paper measures the performance of kNN and PML for comparison in terms of accuracy and processing speed, both against each other and against previously published results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992775321006775}]}], "datasetContent": [{"text": "The research outlined here uses the dataset released by the CoNLL-05 Shared Task (http://www.lsi.upc.edu/~srlconll/soft.html).", "labels": [], "entities": [{"text": "CoNLL-05 Shared Task", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.8055999676386515}]}, {"text": "It includes several Wall Street Journal sections with parse-trees from both Charniak's (2000) parser and Collins' (1999) parser.", "labels": [], "entities": [{"text": "Wall Street Journal sections", "start_pos": 20, "end_pos": 48, "type": "DATASET", "confidence": 0.9452288001775742}]}, {"text": "These sections are also part of the PropBank corpus (http://www.cis.upenn.edu/~treebank).", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9631117582321167}]}, {"text": "WSJ sections 20 and 21 (with Charniak's parses) were used as test data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9410059452056885}]}, {"text": "PARA operates directly on the parse tree.", "labels": [], "entities": [{"text": "PARA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.3935023248195648}]}, {"text": "Evaluation is carried out using precision, recall and F 1 measures of assignmentaccuracy of predicated arguments.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996092915534973}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9996428489685059}, {"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9933365881443024}]}, {"text": "Precision (p) is the proportion of arguments predicated by the system that are correct.", "labels": [], "entities": [{"text": "Precision (p)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9575421363115311}]}, {"text": "Recall (r) is the proportion of correct arguments in the dataset that are predicated by the system.", "labels": [], "entities": [{"text": "Recall (r)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9640984535217285}]}, {"text": "Finally, the F 1 measure computes the harmonic mean of precision and recall, such that F 1 =2*p*r / (p+r), and is the most commonly used primary measure when comparing different SRL systems.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9460156361262003}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9986979961395264}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9980675578117371}, {"text": "F 1", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9809670448303223}, {"text": "SRL", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.9458288550376892}]}, {"text": "For consistency, the performance of PARA for boundary recognition is tested using the official evaluation script from  Experimental results were obtained for part of the Brown corpus (the part provided by) and for Wall Street Journal (WSJ) Sections 21, 23, and 24 using different training data sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to 21) shown in.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9478421807289124}, {"text": "boundary recognition", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.8365235328674316}, {"text": "Brown corpus", "start_pos": 170, "end_pos": 182, "type": "DATASET", "confidence": 0.9747426211833954}, {"text": "Wall Street Journal (WSJ) Sections 21", "start_pos": 214, "end_pos": 251, "type": "DATASET", "confidence": 0.9405450075864792}, {"text": "WSJ", "start_pos": 300, "end_pos": 303, "type": "DATASET", "confidence": 0.9479740262031555}, {"text": "WSJ", "start_pos": 308, "end_pos": 311, "type": "DATASET", "confidence": 0.8955081105232239}, {"text": "WSJ", "start_pos": 326, "end_pos": 329, "type": "DATASET", "confidence": 0.9200780987739563}]}, {"text": "There are two tasks, Role classification with known arguments as input, and Boundary recognition & Role classification with gold (hand-corrected) parses or auto (Charniak's) parses.", "labels": [], "entities": [{"text": "Role classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.9273755550384521}, {"text": "Boundary recognition", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.8743566274642944}, {"text": "Role classification", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.8247428834438324}]}, {"text": "In addition, execution speed, the learning curve, and some further results for exploration of kNN and PML are also included below.", "labels": [], "entities": []}, {"text": "shows the results from kNN and PML with known boundaries/arguments (i.e. the systems are given the correct arguments for role classification).", "labels": [], "entities": [{"text": "role classification", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.8260837197303772}]}, {"text": "All training datasets (WSJ02-21) include Charniak's parse trees.", "labels": [], "entities": [{"text": "WSJ02-21", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.9199491739273071}]}, {"text": "The table shows that PML achieves F1: 2.69 better than kNN.", "labels": [], "entities": [{"text": "PML", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.531844973564148}, {"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.999365508556366}]}, {"text": "shows that performance improves as more training data is provided; and that PML outperforms kNN by about F 1 :2.8 on average for WSJ 24 for the three different training sets, mainly because the backoff lattice improves both recall and precision.", "labels": [], "entities": [{"text": "PML", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.8253918886184692}, {"text": "recall", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.9991119503974915}, {"text": "precision", "start_pos": 235, "end_pos": 244, "type": "METRIC", "confidence": 0.995905876159668}]}, {"text": "The table shows that it is not always beneficial to include all features for labeling all roles.", "labels": [], "entities": []}, {"text": "While P(r | hw, pt, pre, pp) is mainly for adjunctive roles (e.g. AM-TMP), P(r | pt, di, vo, pr, pp) is mainly for core roles (e.g. A0).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. There are two tasks, Role  classification with known arguments as input,  and Boundary recognition & Role classification  with gold (hand-corrected) parses or auto  (Charniak's) parses. In addition, execution speed,  the learning curve, and some further results for  exploration of kNN and PML are also included  below.", "labels": [], "entities": [{"text": "Role  classification", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9053798317909241}, {"text": "Boundary recognition", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7478426396846771}, {"text": "Role classification", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7850038111209869}]}, {"text": " Table 3 shows the contribution of each feature  and the actor heuristic by excluding one feature  or heuristic. It indicates that Head Word, Prepo- sition, and Distance are the three features that  contribute most to system accuracy, and the addi- tional Actor heuristic is fourth. Path, Phrase type  and Voice are the three features contibuting the  least for both classification algorithms.", "labels": [], "entities": [{"text": "Prepo- sition", "start_pos": 142, "end_pos": 155, "type": "METRIC", "confidence": 0.9166857997576395}, {"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9908928871154785}]}, {"text": " Table 5. Illustration of results for execution  time by kNN and PML on WSJ 24 with known  arguments", "labels": [], "entities": [{"text": "kNN", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8663301467895508}, {"text": "WSJ 24", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.9543664455413818}]}, {"text": " Table 9. Illustration of results with different training datasets by kNN (k=1) and PML on Brown Cor- pus with Charniak's parses and PARA", "labels": [], "entities": [{"text": "PML", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9974266886711121}, {"text": "PARA", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9962873458862305}]}]}