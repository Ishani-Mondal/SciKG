{"title": [{"text": "Noun Phrase Generation for Situated Dialogs", "labels": [], "entities": [{"text": "Noun Phrase Generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6940890550613403}]}], "abstractContent": [{"text": "We report on a study examining the generation of noun phrases within a spoken dialog agent fora navigation domain.", "labels": [], "entities": []}, {"text": "The task is to provide real-time instructions that direct the user to move between a series of destinations within a large interior space.", "labels": [], "entities": []}, {"text": "A subtask within sentence planning is determining what form to choose for noun phrases.", "labels": [], "entities": []}, {"text": "This choice is driven by both the discourse history and spatial context features derived from the direction-follower's position, e.g. his view angle, distance from the target referent and the number of similar items in view.", "labels": [], "entities": []}, {"text": "The algorithm was developed as a decision tree and its output was evaluated by a group of human judges who rated 62.6% of the expressions generated by the system to be as good as or better than the language originally produced by human dialog partners.", "labels": [], "entities": []}], "introductionContent": [{"text": "In today's world of mobile, context-aware computing, intelligent software agents are being deployed in a wide variety of domains to aid humans in performing navigation tasks.", "labels": [], "entities": []}, {"text": "Examples include hand-held tourist information portals () campus tour guides (), direction-giving avatars for visitors to a building (), in-car driving direction systems (), and pedestrian navigation systems).", "labels": [], "entities": [{"text": "pedestrian navigation", "start_pos": 178, "end_pos": 199, "type": "TASK", "confidence": 0.7430281043052673}]}, {"text": "These applications present an exciting and challenging new frontier for dialog agents, since attributes of the real-world setting must be combined with other contextual factors for the agent to communicate successfully.", "labels": [], "entities": []}, {"text": "In the current work, we focus on a scenario in which the system provides incremental directions to a mobile user who is following the instructions as they are produced.", "labels": [], "entities": []}, {"text": "Unlike the rigid directions produced by applications like Mapquest, 1 which describes the entire route from start to finish, this task requires realtime instructions issued while monitoring the user's progress.", "labels": [], "entities": []}, {"text": "Instructions are based on dynamic local context variables such as the visibility of and distance to reference points.", "labels": [], "entities": []}, {"text": "In referring to items in the setting, human speakers produce a wide variety of noun phrase forms, including descriptions that are headed by a common noun and that employ a definite, indefinite, or demonstrative determiner, one anaphors, and pronouns such as it, this and that.", "labels": [], "entities": []}, {"text": "Our goal in the current work is to model that entire space of variation, which makes the task more difficult than the noun phrase generation task defined in many previous studies that simplify the alternatives down to description or pronoun.", "labels": [], "entities": [{"text": "noun phrase generation", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.666605939467748}]}, {"text": "In order to study this process, we developed a task domain in which a human partner is directed through an interior space (a graphically-presented 3D virtual world) to perform a sequence of manipulation tasks.", "labels": [], "entities": []}, {"text": "In the first stages of the work, we collected and annotated a corpus of human-human dialogs from this domain.", "labels": [], "entities": []}, {"text": "Then, using this data, we trained a decision-tree classifier to utilize context variables such as distance, target object visibility, discourse history, etc., to determine lexical properties of referring expressions to be produced by the generation component of our dialog system.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report several methods of evaluating the NP frames produced using the process given by the decision trees.", "labels": [], "entities": []}, {"text": "First, we report the results of a strict evaluation in which the system's output must exactly match expressions produced by the human subjects.", "labels": [], "entities": []}, {"text": "We also compare this result with a hand-crafted Centering-style generation algorithm.", "labels": [], "entities": [{"text": "Centering-style generation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.8233956396579742}]}, {"text": "Requiring the algorithm to exactly match human performance is an overly-strict criterion, since in many contexts several possible referring expression forms could be equally felicitous in a given context, so we also conducted a human judgment study.", "labels": [], "entities": []}, {"text": "The 5 test dialogs contain 295 target expressions.", "labels": [], "entities": []}, {"text": "The output of the decision tree classifier was compared to the expressions observed in the test dialog.", "labels": [], "entities": []}, {"text": "reports the results of this evaluation.", "labels": [], "entities": []}, {"text": "The accuracy obtained was 31.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997938275337219}]}, {"text": "The most frequent tag gives a 20.0% baseline performance using this strict match criterion.", "labels": [], "entities": []}, {"text": "Evaluating generation studies by calculating their similarity to human spontaneous speech may not be the ideal performance metric, since several different realizations maybe equally felicitous in a  given context.", "labels": [], "entities": []}, {"text": "Therefore, we also performed a human judgement evaluation.", "labels": [], "entities": []}, {"text": "In this evaluation, judges compared the NPs generated by our algorithm to the NPs produced by human subjects, and to NPs with randomly generated feature assignments.", "labels": [], "entities": []}, {"text": "Judges viewed test NPs in the context of the original test corpus.", "labels": [], "entities": []}, {"text": "To re-create the context in which the original expression was produced, the video, audio, and dialog transcript were played for the judges using the Anvil annotation tool).", "labels": [], "entities": []}, {"text": "The judges could play or pause the video as they wished.", "labels": [], "entities": []}, {"text": "Using the word-alignments established during the data annotation phase, the audio of the test NPs was replaced by silence, and the words were removed in the transcript shown in the timeline viewer.", "labels": [], "entities": []}, {"text": "For each test item, the judges were presented with a selection box showing two possible referring expressions that they were asked to compare using a qualitative ranking (option 1 is better, option 2 is better, or they are equal), given a particular target ID and the context.", "labels": [], "entities": []}, {"text": "shows a screen-shot of the judges' annotation tool.", "labels": [], "entities": []}, {"text": "The judges did not know the source of the expressions they evaluated (system, human production, or random).", "labels": [], "entities": []}, {"text": "The 10 judges were volunteers from the university community who were self-identified native speakers of English.", "labels": [], "entities": []}, {"text": "They were not compensated for their time.", "labels": [], "entities": []}, {"text": "The decision tree selected NP-frame slot values which were converted into realized NPs.", "labels": [], "entities": []}, {"text": "The Det and Head choices were directly translated into surface forms (for Head=noun we chose a consistent common noun for each semantic class: but-  ton, door or cabinet.", "labels": [], "entities": []}, {"text": "If the system's selection of Mod feature matched the value from the corpus, we used the expression produced by the original speaker.", "labels": [], "entities": []}, {"text": "If the original expression did not include a modifier, but the system selected Mod:+, we lexicalized this feature to a simple but correct spatial description like on the right, on the left or in front.", "labels": [], "entities": []}, {"text": "shows the results of human judging.", "labels": [], "entities": []}, {"text": "The system's output was either equal or preferred to the original spontaneous language in 62.6% of cases where these two choices were compared directly.", "labels": [], "entities": []}, {"text": "Interestingly, the randomly-generated choice was preferred over the original spontaneous language in 13.0% of trials, and was preferred over the system's output in 22.5% of trials.", "labels": [], "entities": []}, {"text": "Aggregating overall judges, the system's performance was judged to be much better than random, but not as good as the original human language.", "labels": [], "entities": []}, {"text": "Trials were balanced among judges so that each target item was seen by four judges: with two comparing the system's response to the original human language, one comparing the system to random, and one comparing the human to random.", "labels": [], "entities": []}, {"text": "There were 282 trials for which 2 judges saw the identical pair of choices.", "labels": [], "entities": []}, {"text": "Out of these, the two judges' responses agreed in 197 cases, producing an inter-annotator reliability (kappa score) of 0.51, with raw agreement of 69% and expected agreement of 37%.", "labels": [], "entities": [{"text": "inter-annotator reliability (kappa score)", "start_pos": 74, "end_pos": 115, "type": "METRIC", "confidence": 0.8116420408089956}, {"text": "agreement", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.6629540920257568}, {"text": "agreement", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.7229085564613342}]}, {"text": "Although this is a relatively low kappa value, we believe that the aggregate judgments of all of the judges overall of the test items are still informative, since the scores of items for which we have two judgements follow a very sim-ilar pattern to the overal distribution of responses.", "labels": [], "entities": []}, {"text": "The low inter-annotator agreement maybe due to the substitutability of the expressions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The Context Features Used by the Algorithm", "labels": [], "entities": []}, {"text": " Table 4: Classifier results using Exact-match criterion", "labels": [], "entities": []}, {"text": " Table 5: Comparison to Coherence-based Generation", "labels": [], "entities": []}, {"text": " Table 6: Results of Human Judging", "labels": [], "entities": [{"text": "Human Judging", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.6614634245634079}]}]}