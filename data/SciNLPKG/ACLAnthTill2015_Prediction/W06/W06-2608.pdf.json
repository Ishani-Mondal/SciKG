{"title": [{"text": "Syntagmatic Kernels: a Word Sense Disambiguation Case Study", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6537589828173319}]}], "abstractContent": [{"text": "In this paper we present a family of kernel functions, named Syntagmatic Kernels , which can be used to model syn-tagmatic relations.", "labels": [], "entities": []}, {"text": "Syntagmatic relations hold among words that are typically collo-cated in a sequential order, and thus they can be acquired by analyzing word sequences.", "labels": [], "entities": []}, {"text": "In particular, Syntagmatic Kernels are defined by applying a Word Sequence Kernel to the local contexts of the words to be analyzed.", "labels": [], "entities": []}, {"text": "In addition, this approach allows us to define a semi supervised learning schema where external lexical knowledge is plugged into the supervised learning process.", "labels": [], "entities": []}, {"text": "Lexical knowledge is acquired from both unlabeled data and handmade lexical resources, such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9653141498565674}]}, {"text": "We evaluated the syntagmatic kernel on two standard Word Sense Dis-ambiguation tasks (i.e. English and Ital-ian lexical-sample tasks of Senseval-3), where the syntagmatic information plays a crucial role.", "labels": [], "entities": []}, {"text": "We compared the Syntag-matic Kernel with the standard approach, showing promising improvements in performance .", "labels": [], "entities": [{"text": "Syntag-matic Kernel", "start_pos": 16, "end_pos": 35, "type": "DATASET", "confidence": 0.7608334124088287}]}], "introductionContent": [{"text": "In computational linguistics, it is usual to deal with sequences: words are sequences of letters and syntagmatic relations are established by sequences of words.", "labels": [], "entities": []}, {"text": "Sequences are analyzed to measure morphological similarity, to detect multiwords, to represent syntagmatic relations, and soon.", "labels": [], "entities": []}, {"text": "Hence modeling syntagmatic relations is crucial fora wide variety of NLP tasks, such as Named Entity Recognition () and Word Sense Disambiguation (WSD) ().", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.5856757263342539}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.7560627808173498}]}, {"text": "In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts, and each word is regarded as a different instance to classify.", "labels": [], "entities": []}, {"text": "For instance, occurrences of a given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts.", "labels": [], "entities": []}, {"text": "For example the token Rossi, whenever is preceded by the token Prof., often represents the name of a person.", "labels": [], "entities": []}, {"text": "Another task that can benefit from modeling this kind of relations is WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8920814990997314}]}, {"text": "To solve ambiguity it is necessary to analyze syntagmatic relations in the local context of the word to be disambiguated.", "labels": [], "entities": []}, {"text": "In this paper we propose a kernel function that can be used to model such relations, the Syntagmatic Kernel, and we apply it to two (English and Italian) lexical-sample WSD tasks of the Senseval-3 competition ().", "labels": [], "entities": []}, {"text": "Ina lexical-sample WSD task, training data are provided as a set of texts, in which for each text a given target word is manually annotated with a sense from a predetermined set of possibilities.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.8774846792221069}]}, {"text": "To model syntagmatic relations, the typical supervised learning framework adopts as features bigrams and trigrams in a local context.", "labels": [], "entities": []}, {"text": "The main drawback of this approach is that non contiguous or shifted col-locations cannot be identified, decreasing the generalization power of the learning algorithm.", "labels": [], "entities": []}, {"text": "For example, suppose that the verb to score has to be disambiguated into the sentence \"Ronaldo scored the goal\", and that the sense tagged example \"the football player scores#1 the first goal\" is provided for training.", "labels": [], "entities": []}, {"text": "A traditional feature mapping would extract the bigram w +1 w +2 :the goal to represent the former, and the bigram w +1 w +2 :the first to index the latter.", "labels": [], "entities": []}, {"text": "Evidently such features will not match, leading the algorithm to a misclassification.", "labels": [], "entities": []}, {"text": "In the present paper we propose the Syntagmatic Kernel as an attempt to solve this problem.", "labels": [], "entities": []}, {"text": "The Syntagmatic Kernel is based on a Gap-Weighted Subsequences Kernel).", "labels": [], "entities": []}, {"text": "In the spirit of Kernel Methods, this kernel is able to compare sequences directly in the input space, avoiding any explicit feature mapping.", "labels": [], "entities": []}, {"text": "To perform this operation, it counts how many times a (non-contiguous) subsequence of symbols u of length n occurs in the input string s, and penalizes non-contiguous occurrences according to the number of the contained gaps.", "labels": [], "entities": []}, {"text": "To define our Syntagmatic Kernel, we adapted the generic definition of the Sequence Kernels to the problem of recognizing collocations in local word contexts.", "labels": [], "entities": []}, {"text": "In the above definition of Syntagmatic Kernel, only exact word-matches contribute to the similarity.", "labels": [], "entities": [{"text": "Syntagmatic Kernel", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8101532757282257}]}, {"text": "One shortcoming of this approach is that (near-)synonyms will never be considered similar, leading to a very low generalization power of the learning algorithm, that requires a huge amount of data to converge to an accurate prediction.", "labels": [], "entities": []}, {"text": "To solve this problem we provided external lexical knowledge to the supervised learning algorithm, in order to define a \"soft-matching\" schema for the kernel function.", "labels": [], "entities": []}, {"text": "For example, if we consider as equivalent the terms Ronaldo and football player, the proposition \"The football player scored the first goal\" is equivalent to the sentence \"Ronaldo scored the first goal\", providing a strong evidence to disambiguate the latter occurrence of the verb.", "labels": [], "entities": []}, {"text": "We propose two alternative soft-matching criteria exploiting two different knowledge sources: (i) handmade resources and (ii) unsupervised term similarity measures.", "labels": [], "entities": []}, {"text": "The first approach performs a softmatching among all those synonyms words in WordNet, while the second exploits domain relations, acquired from unlabeled data, for the same purpose.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9649649262428284}]}, {"text": "Our experiments, performed on two standard WSD benchmarks, show the superiority of the Syntagmatic Kernel with respect to a classical flat vector representation of bigrams and trigrams.", "labels": [], "entities": [{"text": "WSD benchmarks", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.7951906621456146}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the Sequence Kernels.", "labels": [], "entities": [{"text": "Sequence Kernels", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.8622453510761261}]}, {"text": "In Section 3 the Syntagmatic Kernel is defined.", "labels": [], "entities": []}, {"text": "Section 4 explains how soft-matching can be exploited by the Collocation Kernel, describing two alternative criteria: WordNet Synonymy and Domain Proximity.", "labels": [], "entities": []}, {"text": "Section 5 gives a brief sketch of the complete WSD system, composed by the combination of different kernels, dealing with syntagmatic and paradigmatic aspects.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8790950179100037}]}, {"text": "Section 6 evaluates the Syntagmatic Kernel, and finally Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the Syntagmatic Kernel, showing that it improves over the standard feature extraction technique based on bigrams and trigrams of words and PoS tags.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7452316582202911}]}, {"text": "We conducted the experiments on two lexical sample tasks (English and Italian) of the Senseval-3 competition ().", "labels": [], "entities": []}, {"text": "In lexical-sample WSD, after selecting some target words, training data is provided as a set of texts.", "labels": [], "entities": [{"text": "WSD", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.7993779182434082}]}, {"text": "For each text a given target word is manually annotated with a sense from a predetermined set of possibilities.", "labels": [], "entities": []}, {"text": "shows the performance of the Syntagmatic Kernel on both data sets.", "labels": [], "entities": []}, {"text": "As baseline, we report the result of a standard approach consisting on explicit bigrams and trigrams of words and PoS tags around the words to be disambiguated.", "labels": [], "entities": []}, {"text": "The results show that the Syntagmatic Kernel outperforms the baseline in any configuration (hard/soft-matching", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. DMs can be acquired from texts by exploit-", "labels": [], "entities": []}]}