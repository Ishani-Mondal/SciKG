{"title": [{"text": "Word Sense Disambiguation Using Automatically Translated Sense Examples", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.643025686343511}]}], "abstractContent": [{"text": "We present an unsupervised approach to Word Sense Disambiguation (WSD).", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7867981791496277}]}, {"text": "We automatically acquire English sense examples using an English-Chinese bilingual dictionary, Chinese monolingual corpora and Chinese-English machine translation software.", "labels": [], "entities": [{"text": "Chinese-English machine translation", "start_pos": 127, "end_pos": 162, "type": "TASK", "confidence": 0.6751204530398051}]}, {"text": "We then train machine learning classifiers on these sense examples and test them on two gold standard En-glish WSD datasets, one for binary and the other for fine-grained sense identification.", "labels": [], "entities": [{"text": "WSD datasets", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.725967139005661}, {"text": "fine-grained sense identification", "start_pos": 158, "end_pos": 191, "type": "TASK", "confidence": 0.6680851181348165}]}, {"text": "On binary disambiguation, performance of our unsupervised system has approached that of the state-of-the-art supervised ones.", "labels": [], "entities": []}, {"text": "On multi-way disambiguation, it has achieved a very good result that is competitive to other state-of-the-art unsu-pervised systems.", "labels": [], "entities": []}, {"text": "Given the fact that our approach does not rely on manually annotated resources, such as sense-tagged data or parallel corpora, the results are very promising.", "labels": [], "entities": []}], "introductionContent": [{"text": "Results from recent Senseval workshops have shown that supervised Word Sense Disambiguation (WSD) systems tend to outperform their unsupervised counterparts.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.7456200669209162}]}, {"text": "However, supervised systems rely on large amounts of accurately senseannotated data to yield good results and such resources are very costly to produce.", "labels": [], "entities": []}, {"text": "It is difficult for supervised WSD systems to perform well and reliably on words that do not have enough sensetagged training data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9748011231422424}]}, {"text": "This is the so-called knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8587474822998047}]}, {"text": "To overcome this bottleneck, unsupervised WSD approaches have been proposed.", "labels": [], "entities": [{"text": "WSD", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9761177897453308}]}, {"text": "Among them, systems under the multilingual paradigm have shown great promise ().", "labels": [], "entities": []}, {"text": "The underlying hypothesis is that mappings between word forms and meanings can be different from language to language.", "labels": [], "entities": []}, {"text": "Much work have been done on extracting sense examples from parallel corpora for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9049861431121826}]}, {"text": "For example, proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora.", "labels": [], "entities": []}, {"text": "They grouped senses that share the same Chinese translation, and then the occurrences of the word on the English side of the parallel corpora were considered to have been disambiguated and \"sense tagged\" by the appropriate Chinese translations.", "labels": [], "entities": []}, {"text": "Their system was evaluated on the nouns in Senseval-2 English lexical sample dataset, with promising results.", "labels": [], "entities": [{"text": "Senseval-2 English lexical sample dataset", "start_pos": 43, "end_pos": 84, "type": "DATASET", "confidence": 0.8743527054786682}]}, {"text": "Their follow-up work) has successfully scaled up the approach and achieved very good performance on the Senseval-2 English all-word task.", "labels": [], "entities": []}, {"text": "Despite the promising results, there are problems with relying on parallel corpora.", "labels": [], "entities": []}, {"text": "For example, there is alack of matching occurrences for some Chinese translations to English senses.", "labels": [], "entities": []}, {"text": "Thus gathering training examples for them might be difficult, as reported in).", "labels": [], "entities": []}, {"text": "Also, parallel corpora themselves are rare resources and not available for many language pairs.", "labels": [], "entities": []}, {"text": "Some researchers seek approaches using monolingual resources in a second language and then try to map the two languages using bilingual dictionaries.", "labels": [], "entities": []}, {"text": "For example, carried out WSD experiments using monolingual corpora, a bilingual lexicon and a parser for the source language.", "labels": [], "entities": [{"text": "WSD", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9554571509361267}]}, {"text": "One problem of this method is that for many languages, accurate parsers do not exist.", "labels": [], "entities": []}, {"text": "proposed to use monolingual corpora and bilingual dictionaries to automatically acquire sense examples.", "labels": [], "entities": []}, {"text": "Their system was unsupervised and achieved very promising results on the Senseval-2 lexical sample dataset.", "labels": [], "entities": [{"text": "Senseval-2 lexical sample dataset", "start_pos": 73, "end_pos": 106, "type": "DATASET", "confidence": 0.9483908712863922}]}, {"text": "Their system also has better portability, i.e., it runs on any language pair as long as a bilingual dictionary is available.", "labels": [], "entities": []}, {"text": "However, sense examples acquired using the dictionary-based word-by-word translation can only provide \"bag-of-words\" features.", "labels": [], "entities": []}, {"text": "Many other features useful for machine learning (ML) algorithms, such as the ordering of words, part-of-speech (POS), bigrams, etc., have been lost.", "labels": [], "entities": []}, {"text": "It could be more interesting to translate Chinese text snippets using machine translation (MT) software, which would provide richer contextual information that might be useful for WSD learners.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8365783572196961}, {"text": "WSD learners", "start_pos": 180, "end_pos": 192, "type": "TASK", "confidence": 0.8992740213871002}]}, {"text": "Although MT systems themselves are expensive to build, once they are available, they can be used repeatedly to automatically generate as much data as we want.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9752423763275146}]}, {"text": "This is an advantage over relying on other expensive resources such as manually sense-tagged data and parallel copora, which are limited in size and producing additional data normally involves further costly investments.", "labels": [], "entities": []}, {"text": "We carried out experiments on acquiring sense examples using both MT software and a bilingual dictionary.", "labels": [], "entities": []}, {"text": "When we had the two sets of sense examples ready, we trained a ML classifier on them and then tested them on coarse-grained and finegrained gold standard WSD datasets, respectively.", "labels": [], "entities": [{"text": "WSD datasets", "start_pos": 154, "end_pos": 166, "type": "DATASET", "confidence": 0.7925944328308105}]}, {"text": "We found that on both test datasets the classifier using MT translated sense examples outperformed the one using those translated by a dictionary, given the same amount of training examples used on each word sense.", "labels": [], "entities": [{"text": "MT translated sense", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.6595082481702169}]}, {"text": "This confirms our assumption that a richer feature set, although from a noisy data source, such as machine translated text, might help ML algorithms.", "labels": [], "entities": [{"text": "ML", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.9920629858970642}]}, {"text": "In addition, both systems performed very well comparing to other state-of-the-art WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.8941417336463928}]}, {"text": "As we expected, our system is particularly good on coarse-grained disambiguation.", "labels": [], "entities": []}, {"text": "Being an unsupervised approach, it achieved a performance competitive to state-ofthe-art supervised systems.", "labels": [], "entities": []}, {"text": "This paper is organised as follows: Section 2 revisits the process of acquiring sense examples proposed in () and then describes our adapted approach.", "labels": [], "entities": []}, {"text": "Section 3 outlines resources, the ML algorithm and evaluation metrics that we used.", "labels": [], "entities": [{"text": "ML", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.8564203977584839}]}, {"text": "Section 4 and Section 5 detail experiments we carried out on gold standard datasets.", "labels": [], "entities": [{"text": "gold standard datasets", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.8481101989746094}]}, {"text": "We also report our results and error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.9197350442409515}]}, {"text": "Finally, Section 6 concludes the paper and draws future directions.", "labels": [], "entities": []}, {"text": "proposed an automatic approach to acquire sense examples from large amount of Chinese text and English-Chinese and Chinese-English dictionaries.", "labels": [], "entities": []}, {"text": "The acquisition process is summarised as follows:", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our WSD classifier on both coarse-grained and fine-grained datasets.", "labels": [], "entities": [{"text": "WSD classifier", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.8333533704280853}]}, {"text": "For coarse-grained WSD evaluation, we used TWA dataset, which is a binarily sense-tagged corpus drawn from the British National Corpus (BNC), for 6 nouns.", "labels": [], "entities": [{"text": "WSD evaluation", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.9562482535839081}, {"text": "TWA dataset", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9040277600288391}, {"text": "British National Corpus (BNC)", "start_pos": 111, "end_pos": 140, "type": "DATASET", "confidence": 0.9727944235006968}]}, {"text": "For finegrained evaluation, we used Senseval-3 English lexical sample dataset (), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives).", "labels": [], "entities": [{"text": "Senseval-3 English lexical sample dataset", "start_pos": 36, "end_pos": 77, "type": "DATASET", "confidence": 0.5667018234729767}]}, {"text": "The examples were mainly drawn from BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9833184480667114}]}, {"text": "WordNet texts, mainly from the Brown Corpus, comprising about 200,000 words, where all content words have been manually tagged with senses from WordNet.", "labels": [], "entities": [{"text": "WordNet texts", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9684507548809052}, {"text": "Brown Corpus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9881687462329865}, {"text": "WordNet", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.9772177338600159}]}, {"text": "Throughout the paper we will use the concepts of precision and recall to measure the performance of WSD systems, where precision refers to the ratio of correct answers to the total number of answers given by the system, and recall indicates the ratio of correct answers to the total number of instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9992334842681885}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9963759779930115}, {"text": "WSD", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9218080639839172}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9983432292938232}, {"text": "recall", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.9988676309585571}]}, {"text": "Our ML systems attempt every instance and always give a unique answer, and hence precision equals to recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9993211030960083}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9975885152816772}]}, {"text": "When comparing with other systems that participated in Senseval-3 in, both recall and precision are shown.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9997249245643616}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9997153878211975}]}, {"text": "When POS and overall averages are given, they are calculated by micro-averaging the number of examples per word.", "labels": [], "entities": [{"text": "POS", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9598367214202881}]}, {"text": "First we trained a VSM classifier on the sense examples translated with the Systran MT software (we use notion \"MT-based approach\" to refer to this process), and then tested it on the TWA test dataset.", "labels": [], "entities": [{"text": "Systran MT software", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.7155745625495911}, {"text": "TWA test dataset", "start_pos": 184, "end_pos": 200, "type": "DATASET", "confidence": 0.981910506884257}]}, {"text": "We tried two combinations of features: one only used topical features and the other used the whole feature set (i.e., topical and domain features).", "labels": [], "entities": []}, {"text": "summarises the sizes of the training/test data, the Most Frequent Sense (MFS) baseline and performances when applying the two different feature combinations.", "labels": [], "entities": [{"text": "Most Frequent Sense (MFS) baseline", "start_pos": 52, "end_pos": 86, "type": "METRIC", "confidence": 0.8584310497556414}]}, {"text": "We can see that best results were obtained when using all the features.", "labels": [], "entities": []}, {"text": "It also shows that both our systems achieved a significant improvement over the MFS baseline.", "labels": [], "entities": [{"text": "MFS baseline", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7568899989128113}]}, {"text": "Therefore, in the subsequent WSD experiments following the MT-based approach, we decided to use the entire feature set.", "labels": [], "entities": [{"text": "WSD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8845603466033936}]}, {"text": "To compare the machine-translated sense examples with the ones translated word-by-word, we then trained the same VSM classifier on the examples translated with a bilingual dictionary (we use notion \"dictionary-based approach\" to refer to this process) and evaluated it on the same test dataset.", "labels": [], "entities": []}, {"text": "shows results of the dictionarybased approach and the MT-based approach.", "labels": [], "entities": []}, {"text": "For comparison, we include results from another system  cross-validation on the TWA data, which indicates the score that a supervised system would attain, taking additional advantage that the examples for training and test are drawn from the same corpus.", "labels": [], "entities": [{"text": "TWA data", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9720914661884308}]}, {"text": "We can see that our MT-based approach has achieved significantly better recall than the other two automatic methods.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9828492999076843}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9992994070053101}]}, {"text": "Besides, the results of our unsupervised system are approaching the performance achieved with hand-tagged data.", "labels": [], "entities": []}, {"text": "It is worth mentioning that Mihalcea (2003) applied a similar supervised cross-validation method on this dataset that scored 83.35%, very close to our unsupervised system 9 . Thus, we can conclude that the MT-based system is able to reach the best performance reported on this dataset for an unsupervised system.", "labels": [], "entities": []}, {"text": "In this section we describe the experiments carried out on the Senseval-3 lexical sample dataset.", "labels": [], "entities": [{"text": "Senseval-3 lexical sample dataset", "start_pos": 63, "end_pos": 96, "type": "DATASET", "confidence": 0.9436503648757935}]}, {"text": "First, we introduce a heuristic method to deal with the problem of fine-grainedness of WordNet senses.", "labels": [], "entities": []}, {"text": "The remaining two subsections will be devoted to the experiments of the baseline system and the contribution of the heuristic to the final system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:Recall(%) of the VSM classifier trained on the MT- translated sense examples, with different sets of features. The  MFS baseline(%) and the number of training and test exam- ples are also shown.", "labels": [], "entities": [{"text": "Recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9788807034492493}, {"text": "MT- translated sense", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.5260685160756111}, {"text": "MFS baseline", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.5616293251514435}]}, {"text": " Table 2:Recall(%) on TWA dataset for 3 unsupervised sys- tems and a supervised cross-validation on test data.", "labels": [], "entities": [{"text": "Recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9926546812057495}, {"text": "TWA dataset", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.9110355079174042}]}, {"text": " Table 3:Sense filtering by relative-threshold on SemCor. For  each threshold the number of removed senses/tokens and am- biguity are shown.", "labels": [], "entities": [{"text": "Sense filtering", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.788526862859726}]}, {"text": " Table 4:Averaged recall(%) for the dictionary-based and MT- based methods in Senseval-3 lexical-sample data. The MFS  baseline(%) and the number of testing examples are also  shown.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9039958715438843}, {"text": "Senseval-3 lexical-sample data", "start_pos": 78, "end_pos": 108, "type": "DATASET", "confidence": 0.9115950465202332}]}, {"text": " Table 5:Average ambiguity and recall(%) for the relative- based threshold on Senseval-3 training data and SemCor (for  nouns only). Best results shown in bold.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9980825185775757}, {"text": "Senseval-3 training data", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.7075196305910746}]}, {"text": " Table 6:Final results(%) for all nouns in Senseval-3 test data.  Together with the number of test examples and MFS base- line(%).", "labels": [], "entities": [{"text": "Senseval-3 test data", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.8564409613609314}, {"text": "MFS base- line", "start_pos": 112, "end_pos": 126, "type": "METRIC", "confidence": 0.8573984652757645}]}, {"text": " Table 7:Comparison of unsupervised S3 systems for nouns  (sorted by recall(%)). Our system given in bold.", "labels": [], "entities": [{"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9977529644966125}]}, {"text": " Table 8:Coarse-grained evaluation of unsupervised S3 sys- tems for nouns (sorted by recall(%)). Our system given in  bold.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9961600303649902}]}]}