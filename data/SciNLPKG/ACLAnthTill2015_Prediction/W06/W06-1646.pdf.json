{"title": [{"text": "Corrective Models for Speech Recognition of Inflected Languages", "labels": [], "entities": [{"text": "Speech Recognition of Inflected Languages", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.8342236936092376}]}], "abstractContent": [{"text": "This paper presents a corrective model for speech recognition of inflected languages.", "labels": [], "entities": [{"text": "speech recognition of inflected languages", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.7724599242210388}]}, {"text": "The model, based on a discrim-inative framework, incorporates word n-grams features as well as factored morphological features, providing error reduction over the model based solely on word n-gram features.", "labels": [], "entities": []}, {"text": "Experiments on a large vocabulary task, namely the Czech portion of the MALACH corpus, demonstrate performance gain of about 1.1-1.5% absolute in word error rate, wherein morphological features contribute about a third of the improvement.", "labels": [], "entities": [{"text": "Czech portion of the MALACH corpus", "start_pos": 51, "end_pos": 85, "type": "DATASET", "confidence": 0.869431068499883}, {"text": "absolute in word error rate", "start_pos": 134, "end_pos": 161, "type": "METRIC", "confidence": 0.6654965996742248}]}, {"text": "A simple feature selection mechanism based on \u03c7 2 statistics is shown to be effective in reducing the number of features by about 70% without any loss in performance, making it feasible to explore yet larger feature spaces.", "labels": [], "entities": []}], "introductionContent": [{"text": "N -gram models have long been the stronghold of statistical language modeling approaches.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.7696028351783752}]}, {"text": "Within the n-gram paradigm, straightforward approaches for increasing accuracy include using larger training sets and augmenting the contextual information within the n-gram window.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9971296191215515}]}, {"text": "Incorporating syntactic features into the context has been at the forefront of recent research;).", "labels": [], "entities": []}, {"text": "However, much of the previous work has focused on English language syntax.", "labels": [], "entities": [{"text": "English language syntax", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.5900266766548157}]}, {"text": "This paper addresses syntax as captured by the inflectional morphology of highly inflected language.", "labels": [], "entities": []}, {"text": "High inflection in a language is generally correlated with some level of word-order flexibility.", "labels": [], "entities": []}, {"text": "Morphological features either directly identify or help disambiguate the syntactic participants of a sentence.", "labels": [], "entities": []}, {"text": "Inflectional morphology works as a proxy for structured syntax in a language.", "labels": [], "entities": [{"text": "Inflectional morphology", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7754732072353363}]}, {"text": "Modeling morphological features in these languages not only provides an additional source of information but can also alleviate data sparsity problems.", "labels": [], "entities": []}, {"text": "Czech speech recognition needs to deal with two sources of errors which are absent in English, namely, the inflectional morphology and the differences in the formal (written) and colloquial (spoken) forms.", "labels": [], "entities": [{"text": "Czech speech recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6065369447072347}]}, {"text": "presents an example output of our speech recognizer on an utterance from a Holocaust survivor, who is recounting General Romel's desert campaign during the Second World War.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.6662073135375977}]}, {"text": "In this example, the feminine past-tense form of the Czech verb for to be is chosen mistakenly, which is followed by a sequence of incorrect words chosen primarily to maintain agreement with the feminine form of the verb.", "labels": [], "entities": []}, {"text": "This is an example of what we refer to as the morphological grouping effect.", "labels": [], "entities": [{"text": "morphological grouping", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.720086008310318}]}, {"text": "When the acoustic model prefers a word with an incorrect inflection, the language model effectively propagates the error to later words.", "labels": [], "entities": []}, {"text": "A language model based on wordforms prefers sequences observed in the training data, which will implicitly force an agreement with the inflections of preceding words, making it difficult to stop propagating errors.", "labels": [], "entities": []}, {"text": "Although this analysis is anecdotal in nature, the grouping effect appears to be prevalent in the Czech dataset used in this work.", "labels": [], "entities": [{"text": "Czech dataset", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.923206627368927}]}, {"text": "The proposed corrective model with morphological features is expected to alleviate the grouping effect as well as to improve the recognition of inflected languages in general.", "labels": [], "entities": []}, {"text": "In the following section, we present a brief review of related work on morphological language modeling and discriminative language mod-: An example of the grouping effect.", "labels": [], "entities": [{"text": "morphological language modeling", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6304129163424174}, {"text": "discriminative language mod", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.7125057379404703}]}, {"text": "The incorrect form of the verb to be begins a group of incorrect words in the hypothesis, but these words agree in their morphological inflection. els.", "labels": [], "entities": []}, {"text": "We begin the description of our work in section 3 with the type of morphological features modeled as well as their computation from the output word-lattices of a speech recognizer.", "labels": [], "entities": []}, {"text": "Section 4 presents the corrective model and the training approach explored in the current work.", "labels": [], "entities": []}, {"text": "A simple and effective feature selection mechanism is described in section 5.", "labels": [], "entities": []}, {"text": "In section 6, the proposed framework is evaluated on a large vocabulary Czech speech recognition task.", "labels": [], "entities": [{"text": "large vocabulary Czech speech recognition task", "start_pos": 55, "end_pos": 101, "type": "TASK", "confidence": 0.6968872000773748}]}, {"text": "Results show that the morphological features provide a significant improvement over models lacking these features; subsequently, two different analyses are provided to understand the contribution of different morphological features.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corrective model presented in this work is evaluated on a large vocabulary task consisting of spontaneous spoken testimonies in Czech language, which is a subset of the multilingual MALACH corpus ().", "labels": [], "entities": [{"text": "MALACH corpus", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.7022765874862671}]}, {"text": "We present a set of contrastive experiments to gauge the performance of the corrective models and the contribution of morphological features.", "labels": [], "entities": []}, {"text": "For training the corrective models, 50 best hypotheses are generated for each utterance using the  jack-knife procedure mentioned earlier.", "labels": [], "entities": []}, {"text": "For each hypothesis, bigram and unigram features are computed which consist of word-forms, lemmas, morphologoical tags, factored morphological tags, and the likelihood from the baseline ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 186, "end_pos": 189, "type": "TASK", "confidence": 0.9078006148338318}]}, {"text": "For testing, the baseline ASR system is used to generate 1000 best hypotheses for each utterance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6060070991516113}]}, {"text": "These are then evaluated using the corrective models and the best scored hypothesis is chosen as the output.: The word error rate of the corrective model is compared with that of the baseline ASR system, illustrating the improvement in performance with morphological features.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.7179956833521525}, {"text": "ASR", "start_pos": 192, "end_pos": 195, "type": "TASK", "confidence": 0.9127139449119568}]}, {"text": "The gains on the dev set are significant at the level of p < 0.001 for three standard NIST tests, namely, matched pair sentence segment, signed pair comparison, and Wilcoxon signed rank tests.", "labels": [], "entities": []}, {"text": "For the smaller eval set the significant levels were lower for morphological features.", "labels": [], "entities": []}, {"text": "The relative gains observed are consistent over a variety of conditions that we have tested including the ones reported below.", "labels": [], "entities": []}, {"text": "Subsequently, we investigated the impact of reducing the number of features using \u03c7 2 statistics, as described in section 5.", "labels": [], "entities": []}, {"text": "The experiments with bigram features of word-forms and morphology were repeated using reduced feature sets, and the performance was measured at 10%, 30% and 60% of their original features.", "labels": [], "entities": []}, {"text": "The results, as illustrated in, show that the word error rate does not change significantly even after the number of features are reduced by 70%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.6610406935214996}]}, {"text": "We have also observed that most of the gain can be achieved by evaluating 200 best hypotheses from the baseline ASR system, which could further reduce the computational cost for time-sensitive applications.", "labels": [], "entities": [{"text": "ASR", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9625934362411499}]}], "tableCaptions": [{"text": " Table 2: Czech morphological features used in the  current work. The # Values field indicates the size  of the closed set of possible values. Not all values  are used in the annotated data.", "labels": [], "entities": []}, {"text": " Table 6: The word error rate of the corrective  model is compared with that of the baseline ASR  system, illustrating the improvement in perfor- mance with morphological features.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.668821781873703}, {"text": "ASR", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9298867583274841}]}]}