{"title": [{"text": "DUC 2005: Evaluation of Question-Focused Summarization Systems", "labels": [], "entities": [{"text": "DUC 2005", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8906295299530029}, {"text": "Question-Focused Summarization", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.571478083729744}]}], "abstractContent": [{"text": "The Document Understanding Conference (DUC) 2005 evaluation had a single user-oriented, question-focused summarization task, which was to synthesize from a set of 25-50 documents a well-organized, fluent answer to a complex question.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC) 2005", "start_pos": 4, "end_pos": 48, "type": "TASK", "confidence": 0.8190850615501404}]}, {"text": "The evaluation shows that the best summariza-tion systems have difficulty extracting relevant sentences in response to complex questions (as opposed to representative sentences that might be appropriate to a generic summary).", "labels": [], "entities": []}, {"text": "The relatively generous allowance of 250 words for each answer also reveals how difficult it is for current summarization systems to produce fluent text from multiple documents.", "labels": [], "entities": [{"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9766618013381958}]}], "introductionContent": [{"text": "The Document Understanding Conference (DUC) is a series of evaluations of automatic text summarization systems.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.8198326279719671}, {"text": "text summarization", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.580324575304985}]}, {"text": "It is organized by the National Institute of Standards of Technology with the goals of furthering progress in automatic summarization and enabling researchers to participate in large-scale experiments.", "labels": [], "entities": [{"text": "summarization", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.763533353805542}]}, {"text": "Ina growing number of research groups participated in the evaluation of generic and focused summaries of English newspaper and newswire data.", "labels": [], "entities": [{"text": "English newspaper and newswire data", "start_pos": 105, "end_pos": 140, "type": "DATASET", "confidence": 0.6509644210338592}]}, {"text": "Various target sizes were used (10-400 words) and both singledocument summaries and summaries of multiple documents were evaluated (around 10 documents per set).", "labels": [], "entities": []}, {"text": "Summaries were manually judged for both content and readability.", "labels": [], "entities": []}, {"text": "To evaluate content, each peer (human or automatic) summary was compared against a single model (human) summary using SEE (http://www.isi.edu/ cyl/SEE/) to estimate the percentage of information in the model that was covered in the peer.", "labels": [], "entities": [{"text": "SEE", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9875338077545166}]}, {"text": "Additionally, automatic evaluation of content coverage using ROUGE) was explored in 2004.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9654683470726013}]}, {"text": "Human summaries vary in both writing style and content.", "labels": [], "entities": []}, {"text": "For example, () noted that a human summary can vary in its level of granularity, whether the summary has a very high-level analysis or primarily contains details.", "labels": [], "entities": []}, {"text": "They analyzed the effects of human variaion in the DUC evaluations and concluded that despite large variation in model summaries, the rankings of the systems when compared against a single model for each document set remained stable when averaged over a large number of document sets and human assessors.", "labels": [], "entities": []}, {"text": "The use of a large test set to smooth over natural human variation is not anew technique; it is the approach that has been taken in TREC (Text Retrieval Conference) for many years ().", "labels": [], "entities": [{"text": "TREC (Text Retrieval Conference)", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.6063102980454763}]}, {"text": "While evaluators can achieve stable overall system rankings by averaging scores over a large number of document sets, system builders are still faced with the challenge of producing a summary fora given document set that is most likely to satisfy any human user (since they cannot know ahead of time which human will be using or judging the summary).", "labels": [], "entities": []}, {"text": "Thus, system developers desire an evaluation methodology that takes into account human variation in summaries for any given document set.", "labels": [], "entities": []}, {"text": "DUC 2005 marked a major change in direction from previous years.", "labels": [], "entities": [{"text": "DUC 2005", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9488113224506378}]}, {"text": "The road mapping committee had strongly recommended that new tasks be undertaken that were strongly tied to a clear user application.", "labels": [], "entities": [{"text": "road mapping", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8199255466461182}]}, {"text": "At the same time, the program committee wanted to work on new evaluation methodologies and metrics that would take into account variation of content in human-authored summaries.", "labels": [], "entities": []}, {"text": "Therefore, DUC 2005 had a single user-oriented system task that allowed the community to put sometime and effort into helping with anew evaluation framework.", "labels": [], "entities": [{"text": "DUC 2005", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.9388851523399353}]}, {"text": "The system task modeled realworld complex question answering ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7142994254827499}]}, {"text": "Systems were to synthesize from a set of 25-50 documents a brief, well-organized, fluent answer to a need for information that could not be met by just stating a name, date, quantity, etc.", "labels": [], "entities": []}, {"text": "Summaries were evaluated for both content and readability.", "labels": [], "entities": []}, {"text": "The task design attempted to constrain two parameters that could produce summaries with widely different content: focus and granularity.", "labels": [], "entities": []}, {"text": "Having a question to focus the summary was intended to improve agreement in content between the model summaries.", "labels": [], "entities": [{"text": "agreement", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9477773904800415}]}, {"text": "Additionally, the assessor who developed each topic specified the desired granularity (level of generalization) of the summary.", "labels": [], "entities": []}, {"text": "Granularity was away to express one type of user preference; one user might want a general background or overview summary, while another user might want specific details that would allow him to answer questions about specific events or situations.", "labels": [], "entities": []}, {"text": "Because it is both impossible and unnatural to eliminate all human variation, our assessors created as many manual summaries as feasible for each topic, to provide examples of the range of normal human variability in the summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 221, "end_pos": 239, "type": "TASK", "confidence": 0.914569228887558}]}, {"text": "These multiple models would provide more representative training data to system developers, while enabling additional experiments to investigate the effect of human variability on the evaluation of summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 198, "end_pos": 211, "type": "TASK", "confidence": 0.9402896761894226}]}, {"text": "As in past DUCs, assessors manually evaluated each summary for readability using a set of linguistic quality questions.", "labels": [], "entities": []}, {"text": "Summary content was manually evaluated using the pseudoextrinsic measure of responsiveness, which does not attempt pairwise comparison of peers against a model summary but gives a coarse ranking of all the summaries based on responsiveness of the summary to the topic.", "labels": [], "entities": [{"text": "Summary content", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9263169467449188}]}, {"text": "In parallel, ISI and Columbia University led the summarization research community in two exploratory efforts at intrinsic evaluation of summary content; these evaluations compared peer summaries against multiple reference summaries, using Basic Elements at ISI and Pyramids at Columbia University.", "labels": [], "entities": [{"text": "summarization", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9850012063980103}]}, {"text": "This paper describes the DUC 2005 task and the results of our evaluations of summary content and readability.", "labels": [], "entities": [{"text": "DUC 2005 task", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8562106887499491}]}, {"text": "() and () provide additional details and results of the evaluations of summary content using Basic Elements and Pyramids.", "labels": [], "entities": []}], "datasetContent": [{"text": "Summaries were manually evaluated by 10 assessors.", "labels": [], "entities": []}, {"text": "All summaries fora given topic were judged by a single assessor (who was usually the same as the topic developer).", "labels": [], "entities": []}, {"text": "In all cases, the assessor was one of the summarizers for the topic.", "labels": [], "entities": []}, {"text": "All summaries for the topic (including the one written by the assessor) were anonymously presented to the assessor, in a random order, and the ssessor judged each summary for readability and responsiveness to the topic, giving separate scores for responsiveness and each of 5 linguistic qualities.", "labels": [], "entities": []}, {"text": "This allowed participants who could notwork on optimizing all 6 manual scores, to focus on only the elements that they were interested in or had the resources to address.", "labels": [], "entities": []}, {"text": "No single score was reported that reflected a combination of readability and content.", "labels": [], "entities": []}, {"text": "In previous years, responsiveness considered both the content and readability of the summary.", "labels": [], "entities": []}, {"text": "While it tracked SEE coverage, responsiveness could not be seen as a direct measure of content due to possible effects of readability on the score.", "labels": [], "entities": [{"text": "SEE coverage", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.5039382576942444}]}, {"text": "Because we needed an inexpensive manual measure of coverage, we revised the definition of responsiveness in 2005 so that it considered only the information content and not the readability of the summary, to the extent possible.", "labels": [], "entities": []}, {"text": "The readability of the summaries was assessed using five linguistic quality questions which measured qualities of the summary that do not involve comparison with a reference summary or DUC topic.", "labels": [], "entities": []}, {"text": "The linguistic qualities measured were Grammaticality, Non-redundancy, Referential clarity, Focus, and Structure and coherence.", "labels": [], "entities": [{"text": "Referential clarity", "start_pos": 71, "end_pos": 90, "type": "METRIC", "confidence": 0.6814219951629639}, {"text": "Focus", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9945684671401978}]}, {"text": "We performed manual pseudo-extrinsic evaluation of peer summaries in the form of assessment of responsiveness.", "labels": [], "entities": []}, {"text": "Responsiveness is different from SEE coverage in that it does not compare a peer summary against a single reference; however, responsiveness tracked SEE coverage in, and was used to provide a coarsegrained measure of content in 2005.", "labels": [], "entities": [{"text": "SEE coverage", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.824584037065506}]}, {"text": "We also computed ROUGE scores as was done in DUC 2004.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.9658815562725067}, {"text": "DUC 2004", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9551573097705841}]}], "tableCaptions": [{"text": " Table 1: Frequency of scores for each linguistic  quality, broken down by source of summary (Hu- mans, Baseline, Participants).", "labels": [], "entities": [{"text": "Frequency of scores", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9468276699384054}]}, {"text": " Table 3: Multiple comparison of all peers based on ANOVA of ROUGE-2 recall", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9152098894119263}, {"text": "ROUGE-2", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.7699251770973206}]}, {"text": " Table 4: Correlation between primary and sec- ondary average scaled responsiveness (20 topics),  with 95% confidence intervals for Pearson's r.", "labels": [], "entities": []}, {"text": " Table 5: Correlation between average scaled re- sponsiveness and macro-average ROUGE recall  over all topics and either all peers or only auto- matic peers.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9444186687469482}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.5863596796989441}]}]}