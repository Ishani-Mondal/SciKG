{"title": [{"text": "Using Scenario Knowledge in Automatic Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.6557918787002563}]}], "abstractContent": [{"text": "This paper describes a novel framework for using scenario knowledge in open-domain Question Answering (Q/A) applications that uses a state-of-the-art textual entailment system (Hickl et al., 2006b) in order to discover textual information relevant to the set of topics associated with a scenario description.", "labels": [], "entities": [{"text": "Question Answering (Q/A)", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.8556520385401589}]}, {"text": "An intrinsic and an extrinsic evaluation of this method is presented in the context of an automatic Q/A system and results from several user scenarios are discussed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Users of today's automatic question-answering (Q/A) systems generally have complex information needs that cannot be satisfied by asking single questions in isolation.", "labels": [], "entities": [{"text": "question-answering (Q/A)", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6414982080459595}]}, {"text": "When users interact with Q/A systems, they often formulate sets of queries that they believe will help them gather the information that needed to perform one or more specific tasks.", "labels": [], "entities": []}, {"text": "While human users are generally able to identify their information needs independently, the information needs of organizations are often presented in the form of short prose descriptions -known as scenarios -which outline the range of knowledge sought by a customer in order to achieve a specific outcome or to accomplish a particular task.", "labels": [], "entities": [{"text": "presented in the form of short prose descriptions -known as scenarios -which outline the range of knowledge sought by a customer in order to achieve a specific outcome or to accomplish a particular task", "start_pos": 137, "end_pos": 339, "type": "Description", "confidence": 0.8673816000421842}]}, {"text": "(An example of one scenario is presented in.)", "labels": [], "entities": []}, {"text": "Recent work in Q/A has sought to use information derived from these kinds of scenarios in order to retrieve sets of answers that are more relevant -and responsive -to a customer's information needs.", "labels": [], "entities": [{"text": "Q/A", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7239644726117452}]}, {"text": "While () used topic signatures;", "labels": [], "entities": []}], "datasetContent": [{"text": "Since we believe CE is intrinsic to the Q/A task, we have evaluated the impact of contextual entailment on our Q/A system in two ways.", "labels": [], "entities": []}, {"text": "First, we compared the quality of the answers produced, with and without contextual entailment.", "labels": [], "entities": []}, {"text": "Second, we evaluated the quality of the ranked list of paragraphs against the list of entailed paragraphs identified by the CE system and the set of relevant answers identified by the Q/A system.", "labels": [], "entities": [{"text": "CE system", "start_pos": 124, "end_pos": 133, "type": "DATASET", "confidence": 0.7666300237178802}]}, {"text": "This comparison was performed for each of the three cases of entailment presented in.", "labels": [], "entities": []}, {"text": "We have explored the impact of knowledge derived from the user scenario through different forms of contextual entailment by comparing the results of such knowledge integration in a Q/A system against the usage of scenario knowledge reported in ().", "labels": [], "entities": []}, {"text": "Topic signatures, derived from the user scenario and from documents are used to establish text passages that are relevant to the scenario, and thus constitute relevant answers.", "labels": [], "entities": []}, {"text": "For each such answer, one or multiple questions were built automatically with the method reported in (.", "labels": [], "entities": []}, {"text": "When anew question was asked, its similarity to any of the questions generated based on the knowledge of the scenario is computed, and its corresponding answer is provided as an answer for the current question as well.", "labels": [], "entities": []}, {"text": "Since the questions are ranked by similarity to the current question, the answers are also ranked and produce the Answer Set 1 illustrated in.", "labels": [], "entities": []}, {"text": "When a Q/A system is used for answering the question, the scenario knowledge can be used in two ways.", "labels": [], "entities": []}, {"text": "First, the keywords extracted by the Question Processing module can be enhanced with concepts from the topic signatures to produce a ranked list of paragraphs, resulting from the Passage Retrieval Module.", "labels": [], "entities": []}, {"text": "These passages together with the question and the user scenario are used in one of the contextual entailment configurations to derive a list of entailed paragraphs from which the Answer Processing module can extract the answer set 2 illustrated in.", "labels": [], "entities": []}, {"text": "In another way, the ranked list of paragraphs is passed to the Answer Processing module, which provides a set of ranked answers to the contextual entailment configurations to derive a list of entailed answers, represented as answer set 3 in.", "labels": [], "entities": []}, {"text": "We evaluate the quality of each set of answers, and for the answer set 2 and 3, we produce separate evaluation for each configuration for the contextual entailment.", "labels": [], "entities": []}, {"text": "Questions asked in response to a user scenario tend to be complex.", "labels": [], "entities": []}, {"text": "Following work in (), we believe complex questions can be answered in one of two ways: either by (1) using techniques (similar to the ones proposed in ( )) for automatically decomposing complex questions into sets of informationally-simpler questions, or by (2) using a multi-document summarization (MDS) system (such as the one described in ( ) in order to assemble a ranked list of passages which contain information that is potentially relevant to the user's question.", "labels": [], "entities": []}, {"text": "First, we expect that contextual entailment can be used to select the decompositions of a complex question that are most closely related to a scenario.", "labels": [], "entities": []}, {"text": "By assigning more confidence to the decompositions that are contextually entailed by a scenario, systems can select a set of answers that are relevant to both the user scenario -and the user's question.", "labels": [], "entities": []}, {"text": "In contrast, contextual entailment can be used in conjunction with the output of a MDS system: once a summary has been constructed from the passages retrieved fora query, contextual en- tailment can be used to select the most relevant sentences from the summary.", "labels": [], "entities": []}, {"text": "The architecture of this proposed system is illustrated in.", "labels": [], "entities": []}, {"text": "When using contextual entailment for selecting question decompositions, we rely on the method reported in ( ) which generates questions by using a random walk on a bipartite graph of salient relations and answers.", "labels": [], "entities": []}, {"text": "In this case, the recognition of entailment between questions operates as a filter, forcing questions that are not entailed by any of the signature answers derived from the scenario context (see) to be dropped from consideration.", "labels": [], "entities": [{"text": "recognition of entailment between questions", "start_pos": 18, "end_pos": 61, "type": "TASK", "confidence": 0.8456899762153626}]}, {"text": "When entailment information is used for reranking candidate answers, the summary is added to the scenario context, each summary sentence being treated akin to a signature answer.", "labels": [], "entities": []}, {"text": "We believe that the summary contains the most informative information from both the question and the scenario, since the queries that produced it originated both in the question and in the scenario.", "labels": [], "entities": []}, {"text": "By adding summary sentences to the scenario context, we have introduced anew dimension to the processing of the scenario.", "labels": [], "entities": []}, {"text": "The contextual entailment is based on the textual entailments between any of the texts from the scenario context and any of the candidate answers.", "labels": [], "entities": []}, {"text": "In this section, we present preliminary results from four sets of experiments which show how forms of textual -and contextual -entailment can enhance the quality of answers returned by an automatic Q/A system.", "labels": [], "entities": []}, {"text": "Questions used in these experiments were gathered from human interactions with the interactive Q/A system described in (.", "labels": [], "entities": []}, {"text": "A total of 6 users were asked to spend approximately 90 minutes gathering information related to three different information-gathering scenarios similar to the one in.", "labels": [], "entities": []}, {"text": "Each user researched two different scenarios, resulting in a total of 12 total research sessions.", "labels": [], "entities": []}, {"text": "Once all research sessions were completed, linguistically well-formed questions were extracted from the system logs for each session for use in our experiments; ungrammatical questions or keyword-style queries were not used in our experiments.", "labels": [], "entities": []}, {"text": "presents a breakdown of the total number of questions collected for each of the 6 scenarios.", "labels": [], "entities": []}, {"text": "In order to evaluate the performance of our Q/A system under each of the experimental conditions described below, questions were re-submitted to the Q/A system and the top 10 answers were retrieved.", "labels": [], "entities": [{"text": "Q/A system", "start_pos": 149, "end_pos": 159, "type": "DATASET", "confidence": 0.6827973574399948}]}, {"text": "Two annotators were then tasked with judging the correctness -or \"relevance\" -of each returned answer to the original question.", "labels": [], "entities": []}, {"text": "If the answer could be considered to provide either a complete or partial answer to the original question, it was marked as correct; if the answer contained information that could not be construed as an answer to the original question, it was marked as incorrect.", "labels": [], "entities": []}, {"text": "In order to evaluate the impact of CE on a Q/A system, we compared the quality of answers produced (1) when no CE information was used (AnsSet 1 ), (2) when CE information was used to select a list of entailed paragraphs that were submitted to an Answer Processing module (AnsSet 2 ), and (3) when CE information was used directly to select answers (AnsSet 3 ).", "labels": [], "entities": []}, {"text": "Results from these three experiments are presented in  As with the TE-based experiments described in Section 7.1, we found that the Q/A system was more likely to return at least one relevant answer among the top-ranked answers when contextual entailment information was used to either rank or select answers.", "labels": [], "entities": []}, {"text": "When CE was used to rank passages for Answer Processing (AnsSet 2 ), accuracy increased by nearly 9% over the baseline (AnsSet 1 ), while accuracy increased by almost 14% overall when CE was used to select answers directly (AnsSet 3 ).", "labels": [], "entities": [{"text": "Answer Processing (AnsSet", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7748950123786926}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9992817044258118}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9989562034606934}]}, {"text": "In order to evaluate the performance of the framework illustrated in, we compared the performance of a question-focused MDS system (first described in ( )) that did not use CE with a similar system that used CE to rank passages fora summary answer.", "labels": [], "entities": []}, {"text": "When CE was not used, sentences identified by the system's Q/A and MDS system for each question were combined and ranked based on number of question keywords found in each sentence.", "labels": [], "entities": [{"text": "MDS", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.6825682520866394}]}, {"text": "In the CE-enabled system (analogous to the system depicted in), only the sentences that were contextually entailed by the scenario were considered; sentences were then ranked using the realvalued entailment confidence computed by the CE system for each sentence.", "labels": [], "entities": []}, {"text": "Results from this system are presented in.", "labels": [], "entities": []}, {"text": "Although the CE-enabled system was more likely to return a scenario-relevant sentence in top  position (48.23%) than the system that did not use CE (41.09%), differences between the systems were much less apparent when the top 5 answers returned by each system were compared.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Each user researched two  different scenarios, resulting in a total of 12 to- tal research sessions. Once all research sessions  were completed, linguistically well-formed ques- tions were extracted from the system logs for each  session for use in our experiments; ungrammatical  questions or keyword-style queries were not used  in our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Questions Collected from User Experi- ments.", "labels": [], "entities": []}, {"text": " Table 3: Impact of Textual Entailment on Q/A.", "labels": [], "entities": []}, {"text": " Table 4: Distribution of CE.", "labels": [], "entities": [{"text": "Distribution of CE", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6902672251065572}]}, {"text": " Table 5: Intrinsic Evaluation of CE on Q/A Per- formance.", "labels": [], "entities": []}]}