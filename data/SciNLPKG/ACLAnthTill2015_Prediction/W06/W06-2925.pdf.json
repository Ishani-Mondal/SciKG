{"title": [], "abstractContent": [{"text": "We describe an online learning dependency parser for the CoNLL-X Shared Task, based on the bottom-up projective algorithm of Eisner (2000).", "labels": [], "entities": [{"text": "online learning dependency parser", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.6240392029285431}]}, {"text": "We experiment with a large feature set that models: the tokens involved in dependencies and their immediate context, the surface-text distance between tokens, and the syntactic context dominated by each dependency.", "labels": [], "entities": []}, {"text": "In experiments, the treatment of multilingual information was totally blind.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (), for 13 different languages.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.5942002832889557}]}, {"text": "Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by.", "labels": [], "entities": []}, {"text": "The parser uses a learning function that scores all possible labeled dependencies.", "labels": [], "entities": []}, {"text": "This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes.", "labels": [], "entities": [{"text": "parsing training sentences", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.8494757612546285}]}, {"text": "The features used to score, while based on the previous work in dependency parsing), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8698245286941528}]}, {"text": "Regarding experimentation, the treatment of multilingual data has been totally blind, with no special processing or features that depend on the language.", "labels": [], "entities": []}, {"text": "Considering its simplicity, our system achieves moderate but encouraging results, with an overall labeled attachment accuracy of 74.72% on the CoNLL-X test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.503454327583313}, {"text": "CoNLL-X test set", "start_pos": 143, "end_pos": 159, "type": "DATASET", "confidence": 0.9400493502616882}]}], "datasetContent": [{"text": "We experimented on the 13 languages proposed in the CoNLL-X Shared Task (;.", "labels": [], "entities": []}, {"text": "Our approach to deal with many different languages was totally blind: we did not inspect the data to motivate language-specific features or processes.", "labels": [], "entities": []}, {"text": "We did feature filtering based on frequency counts.", "labels": [], "entities": []}, {"text": "Our feature extraction patterns, that exploit both lexicalization and combination, generate millions of feature dimensions, even with small datasets.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7338269650936127}]}, {"text": "Our criterion was to use at most 500,000 different dimensions in each label weight vector.", "labels": [], "entities": []}, {"text": "For each language, we generated all possible features, and then filtered out most of them according to the counts.", "labels": [], "entities": []}, {"text": "Depending on the number of training sentences, our counts cut-offs vary from 3 to 15.", "labels": [], "entities": [{"text": "counts cut-offs", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.9551151096820831}]}, {"text": "For each language, we held out from training data a portion of sentences (300, 500 or 1000 depending on the total number of sentences) and trained a model for up to 20 epochs in the rest of the data.", "labels": [], "entities": []}, {"text": "We evaluated each model on the held out data for different number of training epochs, and selected the optimum point.", "labels": [], "entities": []}, {"text": "Then, we retrained each model on the whole training set for the selected number of epochs.", "labels": [], "entities": []}, {"text": "shows the attachment scores obtained by our system, both unlabeled (UAS) and labeled (LAS).", "labels": [], "entities": [{"text": "UAS) and labeled (LAS)", "start_pos": 68, "end_pos": 90, "type": "METRIC", "confidence": 0.7451613971165248}]}, {"text": "The first column (GOLD) presents the LAS obtained with a perfect scoring function: the loss inaccuracy is related to the projectivity assumption of our parsing algorithm.", "labels": [], "entities": [{"text": "GOLD", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8409444689750671}]}, {"text": "Dutch turns out to be the most non-projective language, with a loss inaccuracy of 5.44%.", "labels": [], "entities": []}, {"text": "In our opinion, the loss in other languages is relatively small, and is not a major limitation to achieve a high performance in the task.", "labels": [], "entities": []}, {"text": "Our system achieves an overall LAS of 74.72%, with substantial variation from one language to another.", "labels": [], "entities": [{"text": "LAS", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9947781562805176}]}, {"text": "Turkish, Arabic, Dutch, Slovene and Czech turnout to be the most difficult languages for our system, with accuracies below 70%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9961273074150085}]}, {"text": "The easiest language is clearly Japanese, with a LAS of 88.13%, followed by Chinese, Portuguese, Bulgarian and German, all with LAS above 80%.", "labels": [], "entities": [{"text": "LAS", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.998691737651825}, {"text": "LAS", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9958643913269043}]}, {"text": "shows the contribution of base feature extraction functions.", "labels": [], "entities": []}, {"text": "For four languages, we trained models that increasingly incorporate base functions.", "labels": [], "entities": []}, {"text": "It can be shown that all functions contribute to a better score.", "labels": [], "entities": []}, {"text": "Contextual features (\u03c6 3 ) bring the system to the final order of performance, while distance (\u03c6 4 ) and runtime (\u03c6) features still yield substantial improvements.", "labels": [], "entities": [{"text": "distance (\u03c6 4 )", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.8801973700523377}]}], "tableCaptions": [{"text": " Table 5: Results of the system on test data. GOLD: labeled", "labels": [], "entities": [{"text": "GOLD", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.7766651511192322}]}, {"text": " Table 6: Labeled attachment scores at increasing feature con-", "labels": [], "entities": [{"text": "Labeled attachment", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.5040484219789505}]}, {"text": " Table 7: F \u03b2=1 score related to dependency token distance.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9811170697212219}]}, {"text": " Table 8: Degree of dependency lexicalization.", "labels": [], "entities": []}]}