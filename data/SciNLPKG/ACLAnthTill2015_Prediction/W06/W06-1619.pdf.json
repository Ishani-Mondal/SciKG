{"title": [{"text": "Extremely Lexicalized Models for Accurate and Fast HPSG Parsing", "labels": [], "entities": [{"text": "HPSG Parsing", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.615030825138092}]}], "abstractContent": [{"text": "This paper describes an extremely lexi-calized probabilistic model for fast and accurate HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.7049907445907593}]}, {"text": "In this model, the probabilities of parse trees are defined with only the probabilities of selecting lexical entries.", "labels": [], "entities": []}, {"text": "The proposed model is very simple, and experiments revealed that the implemented parser runs around four times faster than the previous model and that the proposed model has a high accuracy comparable to that of the previous model for probabilistic HPSG, which is defined over phrase structures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9989917874336243}]}, {"text": "We also developed a hybrid of our probabilis-tic model and the conventional phrase-structure-based model.", "labels": [], "entities": []}, {"text": "The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model.", "labels": [], "entities": [{"text": "accurate", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9896897077560425}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9991255402565002}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9992020726203918}]}], "introductionContent": [{"text": "For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued.", "labels": [], "entities": [{"text": "wide-coverage parsing", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.498904749751091}]}, {"text": "In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences.", "labels": [], "entities": []}, {"text": "For example, probabilities were defined over grammar rules in probabilistic CFG) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG).", "labels": [], "entities": []}, {"text": "Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules.", "labels": [], "entities": []}, {"text": "Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.", "labels": [], "entities": [{"text": "parsing", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.9625257253646851}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9458513259887695}]}, {"text": "Another interesting approach to this problem was using supertagging, which was originally developed for lexicalized tree adjoining grammars (LTAG) ().", "labels": [], "entities": []}, {"text": "Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 221, "end_pos": 225, "type": "DATASET", "confidence": 0.9471494555473328}]}, {"text": "Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9731675386428833}]}, {"text": "claimed that if words can be assigned correct supertags, syntactic parsing is almost trivial.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8818367421627045}]}, {"text": "What this means is that if supertags are correctly assigned, syntactic structures are almost de-termined because supertags include rich syntactic information such as subcategorization frames.", "labels": [], "entities": []}, {"text": "showed that the accuracy of LTAG parsing reached about 97%, assuming that the correct supertags were given.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.999761164188385}, {"text": "LTAG parsing", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.698855847120285}]}, {"text": "The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser) with the result of a drastic improvement in the parsing speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 183, "end_pos": 190, "type": "TASK", "confidence": 0.9584800601005554}]}, {"text": "also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser.", "labels": [], "entities": []}, {"text": "They achieved accuracy as high as the state-of-the-art parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9993409514427185}]}, {"text": "However, a supertagger itself was used as an external tagger that enumerates candidates of lexical entries or filters out unlikely lexical entries just to help parsing, and the best parse trees were selected mainly according to the probabilistic model for phrase structures or dependencies with/without the probabilistic model for supertagging.", "labels": [], "entities": [{"text": "parsing", "start_pos": 160, "end_pos": 167, "type": "TASK", "confidence": 0.9625828266143799}]}, {"text": "We investigate an extreme case of HPSG parsing in which the probabilistic model is defined with only the probabilities of lexical entry selection; i.e., the model is never sensitive to characteristics of phrase structures.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.7509126961231232}]}, {"text": "The model is simply defined as the product of the supertagging probabilities, which are provided by the discriminative method with machine learning features of word trigrams and part-of-speech (POS) 5-grams as defined in the CCG supertagging).", "labels": [], "entities": [{"text": "CCG supertagging", "start_pos": 225, "end_pos": 241, "type": "DATASET", "confidence": 0.9281575083732605}]}, {"text": "The model is implemented in an HPSG parser instead of the phrase-structure-based probabilistic model; i.e., the parser returns the parse tree assigned the highest probability of supertagging among the parse trees licensed by an HPSG.", "labels": [], "entities": []}, {"text": "Though the model uses only the probabilities of lexical entry selection, the experiments revealed that it was as accurate as the previous phrasestructure-based model.", "labels": [], "entities": []}, {"text": "Interestingly, this means that accurate parsing is possible using rather simple mechanisms.", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.815019428730011}]}, {"text": "We also tested a hybrid model of the supertagging and the previous phrase-structurebased probabilistic model.", "labels": [], "entities": []}, {"text": "In the hybrid model, the probabilities of the previous model are multiplied by the supertagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries . In the previous model, the preliminary probabilistic model is defined as the probability of unigram supertagging.", "labels": [], "entities": []}, {"text": "So, the hybrid model can be regarded as an extension of supertagging from unigram to n-gram.", "labels": [], "entities": []}, {"text": "The hybrid model can also be regarded as a variant of the statistical CDG parser), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency probabilities.", "labels": [], "entities": []}, {"text": "In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and accuracy, by around two points in both precision and recall, over the previous model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.9682649374008179}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.999681830406189}, {"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9993579983711243}, {"text": "recall", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.9971811771392822}]}, {"text": "This implies that finer probabilistic model of lexical entry selection can improve the phrase-structure-based model.) is a syntactic theory based on lexicalized grammar formalism.", "labels": [], "entities": [{"text": "lexical entry selection", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.6901817917823792}]}, {"text": "In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics.", "labels": [], "entities": []}, {"text": "The structures of sentences are explained using combinations of schemata and lexical entries.", "labels": [], "entities": []}, {"text": "Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English ( ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9992671608924866}, {"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9605090022087097}, {"text": "HPSG grammar", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.9315022826194763}]}, {"text": "The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank () (39,832 sentences).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.8872578740119934}]}, {"text": "The grammar consisted of 3,797 lexical entries for 10,536 words 1 . The probabilistic models were trained using the same portion of the treebank.", "labels": [], "entities": []}, {"text": "We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing ( ) and other tech-    With these thresholding parameters, the parser iterated at most five times for each sentence.", "labels": [], "entities": [{"text": "preserved iterative parsing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.5776293377081553}]}, {"text": "We measured the accuracy of the predicateargument relations output of the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995762705802917}]}, {"text": "A predicate-argument relation is defined as a tuple \u03c3, w h , a, w a , where \u03c3 is the predicate type (e.g., adjective, intransitive verb), w h is the headword of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and w a is the headword of the argument.", "labels": [], "entities": [{"text": "MODARG", "start_pos": 201, "end_pos": 207, "type": "DATASET", "confidence": 0.8825451135635376}, {"text": "ARG1", "start_pos": 209, "end_pos": 213, "type": "DATASET", "confidence": 0.6760377287864685}, {"text": "ARG4", "start_pos": 220, "end_pos": 224, "type": "DATASET", "confidence": 0.9007173180580139}]}, {"text": "Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser 3 . Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label.", "labels": [], "entities": [{"text": "Labeled precision (LP)/labeled recall (LR)", "start_pos": 0, "end_pos": 42, "type": "METRIC", "confidence": 0.8501857936382293}, {"text": "Unlabeled precision (UP)/unlabeled recall (UR)", "start_pos": 105, "end_pos": 151, "type": "METRIC", "confidence": 0.9060165524482727}]}, {"text": "This evaluation scheme was the same as used in previous evaluations of lexicalized grammars Deep parsing techniques include quick check () and large constituent inhibition () as described by , but hybrid parsing with a CFG chunk parser was not used.", "labels": [], "entities": [{"text": "quick check", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.948506087064743}]}, {"text": "This is because we did not observe a significant improvement for the development set by the hybrid parsing and observed only a small improvement in the parsing speed by around 10 ms.", "labels": [], "entities": [{"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.7389014959335327}, {"text": "parsing", "start_pos": 152, "end_pos": 159, "type": "TASK", "confidence": 0.9593759775161743}]}, {"text": "3 When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9610224366188049}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9995211362838745}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9994470477104187}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9994127750396729}]}, {"text": "ran, 2004b; ).", "labels": [], "entities": []}, {"text": "The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU.", "labels": [], "entities": []}, {"text": "Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of \u2264 40 and 100 words in Section 23.", "labels": [], "entities": [{"text": "Section 22 of the Treebank", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.6957604646682739}]}, {"text": "The performance of each parsing technique was analyzed using the sentences in Section 24 of \u2264 100 words.", "labels": [], "entities": []}, {"text": "details the numbers and average lengths of the tested sentences of \u2264 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24.", "labels": [], "entities": []}, {"text": "The parsing performance for Section 23 is shown in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9655219316482544}]}, {"text": "The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger ( ).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9962555468082428}]}, {"text": "The left and right sides of the table show the performances for the sentences of \u2264 40 and \u2264 100 words.", "labels": [], "entities": []}, {"text": "Our models significantly increased not only the parsing speed but also the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9783411622047424}, {"text": "parsing", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.9666809439659119}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9660236239433289}]}, {"text": "Model 3 was around three to four times faster and had around two points higher precision and recall than the previous model.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9995536208152771}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.999397873878479}]}, {"text": "Surprisingly, model 1, which used only lexical information, was very fast and as accurate as the previous model.", "labels": [], "entities": [{"text": "accurate", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9858065247535706}]}, {"text": "Model 2 also improved the accuracy slightly without information of phrase structures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993712306022644}]}, {"text": "When the automatic POS tagger was introduced, both precision and recall dropped by around 2 points, but the tendency towards improved speed and accuracy was again ob-  served.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.7112131416797638}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9997245669364929}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9996210336685181}, {"text": "speed", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.991696298122406}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9985546469688416}]}, {"text": "The unlabeled precisions and recalls of the previous model and models 1, 2, and 3 were significantly different as measured using stratified shuffling tests with p-values < 0.05.", "labels": [], "entities": [{"text": "precisions", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.98866206407547}, {"text": "recalls", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.8638118505477905}]}, {"text": "The labeled precisions and recalls were significantly different among models 1, 2, and 3 and between the previous model and model 3, but were not significantly different between the previous model and model 1 and between the previous model and model 2.", "labels": [], "entities": [{"text": "precisions", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9413508176803589}, {"text": "recalls", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9664492011070251}]}, {"text": "The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of \u2264 100 words are graphed in.", "labels": [], "entities": [{"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9797001481056213}]}, {"text": "The superiority of our models is clearly observed in the figure.", "labels": [], "entities": []}, {"text": "Model 3 performed significantly better than the previous model.", "labels": [], "entities": []}, {"text": "Models 1 and 2 were significantly faster with almost the same accuracy as the previous model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9993625283241272}]}], "tableCaptions": [{"text": " Table 3: Statistics of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9645029902458191}]}, {"text": " Table 4: Experimental results for Section 23.", "labels": [], "entities": []}, {"text": " Table 6. The test  sentences were automatically POS-tagged. Re- sults of other supertaggers for automatically ex-", "labels": [], "entities": [{"text": "POS-tagged", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.894037663936615}]}, {"text": " Table 5: Accuracy of single-tag supertaggers. The  numbers under \"test data\" are the PTB section  numbers of the test data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9849938750267029}, {"text": "PTB", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.5697593688964844}]}, {"text": " Table 6: Accuracy of multi-supertagging.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9941760301589966}]}]}