{"title": [], "abstractContent": [{"text": "We describe the LDV-COMBO system presented at the Shared Task.", "labels": [], "entities": []}, {"text": "Our approach explores the possibility of working with alignments at different levels of abstraction using different degrees of linguistic analysis from the lexical to the shallow syntactic level.", "labels": [], "entities": []}, {"text": "Translation models are built on top of combinations of these alignments.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9635589122772217}]}, {"text": "We present results for the Spanish-to-English and English-to-Spanish tasks.", "labels": [], "entities": []}, {"text": "We show that liniguistic information maybe helpful, specially when the target language has a rich morphology.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments.", "labels": [], "entities": [{"text": "word and phrase alignments", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.6502573117613792}]}, {"text": "In the last years, many efforts have been devoted to this matter (.", "labels": [], "entities": []}, {"text": "Following our previous work (), we use shallow syntactic information to generate more precise alignments.", "labels": [], "entities": []}, {"text": "Far from full syntactic complexity, we suggest going back to the simpler alignment methods first described by.", "labels": [], "entities": []}, {"text": "Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks).", "labels": [], "entities": []}, {"text": "Apart from redefining the scope of the alignment unit, we may use different linguistic data views.", "labels": [], "entities": []}, {"text": "We enrich tokens with features further than lexical such as part-of-speech (PoS), lemma, and chunk IOB label.", "labels": [], "entities": []}, {"text": "For instance, suppose the case illustrated in where the lexical item 'plays' is seen acting as a verb and as a noun.", "labels": [], "entities": []}, {"text": "Considering these two words, with the same lexical realization, as a single token adds noise to the word alignment process.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7114828824996948}]}, {"text": "Representing this information, by means of linguistic data views, as 'playsV BZ ' and 'playsNNS' would allow us to distinguish between the two cases.", "labels": [], "entities": []}, {"text": "Ideally, one would wish to have still deeper information, moving through syntax onto semantics, such as word senses.", "labels": [], "entities": []}, {"text": "Therefore, it would be possible to distinguish for instance between two realizations of 'plays' with different meanings: 'hePRP playsV BG guitarNN ' and 'hePRP playsV BG footballNN '.", "labels": [], "entities": [{"text": "footballNN", "start_pos": 170, "end_pos": 180, "type": "DATASET", "confidence": 0.8319463133811951}]}, {"text": "Of course, there is a natural trade-off between the use of linguistic data views and data sparsity.", "labels": [], "entities": []}, {"text": "Fortunately, we hava data enough so that statistical parameter estimation remains reliable.", "labels": [], "entities": [{"text": "statistical parameter estimation", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.7003719806671143}]}, {"text": "The approach which is closest to ours is that by who suggested a combination of models based on shallow syntactic analysis (part-of-speech tagging and phrase chunking).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7423357963562012}, {"text": "phrase chunking", "start_pos": 151, "end_pos": 166, "type": "TASK", "confidence": 0.7215792089700699}]}, {"text": "They followed a backoff strategy in the application of their models.", "labels": [], "entities": []}, {"text": "Decoding was based on Finite State Automata.", "labels": [], "entities": []}, {"text": "Although no significant improvement in MT quality was reported, results were promising taking into account the short time spent in the development of the linguistic tools utilized.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9920448064804077}]}, {"text": "Our system is further described in Section 2.", "labels": [], "entities": []}, {"text": "Results are reported in Section 3.", "labels": [], "entities": []}, {"text": "Conclusions and further work are briefly outlined in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: MT results comparing the LDV-COMBO system to a baseline system, for the test set both on the  Spanish-to-English and English-to-Spanish tasks.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8938175439834595}]}]}