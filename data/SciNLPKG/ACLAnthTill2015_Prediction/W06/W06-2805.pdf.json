{"title": [{"text": "Learning to Recognize Blogs: A Preliminary Exploration", "labels": [], "entities": [{"text": "Learning to Recognize Blogs", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6994031220674515}]}], "abstractContent": [{"text": "We present results of our experiments with the application of machine learning on binary blog classification, i.e. determining whether a given web page is a blog page.", "labels": [], "entities": [{"text": "binary blog classification", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.7508301933606466}]}, {"text": "We have gathered a corpus in excess of half a million blog or blog-like pages and pre-classified them using a simple baseline.", "labels": [], "entities": []}, {"text": "We investigate which algorithms attain the best results for our classification problem and experiment with resampling techniques, with the aim of utilising our large dataset to improve upon our baseline.", "labels": [], "entities": [{"text": "classification problem", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9190549552440643}]}, {"text": "We show that the application of off-the-shelf machine learning technology to perform binary blog classification offers substantial improvement over our baseline.", "labels": [], "entities": [{"text": "binary blog classification", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.6543885668118795}]}, {"text": "Further gains can sometimes be achieved using resampling techniques, but these improvements are relatively small compared to the initial gain.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, weblogs (online journals in which the owner posts entries on a regular basis) have not only rapidly become popular as anew and easily accessible publishing tool for the masses, but its content is becoming evermore valuable as a \"window to the world,\" an extensive medium brimming with subjective content that can be mined and analysed to discover what people are talking about and why.", "labels": [], "entities": []}, {"text": "In recent years the volume of blogs is estimated to have doubled approximately every six months.", "labels": [], "entities": []}, {"text": "Technorati report that about 11% of internet users are blog readers and that about 70 thousand new blogs are created daily.", "labels": [], "entities": [{"text": "Technorati", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9392496943473816}]}, {"text": "Popular blogosphere (the complete collection of all blogs) analysis tools estimate the blogosphere to contain anywhere between 20 1 and 24 million 2 blogs at time of writing.", "labels": [], "entities": []}, {"text": "Given this growing popularity and size, research on blogs and the blogosphere is also increasing.", "labels": [], "entities": []}, {"text": "A large amount of this research is being done on the content provided by the blogosphere and the nature of this content, like for example (Mishne and de), or the structure of the blogosphere (.", "labels": [], "entities": []}, {"text": "In this paper, however, we address the task of binary blog classification: given a (web) document, is this a blog or not?", "labels": [], "entities": [{"text": "binary blog classification", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7140645980834961}]}, {"text": "Our aim is to base this classification mostly on blog characteristics rather than content.", "labels": [], "entities": []}, {"text": "We will by no means ignore content but it should not become a crucial part of the classification process.", "labels": [], "entities": [{"text": "classification process", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.9078702330589294}]}, {"text": "Reliable blog classification is an important task in the blogosphere as it allows researchers, ping feeds (used to broadcast blog updates), trend analysis tools and many others to separate real blog content from blog-like content such as bulletin boards, newsgroups or trade markets.", "labels": [], "entities": [{"text": "Reliable blog classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6622466842333475}]}, {"text": "It is a task that so far has proved difficult as can be witnessed by checking any of the major blog update feeds such as weblogs.com or blo.gs.", "labels": [], "entities": []}, {"text": "Both will at any given time list content that clearly is not a blog.", "labels": [], "entities": []}, {"text": "In this paper we will explore blog classification using machine learning to improve blog detection and experiment with several methods to try and further improve the percentage of instances classified correctly.", "labels": [], "entities": [{"text": "blog classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7117487788200378}, {"text": "blog detection", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.7503092288970947}]}, {"text": "The main research question we address in this paper is exploratory in nature: -How hard is binary blog classification?", "labels": [], "entities": [{"text": "binary blog classification", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.6542341907819113}]}, {"text": "Put more specifically, -What is the performance of basic off-theshelf machine learning algorithms on this task? and -Can the performance of these methods be improved using resampling methods such as bootstrapping and co-training?", "labels": [], "entities": []}, {"text": "An important complicating factor is the lack of labeled data.", "labels": [], "entities": []}, {"text": "It is widely accepted that given a sufficient amount of training data, most machine learning algorithms will achieve similar performance levels.", "labels": [], "entities": []}, {"text": "For our experiments, we will have a very limited amount of training material available.", "labels": [], "entities": []}, {"text": "Therefore, we expect to see substantial differences between algorithms.", "labels": [], "entities": []}, {"text": "In this paper we will first discuss related work in the following section, before describing the experiments in detail and reporting on the results.", "labels": [], "entities": []}, {"text": "Finally, we will draw conclusions based on the experiments and the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our later resampling experiments, a large amount of data was gathered, as will be explained further on in this paper.", "labels": [], "entities": [{"text": "resampling", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.9699478149414062}]}, {"text": "To create a dataset for this experiment, 201 blog / blog-like pages were randomly selected from the collection, processed into Weka's arff format and manually annotated.", "labels": [], "entities": []}, {"text": "These instances were then excluded from the rest of the collection.", "labels": [], "entities": []}, {"text": "This yielded a small but reliable dataset, which we hoped would be sufficient for this task.", "labels": [], "entities": []}, {"text": "For this experiment, we trained a wide range of learners using the manually annotated data and tested using ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "We then compared the results to a baseline.", "labels": [], "entities": []}, {"text": "This baseline is based mostly on simple heuristics, and is an extended version of the WWWBlog-Identify 8 perl module that is freely available online.", "labels": [], "entities": [{"text": "WWWBlog-Identify 8 perl module", "start_pos": 86, "end_pos": 116, "type": "DATASET", "confidence": 0.8547223508358002}]}, {"text": "First of all, a URL check is done which looks fora large number of the wellknown blog hosts as an indicator.", "labels": [], "entities": []}, {"text": "Should this fail, a search is done for metatags which indicate the use of well-known blog creation tools such as Nucleus, Greymatter, 10 Movable Type 11 etc.", "labels": [], "entities": [{"text": "Greymatter", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.9041929841041565}]}, {"text": "Should this also fail, an actual content search is done for other indicators such as particular icons blog creation tools leave on pages (\"created using\u2026 .gif\" etc).", "labels": [], "entities": []}, {"text": "Next, the module checks for an RSS feed, and as a very last resort checks the number of times the term \"blog\" is used on the page as an indicator.", "labels": [], "entities": [{"text": "RSS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8715160489082336}]}, {"text": "In earlier research, our version of the module was manually tested by a small group of individuals and found to have an accuracy of roughly 80% which means it is very useful as a target to aim for with our machine learning algorithms and a good baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9993764758110046}]}, {"text": "It is clear that all algorithms bar ZeroR perform well, most topping 90%.", "labels": [], "entities": []}, {"text": "ZeroR achieves no more than 73%, and is the only algorithm that actually performs worse than our baseline.", "labels": [], "entities": []}, {"text": "The best algorithm for this task, and on this dataset, is clearly the support vector-based algorithm SMO, which scores 94.75%.", "labels": [], "entities": [{"text": "SMO", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.3670058250427246}]}, {"text": "These scores can be considered excellent fora classification task, and the wide success across the range of algorithms shows that our attribute selection has been a success.", "labels": [], "entities": []}, {"text": "The attributes clearly describe the data well.", "labels": [], "entities": []}, {"text": "Generally, bootstrapping is an iterative process whereat every iteration unlabeled data is labeled using predictions made by the learner model based on the previously available training set ().", "labels": [], "entities": []}, {"text": "These newly labeled instances are then added to the training set and the whole process repeats.", "labels": [], "entities": []}, {"text": "Our expectation was that the increase in available training instances should improve the algorithm's accuracy, especially as it proved quite accurate to begin with so the algorihm's predictions should prove quite reliable.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9994295239448547}]}, {"text": "For this experiment we used the best performing algorithm from Section 3, the SMO supportvector based algorithm.", "labels": [], "entities": []}, {"text": "The bootstrapping method is applied to this problem as follows: -Initialisation: use the training set containing 100 manually annotated instances to predict the labels of the first subset of 1000 unlabeled instances.", "labels": [], "entities": []}, {"text": "-Iterations: Label the unlabeled instances according to the algorithm's prediction and add these instances to the previous training set to form anew training set.", "labels": [], "entities": []}, {"text": "Build anew model based on the new training set and use it to predict the labels of the next subset.", "labels": [], "entities": []}, {"text": "As mentioned in Section 2, we will use the predictions of several of the most successful learning algorithms from Section 3 as our indicators in this experiment.", "labels": [], "entities": []}, {"text": "The goal of our co-training experiment is to take unanimous predictions from the three best performing algorithms from Section 3, and use those predictions, which we assume to have a very high degree of confidence, to bootstrap the training set.", "labels": [], "entities": []}, {"text": "We will then test to see if it offers an improvement over the SMO algorithm by itself.", "labels": [], "entities": [{"text": "SMO", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9410815238952637}]}, {"text": "By unanimous predictions we mean the predictions of those instances, on which all the algorithms agree unanimously after they have been allowed to predict labels using their respective models.", "labels": [], "entities": []}, {"text": "As instances for which the predictions are unanimous can be reasoned to have a very high level of confidence, the predictions for those instances are almost certainly correct.", "labels": [], "entities": []}, {"text": "Therefore we expect this method to offer substantial improvements over any single algorithm as it potentially yields a very large number of correctly labeled instances for the learner to train on.", "labels": [], "entities": []}, {"text": "We chose to adapt the co-training idea in this fashion as we believe it to be a good way of radically reducing the fuzziness of potential predictions and away to gain a very high degree of confidence in the labels attached to previously unlabeled data.", "labels": [], "entities": []}, {"text": "Should the algorithms disagree on a large number of instances there would still not be a problem as we have a very large pool of unlabeled instances (133.000, we only used part of our corpus for our experiments as our dataset was so large that there was no need to use all the data available).", "labels": [], "entities": []}, {"text": "The potential maximum of 133 iterations should prove quite sufficient even if the growth of the training set per iteration proves to be very small.", "labels": [], "entities": []}, {"text": "The algorithms we chose for this experiment were SMO (support vector), J48 (decision tree, a C4.5 implementation) and Jrip (rule based).", "labels": [], "entities": []}, {"text": "We chose not to use nearest neighbour algorithms for this experiment even though they performed well individually as we feared it would prove a less successful approach given the large training set sizes.", "labels": [], "entities": []}, {"text": "Indeed, an earlier experiment done during our blog classification research showed the performance of near neighbour algorithms bottomed out very quickly so no real improvement can be expected from those algorithms given larger training sets and given the unanimous nature of this method of co-training it may spoil any gain that might otherwise be achieved.", "labels": [], "entities": []}, {"text": "The process started with the manually annotated training set and used the predictions from the three algorithms, for unlabeled instances they agree unanimously on, to label those instances.", "labels": [], "entities": []}, {"text": "Those instances were subsequently added to the trainingset and using this new trainingset, a number of the instances in another unlabeled set (1000 instances per set) were to be labeled (again, only those instances on which the algorithms agree unanimously).", "labels": [], "entities": []}, {"text": "Once again, those instances are added to the training set and soon and so forth for as many iterations as possible.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Overview of results using our unani- mous co-training method.", "labels": [], "entities": []}]}