{"title": [{"text": "Tools to Address the Interdependence between Tokenisation and Standoff Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we discuss technical issues arising from the interdependence between tokenisation and XML-based annotation tools, in particular those which use stand-off annotation in the form of pointers to word tokens.", "labels": [], "entities": []}, {"text": "It is common practice for an XML-based annotation tool to use word tokens as the target units for annotating such things as named entities because it provides appropriate units for stand-off annotation.", "labels": [], "entities": []}, {"text": "Furthermore, these units can be easily selected, swept out or snapped to by the annotators and certain classes of annotation mistakes can be prevented by building a tool that does not permit selection of a substring which does not entirely span one or more XML elements.", "labels": [], "entities": []}, {"text": "There is a downside to this method of annotation , however, in that it assumes that for any given data set, in whatever domain, the optimal tokenisation is known before any annotation is performed.", "labels": [], "entities": []}, {"text": "If mistakes are made in the initial tokenisation and the word boundaries conflict with the annota-tors' desired actions, then either the annotation is inaccurate or expensive retokeni-sation and reannotation will be required.", "labels": [], "entities": []}, {"text": "Here we describe the methods we have developed to address this problem.", "labels": [], "entities": []}, {"text": "We also describe experiments which explore the effects of different granularities of to-kenisation on NER tagger performance.", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 102, "end_pos": 112, "type": "TASK", "confidence": 0.9543685913085938}]}], "introductionContent": [{"text": "A primary consideration when designing an annotation tool for annotation tasks such as Named Entity (NE) annotation is to provide an interface that makes it easy for the annotator to select contiguous stretches of text for labelling ().", "labels": [], "entities": []}, {"text": "This can be accomplished by enabling actions such as click and snapping to the ends of word tokens.", "labels": [], "entities": []}, {"text": "Not only do such features make the task easier for annotators, they also help to reduce certain kinds of annotator error which can occur with interfaces which require the annotator to sweep out an area of text: without the safeguard of requiring annotations to span entire tokens, it is easy to sweep too little or too much text and create an annotation which takes in too few or too many characters.", "labels": [], "entities": []}, {"text": "Thus the tokenisation of the text should be such that it achieves an optimal balance between increasing annotation speed and reducing annotation error rate.", "labels": [], "entities": [{"text": "annotation speed", "start_pos": 104, "end_pos": 120, "type": "METRIC", "confidence": 0.8456171751022339}, {"text": "annotation error rate", "start_pos": 134, "end_pos": 155, "type": "METRIC", "confidence": 0.8549186786015829}]}, {"text": "In Section 2 we describe a recently implemented XMLbased annotation tool which we have used to create an NE-annotated corpus in the biomedical domain.", "labels": [], "entities": []}, {"text": "This tool uses standoff annotation in a similar way to the NXT annotation tool (), though the annotations are recorded in the same file, rather than in a separate file.", "labels": [], "entities": []}, {"text": "To perform annotation with this tool, it is necessary to first tokenise the text and identify sentence and word tokens.", "labels": [], "entities": []}, {"text": "We have found however that conflicts can arise between the segmentation that the tokeniser creates and the segmentation that the annotator needs, especially in scientific text where many details of correct tokenisation are not apparent in advance to a non-expert in the domain.", "labels": [], "entities": []}, {"text": "We discuss this problem in Section 3 and illustrate it with examples from two domains, biomedicine and astrophysics.", "labels": [], "entities": []}, {"text": "In order to meet requirements from both the annotation tool and the tokenisation needs of the annotators, we have extended our tool to allow: Screenshot of the Annotation Tool the annotator to override the initial tokenisation where necessary and we have developed a method of recording the result of overriding in the XML mark-up.", "labels": [], "entities": []}, {"text": "This allows us to keep a record of the optimal annotation and ensures that it will not be necessary to take the expensive step of having data reannotated in the event that the tokenisation needs to be redone.", "labels": [], "entities": []}, {"text": "As improved tokenisation procedures become available we can retokenise both the annotated material and the remaining unannotated data using a program which we have developed for this task.", "labels": [], "entities": [{"text": "tokenisation", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9685723185539246}]}, {"text": "We describe the extension to the annotation tool, the XML representation of conflict and the retokenisation program in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: NER Results for Different Tokenisations  of the BioNLP corpus", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7108553647994995}, {"text": "BioNLP", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.8772218227386475}]}]}