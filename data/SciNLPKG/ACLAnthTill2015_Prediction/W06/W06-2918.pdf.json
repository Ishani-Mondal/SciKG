{"title": [{"text": "Using Gazetteers in Discriminative Information Extraction", "labels": [], "entities": [{"text": "Discriminative Information Extraction", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.6125829915205637}]}], "abstractContent": [{"text": "Much work on information extraction has successfully used gazetteers to recognise uncommon entities that cannot be reliably identified from local context alone.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8866903483867645}]}, {"text": "Approaches to such tasks often involve the use of maximum entropy-style models, where gazetteers usually appear as highly informative features in the model.", "labels": [], "entities": []}, {"text": "Although such features can improve model accuracy, they can also introduce hidden negative effects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9870563745498657}]}, {"text": "In this paper we describe and analyse these effects and suggest ways in which they maybe overcome.", "labels": [], "entities": []}, {"text": "In particular, we show that by quarantining gazetteer features and training them in a separate model, then decoding using a logarithmic opinion pool (Smith et al., 2005), we may achieve much higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9965051412582397}]}, {"text": "Finally, we suggest ways in which other features with gazetteer feature-like behaviour maybe identified.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER), noun phrase chunking and information extraction from research papers ().", "labels": [], "entities": [{"text": "information extraction tasks in natural language processing (NLP)", "start_pos": 98, "end_pos": 163, "type": "TASK", "confidence": 0.7383947193622589}, {"text": "named entity recognition (NER)", "start_pos": 173, "end_pos": 203, "type": "TASK", "confidence": 0.7881461282571157}, {"text": "noun phrase chunking", "start_pos": 205, "end_pos": 225, "type": "TASK", "confidence": 0.7467374006907145}, {"text": "information extraction from research papers", "start_pos": 230, "end_pos": 273, "type": "TASK", "confidence": 0.8678162336349488}]}, {"text": "Discriminative models offer a significant advantage over their generative counterparts by allowing the specification of powerful, possibly non-independent features which would be difficult to tractably encode in a generative model.", "labels": [], "entities": []}, {"text": "Ina task such as NER, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has not be seen before.", "labels": [], "entities": [{"text": "NER", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9304061532020569}]}, {"text": "In these cases, a gazetteer or dictionary of possible entity identifiers is often useful.", "labels": [], "entities": []}, {"text": "Such identifiers could be names of people, places, companies or other organisations.", "labels": [], "entities": []}, {"text": "Using gazetteers one may define additional features in the model that represent the dependencies between a word's NER label and its presence in a particular gazetteer.", "labels": [], "entities": []}, {"text": "Such gazetteer features are often highly informative, and their inclusion in the model should in principle result in higher model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9897359609603882}]}, {"text": "However, these features can also introduce hidden negative effects taking the form of labelling errors that the model makes at places where a model without the gazetteer features would have labelled correctly.", "labels": [], "entities": []}, {"text": "Consequently, ensuring optimal usage of gazetteers can be difficult.", "labels": [], "entities": []}, {"text": "In this paper we describe and analyse the labelling errors made by a model, and show that they generally result from the model's over-dependence on the gazetteer features for making labelling decisions.", "labels": [], "entities": []}, {"text": "By including gazetteer features in the model we may, in some cases, transfer too much explanatory dependency to the gazetteer features from the nongazetteer features.", "labels": [], "entities": []}, {"text": "In order to avoid this problem, a more careful treatment of these features is required during training.", "labels": [], "entities": []}, {"text": "We demonstrate that a traditional regularisation approach, where different features are regularised to different degrees, does not offer a sat-isfactory solution.", "labels": [], "entities": [{"text": "regularisation", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9592943787574768}]}, {"text": "Instead, we show that by training gazetteer features in a separate model to the other features, and decoding using a logarithmic opinion pool (LOP) (), much greater accuracy can be obtained.", "labels": [], "entities": [{"text": "logarithmic opinion pool (LOP)", "start_pos": 117, "end_pos": 147, "type": "METRIC", "confidence": 0.67673559486866}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.998831570148468}]}, {"text": "Finally, we identify other features with gazetteer feature-like properties and show that similar results maybe obtained using our method with these features.", "labels": [], "entities": []}, {"text": "We take as our model a linear chain conditional random field (CRF), and apply it to NER in English.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experimental setup, and provide results for the baseline models.", "labels": [], "entities": []}, {"text": "Named entity recognition (NER) involves the identification of the location and type of pre-defined entities within a sentence.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7959022174278895}, {"text": "identification of the location and type of pre-defined entities within a sentence", "start_pos": 44, "end_pos": 125, "type": "TASK", "confidence": 0.46742183715105057}]}, {"text": "The CRF is presented with a set of sentences and must label each word so as to indicate whether the word appears outside an entity, at the beginning of an entity of a certain type or within the continuation of an entity of a certain type.", "labels": [], "entities": []}, {"text": "Our results are reported on the CoNLL-2003 shared task English dataset.", "labels": [], "entities": [{"text": "CoNLL-2003 shared task English dataset", "start_pos": 32, "end_pos": 70, "type": "DATASET", "confidence": 0.8537446975708007}]}, {"text": "For this dataset the entity types are: persons (PER), locations (LOC), organisations (ORG) and miscellaneous (MISC).", "labels": [], "entities": []}, {"text": "The training set consists of 14", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model F scores", "labels": [], "entities": []}, {"text": " Table 2: Test set errors", "labels": [], "entities": []}, {"text": " Table 3: FDR development set F scores", "labels": [], "entities": [{"text": "FDR development set", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.794770081837972}, {"text": "F", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.5830470323562622}]}, {"text": " Table 4: standard+g feature subsets", "labels": [], "entities": []}, {"text": " Table 4. The simple structural features model label- label and label-word dependencies, while the ad-", "labels": [], "entities": []}, {"text": " Table 5: Reg. LOP F scores", "labels": [], "entities": [{"text": "Reg. LOP F", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.6901653632521629}]}, {"text": " Table 6: Reg. LOP weights", "labels": [], "entities": [{"text": "Reg. LOP", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.7486234505971273}]}, {"text": " Table 7: Test set errors", "labels": [], "entities": []}, {"text": " Table 8: Unreg. LOP F scores", "labels": [], "entities": [{"text": "Unreg. LOP F", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.6336459964513779}]}, {"text": " Table 9: Reg. LOP F scores", "labels": [], "entities": [{"text": "Reg. LOP F", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.701603427529335}]}]}