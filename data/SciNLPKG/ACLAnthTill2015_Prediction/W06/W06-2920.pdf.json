{"title": [{"text": "CoNLL-X shared task on Multilingual Dependency Parsing", "labels": [], "entities": [{"text": "Multilingual Dependency Parsing", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.5968697369098663}]}], "abstractContent": [{"text": "Each year the Conference on Computational Natural Language Learning (CoNLL) 1 features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.", "labels": [], "entities": [{"text": "Conference on Computational Natural Language Learning (CoNLL) 1", "start_pos": 14, "end_pos": 77, "type": "TASK", "confidence": 0.6855850696563721}]}, {"text": "The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.", "labels": [], "entities": [{"text": "CoNLL (CoNLL-X)", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8487776070833206}, {"text": "Multilingual Dependency Parsing", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.7627341151237488}]}, {"text": "In this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.", "labels": [], "entities": []}, {"text": "We also give an overview of the parsing approaches that participants took and the results that they achieved.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9745303392410278}]}, {"text": "Finally, we try to draw general conclusions about multilingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?", "labels": [], "entities": [{"text": "multilingual parsing", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6555652171373367}]}, {"text": "Acknowledgement Many thanks to Amit Dubey and Yuval Kry-molowski, the other two organizers of the shared task, for discussions, converting treebanks, writing software and helping with the papers.", "labels": [], "entities": [{"text": "Acknowledgement", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9158315062522888}]}], "introductionContent": [{"text": "Previous CoNLL shared tasks focused on NP chunking (1999), general chunking (2000), clause identification, named entity recognition, and semantic role labeling.", "labels": [], "entities": [{"text": "NP chunking (1999)", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8459312558174134}, {"text": "clause identification", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.8708504140377045}, {"text": "named entity recognition", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.6088601648807526}, {"text": "semantic role labeling", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.6762868960698446}]}, {"text": "This shared task on full (dependency) parsing is the logical next step.", "labels": [], "entities": [{"text": "full (dependency) parsing", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6131199955940246}]}, {"text": "Parsing is an important preprocessing step for many NLP applications and therefore of considerable practical interest.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9028708338737488}]}, {"text": "It is a complex task and as it is not straightforwardly mappable to a \"classical\" segmentation, classification or sequence prediction problem, it also poses theoretical challenges to machine learning researchers.", "labels": [], "entities": [{"text": "segmentation, classification or sequence prediction", "start_pos": 82, "end_pos": 133, "type": "TASK", "confidence": 0.808964341878891}]}, {"text": "During the last decade, much research has been done on data-driven parsing and performance has increased steadily.", "labels": [], "entities": [{"text": "data-driven parsing", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.4780273288488388}]}, {"text": "For training these parsers, syntactically annotated corpora (treebanks) of thousands to tens of thousands of sentences are necessary; so initially, research has focused on English.", "labels": [], "entities": []}, {"text": "During the last few years, however, treebanks for other languages have become available and some parsers have been applied to several different languages.", "labels": [], "entities": []}, {"text": "See Section 2 fora more detailed overview of related previous research.", "labels": [], "entities": []}, {"text": "So far, there has not been much comparison between different dependency parsers on exactly the same data sets (other than for English).", "labels": [], "entities": []}, {"text": "One of the reasons is the lack of a de-facto standard for an evaluation metric (labeled or unlabeled, separate root accuracy?), for splitting the data into training and testing portions and, in the case of constituency treebanks converted to dependency format, for this conversion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.6904980540275574}]}, {"text": "Another reason are the various annotation schemes and logical data formats used by different treebanks, which make it tedious to apply a parser to many treebanks.", "labels": [], "entities": []}, {"text": "We hope that this shared task will improve the situation by introducing a uniform approach to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8715084195137024}]}, {"text": "See Section 3 for the detailed task definition and Section 4 for information about the conversion of all 13 treebanks.", "labels": [], "entities": []}, {"text": "In this shared task, participants had two to three months to implement a parsing system that could be trained for all these languages and four days to parse unseen test data for each.", "labels": [], "entities": []}, {"text": "19 participant groups submitted parsed test data.", "labels": [], "entities": []}, {"text": "Of these, all but one parsed all 12 required languages and 13 also parsed the optional Bulgarian data.", "labels": [], "entities": [{"text": "Bulgarian data", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.7453260123729706}]}, {"text": "A wide variety of parsing approaches were used: some are extensions of previously published approaches, others are new.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9747119545936584}]}, {"text": "See Section 5 for an overview.", "labels": [], "entities": []}, {"text": "Systems were scored by computing the labeled attachment score (LAS), i.e. the percentage of \"scoring\" tokens for which the system had predicted the correct head and dependency label.", "labels": [], "entities": [{"text": "labeled attachment score (LAS)", "start_pos": 37, "end_pos": 67, "type": "METRIC", "confidence": 0.8365641882022222}]}, {"text": "Punctuation tokens were excluded from scoring.", "labels": [], "entities": []}, {"text": "Results across languages and systems varied widely from 37.8% (worst score on Turkish) to 91.7% (best score on Japanese).", "labels": [], "entities": []}, {"text": "See Section 6 for detailed results.", "labels": [], "entities": []}, {"text": "However, variations are consistent enough to allow us to draw some general conclusions.", "labels": [], "entities": []}, {"text": "Section 7 discusses the implications of the results and analyzes the remaining problems.", "labels": [], "entities": []}, {"text": "Finally, Section 8 describes possible directions for future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Characteristics of the data sets for the 13 languages (abbreviated by their first two letters): language family (Semitic,", "labels": [], "entities": []}]}