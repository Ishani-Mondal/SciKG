{"title": [{"text": "Shared-task Evaluations in HLT: Lessons for NLG", "labels": [], "entities": [{"text": "Shared-task Evaluations", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8379600048065186}, {"text": "NLG", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.43572065234184265}]}], "abstractContent": [], "introductionContent": [{"text": "While natural language generation (NLG) has a strong evaluation tradition, in particular in userbased and task-oriented evaluation, it has never evaluated different approaches and techniques by comparing their performance on the same tasks (shared-task evaluation, STE).", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 6, "end_pos": 39, "type": "TASK", "confidence": 0.8241011102994283}]}, {"text": "NLG is characterised by alack of consolidation of results, and by isolation from the rest of NLP where STE is now standard.", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8392136096954346}, {"text": "STE", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.4544134736061096}]}, {"text": "It is, moreover, a shrinking field (state-of-the-art MT and summarisation no longer perform generation as a subtask) which lacks the kind of funding and participation that natural language understanding (NLU) has attracted.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9594618082046509}, {"text": "natural language understanding (NLU)", "start_pos": 172, "end_pos": 208, "type": "TASK", "confidence": 0.8012979179620743}]}, {"text": "Evidence from other NLP fields shows that STE campaigns (STECs) can lead to rapid technological progress and substantially increased participation.", "labels": [], "entities": [{"text": "STE campaigns (STECs)", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.8611815333366394}]}, {"text": "The past year has seen a groundswell of interest in comparative evaluation among NLG researchers, the first comparative results are being reported ), and the move towards some form of comparative evaluation seems inevitable.", "labels": [], "entities": [{"text": "comparative evaluation", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.8254210948944092}, {"text": "comparative evaluation", "start_pos": 184, "end_pos": 206, "type": "TASK", "confidence": 0.8729357123374939}]}, {"text": "In this paper we look at how two decades of NLP STECs might help us decide how best to make this move.", "labels": [], "entities": [{"text": "NLP STECs", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.5519198030233383}]}], "datasetContent": [{"text": "Over the past twenty years, virtually every field of research inhuman language technology (HLT) has introduced STECs.", "labels": [], "entities": [{"text": "inhuman language technology (HLT)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.662569502989451}]}, {"text": "A small selection is presented in the table below . NLG researchers have tended to be somewhat unconvinced of the benefits of comparative evaluation in general, and the kind of competitive, numbers-driven STECs that have been typical of NLU in particular.", "labels": [], "entities": []}, {"text": "Yet STECs do not have to be hugely competitive events fixated on one task with associated input/output data and single evaluation metric, static overtime.", "labels": [], "entities": [{"text": "STECs", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.9249110221862793}]}, {"text": "Tasks: There is a distinction between (i) evaluations designed to help potential users to decide 1 Apologies for omissions, and for bias towards English. whether the technology will be valuable to them, and (ii) evaluations designed to help system developers improve the core technology).", "labels": [], "entities": [{"text": "Apologies", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9943451881408691}]}, {"text": "In the former, the application context is a critical variable in the task definition; in the latter it is fixed.", "labels": [], "entities": []}, {"text": "Developer-oriented evaluation promotes focus on the task in isolation, but if the context is fixed badly, or if the outside world changes but the evaluation does not, then it becomes irrelevant.", "labels": [], "entities": []}, {"text": "NLP STECs have so far focused on developer-oriented evaluation, but there are increasing calls for more 'embedded', more task-based types of evaluations 2 . Existing NLP STECs show that tasks need to be broadly based and continuously evolving.", "labels": [], "entities": []}, {"text": "To begin with, the task needs to be simple, easy to understand and easy for people to recognise as their task.", "labels": [], "entities": []}, {"text": "Over time, as the limitations of the simple task are noted and a more substantial community is 'on board', tasks can multiply, diversify and become more sophisticated.", "labels": [], "entities": []}, {"text": "This is something that TREC has been good at (still going strong 14 years on), and the parsing community has failed to achieve (see notes in table).", "labels": [], "entities": [{"text": "TREC", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.6322518587112427}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9796379208564758}]}, {"text": "Evaluation: NLP STECs have tended to use automatic evaluations because of their speed and reproducibility, but some have used human evaluators, in particular in fields where language is generated (MT, summarisation, speech synthesis).", "labels": [], "entities": [{"text": "MT, summarisation", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.7296098669370016}]}, {"text": "Evaluation scores are not independent of the task and context for which they are calculated.", "labels": [], "entities": []}, {"text": "This is clearly true of human-based evaluation, but even scores by a simple metric like word error rate in speech recognition are not comparable unless certain parameters are the same: backgroundnoise, language, whether or not speech is controlled.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 88, "end_pos": 103, "type": "METRIC", "confidence": 0.7046298185984293}, {"text": "speech recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.6372592449188232}]}, {"text": "Development of evaluation methods and benchmark tasks therefore must go hand in hand.", "labels": [], "entities": []}, {"text": "Evaluation methods have to be accepted by the research community as providing a true approxi- This has proved problematic: the parsing community no longer accepts the PARSEVAL measure, but there has been no organisational framework for establishing an alternative.", "labels": [], "entities": [{"text": "parsing community", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.8875471651554108}, {"text": "PARSEVAL measure", "start_pos": 167, "end_pos": 183, "type": "METRIC", "confidence": 0.6108143031597137}]}, {"text": "2. SEMEVAL did not proceed largely because it was too ambitious and agreement between people with different interests and theoretical positions was not achieved.", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.852912425994873}]}, {"text": "It was eventually reduced in scope and aspects became incorporated in MUC, SUMMAC and SENSEVAL.", "labels": [], "entities": [{"text": "MUC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9220393300056458}, {"text": "SUMMAC", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.6145930290222168}, {"text": "SENSEVAL", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.6612341403961182}]}, {"text": "3. MT has been transformed by corpus methods, which have shifted MT from a backwater to perhaps the most vibrant area of NLP in the last five years.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9410825967788696}, {"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9745954275131226}]}, {"text": "4. In TC-STAR, the SST task is broken down into numerous subtasks.", "labels": [], "entities": [{"text": "TC-STAR", "start_pos": 6, "end_pos": 13, "type": "TASK", "confidence": 0.653401792049408}, {"text": "SST task", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.9212178289890289}]}, {"text": "The modules and systems that meet the given criteria are exchanged among the participants, lowering the barrier to entry.", "labels": [], "entities": []}, {"text": "E.g. BLEU is strongly disliked in the non-statistical part of the MT community because it is biased in favour of statistical MT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9952352643013}, {"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9720923900604248}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9037827849388123}]}, {"text": "PARSEVAL stopped being used when the parsing community moved towards dependency parsing and related approaches.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.970535159111023}, {"text": "dependency parsing", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8414730429649353}]}, {"text": "Sharing: As PARSEVAL shows, measures and resources alone are not enough.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.5180807113647461}]}, {"text": "Also required are (i) an event (or better, cycle of events) so people can attend and feel part of a community; (ii) a forum for reviewing task definitions and evaluation methods; (iii) a committee which 'owns' the STEC, and organises the next campaign.", "labels": [], "entities": [{"text": "STEC", "start_pos": 214, "end_pos": 218, "type": "TASK", "confidence": 0.527628481388092}]}, {"text": "Funding is usually needed for gold-standard corpus creation but rarely for anything else).", "labels": [], "entities": [{"text": "gold-standard corpus creation", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.6132235825061798}]}, {"text": "Participants can be expected to cover the cost of system development and workshop attendance.", "labels": [], "entities": []}, {"text": "A funded project is best seen as supporting and enabling the STEC (especially during the early stages) rather than being it.", "labels": [], "entities": [{"text": "STEC", "start_pos": 61, "end_pos": 65, "type": "TASK", "confidence": 0.864118754863739}]}, {"text": "In sum, STECs are good for community building.", "labels": [], "entities": [{"text": "STECs", "start_pos": 8, "end_pos": 13, "type": "TASK", "confidence": 0.856356143951416}, {"text": "community building", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7885345816612244}]}, {"text": "They produce energy (as we saw when the possibility was raised for NLG at UCNLG'05 and ENLG'05) which can lead to rapid scientific and technological progress.", "labels": [], "entities": [{"text": "UCNLG'05", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.8513879776000977}, {"text": "ENLG'05", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9271180629730225}]}, {"text": "They make the field look like a game and draw people in.", "labels": [], "entities": []}], "tableCaptions": []}