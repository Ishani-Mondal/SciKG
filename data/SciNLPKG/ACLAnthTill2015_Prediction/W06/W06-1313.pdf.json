{"title": [{"text": "An Information State-Based Dialogue Manager for Call for Fire Dialogues", "labels": [], "entities": [{"text": "Call for Fire Dialogues", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6073746755719185}]}], "abstractContent": [{"text": "We present a dialogue manager for \"Call for Fire\" training dialogues.", "labels": [], "entities": [{"text": "Call for Fire\" training dialogues", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.648081382115682}]}, {"text": "We describe the training environment, the domain, the features of its novel information state-based dialogue manager, the system it is apart of, and preliminary evaluation results.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We conducted an evaluation of the Radiobot-CFF system in fully-automated, semi-automated, and human-controlled conditions.", "labels": [], "entities": []}, {"text": "The system performed well in a number of measures; for example, shows the scores for median time-tofire and task-completion rates.", "labels": [], "entities": []}, {"text": "Additional measures and further details are available in).", "labels": [], "entities": []}, {"text": "Of particular relevance here, we performed an evaluation of the dialogue manager, using the evaluation corpus of 17 missions run on 8 sessions, a total of 408 FO utterances.", "labels": [], "entities": []}, {"text": "We took transcribed recordings of the FO utterances, ran them through the Interpreter, and corrected them.", "labels": [], "entities": [{"text": "FO utterances", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8813376426696777}, {"text": "Interpreter", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9051160216331482}]}, {"text": "For each session, we ran corrected Interpreter output through the Dialogue Manager to printout the values of the informational components at the end of every turn.", "labels": [], "entities": []}, {"text": "We then corrected those, and compared the corrections to the uncorrected values to receive precision, accuracy, and f-scores of 0.99 each.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9997729659080505}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.998729407787323}, {"text": "f-scores", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9917240738868713}]}], "tableCaptions": [{"text": " Table 1: Example Evaluation Measures", "labels": [], "entities": [{"text": "Example Evaluation Measures", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.743864099184672}]}]}