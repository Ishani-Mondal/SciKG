{"title": [{"text": "SPMT: Statistical Machine Translation with Syntactified Target Language Phrases", "labels": [], "entities": [{"text": "SPMT", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9231831431388855}, {"text": "Statistical Machine Translation", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.7648330330848694}]}], "abstractContent": [{"text": "We introduce SPMT, anew class of statistical Translation Models that use Syn-tactified target language Phrases.", "labels": [], "entities": [{"text": "statistical Translation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6357513070106506}]}, {"text": "The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9867421984672546}, {"text": "NIST 2003 Chinese-English test corpus", "start_pos": 101, "end_pos": 138, "type": "DATASET", "confidence": 0.9705422401428223}]}], "introductionContent": [{"text": "During the last four years, various implementations and extentions to phrase-based statistical models () have led to significant increases in machine translation accuracy.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7833636701107025}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.8553726673126221}]}, {"text": "Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.", "labels": [], "entities": []}, {"text": "Recent models that exploit syntactic information of the source language () have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.", "labels": [], "entities": []}, {"text": "And syntax-inspired formal models, in spite of being trained on significantly less data, have shown promising results when compared on the same test sets with mature phrase-based systems.", "labels": [], "entities": []}, {"text": "To our knowledge though, no previous research has demonstrated that a syntax-based statistical translation system could produce better results than a phrase-based system on a large-scale, well-established, open domain translation task.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.6926283836364746}, {"text": "open domain translation task", "start_pos": 206, "end_pos": 234, "type": "TASK", "confidence": 0.7453645467758179}]}, {"text": "In this paper we present such a system.", "labels": [], "entities": []}, {"text": "Our translation models rely upon and naturally exploit submodels (feature functions) that have been initially developed in phrase-based systems for choosing target translations of source language phrases, and use new, syntax-based translation and target language submodels for assembling target phrases into well-formed, grammatical outputs.", "labels": [], "entities": []}, {"text": "After we introduce our models intuitively, we discuss their formal underpinning and parameter training in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we present our decoder and, in Section 4, we evaluate our models empirically.", "labels": [], "entities": []}, {"text": "In Section 5, we conclude with a brief discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on a Chinese to English machine translation task.", "labels": [], "entities": [{"text": "Chinese to English machine translation task", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.6256648848454157}]}, {"text": "We use the same training corpus, 138.7M words of parallel Chinese-English data released by LDC, in order to train several statistical-based MT systems: \u2022 PBMT, a strong state of the art phrase-based system that implements the alignment template model (; this is the system ISI has used in the 2004 and 2005 NIST evaluations.", "labels": [], "entities": [{"text": "MT", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.8842081427574158}, {"text": "NIST", "start_pos": 307, "end_pos": 311, "type": "DATASET", "confidence": 0.8204458951950073}]}, {"text": "\u2022 four SPMT systems (M1, M1C, M2, M2C) that implement each of the models discussed in this paper; \u2022 a SPMT system, Comb, that combines the outputs of all SPMT models using the procedure described in Section 3.2.", "labels": [], "entities": []}, {"text": "In all systems, we use a rule extraction algorithm that limits the size of the foreign/source phrases to four words.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7295707166194916}]}, {"text": "For all systems, we use a Kneser-Ney (1995) smoothed trigram language model trained on 2.3 billion words of English.", "labels": [], "entities": []}, {"text": "As development data for the SPMT systems, we used the sentences in the 2002 NIST development corpus that are shorter than 20 words; we made this choice in order to finish all experiments in time for this submission.", "labels": [], "entities": [{"text": "SPMT", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.934752881526947}, {"text": "NIST development corpus", "start_pos": 76, "end_pos": 99, "type": "DATASET", "confidence": 0.8757112820943197}]}, {"text": "The PBMT system used all sentences in the 2002 NIST corpus for development.", "labels": [], "entities": [{"text": "PBMT system", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8661993741989136}, {"text": "NIST corpus", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.8519001305103302}]}, {"text": "As test data, we used the 2003 NIST test set.", "labels": [], "entities": [{"text": "NIST test set", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.8696901400883993}]}, {"text": "shows the number of string-to-string or tree-to-string rules extracted by each system and the performance on both the subset of sentences in the test corpus that were shorter than 20 words and the entire test corpus.", "labels": [], "entities": []}, {"text": "The performance is measured using the Bleu metric () on lowercased, tokenized outputs/references.", "labels": [], "entities": [{"text": "Bleu metric", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.8962712287902832}]}, {"text": "The results show that the SPMT models clearly outperform the phrase-based systems -the 95% confidence intervals computed via bootstrap resampling in all cases are around 1 Bleu point.", "labels": [], "entities": [{"text": "SPMT", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9372895359992981}, {"text": "Bleu", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9515762329101562}]}, {"text": "The results also show that the simple system combination procedure that we have employed is effective in our setting.", "labels": [], "entities": []}, {"text": "The improvement on the development corpus transfers to the test setting as well.", "labels": [], "entities": []}, {"text": "A visual inspection of the outputs shows significant differences between the outputs of the four models.", "labels": [], "entities": []}, {"text": "The models that use composed rules prefer to produce outputs by using mostly lexicalized rules; in contrast, the simple M1 and M2 models produce outputs in which content is translated primarily using lexicalized rules and reorderings and word insertions are explained primarily by the non-lexical rules.", "labels": [], "entities": [{"text": "word insertions", "start_pos": 238, "end_pos": 253, "type": "TASK", "confidence": 0.7113172262907028}]}, {"text": "It appears that the two strategies are complementary, succeeding and failing in different instances.", "labels": [], "entities": []}, {"text": "We believe that this complementarity and the overcoming of some of the search errors in our decoder during the model rescoring phase explain the success of the system combination experiments.", "labels": [], "entities": []}, {"text": "We suspect that our decoder still makes many search errors.", "labels": [], "entities": []}, {"text": "In spite of this, the SPTM outputs are still significantly better than the PBMT outputs.", "labels": [], "entities": [{"text": "SPTM", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.7927080392837524}]}, {"text": "We also tested whether the Bleu score improvements translate into improvements that can be perceived by humans.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9031657874584198}]}, {"text": "To this end, we randomly selected 138 sentences of less than 20 words from our development corpus; we expected the translation quality of sentences of this size to be easier to assess than that of sentences that are very long.", "labels": [], "entities": []}, {"text": "We prepared a web-based evaluation interface that showed for each input sentence: \u2022 the Chinese input; \u2022 three English reference translations; \u2022 the output of seven \"MT systems\".", "labels": [], "entities": [{"text": "MT", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.93964684009552}]}, {"text": "The evaluated \"MT systems\" were the six systems shown in and one of the reference translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9554294943809509}]}, {"text": "The reference translation presented as automatically produced output was selected from the set of four reference translations provided by NIST so as to be representative of human translation quality.", "labels": [], "entities": [{"text": "NIST", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.9338979125022888}]}, {"text": "More precisely, we chose the second best reference translation in the NIST corpus according to its Bleu score against the other three reference translations.", "labels": [], "entities": [{"text": "NIST corpus", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9693010151386261}, {"text": "Bleu score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.864536702632904}]}, {"text": "The seven outputs were randomly shuffled and presented to three English speakers for assessment.", "labels": [], "entities": []}, {"text": "The judges who participated in our experiment were instructed to carefully read the three reference translations and seven machine translation outputs, and assign a score between 1 and 5 to each translation output on the basis of its quality.", "labels": [], "entities": []}, {"text": "Human judges were told that the translation quality assessment should take into consideration both the grammatical fluency of the outputs and their translation adequacy.", "labels": [], "entities": []}, {"text": "shows the average scores obtained by each system according to each judge.", "labels": [], "entities": []}, {"text": "For convenience, the table also shows the Bleu scores of all systems (including the human translations) on three reference translations.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9018238186836243}]}, {"text": "The results in show that the human judges are remarkably consistent in preferring the syntax-based outputs over the phrase-based outputs.", "labels": [], "entities": []}, {"text": "On a 1 to 5 quality scale, the difference between the phrase-based and syntax-based systems was, on average, between 0.2 and 0.3 points.", "labels": [], "entities": []}, {"text": "All differences between the phrase-based baseline and the syntax-based outputs were statistically significant.", "labels": [], "entities": []}, {"text": "For example, when comparing the phrasebased baseline against the combined system, the improvement inhuman scores was significant at P = 4.04e \u22126 (t = 4.67, df = 413).", "labels": [], "entities": [{"text": "P", "start_pos": 132, "end_pos": 133, "type": "METRIC", "confidence": 0.9871671795845032}]}, {"text": "The results also show that the LDC reference translations are far from being perfect.", "labels": [], "entities": []}, {"text": "Although we selected from the four references the second best according to the Bleu metric, this human reference was judged to beat a quality level of only 4.67 on a scale from 1 to 5.", "labels": [], "entities": [{"text": "Bleu metric", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.7020376920700073}]}, {"text": "Most of the translation errors were fluency errors.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9710890650749207}]}, {"text": "Although the human outputs had most of the time the right meaning, the syntax was sometimes incorrect.", "labels": [], "entities": []}, {"text": "In order to give readers a flavor of the types of re-orderings enabled by the SPMT models, we present in: Human-based evaluation results.", "labels": [], "entities": []}, {"text": "The outputs were selected to reflect both positive and negative effects of largescale re-orderings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results.", "labels": [], "entities": []}, {"text": " Table 2: Human-based evaluation results.", "labels": [], "entities": []}]}