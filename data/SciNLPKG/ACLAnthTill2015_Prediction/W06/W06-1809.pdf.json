{"title": [{"text": "Incorporating User Models in Question Answering to Improve Readability", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7357616126537323}, {"text": "Improve Readability", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8123290538787842}]}], "abstractContent": [{"text": "Most question answering and information retrieval systems are insensitive to different users' needs and preferences, as well as their reading level.", "labels": [], "entities": [{"text": "question answering and information retrieval", "start_pos": 5, "end_pos": 49, "type": "TASK", "confidence": 0.7293052554130555}]}, {"text": "In (Quarteroni and Manandhar, 2006), we introduce a hybrid QA-IR system based on a a user model.", "labels": [], "entities": []}, {"text": "In this paper we focus on how the system filters and re-ranks the search engine results fora query according to their reading difficulty, providing user-tailored answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) systems are information retrieval systems accepting queries in natural language and returning the results in the form of sentences (or paragraphs, or phrases).", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8984283208847046}]}, {"text": "They move beyond standard information retrieval (IR) where results are presented in the form of a ranked list of query-relevant documents.", "labels": [], "entities": [{"text": "standard information retrieval (IR)", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.8060375650723776}]}, {"text": "Such a finer answer presentation is possible thanks to the application of computational linguistics techniques in order to filter irrelevant documents, and of a consistent amount of question pre-processing and result postprocessing.", "labels": [], "entities": []}, {"text": "However, inmost state-of-the-art QA systems the output remains independent of the questioner's characteristics, goals and needs; in other words, there is alack of user modelling.", "labels": [], "entities": []}, {"text": "For instance, an elementary schoolchild and a University history student would get the same answer to the question: \"When did the Middle Ages begin?\".", "labels": [], "entities": []}, {"text": "Secondly, most QA systems focus on factoid questions, i.e. questions concerning people, dates, numerical quantities etc., which can generally be answered by a short sentence or phrase ().", "labels": [], "entities": []}, {"text": "The mainstream approach to QA evaluation, represented by TREC-QA campaigns , has long fostered the criterion that a \"good\" system is one that returns the \"correct\" answer in the shortest possible formulation.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9704783260822296}]}, {"text": "Although recent efforts in) denoted an interest towards list questions and definitional (or \"other\") questions, we believe that there has not been enough interest towards non-factoid answers.", "labels": [], "entities": []}, {"text": "The real issue is \"realizing\" that the answer to a question is sometimes too complex to be formulated and evaluated as a factoid: some queries have multiple, complex or controversial answers (take e.g. \"What were the causes of World War II?\").", "labels": [], "entities": []}, {"text": "In such situations, returning a short paragraph or text snippet is more appropriate than exact answer spotting.", "labels": [], "entities": [{"text": "exact answer spotting", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.5829461415608724}]}, {"text": "For instance, the answer to \"What is a metaphor?\" maybe better understood with the inclusion of examples.", "labels": [], "entities": []}, {"text": "This viewpoint is supported by recent user behaviour studies which showed that even in the case of factoid-based QA systems, the most eligible result format consisted in a paragraph where the sentence containing the answer was highlighted (.", "labels": [], "entities": []}, {"text": "The issue of non-factoids is related to the user modelling problem: while factoid answers do not necessarily require to be contextualized within the user's knowledge and viewpoint, the need is much stronger in the case of definitions, explanations and descriptions.", "labels": [], "entities": []}, {"text": "This is mentioned in the TREC 2003 report when discussing the evaluation of definitional questions: however, the issue is expeditiously solved by assuming a fixed user profile (the \"average news reader\").", "labels": [], "entities": [{"text": "TREC 2003 report", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9171903332074484}]}, {"text": "We are currently developing an adaptive system which adjusts its output with respect to a user model.", "labels": [], "entities": []}, {"text": "The system can be seen as an enhanced IR system which adapts both the content and presentation of the final results, improving their quality.", "labels": [], "entities": [{"text": "IR", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9825872778892517}]}, {"text": "In this paper, we show that QA systems can benefit from the contribution of user models, and explain how these can be used to filter the information presented as an answer based on readability.", "labels": [], "entities": []}, {"text": "Eventually, we describe preliminary results obtained via an evaluation framework inspired by user-centered search engine evaluation.", "labels": [], "entities": []}, {"text": "The QA module, described in the following section, is organized according to the three-tier partition underlying most state-of-the-art systems: 1) question processing, 2) document retrieval, 3) answer generation.", "labels": [], "entities": [{"text": "question processing", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.8366207480430603}, {"text": "document retrieval", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.7221085131168365}, {"text": "answer generation", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.9201604723930359}]}, {"text": "The module makes use of a web search engine for document retrieval and consults the user model to obtain the criteria to filter and re-rank the search engine results and to eventually present them appropriately to the user.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7512397766113281}]}], "datasetContent": [{"text": "We performed our evaluation by running 24 queries (partly reported in) on both Google and YourQA . The results -i.e. snippets from the Google result page and passages returned by YourQA -were given to 20 evaluators.", "labels": [], "entities": [{"text": "YourQA", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.8871897459030151}]}, {"text": "These were aged between 16 and 52, all having a selfassessed good or medium English reading level.", "labels": [], "entities": []}, {"text": "They came from various backgrounds (University students/graduates, professionals, high school) and mother-tongues.", "labels": [], "entities": []}, {"text": "Evaluators filled in a questionnaire assessing the relevance of each passage, the success and result readability of the single queries, and the overall utility of the system; values were thus computed for the metrics in.", "labels": [], "entities": []}], "tableCaptions": []}