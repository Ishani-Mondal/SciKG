{"title": [{"text": "Mining Atomic Chinese Abbreviation Pairs: A Probabilistic Model for Single Character Word Recovery", "labels": [], "entities": [{"text": "Single Character Word Recovery", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.6238034218549728}]}], "abstractContent": [{"text": "An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of \" atomic abbreviation pairs\"from a large text corpus.", "labels": [], "entities": [{"text": "HMM-based Single Character Recovery (SCR)", "start_pos": 3, "end_pos": 44, "type": "TASK", "confidence": 0.732596720967974}]}, {"text": "By an \" atomic abbreviation pair,\"it refers to an abbreviated word and its root word (i.e., unabbreviated form) in which the abbreviation is a single Chinese character.", "labels": [], "entities": []}, {"text": "This task is interesting since the abbreviation process for Chinese compound words seems to be \" compositional\" ; in other words, one can often decode an abbreviated word, such as \" \u53f0\u5927\"(Taiwan University), character-by-character back to its root form.", "labels": [], "entities": []}, {"text": "With a large atomic abbreviation dictionary, one maybe able to recover multiple-character abbreviations more easily.", "labels": [], "entities": []}, {"text": "With only a few training iterations, the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set, respectively, from the ASWSC-2001 corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9680874943733215}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9992696642875671}, {"text": "ASWSC-2001 corpus", "start_pos": 174, "end_pos": 191, "type": "DATASET", "confidence": 0.9710870683193207}]}], "introductionContent": [{"text": "Chinese abbreviations are widely used in the modern Chinese texts.", "labels": [], "entities": []}, {"text": "They area special form of unknown words, which cannot be exhaustively enumerated in an ordinary dictionary.", "labels": [], "entities": []}, {"text": "Many of them originated from important lexical units such as named entities.", "labels": [], "entities": []}, {"text": "However, the sources for Chinese abbreviations are not solely from the noun class, but from most major categories, including verbs, adjectives adverbs and others.", "labels": [], "entities": []}, {"text": "No matter what lexical or syntactic structure a string of characters could be, one can almost always find away to abbreviate it into a shorter form.", "labels": [], "entities": []}, {"text": "Therefore, it maybe necessary to handle them beyond a class-based model.", "labels": [], "entities": []}, {"text": "Furthermore, abbreviated words are semantically ambiguous.", "labels": [], "entities": []}, {"text": "For example, \" \u6e05\u5927\" can be the abbreviation for \" \u6e05\u83ef\u5927\u5b78\"or \" \u6e05\u6f54 \u5927 \u968a \" ; on the opposite direction, multiple choices for abbreviating a word are also possible.", "labels": [], "entities": []}, {"text": "For instance, \" \u53f0\u5317\u5927\u5b78\"may be abbreviated as \" \u53f0\u5927\" , \" \u5317\u5927\"or \" \u53f0\u5317\u5927\" . This results in difficulty for correct Chinese processing and applications, including word segmentation, information retrieval, query expansion, lexical translation and much more.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.7803937494754791}, {"text": "information retrieval", "start_pos": 173, "end_pos": 194, "type": "TASK", "confidence": 0.818415641784668}, {"text": "query expansion", "start_pos": 196, "end_pos": 211, "type": "TASK", "confidence": 0.7316133677959442}, {"text": "lexical translation", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.7586474120616913}]}, {"text": "An abbreviation model or a large abbreviation lexicon is therefore highly desirable for Chinese language processing.", "labels": [], "entities": [{"text": "Chinese language processing", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6669681668281555}]}, {"text": "Since the smallest possible Chinese lexical unit into which other words can be abbreviated is a single character, identifying the set of multi-character words which can be abbreviated into a single character is especially interesting.", "labels": [], "entities": []}, {"text": "Actually, the abbreviation of a compound word can often be acquired by the principle of composition.", "labels": [], "entities": []}, {"text": "In other words, one can decompose a compound word into its constituents and then concatenate their single character equivalents to form its abbreviated form.", "labels": [], "entities": []}, {"text": "The reverse process to predict the unabbreviated form from an abbreviation shares the same compositional property.", "labels": [], "entities": []}, {"text": "The Chinese abbreviation problem can be regarded as an error recovery problem in which t he s u s p e ct r o o two rd s a re t he \" er r or s \" to be recovered from a set of candidates.", "labels": [], "entities": [{"text": "error recovery", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.6459958851337433}]}, {"text": "Such a problem can be mapped to an HMM-based generation model for both abbreviation identification and root word recovery; it can also be integrated as part of a unified word segmentation model when the input extends to a complete sentence.", "labels": [], "entities": [{"text": "abbreviation identification", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.8874864876270294}, {"text": "root word recovery", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6501138110955557}]}, {"text": "As such, we can find the most likely root words, by finding those candidates that maximizes the likelihood of the whole text.", "labels": [], "entities": []}, {"text": "An abbreviation lexicon, which consists of the root-abbreviation pairs, can thus be constructed automatically.", "labels": [], "entities": []}, {"text": "Ina preliminary study (), some probabilistic models had been developed to handle this problem by applying the models to a parallel corpus of compound words and their abbreviations, without knowing the context of the abbreviation pairs.", "labels": [], "entities": []}, {"text": "In this work, the same framework is extended and a method is proposed to automatically acquire a large abbreviation lexicon for indivisual characters from web texts or large corpora, instead of building abbreviation models based on aligned abbreviation pairs of short compound words.", "labels": [], "entities": []}, {"text": "Unlike the previous task, which trains the abbreviation model parameters from a list of known abbreviation pairs, the current work aims at extracting abbreviation pairs from a corpus of free text, in which the locations of prospective abbreviations and full forms are unknown and the correspondence between them is not known either.", "labels": [], "entities": []}, {"text": "In particular, a Single Character Recovery (SCR) Model is exploited in the current work to extract \" atomic abbreviation pairs\"from a large text corpus.", "labels": [], "entities": [{"text": "Single Character Recovery (SCR)", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.784399206439654}]}, {"text": "With only a few training iterations, the acquisition accuracy achieves 62% and 50 % precision for training set and test set from the ASWSC-2001 corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9537093639373779}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9995323419570923}, {"text": "ASWSC-2001 corpus", "start_pos": 133, "end_pos": 150, "type": "DATASET", "confidence": 0.9767749607563019}]}], "datasetContent": [{"text": "To evaluate the SCR Model, the Academia Sinica Word Segmentation,, is adopted for parameter estimation and performance evaluation.", "labels": [], "entities": [{"text": "Academia Sinica Word Segmentation", "start_pos": 31, "end_pos": 64, "type": "DATASET", "confidence": 0.8551165014505386}, {"text": "parameter estimation", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.6551131457090378}]}, {"text": "Among the 94 files in this balanced corpus, 83 of them (13,086KB) are randomly selected as the training set and 11 of them (162KB) are used as the test set.", "labels": [], "entities": []}, {"text": "shows some examples of atomic abbreviation pairs acquired from the training corpus.", "labels": [], "entities": []}, {"text": "The examples here partially justify the possibility to use the SCR model for acquiring atomic abbreviation pairs from a large corpus.", "labels": [], "entities": []}, {"text": "The iterative training process converges quickly after 3~4 iterations.", "labels": [], "entities": []}, {"text": "The numbers of unique abbreviation patterns for the training and test sets are 20,250 and 3,513, respectively.", "labels": [], "entities": []}, {"text": "Since the numbers of patterns are large, a rough estimate on the acquisition accuracy rates is conducted by randomly sampling 100 samples of the <root, abbreviation> pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9763385653495789}]}, {"text": "The pattern is then manually examined to see if the root is correctly recovered.", "labels": [], "entities": []}, {"text": "The precision is estimated as 50% accuracy for the test set, and 62% for the training set on convergence.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996287822723389}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995978474617004}]}, {"text": "Although a larger sample maybe necessary to get a better estimate, the preliminary result is encouraging.", "labels": [], "entities": []}, {"text": "demonstrate the curve of convergence for the iterative training process in terms of pattern number and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9984015822410583}]}], "tableCaptions": [{"text": " Table 3. Examples of atomic abbreviation pairs.", "labels": [], "entities": []}]}