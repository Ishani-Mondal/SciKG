{"title": [{"text": "An Ontology-Based Approach to Disambiguation of Semantic Relations", "labels": [], "entities": [{"text": "Disambiguation of Semantic Relations", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.7489770501852036}]}], "abstractContent": [{"text": "This paper describes experiments in using machine learning for relation disambiguation.", "labels": [], "entities": [{"text": "relation disambiguation", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.8751621842384338}]}, {"text": "There have been succesfuld experiments in combining machine learning and ontologies, or lightweight ontologies such as WordNet, for word sense disambiguation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9481247067451477}, {"text": "word sense disambiguation", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.7757607897122701}]}, {"text": "However, what we are trying to do, is to disambiguate complex concepts consisting of two simpler concepts and the relation that holds between them.", "labels": [], "entities": []}, {"text": "The motivation behind the approach is to expand existing methods for content based information retrieval.", "labels": [], "entities": [{"text": "content based information retrieval", "start_pos": 69, "end_pos": 104, "type": "TASK", "confidence": 0.6181636899709702}]}, {"text": "The experiments have been performed using an annotated extract of a corpus, consisting of prepositions surrounded by noun phrases, where the prepositions denote the relation we are trying disambigu-ate.", "labels": [], "entities": []}, {"text": "The results show an unexploited opportunity of including prepositions and the relations they denote, e.g. in content based information retrieval.", "labels": [], "entities": [{"text": "content based information retrieval", "start_pos": 109, "end_pos": 144, "type": "TASK", "confidence": 0.5624433532357216}]}], "introductionContent": [{"text": "What we describe in this paper, which we refer to as relation disambiguation, is in some sense similar to word sense disambiguation.", "labels": [], "entities": [{"text": "relation disambiguation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7598406374454498}]}, {"text": "In traditional word sense disambiguation the objective is to associate a distinguishable sense with a given word).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.740829586982727}]}, {"text": "It is not a novel idea to use machine learning in connection with traditional word sense disambiguation, and as such it is not a novel idea to include some kind of generalization of the concept that a word expresses in the learning task either.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.6606440246105194}]}, {"text": "Other projects have used light-weight ontologies such as WordNet in this kind of learning task).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9739631414413452}]}, {"text": "What we believe is our contribution with this work is the fact that we attempt to learn complex concepts that consist of two simpler concepts, and the relation that holds between them.", "labels": [], "entities": []}, {"text": "Thus, we start outwith the knowledge that some relation holds between two concepts, which we could express as REL(concept1,concept2), and what we aim at being able to do is to fill in a more specific relation type than the generic REL, and get e.g. POF in the case where a preposition expresses a partitive relation.", "labels": [], "entities": [{"text": "REL", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9403133988380432}, {"text": "POF", "start_pos": 249, "end_pos": 252, "type": "METRIC", "confidence": 0.9910353422164917}]}, {"text": "This makes it e.g. possible to determine from the sentence \"France is in Europe\" that France is apart of Europe.", "labels": [], "entities": []}, {"text": "As in word sense disambiguation we here presuppose a finite and minimal set of relations, which is described in greater detail in section 2.", "labels": [], "entities": []}, {"text": "The ability to identify these complex structures in text, can facilitate a more content based information retrieval as opposed to more traditional search engines, where the information retrieval relies more or less exclusively on keyword recognition.", "labels": [], "entities": [{"text": "keyword recognition", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.7324638962745667}]}, {"text": "In the OntoQuery project 1 , pertinent text segments are retrieved based on the conceptual content of the search phrase as well as the text segments).", "labels": [], "entities": [{"text": "OntoQuery project 1", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.8017159700393677}]}, {"text": "Concepts are here identified through their corresponding surface form (noun phrases), and mapped into the ontology.", "labels": [], "entities": []}, {"text": "As a result, we come from a flat structure in a text to a graph structure, which describes the concepts that are referred to in a given text segment, in relation to each other.", "labels": [], "entities": []}, {"text": "However, at the moment the ontology is strictly a subsumption-based hierarchy and, further, only relatively simple noun phrases are recognized and mapped into the ontology.", "labels": [], "entities": []}, {"text": "The work presented here expands this scope by including other semantic relations between noun phrases.", "labels": [], "entities": []}, {"text": "Our first experiments in this direction have been an analysis of prepositions with surrounding noun phrases (NPs).", "labels": [], "entities": []}, {"text": "Our aim is to show that there is an affinity between the ontological types of the NP-heads and the relation that the preposition denotes, which can be used to represent the text as a complex semantic structure, as opposed to simply running text.", "labels": [], "entities": []}, {"text": "The approach to showing this has been to annotate a corpus and use standard machine learning methods on this corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "The annotation process generates af a feature space of six dimensions, namely the lemmatized form of the two heads of the noun phrases, the ontological types of the heads, the preposition and the relation.", "labels": [], "entities": []}, {"text": "In the corpus there is a total of only 952 text segments.", "labels": [], "entities": []}, {"text": "In general the distribution of the data is highly skewed and sparseness is a serious problem.", "labels": [], "entities": []}, {"text": "More than half of the instances are of the relation type WRT or PNT, and the rest of the instances are distributed among the remaining 10 relations with only 14 instances scattered over the tree smallest classes.", "labels": [], "entities": []}, {"text": "There are 332 different combinations of ontological types where 197 are unique.", "labels": [], "entities": []}, {"text": "There are 681 different heads and 403 of them are unique, with all of them being lemmatized.", "labels": [], "entities": []}, {"text": "Our assumption is that there is consistency in which relations prepositions usually denote in particular contexts, and hence the learning algorithms should be able to generalize well.", "labels": [], "entities": []}, {"text": "We also assume that the addition of the ontological types of the head of the NP, is the most vital information in classifying the relation type, at least in this case where data is sparse.", "labels": [], "entities": []}, {"text": "We have run the experiments with a Support Vector Machine algorithm SMO () and the prepositional rule learning algorithm JRip.", "labels": [], "entities": []}, {"text": "The former in order to get high precision, the latter in order to get easily interpretable rules for later analysis (see section 4.1).", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9988798499107361}]}, {"text": "The experiments were run using 10-fold-cross-validation, with a further partition of the training set at each fold into a tuning and a training set.", "labels": [], "entities": []}, {"text": "The tuning set was used to optimize the parameter 4 settings for each algorithm . The implementation of the algorithms that we used, was the WEKA software package).", "labels": [], "entities": [{"text": "WEKA software package", "start_pos": 141, "end_pos": 162, "type": "DATASET", "confidence": 0.9541358550389608}]}, {"text": "The experiments were run on seven different combinations of the feature space, ranging from using only the heads to using both heads, preposition and ontological types of the heads.", "labels": [], "entities": []}, {"text": "This was done in order to get insight into the importance of using ontological types in the learning.", "labels": [], "entities": []}, {"text": "The results of these experiments are shown in table 3.", "labels": [], "entities": []}, {"text": "The last column shows the precision fora projected classifier (PC) in the cases where it outperforms the trivial rejector.", "labels": [], "entities": [{"text": "precision fora projected classifier (PC)", "start_pos": 26, "end_pos": 66, "type": "METRIC", "confidence": 0.7741326604570661}]}, {"text": "The projected classifier, in this case, assigns the relation that is most common for the corresponding input pair; e.g if the ontological types are DIS/HUM, then the most common relation is PNT.", "labels": [], "entities": []}, {"text": "The trivial rejector, which assigns the most common relation, in this case WRT, to all the instances, achieves a precision of 37.8%.: The precision of SVM, JRip and a projected classifier on the seven different combinations of input features.", "labels": [], "entities": [{"text": "WRT", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.907669723033905}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9989873766899109}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9982277750968933}]}, {"text": "\"Lemma\" here is short for lemmatized NP head.", "labels": [], "entities": []}, {"text": "The following conclusions can be drawn from table 3.", "labels": [], "entities": []}, {"text": "The support vector machine algorithm produces a result which in all cases is better than the baseline, i.e. we are able to produce a model that generalizes well over the training instances compared to the projected classifier or the trivial rejector.", "labels": [], "entities": []}, {"text": "This difference is not statistically significant at a confidence level of 0.95 when only training on the surface form of prepositions.", "labels": [], "entities": []}, {"text": "A comparison of line 1-3 shows that training on ontological types seems to be superior to using lemmatized NP heads or prepositions, though the superiority is not statistically significant when comparing to the lemmatized NP heads.", "labels": [], "entities": []}, {"text": "When comparing line 4-7 the difference between the results are not statistically significant.", "labels": [], "entities": []}, {"text": "This fact may owe to the data sparseness.", "labels": [], "entities": []}, {"text": "However, comparing line 1 to line 6 or 7, shows that the improvement of adding the preposition and the lemmatized NP heads to the ontological types is statistically significant.", "labels": [], "entities": []}, {"text": "In general, the results reveal an unexplored opportunity to include ontological types and the relations that prepositions denote in information retrieval.", "labels": [], "entities": []}, {"text": "In the next section, we will look more into the rules created by the JRip algorithm from a linguistic point of view.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The precision of SVM, JRip and a projected  classifier on the seven different combinations of input  features. \"Lemma\" here is short for lemmatized NP  head.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9991371035575867}]}]}