{"title": [{"text": "Latent Features in Automatic Tense Translation between Chinese and English", "labels": [], "entities": [{"text": "Automatic Tense Translation between Chinese and English", "start_pos": 19, "end_pos": 74, "type": "TASK", "confidence": 0.7983714597565787}]}], "abstractContent": [{"text": "On the task of determining the tense to use when translating a Chinese verb into En-glish, current systems do not perform as well as human translators.", "labels": [], "entities": []}, {"text": "The main focus of the present paper is to identify features that human translators use, but which are not currently automatically extractable.", "labels": [], "entities": []}, {"text": "The goal is twofold: to test a particular hypothesis about what additional information human translators might be using, and as a pilot to determine whereto focus effort on developing automatic extraction methods for features that are somewhat beyond the reach of current feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 272, "end_pos": 290, "type": "TASK", "confidence": 0.7653526663780212}]}, {"text": "The paper shows that incorporating several latent features into the tense clas-sifier boosts the tense classifier's performance , and a tense classifier using only the latent features outperforms one using only the surface features.", "labels": [], "entities": []}, {"text": "Our findings confirm the utility of the latent features in automatic tense classification, explaining the gap between automatic classification systems and the human brain.", "labels": [], "entities": [{"text": "automatic tense classification", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6955467661221822}]}], "introductionContent": [{"text": "Language speakers make two types of distinctions about temporal relations: the first type of relation is based on precedence between events and can be expanded into a finer grained taxonomy as proposed by.", "labels": [], "entities": []}, {"text": "The second type of relation is based on the relative positioning between the following three time parameters proposed by: speech time (S), event time (E) and reference time (R).", "labels": [], "entities": [{"text": "reference time (R)", "start_pos": 158, "end_pos": 176, "type": "METRIC", "confidence": 0.8627329230308532}]}, {"text": "In the past couple of decades, the NLP community has seen an emergent interest in the first type of temporal relation.", "labels": [], "entities": []}, {"text": "In the cross-lingual context, while the first type of relationship can be easily projected across a language pair, the second type of relationship is often hard to be projected across a language pair.", "labels": [], "entities": []}, {"text": "In contrast to this challenge, cross-lingual temporal reference distinction has been poorly explored.", "labels": [], "entities": [{"text": "cross-lingual temporal reference distinction", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.6855581402778625}]}, {"text": "Languages vary in the granularity of their tense and aspect representations; some have finergrained tenses or aspects than others.", "labels": [], "entities": []}, {"text": "Tense generation and tense understanding in natural language texts are highly dynamic and context-dependent processes, since any previously established time point or interval, whether explicitly mentioned in the context or not, could potentially serve as the reference time for the event in question.", "labels": [], "entities": [{"text": "Tense generation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9053040146827698}, {"text": "tense understanding", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7468523383140564}]}, {"text": "captures this nature of temporal reference organization in discourse through a multiple temporal reference model.", "labels": [], "entities": []}, {"text": "He defines a set (S 1 , S 2 , ..., Sn ) that is an element of tense.", "labels": [], "entities": []}, {"text": "S 1 corresponds to the speech time, Sn is the event time, and (S i , i=2, ..., n-1) stand fora sequence of time references from which the reference time of a particular event could come.", "labels": [], "entities": []}, {"text": "Given the elusive nature of reference time shift, it is extremely hard to model the reference time point directly in temporal information processing.", "labels": [], "entities": []}, {"text": "The above reasons motivate classifying temporal reference distinction automatically, using machine learning algorithms such as Conditional Random Fields (CRFs).", "labels": [], "entities": [{"text": "classifying temporal reference distinction", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.8277889043092728}]}, {"text": "Many researchers in Natural Language Processing seem to believe that an automatic system does not have to follow the mechanism of human brain in order to optimize its performance, for example, the feature space for an automatic classification system does not have to replicate the knowledge sources that human beings utilize.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6673229734102885}]}, {"text": "There has been very little research that pursues to testify this faith.", "labels": [], "entities": []}, {"text": "The current work attempts to identify which features are most important for tense generation in Chinese to English translation scenario, which can point to direction of future research effort for automatic tense translation between Chinese and English.", "labels": [], "entities": [{"text": "tense generation in Chinese to English translation", "start_pos": 76, "end_pos": 126, "type": "TASK", "confidence": 0.6383079673562732}, {"text": "automatic tense translation between Chinese and English", "start_pos": 196, "end_pos": 251, "type": "TASK", "confidence": 0.7753078384058816}]}, {"text": "The remaining part of the paper is organized as follows: Section 2 summarizes the significant related works in temporal information annotation and points out how this study relates to yet differs from them.", "labels": [], "entities": [{"text": "temporal information annotation", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.6257472236951193}]}, {"text": "Section 3 formally defines the problem, tense taxonomy and introduces the data.", "labels": [], "entities": []}, {"text": "Section 4 discusses the feature space and proposes the latent features for the tense classification task.", "labels": [], "entities": [{"text": "tense classification task", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.8577963511149088}]}, {"text": "Section 5 presents the classification experiments in Conditional Random Fields as well as Classification Tree and reports the evaluation results.", "labels": [], "entities": [{"text": "Classification Tree", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.7627237141132355}]}, {"text": "Section 6 concludes the paper and section 7 points out directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train a tense classifier on our data set in two stages: first on the surface features, and then on the combined space of both surface features (discussed in 4.1) and latent features (discussed in 4.2-4.4).", "labels": [], "entities": []}, {"text": "It is conceivable that the granularity of sequences may matter in learning from data with sequential relationship, and in the context of verb tense tagging, it naturally maps to the granularity of discourse.", "labels": [], "entities": [{"text": "verb tense tagging", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.6903588175773621}]}, {"text": "shows that there is no significant difference between sentence-level sequences and paragraph-level sequences.", "labels": [], "entities": []}, {"text": "Therefore, we experiment with only sentence-level sequences.", "labels": [], "entities": []}, {"text": "To verify the stability of the utility of the latent features, we also experiment with classification tree learning on the same features space as: Apparent Accuracy for the Training Data of the Classification Tree Classifiers discussed above.", "labels": [], "entities": [{"text": "Apparent Accuracy", "start_pos": 147, "end_pos": 164, "type": "METRIC", "confidence": 0.8020549118518829}]}, {"text": "Classification Trees are used to predict membership of cases or objects in the classes of a categorical dependent variable from their measurements on one or more predictor variables.", "labels": [], "entities": []}, {"text": "The main idea of Classification Tree is to do a recursive partitioning of the variable space to achieve good separation of the classes in the training dataset.", "labels": [], "entities": []}, {"text": "We use the Recursive Partitioning and Regression Trees(Rpart) package provided by R statistical computing software for the implementation of classification trees.", "labels": [], "entities": []}, {"text": "In order to avoid over-fitting, we prune the tree by setting the minimum number of objects in anode to attempt a split and the minimum number of objects in any terminal node to be 10 and 3 respectively.", "labels": [], "entities": []}, {"text": "In the constructed classification tree when we use all features including both surface and latent features, the top split at the root node in the tree is based on telicity feature of the English verb, indicating the importance of telicity feature for English verb among all of the features.", "labels": [], "entities": []}, {"text": "All results are obtained by 5-fold cross validation.", "labels": [], "entities": []}, {"text": "The classifier's performance is evaluated against the tenses from the best-ranked human-generated English translation.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the CRFs tense classifier, we compute the precision, recall, general accuracy and F, which are defined as follow.", "labels": [], "entities": [{"text": "CRFs tense classifier", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7589801152547201}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9998148083686829}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.999607503414154}, {"text": "general", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9653159379959106}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8282788991928101}, {"text": "F", "start_pos": 113, "end_pos": 114, "type": "METRIC", "confidence": 0.9997745156288147}]}, {"text": "5. S: Size of perfect hitlist; From, we see that past tense, which occurs most frequently in the training data, has the highest precision, recall and F. Future tense, which occurs least frequently, has the lowest F.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9992910623550415}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9996002316474915}, {"text": "F.", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.999298095703125}, {"text": "F", "start_pos": 213, "end_pos": 214, "type": "METRIC", "confidence": 0.9966505169868469}]}, {"text": "Precision and recall do not show clear pattern across different tense classes.: Evaluations in General Accuracy", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9828063249588013}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.998691976070404}, {"text": "Accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8417093753814697}]}], "tableCaptions": [{"text": " Table 1: Evaluation Results for CRFs Classifier in Precision, Recall and F Using All Features", "labels": [], "entities": [{"text": "Precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.991370439529419}, {"text": "Recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.972282350063324}, {"text": "F", "start_pos": 74, "end_pos": 75, "type": "METRIC", "confidence": 0.9940251708030701}]}, {"text": " Table 2: Apparent Accuracy for the Training Data of the Classification Tree Classifiers", "labels": [], "entities": [{"text": "Apparent", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9935778379440308}, {"text": "Accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.5679514408111572}, {"text": "Classification Tree Classifiers", "start_pos": 57, "end_pos": 88, "type": "DATASET", "confidence": 0.7509757379690806}]}, {"text": " Table 3: Evaluations in General Accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.7716057896614075}]}]}