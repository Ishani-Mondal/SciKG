{"title": [{"text": "A Multiclassifier based Document Categorization System: profiting from the Singular Value Decomposition Dimensionality Reduction Technique", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a multiclassifier approach for multilabel document classification problems, where a set of k-NN clas-sifiers is used to predict the category of text documents based on different training subsampling databases.", "labels": [], "entities": [{"text": "multilabel document classification", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.6343646744887034}]}, {"text": "These databases are obtained from the original training database by random subsampling.", "labels": [], "entities": []}, {"text": "In order to combine the predictions generated by the multiclassifier, Bayesian voting is applied.", "labels": [], "entities": []}, {"text": "Through all the classification process , a reduced dimension vector representation obtained by Singular Value Decomposition (SVD) is used for training and testing documents.", "labels": [], "entities": []}, {"text": "The good results of our experiments give an indication of the potentiality of the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document Categorization, the assignment of natural language texts to one or more predefined categories based on their content, is an important component in many information organization and management tasks.", "labels": [], "entities": [{"text": "Document Categorization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8187757730484009}]}, {"text": "Researchers have concentrated their efforts in finding the appropriate way to represent documents, index them and construct classifiers to assign the correct categories to each document.", "labels": [], "entities": []}, {"text": "Both, document representation and classification method are crucial steps in the categorization process.", "labels": [], "entities": [{"text": "document representation", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.7572499215602875}]}, {"text": "In this paper we concentrate on both issues.", "labels": [], "entities": []}, {"text": "On the one hand, we use Latent Semantic Indexing (LSI), which is a variant of the vector space model (VSM), in order to obtain the vector representation of documents.", "labels": [], "entities": []}, {"text": "This technique compresses vectors representing documents into vectors of a lower-dimensional space.", "labels": [], "entities": []}, {"text": "LSI, which is based on Singular Value Decomposition (SVD) of matrices, has showed to have the ability to extract the relations among words and documents by means of their context of use, and has been successfully applied to Information Retrieval tasks.", "labels": [], "entities": [{"text": "Information Retrieval tasks", "start_pos": 224, "end_pos": 251, "type": "TASK", "confidence": 0.8887535929679871}]}, {"text": "On the other hand, we construct a multiclassifier ( which uses different training databases.", "labels": [], "entities": []}, {"text": "These databases are obtained from the original training set by random subsampling.", "labels": [], "entities": []}, {"text": "We implement this approach by bagging, and use the k-NN classification algorithm to make the category predictions for testing documents.", "labels": [], "entities": []}, {"text": "Finally, we combine all predictions made fora given document by Bayesian voting.", "labels": [], "entities": []}, {"text": "The experiment we present has been evaluated for Reuters-21578 standard document collection.", "labels": [], "entities": [{"text": "Reuters-21578 standard document collection", "start_pos": 49, "end_pos": 91, "type": "DATASET", "confidence": 0.9228886663913727}]}, {"text": "Reuters-21578 is a multilabel document collection, which means that categories are not mutually exclusive because the same document maybe relevant to more than one category.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9835001230239868}]}, {"text": "Being aware of the results published in the most recent literature, and having obtained good results in our experiments, we consider the categorization method presented in this paper an interesting contribution for text categorization tasks.", "labels": [], "entities": [{"text": "text categorization tasks", "start_pos": 215, "end_pos": 240, "type": "TASK", "confidence": 0.8562259078025818}]}, {"text": "The remainder of this paper is organized as follows: Section 2, discusses related work on document categorization for Reuters-21578 collection.", "labels": [], "entities": [{"text": "Reuters-21578 collection", "start_pos": 118, "end_pos": 142, "type": "DATASET", "confidence": 0.9352344572544098}]}, {"text": "In Section 3, we present our approach to deal with the multilabel text categorization task.", "labels": [], "entities": [{"text": "multilabel text categorization task", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.7204950824379921}]}, {"text": "In Section 4 the experimental setup is introduced, and details about the Reuters database, the preprocessing applied and some parameter setting are provided.", "labels": [], "entities": [{"text": "Reuters database", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.9769730865955353}]}, {"text": "In Section 5, experimental results are presented and discussed.", "labels": [], "entities": []}, {"text": "Finally, Section 6 contains some conclusions and comments on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of a text categorization system is usually done experimentally, by measuring the effectiveness, i.e. average correctness of the categorization.", "labels": [], "entities": []}, {"text": "In binary text categorization, two known statistics are widely used to measure this effectiveness: precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9996311664581299}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9993867874145508}]}, {"text": "Precision (Prec) is the percentage of documents correctly classified into a given category, and recall (Rec) is the percentage of documents belonging to a given category that are indeed classified into it.", "labels": [], "entities": [{"text": "Precision (Prec)", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9515306204557419}, {"text": "recall (Rec)", "start_pos": 96, "end_pos": 108, "type": "METRIC", "confidence": 0.9770743101835251}]}, {"text": "In general, there is a trade-off between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9994152784347534}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.997552216053009}]}, {"text": "Thus, a classifier is usually evaluated by means of a measure which combines precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.999408483505249}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9968898892402649}]}, {"text": "Various such measures have been proposed.", "labels": [], "entities": []}, {"text": "The breakeven point, the value at which precision equals recall, has been frequently used during the past decade.", "labels": [], "entities": [{"text": "breakeven point", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9902194142341614}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.998741090297699}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9986162185668945}]}, {"text": "However, it has been recently criticized by its proposer () footnote 19).", "labels": [], "entities": []}, {"text": "Nowadays, the F 1 score is more frequently used.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9891481598218282}]}, {"text": "The F 1 score combines recall and precision with an equal weight in the following way: Since precision and recall are defined only for binary classification tasks, for multiclass problems results need to be averaged to get a single performance value.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9710675676663717}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9979066848754883}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9969603419303894}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9975825548171997}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9940879344940186}]}, {"text": "This will be done using microaveraging and macroaveraging.", "labels": [], "entities": []}, {"text": "In microaveraging, which is calculated by globally summing overall individual cases, categories count proportionally to the number of their positive testing examples.", "labels": [], "entities": []}, {"text": "In macroaveraging, which is calculated by averaging over the results of the different categories, all categories count the same.", "labels": [], "entities": []}, {"text": "See) for more detailed explanation of the evaluation measures mentioned above.", "labels": [], "entities": []}, {"text": "The aim of this section is to describe the document collection used in our experiment and to give an account of the preprocessing techniques and parameter settings we have applied.", "labels": [], "entities": []}, {"text": "When machine learning and other approaches are applied to text categorization problems, a common technique has been to decompose the multiclass problem into multiple, independent binary classification problems.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.756417989730835}]}, {"text": "In this paper, we adopt a different approach.", "labels": [], "entities": []}, {"text": "We will be primarily interested in a classifier which produces a ranking of possible labels fora given document, with the hope that the appropriate labels will appear at the top of the ranking.", "labels": [], "entities": []}, {"text": "In microaveraged F 1 scores obtained in our experiment are shown.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9323408007621765}]}, {"text": "As it could be expected, a simple stemming process increases slightly results, and it can be observed that the best result for the three category subsets has been obtained for the stemmed corpus, even though gain is low (less than 0.6).", "labels": [], "entities": []}, {"text": "The evaluation for the Top-10 category subset gives the best results, reaching up to 93.57%.", "labels": [], "entities": []}, {"text": "In fact, this is the expected behavior, as the number of categories to be evaluated is small and the number of documents in each category is high.", "labels": [], "entities": []}, {"text": "For this subset the best result has been obtained for 100 dimensions, although the variation is low among results for 100, 300 and 500 dimensions.", "labels": [], "entities": []}, {"text": "When using higher dimensions results become poorer.", "labels": [], "entities": []}, {"text": "According to the R(90) and R(115) subsets, the best results are 87.27% and 87.01% respectively.", "labels": [], "entities": []}, {"text": "Given that the difficulty of these subsets is quite similar, their behavior is also analogous.", "labels": [], "entities": []}, {"text": "As we can see in the table, most of the best results for these subsets have been obtained by reducing the dimension of the space to 500.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Microaveraged breakeven point results  reported by Sebastiani for the Reuters-21578  ModApte split.", "labels": [], "entities": [{"text": "Reuters-21578  ModApte split", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.8792919317881266}]}, {"text": " Table 2: F 1 results reported for the Reuters-21578  ModApte split.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9706794619560242}, {"text": "Reuters-21578  ModApte split", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.8772910237312317}]}, {"text": " Table 3: Microaveraged F 1 scores for Reuters- 21578 ModApte split.", "labels": [], "entities": [{"text": "Microaveraged F 1 scores", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.646866001188755}, {"text": "Reuters- 21578 ModApte split", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.8869101047515869}]}]}