{"title": [{"text": "A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance *", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a probabilistic approach to content selection for meeting summarization.", "labels": [], "entities": [{"text": "content selection", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7000196427106857}, {"text": "summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.7879681587219238}]}, {"text": "We use skip-chain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as QUESTION-ANSWER that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task.", "labels": [], "entities": []}, {"text": "We also discuss different approaches for ranking all utterances in a sequence using CRFs.", "labels": [], "entities": []}, {"text": "Our best performing system achieves 91.3% of human performance when evaluated with the Pyramid evaluation metric, which represents a 3.9% absolute increase compared to our most competitive non-sequential classifier.", "labels": [], "entities": []}], "introductionContent": [{"text": "Summarization of meetings faces many challenges not found in texts, i.e., high word error rates, absence of punctuation, and sometimes lack of grammaticality and coherent ordering.", "labels": [], "entities": [{"text": "Summarization of meetings", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9149441719055176}]}, {"text": "On the other hand, meetings present a rich source of structural and pragmatic information that makes summarization of multi-party speech quite unique.", "labels": [], "entities": [{"text": "summarization", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.9849256873130798}]}, {"text": "In particular, our analyses of patterns in the verbal exchange between participants found that adjacency pairs (AP), a concept drawn from the conversational analysis literature (, have particular relevance to summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 209, "end_pos": 222, "type": "TASK", "confidence": 0.9900687336921692}]}, {"text": "APs are pairs of utterances such as QUESTION-ANSWER or OFFER-ACCEPT, in which the second utterance is said to be conditionally relevant on the first.", "labels": [], "entities": [{"text": "APs", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.815383791923523}, {"text": "QUESTION-ANSWER", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.8572741150856018}, {"text": "OFFER-ACCEPT", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.98698890209198}]}, {"text": "We show that there is a strong correlation between the two elements of an AP in summarization, and that one is unlikely to be included if the other element is not present in the summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9620809555053711}]}, {"text": "Most current statistical sequence models in natural language processing (NLP), such as hidden * This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grants No. IIS-0121396 and IIS-05-34871, and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7968775729338328}]}, {"text": "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the NSF or DARPA.", "labels": [], "entities": [{"text": "DARPA", "start_pos": 164, "end_pos": 169, "type": "DATASET", "confidence": 0.5844681859016418}]}, {"text": "Markov models (HMMs), are linear chains that only encode local dependencies between utterances to be labeled.", "labels": [], "entities": []}, {"text": "In multi-party speech, the two elements of an AP are generally arbitrarily distant, and such models can only poorly account for dependencies underlying APs in summarization.", "labels": [], "entities": []}, {"text": "We use instead skip-chain sequence models (, which allow us to explicitly model dependencies between distant utterances, and turnout to be particularly effective in the summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.9153441488742828}]}, {"text": "In this paper, we compare two types of network structures-linear-chain and skip-chain-and two types of network semantics-Bayesian Networks (BNs) and Conditional Random Fields (CRFs).", "labels": [], "entities": []}, {"text": "We discuss the problem of estimating the class posterior probability of each utterance in a sequence in order to extract the N most probable ones, and show that the cost assigned by a CRF to each utterance needs to be locally normalized in order to outperform BNs.", "labels": [], "entities": []}, {"text": "After analyzing the predictive power of a large set of durational, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have a competitive set of predictors to test the different models.", "labels": [], "entities": []}, {"text": "Empirical evaluations using two standard summarization metrics-the Pyramid method) and ROUGE)-show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9952278137207031}]}], "datasetContent": [{"text": "Evaluating summarization is a difficult problem and there is no broad consensus on how to best perform this task.", "labels": [], "entities": [{"text": "Evaluating summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8361976444721222}]}, {"text": "Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE).", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.5052662044763565}, {"text": "ROUGE", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9927003979682922}]}, {"text": "Pyramid and ROUGE are techniques looking for content units repeated in different model summaries, i.e., summary content units (SCUs) such as clauses and noun phrases for the Pyramid method, and ngrams for ROUGE.", "labels": [], "entities": []}, {"text": "The underlying hypothesis is that different model sentences, clauses, or phrases may convey the same meaning, which is a reasonable assumption when dealing with reference summaries produced by different authors, since it is quite unlikely that any two abstractors would use the exact same words to convey the same idea.", "labels": [], "entities": []}, {"text": "Our situation is however quite different, since all model summaries of a given document are utterance extracts of that same document, as this can been seen in the excerpt of.", "labels": [], "entities": []}, {"text": "In our own annotation of three meetings with SCUs defined as in (Nenkova and Passonneau, 2004a), we found that repetitions and reformulation of the same information are particularly infrequent, and that textual units that express the same content among model summaries are generally originating from the same document sentence (e.g., in the figure, the first sentence in model 1 and 2 emanate from the same document sentence).", "labels": [], "entities": []}, {"text": "Very short SCUs (e.g., base noun phrases) sometimes appeared in different locations of a meeting, but we think it is problematic to assume that connections between such short units are indicative of any similarity of sentential meaning: the contexts are different, and words maybe uttered by different speakers, which may lead to unrelated or conflicting pragmatic forces.", "labels": [], "entities": []}, {"text": "For instance, an SCU realized as \"DC offset\" and \"DC component\" appears in two different sentences in the figure, i.e. those identified as 1-13 and 31-41.", "labels": [], "entities": []}, {"text": "However, the two sentences have contradictory meanings, and it would be unfortunate to increase the score of a peer summary containing the former sentence because the latter is included in some model summaries.", "labels": [], "entities": []}, {"text": "For all these reasons, we believe that summarization evaluation in our case should rely on the following restrictive matching: two summary units should be considered equivalent if and only if they are extracted from the same location in the original document (e.g., the \"DC\" appearing in models 1 and 2 is not the same as the \"DC\" in the peer summary, since they are extracted from different sentences).", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.9422633945941925}]}, {"text": "This constraint on the matching is reflected in our Pyramid evaluation, and we define an SCU as a word and its document position, which lets us distinguish (\"DC\",11) from (\"DC\",33).", "labels": [], "entities": []}, {"text": "While this restriction on SCUs forces us to disregard scarcely occurring paraphrases and repetitions of the same information, it provides the benefit of automated evaluation.", "labels": [], "entities": []}, {"text": "Once all SCUs have been identified, the Pyramid method is applied as in (Nenkova and Passonneau, 2004b): we compute a score D by adding for each SCU present in the summary a score equal to the number of model summaries in which that SCU appears.", "labels": [], "entities": []}, {"text": "The Pyramid score P is computed by dividing D by the maximum D * value that is obtainable given the constraint on length.", "labels": [], "entities": [{"text": "Pyramid score P", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7849952181180319}]}, {"text": "For instance, the peer summary in the figure gets a score D = 9 (since the 9 SCUs in range 43-51 occur in one model), and the maximum obtainable score is D * = 44 (all SCUs of the optimal summary appear in exactly two model summaries), hence the peer summary's score is P = .204.", "labels": [], "entities": [{"text": "P", "start_pos": 270, "end_pos": 271, "type": "METRIC", "confidence": 0.9829544425010681}]}, {"text": "While our evaluation scheme is similar to comparing the binary predictions of model and peer summaries-each prediction determining whether a given transcription word is included or notand averaging precision scores overall peer-model pairs, the Pyramid evaluation differs on an important point, which makes us prefer the Pyramid evaluation method: the maximum possible Pyramid score is always guaranteed to be 1, but average precision scores can become arbitrarily low as the consensus between summary annotators decreases.", "labels": [], "entities": [{"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9063886404037476}, {"text": "precision", "start_pos": 425, "end_pos": 434, "type": "METRIC", "confidence": 0.957992672920227}]}, {"text": "For instance, the average precision score of the optimal summary in the figure is PR = 2 3 . 2 2 Precision scores of the optimal summary compared against the the three model summaries are .5, 1, and .5, respectively, and hence average . We can show that P = P R/P R * , where PR * is the average precision of the optimal summary.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9874402284622192}, {"text": "PR", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9688790440559387}, {"text": "precision", "start_pos": 296, "end_pos": 305, "type": "METRIC", "confidence": 0.9596686959266663}]}, {"text": "Lack of space prevent us from providing a proof, so we will just show that the equality holds in our example: since the peer summary's precision scores against the three model summaries are respectively  We follow () in using the same six meetings as test data, since each of these meetings has multiple reference summaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9972376823425293}]}, {"text": "The remaining 69 meetings were used for training, which represent in total more than 103,000 training instances (or DA units), of which 6,464 are positives (6.24%).", "labels": [], "entities": []}, {"text": "The multi-reference test set contains more than 28,000 instances.", "labels": [], "entities": []}, {"text": "The goal of a preliminary experiment was to devise a set of useful predictors from a full set of 1171.", "labels": [], "entities": []}, {"text": "We performed feature selection by incrementally growing a log-linear model with order-0 features f (x, y t ) using a forward feature selection procedure similar to.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8136104345321655}]}, {"text": "Probably due to the imbalance between positive and negative samples, we found it more effective to rank candidate features by gains in F -measure (through 5-fold cross validation on the entire training set).", "labels": [], "entities": [{"text": "F -measure", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9909154375394186}]}, {"text": "The increase in F 1 by adding new features to the model is displayed in; this greedy search resulted in a set S of 217 features.", "labels": [], "entities": [{"text": "F 1", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9904609024524689}]}, {"text": "We now analyze the performance of different sequence models on our test set.", "labels": [], "entities": []}, {"text": "The target length of each summary was set to 12.7% of the number of words of the full document, which is the aver-age on the entire training data (the average on the test data is 12.9%).", "labels": [], "entities": [{"text": "aver-age", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9930887818336487}]}, {"text": "In, we use an order-0 CRF to compare S against all features and various categorical groupings.", "labels": [], "entities": []}, {"text": "Overall, we notice lexical predictors and statistics derived from them (e.g. LSA features) represent the most helpful feature group (.497), though all other features combined achieve a competitive performance (.476).", "labels": [], "entities": []}, {"text": "displays performance for sequence models incorporating linear-chain features of increasing order k.", "labels": [], "entities": []}, {"text": "Its second column indicates what criterion was used to rank utterances.", "labels": [], "entities": []}, {"text": "In the case of 'pred', we used actual model {\u22121, 1} predictions, which in all cases generated summaries much shorted than the allowable length, and produced poor performance.", "labels": [], "entities": []}, {"text": "'Costs' and 'norm-CRF' refer to the two ranking criteria presented in Section 5, and it is clear that the performance of CRFs degrades with increasing orders without local normalization.", "labels": [], "entities": [{"text": "Costs'", "start_pos": 1, "end_pos": 7, "type": "METRIC", "confidence": 0.9516817331314087}]}, {"text": "While the contingency counts in Table 2 only hinted a limited benefit of linear-chain features, empirical results show the contraryespecially for order k = 2.", "labels": [], "entities": []}, {"text": "However, the further increase of k causes overfitting, and skip-chain features seem a better way to capture non-local dependencies while keeping the number of model parameters relatively small.", "labels": [], "entities": []}, {"text": "Overall, the addition of skip-chain edges to linear-chain models provide noticeable improvement in Pyramid scores.", "labels": [], "entities": []}, {"text": "Our system that performed best on cross-validation data is an order-2 CRF with skip-chain transitions, which achieves a Pyramid score of P = .554.", "labels": [], "entities": [{"text": "Pyramid score", "start_pos": 120, "end_pos": 133, "type": "METRIC", "confidence": 0.9735895693302155}, {"text": "P", "start_pos": 137, "end_pos": 138, "type": "METRIC", "confidence": 0.9387844800949097}]}, {"text": "We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references; (3) \"optimal\" summaries generated using the procedure explained in) by ranking document utterances by the number of model summaries in which they appear.", "labels": [], "entities": []}, {"text": "It appears that our system is considerably better than the baseline, and achieves 91.3% of human performance in terms of Pyramid scores, and 83% if using ASR transcription.", "labels": [], "entities": [{"text": "Pyramid scores", "start_pos": 121, "end_pos": 135, "type": "METRIC", "confidence": 0.9152688086032867}]}, {"text": "This last result is particularly positive if we consider our strong reliance on lexical features.", "labels": [], "entities": []}, {"text": "For completeness, we also included standard ROUGE (1, 2, and L) scores in, which were obtained using parameters defined for the .515: Pyramid, and average ROUGE scores for summaries produces by a baseline (lead summarizer), our best system, humans, and the optimal summarizer.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9779021739959717}, {"text": "ROUGE", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.945669412612915}]}, {"text": "Since system summaries have on average approximately the same length as references, we only report recall measures of ROUGE (precision and F averages are within \u00b1 .002).", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9991401433944702}, {"text": "ROUGE", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.9990666508674622}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9974266886711121}, {"text": "F", "start_pos": 139, "end_pos": 140, "type": "METRIC", "confidence": 0.9788201451301575}]}, {"text": "It may come as a surprise that our best system (both with ASR and true words) performs almost as well as humans; it seems more reasonable to conclude that, in our case, ROUGE has trouble discriminating between systems with moderately close performance.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.814440131187439}]}, {"text": "This seems to confirm our impression that content evaluation in our task should be based on exact matches.", "labels": [], "entities": [{"text": "content evaluation", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7146253883838654}]}, {"text": "We performed a last experiment to compare our best system against, who used the same test data, but constrained summary sizes in terms of number of DA units instead of words.", "labels": [], "entities": []}, {"text": "In their experiments, 10% of DAs had to be selected.", "labels": [], "entities": []}, {"text": "Our system achieves .91 recall, .5 precision, and .64 F 1 with the same length constraint.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9807012677192688}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9967336654663086}, {"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9090020954608917}]}, {"text": "The discrepancy between recall and precision is largely due to the fact that generated summaries are on average much longer than model summaries (10% vs. 6.26% of DAs), which explains why our precision is relatively low in this last evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9988381266593933}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9983885288238525}, {"text": "precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.9984762072563171}]}, {"text": "The best ROUGE-1 measure reported in () is .69 recall, which is significantly lower than ours according to confidence intervals.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9939368367195129}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9684080481529236}]}], "tableCaptions": [{"text": " Table 1: Snippet of a meeting displaying an AP construction, where a question (A) initiates three responses (B). Sentences in  italic are not present in the reference summary.", "labels": [], "entities": []}, {"text": " Table 2: Contingency tables: while the correlation between  adjacent labels yt\u22121 and yt is not significant (\u03c7 2 = 2.3,  p > .05), empirical evidence clearly shows that ys and y d  influence each other (\u03c7 2 = 78948, p < .001).", "labels": [], "entities": []}, {"text": " Table 4: Forward feature selection.", "labels": [], "entities": [{"text": "Forward feature selection", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7666665514310201}]}, {"text": " Table 5: Pyramid score for each feature set.", "labels": [], "entities": [{"text": "Pyramid score", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9671313166618347}]}, {"text": " Table 6: Pyramid scores for different sequence models, where  k stands for the order of linear-chain features. The value in  bold is the performance of the model that was selected after  a 5-fold cross validation on the training data, which obtained  the highest F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 264, "end_pos": 272, "type": "METRIC", "confidence": 0.982019692659378}]}, {"text": " Table 7: Pyramid, and average ROUGE scores for summaries  produces by a baseline (lead summarizer), our best system,  humans, and the optimal summarizer.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9642853736877441}, {"text": "summaries", "start_pos": 48, "end_pos": 57, "type": "TASK", "confidence": 0.9739876389503479}]}]}