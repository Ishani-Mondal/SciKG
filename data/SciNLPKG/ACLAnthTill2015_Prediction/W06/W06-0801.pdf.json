{"title": [{"text": "Indonesian-Japanese CLIR Using Only Limited Resource", "labels": [], "entities": []}], "abstractContent": [{"text": "Our research aim here is to build a CLIR system that works fora language pair with poor resources where the source language (e.g. Indonesian) has limited language resources.", "labels": [], "entities": []}, {"text": "Our Indonesian-Japanese CLIR system employs the existing Japanese IR system, and we focus our research on the Indonesian-Japanese query translation.", "labels": [], "entities": [{"text": "Indonesian-Japanese query translation", "start_pos": 110, "end_pos": 147, "type": "TASK", "confidence": 0.5598076184590658}]}, {"text": "There are two problems in our limited resource query translation: the OOV problem and the translation ambiguity.", "labels": [], "entities": [{"text": "limited resource query translation", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.6481785029172897}, {"text": "OOV", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8125542998313904}]}, {"text": "The OOV problem is handled using target language's resources (English-Japanese dictionary and Japanese proper name dictionary).", "labels": [], "entities": [{"text": "OOV", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8502771258354187}]}, {"text": "The translation ambiguity is handled using a Japanese monolingual corpus in our translation filtering.", "labels": [], "entities": [{"text": "translation filtering", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.8192132115364075}]}, {"text": "We select the final translation set using the mutual information score and the TF\u00d7IDF score.", "labels": [], "entities": [{"text": "TF\u00d7IDF score", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.7088121771812439}]}, {"text": "The result on NTCIR 3 (NII-NACSIS Test Collection for IR Systems) Web Retrieval Task shows that the translation method achieved a higher IR score than the transitive machine translation (using Kataku (Indonesian-English) and Babelfish/ Excite (English-Japanese) engine) result.", "labels": [], "entities": [{"text": "NTCIR 3 (NII-NACSIS Test Collection", "start_pos": 14, "end_pos": 49, "type": "DATASET", "confidence": 0.8272731204827627}, {"text": "IR Systems) Web Retrieval", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.4963230490684509}, {"text": "transitive machine translation", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.6341653565565745}]}, {"text": "The best result achieved about 49% of the monolingual retrieval.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We measure our query translation performance by the IR score achieved by a CLIR system because CLIR is areal application and includes the performance of keyword expansion.", "labels": [], "entities": [{"text": "query translation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.6813177466392517}, {"text": "IR score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9713285863399506}, {"text": "keyword expansion", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.6970449686050415}]}, {"text": "For this, we do not use word translation accuracy, as for the CLIR, since a one-to-one translation rate is not suitable, given there are so many semantically equivalent words.", "labels": [], "entities": [{"text": "word translation", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7293932437896729}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.882619321346283}]}, {"text": "Our CLIR experiments are conducted on NTCIR-3 Web Retrieval Task data (100 Gb Japanese documents), in which the Japanese queries and translated English queries were prepared.", "labels": [], "entities": [{"text": "NTCIR-3 Web Retrieval Task data", "start_pos": 38, "end_pos": 69, "type": "DATASET", "confidence": 0.785983681678772}]}, {"text": "The Indonesian queries (47 queries) are manually translated from English queries.", "labels": [], "entities": []}, {"text": "The 47 queries contain 528 Indonesian words (225 are not stop words), 35 English borrowed words, and 16 transliterated Japanese words (proper nouns).", "labels": [], "entities": []}, {"text": "The IR system (Fujii and Ishikawa) is borrowed from Atsushi Fujii (Tsukuba University).", "labels": [], "entities": [{"text": "IR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.4766342341899872}, {"text": "Tsukuba University)", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.7662257154782613}]}, {"text": "External resources used in the query translation are listed in.", "labels": [], "entities": [{"text": "query translation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7621462643146515}]}, {"text": "Indonesian-Japanese dictionary, 14,823 words ToggleText Kataku Indonesian-English machine translation Excite English-Japanese machine translation Babelfish English-Japanese machine translation  In the experiments, we compare the IR score of each translation method.", "labels": [], "entities": [{"text": "ToggleText Kataku Indonesian-English machine translation Excite English-Japanese machine translation Babelfish English-Japanese machine translation", "start_pos": 45, "end_pos": 192, "type": "TASK", "confidence": 0.8250918342516973}, {"text": "IR", "start_pos": 229, "end_pos": 231, "type": "METRIC", "confidence": 0.987011194229126}]}, {"text": "The IR scores shown in this section are in Mean Average Precision (MAP) scores.", "labels": [], "entities": [{"text": "IR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.504727840423584}, {"text": "Mean Average Precision (MAP) scores", "start_pos": 43, "end_pos": 78, "type": "METRIC", "confidence": 0.9557007125445774}]}, {"text": "The evaluation metrics is referred to.", "labels": [], "entities": []}, {"text": "Each query group has 4 MAP scores: RL (highly relevant document as correct answer with hyperlink information used), RC (highly relevant document as correct answer), PL (partially relevant document as correct answer with hyperlink information used), and PC (partially relevant document as correct answer).", "labels": [], "entities": [{"text": "MAP", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.6771723628044128}, {"text": "RL", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.8771745562553406}, {"text": "PC", "start_pos": 253, "end_pos": 255, "type": "METRIC", "confidence": 0.9826334118843079}]}, {"text": "The documents hyperlinked from retrieved documents are used for relevance assessment.", "labels": [], "entities": [{"text": "relevance assessment", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.9108332097530365}]}], "tableCaptions": []}