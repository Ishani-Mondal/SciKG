{"title": [{"text": "Lexicon Acquisition for Dialectal Arabic Using Transductive Learning", "labels": [], "entities": [{"text": "Dialectal Arabic", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7187658101320267}]}], "abstractContent": [{"text": "We investigate the problem of learning a part-of-speech (POS) lexicon fora resource-poor language, dialectal Arabic.", "labels": [], "entities": []}, {"text": "Developing a high-quality lexicon is often the first step towards building a POS tag-ger, which is in turn the front-end to many NLP systems.", "labels": [], "entities": []}, {"text": "We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Trans-ductive SVMs, Spectral Graph Transducers , and a novel Transductive Clustering method.", "labels": [], "entities": []}, {"text": "We demonstrate that lexicon learning is an important task in resource-poor domains and leads to significant improvements in tagging accuracy for dialec-tal Arabic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9375694990158081}]}], "introductionContent": [{"text": "Due to the rising importance of globalization and multilingualism, there is a need to build natural language processing (NLP) systems for an increasingly wider range of languages, including those languages that have traditionally not been the focus of NLP research.", "labels": [], "entities": []}, {"text": "The development of NLP technologies fora new language is a challenging task since one needs to deal not only with language-specific phenomena but also with a potential lack of available resources (e.g. lexicons, text, annotations).", "labels": [], "entities": []}, {"text": "In this study we investigate the problem of learning a part-of-speech (POS) lexicon fora resource-poor language, dialectal Arabic.", "labels": [], "entities": []}, {"text": "Developing a high-quality POS lexicon is the first step towards training a POS tagger, which in turn is typically the front end for other NLP applications such as parsing and language modeling.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 75, "end_pos": 85, "type": "TASK", "confidence": 0.6563269346952438}, {"text": "parsing", "start_pos": 163, "end_pos": 170, "type": "TASK", "confidence": 0.9676180481910706}, {"text": "language modeling", "start_pos": 175, "end_pos": 192, "type": "TASK", "confidence": 0.6256131827831268}]}, {"text": "In the case of resource-poor languages (and dialectal Arabic in particular), this step is much more critical than is typically assumed: a lexicon with too few constraints on the possible POS tags fora given word can have disastrous effects on tagging accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9123340845108032}]}, {"text": "Whereas such constraints can be obtained from large hand-labeled corpora or highquality annotation tools in the case of resourcerich languages, no such resources are available for dialectal Arabic.", "labels": [], "entities": []}, {"text": "Instead, constraints on possible POS tags must be inferred from a small amount of tagged words, or imperfect analysis tools.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.7824613451957703}]}, {"text": "This can be seen as the problem of learning complex, structured outputs (multi-class labels, with a different number of classes for different words and dependencies among the individual labels) from partially labeled data.", "labels": [], "entities": []}, {"text": "Our focus is on investigating several machine learning techniques for this problem.", "labels": [], "entities": []}, {"text": "In particular, we argue that lexicon learning in resourcepoor languages can be best viewed as transductive learning.", "labels": [], "entities": []}, {"text": "The main contribution of this work are: (1) a comprehensive evaluation of three transductive algorithms (Transductive SVM, Spectral Graph Transducer, and anew technique called Transductive Clustering) as well as an inductive SVM on this task; and (2) a demonstration that lexicon learning is a worthwhile investment and leads to significant improvements in the tagging accuracy for dialectal Arabic.", "labels": [], "entities": [{"text": "tagging", "start_pos": 361, "end_pos": 368, "type": "TASK", "confidence": 0.9565081596374512}, {"text": "accuracy", "start_pos": 369, "end_pos": 377, "type": "METRIC", "confidence": 0.8276272416114807}]}, {"text": "The outline of the paper is as follows: Section 2 describes the problem in more detail and discusses the situation in dialectal Arabic.", "labels": [], "entities": []}, {"text": "The transductive framework and algorithms for lexicon learning are elaborated in Section 3.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 describe the data and system.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 6.", "labels": [], "entities": []}, {"text": "We discuss some related work in Section 7 before concluding in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We seek to answer the following three questions in our experiments: \u2022 How useful is the lexicon learning step in an end-to-end POS tagging system?", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.7632395327091217}]}, {"text": "Do the machine learning algorithms produce lexicons that result in higher tagging accuracies, when compared to a baseline lexicon that simply hypothesizes all POS tags for un-analyzable words?", "labels": [], "entities": []}, {"text": "The answer is a definitive yes.", "labels": [], "entities": []}, {"text": "\u2022 What machine learning algorithms perform the best on this task?", "labels": [], "entities": []}, {"text": "Do transductive learning outperform inductive learning?", "labels": [], "entities": []}, {"text": "The empirical answer is that TSVM performs best, SGT performs worst, and TC and ISVM are in the middle.", "labels": [], "entities": []}, {"text": "Our approach to building a resource-poor POS tagger involves (a) lexicon learning, and (b) un-  supervised training.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.8149919807910919}]}, {"text": "In this section we examine cases where (a) an expert lexicon is available, so that lexicon learning is not required, and (b) sentences are annotated with POS information, so that supervised training is possible.", "labels": [], "entities": []}, {"text": "The goal of these experiments is to determine when alternative approaches involving additional human annotations become worthwhile in this task.", "labels": [], "entities": []}, {"text": "(a) Expert lexicon: First, we build an expert lexicon by collecting all tags per word in the development set (i.e. \"oracle\" POS-sets).", "labels": [], "entities": []}, {"text": "Then, the tagger is trained using EM by treating the development set as raw text (i.e. ignoring the POS annotations).", "labels": [], "entities": []}, {"text": "This achieves an accuracy of 74.45% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9997332692146301}]}, {"text": "Note that this accuracy is significantly higher than the ones in, which represent unsupervised training on more raw text (the training set), but with non-expert lexicons derived from the MSA analyzer and a machine learner.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994257688522339}]}, {"text": "This result further demonstrates the importance of obtaining an accurate lexicon in unsupervised training.", "labels": [], "entities": []}, {"text": "If one were to build this expert lexicon by hand, one would need an annotator to label the POS-sets of 2450 distinct lexicon items.", "labels": [], "entities": []}, {"text": "(b) Supervised training: We build a supervised tagger by training on the POS annotations of the development set, which achieves 82.93% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9808753728866577}]}, {"text": "This improved accuracy comes at the cost of annotating 2.2k sentences (16k tokens) with complete POS information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9991859793663025}]}, {"text": "Finally, we present the same results with reduced data, taking first 50, 100, 200, etc.", "labels": [], "entities": []}, {"text": "sentences in the development set for lexicon or POS annotation.", "labels": [], "entities": []}, {"text": "The learning curve is shown in.", "labels": [], "entities": []}, {"text": "One maybe tempted to draw conclusions regarding supervised vs. unsupervised approaches by directly comparing this table with the results in Section 6.1; we avoid doing so since taggers in Sections 6.1 and 6.2 are trained on different data sets (training vs. development set) and the accuracy differences are compounded by issues such  as ngram coverage, data-set selection, and the way annotations are done.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 283, "end_pos": 291, "type": "METRIC", "confidence": 0.9962584972381592}]}], "tableCaptions": [{"text": " Table 1: Percentage of word types/tokens with N  possible tags, as determined by the Buckwalter an- alyzer. Words with 0 tags are un-analyzable.", "labels": [], "entities": []}, {"text": " Table 3: Tagging Accuracies for lexicons derived  by machine learning (TSVM, TC, ISVM, SGT)  and baseline methods. Accuracy=Overall accu- racy; UnkAcc=Accuracy of unknown words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9853295683860779}, {"text": "Accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9823394417762756}]}, {"text": " Table 4: Statistics of the Lexicons in", "labels": [], "entities": []}, {"text": " Table 5: Tag accuracies by correcting mistakes in  the partial lexicon prior to lexicon learning. In- terestingly, we note ISVM outperforms TC here,  which differs from", "labels": [], "entities": [{"text": "Tag accuracies", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.943441241979599}]}, {"text": " Table 7: (1) Supervised training accuracies with  varying numbers of sentences. (2) Accuracies of  unsupervised training using a expert lexicon of  different vocabulary sizes.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9256571531295776}]}]}