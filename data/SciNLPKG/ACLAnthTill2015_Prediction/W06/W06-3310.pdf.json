{"title": [{"text": "Exploring Text and Image Features to Classify Images in Bioscience Lit- erature", "labels": [], "entities": [{"text": "Bioscience Lit- erature", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.5761265680193901}]}], "abstractContent": [{"text": "A picture is worth a thousand words.", "labels": [], "entities": []}, {"text": "Biomedical researchers tend to incorporate a significant number of images (i.e., figures or tables) in their publications to report experimental results, to present research models, and to display examples of biomedical objects.", "labels": [], "entities": []}, {"text": "Unfortunately, this wealth of information remains virtually inaccessible without automatic systems to organize these images.", "labels": [], "entities": []}, {"text": "We explored supervised machine-learning systems using Support Vector Machines to automatically classify images into six representative categories based on text, image, and the fusion of both.", "labels": [], "entities": []}, {"text": "Our experiments show a significant improvement in the average F-score of the fusion classifier (73.66%) as compared with classifiers just based on image (50.74%) or text features (68.54%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9981210827827454}]}], "introductionContent": [{"text": "A picture is worth a thousand words.", "labels": [], "entities": []}, {"text": "Biomedical researchers tend to incorporate a significant number of figures and tables in their publications to report experimental results, to present research models, and to display examples of biomedical objects (e.g., cell, tissue, organ and other images).", "labels": [], "entities": []}, {"text": "For example, we have found an average of 5.2 images per biological article in the journal Proceedings of the National Academy of Sciences (PNAS).", "labels": [], "entities": []}, {"text": "We discovered that 43% of the articles in the medical journal The Lancet contain biomedical images.", "labels": [], "entities": [{"text": "The Lancet contain biomedical images", "start_pos": 62, "end_pos": 98, "type": "DATASET", "confidence": 0.9208488702774048}]}, {"text": "Physicians may want to access biomedical images reported in literature for the purpose of clinical education or to assist clinical diagnoses.", "labels": [], "entities": []}, {"text": "For example, a physician may want to obtain images that illustrate the disease stage of infants with Retinopathy of Prematurity for the purpose of clinical diagnosis, or to request a picture of erythema chronicum migrans, a spreading annular rash that appears at the site of tick-bite in Lyme disease.", "labels": [], "entities": []}, {"text": "Biologists may want to identify the experimental results or images that support specific biological phenomenon.", "labels": [], "entities": []}, {"text": "For example, shows that a transplanted progeny of a single multipotent stem cell can generate sebaceous glands.", "labels": [], "entities": []}, {"text": "Organizing bioscience images is not anew task.", "labels": [], "entities": []}, {"text": "Related work includes the building of domainspecific image databases.", "labels": [], "entities": []}, {"text": "For example, the Protein Data Bank (PDB)) stores 3-D images of macromolecular structure data.", "labels": [], "entities": [{"text": "Protein Data Bank (PDB))", "start_pos": 17, "end_pos": 41, "type": "DATASET", "confidence": 0.855337197581927}]}, {"text": "WebPath 2 is a medical web-based resource that has been created by physicians to include over 4,700 gross and microscopic medical images.", "labels": [], "entities": []}, {"text": "Textbased image search systems like Google ignore image content.", "labels": [], "entities": []}, {"text": "The SLIF (Subcellular Location Image Finder) system (Murphy et al.,;) searches protein images reported in literature.", "labels": [], "entities": [{"text": "Subcellular Location Image Finder)", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.7277728676795959}]}, {"text": "Other work has explored joint text-image features in classifying protein subcellular location images ().", "labels": [], "entities": [{"text": "classifying protein subcellular location images", "start_pos": 53, "end_pos": 100, "type": "TASK", "confidence": 0.8471423864364624}]}, {"text": "The existing systems, however, have not explored approaches that automatically classify general bioscience images into generic categories.", "labels": [], "entities": []}, {"text": "Classifying images into generic categories is an important task that can benefit many other natural language processing and image processing tasks.", "labels": [], "entities": [{"text": "Classifying images into generic categories", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8672633767127991}, {"text": "image processing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7475469708442688}]}, {"text": "For example, image retrieval and question answering systems may return \"Image-of-Thing\" images (e.g.,), not the other types (e.g.,), to illustrate erythema chronicum migrans.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7727779448032379}, {"text": "question answering", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7653025984764099}]}, {"text": "Biologists may examine \"Gel\" images (e.g.,), rather than \"Model\" (e.g.,) to access specific biological evidence for molecular interactions.", "labels": [], "entities": []}, {"text": "Furthermore, a generic category may ease the task of identifying specific images that maybe sub-categories of the generic category.", "labels": [], "entities": []}, {"text": "For example, a biologist may want to obtain an image of a protein structure prediction, which might be a subcategory of \"Model\"), rather than an image of x-ray crystallography that can be readily obtained from the PDB database.", "labels": [], "entities": [{"text": "protein structure prediction", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.7218433817227682}, {"text": "PDB database", "start_pos": 214, "end_pos": 226, "type": "DATASET", "confidence": 0.9632277190685272}]}, {"text": "This paper represents the first study that defines a generic bioscience image taxonomy, and explores automatic image classification based on the fusion of text and image classifiers.", "labels": [], "entities": [{"text": "automatic image classification", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.6374472181002299}]}, {"text": "Gel-Image consists of gel images such as Northern (for DNA), Southern (for RNA), and Western (for protein).", "labels": [], "entities": [{"text": "Western", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9673386812210083}]}, {"text": "Graph consists of bar charts, column charts, line charts, plots and other graphs that are drawn either by authors or by a computer (e.g., results of patch clamping).", "labels": [], "entities": []}, {"text": "Image-of-Thing refers to images of cells, cell components, tissues, organs, or species.", "labels": [], "entities": []}, {"text": "Mix refers to an image (e.g.,) that incorporates two or more other categories of images.", "labels": [], "entities": []}, {"text": "Model: A model may demonstrate a biological process, molecular docking, or an experimental design.", "labels": [], "entities": []}, {"text": "We include as Model any structure (e.g., chemical, molecular, or cellular) that is illustrated by a drawing.", "labels": [], "entities": []}, {"text": "We also include gene or protein sequences and sequence alignments, as well as phylogenetic trees in this category.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report the widely used recall, precision, and Fscore (also known as F-measure) as the evaluation metrics for image classification.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9992846846580505}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9976422190666199}, {"text": "Fscore", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9993584752082825}, {"text": "F-measure", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9918157458305359}, {"text": "image classification", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.7685215175151825}]}, {"text": "Recall is the total number of true positive predictions divided by the total number of true positives in the set (true pos + false neg).", "labels": [], "entities": []}, {"text": "Precision is the fraction of the number of true positive predictions divided by the total number of positive predictions (true pos + false pos).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.993732750415802}]}, {"text": "F-score is the harmonic mean of recall and precision equal to (C. J. van:   A near-optimal parameter setting for the classifier based on image features alone used a polynomial kernel of order 2 and an upper slack limit of C = 10^4..", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.983041524887085}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9984493255615234}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9983538389205933}]}, {"text": "Precision, Recall, F-score for Fusion Classifier From, it is apparent that the fusion classifier does best on IMAGE_OF_THING and also performs well on GEL and GRAPH.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9947088956832886}, {"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9885016679763794}, {"text": "F-score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.996248185634613}, {"text": "IMAGE_OF_THING", "start_pos": 110, "end_pos": 124, "type": "METRIC", "confidence": 0.7527770876884461}, {"text": "GEL", "start_pos": 151, "end_pos": 154, "type": "DATASET", "confidence": 0.5651028156280518}, {"text": "GRAPH", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.5576079487800598}]}, {"text": "These are substantial improvements over the classifiers that were based on image or text feature alone.", "labels": [], "entities": []}, {"text": "Average F-scores and accuracies are summarized below in.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9980587363243103}, {"text": "accuracies", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9806504845619202}]}, {"text": "The overall accuracy for the fusion classifier = sum of true positives / total number of image = (23+37+15+14+13)/138 = 102/138 = 74%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996048808097839}]}, {"text": "This can be compared with the baseline of 44/138 = 32% if all images were classified as the most popular category, GRAPH.", "labels": [], "entities": [{"text": "GRAPH", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.5168402791023254}]}], "tableCaptions": [{"text": " Table 2. Confusion Matrix for Image Feature Clas- sifier", "labels": [], "entities": []}, {"text": " Table 3. Precision, Recall, F-score for Image Clas- sifier", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9937538504600525}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9886792302131653}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9958871006965637}]}, {"text": " Table 4. Confusion Matrix for Caption Text Clas- sifier", "labels": [], "entities": [{"text": "Caption Text Clas- sifier", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7892016172409058}]}, {"text": " Table 5. Precision, Recall, F-score for Caption  Text Classifier", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.992375910282135}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9850216507911682}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9957669973373413}, {"text": "Caption  Text Classifier", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8554499944051107}]}, {"text": " Table 6. Confusion Matrix for Fusion Classifier", "labels": [], "entities": []}, {"text": " Table 7. Precision, Recall, F-score for Fusion  Classifier", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9981387853622437}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9880287051200867}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9970186948776245}]}, {"text": " Table 8. Comparison of Average F-scores and Ac- curacy among all three Classifiers", "labels": [], "entities": [{"text": "F-scores", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.5330932140350342}, {"text": "Ac- curacy", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.977298378944397}]}]}