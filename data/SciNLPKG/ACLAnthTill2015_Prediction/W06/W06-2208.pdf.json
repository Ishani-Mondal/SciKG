{"title": [{"text": "Expanding the Recall of Relation Extraction by Bootstrapping", "labels": [], "entities": [{"text": "Recall of Relation Extraction", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.9178742319345474}]}], "abstractContent": [{"text": "Most works on relation extraction assume considerable human effort for making an annotated corpus or for knowledge engineering.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.9445638060569763}, {"text": "knowledge engineering", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.7573440968990326}]}, {"text": "Generic patterns employed in KnowItAll achieve unsupervised, high-precision extraction, but often result in low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9978867173194885}]}, {"text": "This paper compares two boot-strapping methods to expand recall that start with automatically extracted seeds by KnowItAll.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9958963394165039}]}, {"text": "The first method is string pattern learning, which learns string contexts adjacent to a seed tuple.", "labels": [], "entities": [{"text": "string pattern learning", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.8111181457837423}]}, {"text": "The second method learns less restrictive patterns that include bags of words and relation-specific named entity tags.", "labels": [], "entities": []}, {"text": "Both methods improve the recall of the generic pattern method.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9986336827278137}]}, {"text": "In particular, the less restrictive pattern learning method can achieve a 250% increase in recall at 0.87 precision, compared to the generic pattern method.", "labels": [], "entities": [{"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9979323148727417}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9768099188804626}]}], "introductionContent": [{"text": "Relation extraction is a task to extract tuples of entities that satisfy a given relation from textual documents.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.952705591917038}]}, {"text": "Examples of relations include CeoOf(Company, Ceo) and Acquisition(Organization, Organization).", "labels": [], "entities": []}, {"text": "There has been much work on relation extraction; most of it employs knowledge engineering or supervised machine learning approaches;).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9650482535362244}]}, {"text": "Both approaches are labor intensive.", "labels": [], "entities": []}, {"text": "We begin with a baseline information extraction system,), that does unsupervised information extraction at Web scale.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7353434562683105}, {"text": "information extraction", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7496531903743744}]}, {"text": "KnowItAll uses a set of generic extraction patterns, and automatically instantiates rules by combining these patterns with user supplied relation labels.", "labels": [], "entities": []}, {"text": "For example, KnowItAll has patterns fora generic \"of\" relation: NP1 's \ud97b\udf59relation\ud97b\udf59 , NP2 NP2 , \ud97b\udf59relation\ud97b\udf59 of NP1 where NP1 and NP2 are simple noun phrases that extract values of argument1 and argument2 of a relation, and \ud97b\udf59relation\ud97b\udf59 is a user-supplied string associated with the relation.", "labels": [], "entities": []}, {"text": "The rules may also constrain NP1 and NP2 to be proper nouns.", "labels": [], "entities": []}, {"text": "If a user supplies the relation labels \"ceo\" and \"chief executive officer\" for the relation CeoOf(Company, Ceo), KnowItAll inserts these labels into the generic patterns shown above, to create 4 extraction rules: NP1 's ceo , NP2 NP1 's chief executive officer , NP2 NP2 , ceo of NP1 NP2 , chief executive officer of NP1 The same generic patterns with different labels can also produce extraction rules fora MayorOf relation or an InventorOf relation.", "labels": [], "entities": []}, {"text": "These rules have alternating context strings (exact string match) and extraction slots (typically an NP or head of an NP).", "labels": [], "entities": []}, {"text": "This can produce rules with high precision, but low recall, due to the wide variety of contexts describing a relation.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.997617781162262}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9993817806243896}]}, {"text": "This paper looks at ways to enhance recall over this baseline system while maintaining high precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9982796907424927}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9975368976593018}]}, {"text": "To enhance recall, we employ bootstrapping techniques which start with seed tuples, i.e. the most frequently extracted tuples by the baseline system.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9921869039535522}]}, {"text": "The first method represents rules with three context strings of tokens immediately adjacent to the extracted arguments: a left context, middle context, and right context.", "labels": [], "entities": []}, {"text": "These are induced from context strings found adjacent to seed tuples.", "labels": [], "entities": []}, {"text": "The second method uses a less restrictive pattern representation such as bag of words, similar to that of SnowBall(.", "labels": [], "entities": [{"text": "SnowBall", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9695000648498535}]}, {"text": "SnowBall is a semi-supervised relation extraction system.", "labels": [], "entities": [{"text": "SnowBall", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9591766595840454}, {"text": "relation extraction", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7240030020475388}]}, {"text": "The input of Snowball is a few hand labeled correct seed tuples fora relation (e.g. <Microsoft, Steve Ballmer> for CeoOf relation).", "labels": [], "entities": [{"text": "Snowball", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9750174880027771}]}, {"text": "SnowBall clusters the bag of words representations generated from the context strings adjacent to each seed tuple, and generates rules from them.", "labels": [], "entities": [{"text": "SnowBall", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9295386672019958}]}, {"text": "It calculates the confidence of candidate tuples and the rules iteratively by using an EM-algorithm.", "labels": [], "entities": []}, {"text": "Because it can extract any tuple whose entities co-occur within a window, the recall can be higher than the string pattern learning method.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9996734857559204}]}, {"text": "The main disadvantage of SnowBall or a method which employs less restrictive patterns is that it requires Named Entity Recognizer (NER).", "labels": [], "entities": [{"text": "SnowBall", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9012551307678223}, {"text": "Named Entity Recognizer (NER)", "start_pos": 106, "end_pos": 135, "type": "METRIC", "confidence": 0.7059987336397171}]}, {"text": "We introduce Relation-dependent NER (Relation NER), which trains an off-the-shelf supervised NER based on CRF() with bootstrapping.", "labels": [], "entities": []}, {"text": "This learns relation-specific NE tags, and we present a method to use these tags for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8294236063957214}]}, {"text": "This paper compares the following two bootstrapping strategies.", "labels": [], "entities": []}, {"text": "SPL: a simple string pattern learning method.", "labels": [], "entities": []}, {"text": "It learns string patterns adjacent to a seed tuple.", "labels": [], "entities": []}, {"text": "LRPL: a less restrictive pattern learning method.", "labels": [], "entities": [{"text": "LRPL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7279238700866699}]}, {"text": "It learns a variety of bag of words patterns, after training a Relation NER.", "labels": [], "entities": [{"text": "Relation NER", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.563376247882843}]}, {"text": "Both methods are completely self-supervised extensions to the unsupervised KnowItAll.", "labels": [], "entities": []}, {"text": "A user supplies KnowItAll with one or more relation labels to be applied to one or more generic extraction patterns.", "labels": [], "entities": []}, {"text": "No further tagging or manual selection of seeds is required.", "labels": [], "entities": []}, {"text": "Each of the bootstrapping methods uses seeds that are automatically selected from the output of the baseline KnowItAll system.", "labels": [], "entities": []}, {"text": "The results show that both bootstrapping methods improve the recall of the baseline system.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9988939166069031}]}, {"text": "The two methods have comparable results, with LRPL outperforms SPL for some relations and SPL outperforms LRPL for other relations.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 and 3 describe SPL and LRPL respectively.", "labels": [], "entities": [{"text": "SPL", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8420088291168213}, {"text": "LRPL", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.8937616944313049}]}, {"text": "Section 4 reports on our experiments, and section 5 and 6 describe related works and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The focus of this paper is the comparison between bootstrapping strategies for extraction, i.e., string pattern learning and less restrictive pattern learning having Relation NER.", "labels": [], "entities": []}, {"text": "Therefore, we first compare these two bootstrapping methods with the baseline system.", "labels": [], "entities": []}, {"text": "Furthermore, we also compare Relation NER with a generic NER, which is trained on a pre-existing hand annotated corpus.", "labels": [], "entities": [{"text": "Relation NER", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.7421994507312775}]}], "tableCaptions": [{"text": " Table 1: Weights corresponding to a context vector  (\u00db \u00d7\u00d8 ).", "labels": [], "entities": []}, {"text": " Table 2: The number of entities and unique entities  in MUC7 corpus. The number of documents is  225.  entity  all  uniq  Organization 3704 993  Person  2120 1088  Location  2912 692", "labels": [], "entities": [{"text": "MUC7 corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9661988317966461}, {"text": "Organization 3704 993  Person  2120 1088  Location  2912 692", "start_pos": 123, "end_pos": 183, "type": "DATASET", "confidence": 0.6860980292161306}]}, {"text": " Table 3: The argument precision and recall is the average over all arguments for CeoOf, and MayorOf  relations. The Location is for MayorOf, Organization is for CeoOf, and person is the average of both  relations.  Argument  Location Organization Person  Recall Precision F  Precision Precision  Precision  R-NER  0.650 0.912  0.758 0.922  0.906  0.955  G-NER  0.392 0.663  0.492 0.682  0.790  0.809  G-NER+dic 0.577 0.643  0.606 0.676  0.705  0.842", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9839590191841125}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9993415474891663}]}]}