{"title": [{"text": "Lessons Learned from Large Scale Evaluation of Systems that Produce Text: Nightmares and Pleasant Surprises", "labels": [], "entities": []}], "abstractContent": [{"text": "Extended Abstract As the language generation community explores the possibility of an evaluation program for language generation, it behooves us to examine our experience in evaluation of other systems that produce text as output.", "labels": [], "entities": [{"text": "language generation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.743058443069458}]}, {"text": "Large scale evaluation of sum-marization systems and of question answering systems has been carried out for several years now.", "labels": [], "entities": [{"text": "question answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8446461856365204}]}, {"text": "Summarization and question answering systems produce text output given text as input, while language generation produces text from a semantic representation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7665652632713318}, {"text": "language generation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7105708122253418}]}, {"text": "Given that the output has the same properties, we can learn from the mistakes and the understandings gained in earlier evaluations.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}