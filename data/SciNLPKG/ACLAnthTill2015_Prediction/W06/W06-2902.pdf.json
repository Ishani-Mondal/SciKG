{"title": [{"text": "Porting Statistical Parsers with Data-Defined Kernels", "labels": [], "entities": [{"text": "Porting Statistical Parsers", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.845067540804545}]}], "abstractContent": [{"text": "Previous results have shown disappointing performance when porting a parser trained on one domain to another domain where only a small amount of data is available.", "labels": [], "entities": []}, {"text": "We propose the use of data-defined kernels as away to exploit statistics from a source domain while still specializing a parser to a target domain.", "labels": [], "entities": []}, {"text": "A probabilistic model trained on the source domain (and possibly also the target domain) is used to define a kernel, which is then used in a large margin classifier trained only on the target domain.", "labels": [], "entities": []}, {"text": "With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, significant progress has been made in the area of natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6454348862171173}]}, {"text": "This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus).", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 129, "end_pos": 153, "type": "DATASET", "confidence": 0.9845778942108154}]}, {"text": "The best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from a different domain.", "labels": [], "entities": []}, {"text": "This is an important problem because we cannot expect to have large annotated corpora available for most domains.", "labels": [], "entities": []}, {"text": "While identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9752064347267151}]}, {"text": "Instead they propose methods for training a standard parser with a large amount of out-of-domain data and a small amount of in-domain data.", "labels": [], "entities": []}, {"text": "In this paper, we propose using data-defined kernels and large margin methods to specifically address porting a parser to anew domain.", "labels": [], "entities": []}, {"text": "Data-defined kernels are used to construct anew parser which exploits information from a parser trained on a large out-of-domain corpus.", "labels": [], "entities": []}, {"text": "Large margin methods are used to train this parser to optimize performance on a small in-domain corpus.", "labels": [], "entities": []}, {"text": "Large margin methods have demonstrated substantial success in applications to many machine learning problems, because they optimize a measure which is directly related to the expected testing performance.", "labels": [], "entities": []}, {"text": "They achieve especially good performance compared to other classifiers when only a small amount of training data is available.", "labels": [], "entities": []}, {"text": "Most of the large margin methods need the definition of a kernel.", "labels": [], "entities": []}, {"text": "Work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees (e.g. (), which are chosen on the basis of domain knowledge.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6741853753725687}]}, {"text": "In) it was proposed to apply a class of kernels derived from probabilistic models to the natural language parsing problem.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.6178309917449951}]}, {"text": "In), the kernel is constructed using the parameters of a trained probabilistic model.", "labels": [], "entities": []}, {"text": "This type of kernel is called a datadefined kernel, because the kernel incorporates information from the data used to train the probabilistic model.", "labels": [], "entities": []}, {"text": "We propose to exploit this property to transfer information from a large corpus to a statis-tical parser fora different domain.", "labels": [], "entities": []}, {"text": "Specifically, we propose to train a statistical parser on data including the large corpus, and to derive the kernel from this trained model.", "labels": [], "entities": []}, {"text": "Then this derived kernel is used in a large margin classifier trained on the small amount of training data available for the target domain.", "labels": [], "entities": []}, {"text": "In our experiments, we consider two different scenarios for porting parsers.", "labels": [], "entities": [{"text": "porting parsers", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.9100634157657623}]}, {"text": "The first scenario is the pure porting case, which we call \"transferring\".", "labels": [], "entities": [{"text": "porting", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9024824500083923}]}, {"text": "Here we only require a probabilistic model trained on the large corpus.", "labels": [], "entities": []}, {"text": "This model is then reparameterized so as to extend the vocabulary to better suit the target domain.", "labels": [], "entities": []}, {"text": "The kernel is derived from this reparameterized model.", "labels": [], "entities": []}, {"text": "The second scenario is a mixture of parser training and porting, which we call \"focusing\".", "labels": [], "entities": [{"text": "porting", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.9706125259399414}]}, {"text": "Here we train a probabilistic model on both the large corpus and the target corpus.", "labels": [], "entities": []}, {"text": "The kernel is derived from this trained model.", "labels": [], "entities": []}, {"text": "In both scenarios, the kernel is used in a SVM classifier) trained on a small amount of data from the target domain.", "labels": [], "entities": []}, {"text": "This classifier is trained to rerank the candidate parses selected by the associated probabilistic model.", "labels": [], "entities": []}, {"text": "We use the Penn Treebank Wall Street Journal corpus as the large corpus and individual sections of the Brown corpus as the target corpora).", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal corpus", "start_pos": 11, "end_pos": 51, "type": "DATASET", "confidence": 0.9730704327424368}, {"text": "Brown corpus", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.926636129617691}]}, {"text": "The probabilistic model is a neural network statistical parser, and the data-defined kernel is a TOP reranking kernel).", "labels": [], "entities": []}, {"text": "With both scenarios, the resulting parser demonstrates improved accuracy on the target domain over the probabilistic model alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9990609288215637}]}, {"text": "In additional experiments, we evaluate the hypothesis that the primary issue for porting parsers between domains is differences in the distributions of words in structures, and not in the distributions of the structures themselves.", "labels": [], "entities": [{"text": "porting parsers between domains", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.8654713183641434}]}, {"text": "We partition the parameters of the probability model into those which define the distributions of words and those that only involve structural decisions, and derive separate kernels for these two subsets of parameters.", "labels": [], "entities": []}, {"text": "The former model achieves virtually identical accuracy to the full model, but the later model does worse, confirming the hypothesis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9991932511329651}]}], "datasetContent": [{"text": "We used the Penn Treebank WSJ corpus and the Brown corpus to evaluate our approach.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.9674856066703796}, {"text": "Brown corpus", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9513539671897888}]}, {"text": "We used the standard division of the WSJ corpus into training, validation, and testing sets.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.7983679175376892}]}, {"text": "In the Brown corpus we ran separate experiments for sections F (informative prose: popular lore), K (imaginative prose: general fiction), N (imaginative prose: adventure and western fiction), and P (imaginative prose: romance and love story).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9407184720039368}, {"text": "imaginative prose: adventure and western fiction", "start_pos": 141, "end_pos": 189, "type": "TASK", "confidence": 0.670581728219986}]}, {"text": "These sections were selected because they are sufficiently large, and because they appeared to be maximally different from each other and from WSJ text.", "labels": [], "entities": [{"text": "WSJ text", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.940744549036026}]}, {"text": "In each Brown corpus section, we selected every third sentence for testing.", "labels": [], "entities": [{"text": "Brown corpus section", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.9410177071889242}]}, {"text": "From the remaining sentences, we used 1 sentence out of 20 for the validation set, and the remainder for training.", "labels": [], "entities": [{"text": "validation", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.9653438329696655}]}, {"text": "The resulting datasets sizes are presented in table 1.", "labels": [], "entities": []}, {"text": "For the large margin classifier, we used the SVMStruct () implementation of SVM, which rescales the margin with F 1 measure of bracketed constituents (see) for details).", "labels": [], "entities": []}, {"text": "Linear slack penalty was employed.", "labels": [], "entities": [{"text": "Linear slack penalty", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.6648635963598887}]}, {"text": "To evaluate the pure porting scenario (transferring), described in section 3.1, we trained the SSN parsing model on the WSJ corpus.", "labels": [], "entities": [{"text": "SSN parsing", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.8908650875091553}, {"text": "WSJ corpus", "start_pos": 120, "end_pos": 130, "type": "DATASET", "confidence": 0.9851592183113098}]}, {"text": "For each tag, there is an unknown-word vocabulary item which is used for all those words not sufficiently frequent with that tag to be included individually in the vocabulary.", "labels": [], "entities": []}, {"text": "vocabulary of the parser, we included the unknownword items and the words which occurred in the training set at least 20 times.", "labels": [], "entities": []}, {"text": "This led to the vocabulary of 4,215 tag-word pairs.", "labels": [], "entities": []}, {"text": "We derived the kernel from the trained model for each target section (F, K, N, P) using reparameterization discussed in section 3.1: we included in the vocabulary all the words which occurred at least twice in the training set of the corresponding section.", "labels": [], "entities": []}, {"text": "This approach led to a smaller vocabulary than that of the initial parser but specifically tied to the target domain (3,613, 2,789, 2,820 and 2,553 tag-word pairs for sections F, K, N and P respectively).", "labels": [], "entities": []}, {"text": "There is no sense in including the words from the WSJ which do not appear in the Brown section training set because the classifier won't be able to learn the corresponding components of its decision vector.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.8909097909927368}, {"text": "Brown section training set", "start_pos": 81, "end_pos": 107, "type": "DATASET", "confidence": 0.8628768622875214}]}, {"text": "The results for the original probabilistic model (SSN-WSJ) and for the kernel method (TOP-Transfer) on the testing set of each section are presented in table 2.", "labels": [], "entities": []}, {"text": "To evaluate the relative contribution of our porting technique versus the use of the TOP kernel alone, we also used this TOP kernel to train an SVM on the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 155, "end_pos": 165, "type": "DATASET", "confidence": 0.9342728555202484}]}, {"text": "We trained the SVM on data from the development set and section 0, so that the size of this dataset (3,267 sentences) was about the same as for each Brown section.", "labels": [], "entities": []}, {"text": "This gave us a \"TOP-WSJ\" model, which we tested on each of the four Brown sections.", "labels": [], "entities": [{"text": "TOP-WSJ", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.8717418313026428}]}, {"text": "In each case, the TOP-WSJ model did worse than the original SSN-WSJ model, as shown in table 2.", "labels": [], "entities": [{"text": "TOP-WSJ", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.6278679966926575}]}, {"text": "This makes it clear that we are getting no improvement from simply using a TOP kernel alone or simply using more data, and all our improvement is from the proposed porting method.", "labels": [], "entities": []}, {"text": "To perform the experiments on the approach suggested in section 3.2 (focusing), we trained the SSN parser on the WSJ training set joined with the training set of the corresponding section.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9571140011151632}]}, {"text": "We included in the vocabulary only words which appeared in the joint training set at least 20 times.", "labels": [], "entities": []}, {"text": "Resulting vocabularies comprised 4,386, 4,365, 4,367 and 4,348 for sections F, K, N and P, respectively.", "labels": [], "entities": []}, {"text": "5 Experiments were done in the same way as for the parser transferring approach, but reparameterization was not performed.", "labels": [], "entities": [{"text": "parser transferring", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8455714881420135}]}, {"text": "Standard measures of accuracy for the original probabilistic model (SSN-WSJ+Br) and the kernel method (TOP-Focus) are also shown in table 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.999413251876831}, {"text": "Br", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.6667258739471436}]}, {"text": "For the sake of comparison, we also trained the SSN parser on only training data from one of the Brown corpus sections (section P), producing a \"SSN-Brown\" model.", "labels": [], "entities": [{"text": "SSN parser", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.8454822301864624}, {"text": "Brown corpus sections", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.8160758217175802}]}, {"text": "This model achieved an F 1 measure of only 81.0% for the P section testing set, which is worse than all the other models and is 3% lower than our best results on this testing set (TOP-Focus).", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9932054678599039}, {"text": "P section testing set", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.6296597868204117}, {"text": "TOP-Focus", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.6488540172576904}]}, {"text": "This result underlines the need to port parsers from domains in which there are large annotated datasets.", "labels": [], "entities": []}, {"text": "We conducted the same set of experiments with the kernel with vocabulary features (TOP-Voc-Transfer and TOP-Voc-Focus) and with the kernel with the structural features (TOP-Str-Transfer and TOP-StrFocus).", "labels": [], "entities": []}, {"text": "Average results for classifiers with these kernels, as well as for the original kernel and the baseline, are presented in table 3.", "labels": [], "entities": []}, {"text": "We would expect some improvement if we used a smaller threshold on the target domain, but preliminary results suggest that this improvement would be small.: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (F \u03b2=1 ) on the individual test sets.", "labels": [], "entities": [{"text": "recall (LR)", "start_pos": 188, "end_pos": 199, "type": "METRIC", "confidence": 0.9486856460571289}, {"text": "precision (LP)", "start_pos": 201, "end_pos": 215, "type": "METRIC", "confidence": 0.9710017293691635}]}], "tableCaptions": [{"text": " Table 1: Number of sentences (words) for each  dataset.", "labels": [], "entities": []}, {"text": " Table 2: Percentage labeled constituent recall (LR),  precision (LP), and a combination of both (F \u03b2=1 ) on  the individual test sets.", "labels": [], "entities": [{"text": "Percentage labeled constituent recall (LR)", "start_pos": 10, "end_pos": 52, "type": "METRIC", "confidence": 0.7690591812133789}, {"text": "precision (LP)", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.9705380946397781}, {"text": "F \u03b2=1 )", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9430620074272156}]}, {"text": " Table 3: Average accuracy of the models on chapters  F, K, N and P of the Brown corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9704644083976746}, {"text": "Brown corpus", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9492406249046326}]}]}