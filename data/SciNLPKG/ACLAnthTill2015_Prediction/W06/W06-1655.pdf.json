{"title": [{"text": "A Hybrid Markov/Semi-Markov Conditional Random Field for Sequence Segmentation", "labels": [], "entities": [{"text": "Sequence Segmentation", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.9214231371879578}]}], "abstractContent": [{"text": "Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmenta-tion and labeling.", "labels": [], "entities": []}, {"text": "Both models have advantages in terms of the type of features they most naturally represent.", "labels": [], "entities": []}, {"text": "We propose a hybrid model that is capable of representing both types of features, and describe efficient algorithms for its training and inference.", "labels": [], "entities": []}, {"text": "We demonstrate that our hybrid model achieves error reductions of 18% and 25% over a standard order-1 CRF and a semi-Markov CRF (resp.) on the task of Chinese word segmentation.", "labels": [], "entities": [{"text": "error", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9838820695877075}, {"text": "Chinese word segmentation", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.5719223419825236}]}, {"text": "We also propose the use of a powerful feature for the semi-Markov CRF: the log conditional odds that a given token sequence constitutes a chunk according to a generative model, which reduces error by an additional 13%.", "labels": [], "entities": [{"text": "error", "start_pos": 191, "end_pos": 196, "type": "METRIC", "confidence": 0.9830254316329956}]}, {"text": "Our best system achieves 96.8% F-measure, the highest reported score on this test set.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9950751662254333}]}], "introductionContent": [{"text": "The problem of segmenting sequence data into chunks arises in many natural language applications, such as named-entity recognition, shallow parsing, and word segmentation in East Asian languages.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.7734572291374207}, {"text": "shallow parsing", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7710363268852234}, {"text": "word segmentation", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.7689182758331299}]}, {"text": "Two popular discriminative models that have been proposed for these tasks are the conditional random field (CRFs) () and the semi-Markov conditional random field (semi-CRF) (.", "labels": [], "entities": []}, {"text": "A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme.", "labels": [], "entities": [{"text": "BEGIN", "start_pos": 154, "end_pos": 159, "type": "METRIC", "confidence": 0.987697422504425}]}, {"text": "CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.7746510903040568}]}, {"text": "In the Second International Chinese Word Segmentation Bakeoff (), two of the highest scoring systems in the closed track competition were based on a CRF model.", "labels": [], "entities": [{"text": "Second International Chinese Word Segmentation Bakeoff", "start_pos": 7, "end_pos": 61, "type": "TASK", "confidence": 0.76278056204319}]}, {"text": "( While the CRF is quite effective compared with other models designed for CWS, one wonders whether it maybe limited by its restrictive independence assumptions on non-adjacent labels: an order-M CRF satisfies the order-M Markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the M labels to its left and right.", "labels": [], "entities": []}, {"text": "Consequently, the model only \"sees\" word boundaries within a moving window of M + 1 characters, which prohibits it from explicitly modeling the tendency of strings longer than that window to form words, or from modeling the lengths of the words.", "labels": [], "entities": []}, {"text": "Although the window can in principle be widened by increasing M , this is not a practical solution as the complexity of training and decoding a linear sequence CRF grows exponentially with the Markov order.", "labels": [], "entities": []}, {"text": "The semi-CRF is a sequence model that is designed to address this difficulty via careful relaxation of the Markov assumption.", "labels": [], "entities": []}, {"text": "Rather than recasting the segmentation problem as a labeling problem, the semi-CRF directly models the distribution of chunk boundaries.", "labels": [], "entities": []}, {"text": "In terms of inde-pendence, using an order-M semi-CRF entails the assumption that, globally conditioned on the input sequence, the position of each chunk boundary is independent of all other boundaries given the positions of the M boundaries to its left and right regardless of how faraway they are.", "labels": [], "entities": []}, {"text": "Even with an order-1 model, this enables several classes of features that one would expect to be of great utility to the word segmentation task, in particular word length and word identity.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.7948065598805746}, {"text": "word identity", "start_pos": 175, "end_pos": 188, "type": "TASK", "confidence": 0.7051503211259842}]}, {"text": "Despite this, the only work of which we are aware exploring the use of a semi-Markov CRF for Chinese word segmentation did not find significant gains over the standard CRF (.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.6140839755535126}]}, {"text": "This is surprising, not only because the additional features a semi-CRF enables are intuitively very useful, but because as we will show, an order-M semi-CRF is strictly more powerful than an order-M CRF, in the sense that any feature that can be used in the latter can also be used in the former, or equivalently, the semi-CRF makes strictly weaker independence assumptions.", "labels": [], "entities": []}, {"text": "Given a judicious choice of features (or simply enough training data) the semi-CRF should be superior.", "labels": [], "entities": []}, {"text": "We propose that the reason for this discrepancy maybe that despite the greater representational power of the semi-CRF, there are some valuable features that are more naturally expressed in a CRF segmentation model, and so they are not typically included in semi-CRFs (indeed, they have not to date been used in any semi-CRF model for any task, to our knowledge).", "labels": [], "entities": []}, {"text": "In this paper, we show that semi-CRFs are strictly more expressive, and also demonstrate how CRF-type features can be used in a semi-CRF model for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 147, "end_pos": 172, "type": "TASK", "confidence": 0.5880886018276215}]}, {"text": "Our experiments show that a model incorporating both types of features can outperform models using only one or the other type.", "labels": [], "entities": []}, {"text": "Orthogonally, we explore in this paper the use of a very powerful feature for the semi-CRF derived from a generative model.", "labels": [], "entities": []}, {"text": "It is common in statistical NLP to use as features in a discriminative model the (logarithm of the) estimated probability of some event according to a generative model.", "labels": [], "entities": []}, {"text": "For example, Collins (2000) uses a discriminative classifier for choosing among the top N parse trees output by a generative baseline model, and uses the log-probability of a parse according to the baseline model as a feature in the reranker.", "labels": [], "entities": []}, {"text": "Similarly, the machine translation system of Och and Ney uses log-probabilities of phrasal translations and other events as features in a log-linear model).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7564244866371155}]}, {"text": "There are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of a discriminative model with the simple parameter estimation and inference of a generative one, and also the fact that generative models are more robust in data sparse scenarios).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9979342222213745}]}, {"text": "For word segmentation, one might want to use as a local feature the log-probability that a segment is a word, given the character sequence it spans.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7764436602592468}]}, {"text": "A curious property of this feature is that it induces a counterintuitive asymmetry between the is-word and is-not-word cases: the component generative model can effectively dictate that a certain chunk is not a word, by assigning it a very low probability (driving the feature value to negative infinity), but it cannot dictate that a chunk is a word, because the log-probability is bounded above.", "labels": [], "entities": []}, {"text": "If instead the log conditional odds log P i (y|x) P i (\u00acy|x) is used, the asymmetry disappears.", "labels": [], "entities": []}, {"text": "We show that such a logodds feature provides much greater benefit than the log-probability, and that it is useful to include such a feature even when the model also includes indicator function features for every word in the training corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the ideas discussed in this paper, we compared the performance of semi-CRFs using various feature sets on a Chinese word segmentation task.", "labels": [], "entities": [{"text": "Chinese word segmentation task", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.6927519515156746}]}, {"text": "The data used was the Microsoft Research Beijing corpus from the Second International Chinese Word Segmentation Bakeoff, and we used the same train/test split used in the competition.", "labels": [], "entities": [{"text": "Microsoft Research Beijing corpus from the Second International Chinese Word Segmentation Bakeoff", "start_pos": 22, "end_pos": 119, "type": "DATASET", "confidence": 0.8995342502991358}]}, {"text": "The training set consists of 87K sentences of Beijing dialect Chinese, hand segmented into 2.37M words.", "labels": [], "entities": []}, {"text": "The test set contains 107K words comprising roughly 4K sentences.", "labels": [], "entities": []}, {"text": "We used a maximum word length k of 15 in our experiments, which accounted for 99.99% of the word tokens in our training set.", "labels": [], "entities": []}, {"text": "The 249 training sentences that contained words longer than 15 characters were discarded.", "labels": [], "entities": []}, {"text": "We did not discard any test sentences.", "labels": [], "entities": []}, {"text": "In order to be directly comparable to the Bakeoff results, we also worked under the very strict \"closed test\" conditions of the Bakeoff, which require that no information or data outside of the training set be used, not even prior knowledge of which characters represent Arabic numerals, Latin characters or punctuation marks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test F-measure for different model con- figurations.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.887575089931488}]}]}