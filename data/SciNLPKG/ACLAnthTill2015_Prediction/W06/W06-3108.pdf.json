{"title": [{"text": "Discriminative Reordering Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Discriminative Reordering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6630754470825195}, {"text": "Statistical Machine Translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.8672314683596293}]}], "abstractContent": [{"text": "We present discriminative reordering models for phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 48, "end_pos": 92, "type": "TASK", "confidence": 0.5858640074729919}]}, {"text": "The models are trained using the maximum entropy principle.", "labels": [], "entities": []}, {"text": "We use several types of features: based on words, based on word classes, based on the local context.", "labels": [], "entities": []}, {"text": "We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus.", "labels": [], "entities": []}, {"text": "Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9664120078086853}]}], "introductionContent": [{"text": "In recent evaluations, phrase-based statistical machine translation systems have achieved good performance.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 23, "end_pos": 67, "type": "TASK", "confidence": 0.6351678222417831}]}, {"text": "Still the fluency of the machine translation output leaves much to desire.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6893921941518784}]}, {"text": "One reason is that most phrase-based systems use a very simple reordering model.", "labels": [], "entities": []}, {"text": "Usually, the costs for phrase movements are linear in the distance, e.g. see.", "labels": [], "entities": [{"text": "phrase movements", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7881962060928345}]}, {"text": "Recently, in (Tillmann and ) and in (), a reordering model has been described that tries to predict the orientation of a phrase, i.e. it answers the question 'should the next phrase be to the left or to the right of the current phrase?'", "labels": [], "entities": []}, {"text": "This phrase orientation probability is conditioned on the current source and target phrase and relative frequencies are used to estimate the probabilities.", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.6929013282060623}]}, {"text": "We adopt the idea of predicting the orientation, but we propose to use a maximum-entropy based model.", "labels": [], "entities": [{"text": "predicting the orientation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.749302864074707}]}, {"text": "The relative-frequency based approach may suffer from the data sparseness problem, because most of the phrases occur only once in the training corpus.", "labels": [], "entities": []}, {"text": "Our approach circumvents this problem by using a combination of phrase-level and word-level features and by using word-classes or part-of-speech information.", "labels": [], "entities": []}, {"text": "Maximum entropy is a suitable framework for combining these different features with a well-defined training criterion.", "labels": [], "entities": []}, {"text": "In () several variants of the orientation model have been tried.", "labels": [], "entities": []}, {"text": "It turned out that for different tasks, different models show the best performance.", "labels": [], "entities": []}, {"text": "Here, we let the maximum entropy training decide which features are important and which features can be neglected.", "labels": [], "entities": []}, {"text": "We will see that additional features do not hurt performance and can be safely added to the model.", "labels": [], "entities": []}, {"text": "The remaining part is structured as follows: first we will describe the related work in Section 2 and give a brief description of the baseline system in Section 3.", "labels": [], "entities": []}, {"text": "Then, we will present the discriminative reordering model in Section 4.", "labels": [], "entities": []}, {"text": "Afterwards, we will evaluate the performance of this new model in Section 5.", "labels": [], "entities": []}, {"text": "This evaluation consists of two parts: first we will evaluate the prediction capabilities of the model on a word-aligned corpus and second we will show improved translation quality compared to the baseline system.", "labels": [], "entities": []}, {"text": "Finally, we will conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus statistics after preprocessing for the BTEC task.  Arabic Chinese Japanese English  Train  Sentences  20 000  Running Words 180 075 176 199 198 453 189 927  Vocabulary  15 371  8 687  9 277  6 870  C-Star'03  Sentences  506  Running Words  3 552  3 630  4 130  3 823", "labels": [], "entities": [{"text": "BTEC task", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.6421162486076355}]}, {"text": " Table 3: Chinese-English NIST task: corpus statis- tics for the bilingual training data and the NIST eval- uation sets of the years 2002 to 2005.", "labels": [], "entities": [{"text": "NIST eval- uation sets", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.7242897629737854}]}, {"text": " Table 4: Classification error rates [%] using two orientation classes.", "labels": [], "entities": [{"text": "Classification error rates", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.772845854361852}]}, {"text": " Table 5: Classification error rates [%] using four orientation classes.", "labels": [], "entities": [{"text": "Classification error rates", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7759063343207041}]}, {"text": " Table 6: Translation Results for the BTEC task.  Language Pair  Reordering  WER [%] PER [%] NIST BLEU [%]  Arabic-English  Distance-based  24.1  20.9  10.0  63.8  Max-Ent based  23.6  20.7  10.1  64.8  Chinese-English Distance-based  50.4  43.0  7.67  44.4  Max-Ent based  49.3  42.4  7.36  45.8  Japanese-English Distance-based  32.1  25.2  8.96  56.2  Max-Ent based  31.2  25.2  9.00  56.8", "labels": [], "entities": [{"text": "BTEC task", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.5193188935518265}, {"text": "WER", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.49072298407554626}, {"text": "PER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.925804078578949}, {"text": "NIST", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.4825756251811981}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.8277792930603027}]}, {"text": " Table 8: Translation results for several evaluation sets of the Chinese-English NIST task.  Evaluation set  2002 (dev)  2003  2004  2005  Reordering  NIST BLEU[%] NIST BLEU[%] NIST BLEU[%] NIST BLEU[%]  None  8.96  33.5  8.67  32.7  8.76  32.0  8.62  30.8  Distance-based 9.19  34.6  8.85  33.2  9.05  33.2  8.79  31.6  Max-Ent based 9.24  35.5  8.87  33.9  9.04  33.6  8.78  32.1", "labels": [], "entities": [{"text": "Reordering  NIST BLEU[%] NIST BLEU[%] NIST BLEU", "start_pos": 139, "end_pos": 186, "type": "METRIC", "confidence": 0.6233381165398492}, {"text": "NIST", "start_pos": 190, "end_pos": 194, "type": "DATASET", "confidence": 0.572460949420929}, {"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.5098822116851807}]}]}