{"title": [], "abstractContent": [{"text": "Ranking documents or sentences according to both topic and sentiment relevance should serve a critical function in helping users when topics and sentiment polarities of the targeted text are not explicitly given, as is often the case on the web.", "labels": [], "entities": []}, {"text": "In this paper, we propose several sentiment information retrieval models in the framework of probabilistic language models, assuming that a user both inputs query terms expressing a certain topic and also specifies a sentiment polarity of interest in some manner.", "labels": [], "entities": [{"text": "sentiment information retrieval", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.7311064402262369}]}, {"text": "We combine sentiment relevance models and topic relevance models with model parameters estimated from training data, considering the topic dependence of the sentiment.", "labels": [], "entities": []}, {"text": "Our experiments prove that our models are effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data.", "labels": [], "entities": [{"text": "retrieval or classification of sentiment information", "start_pos": 95, "end_pos": 147, "type": "TASK", "confidence": 0.7831917256116867}]}, {"text": "The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8640404939651489}]}, {"text": "A number of studies have investigated sentiment classification at document level, e.g.,, and at sentence level, e.g., (); however, the accuracy is still less than desirable.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.9588744938373566}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9992935657501221}]}, {"text": "Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users.", "labels": [], "entities": []}, {"text": "We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8701900839805603}]}, {"text": "Intuitively, the expression of sentiment in text is dependent on the topic.", "labels": [], "entities": []}, {"text": "For example, a negative view for some voting event maybe expressed using 'flaw', while a negative view for some politician maybe expressed using 'reckless'.", "labels": [], "entities": []}, {"text": "Moreover, sentiment polarities are also dependent on topics or domains.", "labels": [], "entities": [{"text": "sentiment polarities", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9521237015724182}]}, {"text": "For example, the adjective 'unpredictable' may have a negative orientation in an automotive review, in a phrase such as 'unpredictable steering', but it could have a positive orientation in a movie review, in a phrase such as 'unpredictable plot', as mentioned in) in the context of his sentiment word detection.", "labels": [], "entities": [{"text": "sentiment word detection", "start_pos": 287, "end_pos": 311, "type": "TASK", "confidence": 0.7252301971117655}]}, {"text": "We propose sentiment retrieval models in the framework of generative language modeling, not only assuming query terms expressing a certain topic, but also assuming that the polarity of sentiment interest is specified by the user in some manner, where the topic dependence of the sentiment is considered.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7993238866329193}, {"text": "generative language modeling", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.9332966407140096}]}, {"text": "To the best of our knowledge, there have been no other studies on a retrieval model unifying both topic and sentiment, and further, there have been no other studies on sentiment retrieval.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.8462499976158142}]}, {"text": "The sentiment information often appears as local in a document, and therefore focusing on finer levels, i.e., sentence or passage levels rather than document level, is crucial.", "labels": [], "entities": []}, {"text": "We thus experiment on sentiment retrieval at the sentence level in this paper.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.9673503935337067}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the work related to this study.", "labels": [], "entities": []}, {"text": "Section 3 describes a generative model of sentiment, which is proposed here as a theoretical framework for our work.", "labels": [], "entities": [{"text": "generative model of sentiment", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.8126709312200546}]}, {"text": "Section 4 describes the task definition and our sentiment retrieval model.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8722297549247742}]}, {"text": "Section 5 explains the data we used for our experiments, and gives our experimental results.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We \u00af The majority of the articles are on 10 different topics, which are labeled at document level, but, in addition to these, a number of additional articles were randomly selected from a larger corpus of 270,000 documents.", "labels": [], "entities": []}, {"text": "\u00af Each article was manually annotated using an annotation scheme for opinions and other private states at phrase level.", "labels": [], "entities": []}, {"text": "We only used the annotations for sentiments that included some attributes such as polarity and strength.", "labels": [], "entities": []}, {"text": "In this data set, the topic relevance for the 10 topics is known at the document level, but unknown at the sentence level.", "labels": [], "entities": []}, {"text": "We assumed that all the sentences in a relevant document could be considered relevant to the topic.", "labels": [], "entities": []}, {"text": "This data set was annotated with sentiment polarities at the phrase level, but not explicitly annotated at the sentence level.", "labels": [], "entities": []}, {"text": "Therefore, we provided sentiment polarities at the sentence level to prepare training data and data for evaluation.", "labels": [], "entities": []}, {"text": "We set the sentence-level sentiment polarity equal to the polarity with the highest strength in each sentence.", "labels": [], "entities": []}, {"text": "Queries were expressed using the title of one of the 10 topics and specified as positive or negative.", "labels": [], "entities": []}, {"text": "Thus, we had 20 types of queries for our experiments.", "labels": [], "entities": []}, {"text": "Because the supposed relevance judgments in this setting are imperfect at sentence level, we used bpref (), in both the training and testing phases, as it is known to be tolerant of imperfect judgments.", "labels": [], "entities": []}, {"text": "Bpref uses binary relevance judgments to define the preference relation (i.e., any relevant document is preferred over any nonrelevant document fora given topic), while other measures, such as mean average precision, depend only on the ranks of the relevant documents.", "labels": [], "entities": [{"text": "Bpref", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8989090919494629}, {"text": "mean average precision", "start_pos": 193, "end_pos": 215, "type": "METRIC", "confidence": 0.8573004603385925}]}, {"text": "This is a strong assumption to make and may not be true in all cases.", "labels": [], "entities": []}, {"text": "A larger, more complete data set is required to perform a more detailed analysis, which is left as future work.", "labels": [], "entities": []}, {"text": "We disregarded 'neutral' and 'both' if other polarities appeared.", "labels": [], "entities": []}, {"text": "We can also set the sentence-level sentiment polarity according to the presence of polarity in each sentence, but we did not consider this setting here.", "labels": [], "entities": []}, {"text": "We conducted experiments on the training-based task described in Section 4.1, using either manual annotation as described in Section 5.2.1 or automatic annotation as described in Section 5.2.2.", "labels": [], "entities": []}, {"text": "contrasts sample probabilities from topicindependent sentiment relevance models and those from topic-dependent sentiment relevance models.", "labels": [], "entities": []}, {"text": "In the left two columns of this table, two sets of sample probabilities using the topic-independent model are presented.", "labels": [], "entities": []}, {"text": "One was computed from the manual annotation and the other was computed from the automatic annotation.", "labels": [], "entities": []}, {"text": "In the remaining columns, samples using the topic-dependent model are shown according to the three topics: (1) \"reaction to President Bush's 2002 State of the Union Address\", (2) \"2002 presidential election in Zimbabwe\", and (3) \"Israeli settlements in Gaza and West Bank\".", "labels": [], "entities": []}, {"text": "Some expressions were unexpectedly generated regardless of the types of annotation, e.g., 'palestinian' for Topic (3); however, we found some characteristics in the results using automatic annotation.", "labels": [], "entities": []}, {"text": "Some expressions on opinions that did not convey sentiments, such as 'state', frequently appeared regardless of topic.", "labels": [], "entities": [{"text": "state'", "start_pos": 70, "end_pos": 76, "type": "TASK", "confidence": 0.809506356716156}]}, {"text": "This sort of expression may effectively function as degrading sentences only conveying facts, but may function harmfully by catching sentences conveying opinions without sentiments in the task of sentiment retrieval.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.8508618474006653}]}, {"text": "Some topic expressions, such as 'settle' (stemmed from 'settlement' or not) for Topic (3), were generated, because such words convey positive sentiments in some other contexts and thus they were contained in the list of sentiment-bearing words that we used for automatic annotation.", "labels": [], "entities": [{"text": "settle", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9521356225013733}]}, {"text": "This will not cause a topic relevance model to drift, because we modeled the topic relevance using whole sentences, as described in Section 5.2.2; however, it may harm the sentiment relevance model to some extent.", "labels": [], "entities": []}, {"text": "We performed retrieval experiments in the steps described in Section 4.1.", "labels": [], "entities": []}, {"text": "For this purpose, we split the data into three parts: (i) \u00dc% as the training data, (ii) \u00b4\u00b4\u00bc \u00dc\u00b5% as the evaluation data, and (iii) \ud97b\udf59\u00bc% as the test data.", "labels": [], "entities": [{"text": "\u00dc", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.9960678815841675}, {"text": "\u00dc\u00b5", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9682342410087585}]}, {"text": "The test results of training-based task using manually annotated data and automatically annotated data are shown in Tables 2 and 3, respectively.", "labels": [], "entities": []}, {"text": "The scores were computed according to the bpref evaluation measure (), as mentioned in Section 5.1.", "labels": [], "entities": [{"text": "bpref evaluation measure", "start_pos": 42, "end_pos": 66, "type": "METRIC", "confidence": 0.8733300367991129}]}, {"text": "In addition to the bpref, mean average precision values are presented as 'AvgP' in the tables, for reference.", "labels": [], "entities": [{"text": "mean average precision", "start_pos": 26, "end_pos": 48, "type": "METRIC", "confidence": 0.7769977847735087}, {"text": "AvgP", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9876649975776672}]}, {"text": "In these tables, the top row indicates the percentages of the training data \u00dc.", "labels": [], "entities": []}, {"text": "It turned out that in all our experiments the appropriate fraction of training data was 40%.", "labels": [], "entities": []}, {"text": "In this setting, our slm model worked 76.1% better than the query likelihood model and 32.6% better than the conventional relevance model, when using manual annotation, and both improvements were statistically significant according to the Wilcoxon signed-rank test.", "labels": [], "entities": []}, {"text": "When using automatic annotation, the slm model worked 67.2% better than the query likelihood model and 25.9% better than the conventional relevance model, where both improvements were statistically significant.", "labels": [], "entities": []}, {"text": "The rmt-base model also worked well with automatic annotation.", "labels": [], "entities": []}, {"text": "For experiments on the seed-based task that was described in Section 4.1, we used three groups of: Experimental results of training-based task using automatically annotated data  We experimented with the seed-based task, making use of each of these seed word groups, in the steps described in Section 4.1.", "labels": [], "entities": []}, {"text": "For this purpose, we split the data into two parts: (i) 50% as the estimation data and (ii) 50% as the test data.", "labels": [], "entities": []}, {"text": "The test results using manually annotated data and automatically annotated data are shown in Tables 4 and 5, respectively, where the scores were computed according to the bpref evaluation measure.", "labels": [], "entities": []}, {"text": "Mean average precision values are also presented as 'AvgP' in the tables, for reference.", "labels": [], "entities": [{"text": "Mean average precision", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.802950362364451}, {"text": "AvgP", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9946414232254028}]}, {"text": "When using the manually annotated approach, our slm model worked well, especially with the seed word group \u00c7\u00ca, as shown in.", "labels": [], "entities": []}, {"text": "Using \u00c7\u00ca, the slm model worked 61.2% better than the query likelihood model and 15.2% better than the conventional relevance model, where both improvements were statistically significant according to the Wilcoxon signed-rank test.", "labels": [], "entities": []}, {"text": "Even: Experimental results of seed-based task using manually annotated data using the other seed word groups, the slm model worked 49-56% better than the query likelihood model and 6-12% better than the conventional relevance model; however, the latter improvement was not statistically significant.", "labels": [], "entities": []}, {"text": "The rmt-slm model also worked well with manual annotation.", "labels": [], "entities": []}, {"text": "When using automatic annotation, the slm model worked 46-48% better than the query likelihood model and 4-6% better than the conventional relevance model, as shown in.", "labels": [], "entities": []}, {"text": "The improvements over the conventional relevance model were statistically significant only when using \u00cc \u00cd\u00ca or \u00c3\u00c5; however, the score when using \u00c7\u00ca is almost comparable with the others.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results of training-based  task using manually annotated data", "labels": [], "entities": []}, {"text": " Table 3: Experimental results of training-based  task using automatically annotated data", "labels": [], "entities": []}]}