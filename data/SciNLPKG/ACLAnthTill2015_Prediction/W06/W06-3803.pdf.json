{"title": [{"text": "Graph-Based Text Representation for Novelty Detection", "labels": [], "entities": [{"text": "Graph-Based Text Representation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6680192848046621}, {"text": "Novelty Detection", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.830243319272995}]}], "abstractContent": [{"text": "We discuss several feature sets for novelty detection at the sentence level, using the data and procedure established in task 2 of the TREC 2004 novelty track.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.8855658173561096}, {"text": "TREC 2004 novelty track", "start_pos": 135, "end_pos": 158, "type": "DATASET", "confidence": 0.8366013467311859}]}, {"text": "In particular, we investigate feature sets derived from graph representations of sentences and sets of sentences.", "labels": [], "entities": []}, {"text": "We show that a highly connected graph produced by using sentence-level term distances and pointwise mutual information can serve as a source to extract features for novelty detection.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.84650519490242}]}, {"text": "We compare several feature sets based on such a graph representation.", "labels": [], "entities": []}, {"text": "These feature sets allow us to increase the accuracy of an initial novelty classifier which is based on a bag-of-word representation and KL divergence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9991945624351501}]}, {"text": "The final result ties with the best system at TREC 2004.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.8095922768115997}]}], "introductionContent": [{"text": "Novelty detection is the task of identifying novel information given a set of already accumulated background information.", "labels": [], "entities": [{"text": "Novelty detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8379813134670258}]}, {"text": "Potential applications of novelty detection systems are abundant, given the \"information overload\" in email, web content etc., for example, describe a scenario in which a newsfeed is personalized based on a measure of information novelty: the user can be presented with pieces of information that are novel, given the documents that have already been reviewed.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7541885077953339}]}, {"text": "This will spare the user the task of sifting through vast amounts of duplicate and redundant information on a topic to find bits and pieces of information that are of interest.", "labels": [], "entities": []}, {"text": "In 2002 TREC introduced a novelty track, which continued -with major changes -in 2003 and).", "labels": [], "entities": [{"text": "TREC", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.6297916173934937}]}, {"text": "In 2002 the task was to identify the set of relevant and novel sentences from an ordered set of documents within a TREC topic.", "labels": [], "entities": []}, {"text": "Novelty was defined as \"providing new information that has not been found in any previously picked sentences\".", "labels": [], "entities": []}, {"text": "Relevance was defined as \"relevant to the question or request made in the description section of the topic\".", "labels": [], "entities": [{"text": "Relevance", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9721800088882446}]}, {"text": "There were 50 topics for the novelty task in 2002.", "labels": [], "entities": []}, {"text": "For the 2003 novelty track a number of major changes were made.", "labels": [], "entities": []}, {"text": "Relevance and novelty detection were separated into different tasks, allowing a separate evaluation of relevance detection and novelty detection.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7013680040836334}, {"text": "relevance detection", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7391679286956787}, {"text": "novelty detection", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7326135337352753}]}, {"text": "In the 2002 track, the data proved to be problematic since the percentage of relevant sentences in the documents was small.", "labels": [], "entities": []}, {"text": "This, in turn, led to a very high percentage of relevant sentences being novel, given that amongst the small set of relevant sentences there was little redundancy.", "labels": [], "entities": []}, {"text": "50 new topics were created for the 2003 task, with a better balance of relevant and novel sentences.", "labels": [], "entities": []}, {"text": "Slightly more than half of the topics dealt with \"events,\" the rest with \"opinions.\"", "labels": [], "entities": []}, {"text": "The 2004 track used the same tasks, the same number of topics and the same split between event and opinion topics as the 2003 track.", "labels": [], "entities": []}, {"text": "For the purpose of this paper, we are only concerned with novelty detection, specifically with task 2 of the 2004 novelty track, as described in more detail in the following section.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.8824012279510498}]}, {"text": "The question that we investigate here is: what is a meaningful feature set for text representation for novelty detection?", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.9201458096504211}]}, {"text": "This is obviously a far-reaching and loaded question.", "labels": [], "entities": []}, {"text": "Possibilities range from simple bag-of-word features to features derived from sophisticated linguistic representations.", "labels": [], "entities": []}, {"text": "Ultimately, the question is open-ended since there will always be another feature or feature combination that could/should be exploited.", "labels": [], "entities": []}, {"text": "For our experiments, we have decided to focus more narrowly on the usefulness of features derived from graph representations and we have restricted ourselves to representations that do not require linguistic analysis.", "labels": [], "entities": []}, {"text": "Simple bag-of-word metrics like KL divergence establish a baseline for classifier performance.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.6936604976654053}]}, {"text": "More sophisticated metrics can be defined on the basis of graph representations.", "labels": [], "entities": []}, {"text": "Graph representations of text can be constructed without performing linguistic analysis, by using term distances in sentences and pointwise mutual information between terms to form edges between term-vertices.", "labels": [], "entities": []}, {"text": "A term-distance based representation has been used successfully fora variety of tasks in Mihalcea (2004) and.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table  1, with the best results boldfaced.", "labels": [], "entities": []}]}