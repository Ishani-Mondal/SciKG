{"title": [{"text": "Language Models and Reranking for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7319178730249405}]}], "abstractContent": [{"text": "Complex Language Models cannot be easily integrated in the first pass decoding of a Statistical Machine Translation system-the decoder queries the LM a very large number of times; the search process in the decoding builds the hypotheses incremen-tally and cannot make use of LMs that analyze the whole sentence.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.7094074289004008}]}, {"text": "We present in this paper the Language Computer's system for WMT06 that employs LM-powered reranking on hypotheses generated by phrase-based SMT systems", "labels": [], "entities": [{"text": "WMT06", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.7923066020011902}, {"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.7971643209457397}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems combine a number of translation models with one or more language models.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8076387246449789}]}, {"text": "Adding complex language models in the incremental process of decoding is a very challenging task.", "labels": [], "entities": []}, {"text": "Some language models can only score sentences as a whole.", "labels": [], "entities": []}, {"text": "Also, SMT decoders generate during the search process a very large number of partial hypotheses and query the language model/models . The solution to these problems is either to use multiple iterations for decoding, to make use of the complex LMs only for complete hypotheses in the search space or to generate n-best lists and to rescore the hypotheses using also the additional LMs.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9301329255104065}]}, {"text": "For the WMT 2006 shared task we opted for the reranking solution.", "labels": [], "entities": [{"text": "WMT 2006 shared task", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.8090362697839737}]}, {"text": "This paper describes our solution and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We generated the translation tables for each pair of languages using the alignment provided for this shared task.", "labels": [], "entities": []}, {"text": "We split the dev2006 files into two halves.", "labels": [], "entities": []}, {"text": "The first half was used to determine \u03bb 1 . Using \u03bb 1 , we created a 500-best list for each sentence in the second half.", "labels": [], "entities": []}, {"text": "We calculated the value of the enhanced features (EGW and Charniak) for each of these hypotheses.", "labels": [], "entities": [{"text": "EGW", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7191892266273499}]}, {"text": "Over this set of almost 500 K hypotheses, we computed 10 different \u03bb 2 using MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8486964106559753}]}, {"text": "The search process was seeded using \u03bb 1 padded with 0 for the new 13 features.", "labels": [], "entities": []}, {"text": "We sorted the \u03bb 2 s by the BLEU score estimated by the MERT algorithm.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9823600351810455}, {"text": "MERT", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8353299498558044}]}, {"text": "We pruned manually the \u03bb 2 s that diverge too much from the overall set of \u03bb 2 s (based on the observation that   these weights are overfitting).", "labels": [], "entities": []}, {"text": "We picked from the remaining set the best \u03bb 2 and a preferred subset of \u03bb 2 s to be used in voting.", "labels": [], "entities": []}, {"text": "The \u03bb 1 was also used to decode a 500-best list for each sentence in the devtest2006 and test2006 sets.", "labels": [], "entities": []}, {"text": "After computing value of the enhanced features for each of these hypotheses, we applied the reranking algorithm to pick anew first-best hypothesis -the output of our system.", "labels": [], "entities": []}, {"text": "We used the following parameters for decoding: -dl 5 -b 0.0001 -ttable-limit 30 -s 200 for French and Spanish and -dl 9 -b 0.00001 -ttable-limit 30 -s 200 for German.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: BLEU scores on the devtest2006 datasets.  Comparison with WPT05 results", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988501071929932}, {"text": "devtest2006 datasets", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.8503973186016083}, {"text": "WPT05", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.854831337928772}]}, {"text": " Table 4: BLEU scores on the test2006 datasets. Sub- mitted results are bolded.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988262057304382}, {"text": "test2006 datasets", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.946795791387558}]}]}