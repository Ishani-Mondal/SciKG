{"title": [{"text": "Partially Supervised Coreference Resolution for Opinion Summarization through Structured Rule Learning", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8611843883991241}, {"text": "Opinion Summarization", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7069959342479706}]}], "abstractContent": [{"text": "Combining fine-grained opinion information to produce opinion summaries is important for sentiment analysis applications.", "labels": [], "entities": [{"text": "opinion summaries", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.700305312871933}, {"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.97139772772789}]}, {"text": "Toward that end, we tackle the problem of source coreference resolution-linking together source mentions that refer to the same entity.", "labels": [], "entities": [{"text": "source coreference resolution-linking together source mentions", "start_pos": 42, "end_pos": 104, "type": "TASK", "confidence": 0.7972448269526163}]}, {"text": "The partially supervised nature of the problem leads us to define and approach it as the novel problem of partially supervised clustering.", "labels": [], "entities": []}, {"text": "We propose and evaluate anew algorithm for the task of source coreference resolution that outperforms competitive baselines.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.7593679626782736}]}], "introductionContent": [{"text": "Sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9364275634288788}, {"text": "extracting attitudes, opinions, evaluations, and sentiment from text", "start_pos": 37, "end_pos": 105, "type": "TASK", "confidence": 0.6753123971548948}]}, {"text": "Work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business, and politics (e.g.).", "labels": [], "entities": []}, {"text": "Additionally, sentiment analysis can augment existing NLP applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g., ).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.9446996450424194}, {"text": "question answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8583466708660126}, {"text": "information retrieval", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.8023778796195984}, {"text": "summarization", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9787691831588745}]}, {"text": "To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information).", "labels": [], "entities": [{"text": "extracting sentiment", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.9238132238388062}]}, {"text": "In contrast, our work concerns the summarization of fine-grained information about opinions.", "labels": [], "entities": [{"text": "summarization of fine-grained information about opinions", "start_pos": 35, "end_pos": 91, "type": "TASK", "confidence": 0.8585716585318247}]}, {"text": "In particular, while recent research efforts have shown that fine-grained opinions (e.g.,, ) as well as their sources (e.g.,,) can be extracted automatically, little has been done to create opinion summaries, where opinions from the same source/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are aggregated.", "labels": [], "entities": []}, {"text": "A simple opinion summary is shown in.", "labels": [], "entities": []}, {"text": "We expect that this type of opinion summary, based on fine-grained opinion information, will be important for information analysis applications in any domain where the analysis of opinions is critical.", "labels": [], "entities": [{"text": "information analysis", "start_pos": 110, "end_pos": 130, "type": "TASK", "confidence": 0.7601428627967834}]}, {"text": "This paper addresses the problem of opinion summarization by considering the creation of simple opinion summaries like those of figure 1.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.741076648235321}]}, {"text": "We propose source coreference resolution -the task of determining which mentions of opinion sources refer to the same entity -as the primary mechanism for identifying the set of opinions attributed to each real-world source.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.799099882443746}]}, {"text": "For this type of summary, source coreference resolution constitutes an integral step in the process of generating full opinion summaries.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.8047279715538025}, {"text": "generating full opinion summaries", "start_pos": 103, "end_pos": 136, "type": "TASK", "confidence": 0.5339299440383911}]}, {"text": "For example, given the opinion expressions of figure 1, their polarity, and the associated opinion sources and targets, the bulk of the resulting summary can be produced by recognizing that source mentions \"Zacarias Moussaoui\", \"he\", \"my\", and \"Mr. Moussaoui\" all refer to the same person; and that source mentions \"Mr. Zerkin\" and \"Zerkin\" refer to the same person.", "labels": [], "entities": []}, {"text": "At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g.,).", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.6604886750380198}, {"text": "noun phrase coreference resolution", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.7086901962757111}, {"text": "coreference resolution", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.8439708054065704}]}, {"text": "We hypothesize in Section 3, however, that the task is likely to succumb to a better solution by treating it in the context of anew machine learning setting that we refer to as partially supervised clustering.", "labels": [], "entities": []}, {"text": "In particular, due to high coreference annotation costs, data sets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in a document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or targets).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 276, "end_pos": 298, "type": "TASK", "confidence": 0.7910424470901489}]}, {"text": "As a result, we define the task of partially supervised clustering, the goal of which is to learn a clustering function from a set of partially specified clustering examples (Section 4).", "labels": [], "entities": []}, {"text": "We are not aware of prior work on the problem of partially supervised clustering and argue that it differs substantially from that of semi-supervised clustering.", "labels": [], "entities": []}, {"text": "We propose an algorithm for partially supervised clustering that extends a rule learner with structure information and is generally applicable to problems that fit the partially supervised clustering definition (Section 5).", "labels": [], "entities": [{"text": "partially supervised clustering", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.613748570283254}]}, {"text": "We apply the algorithm to the source coreference resolution task and evaluate its performance on a standard sentiment analysis data set that includes source coreference chains (Section 6).", "labels": [], "entities": [{"text": "source coreference resolution task", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.7130962312221527}, {"text": "sentiment analysis data set", "start_pos": 108, "end_pos": 135, "type": "DATASET", "confidence": 0.7006203532218933}]}, {"text": "We find that our algorithm outperforms highly competitive baselines by a considerable margin -B 3 score of 83.2 vs. 81.8 and 67.1 vs. 60.9 F1 score for the identification of positive source coreference links.", "labels": [], "entities": [{"text": "B 3 score", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9830195109049479}, {"text": "F1", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9992386102676392}]}], "datasetContent": [{"text": "This section describes the source coreference data set, the baselines, our implementation of StRip, and the results of our experiments.", "labels": [], "entities": [{"text": "StRip", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.7902274131774902}]}, {"text": "In addition to the baselines described above, we evaluate StRip both with and without unlabeled data.", "labels": [], "entities": [{"text": "StRip", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7052731513977051}]}, {"text": "That is, we train on the MPQA corpus StRip using either all NPs or just opinion source NPs.", "labels": [], "entities": [{"text": "MPQA corpus StRip", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9166756669680277}]}, {"text": "We use the B 3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions.", "labels": [], "entities": [{"text": "B 3 (Bagga and Baldwin, 1998) evaluation measure", "start_pos": 11, "end_pos": 59, "type": "METRIC", "confidence": 0.6834651556881991}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9997585415840149}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9991104006767273}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.999832034111023}]}, {"text": "B 3 is a measure widely used for evaluating coreference resolution algorithms.", "labels": [], "entities": [{"text": "B 3", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.932030588388443}, {"text": "coreference resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9383285045623779}]}, {"text": "The measure computes the precision and recall for each NP mention in a document, and then averages them to produce combined results for the entire output.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9995025396347046}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9984865188598633}]}, {"text": "More precisely, given a mention i that has been assigned to chain c i , the precision for mention i is defined as the number of correctly identified mentions inc i divided by the total number of mentions inc i . Recall for i is defined as the number of correctly identified mentions inc i divided by the number of mentions in the gold standard chain for i.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9990144968032837}, {"text": "Recall", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.9940749406814575}]}, {"text": "Results are shown in  on the MUC data, which indicates that for our task the similarity of the documents in the training and test sets appears to be more important than the presence of complete supervisory information.", "labels": [], "entities": [{"text": "MUC data", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9046109020709991}, {"text": "similarity", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9752317667007446}]}, {"text": "(Improvements over the RIPPER runs trained on the MUC corpora are statistically significant 7 , while improvements over the SVM runs are not.) also shows that StRip outperforms the baselines on both performance metrics.", "labels": [], "entities": [{"text": "MUC corpora", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9510869979858398}]}, {"text": "StRip's performance is better than the baselines when trained on MPQA src (improvement not statistically significant, p > 0.20) and even better when trained on the full MPQA corpus, which includes the unlabeled NPs (improvement over the baselines and the former StRip run statistically significant).", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 169, "end_pos": 180, "type": "DATASET", "confidence": 0.9011377096176147}]}, {"text": "These results confirm our hypothesis that StRip improves due to two factors: first, considering pairwise decisions in the context of the clustering function leads to improvements in the classifier; and, second, StRip can take advantage of the unlabeled portion of the data.", "labels": [], "entities": []}, {"text": "StRip's performance is all the more impressive considering the strength of the SVM and RIPPER baselines, which which represent the best runs across the 336 different parameter settings tested for SV M light and 144 different settings tested for RIPPER.", "labels": [], "entities": []}, {"text": "In contrast, all four of the StRip runs using the full MPQA corpus (we vary the loss ratio for false positive/false negative cost) outperform those baselines.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9566158354282379}]}], "tableCaptions": [{"text": " Table 1: Results for Source Coreference. MPQA src stands for the MPQA corpus limited to only source  NPs, while MPQA full contains the unlabeled NPs.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.8713926672935486}]}]}