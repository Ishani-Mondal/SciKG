{"title": [{"text": "LingPars, a Linguistically Inspired, Language-Independent Machine Learner for Dependency Treebanks", "labels": [], "entities": [{"text": "LingPars", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9353070259094238}]}], "abstractContent": [{"text": "This paper presents a Constraint Grammar-inspired machine learner and parser, Ling\u00ad Pars, that assigns dependencies to morpho\u00ad logically annotated treebanks in a function-centred way.", "labels": [], "entities": []}, {"text": "The system not only bases at\u00ad tachment probabilities for PoS, case, mood, lemma on those features' function probabili\u00ad ties, but also uses topological features like function/PoS n-grams, barrier tags and daughter-sequences.", "labels": [], "entities": []}, {"text": "In the CoNLL shared task, performance was below average on at\u00ad tachment scores, but a relatively higher score for function tags/deprels in isolation suggests that the system's strengths were not fully exploited in the current architecture.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes LingPars, a Constraint Gram\u00ad mar-inspired language-independent treebank-learn\u00ad er developed from scratch between January 9 th and March 9 th 2006 in the context of the CoNLL-X 2006 shared task (http://nextens.uvt.nl/~conll/), or\u00ad ganized by Sabine Buchholz, Erwin Marsi, Yval Krymolowski and Amit Dubey.", "labels": [], "entities": []}, {"text": "Training treebanks and test data were provided for 13 different lan\u00ad guages: Arabic (,), Czech (), Danish (Kromann 2003), Dutch (van der, German, Japanese (Kawata and Bartels), Portuguese (, Slovene (), Spanish (), Swedish (), Turkish, Bulgarian (.", "labels": [], "entities": []}, {"text": "A number of these treebanks were not originally annotated in dependency style, but transformed from constituent tree style for the task, and all differ widely in terms of tag granulari\u00ad ty (21-302 part-of-speech tags, 7-82 function la\u00ad bels).", "labels": [], "entities": []}, {"text": "Also, not all treebanks included morphologi\u00ad cal information, and only half offered a lemma field.", "labels": [], "entities": []}, {"text": "Such descriptive variation proved to be a considerable constraint for our parser design, as will be explained in chapter 2.", "labels": [], "entities": []}, {"text": "No external re\u00ad sources and no structural preprocessing were used 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Because of LingPars' strong focus on function tags, a separate analysis of attachment versus label per\u00ad formance was thought to be of interest.", "labels": [], "entities": []}, {"text": "1 plots the latter (Y-axis) against the former (X-axis), with dot size symbolizing treebank size.", "labels": [], "entities": []}, {"text": "In this evalua\u00ad tion, a fixed training chunk size of 50,000 tokens 11 was used, and tested on a different sample of 5,000 tokens (see also 5/50 evaluation in ill. 2).", "labels": [], "entities": []}, {"text": "For most languages, function performance was better than attachment performance (3.2 percentage points on average, as opposed to 0.44 for the CoNLL sys\u00ad tems overall), with dots above the hyphenated \"di\u00ad agonal of balance\".", "labels": [], "entities": [{"text": "CoNLL sys\u00ad tems", "start_pos": 142, "end_pos": 157, "type": "DATASET", "confidence": 0.7825064212083817}]}, {"text": "Interestingly, the graphics also makes it clear that performance was lower for small treebanks, despite the fact that training cor\u00ad pus size had been limited in the experiment, possi\u00ad bly indicating correlated differences in the balance between tag set size and treebank size.", "labels": [], "entities": [{"text": "training cor\u00ad pus size", "start_pos": 118, "end_pos": 140, "type": "METRIC", "confidence": 0.5872112214565277}]}, {"text": "2 keeps the information from ill.", "labels": [], "entities": []}, {"text": "1 (5/50-dep and 5/50-func), represented in the two lower lines, but adds performance for maximal training corpus size 12 with (a) a randomly chosen test chunk of 5,000 tokens not included in the training corpus (5/all-5) and (b) a 20,000 token chunk from the training corpus (20/all).", "labels": [], "entities": []}, {"text": "Languages were sorted ac\u00ad Smaller for Slovene and Arabic (for these languages: largest possible) Due to deadline time constraints, an upper limit of 400,000 lines was forced on the biggest treebanks, when training for unknown test data, meaning that only \u00bd of the German data and 1/3 of the Czech data could be used.", "labels": [], "entities": []}, {"text": "cording to 20/all-func accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9961225390434265}]}, {"text": "As can be seen from the dips in the remaining (lower) curves, small training corpora (asterisk-marked languages) made it difficult for the parser (1) to match 20/all attachment performance on unknown data, and to learn labels/functions in general (dips in all function curves, even 20/all).", "labels": [], "entities": []}, {"text": "For the larger tree\u00ad banks, the parser performed better (1-3 percentage points) for the full training set than for the 50,000 token training set.", "labels": [], "entities": []}], "tableCaptions": []}