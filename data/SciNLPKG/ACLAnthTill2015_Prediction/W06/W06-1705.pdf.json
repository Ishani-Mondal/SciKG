{"title": [], "abstractContent": [{"text": "This paper presents a proposal to facilitate the use of the annotated web as corpus by alleviating the annotation bottleneck for corpus data drawn from the web.", "labels": [], "entities": []}, {"text": "We describe a framework for large-scale distributed corpus annotation using peer-to-peer (P2P) technology to meet this need.", "labels": [], "entities": [{"text": "distributed corpus annotation", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6530364950497946}]}, {"text": "We also propose to annotate a large reference corpus in order to evaluate this framework.", "labels": [], "entities": []}, {"text": "This will allow us to investigate the affordances offered by distributed techniques to ensure replicability of linguistic research based on web-derived corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.", "labels": [], "entities": []}, {"text": "Its significance is reflected both in the growing interest in annotation software for word sense tagging () and in the long-standing use of part-of-speech taggers, parsers and morphological analysers for data from English and many other languages.", "labels": [], "entities": [{"text": "word sense tagging", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7504109938939413}]}, {"text": "Linguists, lexicographers, social scientists and other researchers are using ever larger amounts of corpus data in their studies.", "labels": [], "entities": []}, {"text": "In corpus linguistics the progression has been from the 1 millionword Brown and LOB corpora of the 1960s, to the 100 million-word British National Corpus of the 1990s.", "labels": [], "entities": [{"text": "Brown and LOB corpora", "start_pos": 70, "end_pos": 91, "type": "DATASET", "confidence": 0.5878536850214005}, {"text": "British National Corpus", "start_pos": 130, "end_pos": 153, "type": "DATASET", "confidence": 0.9553550680478414}]}, {"text": "In lexicography this progression is paralleled, for example, by Collins Dictionaries' initial 10 million word corpus growing to their current corpus of around 600 million words.", "labels": [], "entities": [{"text": "Collins Dictionaries' initial 10 million word corpus", "start_pos": 64, "end_pos": 116, "type": "DATASET", "confidence": 0.9476297242300851}]}, {"text": "In addition, the requirement for mega-and even giga-corpora 1 extends to other applications, such as lexical frequency studies, neologism research, and statistical natural language processing where models of sparse data are built.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 152, "end_pos": 191, "type": "TASK", "confidence": 0.6801216378808022}]}, {"text": "The motivation for increasingly large data sets remains the same.", "labels": [], "entities": []}, {"text": "Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type.", "labels": [], "entities": []}, {"text": "In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible unless the web is used as a corpus.", "labels": [], "entities": [{"text": "corpus linguistics building", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.8026639223098755}]}, {"text": "Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem).", "labels": [], "entities": []}, {"text": "This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.", "labels": [], "entities": []}, {"text": "In addition, the advantages of using linguistically annotated data over raw data are well documented.", "labels": [], "entities": []}, {"text": "As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "labels": [], "entities": []}, {"text": "Although processing power is steadily growing, it has already become impractical fora single computer to annotate a mega-corpus.", "labels": [], "entities": []}, {"text": "Creating a large-scale annotated corpus from the web requires away to overcome the limitations on processing power.", "labels": [], "entities": []}, {"text": "We propose distributed techniques to alleviate the limitations on the volume of data that can be tagged by a single processor.", "labels": [], "entities": []}, {"text": "The task of annotating the data will be shared by computers at collaborating institutions around the world, taking advantage of processing power and bandwidth that would otherwise go unused.", "labels": [], "entities": []}, {"text": "Such large-scale parallel processing removes the workload bottleneck imposed by a server based structure.", "labels": [], "entities": []}, {"text": "This allows for tagging a greater amount of textual data in a given amount of time while permitting other users to use the system simultaneously.", "labels": [], "entities": []}, {"text": "Vast amounts of data can be analysed with distributed techniques.", "labels": [], "entities": []}, {"text": "The feasibility of this approach has been demonstrated by the SETI@home project 2 . The framework we propose can incorporate other annotation or analysis systems, for example, lemmatisation, frequency profiling, or shallow parsing.", "labels": [], "entities": [{"text": "SETI@home", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.8059406479199728}, {"text": "shallow parsing", "start_pos": 215, "end_pos": 230, "type": "TASK", "confidence": 0.5773890763521194}]}, {"text": "To realise and evaluate the framework, it will be developed fora peer-to-peer (P2P) network and deployed along with an existing lexicographic toolset, the Sketch Engine.", "labels": [], "entities": []}, {"text": "A P2P approach allows fora low cost implementation that draws upon available resources (existing user PCs).", "labels": [], "entities": []}, {"text": "As a case study for evaluation, we plan to collect a large reference corpus from the web to be hosted on servers from Lexical Computing Ltd.", "labels": [], "entities": [{"text": "Lexical Computing Ltd", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.660509874423345}]}, {"text": "We can evaluate annotation speed gains of our approach comparatively against the single server version by utilising processing power in computer labs at Lancaster University and the United States Naval Academy (USNA) and we will call for volunteers from the corpus community to be involved in the evaluation as well.", "labels": [], "entities": []}, {"text": "A key aspect of our case study research will be to investigate extending corpus collection to new document types.", "labels": [], "entities": [{"text": "corpus collection", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7022075802087784}]}, {"text": "Most web-derived corpora have exploited raw text or HTML pages, so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE, Tidy and Parcels 3 (Baroni and).", "labels": [], "entities": [{"text": "boilerplate removal", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7573677599430084}]}, {"text": "Other document formats such as Adobe PDF and MS-Word have been neglected due to the extra conversion and clean-up problems they entail.", "labels": [], "entities": []}, {"text": "By excluding PDF documents, web-derived corpora are less representative of certain genres such as academic writing.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will evaluate the framework and prototype developed by applying it as a pre-processor step for the Sketch Engine system.", "labels": [], "entities": [{"text": "Sketch Engine", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.6739640235900879}]}, {"text": "The Sketch Engine requires a large well-balanced corpus which has been part-of-speech tagged and shallow parsed to find subjects, objects, heads, and modifiers.", "labels": [], "entities": []}, {"text": "We will use the existing non-distributed processing tools on the Sketch Engine as a baseline fora comparative evaluation of the AWAC framework instantiation by utilising processing power and bandwidth in learning labs at Lancaster University and USNA during off hours.", "labels": [], "entities": [{"text": "USNA", "start_pos": 246, "end_pos": 250, "type": "DATASET", "confidence": 0.8125764727592468}]}, {"text": "We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.", "labels": [], "entities": []}, {"text": "The initial solution will be to store the resulting reference http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine.", "labels": [], "entities": []}, {"text": "We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.", "labels": [], "entities": []}, {"text": "Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.", "labels": [], "entities": []}, {"text": "Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by Fletcher (2004a).", "labels": [], "entities": []}, {"text": "Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.", "labels": [], "entities": []}, {"text": "To improve the representative nature of webderived corpora, we will research techniques to enable the importing of additional document types such as PDF.", "labels": [], "entities": []}, {"text": "We will reuse and extend techniques implemented in the collection, encoding and annotation of the PERC Corpus of Professional English . A majority of this corpus has been collected by conversion of on-line academic journal articles from PDF to XML with a combination of semi-automatic tools and techniques (including Adobe Acrobat version 6).", "labels": [], "entities": [{"text": "PERC Corpus of Professional English", "start_pos": 98, "end_pos": 133, "type": "DATASET", "confidence": 0.947063946723938}]}, {"text": "Basic issues such as character encoding, table/figure extraction and maintaining text flow around embedded images need to be dealt with before annotation processing can begin.", "labels": [], "entities": [{"text": "character encoding", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8797231018543243}, {"text": "table/figure extraction", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7448378205299377}]}, {"text": "We will comparatively evaluate our techniques against others such as pdf2txt, and Multivalent PDF ExtractText . Part of the evaluation will be to collect and annotate a sample corpus.", "labels": [], "entities": []}, {"text": "We aim to collect a corpus from the web that is comparable to the BNC in content and annotation.", "labels": [], "entities": [{"text": "BNC", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9067752957344055}]}, {"text": "This corpus will be tagged using the P2P framework.", "labels": [], "entities": []}, {"text": "It will form a test-bed for the framework and we will utilise the non-distributed annotation system on the Sketch Engine as a baseline for comparison and evaluation.", "labels": [], "entities": []}, {"text": "To evaluate text conversion and clean-up routines for PDF documents, we will use a 5-million-word gold-standard sub-corpus extracted from the PERC Corpus of Professional English 14 .", "labels": [], "entities": [{"text": "text conversion", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7376241385936737}, {"text": "PERC Corpus of Professional English 14", "start_pos": 142, "end_pos": 180, "type": "DATASET", "confidence": 0.960532526175181}]}], "tableCaptions": []}