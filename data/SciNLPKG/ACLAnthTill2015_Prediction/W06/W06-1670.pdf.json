{"title": [{"text": "Broad-Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger *", "labels": [], "entities": [{"text": "Broad-Coverage Sense Disambiguation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6198928654193878}, {"text": "Information Extraction", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6976329386234283}]}], "abstractContent": [{"text": "In this paper we approach word sense disambiguation and information extraction as a unified tagging problem.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.7347211043039957}, {"text": "information extraction", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.8101766705513}]}, {"text": "The task consists of annotating text with the tagset defined by the 41 Wordnet super-sense classes for nouns and verbs.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9260902404785156}]}, {"text": "Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9311851263046265}, {"text": "word sense disambiguation", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.674028754234314}]}, {"text": "Furthermore, since the noun tags include the standard named entity detection classes-person, location, organization, time, etc.-the tagger, as a by-product, returns extended named entity information.", "labels": [], "entities": []}, {"text": "We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.743282675743103}]}, {"text": "Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known \"first-sense\" baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER) is the most studied information extraction (IE) task.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8161840935548147}, {"text": "information extraction (IE) task", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8680159052213033}]}, {"text": "NER typically focuses on detecting instances of \"person\", \"location\", \"organization\" names and optionally instances of \"miscellaneous\" or \"time\" categories.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9457680583000183}]}, {"text": "The scalability of statistical NER allowed researchers to apply it successfully on large collections of newswire text, in several languages, and biomedical literature.", "labels": [], "entities": [{"text": "statistical NER", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.5993026793003082}]}, {"text": "Newswire NER performance, in terms of F-score, is in the upper * The first author is now at Yahoo!", "labels": [], "entities": [{"text": "Newswire NER", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8043067455291748}, {"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9981130361557007}]}, {"text": "The tagger described in this paper is free software and can be downloaded from http://www.loa-cnr.it/ciaramita.html.", "labels": [], "entities": []}, {"text": "80s (, while Bio-NER accuracy ranges between the low 70s and 80s, depending on the data-set used for training/evaluation ().", "labels": [], "entities": [{"text": "Bio-NER", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9321885108947754}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8335722088813782}]}, {"text": "One shortcoming of NER is its over-simplified ontological model, leaving instances of other potentially informative categories unidentified.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9043854475021362}]}, {"text": "Hence, the utility of named entity information is limited.", "labels": [], "entities": []}, {"text": "In addition, instances to be detected are mainly restricted to (sequences of) proper nouns.", "labels": [], "entities": []}, {"text": "Word sense disambiguation (WSD) is the task of deciding the intended sense for ambiguous words in context.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8221349020799001}, {"text": "deciding the intended sense for ambiguous words in context", "start_pos": 47, "end_pos": 105, "type": "TASK", "confidence": 0.5084967646333907}]}, {"text": "With respect to NER, WSD lies at the other end of the semantic tagging spectrum, since the dictionary defines tens of thousand of very specific word senses, including NER categories.", "labels": [], "entities": [{"text": "WSD", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8449903726577759}]}, {"text": "Wordnet , possibly the most used resource for WSD, defines word senses for verbs, common and proper nouns.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9648815989494324}, {"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8961037397384644}]}, {"text": "Word sense disambiguation, at this level of granularity, is a complex task which resisted all attempts of robust broad-coverage solutions.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6189947028954824}]}, {"text": "Many distinctions are too subtle to be captured automatically, and the magnitude of the class space -several orders larger than NER's -makes it hard to approach the problem with sophisticated, but scalable, machine learning methods.", "labels": [], "entities": []}, {"text": "Lastly, even if the methods would scale up, there are not enough manually tagged data, at the word sense level, for training a model.", "labels": [], "entities": []}, {"text": "The performance of state of the art WSD systems on realistic evaluations is only comparable to the \"first sense\" baseline (cf. Section 5.3).", "labels": [], "entities": [{"text": "WSD", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9562137722969055}]}, {"text": "Notwithstanding much research, the benefits of disambiguated lexical information for language processing are still mostly speculative.", "labels": [], "entities": [{"text": "language processing", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7548024952411652}]}, {"text": "This paper presents a novel approach to broad- coverage information extraction and word sense disambiguation.", "labels": [], "entities": [{"text": "broad- coverage information extraction", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.7788142442703248}, {"text": "word sense disambiguation", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.7556605736414591}]}, {"text": "Our goal is to simplify the disambiguation task, for both nouns and verbs, to a level at which it can be approached as any other tagging problem, and can be solved with state of the art methods.", "labels": [], "entities": []}, {"text": "As a by-product, this task includes and extends NER.", "labels": [], "entities": []}, {"text": "We define a tagset based on Wordnet's lexicographers classes, or supersenses (Ciaramita and Johnson, 2003), cf..", "labels": [], "entities": []}, {"text": "The size of the supersense tagset allows us to adopt a structured learning approach, which takes local dependencies between labels into account.", "labels": [], "entities": []}, {"text": "To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model (HMM), based on that of, on the manually annotated Semcor corpus ().", "labels": [], "entities": [{"text": "supersense tagging problem", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7432025472323099}, {"text": "discriminative Hidden Markov Model (HMM)", "start_pos": 95, "end_pos": 135, "type": "METRIC", "confidence": 0.6615016630717686}, {"text": "Semcor corpus", "start_pos": 181, "end_pos": 194, "type": "DATASET", "confidence": 0.9152749180793762}]}, {"text": "In two experiments we evaluate the accuracy of the tagger on the Semcor corpus itself, and on the English \"all words\" Senseval 3 shared task data ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9994889497756958}, {"text": "Semcor corpus", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9455337226390839}, {"text": "Senseval 3 shared task data", "start_pos": 118, "end_pos": 145, "type": "DATASET", "confidence": 0.6675394117832184}]}, {"text": "The model outperforms remarkably the best known baseline, the first sense heuristic -to the best of our knowledge, for the first time on the most realistic \"all words\" evaluation setting.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the tagset, Section 3 discusses related work and Section 4 the learning model.", "labels": [], "entities": []}, {"text": "Section 5 reports on experimental settings and results.", "labels": [], "entities": []}, {"text": "In Section 6 we summarize our contribution and consider directions for further research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3. Statistics of the datasets. The row \"Super- senses\" lists the number of instances of supersense  labels, partitioned, in the following two rows, between  verb and noun supersense labels. The lowest four rows  summarize average polysemy figures at the synset and  supersense level for both nouns and verbs.", "labels": [], "entities": []}, {"text": " Table 4. Summary of results for random and first sense baselines and supersense tagger, \u03c3 is the standard error  computed on the five trials results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9139283895492554}]}, {"text": " Table 5. Summary of results of baseline and tagger on selected subsets of labels: NER categories evaluated on  Semcor (upper section), and 5 most frequent verb (middle) and noun (bottom) categories evaluated on Senseval.", "labels": [], "entities": []}]}