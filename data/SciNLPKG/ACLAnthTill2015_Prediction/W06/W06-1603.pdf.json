{"title": [{"text": "Paraphrase Recognition via Dissimilarity Significance Classification", "labels": [], "entities": [{"text": "Paraphrase Recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9037924110889435}]}], "abstractContent": [{"text": "We propose a supervised, two-phase framework to address the problem of paraphrase recognition (PR).", "labels": [], "entities": [{"text": "paraphrase recognition (PR)", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.885434627532959}]}, {"text": "Unlike most PR systems that focus on sentence similarity, our framework detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities.", "labels": [], "entities": []}, {"text": "The ability to differentiate significant dissimilarities not only reveals what makes two sentences a non-paraphrase, but also helps to recall additional paraphrases that contain extra but insignificant information.", "labels": [], "entities": []}, {"text": "Experimental results show that while being accurate at discerning non-paraphrasing dissimilar-ities, our implemented system is able to achieve higher paraphrase recall (93%), at an overall performance comparable to the alternatives.", "labels": [], "entities": [{"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9769908785820007}]}], "introductionContent": [{"text": "The task of sentence-level paraphrase recognition (PR) is to identify whether a set of sentences (typically, a pair) are semantically equivalent.", "labels": [], "entities": [{"text": "sentence-level paraphrase recognition (PR)", "start_pos": 12, "end_pos": 54, "type": "TASK", "confidence": 0.8264360080162684}]}, {"text": "In such a task, \"equivalence\" takes on a relaxed meaning, allowing sentence pairs with minor semantic differences to still be considered as paraphrases.", "labels": [], "entities": []}, {"text": "PR can bethought of as synonym detection extended for sentences, and it can play an equally important role in natural language applications.", "labels": [], "entities": [{"text": "PR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.892993152141571}, {"text": "synonym detection", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8057378232479095}]}, {"text": "As with synonym detection, applications such as summarization can benefit from the recognition and canonicalization of concepts and actions that are shared across multiple documents.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.9623503088951111}, {"text": "summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9883592128753662}]}, {"text": "Automatic construction of large paraphrase corpora could mine alternative ways to express the same concept, aiding machine translation and natural language generation applications.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7601202428340912}, {"text": "natural language generation", "start_pos": 139, "end_pos": 166, "type": "TASK", "confidence": 0.7158660491307577}]}, {"text": "In our work on sentence-level PR, we have identified two main issues through observation of sample sentences.", "labels": [], "entities": [{"text": "sentence-level PR", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.6460792273283005}]}, {"text": "The first is to identify all discrete information nuggets, or individual semantic content units, shared by the sentences.", "labels": [], "entities": []}, {"text": "For a pair of sentences to be deemed a paraphrase, they must share a substantial amount of these nuggets.", "labels": [], "entities": []}, {"text": "A trivial case is when both sentences are identical, word for word.", "labels": [], "entities": []}, {"text": "However, paraphrases often employ different words or syntactic structures to express the same concept.", "labels": [], "entities": []}, {"text": "shows two sentence pairs, in which the first pair is a paraphrase while the second is not.", "labels": [], "entities": []}, {"text": "The paraphrasing pair (also denoted", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our evaluation is to show that our system can reliably determine the cause(s) of non-  For evaluation, we conduct both component evaluations as well as a holistic one, resulting in three separate experiments.", "labels": [], "entities": []}, {"text": "In evaluating the first tuple pairing component, we aim for high precision, so that sentences that have all tuples paired can be safely assumed to be paraphrases.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9976375102996826}]}, {"text": "In evaluating the dissimilarity classifier, we simply aim for high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9978225231170654}]}, {"text": "In our overall system evaluation, we compare our system versus other PR systems on standard corpora.", "labels": [], "entities": []}, {"text": "For these experiments, we utilized two widely-used corpora for paraphrasing evaluation: the MSR and PASCAL RTE corpora.", "labels": [], "entities": [{"text": "PASCAL RTE corpora", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.6343475580215454}]}, {"text": "The Microsoft Research Paraphrase coupus () consists of 5801 newswire sentence pairs, 3900 of which are annotated as semantically equivalent by human annotators.", "labels": [], "entities": []}, {"text": "It reflects ordinary paraphrases that people often encounter in news articles, and maybe viewed as atypical domain-general paraphrase recognition task that downstream NLP systems will need to deal with.", "labels": [], "entities": [{"text": "domain-general paraphrase recognition", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.6888220906257629}]}, {"text": "The corpus comes divided into standard training (70%) and testing (30%) divisions, a partition we follow in our experiments.", "labels": [], "entities": []}, {"text": "ASSERT (the semantic role labeler) shows for this corpus a sentence contains 2.24 predicate argument tuples on average.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8133569359779358}]}, {"text": "The second corpus is the paraphrase acquisition subset of the PASCAL Recognizing Textual Entailment (RTE) Challenge corpus ().", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.8129730820655823}, {"text": "PASCAL Recognizing Textual Entailment (RTE) Challenge corpus", "start_pos": 62, "end_pos": 122, "type": "DATASET", "confidence": 0.6809830533133613}]}, {"text": "This is much smaller, consisting of 50 pairs, which we employ for testing only to show portability.", "labels": [], "entities": []}, {"text": "To assess the component performance, we need additional ground truth beyond the {+pp, \u2212pp} labels provided by the corpora.", "labels": [], "entities": []}, {"text": "For the first evaluation, we need to ascertain whether a sentence pair's tuples are correctly paired, misidentified or mispaired.", "labels": [], "entities": []}, {"text": "For the second, which tuple(s) (if any) are responsible fora \u2212pp instance.", "labels": [], "entities": []}, {"text": "However, creating ground truth by manual annotation is expensive, and thus we only sampled the data set to get an indicative assessment of performance.", "labels": [], "entities": []}, {"text": "We sampled 200 random instances from the total MSR testing set, and first processed them through our framework.", "labels": [], "entities": []}, {"text": "Then, five human annotators (two authors and three volunteers) annotated the ground truth for tuple pairing and the semantic significance of the unpaired tuples, while checking system output.", "labels": [], "entities": [{"text": "tuple pairing", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.7196979522705078}]}, {"text": "They also independently came up with their own {+pp,-pp} judgment so we could assess the reliability of the provided annotations.", "labels": [], "entities": [{"text": "reliability", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.9700620770454407}]}, {"text": "The results of this annotation is shown in Table 2.", "labels": [], "entities": []}, {"text": "Examining this data, we can see that the similarity detector performs well, despite its simplicity and assumption of a one-to-one mapping.", "labels": [], "entities": []}, {"text": "Out of the 157 predicate argument tuple pairs identified through similarity detection, annotators agreed that 144 (92%) are semantically similar or equivalent.", "labels": [], "entities": [{"text": "similarity detection", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.6843413859605789}]}, {"text": "However, 31 similar pairs were missed by the system, resulting in 82% recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9992017149925232}]}, {"text": "We defer discussion on the other details of this table to Section 7.", "labels": [], "entities": []}, {"text": "To assess the dissimilarity classifier, we focus on the \u2212pp subset of 55 instances recognized by the system.", "labels": [], "entities": []}, {"text": "For 43 unpaired tuples from 40 sentence pairs (73% of 55), the annotators' judgments agree with the classifier's claim that they are significant.", "labels": [], "entities": []}, {"text": "For these cases, the system is able to both recognize that the sentence pair is not a paraphrase and further correctly establish a cause of the nonparaphrase.", "labels": [], "entities": []}, {"text": "In addition to this ground truth sampled evaluation, we also show the effectiveness of the classifier by examining its performance on PS and NS tuples in the MSR corpus as described in Section 5.", "labels": [], "entities": [{"text": "MSR corpus", "start_pos": 158, "end_pos": 168, "type": "DATASET", "confidence": 0.7447136640548706}]}, {"text": "The test set consists of 413 randomly selected PS and NS instances among which 145 are significant (leading to non-paraphrases).", "labels": [], "entities": []}, {"text": "The classifier predicts predicate argument tuple significance at an accuracy of 71%, outperforms a majority classifier (65%), again which is marginally statistically significant (p < .09: Results on PASCAL PP test set score greater than zero), which is effectively a 98% recall of insignificant tuples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9969528913497925}, {"text": "PASCAL PP test set score", "start_pos": 199, "end_pos": 223, "type": "DATASET", "confidence": 0.7849563598632813}, {"text": "recall", "start_pos": 271, "end_pos": 277, "type": "METRIC", "confidence": 0.9388262629508972}]}, {"text": "However, the precision is less satisfatory.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996460676193237}]}, {"text": "We suspect this is partially due the tuples that fail to be paired up with their counterpart.", "labels": [], "entities": []}, {"text": "Such noise is found among the automatically collected PS instances used in training.", "labels": [], "entities": []}, {"text": "For the final system-wide evaluation, we implemented two baseline systems: a majority classifier and SimFinder (), a bag-of-words sentence similarity module incorporating lexical, syntactic and semantic features.", "labels": [], "entities": []}, {"text": "In, precision and recall are measured with respect to the paraphrasing class.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996210336685181}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9996839761734009}]}, {"text": "The table shows sentence pairs falling under the column \"pairs without unpaired tuples\" are more likely to be paraphrasing than an arbitrary pair (79.5% versus 66.5%), providing further validation for using predicate argument tuples as information nuggets.", "labels": [], "entities": []}, {"text": "The results for the experiment benchmarking the overall system performance are shown under the \"Overall\" column: our approach performs comparably to the baselines at both accuracy and paraphrase recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9993828535079956}, {"text": "recall", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9496605396270752}]}, {"text": "The system performance reported in (CM05;), which is among the best we are aware of, is also included for comparison.", "labels": [], "entities": [{"text": "CM05", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.9414606094360352}]}, {"text": "We also ran our system (trained on the MSR corpus) on the 50 instances in the PASCAL paraphrase acquisition subset.", "labels": [], "entities": [{"text": "MSR corpus", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7673641443252563}, {"text": "PASCAL paraphrase acquisition", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.6855698525905609}]}, {"text": "Again, the system performance (as shown in) is comparable to the baseline systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: (H)uman annotations vs. (C)orpus anno- tations and (S)ystem output", "labels": [], "entities": []}, {"text": " Table 3: Results on MSR test set", "labels": [], "entities": [{"text": "MSR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9273573756217957}]}, {"text": " Table 4: Results on PASCAL PP test set", "labels": [], "entities": [{"text": "PASCAL PP test", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.8181732495625814}]}]}