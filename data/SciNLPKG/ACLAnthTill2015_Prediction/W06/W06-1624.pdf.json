{"title": [{"text": "A Weakly Supervised Learning Approach for Spoken Language Understanding", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.9281199177106222}]}], "abstractContent": [{"text": "In this paper, we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.6464764177799225}]}, {"text": "We model the task of spoken language understanding as a successive classification problem.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7110527753829956}]}, {"text": "The first classifier (topic classifier) is used to identify the topic of an input utterance.", "labels": [], "entities": []}, {"text": "With the restriction of the recognized target topic, the second classifier (semantic classifier) is trained to extract the corresponding slot-value pairs.", "labels": [], "entities": []}, {"text": "It is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language.", "labels": [], "entities": []}, {"text": "Most importantly , it allows the employment of weakly supervised strategies for training the two classifiers.", "labels": [], "entities": []}, {"text": "We first apply the training strategy of combining active learning and self-training (Tur et al., 2005) for topic classifier.", "labels": [], "entities": []}, {"text": "Also, we propose a practical method for bootstrapping the topic-dependent semantic classifiers from a small amount of labeled sentences.", "labels": [], "entities": []}, {"text": "Experiments have been conducted in the context of Chinese public transportation information inquiry domain.", "labels": [], "entities": [{"text": "Chinese public transportation information inquiry domain", "start_pos": 50, "end_pos": 106, "type": "TASK", "confidence": 0.5812653203805288}]}, {"text": "The experimental results demonstrate the effectiveness of our proposed SLU framework and show the possibility to reduce human labeling efforts significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken Language Understanding (SLU) is one of the key components in spoken dialogue systems.", "labels": [], "entities": [{"text": "Spoken Language Understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8694466352462769}]}, {"text": "Its task is to identify the user's goal and extract from the input utterance the information needed to complete the query.", "labels": [], "entities": []}, {"text": "Traditionally, there are mainly two mainstreams in the SLU researches: knowledge-based approaches, which are based on robust parsing or template matching techniques; and data-driven approaches, which are generally based on stochastic models.", "labels": [], "entities": [{"text": "parsing or template matching", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.6880150660872459}]}, {"text": "Both approaches have their drawbacks, however.", "labels": [], "entities": []}, {"text": "The former approach is cost-expensive to develop since its grammar development is timeconsuming, laboursome and requires linguistic skills.", "labels": [], "entities": []}, {"text": "It is also strictly domain-dependent and hence difficult to be adapted to new domains.", "labels": [], "entities": []}, {"text": "On the other hand, although addressing such drawbacks associated with knowledge-based approaches, the latter approach often suffers the data sparseness problem and hence needs a fully annotated corpus in order to reliably estimate an accurate model.", "labels": [], "entities": []}, {"text": "More recently, some new variation methods are proposed through certain tradeoffs, such as the semi-automatically grammar learning approach () and Hidden Vector State (HVS) model).", "labels": [], "entities": []}, {"text": "The two methods require only minimally annotated data (only the semantic frames are annotated).", "labels": [], "entities": []}, {"text": "This paper proposes a novel weakly supervised spoken language understanding approach.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.6983628869056702}]}, {"text": "Our SLU framework mainly includes two successive classifiers: topic classifier and semantic classifier.", "labels": [], "entities": []}, {"text": "The main advantage of the proposed approach is that it is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language.", "labels": [], "entities": []}, {"text": "In particular, the two classifiers are trained using weakly supervised strategies: the former one is trained through the combination of active learning and self-training (, and the latter one is trained using a practical bootstrapping technique.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were carried out in the context of Chinese public transportation information inquiry domain.", "labels": [], "entities": [{"text": "Chinese public transportation information inquiry domain", "start_pos": 51, "end_pos": 107, "type": "TASK", "confidence": 0.49427470564842224}]}, {"text": "We collected two kinds of corpus for our domain using different ways.", "labels": [], "entities": []}, {"text": "Firstly, a natural language corpus was collected through a specific website which simulated a dialog system.", "labels": [], "entities": []}, {"text": "The user can conduct some mixed-initiative conversational dialogues with it by typing Chinese queries.", "labels": [], "entities": []}, {"text": "Then we collected 2,286 natural language utterances through this way.", "labels": [], "entities": []}, {"text": "It was divided into two parts: the training set contained 1,800 sentences (TR), and the test set contained 486 sentences (TS1).", "labels": [], "entities": []}, {"text": "Also, a spoken language corpus was collected through the deployment of a preliminary version of telephone-based dialog system, of which the speech recognizer is based on the speaker-independent Chinese dictation system of IBM ViaVoice Telephony and the SLU component is a robust rule-based parser.", "labels": [], "entities": []}, {"text": "The spoken utterances corpus contained 363 spoken utterances.", "labels": [], "entities": []}, {"text": "Then we obtained two test set from this corpus: one consisted of the recognized text (TS2); the other consisted of the corresponding transcription (TS3).", "labels": [], "entities": []}, {"text": "The Chinese character error rate and concept error rate of TS2 are 35.6% and 41.1% respectively.", "labels": [], "entities": [{"text": "Chinese character error rate", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.6450947970151901}, {"text": "concept error rate", "start_pos": 37, "end_pos": 55, "type": "METRIC", "confidence": 0.8906551798184713}, {"text": "TS2", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.4583060145378113}]}, {"text": "We defined ten types of topic for our domain: ListStop, ShowFare, ShowRoute, ShowRouteTime, etc.", "labels": [], "entities": [{"text": "ShowFare", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.896797776222229}, {"text": "ShowRoute", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.8636237382888794}, {"text": "ShowRouteTime", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.8743218183517456}]}, {"text": "The first corpus covers all the ten topic types and the second corpus only covers four topic types.", "labels": [], "entities": []}, {"text": "The total number of Chinese characters appear in the data set is 923.", "labels": [], "entities": []}, {"text": "All the sentences were annotated against the semantic frame.", "labels": [], "entities": []}, {"text": "In our experiments, the topic classifier and semantic classifiers were trained on the natural language training set (TR) and tested on three test sets (TS1, TS2 and TS3).", "labels": [], "entities": []}, {"text": "The performance of topic classification and semantic classification are measured in terms of topic error rate and slot error rate respectively.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8902549743652344}, {"text": "semantic classification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.8142293691635132}, {"text": "topic error rate", "start_pos": 93, "end_pos": 109, "type": "METRIC", "confidence": 0.7949125369389852}, {"text": "slot error rate", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.9649626215298971}]}, {"text": "Topic performance is measured by comparing the topic of a sentence predicated by the topic classifier with the reference topic.", "labels": [], "entities": []}, {"text": "The slot error rate is measured by counting the insertion, deletion and substitution errors between the slots generated by our system and these in the reference annotation.", "labels": [], "entities": [{"text": "slot error rate", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7935717900594076}]}, {"text": "Firstly, in order to validate the effectiveness of our proposed SLU system using successive learners, we compared our system with a rulebased robust semantic parser.", "labels": [], "entities": []}, {"text": "The parsing algorithm of this parser is same as the local chart parser used by the preprocessor.", "labels": [], "entities": []}, {"text": "The handcrafted grammar for this semantic parser took a linguistic expert one month to develop, which consists of 798 rules (except the lexical rules for named entities such as).", "labels": [], "entities": []}, {"text": "In our SLU system, we first use the SVMs to identify the topic and then apply the semantic classifier (decision list) related to the identified topic to assign the slots to the concepts.", "labels": [], "entities": []}, {"text": "The SVMs used the augmented binary features (923 Chinese characters and 20 semantic class labels).", "labels": [], "entities": []}, {"text": "A general developer independently annotated the TR set against the semantic frame, which took only four days.", "labels": [], "entities": []}, {"text": "Through feature extraction from the TR set and feature pruning, we obtained 2,259 literal context features and 369 slot context features for 20 kinds of concepts in our domain.", "labels": [], "entities": []}, {"text": "Shows that our SLU method has better performance than the rule-based robust parser in both topic classification and slot identification.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.8055383265018463}, {"text": "slot identification", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8683569431304932}]}, {"text": "Due to the high concept error rate of recognized utterances, the performance of semantic classification on the TS2 is relatively poor.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.8178345561027527}]}, {"text": "However, if considering only the correctly identified concepts on TS2, the slot error rate is 9.2%.", "labels": [], "entities": [{"text": "TS2", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.8247994780540466}, {"text": "slot error rate", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.9117582440376282}]}, {"text": "Note that, since the TS2 (recognized speech) covers only four types of topic but TS1 (typed utterance) covers ten topics, the topic error on the TS2 (recognized speech) is lower than that on TS1.", "labels": [], "entities": []}, {"text": "also compares our system with the two-stage classification with the reversed order.", "labels": [], "entities": []}, {"text": "Another alternative for our system is to reverse the two main processing stages, i.e., finding the roles for the concepts prior to identifying the topic.", "labels": [], "entities": []}, {"text": "For instance, in the example sentence in, the concept (e.g.,) in the preprocessed sequence is first recognized as slots (e.g.,.) before topic classification.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.783691793680191}]}, {"text": "Therefore, the slots like.", "labels": [], "entities": []}, {"text": "[origin] can be included as features for topic classification, which is deeper than the concepts like and potential to achieve improvement on performance of topic classification.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8874892890453339}, {"text": "topic classification", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.7256486564874649}]}, {"text": "This strategy was adopted in some previous works.", "labels": [], "entities": []}, {"text": "However, the results indicate that, at least in our two-stage classification formwork, the strategy of identifying the topic before assigning the slots to the concepts is more optimal.", "labels": [], "entities": []}, {"text": "According to our error analysis, the unsatisfied performance of the reversed two-stage classification system can be explained as follows: Since the semantic classification is performed on all topics, the search space is much bigger and the ambiguities increase.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.7373052537441254}]}, {"text": "This deteriorates the performance of semantic classification.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8741752803325653}]}, {"text": "In the case that the slots and Chinese characters are included as features, the topic classifier relies heavily on the slot features.", "labels": [], "entities": []}, {"text": "Then, the errors of semantic classification have serious negative effect on the topic classification.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7773086428642273}, {"text": "topic classification", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7264070212841034}]}, {"text": "In order to evaluate the performance of active learning and self-training, we compared three sampling strategies: random sampling, active learning only, active learning and self-training.", "labels": [], "entities": []}, {"text": "At each iteration of pool-based active learning and self-training, we get 200 sentences (i.e., the pool size is set as 200) and select 50 most unconfident sentences from them for manually labeling and exploit the remaining sentences using selftraining.", "labels": [], "entities": []}, {"text": "All the experiments were repeated ten times with different randomly selected seed sentences and the results were averaged.", "labels": [], "entities": []}, {"text": "plots the learning curves of three strategies trained on TR and tested on the TS1 set.", "labels": [], "entities": [{"text": "TS1 set", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.8243155479431152}]}, {"text": "It is evident that active learning significantly reduces the need for labeled data.", "labels": [], "entities": []}, {"text": "For instance, it requires 1600 examples if they are randomly chosen to achieve a topic error rate of 3.2% on TS1, but only 600 actively selected examples, a saving of 62.5%.", "labels": [], "entities": [{"text": "topic error rate", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.623644103606542}, {"text": "TS1", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8926876187324524}]}, {"text": "The strategy of combing active learning and self-training can further improve the performance of topic classification compared with active learning only with the same amount of labeled data.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7350732684135437}]}, {"text": "We also evaluated the performance of topic classification using active learning and selftraining with the pool size of 200 on the three test sets.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8182029128074646}]}, {"text": "shows that active learning and self-training with the pool size of 200 achieves almost the same performance on three test sets as random sampling, but requires only 33.3% data.", "labels": [], "entities": []}, {"text": "As stated before, the bootstrapping procedure begins with a small amount of sentences annotated against the semantic frame, which is the initial seed sentence or annotated by active learning, and the remaining training sentences, the topics of which are machine-labeled by the resulting topic classifier.", "labels": [], "entities": []}, {"text": "For example, in the weakly supervised training scenario with the pool size of 200, the active learning and selftraining procedure ran 8 iterations.", "labels": [], "entities": []}, {"text": "At each iteration, 50 sentences were selected by active learning.", "labels": [], "entities": []}, {"text": "So the total number of labeled sentences is 600.", "labels": [], "entities": []}, {"text": "We compared our bootstrapping methods with supervised training for semantic classification.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8737587928771973}]}, {"text": "We tried two bootstrapping methods: using only the literal context features (Bootstrapping 1) and using the literal and slot context features (Bootstrapping 2).", "labels": [], "entities": []}, {"text": "If the step 4 of the bootstrapping algorithm in Section 3.2 is canceled, the new bootstrapping variation corresponds to Bootstrapping 2.", "labels": [], "entities": []}, {"text": "Also, we repeated the experiments ten times with different labeled sentences and the results were averaged.", "labels": [], "entities": []}, {"text": "plots the learning curves of bootstrapping and supervised training with different number of labeled sentences on the TS1 set.", "labels": [], "entities": [{"text": "TS1 set", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.8832351863384247}]}, {"text": "The results indicate that bootstrapping methods can effectively make use of the unlabeled data to improve the semantic classification performance.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7909868359565735}]}, {"text": "In particular, the learning curve of bootstrapping 1 achieves more significant improvement than the curve of bootstrapping 2.", "labels": [], "entities": []}, {"text": "It can be explained as follows: including the slot context features further increases the redundancy of data and hence corrects the initial misclassified cases by the semantic classifier using only literal context features or provides new cases.", "labels": [], "entities": []}, {"text": "Figure 3: Learning curves of bootstrapping methods for semantic classification on TS1.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.8016980588436127}, {"text": "TS1", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.7811219692230225}]}, {"text": "Finally, we compared two SLU systems through weakly supervised and supervised training respectively.", "labels": [], "entities": []}, {"text": "The supervised one was trained using all the annotated sentences in TR (1800 sentences).", "labels": [], "entities": [{"text": "TR", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.47729578614234924}]}, {"text": "In the weakly supervised training scenario (the pool size is still 200), The topic classifier and semantic classifiers were both trained using only 600 labeled sentences.", "labels": [], "entities": []}, {"text": "shows that the weakly supervised scenario achieves comparable performance to the supervised one, but requires only 33.3% labeled data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparsion of the rule- based robust semantic parser, the reversed two- stage classification system and our SLU systems  (TER: Topic Error Rate; SER: Slot Error Rate;  DL: Decision List)", "labels": [], "entities": [{"text": "TER: Topic Error Rate; SER: Slot Error Rate", "start_pos": 144, "end_pos": 187, "type": "METRIC", "confidence": 0.7840226021679965}]}, {"text": " Table 2: The topic error rate using active learn- ing and self-training with pool size of 200 on the  three test sets (AL: Active Learning)", "labels": [], "entities": []}, {"text": " Table 3: Performance comparison of two SLU  systems through weakly supervised and super- vised training on the three test sets (TER: Topic  Error Rate; SER: Slot Error Rate)", "labels": [], "entities": [{"text": "TER: Topic  Error Rate", "start_pos": 129, "end_pos": 151, "type": "METRIC", "confidence": 0.7539644300937652}, {"text": "SER: Slot Error Rate", "start_pos": 153, "end_pos": 173, "type": "METRIC", "confidence": 0.7631795823574066}]}]}