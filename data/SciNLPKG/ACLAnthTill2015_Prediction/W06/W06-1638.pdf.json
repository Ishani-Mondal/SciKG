{"title": [{"text": "Better Informed Training of Latent Syntactic Features", "labels": [], "entities": []}], "abstractContent": [{"text": "We study unsupervised methods for learning refinements of the nonterminals in a treebank.", "labels": [], "entities": []}, {"text": "(2005) and Prescher (2005), we may for example split NP without supervision into NP[0] and NP[1], which behave differently.", "labels": [], "entities": []}, {"text": "We first propose to learn a PCFG that adds such features to nonterminals in such away that they respect patterns of linguistic feature passing: each node's nontermi-nal features are either identical to, or independent of, those of its parent.", "labels": [], "entities": [{"text": "linguistic feature passing", "start_pos": 116, "end_pos": 142, "type": "TASK", "confidence": 0.7182648579279581}]}, {"text": "This linguistic constraint reduces runtime and the number of parameters to be learned.", "labels": [], "entities": []}, {"text": "However , it did not yield improvements when training on the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9936505258083344}]}, {"text": "An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by anneal-ing methods that split nonterminals selectively.", "labels": [], "entities": []}, {"text": "Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9632804989814758}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9108094573020935}]}], "introductionContent": [{"text": "Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank ( work only moderately well.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9960937798023224}]}, {"text": "To address this problem, researchers have used heuristics to add more information.,,, and many subsequent researchers 1 annotated every node with lexical features passed up from its \"head child,\" in order to more precisely reflect the node's \"inside\" contents. and annotated each node with its parent and grandparent nonterminals, to more precisely reflect its \"outside\" context.", "labels": [], "entities": []}, {"text": "split the sentence label S into two versions, representing sentences with and without subjects.", "labels": [], "entities": []}, {"text": "He Not to mention earlier non-PCFG lexicalized statistical parsers, notably for the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.993522971868515}]}, {"text": "also modified the treebank to contain different labels for standard and for base noun phrases.", "labels": [], "entities": []}, {"text": "identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules.", "labels": [], "entities": []}, {"text": "Their unlexicalized parser combined several such heuristics with rule markovization and reached a performance similar to early lexicalized parsers.", "labels": [], "entities": []}, {"text": "In all these cases, choosing which nonterminals to split, and how, was a matter of art.", "labels": [], "entities": []}, {"text": "Ideally such splits would be learned automatically from the given treebank itself.", "labels": [], "entities": []}, {"text": "This would be less costly and more portable to treebanks for new domains and languages.", "labels": [], "entities": []}, {"text": "One might also hope that the automatically learned splits would be more effective.", "labels": [], "entities": []}, {"text": "introduced a model for such learning: PCFG-LA.", "labels": [], "entities": [{"text": "PCFG-LA", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9217120409011841}]}, {"text": "They used EM to induce fine-grained versions of a given treebank's nonterminals and rules.", "labels": [], "entities": []}, {"text": "We present models that similarly learn to propagate fine-grained features through the tree, but only in certain linguistically motivated ways.", "labels": [], "entities": []}, {"text": "Our models therefore allocate a supply of free parameters differently, allowing more fine-grained nonterminals but less finegrained control over the probabilities of rewriting them.", "labels": [], "entities": []}, {"text": "We also present simple methods for deciding selectively (during training) which nonterminals to split and how.", "labels": [], "entities": []}, {"text": "Section 2 describes previous work in finding hidden information in treebanks.", "labels": [], "entities": []}, {"text": "Section 3 describes automatically induced feature grammars.", "labels": [], "entities": []}, {"text": "We start by describing the PCFG-LA model, then introduce new models that use specific agreement patterns to propagate features through the tree.", "labels": [], "entities": []}, {"text": "Section 4 describes annealing-like procedures for training latent-annotation models.", "labels": [], "entities": []}, {"text": "Section 5 describes the motivation and results of our experiments.", "labels": [], "entities": []}, {"text": "We finish by discussing future work and conclusions in sections 6-7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Results on the development set: labeled precision (LP), labeled recall (LR), and their harmonic  mean (F 1 ). \"Basic\" models are trained on a non-markovized treebank (as in", "labels": [], "entities": [{"text": "precision (LP)", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9032514989376068}, {"text": "recall (LR)", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9571875929832458}, {"text": "harmonic  mean (F 1 )", "start_pos": 97, "end_pos": 118, "type": "METRIC", "confidence": 0.8451551000277201}]}]}