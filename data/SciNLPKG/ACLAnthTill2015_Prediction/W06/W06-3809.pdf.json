{"title": [{"text": "Random-Walk Term Weighting for Improved Text Classification", "labels": [], "entities": [{"text": "Random-Walk Term Weighting", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5816934108734131}, {"text": "Improved Text Classification", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6477399865786234}]}], "abstractContent": [{"text": "This paper describes anew approach for estimating term weights in a text classification task.", "labels": [], "entities": [{"text": "estimating term weights", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8214752078056335}, {"text": "text classification task", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.7997657855351766}]}, {"text": "The approach uses term co-occurrence as a measure of dependency between word features.", "labels": [], "entities": []}, {"text": "A random walk model is applied on a graph encoding words and co-occurrence dependencies, resulting in scores that represent a quan-tification of how a particular word feature contributes to a given context.", "labels": [], "entities": []}, {"text": "We argue that by modeling feature weights using these scores, as opposed to the traditional frequency-based scores, we can achieve better results in a text classification task.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.8608150482177734}]}, {"text": "Experiments performed on four standard classification datasets show that the new random-walk based approach outperforms the traditional term frequency approach to feature weighting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Term frequency has long been adapted as a measure of term significance in a specific context).", "labels": [], "entities": []}, {"text": "The logic behind it is that the more a certain term is encountered in a certain context, the more it carries or contributes to the meaning of the context.", "labels": [], "entities": []}, {"text": "Due to this belief, term frequency has been a major factor in estimating the probabilistic distribution of features using maximum likelihood estimates and hence has been incorporated in abroad spectrum of tasks ranging from feature selection techniques) to language models (.", "labels": [], "entities": []}, {"text": "In this paper we introduce anew measure of term weighting, which integrates the locality of a term and its relation to the surrounding context.", "labels": [], "entities": []}, {"text": "We model this local contribution using a co-occurrence relation in which terms that co-occur in a certain context are likely to share between them some of their importance (or significance).", "labels": [], "entities": []}, {"text": "Note that in this model the relation between a given term and its context is not linear, since the context itself consists of a collection of other terms, which in turn have a dependency relation with their own context, which might include the original given term.", "labels": [], "entities": []}, {"text": "In order to model this recursive relation we use a graph-based ranking algorithm, namely the PageRank randomwalk algorithms, and its TextRank adaption to text processing applications.", "labels": [], "entities": []}, {"text": "TextRank takes as input a set of textual entities and relations between them, and uses a graph-based ranking algorithm (also known as random walk algorithm) to produce a set of scores that represent the accumulated weight or rank for each textual entity in their context.", "labels": [], "entities": []}, {"text": "The TextRank model was so far evaluated on three natural language processing tasks: document summarization, word sense disambiguation, and keyword extraction, and despite being fully unsupervised, it has been shown to be competitive with other sometime supervised state-of-the-art algorithms.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.6811599433422089}, {"text": "word sense disambiguation", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.7105352481206259}, {"text": "keyword extraction", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7487181127071381}]}, {"text": "In this paper, we show how TextRank can be used to model the probabilistic distribution of word features in a document, by making further use of the scores produced by the random-walk model.", "labels": [], "entities": []}, {"text": "Through experiments performed on a text classification task, we show that these random walk scores outperform the traditional term frequencies typically used to model the feature weights for this task.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8631553848584493}]}], "datasetContent": [{"text": "To evaluate our random-walk based approach to feature weighting, we integrate it in a text classification algorithm, and evaluate its performance on several standard text classification data sets.", "labels": [], "entities": [{"text": "feature weighting", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.745937705039978}, {"text": "text classification", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.705432191491127}]}], "tableCaptions": [{"text": " Table 1: tf & rw scores", "labels": [], "entities": []}, {"text": " Table 3: Naive Bayes Results 5  N.B.  tf  rw 2 rw 4  rw 6  rw 8  W ebKB 4 81.9 81.9 82.8  82.7  81.2  W ebKB 6 71.7 73.0 74.2 \u2020 74.4 \u2020 73.5  Reuter  83.2 82.5 82.9  83.0  82.8  20N G  81.7 82.0 82.3 \u2021 82.3 \u2021 82.1 \u2020  LSpam  99.3 99.4 99.3  99.3  99.3", "labels": [], "entities": [{"text": "N.B.  tf  rw", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.8533638517061869}, {"text": "Reuter  83.2 82.5 82.9  83.0  82.8  20N G  81.7 82.0 82.3", "start_pos": 142, "end_pos": 199, "type": "DATASET", "confidence": 0.8256775845180858}, {"text": "LSpam  99.3 99.4 99.3  99.3  99.3", "start_pos": 217, "end_pos": 250, "type": "DATASET", "confidence": 0.9276105165481567}]}, {"text": " Table 4: Rocchio Results  ROC  tf  rw 2  rw 4  rw 6  rw 8  W ebKB 4 71.9 77.5 \u2021 78.6 \u2021 80.8 \u2021 80.9 \u2021  W ebKB 6 58.3 69.6 \u2021 72.0 \u2021 76.5 \u2021 76.2 \u2021  Reuter  78.2 80.8 \u2021 81.1 \u2021 81.0 \u2021 81.4 \u2021  20N G  76.2 77.3 \u2021 77.1 \u2021 77.2 \u2021 77.4 \u2021  LSpam  97.5 97.8  97.8  97.7  97.8", "labels": [], "entities": [{"text": "Rocchio Results  ROC  tf  rw", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.8548215150833129}, {"text": "LSpam  97.5 97.8  97.8  97.7  97.8", "start_pos": 229, "end_pos": 263, "type": "DATASET", "confidence": 0.8978269596894582}]}, {"text": " Table 5: KNN Results  KN N  tf  rw 2  rw 4  rw 6  rw 8  W ebKB 4 59.2 68.6 \u2021 67.0 \u2021 64.6 \u2021 66.6 \u2021  W ebKB 6 55.8 63.7 \u2021 55.8  59.9 \u2020 61.0 \u2021  Reuter  73.6 76.9 \u2021 78.1 \u2021 78.5 \u2021 78.5 \u2021  20N G  70.3 76.1 \u2021 76.5 \u2021 77.2 \u2021 77.8 \u2021  LSpam  97.5 97.8  97.8  98.1 \u2020 97.9", "labels": [], "entities": [{"text": "KNN Results  KN N  tf  rw", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.8596966564655304}, {"text": "LSpam  97.5 97.8  97.8  98.1 \u2020 97.9", "start_pos": 225, "end_pos": 260, "type": "DATASET", "confidence": 0.8957850677626473}]}, {"text": " Table 6: SVM Results  SV M  tf  rw 2  rw 4  rw 6  rw 8  W ebKB 4 87.7 87.9  87.9  89 \u2020  88.5  W ebKB 6 82.5 84.5 \u2021 85.2 \u2021 85.2 \u2021 84.6 \u2021  Reuter  83.2 84.5 \u2021 84.4 \u2021 84.6 \u2021 84.1 \u2020  20N G  95.2 95.5 \u2021 95.6 \u2021 95.6 \u2021 95.4 \u2020  LSpam  95.6 96.4 \u2021 96.4 \u2021 96.2 \u2021 96.3 \u2021", "labels": [], "entities": [{"text": "LSpam  95.6 96.4", "start_pos": 221, "end_pos": 237, "type": "DATASET", "confidence": 0.9178125262260437}]}, {"text": " Table 7: Naive Bayes Correlation \u03c1  N.B.  rw 2 rw 4 rw 6 rw 8  W ebKB 4 0.68 0.70 0.70 0.66  W ebKB 6 0.71 0.71 0.71 0.65  Reuter  0.86 0.87 0.87 0.85  20N G  0.82 0.84 0.83 0.82  LSpam  0.89 0.89 0.92 0.92", "labels": [], "entities": [{"text": "Reuter  0.86 0.87 0.87 0.85  20N G  0.82 0.84 0.83 0.82  LSpam  0.89 0.89 0.92 0.92", "start_pos": 124, "end_pos": 207, "type": "DATASET", "confidence": 0.7806855887174606}]}, {"text": " Table 8: Rocchio Correlation \u03c1  ROC  rw 2 rw 4 rw 6 rw 8  W ebKB 4 0.49 0.51 0.53 0.54  W ebKB 6 0.40 0.40 0.41 0.42  Reuter  0.75 0.77 0.75 0.71  20N G  0.77 0.77 0.77 0.77  LSpam  0.82 0.85 0.81 0.78", "labels": [], "entities": [{"text": "Reuter  0.75 0.77 0.75 0.71  20N G  0.77 0.77 0.77 0.77  LSpam  0.82 0.85 0.81 0.78", "start_pos": 119, "end_pos": 202, "type": "DATASET", "confidence": 0.781702559441328}]}, {"text": " Table 9: KNN Correlation \u03c1  KN N  rw 2 rw 4 rw 6 rw 8  W ebKB 4 0.35 0.32 0.36 0.37  W ebKB 6 0.35 0.35 0.37 0.37  Reuter  0.74 0.70 0.68 0.67  20N G  0.62 0.64 0.63 0.59  LSpam  0.66 0.69 0.63 0.57", "labels": [], "entities": [{"text": "Reuter  0.74 0.70 0.68 0.67  20N G  0.62 0.64 0.63 0.59  LSpam  0.66 0.69 0.63 0.57", "start_pos": 116, "end_pos": 199, "type": "DATASET", "confidence": 0.7990049682557583}]}]}