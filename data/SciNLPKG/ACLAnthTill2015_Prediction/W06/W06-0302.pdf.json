{"title": [{"text": "Toward Opinion Summarization: Linking the Sources", "labels": [], "entities": [{"text": "Opinion Summarization", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.7664775550365448}]}], "abstractContent": [{"text": "We target the problem of linking source mentions that belong to the same entity (source coreference resolution), which is needed for creating opinion summaries.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.6404142578442892}]}, {"text": "In this paper we describe how source coref-erence resolution can be transformed into standard noun phrase coreference resolution , apply a state-of-the-art coreference resolution approach to the transformed data, and evaluate on an available corpus of manually annotated opinions.", "labels": [], "entities": [{"text": "source coref-erence resolution", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.6377912362416586}, {"text": "coreference resolution", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7518559396266937}, {"text": "coreference resolution", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.804830938577652}]}], "introductionContent": [{"text": "Sentiment analysis is concerned with the extraction and representation of attitudes, evaluations, opinions, and sentiment from text.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9385606050491333}, {"text": "extraction and representation of attitudes, evaluations, opinions, and sentiment from text", "start_pos": 41, "end_pos": 131, "type": "TASK", "confidence": 0.7568042406014034}]}, {"text": "The area of sentiment analysis has been the subject of much recent research interest driven by two primary motivations.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9592854380607605}]}, {"text": "First, there is a desire to provide applications that can extract, represent, and allow the exploration of opinions in the commercial, government, and political domains.", "labels": [], "entities": []}, {"text": "Second, effective sentiment analysis might be used to enhance and improve existing NLP applications such as information extraction, question answering, summarization, and clustering (e.g. ,).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.938544511795044}, {"text": "information extraction", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.8282949328422546}, {"text": "question answering", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.9235168099403381}, {"text": "summarization", "start_pos": 152, "end_pos": 165, "type": "TASK", "confidence": 0.9670578837394714}]}, {"text": "Several research efforts (e.g.,,,, ) have shown that sentiment information can be extracted at the sentence, clause, or individual opinion expression level (fine-grained opinion information).", "labels": [], "entities": []}, {"text": "However, little has been done to develop methods for combining fine-grained opinion information to form a summary representation in which expressions of opinions from the same source/target 1 are grouped together, multiple opinions from a source toward the same target are accumulated into an aggregated opinion, and cumulative statistics are computed for each source/target.", "labels": [], "entities": []}, {"text": "A simple opinion summary 2 is shown in.", "labels": [], "entities": []}, {"text": "Being able to create opinion summaries is important both for stand-alone applications of sentiment analysis as well as for the potential uses of sentiment analysis as part of other NLP applications.", "labels": [], "entities": [{"text": "opinion summaries", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7027924656867981}, {"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.9456815421581268}, {"text": "sentiment analysis", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.9136745929718018}]}, {"text": "In this work we address the dearth of approaches for summarizing opinion information.", "labels": [], "entities": [{"text": "summarizing opinion information", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.9284022450447083}]}, {"text": "In particular, we focus on the problem of source coreference resolution, i.e. deciding which source mentions are associated with opinions that belong to the same real-world entity.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7568080623944601}]}, {"text": "In the example from performing source coreference resolution amounts to determining that Stanishev, he, and he refer to the same real-world entities.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6518117090066274}]}, {"text": "Given the associated opinion expressions and their polarity, this source coreference information is the critical knowledge needed to produce the summary of Figure 1 (although the two target mentions, Bulgaria and our country, would also need to be identified as coreferent).", "labels": [], "entities": []}, {"text": "Our work is concerned with fine-grained expressions of opinions and assumes that a system can rely on the results of effective opinion and source extractors such as those described in,, Wiebe and  and.", "labels": [], "entities": []}, {"text": "Presented with sources of opinions, we approach the problem of source coreference resolution as the closely: Example of text containing opinions (above) and a summary of the opinions (below).", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.7552379568417867}]}, {"text": "In the text, sources and targets of opinions are marked and opinion expressions are shown in italic.", "labels": [], "entities": []}, {"text": "In the summary graph, + stands for positive opinion and -for negative.", "labels": [], "entities": []}, {"text": "related task of noun phrase coreference resolution.", "labels": [], "entities": [{"text": "noun phrase coreference resolution", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.814534068107605}]}, {"text": "However, source coreference resolution differs from traditional noun phrase (NP) coreference resolution in two important aspects discussed in Section 4.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.7337808807690939}, {"text": "noun phrase (NP) coreference resolution", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.6923602819442749}]}, {"text": "Nevertheless, as a first attempt at source coreference resolution, we employ a state-of-theart machine learning approach to NP coreference resolution developed by.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.6794601082801819}, {"text": "NP coreference resolution", "start_pos": 124, "end_pos": 149, "type": "TASK", "confidence": 0.790795087814331}]}, {"text": "Using a corpus of manually annotated opinions, we perform an extensive evaluation and obtain strong initial results for the task of source coreference resolution.", "labels": [], "entities": [{"text": "source coreference resolution", "start_pos": 132, "end_pos": 161, "type": "TASK", "confidence": 0.7162355383237203}]}], "datasetContent": [{"text": "For evaluation we randomly split the MPQA corpus into a training set consisting of 400 documents and a test set consisting of the remaining 135 documents.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9270369410514832}]}, {"text": "We use the same test set for all evaluations, although not all runs were trained on all 400 training documents as discussed below.", "labels": [], "entities": []}, {"text": "The purpose of our evaluation is to create a strong baseline utilizing the best settings for the NP coreference approach.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7568916976451874}]}, {"text": "As such, we try the two reportedly best machine learning techniques for pairwise classification -RIPPER (for Repeated Incremental Pruning to Produce Error Reduction) and support vector machines (SVMs) in the SV M light implementation.", "labels": [], "entities": [{"text": "pairwise classification", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.6178448647260666}, {"text": "RIPPER", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9931939840316772}, {"text": "Repeated Incremental Pruning to Produce Error Reduction)", "start_pos": 109, "end_pos": 165, "type": "TASK", "confidence": 0.6479695588350296}]}, {"text": "Additionally, to exclude possible effects of parameter selection, we try many different parameter settings for the two classifiers.", "labels": [], "entities": []}, {"text": "For RIPPER we vary the order of classes and the positive/negative weight ratio.", "labels": [], "entities": [{"text": "RIPPER", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.5635469555854797}]}, {"text": "For SVMs we vary C (the margin tradeoff) and the type and parameter of the kernel.", "labels": [], "entities": []}, {"text": "In total, we use 24 different settings for RIPPER and 56 for SV M light . Additionally, Ng and Cardie reported better results when the training data distribution is balanced through instance selection.", "labels": [], "entities": [{"text": "RIPPER", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.916833221912384}]}, {"text": "For instance selection they adopt the method of, which selects for each NP the pairs with then preceding coreferent instances and all intervening non-coreferent pairs.", "labels": [], "entities": []}, {"text": "Following, we perform instance selection with n = 1 (soon1 in the results) and n = 2 (soon2).", "labels": [], "entities": [{"text": "instance selection", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7336184084415436}]}, {"text": "With the three different instance selection algorithms (soon1, soon2, and none), the total number of settings is 72 for RIPPER and 168 for SVMa.", "labels": [], "entities": []}, {"text": "However, not all SVM runs completed in the time limit that we set -200 min, so we selected half of the training set (200 documents) at random and trained all classifiers on that set.", "labels": [], "entities": []}, {"text": "We made sure to run to completion on the full training set those SVM settings that produced the best results on the smaller training set.", "labels": [], "entities": []}, {"text": "lists the results of the best performing runs.", "labels": [], "entities": []}, {"text": "The upper half of the table gives the results for the runs that were trained on 400 documents and the lower half contains the results for the 200-document training set.", "labels": [], "entities": []}, {"text": "We evaluated using the two widely used performance measures for coreference resolution -MUC score ( and B 3 (Bagga and).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9687308073043823}, {"text": "MUC score", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.7144189774990082}]}, {"text": "In addition, we used performance metrics (precision, recall and F1) on the identification of the positive class.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9994826316833496}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9984256029129028}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.999333918094635}]}, {"text": "We compute the latter in two different ways -either by using the pairwise decisions as the classifiers outputs them or by performing the clustering of the source NPs and then considering a pairwise decision to be positive if the two source NPs belong to the same cluster.", "labels": [], "entities": []}, {"text": "The second option (marked actual in) should be more representative of a good clustering, since coreference decisions are important only in the context of the clusters that they create.", "labels": [], "entities": []}, {"text": "shows the performance of the best RIP-PER and SVM runs for each of the four evaluation metrics.", "labels": [], "entities": [{"text": "RIP-PER", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.5908463597297668}]}, {"text": "The table also lists the rank for each run among the rest of the runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for matching sources to noun  phrases.", "labels": [], "entities": []}, {"text": " Table 2: Performance of the best runs. For SVMs, \u03b3 stands for RBF kernel with the shown \u03b3 parameter.", "labels": [], "entities": []}]}