{"title": [{"text": "Chinese Word Segmentation using Various Dictionaries", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5113109548886617}]}], "abstractContent": [{"text": "Most of the Chinese word segmentation systems utilizes monolingual dictionary and are used for monolingual processing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.709084709485372}]}, {"text": "For the tasks of machine translation (MT) and cross-language information retrieval (CLIR), another translation dictionary maybe used to transfer the words of documents from the source languages to target languages.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8476089656352996}, {"text": "cross-language information retrieval (CLIR)", "start_pos": 46, "end_pos": 89, "type": "TASK", "confidence": 0.7729160686333975}]}, {"text": "The inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary) may produce some problems for MT and CLIR.", "labels": [], "entities": [{"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.9302545189857483}]}, {"text": "This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations.", "labels": [], "entities": [{"text": "Chinese word segmentations", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.5984185338020325}]}], "introductionContent": [{"text": "Most of the Chinese word segmentations are used for monolingual processing.", "labels": [], "entities": [{"text": "Chinese word segmentations", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7336889306704203}]}, {"text": "In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information) in a monolingual dictionary, segmentation rules, and some statistical information.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.741408571600914}]}, {"text": "For the tasks of machine translation (MT)) and cross-language information retrieval (CLIR)), another translation dictionary maybe used to transfer the words of documents from the source languages to target languages.", "labels": [], "entities": [{"text": "machine translation (MT))", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.8334698677062988}, {"text": "cross-language information retrieval (CLIR))", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.7334857930739721}]}, {"text": "Because of the inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary), this approach has the problems that some segmented words cannot be found in the transfer dictionary.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the effectiveness of the Chinese word segmentation using different dictionaries.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6664026081562042}]}, {"text": "Four different dictionaries (or word lists) and two different testing collections (testing data) are used to evaluate the results of the Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.62139959136645}]}], "datasetContent": [{"text": "To evaluate the results of Chinese word segmentations, we implement 8 experiments (runs) using the 4 different dictionaries (CEDIC, CK, CT, and CK+CT) mentioned in previous section.", "labels": [], "entities": [{"text": "Chinese word segmentations", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.627012034257253}, {"text": "CEDIC", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.8857309222221375}]}, {"text": "Two test collections (the Sinica Corpus and the City University Corpus) are used to measure the precision, recall, and an evenly-weighted Fmeasure for the Chinese words segmentations.", "labels": [], "entities": [{"text": "Sinica Corpus", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.8092444241046906}, {"text": "City University Corpus", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.9856219093004862}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9995781779289246}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9994250535964966}, {"text": "Fmeasure", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9697456359863281}, {"text": "Chinese words segmentations", "start_pos": 155, "end_pos": 182, "type": "TASK", "confidence": 0.6202260057131449}]}, {"text": "shows the F-measure of the experimental results, and the illustrates the comparisons of the segmentation performances.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9977676868438721}]}, {"text": "The symbol (*) indicates that the run is a closed test, which only uses the training material from the training data for the particular corpus.", "labels": [], "entities": []}, {"text": "We can find that the larger dictionary (CK+CT) produces better segmentation results even the word lists are combined from the different resources (corpora) and followed the different guidelines of word segmentations.", "labels": [], "entities": [{"text": "word segmentations", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.7204391062259674}]}], "tableCaptions": [{"text": " Table 1 lists the number of tokens (#tokens),  the number of ignored tokens (#ignored), the  number of words (#words), and the unique words  (#unique) for each dictionaries. There are  140,971 unique words are extracted from the  training collection of Sinica Corpus, and 75,433  respected to the training set of the City  University Corpus. These two dictionaries are  combined to another dictionary which containing  174,398 unique words.", "labels": [], "entities": [{"text": "Sinica Corpus", "start_pos": 254, "end_pos": 267, "type": "DATASET", "confidence": 0.769144207239151}, {"text": "City  University Corpus", "start_pos": 318, "end_pos": 341, "type": "DATASET", "confidence": 0.982999304930369}]}, {"text": " Table 2. The F-measure results of segmentation per- formances using various dictionaries (*: closed test)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.982049286365509}]}]}