{"title": [{"text": "Chinese Word Segmentation with Maximum Entropy and N-gram Language Model", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6421346863110861}]}], "abstractContent": [{"text": "This paper presents the Chinese word seg-mentation systems developed by Speech and Hearing Research Group of National Laboratory on Machine Perception (NLMP) at Peking University, which were evaluated in the third International Chi-nese Word Segmentation Bakeoff held by SIGHAN.", "labels": [], "entities": [{"text": "International Chi-nese Word Segmentation Bakeoff", "start_pos": 214, "end_pos": 262, "type": "TASK", "confidence": 0.6077238976955414}]}, {"text": "The Chinese character-based maximum entropy model, which switches the word segmentation task to a classification task, is adopted in system developing.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7711090346177419}]}, {"text": "To integrate more linguistics information, an n-gram language model as well as several post processing strategies are also employed.", "labels": [], "entities": []}, {"text": "Both the closed and open tracks regarding to all four corpora MSRA, UPUC, CITYU, CKIP are involved in our systems' evaluation, and good performance are achieved.", "labels": [], "entities": [{"text": "MSRA", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.7850512862205505}, {"text": "UPUC", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.6960137486457825}, {"text": "CKIP", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.8111454248428345}]}, {"text": "Especially , in the closed track on MSRA, our system ranks 1st.", "labels": [], "entities": [{"text": "MSRA", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.9228417873382568}]}], "introductionContent": [{"text": "Chinese word segmentation is one of the core techniques in Chinese language processing and attracts lots of research interests in recent years.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5800099074840546}, {"text": "Chinese language processing", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6448410054047903}]}, {"text": "Several promising methods are proposed by previous researchers, in which Maximum Entropy (ME) model has turned out to be a successful way for this task (Hwee Tou).", "labels": [], "entities": [{"text": "Maximum Entropy (ME)", "start_pos": 73, "end_pos": 93, "type": "METRIC", "confidence": 0.8430572867393493}]}, {"text": "By employing Maximum Entropy (ME) model, the Chinese word segmentation task is regarded as a classification problem, where each character will be classified to one of the four classes, i.e., the beginning, middle, end of a multicharacter word and a single-character word.", "labels": [], "entities": [{"text": "Chinese word segmentation task", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6803473308682442}]}, {"text": "However, in a high degree, ME model pays its emphasis on Chinese characters while debases the consideration on the relationship of the context words.", "labels": [], "entities": [{"text": "ME", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.632651686668396}]}, {"text": "Motivated by this view, several strategies used for reflecting the context words' relationship and integrating more linguistics information, are employed in our systems.", "labels": [], "entities": []}, {"text": "As known, an n-gram language model could express the relationship of the context words well, it therefore as a desirable choice is imported in our system to modify the scoring of the ME model.", "labels": [], "entities": []}, {"text": "An analysis on our preliminary experiments shows the combination ambiguity is another issue that should be specially tackled, and a division and combination strategy is then adopted in our system.", "labels": [], "entities": []}, {"text": "To handle the numeral words, we also introduce a number conjunction strategy.", "labels": [], "entities": [{"text": "number conjunction", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7140933275222778}]}, {"text": "In addition, to deal with the long organization names problem in MSRA corpus, a post processing strategy for organization name is presented.", "labels": [], "entities": [{"text": "MSRA corpus", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.8464445769786835}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our system in detail.", "labels": [], "entities": []}, {"text": "Section 3 presents the experiments and results.", "labels": [], "entities": []}, {"text": "And in last section, we draw our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have participated in both the closed and open tracks of all the four corpora.", "labels": [], "entities": []}, {"text": "For MSRA corpus and other three corpora, we build System I and System II respectively.", "labels": [], "entities": [{"text": "MSRA corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9332495629787445}]}, {"text": "Both systems are based on the ME model and the Maximum Entropy Toolkit 1 , provided by Zhang Le, is adopted.", "labels": [], "entities": []}, {"text": "Four systems are derived from System I with regard to whether or not the n-gram language model and three post processing strategies are used on the closed track of MSRA corpus.: The effect of ME model, n-gram language model and three post processing strategies on the closed track of MSRA corpus.", "labels": [], "entities": [{"text": "MSRA corpus.", "start_pos": 164, "end_pos": 176, "type": "DATASET", "confidence": 0.8920629322528839}, {"text": "MSRA corpus", "start_pos": 284, "end_pos": 295, "type": "DATASET", "confidence": 0.9025863111019135}]}, {"text": "System IA only adopts the ME model.", "labels": [], "entities": [{"text": "ME", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9752384424209595}]}, {"text": "System IB integrates the ME model and the bigram language model.", "labels": [], "entities": []}, {"text": "System IC integrates the division and combination strategy and the numeral words processing strategy.", "labels": [], "entities": []}, {"text": "System ID adds the long organization name processing strategy.", "labels": [], "entities": [{"text": "long organization name processing", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.6013115420937538}]}, {"text": "For the open track of MSRA, an external dictionary is utilized to extract thee and f features.", "labels": [], "entities": [{"text": "MSRA", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9102291464805603}]}, {"text": "The external dictionary is built from six sources, including the Chinese Concept Dictionary from Institute of Computational Linguistics, Peking University(72,716 words), the LDC dictionary(43,120 words), the Noun Cyclopedia(111,633), the word segmentation dictionary from Institute of Computing Technology, Chinese Academy of Sciences(84,763 words), the dictionary from Institute of Acoustics, and the dictionary from Institute of Computational Linguistics, Peking University(68,200 words) and a dictionary collected by ourselves(63,470 words).", "labels": [], "entities": [{"text": "Chinese Concept Dictionary", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.8481051325798035}, {"text": "LDC dictionary", "start_pos": 174, "end_pos": 188, "type": "DATASET", "confidence": 0.8002977669239044}, {"text": "word segmentation", "start_pos": 238, "end_pos": 255, "type": "TASK", "confidence": 0.7102698236703873}]}, {"text": "The union of the six dictionaries forms a big dictionary, and those words appearing in five or six dictionaries are extracted to form a core dictionary.", "labels": [], "entities": []}, {"text": "If a word belongs to one of the following dictionaries or word sets, it is added into the external dictionary.", "labels": [], "entities": []}, {"text": "a) The core dictionary.", "labels": [], "entities": []}, {"text": "b) The intersection of the big dictionary and the training data.", "labels": [], "entities": []}, {"text": "c) The words appearing in the training data twice or more times.", "labels": [], "entities": []}, {"text": "Those words in the external dictionaries will be eliminated, if inmost cases they are divided in the training data.", "labels": [], "entities": []}, {"text": "System II only adopts ME model, the division and combination strategy and the numeral word processing strategy.", "labels": [], "entities": [{"text": "ME", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9422820806503296}]}, {"text": "In the open track of the corpora CKIP and CITYU, the training set and test set from the 2nd Chinese Word Segmentation Backoff are used for training.", "labels": [], "entities": [{"text": "CKIP", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8438301086425781}, {"text": "2nd Chinese Word Segmentation Backoff", "start_pos": 88, "end_pos": 125, "type": "DATASET", "confidence": 0.6239513099193573}]}, {"text": "For the corpora UPUC and CITYU, the external dictionaries are used, which is constructed in the same way as that in the open track of MSRA Corpus.: Official results of our systems on UPUC CKIP and CITYU On the UPUC corpus, an interesting observation is that the performance of the open track is worse than the closed track.", "labels": [], "entities": [{"text": "MSRA Corpus.", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.980079859495163}, {"text": "UPUC corpus", "start_pos": 210, "end_pos": 221, "type": "DATASET", "confidence": 0.832232654094696}]}, {"text": "The investigation and analysis lead to a possible explanation.", "labels": [], "entities": []}, {"text": "That is, the segmentation standard of the dictionaries, which are used to construct the external dictionary, is different from that of the UPUC corpus.", "labels": [], "entities": [{"text": "UPUC corpus", "start_pos": 139, "end_pos": 150, "type": "DATASET", "confidence": 0.9546347856521606}]}], "tableCaptions": [{"text": " Table 2: The effect of ME model, n-gram language  model and three post processing strategies on the  closed track of MSRA corpus.", "labels": [], "entities": [{"text": "ME", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.8555099368095398}, {"text": "MSRA corpus", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.8352145850658417}]}, {"text": " Table 3: The effect of ME model, n-gram language  model, three post processing strategies on the open  track of MSRA.", "labels": [], "entities": [{"text": "MSRA", "start_pos": 113, "end_pos": 117, "type": "TASK", "confidence": 0.7232039570808411}]}, {"text": " Table 4: Official results of our systems on UPUC  CKIP and CITYU", "labels": [], "entities": [{"text": "UPUC  CKIP", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.8419387340545654}, {"text": "CITYU", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.5220156311988831}]}]}