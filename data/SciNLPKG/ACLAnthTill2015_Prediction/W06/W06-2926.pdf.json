{"title": [{"text": "A Pipeline Model for Bottom-Up Dependency Parsing", "labels": [], "entities": [{"text": "Bottom-Up Dependency Parsing", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.5256573756535848}]}], "abstractContent": [{"text": "We present anew machine learning framework for multilingual dependency parsing.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.621769110361735}]}, {"text": "The framework uses a linear, pipeline based, bottom-up parsing algorithm, with a look ahead local search that serves to make the local predictions more robust.", "labels": [], "entities": []}, {"text": "As shown, the performance of the first generation of this algorithm is promising.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this work we used as our learning algorithm a regularized variation of the perceptron update rule as incorporated in SNoW), a multi-class classifier that is specifically tailored for large scale learning tasks.", "labels": [], "entities": []}, {"text": "SNoW uses softmax over the raw activation values as its confidence measure, which can be shown to be a reliable approximation of the labels' probabilities.", "labels": [], "entities": []}, {"text": "This is used both for labeling the actions and types of dependencies.", "labels": [], "entities": []}, {"text": "There is no special language enhancement required for each language.", "labels": [], "entities": []}, {"text": "The resources provided for 12 languages are described in:).", "labels": [], "entities": []}, {"text": "The feature set plays an important role in the quality of the classifier.", "labels": [], "entities": []}, {"text": "Basically, we used the same feature set for the action selection classifiers and for the label classifiers.", "labels": [], "entities": []}, {"text": "In our work, each example has average fifty active features.", "labels": [], "entities": []}, {"text": "For each word pair (w 1 , w 2 ), we used their LEMMA, the POSTAG and also the POSTAG of the children of w 1 and w 2 . We also included the LEMMA and POSTAG of surrounding words in a window of size (2, 4).", "labels": [], "entities": [{"text": "POSTAG", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9270288348197937}, {"text": "POSTAG", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9771954417228699}]}, {"text": "We considered 2 words before w 1 and 4 words after w 2 (we agree with the window size in).", "labels": [], "entities": []}, {"text": "The major difference of our feature set compared with the one in () is that we included the previous predicted action.", "labels": [], "entities": []}, {"text": "We also added some conjunctions of the above features to ensure expressiveness of the model.", "labels": [], "entities": []}, {"text": "( made use of the polynomial kernel of degree 2 so they in fact use more conjunctive features.", "labels": [], "entities": []}, {"text": "Beside these features, we incorporated the information of FEATS for the languages when it is available.", "labels": [], "entities": [{"text": "FEATS", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7899343967437744}]}, {"text": "The columns in the data files we used for our work are the LEMMA, POSTAG, and the FEATS, which is treated as atomic.", "labels": [], "entities": [{"text": "LEMMA", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9882661700248718}, {"text": "POSTAG", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9138973355293274}, {"text": "FEATS", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9901120066642761}]}, {"text": "Due to time limitation, we did not apply the local search algorithm for the languages having the FEATS features.", "labels": [], "entities": [{"text": "FEATS", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.5758366584777832}]}, {"text": "shows our results on Unlabeled Attachment Scores (UAS), Labeled Attachment Scores (LAS), and Label Accuracy score (LAC) for 12 languages.", "labels": [], "entities": [{"text": "Unlabeled Attachment Scores (UAS)", "start_pos": 21, "end_pos": 54, "type": "METRIC", "confidence": 0.6626303493976593}, {"text": "Labeled Attachment Scores (LAS)", "start_pos": 56, "end_pos": 87, "type": "METRIC", "confidence": 0.7431668241818746}, {"text": "Label Accuracy score (LAC)", "start_pos": 93, "end_pos": 119, "type": "METRIC", "confidence": 0.8313558598359426}]}, {"text": "Our results are compared with the average scores (AV) and the standard deviations (SD), of all the systems participating in the shared task of CoNLL-X.", "labels": [], "entities": [{"text": "average scores (AV)", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.7211564183235168}, {"text": "standard deviations (SD)", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.9616488099098206}, {"text": "CoNLL-X", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.8594022393226624}]}], "tableCaptions": [{"text": " Table 1: Our results are compared with the average scores. UAS=Unlabeled Attachment Score,  LAS=Labeled Attachment Score, LAC=Label Accuracy, AV=Average score, and SD=standard deviation.", "labels": [], "entities": [{"text": "UAS=Unlabeled Attachment Score", "start_pos": 60, "end_pos": 90, "type": "METRIC", "confidence": 0.7481572508811951}, {"text": "LAS=Labeled Attachment Score", "start_pos": 93, "end_pos": 121, "type": "METRIC", "confidence": 0.8707846879959107}, {"text": "LAC=Label Accuracy", "start_pos": 123, "end_pos": 141, "type": "METRIC", "confidence": 0.8289717435836792}, {"text": "AV=Average score", "start_pos": 143, "end_pos": 159, "type": "METRIC", "confidence": 0.8371429592370987}]}]}