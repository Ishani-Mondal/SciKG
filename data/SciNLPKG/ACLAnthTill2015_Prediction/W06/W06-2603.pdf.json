{"title": [{"text": "Decomposition Kernels for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a simple solution to the sequence labeling problem based on an extension of weighted decomposition kernels.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7542639573415121}]}, {"text": "We additionally introduce a multi-instance kernel approach for representing lexical word sense information.", "labels": [], "entities": [{"text": "representing lexical word sense information", "start_pos": 63, "end_pos": 106, "type": "TASK", "confidence": 0.7449808835983276}]}, {"text": "These new ideas have been preliminarily tested on named entity recognition and PP attachment disambiguation.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6780823270479838}, {"text": "PP attachment disambiguation", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.8933735092480978}]}, {"text": "We finally suggest how these techniques could be potentially merged using a declarative formalism that may provide a basis for the integration of multiple sources of information when using kernel-based learning in NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many tasks related to the analysis of natural language are best solved today by machine learning and other data driven approaches.", "labels": [], "entities": []}, {"text": "In particular, several subproblems related to information extraction can be formulated in the supervised learning framework, where statistical learning has rapidly become one of the preferred methods of choice.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8270634114742279}]}, {"text": "A common characteristic of many NLP problems is the relational and structured nature of the representations that describe data and that are internally used by various algorithms.", "labels": [], "entities": []}, {"text": "Hence, in order to develop effective learning algorithms, it is necessary to cope with the inherent structure that characterize linguistic entities.", "labels": [], "entities": []}, {"text": "Kernel methods (see e.g. Shawe-) are well suited to handle learning tasks in structured domains as the statistical side of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function.", "labels": [], "entities": []}, {"text": "As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field.", "labels": [], "entities": []}, {"text": "Applications are numerous and diverse and include for example refinement of statistical parsers), tagging named entities (), syntactic chunking), extraction of relations between entities (), semantic role labeling).", "labels": [], "entities": [{"text": "tagging named entities", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.9095566272735596}, {"text": "syntactic chunking", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.6974620819091797}, {"text": "extraction of relations between entities", "start_pos": 146, "end_pos": 186, "type": "TASK", "confidence": 0.8784529685974121}, {"text": "semantic role labeling", "start_pos": 191, "end_pos": 213, "type": "TASK", "confidence": 0.7403727571169535}]}, {"text": "The literature is rich with examples of kernels on discrete data structures such as sequences (), trees), and annotated graphs).", "labels": [], "entities": []}, {"text": "Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels).", "labels": [], "entities": []}, {"text": "Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts.", "labels": [], "entities": []}, {"text": "However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size.", "labels": [], "entities": []}, {"text": "When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (), adversely affecting generalization in spite of using large margin classifiers).", "labels": [], "entities": []}, {"text": "Possible cures include extensive use of prior knowledge to guide the choice of relevant parts), the use of feature selection (), and soft matches ( ).", "labels": [], "entities": []}, {"text": "In () we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts.", "labels": [], "entities": [{"text": "generalization", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9801264405250549}]}, {"text": "Ina weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures.", "labels": [], "entities": []}, {"text": "Here we introduce a position-dependent version of WDK that can solve sequence labeling problems without searching the output space, as required by other recently proposed kernel-based solutions).", "labels": [], "entities": [{"text": "WDK", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7586562633514404}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next two sections we briefly review decomposition kernels and its weighted variant.", "labels": [], "entities": []}, {"text": "In Section 4 we introduce aversion of WDK for solving supervised sequence labeling tasks and report a preliminary evaluation on a named entity recognition problem.", "labels": [], "entities": [{"text": "WDK", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.483898788690567}, {"text": "named entity recognition problem", "start_pos": 130, "end_pos": 162, "type": "TASK", "confidence": 0.752125158905983}]}, {"text": "In Section 5 we suggest a novel multi-instance approach for representing WordNet information and present an application to the PP attachment ambiguity resolution problem.", "labels": [], "entities": [{"text": "PP attachment ambiguity resolution", "start_pos": 127, "end_pos": 161, "type": "TASK", "confidence": 0.9010348469018936}]}, {"text": "In Section 6 we discuss how these ideas could be merged using a declarative formalism in order to integrate multiple sources of information when using kernelbased learning in NLP.", "labels": [], "entities": []}], "datasetContent": [{"text": "Named entities are phrases that contain the names of persons, organizations, locations, times and quantities.", "labels": [], "entities": [{"text": "Named entities are phrases that contain the names of persons, organizations, locations, times and quantities", "start_pos": 0, "end_pos": 108, "type": "Description", "confidence": 0.7339754468864865}]}, {"text": "For example in the following sentence: we are interested in predicting that Wolff and Del Bosque are people's names, that Argentina is a name of a location and that Real Madrid is a name of an organization.", "labels": [], "entities": []}, {"text": "The chosen dataset is provided by the shared task of) which concerns language-independent named entity recognition.", "labels": [], "entities": [{"text": "language-independent named entity recognition", "start_pos": 69, "end_pos": 114, "type": "TASK", "confidence": 0.5711186230182648}]}, {"text": "There are four types of phrases: person names (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC), combined with two tags, B to denote the first item of a phrase and I for any non-initial word; all other phrases are classified as (OTHER).", "labels": [], "entities": [{"text": "OTHER", "start_pos": 254, "end_pos": 259, "type": "METRIC", "confidence": 0.9449986815452576}]}, {"text": "Of the two available languages (Spanish and Dutch), we run experiments only on the Spanish dataset which is a collection of news wire articles made available by the Spanish EFE News Agency.", "labels": [], "entities": [{"text": "Spanish dataset", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.7712675333023071}, {"text": "EFE News Agency", "start_pos": 173, "end_pos": 188, "type": "DATASET", "confidence": 0.8625027139981588}]}, {"text": "We select a subset of 300 sentences for training and we evaluate the performance on test set.", "labels": [], "entities": []}, {"text": "For each category, we evaluate the F \u03b2=1 measure of 4 versions of WDK: word histograms are matched using HIK (Eq.", "labels": [], "entities": [{"text": "F \u03b2=1 measure", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.942455279827118}, {"text": "HIK", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.7602052092552185}]}, {"text": "3) and the kernels on various parts (z LL , z LR ,etc) are combined with a summation SUMHIK or product PROHIK; alternatively the histograms are com-  bined with a PPK (Eq. 4) obtaining SUMPPK, PROPPK.", "labels": [], "entities": []}, {"text": "The word attribute considered for the selector is a word morphologic trait that classifies a word in one of five possible categories: normal word, number, all capital letters, only capital initial and contains non alphabetic characters, while the context histograms are computed counting the exact word frequencies.", "labels": [], "entities": []}, {"text": "2 show that performance is mildly affected by the different choices on how to combine information on the various contexts, though it seems clear that increasing contextual information has a positive influence.", "labels": [], "entities": []}, {"text": "Note that interesting preliminary results can be obtained even without the use of any refined language knowledge, such as part of speech tagging or shallow/deep parsing.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.696121484041214}]}, {"text": "The experiments have been performed using the Wall-Street Journal dataset described in.", "labels": [], "entities": [{"text": "Wall-Street Journal dataset", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.9898682832717896}]}, {"text": "This dataset contains 20, 800 training examples and 3, 097 testing examples.", "labels": [], "entities": []}, {"text": "Each phrase x in the dataset is reduced to a verb xv , its object noun x n 1 and prepositional phrase formed by a preposition x p and a noun x n 2 . The target is either V or N whether the phrase is attached to the verb or the noun.", "labels": [], "entities": []}, {"text": "Data have been preprocessed by assigning to all the words their corresponding synsets.", "labels": [], "entities": []}, {"text": "Additional meanings derived from specific synsets have been attached to the words as described in).", "labels": [], "entities": []}, {"text": "The kernel between two phrases x and x \ud97b\udf59 is then computed by combining the kernels between single words using either the sum or the product.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NER experiment D=4", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5879870653152466}, {"text": "D", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.539371907711029}]}, {"text": " Table 2: NER experiment with D=6", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8637829422950745}, {"text": "D", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9932460188865662}]}]}