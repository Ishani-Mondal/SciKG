{"title": [{"text": "Improved morpho-phonological sequence processing with constraint satisfaction inference", "labels": [], "entities": [{"text": "Improved morpho-phonological sequence processing", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7160203009843826}]}], "abstractContent": [{"text": "In performing morpho-phonological sequence processing tasks, such as letter-phoneme conversion or morphological analysis, it is typically not enough to base the output sequence on local decisions that map local-context input windows to single output tokens.", "labels": [], "entities": [{"text": "letter-phoneme conversion", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7250027358531952}, {"text": "morphological analysis", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7389390170574188}]}, {"text": "We present a global sequence-processing method that repairs inconsistent local decisions.", "labels": [], "entities": []}, {"text": "The approach is based on local predictions of overlapping trigrams of output tokens, which open up a space of possible sequences; a data-driven constraint satisfaction inference step then searches for the optimal output sequence.", "labels": [], "entities": []}, {"text": "We demonstrate significant improvements in terms of word accuracy on English and Dutch letter-phoneme conversion and morphological segmenta-tion, and we provide qualitative analyses of error types prevented by the constraint satisfaction inference method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9361708164215088}, {"text": "English and Dutch letter-phoneme conversion", "start_pos": 69, "end_pos": 112, "type": "TASK", "confidence": 0.5488051354885102}]}], "introductionContent": [{"text": "The fields of computational phonology and morphology were among the earlier fields in computational linguistics to adopt machine learning algorithms as a means to automatically construct processing systems from data.", "labels": [], "entities": []}, {"text": "For instance, letterphoneme conversion was already pioneered, with neural networks initially, at the end of the 1980s, and was shortly after also investigated with memory-based learning and analogical approaches and decision trees.", "labels": [], "entities": [{"text": "letterphoneme conversion", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.8029132783412933}]}, {"text": "The development of these datadriven systems was thrusted by the early existence of lexical databases, originally compiled to serve (psycho)linguistic research purposes, such as the CELEX lexical database for Dutch, English, and German ().", "labels": [], "entities": [{"text": "CELEX lexical database", "start_pos": 181, "end_pos": 203, "type": "DATASET", "confidence": 0.9061182936032613}]}, {"text": "Many researchers have continued and are still continuing this line of work, generally producing successful systems with satisfactory, though still imperfect performance.", "labels": [], "entities": []}, {"text": "A key characteristic of many of these early systems is that they perform decomposed or simplified versions of the full task.", "labels": [], "entities": []}, {"text": "Rather than predicting the full phonemization of a word given its orthography in one go, the task is decomposed in predicting individual phonemes or subsequences of phonemes.", "labels": [], "entities": []}, {"text": "Analogously, rather than generating a full wordform, many morphological generation systems produce transformation codes (e.g., \"add -er and umlaut\") that need to be applied to the input string by a post-processing automaton.", "labels": [], "entities": []}, {"text": "These task simplifications are deliberately chosen to avoid sparseness problems to the machine learning systems.", "labels": [], "entities": []}, {"text": "Such systems tend to perform badly when there are many low-frequent and too case-specific classes; task decomposition allows them to be robust and generic when they process unseen words.", "labels": [], "entities": [{"text": "task decomposition", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7127860486507416}]}, {"text": "This task decomposition strategy has a severe drawback in sequence processing tasks.", "labels": [], "entities": [{"text": "task decomposition", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.7271668016910553}]}, {"text": "Decomposed systems do not have any global method to check whether their local decisions form a globally coherent output.", "labels": [], "entities": []}, {"text": "If a letter-phoneme conversion system predicts schwas on every vowel in a polysyllabic word such as parameter because it is uncertain about the ambiguous mapping of each of the as and es, it produces a bad pronunciation.", "labels": [], "entities": []}, {"text": "Likewise, if a morphological analysis system segments a word such as being as a prefix followed by an inflection, making the locally most likely guesses, it generates an analysis that could never exist, since it lacks a stem.", "labels": [], "entities": []}, {"text": "Global models that coordinate, mediate, or enforce that the output is a valid sequence are typically formulated in the form of linguistic rules, applied during processing or in post-processing, that constrain the space of possible output sequences.", "labels": [], "entities": []}, {"text": "Some present-day research in machine learning of morpho-phonology indeed focuses on satisfying linguistically-motivated constraints as a postprocessing or filtering step; e.g., see) on identifying roots in Hebrew word forms.", "labels": [], "entities": [{"text": "identifying roots in Hebrew word forms", "start_pos": 185, "end_pos": 223, "type": "TASK", "confidence": 0.6988940834999084}]}, {"text": "Optimality Theory () can also be seen as a constraint-based approach to language processing based on linguistically motivated constraints.", "labels": [], "entities": [{"text": "Optimality Theory", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8523813188076019}, {"text": "language processing", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7144565880298615}]}, {"text": "In contrast to being motivated by linguistic theory, constraints in a global model can be learned automatically from data as well.", "labels": [], "entities": []}, {"text": "In this paper we propose such a data-driven constraint satisfaction inference method, that finds a globally appropriate output sequence on the basis of a space of possible sequences generated by a locally-operating classifier predicting output subsequences.", "labels": [], "entities": []}, {"text": "We show that the method significantly improves on the basic method of predicting single output tokens at a time, on English and Dutch letter-phoneme conversion and morphological analysis.", "labels": [], "entities": [{"text": "English and Dutch letter-phoneme conversion", "start_pos": 116, "end_pos": 159, "type": "TASK", "confidence": 0.5201719045639038}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "The constraint satisfaction inference method is outlined in Section 2.", "labels": [], "entities": []}, {"text": "We describe the four morpho-phonological processing tasks, and the lexical data from which we extracted examples for these tasks, in Section 3.", "labels": [], "entities": []}, {"text": "We subsequently list the outcomes of the experiments in Section 4, and conclude with a discussion of our findings in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Numbers of unigram and trigram classes  for the four tasks.", "labels": [], "entities": []}, {"text": " Table 6: Numbers of repaired errors divided over  three categories of morphological analysis classifi- cations (top) and letter-phoneme conversions (bot- tom) of the constraint satisfaction inference method  as compared to the unigram classifier.", "labels": [], "entities": []}]}