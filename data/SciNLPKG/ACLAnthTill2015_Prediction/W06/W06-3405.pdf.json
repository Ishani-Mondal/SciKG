{"title": [{"text": "Shallow Discourse Structure for Action Item Detection", "labels": [], "entities": [{"text": "Item Detection", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7694303095340729}]}], "abstractContent": [{"text": "We investigated automatic action item detection from transcripts of multi-party meetings.", "labels": [], "entities": [{"text": "automatic action item detection", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.6326490566134453}]}, {"text": "Unlike previous work (Gruen-stein et al., 2005), we use anew hierarchical annotation scheme based on the roles utterances play in the action item assignment process, and propose an approach to automatic detection that promises improved classification accuracy while enabling the extraction of useful information for summarization and reporting.", "labels": [], "entities": [{"text": "action item assignment process", "start_pos": 134, "end_pos": 164, "type": "TASK", "confidence": 0.7314944267272949}, {"text": "automatic detection", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.7178434431552887}, {"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.8770648837089539}, {"text": "summarization and reporting", "start_pos": 316, "end_pos": 343, "type": "TASK", "confidence": 0.6595285534858704}]}], "introductionContent": [{"text": "Action items are specific kinds of decisions common in multi-party meetings, characterized by the concrete assignment of tasks together with certain properties such as an associated timeframe and reponsible party.", "labels": [], "entities": []}, {"text": "Our aims are firstly to automatically detect the regions of discourse which establish action items, so their surface form can be used fora targeted report or summary; and secondly, to identify the important properties of the action items (such as the associated tasks and deadlines) that would foster concise and informative semantically-based reporting (for example, adding task specifications to a user's to-do list).", "labels": [], "entities": []}, {"text": "We believe both of these aims are facilitated by taking into account the roles different utterances play in the decision-making process -in short, a shallow notion of discourse structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied's flat annotation schema to transcripts from a sequence of 5 short related meetings with 3 participants recorded as part of the CALO project.", "labels": [], "entities": []}, {"text": "Each meeting was simulated in that its participants were given a scenario, but was not scripted.", "labels": [], "entities": []}, {"text": "In order to avoid entirely dataor scenario-specific results (and also to provide an acceptable amount of training data), we then added a random selection of 6 ICSI and 1 ISL meetings from's annotations.", "labels": [], "entities": [{"text": "ICSI and 1 ISL meetings", "start_pos": 159, "end_pos": 182, "type": "DATASET", "confidence": 0.7197157979011536}]}, {"text": "Like) we used support vector machines via the classifier SVMlight.", "labels": [], "entities": []}, {"text": "Their full set of features are not available to us, but we experimented with combinations of words and n-grams and assessed classification performance via a 5-fold validation on each of the CALO meetings.", "labels": [], "entities": [{"text": "CALO meetings", "start_pos": 190, "end_pos": 203, "type": "DATASET", "confidence": 0.7873181998729706}]}, {"text": "In each case, we trained classifiers on the other 4 meetings in the CALO sequence, plus the fixed ICSI/ISL training selection.", "labels": [], "entities": [{"text": "CALO sequence", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.5626734793186188}, {"text": "ICSI/ISL training", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.49589430540800095}]}, {"text": "Performance (per utterance, on the binary classification problem) is shown in; overall f-score figures are poor even on these short meetings.", "labels": [], "entities": [{"text": "f-score", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9527965784072876}]}, {"text": "These figures were obtained using words (unigrams, after text normalization and stemming) as features -we investigated other discriminative classifier methods, and the use of 2-and 3-grams as features, but no improvements were gained.", "labels": [], "entities": [{"text": "text normalization and stemming", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.7110054939985275}]}, {"text": "We trained individual classifiers for each of the utterance sub-classes, and cross-validated as before.", "labels": [], "entities": []}, {"text": "For agreement utterances, we used a naive n-gram classifier similar to that of () for dialogue act detection, scoring utterances via a set of most predictive n-grams of length 1-3 and making a classification decision by comparing the maximum score to a threshold (where the n-grams, their scores and the threshold are automatically extracted from the training data).", "labels": [], "entities": [{"text": "agreement utterances", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7456133365631104}, {"text": "dialogue act detection", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.8056690891583761}]}, {"text": "For owner, timeframe and task description utterances, we used SVMs as before, using word unigrams as features (2-and 3-grams gave no improvement -probably due to the small amount of training data).", "labels": [], "entities": []}, {"text": "Performance varied greatly by sub-class (see), with some (e.g. agreement) achieving higher accuracy than the baseline flat classifications, but others being worse.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9988825917243958}]}, {"text": "As there is now significantly less training data available to each sub-class than there was for all utterances grouped together in the baseline experiment, worse performance might be expected; yet some sub-classes perform better.", "labels": [], "entities": []}, {"text": "The worst performing class is owner.", "labels": [], "entities": [{"text": "owner", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.7078179121017456}]}, {"text": "Examination of the data shows that owner utterances are more likely than other classes to be assigned to more than one category; they may therefore have more feature overlap with other classes, leading to less accurate classification.", "labels": [], "entities": []}, {"text": "Use of relevant sub-strings for training (rather than full utterances) may help; as may part-of-speech information -while proper names maybe useful features, the name tokens themselves are sparse and maybe better substituted with a generic tag.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline Classification Performance", "labels": [], "entities": [{"text": "Baseline Classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.67092165350914}]}, {"text": " Table 2: Sub-class Classification Performance", "labels": [], "entities": [{"text": "Sub-class Classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7791764140129089}]}, {"text": " Table 3: Combined Classification Performance", "labels": [], "entities": [{"text": "Combined Classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6157417297363281}]}]}