{"title": [{"text": "Non-Local Modeling with a Mixture of PCFGs", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.7997714877128601}]}], "abstractContent": [{"text": "While most work on parsing with PCFGs has focused on local correlations between tree configurations, we attempt to model non-local correlations using a finite mixture of PCFGs.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9820781350135803}]}, {"text": "A mixture grammar fit with the EM algorithm shows improvement over a single PCFG, both in parsing accuracy and in test data likelihood.", "labels": [], "entities": [{"text": "parsing", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.9674606919288635}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9450352191925049}]}, {"text": "We argue that this improvement comes from the learning of specialized grammars that capture non-local correlations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers.", "labels": [], "entities": []}, {"text": "The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees.", "labels": [], "entities": []}, {"text": "Compared to a basic treebank grammar, the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical) or nonlexical () conditioning information.", "labels": [], "entities": []}, {"text": "While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser.", "labels": [], "entities": [{"text": "statistical estimation", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7902803719043732}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9986703395843506}]}, {"text": "However, the configurations exploited in PCFG parsers are quite local: rules' probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations.", "labels": [], "entities": []}, {"text": "For example, it is generally not modeled that if one quantifier phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased.", "labels": [], "entities": [{"text": "Penn Treebank)", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.9664303461710612}]}, {"text": "This kind of effect is neither surprising nor unknown -for example, show experimentally that human language generation demonstrates priming effects.", "labels": [], "entities": []}, {"text": "The mediating variables cannot only include priming effects but also genre or stylistic conventions, as well as many other factors which are not adequately modeled by local phrase structure.", "labels": [], "entities": []}, {"text": "A reasonable way to add a latent variable to a generative model is to use a mixture of estimators, in this case a mixture of PCFGs (see Section 3).", "labels": [], "entities": []}, {"text": "The general mixture of estimators approach was first suggested in the statistics literature by and has since been adopted in machine learning (.", "labels": [], "entities": []}, {"text": "Ina mixture approach, we have anew global variable on which all PCFG productions fora given sentence can be conditioned.", "labels": [], "entities": []}, {"text": "In this paper, we experiment with a finite mixture of PCFGs.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.916245698928833}]}, {"text": "This is similar to the latent nonterminals used in, but because the latent variable we use is global, our approach is more oriented toward learning non-local structure.", "labels": [], "entities": []}, {"text": "We demonstrate that a mixture fit with the EM algorithm gives improved parsing accuracy and test data likelihood.", "labels": [], "entities": [{"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.9701334238052368}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9572551846504211}]}, {"text": "We then investigate what is and is not being learned by the latent mixture variable.", "labels": [], "entities": []}, {"text": "While mixture components are difficult to interpret, we demonstrate that the patterns learned are better than random splits.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}