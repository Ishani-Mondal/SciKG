{"title": [{"text": "Efficient Search for Inversion Transduction Grammar", "labels": [], "entities": [{"text": "Inversion Transduction", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.7608279287815094}]}], "abstractContent": [{"text": "We develop admissible A* search heuris-tics for synchronous parsing with Inversion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding.", "labels": [], "entities": [{"text": "machine translation decoding", "start_pos": 159, "end_pos": 187, "type": "TASK", "confidence": 0.7719388802846273}]}, {"text": "We also combine the dynamic programming hook trick with A* search for decoding.", "labels": [], "entities": []}, {"text": "These techniques make it possible to find optimal alignments much more quickly, and make it possible to find optimal translations for the first time.", "labels": [], "entities": []}, {"text": "Even in the presence of pruning, we are able to achieve higher BLEU scores with the same amount of computation.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9760243892669678}]}], "introductionContent": [{"text": "The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages.", "labels": [], "entities": [{"text": "Inversion Transduction Grammar (ITG)", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7828294386466345}, {"text": "word-level alignments of pairs of translationally equivalent sentences", "start_pos": 107, "end_pos": 177, "type": "TASK", "confidence": 0.7119280099868774}]}, {"text": "The algorithm builds asynchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages.", "labels": [], "entities": []}, {"text": "ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bitext data.", "labels": [], "entities": [{"text": "ITG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8679561018943787}]}, {"text": "A major motivation for the introduction of ITG was the existence of polynomial-time algorithms both for alignment and translation.", "labels": [], "entities": [{"text": "alignment and translation", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.6290886998176575}]}, {"text": "Alignment, whether for training a translation model using EM or for finding the Viterbi alignment of test data, is O(n 6 ) (Wu, 1997), while translation (decoding) is O(n 7 ) using a bigram language model, and O(n 11 ) with trigrams.", "labels": [], "entities": [{"text": "O", "start_pos": 115, "end_pos": 116, "type": "METRIC", "confidence": 0.9877458214759827}]}, {"text": "While polynomial-time algorithms area major improvement over the NPcomplete problems posed by the alignment models of, the degree of these polynomials is high, making both alignment and decoding infeasible for realistic sentences without very significant pruning.", "labels": [], "entities": []}, {"text": "In this paper, we explore use of the \"hook trick\") to reduce the asymptotic complexity of decoding, and the use of heuristics to guide the search.", "labels": [], "entities": []}, {"text": "Our search heuristics area conservative estimate of the outside probability of a bitext cell in the complete synchronous parse.", "labels": [], "entities": []}, {"text": "Some estimate of this outside probability is a common element of modern statistical (monolingual) parsers, and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found.", "labels": [], "entities": []}, {"text": "We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities.", "labels": [], "entities": [{"text": "word translation", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7317456603050232}]}, {"text": "These measures have been used to guide search in word-or phrase-based MT systems (), but in such models optimal search is generally not practical even with good heuristics.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.8151811957359314}]}, {"text": "In this paper, we show that the same assumptions that make ITG polynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the performance of our heuristics for alignment on a Chinese-English newswire corpus.", "labels": [], "entities": [{"text": "alignment", "start_pos": 48, "end_pos": 57, "type": "TASK", "confidence": 0.9609491229057312}, {"text": "Chinese-English newswire corpus", "start_pos": 63, "end_pos": 94, "type": "DATASET", "confidence": 0.7184251646200815}]}, {"text": "Probabilities for the ITG model were trained using Expectation Maximization on a corpus of 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words.", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.756726086139679}]}, {"text": "For EM training, we limited the data to sentences of no more than 25 words in either language.", "labels": [], "entities": [{"text": "EM training", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.918982595205307}]}, {"text": "Here we present timing results for finding the Viterbi alignment of longer sentences using this fixed translation model with different heuristics.", "labels": [], "entities": []}, {"text": "We compute alignments on a total of 117 test sentences, which are broken down by length as shown in. method time speedup full 815s -uniform 547s 1.4 ibm1encn 269s 3.0 ibm1sym 205s 3.9: Total time for each alignment method.", "labels": [], "entities": []}, {"text": "Results are presented both in terms of time and the number of arcs added to the chart before the optimal parse is found.", "labels": [], "entities": []}, {"text": "Full refers to exhaustive parsing, that is, building a complete chart with all n 4 arcs.", "labels": [], "entities": []}, {"text": "Uniform refers to a best-first parsing strategy that expands the arcs with the highest inside probability at each step, but does not incorporate an estimate of the outside probability.", "labels": [], "entities": []}, {"text": "Ibm1encn denotes our heuristic based on IBM model 1, applied to translations from English to Chinese, while ibm1sym applies the Model 1 heuristic in both translation directions and takes the minimum.", "labels": [], "entities": []}, {"text": "The factor by which times were decreased was found to be roughly constant across different length sentences.", "labels": [], "entities": []}, {"text": "The alignment times for the entire test set are shown in, the best heuristic is 3.9 times faster than exhaustive BI-HOOK-A*+BEAM BI-CYK-BEAM: On the left, we compare decoding speed for uniform outside estimate best-first decoding with and without the hook trick, as well as results using our heuristic (labeled A*) and with beam pruning (which no longer produces optimal results).", "labels": [], "entities": [{"text": "BI-HOOK-A*+BEAM BI-CYK-BEAM", "start_pos": 113, "end_pos": 140, "type": "METRIC", "confidence": 0.835763543844223}]}, {"text": "On the right, we show the relationship between computation time and BLEU scores as the pruning threshold is varied for both A* search and bottom-up CYK parsing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9993176460266113}, {"text": "CYK parsing", "start_pos": 148, "end_pos": 159, "type": "TASK", "confidence": 0.7626309096813202}]}, {"text": "We did our ITG decoding experiments on the LDC 2002 MT evaluation data set for translation of Chinese newswire sentences into English.", "labels": [], "entities": [{"text": "LDC 2002 MT evaluation data set", "start_pos": 43, "end_pos": 74, "type": "DATASET", "confidence": 0.9643955032030741}, {"text": "translation of Chinese newswire sentences", "start_pos": 79, "end_pos": 120, "type": "TASK", "confidence": 0.8975748181343078}]}, {"text": "The evaluation data set has 10 human translation references for each sentence.", "labels": [], "entities": [{"text": "evaluation data set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8009718060493469}]}, {"text": "There area total of 371 Chinese sentences of no more than 20 words in the data set.", "labels": [], "entities": []}, {"text": "These sentences are the test set for our different versions of ITG decoders using both a bigram language model and a trigram language model.", "labels": [], "entities": []}, {"text": "We evaluate the translation results by comparing them against the reference translations using the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9962208867073059}]}, {"text": "The word-for-word translation probabilities are from the translation model of IBM Model 4 trained on a 160-million-word English-Chinese parallel corpus using GIZA++.", "labels": [], "entities": [{"text": "word-for-word translation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.575125589966774}]}, {"text": "The language model is trained on a 30-millionword English corpus.", "labels": [], "entities": []}, {"text": "The rule probabilities for ITG are from the same training as in the alignment experiments described above.", "labels": [], "entities": []}, {"text": "We compared the BLEU scores of the A* decoder and the ITG decoder that uses beam ratio pruning at each stage of bottom-up parsing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9995290040969849}, {"text": "ITG decoder", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.98825803399086}]}, {"text": "In the case of bigram-integrated decoding, for each input word, the best 2 translations are put into the bag of output words.", "labels": [], "entities": []}, {"text": "In the case of trigram-integrated decoding, top 5 candidate words are chosen.", "labels": [], "entities": []}, {"text": "The A* decoder is guaranteed to find the Viterbi translation that maximizes the product of n-grams probabilities, translation probabilities (including insertions and deletions) and inversion rule probabilities by choosing the right words and the right word order subject to the ITG constraint.", "labels": [], "entities": []}, {"text": "tained through the hook trick, the heuristic, and pruning, all based on A* search.", "labels": [], "entities": []}, {"text": "shows the improvement of BLEU score after applying the A* algorithm to find the optimal translation under the model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9779248237609863}]}, {"text": "(right) investigates the relationship between the search effort and BLEU score for A* and bottom-up CYK parsing, both with pruning.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.983929455280304}, {"text": "A", "start_pos": 83, "end_pos": 84, "type": "METRIC", "confidence": 0.9574276804924011}, {"text": "CYK parsing", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.7613843679428101}]}, {"text": "Pruning for A* works in such away that we never explore a low probability hypothesis falling out of a certain beam ratio of the best hypothesis within the bucket of X[i, j, * , * ], where * means any word.", "labels": [], "entities": []}, {"text": "shows results for trigramintegrated decoding.", "labels": [], "entities": []}, {"text": "However, due to time constraint, we have not explored time/performance tradeoff as we did for bigram decoding.", "labels": [], "entities": []}, {"text": "The number of combinations in the table is the average number of hyperedges to be explored in searching, proportional to the total number of computation steps.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Length of longer sentence in each pair  from test data.", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.998426079750061}]}]}