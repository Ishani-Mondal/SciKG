{"title": [{"text": "A SVM-based Model for Chinese Functional Chunk Parsing", "labels": [], "entities": [{"text": "Chinese Functional Chunk Parsing", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.5380239859223366}]}], "abstractContent": [{"text": "Functional chunks are defined as a series of non-overlapping, non-nested segments of text in a sentence, representing the implicit grammatical relations between the sentence-level predicates and their arguments.", "labels": [], "entities": [{"text": "Functional chunks are defined as a series of non-overlapping, non-nested segments of text in a sentence, representing the implicit grammatical relations between the sentence-level predicates and their arguments", "start_pos": 0, "end_pos": 210, "type": "Description", "confidence": 0.7830497761567433}]}, {"text": "Its top-down scheme and complexity of internal constitutions bring in anew challenge for automatic parser.", "labels": [], "entities": []}, {"text": "In this paper, anew parsing model is proposed to formulate the complete chunk-ing problem as a series of boundary detection sub tasks.", "labels": [], "entities": []}, {"text": "Each of these sub tasks is only in charge of detecting one type of the chunk boundaries.", "labels": [], "entities": []}, {"text": "As each sub task could be modeled as a binary classification problem, a lot of machine learning techniques could be applied.", "labels": [], "entities": []}, {"text": "In our experiments, we only focus on the subject-predicate (SP) and predicate-object (PO) boundary detection sub tasks.", "labels": [], "entities": [{"text": "predicate-object (PO) boundary detection", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.7459553082784017}]}, {"text": "By applying SVM algorithm to these sub tasks, we have achieved the best F-Score of 76.56% and 82.26% respectively.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9996155500411987}]}], "introductionContent": [{"text": "Parsing is a basic task in natural language processing; however, it has not been successful in achieving the accuracy and efficiency required by real world applications.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9223290681838989}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9988369345664978}]}, {"text": "As an alternative, shallow parsing or partial parsing has been proposed to meet the current needs by obtaining only a limited amount of syntactic information needed by the application.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.5086803138256073}]}, {"text": "In recent years, there has been an increasing interest in chunk parsing.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9055085182189941}]}, {"text": "From CoNLL-2000 to, a lot of efforts have been made in the identification of basic chunks and the methods of combining them from bottom-up to form large, complex units.", "labels": [], "entities": [{"text": "CoNLL-2000", "start_pos": 5, "end_pos": 15, "type": "DATASET", "confidence": 0.8984999060630798}]}, {"text": "In this paper, we will apply functional chunks to Chinese shallow parsing.", "labels": [], "entities": [{"text": "Chinese shallow parsing", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6248526573181152}]}, {"text": "Functional chunks are defined as a series of non-overlapping, non-nested functional units in a sentence, such as subjects, predicates, objects, adverbs, complements and soon.", "labels": [], "entities": [{"text": "Functional chunks are defined as a series of non-overlapping, non-nested functional units in a sentence, such as subjects, predicates, objects, adverbs, complements and", "start_pos": 0, "end_pos": 168, "type": "Description", "confidence": 0.7651303184443506}]}, {"text": "These units represent the implicit grammatical relations between the sentence-level predicates and their arguments.", "labels": [], "entities": []}, {"text": "Different from the basic chunks defined by, functional chunks are generated from a top-down scheme, and thus their constitutions maybe very complex.", "labels": [], "entities": []}, {"text": "In addition, the type of a functional chunk could not be simply determined by its constitution, but depends heavily on the context.", "labels": [], "entities": []}, {"text": "Therefore, we will have new challenges in the functional chunk parsing.", "labels": [], "entities": [{"text": "functional chunk parsing", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6376184622446696}]}, {"text": "first introduced the machine learning techniques to chunking problem.", "labels": [], "entities": [{"text": "chunking problem", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.9233302772045135}]}, {"text": "By formulating the NP-chunking task as a tagging process, they marked each word with a tag from set {B, I, O}, and successfully applied TBL to it.", "labels": [], "entities": []}, {"text": "Inspired by their work, we introduce SVM algorithm to our functional chunking problem.", "labels": [], "entities": [{"text": "functional chunking problem", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.7143990993499756}]}, {"text": "Instead of using the BIO tagging system, we propose anew model for solving this problem.", "labels": [], "entities": [{"text": "BIO tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.6482824087142944}]}, {"text": "In this model, we do not tag the words with BIO tags, but directly discover the chunk boundaries between every two adjacent functional chunks.", "labels": [], "entities": []}, {"text": "Each of these chunk boundaries will be assigned a type to it, which contains the information of the functional chunk types before and after it.", "labels": [], "entities": []}, {"text": "Then we further decompose this model into a series of sub modules, each of which is in charge of detecting only one type of the chunk boundaries.", "labels": [], "entities": []}, {"text": "As each sub module can be modeled as a binary classifier, various machine learning techniques could be applied.", "labels": [], "entities": []}, {"text": "In our experiments, we focus on the subjectpredicate (SP) and predicate-object (PO) boundary detection tasks, which are the most difficult but important parts in our parsing model.", "labels": [], "entities": [{"text": "predicate-object (PO) boundary detection", "start_pos": 62, "end_pos": 102, "type": "TASK", "confidence": 0.6668069064617157}]}, {"text": "By applying SVM algorithm to these tasks, we achieve the best F-Score of 76.56% and 82.26% respectively.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9995929598808289}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we give a brief introduction to the concept of our functional chunks.", "labels": [], "entities": []}, {"text": "In section 3, we propose the parsing model for Chinese functional chunk parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9734269380569458}, {"text": "Chinese functional chunk parsing", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.5387514531612396}]}, {"text": "In section 4, we compare SVM with several other machine learning techniques, and illustrate how competitive SVM is in our chunking task.", "labels": [], "entities": []}, {"text": "In section 5, we build 2 sub modules based on SVM algorithm for SP and PO boundary detection tasks.", "labels": [], "entities": [{"text": "PO boundary detection tasks", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6653330102562904}]}, {"text": "In section 6, some related work on functional chunk parsing is introduced.", "labels": [], "entities": [{"text": "functional chunk parsing", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6711126963297526}]}, {"text": "Section 7 is the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus we use here is a collection of 185 news files which are manually corrected after automatic sentence-split, word segmentation and part-of-speech tagging.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.6994101852178574}, {"text": "part-of-speech tagging", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7026638239622116}]}, {"text": "After these processes, they have been manually annotated with functional chunks.", "labels": [], "entities": []}, {"text": "Among the 185 files, 167 of them are taken as the training data and the remaining 18 are left as the test data, which takes up approximately 10% of all the data.", "labels": [], "entities": []}, {"text": "In our experiments, we will use feature templates to describe which features are to be used in the generation of feature vectors.", "labels": [], "entities": []}, {"text": "For example, if the current feature template we use is w-1t2, then the feature vector generated at position i will take the first word on the left and the second word tag on the right as its features.", "labels": [], "entities": []}, {"text": "Before we perform any experiments, all the data have been converted to the vectors that are acceptable by different machine learning algorithms.", "labels": [], "entities": []}, {"text": "Thus we have a total number of 199268 feature vectors generated from the 185 files.", "labels": [], "entities": []}, {"text": "Among them, 172465 vectors are in the training data and 26803 vectors are in the test data.", "labels": [], "entities": []}, {"text": "Two sets of training and test data are prepared respectively for the SP and PO boundary detection tasks.", "labels": [], "entities": [{"text": "SP", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9100764393806458}, {"text": "PO boundary detection", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.749499499797821}]}, {"text": "The performance of each experiment is measured with 3 rates: precision, recall and F \u03b2=1 , where precision is the percentage of detected boundaries that are correct, recall is the percentage of boundaries in the test data that are found by the parser, and F \u03b2=1 is defined as F \u03b2 =(\u03b2 2 +1)*precision*recall/(\u03b2 2 *precision + recall) with \u03b2=1.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996421337127686}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9983394145965576}, {"text": "F \u03b2", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9877580106258392}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9973123073577881}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.998175859451294}, {"text": "precision", "start_pos": 290, "end_pos": 299, "type": "METRIC", "confidence": 0.9558568000793457}, {"text": "recall", "start_pos": 300, "end_pos": 306, "type": "METRIC", "confidence": 0.5522804856300354}, {"text": "precision + recall)", "start_pos": 313, "end_pos": 332, "type": "METRIC", "confidence": 0.851483017206192}]}], "tableCaptions": [{"text": " Table 3. Length Distribution of S, O and D  Chunks.  Chunk Length # of S # of O # of D  1  5322 3537  12147  2  2093 2228  2499  3  1402 2117  1431  4  917  1624  1010  5  627  1108  696  >5  1559 3675  2013  Sum  11920 14289 19796", "labels": [], "entities": [{"text": "Sum  11920 14289 19796", "start_pos": 210, "end_pos": 232, "type": "DATASET", "confidence": 0.6451771184802055}]}, {"text": " Table 4. The 5 Most Frequently Used Bounda- ries in the Corpus.  Boundary Type Count  PO  14209  DP  11459  SD  6156  DD  5238  SP  5233  The top 5 boundaries take up 67.76% of all the  62418 boundaries in our corpus. If we further  investigate the chunk types associated with these  boundaries, we can find that only four types are  involved: P, D, O and S. Referred to", "labels": [], "entities": [{"text": "Boundary Type Count  PO  14209  DP  11459  SD  6156  DD  5238  SP  5233", "start_pos": 66, "end_pos": 137, "type": "DATASET", "confidence": 0.8471649059882531}]}, {"text": " Table 5. Results of Different Algorithms in SP  Boundary Detection Task.  Algorithm  Precision Recall  F \u03b2=1  SVM  82.21%  57.10% 67.39%  ID3  67.60%  50.70% 57.94%  C4.5  81.10%  44.60% 57.55%  Na\u00efve Bayes 47.90%  51.00% 49.40%  Table 6. Results of Different Algorithms in  PO Boundary Detection Task.  Algorithm  Precision Recall  F \u03b2=1  C4.5  72.00%  74.70% 73.33%  SVM  67.27%  64.96% 66.09%  ID3  70.70%  59.90% 64.85%  Na\u00efve Bayes 48.10%", "labels": [], "entities": [{"text": "SP  Boundary Detection Task", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6234600245952606}, {"text": "PO Boundary Detection Task", "start_pos": 276, "end_pos": 302, "type": "TASK", "confidence": 0.7063350528478622}]}, {"text": " Table 7. Results of Different Algorithms in SP  Boundary Detection Task.  Algorithm Precision Recall  F \u03b2=1  SVM  82.25%  61.22% 70.19%  ID3  64.70%  51.70% 57.47%  C4.5  79.70%  37.40% 50.91%  Table 8. Results of Different Algorithms in  PO Boundary Detection Task.  Algorithm Precision Recall  F \u03b2=1  SVM  74.83%  86.99% 80.45%  C4.5  67.90%  79.90% 73.41%  ID3", "labels": [], "entities": [{"text": "PO Boundary Detection Task", "start_pos": 240, "end_pos": 266, "type": "TASK", "confidence": 0.7229796946048737}]}, {"text": " Table 9. SP Boundary Detection Results.  Feature template Precision Recall  F \u03b2=1  t-2t-1t1t2  76.25% 51.99% 61.83%  w-2w-1w1w2t- 2t-1t1t2", "labels": [], "entities": [{"text": "SP Boundary Detection", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5739643573760986}, {"text": "Precision Recall  F \u03b2", "start_pos": 59, "end_pos": 80, "type": "METRIC", "confidence": 0.8530476242303848}]}, {"text": " Table 11. Wrongly Detected Chunk Bounda- ries in the Test Results of T1 and T4.  CT  #WDB of T1 #WDB of T4 T4-T1  O  17  18  1  S  17  18  1  D  7  6  -1  C  0  1  1  P  2  1  -1  T  1  1  0  Sum 44  45  1  From the above table, we find that the number  of wrongly detected boundaries seems to be un- changed during the expansion of context window.", "labels": [], "entities": [{"text": "CT  #WDB", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.7509071826934814}]}, {"text": " Table 13. Misclassified Chunk Boundaries in  the Test Results of T1 and T4.  MBT #MB of T1 #MB of T4 T4-T1  PJ  17  18  1  PD  9  9  0  PC  8  8  0  SP  6  6  0  PS  5  5  0  SD  5  4  -1  DP  3  2  -1  TS  3  3  0  OD  1  0  -1  PY  1  1  0  Sum 58  56  -2", "labels": [], "entities": [{"text": "MBT", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.8967172503471375}, {"text": "MB", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9096206426620483}, {"text": "OD  1  0  -1  PY  1  1  0  Sum 58  56  -", "start_pos": 217, "end_pos": 257, "type": "METRIC", "confidence": 0.7651425279103793}]}]}