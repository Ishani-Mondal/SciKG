{"title": [{"text": "Study of Some Distance Measures for Language and Encoding Identification", "labels": [], "entities": [{"text": "Some Distance Measures", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.704446812470754}, {"text": "Language and Encoding Identification", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.6366718113422394}]}], "abstractContent": [{"text": "To determine how close two language models (e.g., n-grams models) are, we can use several distance measures.", "labels": [], "entities": []}, {"text": "If we can represent the models as distributions, then the similarity is basically the similarity of distributions.", "labels": [], "entities": [{"text": "similarity", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9853129982948303}, {"text": "similarity", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9729472398757935}]}, {"text": "And a number of measures are based on information theo-retic approach.", "labels": [], "entities": []}, {"text": "In this paper we present some experiments on using such similarity measures for an old Natural Language Processing (NLP) problem.", "labels": [], "entities": []}, {"text": "One of the measures considered is perhaps a novel one, which we have called mutual cross entropy.", "labels": [], "entities": []}, {"text": "Other measures are either well known or based on well known measures, but the results obtained with them visa -vis one-another might help in gaining an insight into how similarity measures work in practice.", "labels": [], "entities": []}, {"text": "The first step in processing a text is to identify the language and encoding of its contents.", "labels": [], "entities": []}, {"text": "This is a practical problem since for many languages, there are no universally followed text encoding standards.", "labels": [], "entities": []}, {"text": "The method we have used in this paper for language and encoding identification uses pruned character n-grams, alone as well augmented with word n-grams.", "labels": [], "entities": [{"text": "language and encoding identification", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.6639615446329117}]}, {"text": "This method seems to give results comparable to other methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many kinds of models in NLP can be seen as distributions of a variable.", "labels": [], "entities": []}, {"text": "For various NLP problems, we need to calculate the similarity of such models or distributions.", "labels": [], "entities": [{"text": "similarity", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9788697361946106}]}, {"text": "One common example of this is the n-grams model.", "labels": [], "entities": []}, {"text": "We might have several reference data sets and then we may want to find out which of those matches most closely with a test data set.", "labels": [], "entities": []}, {"text": "The problem of language and encoding identification can be represented in these terms.", "labels": [], "entities": [{"text": "language and encoding identification", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.6322355791926384}]}, {"text": "One of the most important questions then is which similarity measure to use.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 50, "end_pos": 68, "type": "METRIC", "confidence": 0.9311112463474274}]}, {"text": "We can expect that the performance obtained with the similarity measure will vary with the specific problem and the kind of model used or some other problem specific details.", "labels": [], "entities": []}, {"text": "Still, it will be useful to explore how these measures relate to each other.", "labels": [], "entities": []}, {"text": "The measures we are going to focus on in this paper are all very simple ones and they all try to find the similarity of two models or distributions in a (more or less) information theoretic way, except the out of rank measure proposed by.", "labels": [], "entities": []}, {"text": "This work had started simply as an effort to build a language and encoding identification tool specifically for South Asian languages.", "labels": [], "entities": [{"text": "language and encoding identification", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.6575154885649681}]}, {"text": "During the course of this work, we experimented with various similarity measures and some of the results we obtained were at least a bit surprising.", "labels": [], "entities": []}, {"text": "One of the measures we used was something we have called mutual cross entropy and its performance for the current problem was better than other measures.", "labels": [], "entities": []}, {"text": "Before the content of a Web page or of any kind of text can be processed for computation, its language and encoding has to be known.", "labels": [], "entities": []}, {"text": "In many cases this language-encoding is not known beforehand and has to be determined automatically.", "labels": [], "entities": []}, {"text": "For languages like Hindi, there is no standard encoding followed by everyone.", "labels": [], "entities": []}, {"text": "There are many well known web sites using their own proprietary encoding.", "labels": [], "entities": []}, {"text": "This is one of the biggest problems in actually using the Web as a multilingual corpus and for enabling a crawler to search the text in lan-guages like Hindi.", "labels": [], "entities": []}, {"text": "This means that the content in these languages, limited as it is, is invisible not just to people (which could be just due to lack of display support or unavailability of fonts fora particular encoding) but even to crawlers.", "labels": [], "entities": []}, {"text": "The problem of language identification is similar to some other problems in different fields and the techniques used for one such problem have been found to be effective for other problems too.", "labels": [], "entities": [{"text": "language identification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7613851428031921}]}, {"text": "Some of these problems are text categorization, cryptanalysis and even species identification from genetic sequences.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7891624569892883}, {"text": "cryptanalysis", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.969896674156189}, {"text": "species identification from genetic sequences", "start_pos": 71, "end_pos": 116, "type": "TASK", "confidence": 0.8218218326568604}]}, {"text": "This means that if something works for one of these problems, it is likely to work for these other problems.", "labels": [], "entities": []}, {"text": "It should be noted here that the identification problem here is that of identifying both language and encoding.", "labels": [], "entities": []}, {"text": "This is because (especially for South Asian languages) the same encoding can be used for more than one languages (ISCII for all Indian languages which use Brahmi-origin scripts) and one language can have many encodings (ISCII, Unicode, ISFOC, typewriter, phonetic, and many other proprietary encodings for Hindi).", "labels": [], "entities": []}, {"text": "In this paper we describe a method based mainly on character n-grams for identifying the language-encoding pair of a text.", "labels": [], "entities": []}, {"text": "The method requires some training text for each languageencoding, but this text need not have the same content.", "labels": [], "entities": []}, {"text": "A few pages (2500-10000 words) of text in a particular language-encoding is enough.", "labels": [], "entities": []}, {"text": "A pruned character based n-grams model is created for each language-encoding.", "labels": [], "entities": []}, {"text": "A similar model is created for the test data too and is compared to the training models.", "labels": [], "entities": []}, {"text": "The best match is found using a similarity measure.", "labels": [], "entities": [{"text": "similarity", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9490935802459717}]}, {"text": "A few (5-15) words of test data seems to be enough for identification inmost cases.", "labels": [], "entities": [{"text": "identification", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.9704092741012573}]}, {"text": "The method has been evaluated using various similarity measures and for different test sizes.", "labels": [], "entities": []}, {"text": "We also consider two cases, one in which the pruned character n-grams model is used alone, and the other in which it is augmented with a word n-gram model.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation was performed for all the measures listed earlier.", "labels": [], "entities": []}, {"text": "These are repeated herewith a code for easy reference in table-3.", "labels": [], "entities": []}, {"text": "The number of language-encoding pairs was 53 and the minimum number of test data sets was 840 when we used all available test data.", "labels": [], "entities": []}, {"text": "In other cases, the number was naturally larger as the test files were split in fragments (see table-2).", "labels": [], "entities": []}, {"text": "The languages considered ranged from Esperanto and Modern Greek to Hindi and Telugu.", "labels": [], "entities": []}, {"text": "For Indian languages, especially Hindi, several encodings were tested.", "labels": [], "entities": []}, {"text": "Some of the pairs had UTF8 as the encoding, but the information from UTF8 byte format was not explicitly used for identification.", "labels": [], "entities": []}, {"text": "The number of languages tested was 39 and number encodings was 19.", "labels": [], "entities": []}, {"text": "Total number of language-encoding pairs was 53 (see table-1).", "labels": [], "entities": []}, {"text": "The test and training data for about half of the pairs was collected from web pages (such as Gutenberg).", "labels": [], "entities": []}, {"text": "For Indian languages, most (but not all) data was from what is known as the CIIL corpus.", "labels": [], "entities": [{"text": "CIIL corpus", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.9422160387039185}]}, {"text": "We didn't test on various training data sizes.", "labels": [], "entities": []}, {"text": "The size of the training data ranged from 2495 to 102377 words, with more on the lower side than on the higher.", "labels": [], "entities": []}, {"text": "Note that we have considered the case where both the language and the encoding are unknown, not where one of them is known.", "labels": [], "entities": []}, {"text": "In the latter case, the performance can only improve.", "labels": [], "entities": []}, {"text": "Another point worth mentioning is that the training data was not very clean, i.e., it had noise (such as words or sentences from other languages).", "labels": [], "entities": []}, {"text": "Error details have been given in table-4.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9716416597366333}]}], "tableCaptions": [{"text": " Table 2: NUMBER OF TEST SETS  Size  Number  100  22083  200  10819  500  4091  1000  1867  2000  1524  All test data  840", "labels": [], "entities": [{"text": "TEST SETS  Size  Number  100  22083  200  10819  500  4091  1000  1867  2000  1524  All test data  840", "start_pos": 20, "end_pos": 122, "type": "DATASET", "confidence": 0.6306211352348328}]}, {"text": " Table 3: PRECISION FOR VARIOUS MEASURES AND TEST SIZES  Precision  Test Size (characters) LPD ALPD  CE  RE  CT  JC  MRE MCE", "labels": [], "entities": [{"text": "VARIOUS", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9648230671882629}, {"text": "MEASURES", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.7907186150550842}, {"text": "TEST", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9543811082839966}, {"text": "LPD ALPD  CE  RE  CT  JC  MRE MCE", "start_pos": 91, "end_pos": 124, "type": "METRIC", "confidence": 0.7600187882781029}]}]}