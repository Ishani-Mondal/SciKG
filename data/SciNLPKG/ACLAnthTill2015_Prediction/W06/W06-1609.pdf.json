{"title": [], "abstractContent": [{"text": "Reordering is currently one of the most important problems in statistical machine translation systems.", "labels": [], "entities": [{"text": "Reordering", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9566595554351807}, {"text": "statistical machine translation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.6887224117914835}]}, {"text": "This paper presents a novel strategy for dealing with it: statistical machine reordering (SMR).", "labels": [], "entities": [{"text": "statistical machine reordering (SMR)", "start_pos": 58, "end_pos": 94, "type": "TASK", "confidence": 0.8198492030302683}]}, {"text": "It consists in using the powerful techniques developed for statistical machine translation (SMT) to translate the source language (S) into a reordered source language (S'), which allows for an improved translation into the target language (T).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.7942915658156077}]}, {"text": "The SMT task changes from S2T to S'2T which leads to a monotonized word alignment and shorter translation units.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.889678567647934}]}, {"text": "In addition, the use of classes in SMR helps to infer new word reorderings.", "labels": [], "entities": [{"text": "SMR", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9882397651672363}]}, {"text": "Experiments are reported in the EsEn WMT06 tasks and the ZhEn IWSLT05 task and show significant improvement in translation quality.", "labels": [], "entities": [{"text": "EsEn WMT06 tasks", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.554162989060084}, {"text": "ZhEn IWSLT05 task", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.7789759437243143}, {"text": "translation", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.952994704246521}]}], "introductionContent": [{"text": "During the last few years, SMT systems have evolved from the original word-based approach () to phrase-based translation systems (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9945107698440552}, {"text": "phrase-based translation", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7492278814315796}]}, {"text": "In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by.", "labels": [], "entities": []}, {"text": "Two basic issues differentiate the n-gram-based system from the phrasebased: training data are monotonously segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies.", "labels": [], "entities": []}, {"text": "This translation approach is described in detail by.", "labels": [], "entities": [{"text": "translation", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9730463027954102}]}, {"text": "The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is implemented (, as an alternative to the source-channel approach.", "labels": [], "entities": []}, {"text": "In both systems, introducing reordering capabilities is of crucial importance for certain language pairs.", "labels": [], "entities": []}, {"text": "Recently, new reordering strategies have been proposed in the literature on SMT such as the reordering of each source sentence to match the word order in the corresponding target sentence, see and.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9912295341491699}]}, {"text": "Similarly, describe a method for simultaneously aligning and monotonizing the training corpus.", "labels": [], "entities": []}, {"text": "The main problems of these approaches are: (1) the fact that the proposed monotonization is based on the alignment and cannot be applied to the test sets, and (2) the lack of reordering generalization.", "labels": [], "entities": []}, {"text": "This paper presents a reordering approach called statistical machine reordering (SMR) which improves the reordering capabilities of SMT systems without incurring any of the problems mentioned above.", "labels": [], "entities": [{"text": "statistical machine reordering (SMR)", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.8105575342973074}, {"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9809814691543579}]}, {"text": "SMR is a first-pass translation performed on the source corpus, which converts it into an intermediate representation, in which source-language words are presented in an order that more closely matches that of the target language.", "labels": [], "entities": [{"text": "SMR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9584527015686035}]}, {"text": "SMR and SMT are performed using the same modeling tools as n-gram-based systems but using different statistical log-linear models.", "labels": [], "entities": [{"text": "SMR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9782203435897827}, {"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9857165813446045}]}, {"text": "In order to be able to infer new reorderings we use word classes instead of words themselves as the input to the SMR system.", "labels": [], "entities": [{"text": "SMR", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9862122535705566}]}, {"text": "In fact, the use of classes to help in the reordering is a key difference between our approach and standard SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9889340996742249}]}, {"text": "This paper is organized as follows: Section 2 outlines the baseline system.", "labels": [], "entities": []}, {"text": "Section 3 describes the reordering strategy in detail.", "labels": [], "entities": [{"text": "reordering", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.9667586088180542}]}, {"text": "Section 4 presents and discusses the results, and Section 5 presents our conclusions and suggestions for further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present experiments carried out using the EsEn WMT06 and the ZhEn IWSLT05 parallel corpus.", "labels": [], "entities": [{"text": "EsEn WMT06", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.6712782084941864}, {"text": "ZhEn IWSLT05 parallel corpus", "start_pos": 81, "end_pos": 109, "type": "DATASET", "confidence": 0.8009288161993027}]}, {"text": "We detail the tools which have been used and the corpus statistics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spanish to English task. EuroParl cor- pus: training, development and test data sets.", "labels": [], "entities": [{"text": "EuroParl cor- pus", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.8237898051738739}]}, {"text": " Table 2: Chinese to English task. BTEC corpus:  training, development and test data sets. Develop- ment and test data sets have 16 references.", "labels": [], "entities": [{"text": "BTEC corpus", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.9473245441913605}]}, {"text": " Table 3: Vocabulary of n-grams and embedded words in the translation model.", "labels": [], "entities": []}, {"text": " Table 4: Tuples used to translate the test set (total  number and vocabulary).", "labels": [], "entities": []}, {"text": " Table 5: Results in the test set of the EsEn task using a monotonous search.", "labels": [], "entities": []}, {"text": " Table 6: Results in the test set of the ZhEn task using a monotonous and a non-monotonous search.", "labels": [], "entities": []}]}