{"title": [{"text": "The Affect of Machine Translation on the Performance of Arabic- English QA System", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.6951840370893478}, {"text": "Performance of Arabic- English QA", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.6350757678349813}]}], "abstractContent": [{"text": "The aim of this paper is to investigate how much the effectiveness of a Question Answering (QA) system was affected by the performance of Machine Translation (MT) based question translation.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.8219713926315307}, {"text": "Machine Translation (MT) based question translation", "start_pos": 138, "end_pos": 189, "type": "TASK", "confidence": 0.8488258048892021}]}, {"text": "Nearly 200 questions were selected from TREC QA tracks and ran through a question answering system.", "labels": [], "entities": [{"text": "TREC QA tracks", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.7733092705408732}]}, {"text": "It was able to answer 42.6% of the questions correctly in a monolingual run.", "labels": [], "entities": []}, {"text": "These questions were then translated manually from Eng-lish into Arabic and back into English using an MT system, and then reapplied to the QA system.", "labels": [], "entities": []}, {"text": "The system was able to answer 10.2% of the translated questions.", "labels": [], "entities": []}, {"text": "An analysis of what sort of translation error affected which questions was conducted , concluding that factoid type questions are less prone to translation error than others.", "labels": [], "entities": [{"text": "translation error", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.8242164552211761}]}], "introductionContent": [{"text": "Increased availability of on-line text in languages other than English and increased multi-national collaboration have motivated research in CrossLanguage Information Retrieval (CLIR).", "labels": [], "entities": [{"text": "CrossLanguage Information Retrieval (CLIR)", "start_pos": 141, "end_pos": 183, "type": "TASK", "confidence": 0.7523526599009832}]}, {"text": "The goal of CLIR is to help searchers find relevant documents when their query terms are chosen from a language different from the language in which the documents are written.", "labels": [], "entities": []}, {"text": "Multilinguality has been recognized as an important issue for the future of QA ().", "labels": [], "entities": []}, {"text": "The multilingual QA task was introduced for the first time in the Cross-Language Evaluation Forum CLEF-2003.", "labels": [], "entities": [{"text": "QA task", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.7502480447292328}, {"text": "Cross-Language Evaluation Forum CLEF-2003", "start_pos": 66, "end_pos": 107, "type": "TASK", "confidence": 0.5237349942326546}]}, {"text": "According to the Global Reach website (2004), shown in, it could be estimated that an English speaker has access to around 23 times more digital documents than an Arabic speaker.", "labels": [], "entities": [{"text": "Global Reach website (2004)", "start_pos": 17, "end_pos": 44, "type": "DATASET", "confidence": 0.9207069873809814}]}, {"text": "One can conclude from the given information shown in the Figure that cross-language is potentially very useful when the required information is not available in the users' language.", "labels": [], "entities": []}, {"text": "The goal of a QA system is to find answers to questions in a large collection of documents.", "labels": [], "entities": []}, {"text": "The overall accuracy of a QA system is directly affected by its ability to correctly analyze the questions it receives as input, a Cross Language Question Answering (CLQA) system will be sensitive to any errors introduced during question translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9990322589874268}, {"text": "Cross Language Question Answering (CLQA)", "start_pos": 131, "end_pos": 171, "type": "TASK", "confidence": 0.7382340984685081}, {"text": "question translation", "start_pos": 229, "end_pos": 249, "type": "TASK", "confidence": 0.7245092391967773}]}, {"text": "Many researchers criticize the MTbased CLIR approach.", "labels": [], "entities": [{"text": "MTbased CLIR", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.5860985517501831}]}, {"text": "The reason for their criticism mostly stem from the fact that the current translation quality of MT is poor, in addition MT system are expensive to develop and their application degrades the retrieval efficiency due to the cost of the linguistic analysis.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9220329523086548}]}], "datasetContent": [{"text": "To run this experiment, 199 questions were randomly compiled from the TREC QA track, namely from TREC-8, TREC-9, TREC-11, TREC-2003 and TREC-2004, to be run through AnswerFinder, the results of which are discussed in section 3.1.", "labels": [], "entities": [{"text": "TREC QA track", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.7497544487317404}]}, {"text": "The selected 199 English TREC questions were translated into Arabic by one of the authors (who is an Arabic speaker), and then fed into Systran to translate them into English.", "labels": [], "entities": []}, {"text": "The analysis of translation is discussed in detail in section 3.2.", "labels": [], "entities": [{"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.8575750589370728}]}], "tableCaptions": [{"text": " Table 1. Overall view of AnswerFinder Mono- lingual Accuracy", "labels": [], "entities": []}, {"text": " Table 3. Types of Translation Errors", "labels": [], "entities": [{"text": "Translation Errors", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7483464479446411}]}, {"text": " Table 4. Incorrect use of translation when trans- literation should have been used", "labels": [], "entities": []}, {"text": " Table 5. Wrong translation of person's name", "labels": [], "entities": [{"text": "translation of person's name", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7755348682403564}]}, {"text": " Table 11. Types of questions translated and an- swered correctly in the bilingual run", "labels": [], "entities": []}]}