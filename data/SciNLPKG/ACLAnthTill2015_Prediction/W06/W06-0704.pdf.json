{"title": [{"text": "Situated Question Answering in the Clinical Domain: Selecting the Best Drug Treatment for Diseases", "labels": [], "entities": [{"text": "Situated Question Answering", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7229863405227661}]}], "abstractContent": [{"text": "Unlike open-domain factoid questions, clinical information needs arise within the rich context of patient treatment.", "labels": [], "entities": []}, {"text": "This environment establishes a number of constraints on the design of systems aimed at physicians in real-world settings.", "labels": [], "entities": []}, {"text": "In this paper, we describe a clinical question answering system that focuses on a class of commonly-occurring questions: \"What is the best drug treatment for X?\", where X can be any disease.", "labels": [], "entities": [{"text": "clinical question answering", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7197389205296835}]}, {"text": "To evaluate our system, we built a test collection consisting of thirty randomly-selected diseases from an existing secondary source.", "labels": [], "entities": []}, {"text": "Both an automatic and a manual evaluation demonstrate that our system compares favorably to PubMed, the search system most commonly-used by physicians today.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.9304283261299133}]}], "introductionContent": [{"text": "Over the past several years, question answering (QA) has emerged as a general framework for addressing users' information needs.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.9111021757125854}]}, {"text": "Instead of returning \"hits\", as information retrieval systems do, QA systems respond to natural language questions with concise, targeted information.", "labels": [], "entities": []}, {"text": "Recently, research focus has shifted away from so-called factoid questions such as \"What are pennies made of?\" and \"What country is Aswan High Dam located in?\" to more complex questions such as \"How have South American drug cartels been using banks in Liechtenstein to launder money?\" and \"What was the Pentagon panel's position with respect to the dispute over the US Navy training range on the island of Vieques?\"-so-called \"relationship\" and \"opinion\" questions, respectively.", "labels": [], "entities": [{"text": "Aswan High Dam", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.8045526742935181}]}, {"text": "These complex information needs differ from factoid questions in many important ways.", "labels": [], "entities": []}, {"text": "Unlike factoids, they cannot be answered by namedentities and other short noun phrases.", "labels": [], "entities": []}, {"text": "They do not occur in isolation, but are rather embedded within a broader context, i.e., a \"scenario\".", "labels": [], "entities": []}, {"text": "These complex questions set forth parameters of the desired knowledge, which may include additional facts about the motivation of the information seeker, her assumptions, her current state of knowledge, etc.", "labels": [], "entities": []}, {"text": "Presently, most systems that attempt to tackle such complex questions are aimed at serving intelligence analysts, for activities such as counterterrorism and war-fighting.", "labels": [], "entities": []}, {"text": "Systems for addressing complex information needs are interesting because they provide an opportunity to explore the role of semantic structures in question answering, e.g.,).", "labels": [], "entities": [{"text": "question answering", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.8107525706291199}]}, {"text": "Opportunities include explicit semantic representations for capturing the content of questions and documents, deep inferential mechanisms (, and attempts to model task-specific influences in informationseeking environments).", "labels": [], "entities": []}, {"text": "Our own interest in question answering falls inline with these recent developments, but we focus on a different type of user-the primary care physician.", "labels": [], "entities": [{"text": "question answering", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.8513486683368683}]}, {"text": "The need to answer questions related to patient care at the point of service has been well studied and documented).", "labels": [], "entities": []}, {"text": "However, research has shown that existing search systems, e.g., PubMed, are often unable to supply clinically-relevant answers in a timely manner.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9634348750114441}]}, {"text": "Clinical question answering represents a high-impact application that has the potential to improve the quality of medical care.", "labels": [], "entities": [{"text": "Clinical question answering", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6256489058335623}]}, {"text": "From a research perspective, the clinical domain is attractive because substantial medical knowledge has already been codified in the Unified Medical Language System (UMLS).", "labels": [], "entities": []}, {"text": "This large ontology enables us to explore knowledge-rich techniques and move beyond question answering methods primarily driven by keyword matching.", "labels": [], "entities": [{"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7583425343036652}, {"text": "keyword matching", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.7180012613534927}]}, {"text": "In this work, we describe a paradigm of medical practice known as evidence-based medicine and explain how it can be computationally captured in a semantic domain model.", "labels": [], "entities": []}, {"text": "Two separate evaluations demonstrate that semantic modeling yields gains in question answering performance.", "labels": [], "entities": [{"text": "semantic modeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8488645553588867}, {"text": "question answering", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7148333638906479}]}], "datasetContent": [{"text": "Clinical Evidence (CE) is a periodic report created by the British Medical Journal (BMJ) Publishing Group that summarizes the best treatments fora few dozen diseases at the time of publication.", "labels": [], "entities": [{"text": "Clinical Evidence (CE)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.47134165167808534}, {"text": "British Medical Journal (BMJ) Publishing Group", "start_pos": 59, "end_pos": 105, "type": "DATASET", "confidence": 0.9570197761058807}]}, {"text": "We were able to mine the June 2004 edition to create a test collection to evaluate our system.", "labels": [], "entities": [{"text": "June 2004 edition", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.6957309742768606}]}, {"text": "Note that the existence of such secondary sources does not obviate the need for clinical question answering because they are perpetually falling out of date due to rapid advances in medicine.", "labels": [], "entities": [{"text": "clinical question answering", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6403051614761353}]}, {"text": "Furthermore, such reports are currently created by highlyexperienced physicians, which is an expensive and time-consuming process.", "labels": [], "entities": []}, {"text": "From CE, we randomly extracted thirty diseases, creating a development set of five questions and a test set of twenty-five questions.", "labels": [], "entities": [{"text": "CE", "start_pos": 5, "end_pos": 7, "type": "DATASET", "confidence": 0.8714262843132019}]}, {"text": "Some examples include: acute asthma, chronic prostatitis, community acquired pneumonia, and erectile dysfunction.", "labels": [], "entities": []}, {"text": "We conducted two evaluations-one automatic and one manual-that compare the original PubMed hits and the output of our semantic matcher.", "labels": [], "entities": [{"text": "PubMed hits", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.9395193159580231}]}, {"text": "The first evaluation is based on ROUGE, a commonly-used summarization metric that computes the unigram overlap between a particular text and one or more reference texts.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9927195310592651}]}, {"text": "The treatment overview for each disease in CE is accompanied by a number of citations (used in writing the overview itself)-the abstract texts of these cited articles serve as our references.", "labels": [], "entities": []}, {"text": "We adopt this approach because medical journals require abstracts that provide factual information summarizing the main points of the studies.", "labels": [], "entities": []}, {"text": "We assume that the closer an abstract is to these reference abstracts (as measured by ROUGE-1 precision), the more relevant it is.", "labels": [], "entities": [{"text": "ROUGE-1 precision", "start_pos": 86, "end_pos": 103, "type": "METRIC", "confidence": 0.7813341617584229}]}, {"text": "On average, each disease overview contains 48.4 citations; however, we were only able to gather abstracts of those that were contained in MEDLINE (34.7 citations per disease, min 8, max 100).", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.7021180391311646}]}, {"text": "For evaluation purposes, we restricted abstracts under consideration to those that were published before our edition of CE.", "labels": [], "entities": [{"text": "CE", "start_pos": 120, "end_pos": 122, "type": "DATASET", "confidence": 0.5892287492752075}]}, {"text": "To quantify the performance of our system, we computed the average ROUGE score over the top one, three, five, and ten hits of our EBM and baseline systems.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9770856201648712}]}, {"text": "To supplement our automatic evaluation, we also conducted a double-blind manual evaluation", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of automatic evaluation: average ROUGE score using cited abstracts in CE as references.  The EBM column represents performance of our complete domain model. PICO, SoE, and MeSH rep- resent performance of each component. ( \u2022 denotes n.s., denotes sig. at 0.95, denotes sig. at 0.99)", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9701554179191589}]}]}