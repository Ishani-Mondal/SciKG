{"title": [{"text": "Enhanced Interactive Question-Answering with Conditional Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes anew methodology for enhancing the quality and relevance of suggestions provided to users of interactive Q/A systems.", "labels": [], "entities": []}, {"text": "We show that by using Conditional Random Fields to combine relevance feedback gathered from users along with information derived from discourse structure and coherence, we can accurately identify irrelevant suggestions with nearly 90% F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 235, "end_pos": 244, "type": "METRIC", "confidence": 0.9978330731391907}]}], "introductionContent": [{"text": "Today's interactive question-answering (Q/A) systems enable users to pose questions in the context of extended dialogues in order to obtain information relevant to complex research scenarios.", "labels": [], "entities": []}, {"text": "When working with an interactive Q/A system, users formulate sequences of questions which they believe will return answers that will let them reach certain information goals.", "labels": [], "entities": []}, {"text": "Users need more than answers, however: while they might be cognizant of many of the different types of information that they need, few -if anyusers are capable of identifying all of the questions that must be asked and answered fora particular scenario.", "labels": [], "entities": []}, {"text": "In order to take full advantage of the Q/A capabilities of current systems, users need access to sources of domain-specific knowledge that will expose them to new concepts and ideas and will allow them to ask better questions.", "labels": [], "entities": []}, {"text": "In previous work), we have argued that interactive questionanswering systems should be based on a predictive dialogue architecture which can be used to provide users with both precise answers to their questions as well as suggestions of relevant research topics that could be explored throughout the course of an interactive Q/A dialogue.", "labels": [], "entities": []}, {"text": "Typically, the quality of interactive Q/A dialogues has been measured in three ways: (1) efficiency, defined as the number of questions that the user must pose to find particular information, (2) effectiveness, defined by the relevance of the answer returned, and (3) user satisfaction (.", "labels": [], "entities": []}, {"text": "In our experiments with an interactive Q/A system, (known as FERRET), we found that performance in each of these areas improves as users are provided with suggestions that are relevant to their domain of interest.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9339779615402222}]}, {"text": "In FERRET, suggestions are made to users in the form of predictive questionanswer pairs (known as QUABs) which are either generated automatically from the set of documents returned fora query (using techniques first described in (), or are selected from a large database of questions-answer pairs created offline (prior to a dialogue) by human annotators.", "labels": [], "entities": []}, {"text": "presents an example often QUABs that were returned by FERRET in response to the question \"How are EU countries responding to the worldwide increase of job outsourcing to India?\".", "labels": [], "entities": [{"text": "FERRET", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9446280598640442}]}, {"text": "While FERRET's QUABs are intended to provide users with relevant information about a domain of interest, we can see from that users do not always agree on which QUAB suggestions are relevant.", "labels": [], "entities": [{"text": "FERRET's QUABs", "start_pos": 6, "end_pos": 20, "type": "DATASET", "confidence": 0.9130281408627828}]}, {"text": "For example, while someone unfamiliar to the notion of \"job outsourcing\" could benefit from  a QUAB like QUAB 3 : \"What is job outsourcing?\", we expect that a more experienced researcher would find this definition to be uninformative and potentially irrelevant to his or her particular information needs.", "labels": [], "entities": []}, {"text": "In contrast, a complex QUAB like QUAB 6 : \"How could the anti-globalization movements in EU countries impact the likelihood that the EU Parliament will take steps to prevent job outsourcing to India?\" could provide a domain expert with relevant information, but would not provide enough background information to satisfy a novice user who might not be able to interpret this information in the appropriate context.", "labels": [], "entities": []}, {"text": "In this paper, we present results of anew set of experiments that seek to combine feedback gathered from users with a relevance classifier based on conditional random fields (CRF) in order to provide suggestions to users that are not only related to the topic of their interactive Q/A dialogue, but provide them with the new types of information they need to know.", "labels": [], "entities": []}, {"text": "Section 2 presents the functionality of several of FERRET's modules and describes the NLP techniques for processing questions as well as the framework for acquiring domain knowledge.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.7825814485549927}]}, {"text": "In Section 3 we present two case studies that highlight the impact of user background.", "labels": [], "entities": []}, {"text": "Section 4 describes anew class of user interaction models for interactive Q/A and presents details of our CRF-based classifier.", "labels": [], "entities": []}, {"text": "Section 5 presents results from experiments which demonstrate that user modeling can enhance the quality of suggestions provided to both expert and novice users.", "labels": [], "entities": []}, {"text": "Section 6 summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe results from two experiments that were conducted using data collected from human interactions with FERRET.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 128, "end_pos": 134, "type": "DATASET", "confidence": 0.8580553531646729}]}, {"text": "In order to evaluate the effectiveness of our relevance classifier, we gathered a total of 1000 questions from human dialogues with FERRET.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.8972710967063904}]}, {"text": "500 of these came from interactions (41 dialogues) where the user was a self-described \"expert\" on the topic; another selection of 500 questions came from a total of 23 dialogues resulting from interactions with users who described themselves as \"novice\" or were otherwise unfamiliar with a topic.", "labels": [], "entities": []}, {"text": "In order to validate the user's self-assessment, we selected 5 QUABs at random from the set of manually created QUABs assembled for each topic.", "labels": [], "entities": []}, {"text": "Users were asked to provide written answers to those questions.", "labels": [], "entities": []}, {"text": "Users that were judged to have correctly answered three out of five questions were considered \"experts\" for the purpose of our experiments.", "labels": [], "entities": []}, {"text": "Each of these experiments were run using aversion of FERRET that returned the top 10 most similar QUABs from a database that combined manuallycreated QUABs with the automatically-generated QUABs created for the user's question.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.8693815469741821}]}, {"text": "While a total of 10,000 QUABs were returned to users during these experiments, only 3,998 of these QUABs were unique (39.98%).", "labels": [], "entities": []}, {"text": "We conducted two kinds of experiments with users.", "labels": [], "entities": []}, {"text": "In the first set of experiments, users were asked to mark all of the relevant QUABs that FER-RET returned in response to questions submitted by users.", "labels": [], "entities": [{"text": "FER-RET", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.49121078848838806}]}, {"text": "After performing research on a particular scenario, expert and novice users were then supplied with as many as 65 questions (and associated QUABs) taken from previously-completed dialogues on the same scenario; users were then asked to select checkboxes associated with QUABs that were relevant.", "labels": [], "entities": []}, {"text": "In addition, we also had 2 linguists (who were familiar with all of the research scenarios but did not research any of them) perform the same task for all of the collected questions and QUABs.", "labels": [], "entities": []}, {"text": "Results from these three sets of annotations are found in  As expected, experts believed QUABs to be significantly (p < 0.05) less relevant than novices, who found approximately 38.12% of QUABs to be relevant to the original question submitted by a user.", "labels": [], "entities": []}, {"text": "In contrast, the two linguists found 44.8% of the QUABs to be relevant.", "labels": [], "entities": [{"text": "QUABs", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9063423275947571}]}, {"text": "This number maybe artificially high: since the linguists did not engage in actual Q/A dialogues for each of the scenarios they were annotating, they may not have been appropriately prepared to make a relevance assessment.", "labels": [], "entities": []}, {"text": "In the second set of experiments, we used the UIM in to train CRF-based relevance classifiers.", "labels": [], "entities": [{"text": "UIM", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.8638516664505005}, {"text": "CRF-based relevance classifiers", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7491291562716166}]}, {"text": "We obtained training data for UIM 1 (\"copy-andpaste\"-based), UIM 2 (\"click\"-based), and UIM 3 (\"hybrid\") from 16 different dialogue histories collected from 8 different novice users.", "labels": [], "entities": []}, {"text": "During these dialogues, users were asked to perform research as they normally would; no special instructions were given to users to provide additional relevance feedback to the system.", "labels": [], "entities": []}, {"text": "After the dialogues were completed, QUABs that were copied from or clicked were annotated as \"relevant\" examples (according to each UIM); the remaining QUABs were annotated as \"irrelevant\".", "labels": [], "entities": []}, {"text": "Once features (as described in) were extracted and the classifiers were trained, they were evaluated on a set of 1000 QUABs (500 \"relevant\", 500 \"irrelevant\") selected at random from the annotations performed in the first experiment.", "labels": [], "entities": [{"text": "QUABs", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.8833297491073608}]}, {"text": "Our results suggest that feedback gathered from a user's \"normal\" interactions with FERRET could be used to provide valuable input to a relevance classifier for QUABs When \"copy-and-paste\" events were used to train the classifier, the system detected instances of irrelevant QUABs with over 80% F.", "labels": [], "entities": [{"text": "F", "start_pos": 295, "end_pos": 296, "type": "METRIC", "confidence": 0.997745931148529}]}, {"text": "While we acknowledge that training models on these types of events may not always provide reliable sources of training data -especially as users copy or click on QUAB passages that may not be relevant to their interests in the research scenario, we believe the initial performance of these suggests that accurate forms of relevance feedback can be gathered without the use of mixedinitiative clarification dialogues.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Experimental Results from 3 User Models.", "labels": [], "entities": []}]}