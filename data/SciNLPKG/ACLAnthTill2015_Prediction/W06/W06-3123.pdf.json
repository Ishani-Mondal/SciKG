{"title": [{"text": "Constraining the Phrase-Based, Joint Probability Statistical Translation Model", "labels": [], "entities": [{"text": "Joint Probability Statistical Translation", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.6157022714614868}]}], "abstractContent": [{"text": "The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT).", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 108, "end_pos": 158, "type": "TASK", "confidence": 0.7391797772475651}]}, {"text": "The model's usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level.", "labels": [], "entities": []}, {"text": "We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training.", "labels": [], "entities": [{"text": "Expectation Maximization (EM) training", "start_pos": 115, "end_pos": 153, "type": "TASK", "confidence": 0.8251731296380361}]}, {"text": "Constraining the joint model improves performance, showing results that are very close to state-of-the-art phrase-based models.", "labels": [], "entities": []}, {"text": "It also allows it to scale up to larger corpora and therefore be more widely applicable.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8542555272579193}]}, {"text": "It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data.", "labels": [], "entities": []}, {"text": "The original IBM Models () learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.9051664769649506}, {"text": "word-to-word alignment probabilities", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.7263631820678711}]}, {"text": "Phrase-based SMT models, such as the alignment template model, improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.803160548210144}]}, {"text": "However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics.", "labels": [], "entities": []}, {"text": "These models perform no search for optimal phrasal alignments.", "labels": [], "entities": []}, {"text": "Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.9508886635303497}]}, {"text": "proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretically governed way.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.7726108332475027}]}, {"text": "This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7505697309970856}, {"text": "phrase extraction", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.8379358649253845}]}, {"text": "Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases.", "labels": [], "entities": []}, {"text": "The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation).", "labels": [], "entities": [{"text": "NIST MT Evaluation", "start_pos": 146, "end_pos": 164, "type": "DATASET", "confidence": 0.846222440401713}]}, {"text": "However, considering all possible phrases and all their possible alignments vastly increases the computational complexity of the joint model when compared to its word-based counterpart.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method of constraining the search space of the joint model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are still explored.", "labels": [], "entities": []}, {"text": "The joint model is constrained to phrasal alignments which do not contradict a set high confidence word alignments for each sentence.", "labels": [], "entities": []}, {"text": "These high confidence alignments could incorporate information from both statistical and linguistic sources.", "labels": [], "entities": []}, {"text": "In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity.", "labels": [], "entities": [{"text": "complexity", "start_pos": 181, "end_pos": 191, "type": "METRIC", "confidence": 0.9725493788719177}]}], "datasetContent": [{"text": "All data and software used was from the NAACL 2006 Statistical Machine Translation workshop unless otherwise indicated.", "labels": [], "entities": [{"text": "NAACL 2006", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8616034388542175}, {"text": "Statistical Machine Translation workshop", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.8149041086435318}]}], "tableCaptions": [{"text": " Table 1. The impact of constraining the joint model  trained on 10,000 sentences of the German-English  Europarl corpora and tested with the Europarl test set  used in", "labels": [], "entities": [{"text": "Europarl corpora", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.8018234670162201}, {"text": "Europarl test set", "start_pos": 142, "end_pos": 159, "type": "DATASET", "confidence": 0.9819700519243876}]}, {"text": " Table 2. Basic system comparisons: BLEU scores  and model size in millions of phrase pairs for Spanish- English", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9653260409832001}]}, {"text": " Table 3. Bleu scores for the joint model and the stan- dard model showing the effect of the 5-gram language  model, distortion length of 6 (dl) and the addition of  lexical reordering for the English-Spanish and Spanish- English tasks.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9963021278381348}, {"text": "distortion length of 6 (dl)", "start_pos": 117, "end_pos": 144, "type": "METRIC", "confidence": 0.8862991077559335}]}]}