{"title": [{"text": "Learning of Graph-based Question Answering Rules", "labels": [], "entities": [{"text": "Graph-based Question Answering", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.5649408400058746}]}], "abstractContent": [{"text": "In this paper we present a graph-based approach to question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9080978333950043}]}, {"text": "The method assumes a graph representation of question sentences and text sentences.", "labels": [], "entities": []}, {"text": "Question answering rules are automatically learnt from a training corpus of questions and answer sentences with the answer annotated.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.924828976392746}]}, {"text": "The method is independent from the graph representation formalism chosen.", "labels": [], "entities": []}, {"text": "A particular example is presented that uses a specific graph representation of the logical contents of sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text-based question answering (QA) is the process of automatically finding the answers to arbitrary questions in plain English by searching collections of text files.", "labels": [], "entities": [{"text": "Text-based question answering (QA) is the process of automatically finding the answers to arbitrary questions in plain English by searching collections of text files", "start_pos": 0, "end_pos": 165, "type": "Description", "confidence": 0.6778383805201604}]}, {"text": "Recently there has been intensive research in this area, fostered by evaluation-based conferences such as the Text REtrieval Conference (TREC), the Cross-Lingual Evaluation Forum (CLEF) (, and the NII-NACSIS Test Collection for Information Retrieval Systems workshops (NTCIR) (.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.7893686095873514}, {"text": "NII-NACSIS Test Collection for Information Retrieval Systems", "start_pos": 197, "end_pos": 257, "type": "TASK", "confidence": 0.7010989018848964}]}, {"text": "Current research focuses on factoid question answering, whereby the answer is a short string that indicates a fact, usually a named entity.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.8130658666292826}]}, {"text": "An example of a factoid question is Who won the 400m race in the 2000 Summer Olympic games?, which has a short answer: Cathy Freeman.", "labels": [], "entities": []}, {"text": "There are various approaches to question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8825547695159912}]}, {"text": "The focus of this paper is on rule-based systems.", "labels": [], "entities": []}, {"text": "A rule could be, say, \"if the question is of the form Who is the <position> of <country>\" and a text sentence says <position> of <country> Y and Y consists of two capitalised words, then Y is the answer\").", "labels": [], "entities": []}, {"text": "Such a rule was used by, who developed a system who obtained the best accuracy in the 2001 Text REtrieval Conference).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9989874958992004}, {"text": "Text REtrieval Conference", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.5067083835601807}]}, {"text": "The system developed by relied on the development of a large set of patterns of potential answer expressions, and the allocation of those patterns to types of questions.", "labels": [], "entities": []}, {"text": "The patterns were developed by hand by examining the data.'s work shows that a rule-based QA system can produce good results if the rule set is comprehensive enough.", "labels": [], "entities": []}, {"text": "Unfortunately, if the system is ported to anew domain the set of rules needs to be ported as well.", "labels": [], "entities": []}, {"text": "It has not been proven that rules like the ones developed by, which were designed for the TREC QA task, can be ported to other domains.", "labels": [], "entities": [{"text": "TREC QA task", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.7518772681554159}]}, {"text": "Furthermore, the process of producing the rules was presumably very labour intensive.", "labels": [], "entities": []}, {"text": "Consequently, the cost of manually producing new rules fora specialised domain could become too expensive for some domains.", "labels": [], "entities": []}, {"text": "In this paper we present a method for the automatic learning of question answering rules by applying graph manipulation methods.", "labels": [], "entities": [{"text": "automatic learning of question answering rules", "start_pos": 42, "end_pos": 88, "type": "TASK", "confidence": 0.6815986086924871}]}, {"text": "The method relies on the representation of questions and answer sentences as graphs.", "labels": [], "entities": []}, {"text": "Section 2 describes the general format of the graph-based QA rules and section 3 describes the method to learn the rules.", "labels": [], "entities": []}, {"text": "The methods described on the above two sections are independent of the actual sentence representation formalism, as long as the representation is a graph.", "labels": [], "entities": []}, {"text": "Section 4 presents a specific application using logical graphs.", "labels": [], "entities": []}, {"text": "Finally, sections 5 and 6 focus on related research and final conclusions, respectively.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}