{"title": [{"text": "Learning to Identify Definitions using Syntactic Features", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an approach to learning concept definitions which operates on fully parsed text.", "labels": [], "entities": [{"text": "learning concept definitions", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.6856793761253357}]}, {"text": "A subcorpus of the Dutch version of Wikipedia was searched for sentences which have the syntactic properties of definitions.", "labels": [], "entities": []}, {"text": "Next, we experimented with various text classification techniques to distinguish actual definitions from other sentences.", "labels": [], "entities": [{"text": "text classification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7115415185689926}]}, {"text": "A maximum entropy classifier which incorporates features referring to the position of the sentence in the document as well as various syntactic features, gives the best results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Answering definition questions is a challenge for question answering systems.", "labels": [], "entities": [{"text": "Answering definition questions", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9551828900973002}, {"text": "question answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8625382781028748}]}, {"text": "Much work in QA has focused on answering factoid questions, which are characterized by the fact that given the question, one can typically make strong predictions about the type of expected answer (i.e. a date, name of a person, amount, etc.).", "labels": [], "entities": [{"text": "QA", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9399471282958984}]}, {"text": "Definition questions require a different approach, as a definition can be a phrase or sentence for which only very global characteristics hold.", "labels": [], "entities": []}, {"text": "In the CLEF 2005 QA task, 60 out of 200 questions were asking for the definition of a named entity (a person or organization) such as Who is Goodwill Zwelithini? or What is IKEA?", "labels": [], "entities": [{"text": "CLEF 2005 QA task", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.8998934328556061}, {"text": "What is IKEA?", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.5245033949613571}]}, {"text": "Answers are phrases such as current king of the Zulu nation, or Swedish home furnishings retailer.", "labels": [], "entities": []}, {"text": "For answering definition questions restricted to named entities, it generally suffices to search for noun phrases consisting of the named entity and a preceding or following nominal phrase.", "labels": [], "entities": [{"text": "answering definition questions", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8251481652259827}]}, {"text": "extract all such noun phrases from the Dutch CLEF corpus off-line, and return the most frequent heads of co-occurring nominal phrases expanded with adjectival or prepositional modifiers as answer to named entity definition questions.", "labels": [], "entities": [{"text": "Dutch CLEF corpus", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.8111257155736288}]}, {"text": "The resulting system answers 50% of the CLEF 2005 definition questions correctly.", "labels": [], "entities": [{"text": "CLEF 2005 definition questions", "start_pos": 40, "end_pos": 70, "type": "DATASET", "confidence": 0.9376124292612076}]}, {"text": "For a Dutch medical QA system, which is being developed as part of the IMIX project 1 , several sets of test questions were collected.", "labels": [], "entities": [{"text": "Dutch medical QA", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.41493910551071167}]}, {"text": "Approximately 15% of the questions are definition questions, such as What is a runner's knee? and What is cerebrovascular accident?.", "labels": [], "entities": []}, {"text": "Answers to such questions (asking for the definition of a concept) are typically found in sentences such as A runner's knee is a degenerative condition of the cartilage surface of the back of the kneecap, or patella or A cerebrovascular accident is a decrease in the number of circulating white blood cells (leukocytes) in the blood.", "labels": [], "entities": []}, {"text": "One approach to finding answers to concept definitions simply searches the corpus for sentences consisting of a subject, a copular verb, and a predicative phrase.", "labels": [], "entities": []}, {"text": "If the concept matches the subject, the predicative phrase can be returned as answer.", "labels": [], "entities": []}, {"text": "A preliminary evaluation of this technique in Tjong Kim revealed that only 18% of the extracted sentences (from a corpus consisting of a mixture of encyclopedic texts and web documents) is actually a definition.", "labels": [], "entities": [{"text": "Tjong Kim", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.809223473072052}]}, {"text": "For instance, sentences such as RSI is a major problem in the Netherlands, every suicide attempt is an emergency or an infection of the lungs is the most serious complication are of the relevant syntactic form, but do not constitute definitions.", "labels": [], "entities": [{"text": "RSI", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8755158185958862}]}, {"text": "In this paper, we concentrate on a method for improving the precision of recognizing definition sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9976456761360168}, {"text": "recognizing definition sentences", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6550900936126709}]}, {"text": "In particular, we investigate to what extent machine learning techniques can be used to distinguish definitions from non-definitions in a corpus of sentences containing a subject, copular verb, and predicative phrase.", "labels": [], "entities": []}, {"text": "A manually annotated subsection of the corpus was divided into definition and non-definition sentences.", "labels": [], "entities": []}, {"text": "Next, we trained various classifiers using unigram and bigram features, and various syntactic features.", "labels": [], "entities": []}, {"text": "The best classifier achieves a 60% error reduction compared to our baseline system.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.9797703921794891}]}], "datasetContent": [{"text": "We evaluated each configuration of Section 5 and each learning method of Section 6 on the dataset which consists of 1336 definitions and 963 nondefinitions sentences.", "labels": [], "entities": []}, {"text": "reports the accuracy and standard error estimated from this experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997830986976624}, {"text": "standard error", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.95706707239151}]}, {"text": "In all experiment runs, all of the classifiers in all configurations outperform our baseline (75.9%).", "labels": [], "entities": []}, {"text": "The best accuracy of each classifier (bold) is between 11.57% to 16.31% above the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995088577270508}]}, {"text": "The bigram only attributes (config. 2) clearly outperform the simplest setting (bag-of-word only attributes) for all classifiers.", "labels": [], "entities": []}, {"text": "The combination of both attributes (config. 3) achieves some improvement between 0.17% to 4.41% from configuration 2.", "labels": [], "entities": []}, {"text": "It is surprising that naive Bayes shows the best and relatively high accuracy in this base configuration (89.82%) and even outperforms all other settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9993759989738464}]}, {"text": "Adding syntactic properties (config. 4) or position of sentences in documents (config.", "labels": [], "entities": []}, {"text": "6) to the base configuration clearly gives some improvement (in 4 and 5 classifiers respectively for each configuration).", "labels": [], "entities": []}, {"text": "But, adding root forms (config. 7) does not significantly contribute to an improvement.", "labels": [], "entities": []}, {"text": "These results show that in general, syntactic properties can improve the performance of most classifiers.", "labels": [], "entities": []}, {"text": "The results also support the intuition that the position of sentences in documents plays important role in identifying definition sentences.", "labels": [], "entities": [{"text": "identifying definition sentences", "start_pos": 107, "end_pos": 139, "type": "TASK", "confidence": 0.850090742111206}]}, {"text": "Moreover, this intuition is also supported by the result that the best performance of naive Bayes is achieved at configuration 6 (90.26%).", "labels": [], "entities": []}, {"text": "Compared to the syntactic features, sentence positions give better accuracy in all classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9983970522880554}]}, {"text": "The above results demonstrate an interesting finding that a simple attribute set which consists of bag-of-words, bigrams, and sentence position under a fast and simple classifier (e.g. naive Bayes) could give a relatively high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9945476055145264}]}, {"text": "One explanation that we can think of is that candidate sentences have been syntactically very well extracted with our filter.", "labels": [], "entities": []}, {"text": "Thus, the sentences are biased by the filter from which important words and bigrams of definitions can be found inmost of the sen-tences.", "labels": [], "entities": []}, {"text": "For example, the word and bigrams is een (is a), een (a), zijn (are), is (is), zijn de (are the), and is van (is of) are good clues to definitions and consequently have high information gain.", "labels": [], "entities": []}, {"text": "We have to test this result in a future work on candidate definition sentences which are extracted by filters using various other syntactic patterns.", "labels": [], "entities": []}, {"text": "More improvement is shown when both syntactic properties and sentence position are added together (config. 8).", "labels": [], "entities": []}, {"text": "All of the classifiers in this configuration obtain more error reduction compared to the base configuration.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.9548643231391907}]}, {"text": "Moreover, the best accuracy of this experiment is shown by maximum entropy at this configuration (92.21%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9996098875999451}]}, {"text": "This maybe a sign that our proposed syntactic properties are good indicators to identify definition sentences.", "labels": [], "entities": [{"text": "identify definition sentences", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.7345665891965231}]}, {"text": "Other interesting findings can be found in the addition of named entity classes to configuration 3 (config. 5), to configuration 8 (config. 9) and to configuration 10 (config. 11).", "labels": [], "entities": []}, {"text": "In these configurations, adding NEC increases accuracies of almost all classifiers.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9953261613845825}]}, {"text": "On the other hand, adding root forms to configuration 3 (config. 7) and to configuration 8 (config. 10) does not improve accuracies.", "labels": [], "entities": []}, {"text": "However, the best accuracies of naive Bayes (90.26%) and maximum entropy (92.21%) are achieved when named entity and root forms are not included as attributes.", "labels": [], "entities": []}, {"text": "We now evaluate the classifiers.", "labels": [], "entities": []}, {"text": "It is clear from the table that SVM1 and SVM2 settings cannot achieve better accuracy compared to the naive Bayes setting, while SVM3 setting marginally outperforms naive Bayes (on 6 out of 11 configurations).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9984765648841858}]}, {"text": "This result is contrary to the superiority of SVMs in many text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.8654908537864685}]}, {"text": "reported that both classifiers show similar predictive accuracy and AUC (area under the ROC (Receiver Operating Characteristics) curve) scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8973827958106995}, {"text": "AUC", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9983464479446411}]}, {"text": "This performance of naive Bayes supports the motivation behind its renaisance in machine learning.", "labels": [], "entities": []}, {"text": "From the three SVM settings, SVM with RBF kernel appears as the best classifier for our task in which it outperforms other SVMs settings in all configurations.", "labels": [], "entities": []}, {"text": "This result supports the above mentioned argument that if the best C and \u03b3 can be selected, we do not need to consider linear SVM (e.g. the svm1 setting).", "labels": [], "entities": []}, {"text": "Among all of the classifiers, maximum entropy shows the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9983580708503723}]}, {"text": "It wins at 9 out of 11 configurations in all experiments.", "labels": [], "entities": []}, {"text": "This result confirms previous reports e.g. in that maximum entropy performs better than naive Bayes in some text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.8596603075663248}]}], "tableCaptions": [{"text": " Table 1: Number of sentences in the first and  other position of documents annotated as defini- tion, non-definition, and undecided.", "labels": [], "entities": []}, {"text": " Table 2: Percentage of determiner types of sub- jects in definition and non-definition sentences.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of determiner types of  predicative complements in definition and non- definition sentences.", "labels": [], "entities": []}, {"text": " Table 4: Percentage of named-entity classes of  subjects in definition and non-definition sentences.", "labels": [], "entities": []}, {"text": " Table 6: The description of the attribute configu- rations.", "labels": [], "entities": []}, {"text": " Table 7: Accuracy and standard error (%) estimates for the dataset using naive Bayes (NB), maximum  entropy (ME), and three SVM settings at the different attribute configurations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980301260948181}, {"text": "standard error", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.9669238328933716}, {"text": "maximum  entropy (ME)", "start_pos": 92, "end_pos": 113, "type": "METRIC", "confidence": 0.7500962734222412}]}]}