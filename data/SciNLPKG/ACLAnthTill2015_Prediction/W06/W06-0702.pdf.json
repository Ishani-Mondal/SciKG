{"title": [{"text": "Challenges in Evaluating Summaries of Short Stories", "labels": [], "entities": [{"text": "Evaluating Summaries of Short Stories", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.9294668912887574}]}], "abstractContent": [{"text": "This paper presents experiments with the evaluation of automatically produced summaries of literary short stories.", "labels": [], "entities": [{"text": "summaries of literary short stories", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.8125063896179199}]}, {"text": "The summaries are tailored to a particular purpose of helping a reader decide whether she wants to read the story.", "labels": [], "entities": []}, {"text": "The evaluation procedure includes extrinsic and intrinsic measures, as well as subjective and factual judgments about the summaries pronounced by human subjects.", "labels": [], "entities": []}, {"text": "The experiments confirm the experience of summarizing more conventional genres: sentence overlap between human-and machine-made summaries is not a complete picture of the quality of a summary.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9844451546669006}]}, {"text": "In fact, in our case, sentence overlap does not correlate well with human judgment.", "labels": [], "entities": [{"text": "sentence overlap", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.6927031427621841}]}, {"text": "We explain the evaluation procedures and discuss several challenges of evaluating summaries of works of fiction.", "labels": [], "entities": [{"text": "summaries of works of fiction", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.8849810242652894}]}], "introductionContent": [{"text": "In recent years the automatic text summarization community has increased its focus on reliable evaluation.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6791940232117971}]}, {"text": "The much used evaluation methods based on sentence overlap with reference summaries have been called into question as they provide only a rough approximation of semantic similarity between summaries.", "labels": [], "entities": []}, {"text": "A number of deeper, more semanticallymotivated approaches have been proposed, such as the factoid method and the pyramid method ().", "labels": [], "entities": []}, {"text": "These methods measure similarity between reference and generated summaries more reliably but, unfortunately, have a disadvantage of being very labour-intensive.", "labels": [], "entities": []}, {"text": "This paper describes experiments in evaluating automatically produced summaries of literary short stories.", "labels": [], "entities": [{"text": "summaries of literary short stories", "start_pos": 70, "end_pos": 105, "type": "TASK", "confidence": 0.855856454372406}]}, {"text": "It presents an approach that evaluates summaries from two different perspectives: comparing computer-made summaries to those produced by humans based on sentence-overlap and measuring usefulness and informativeness of the summaries by themselves -a step critical when creating and evaluating summaries of a relatively unexplored genre.", "labels": [], "entities": [{"text": "summaries", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9636715650558472}]}, {"text": "The paper also points out several challenges specific to evaluating summaries of fiction such as questionable suitability of traditional metrics (those based on sentence overlap), unavailability of clearly defined criteria to judge \"goodness\" of a summary and a higher degree of redundancy in such texts.", "labels": [], "entities": [{"text": "summaries of fiction", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.9088593522707621}]}, {"text": "We achieve these goals by performing a two-step evaluation of our summaries.", "labels": [], "entities": []}, {"text": "Initially, for each story in the test set we compare sentence overlap between summaries which the system generates and those produced by three human subjects.", "labels": [], "entities": []}, {"text": "These experiments reveal that inter-rater agreement measures tend to be pessimistic where fiction is concerned.", "labels": [], "entities": []}, {"text": "This seems due to a higher degree of redundancy and paraphrasing in such texts.", "labels": [], "entities": []}, {"text": "The second stage of the evaluation process seeks to measure usefulness of the summaries in a more tangible way.", "labels": [], "entities": []}, {"text": "To this end, three subjects answered a number of questions, first after  . On the camino real along the beach the two saddle mules and the four pack mules of Don Se\u00f1or Johnny Armstrong stood, patiently awaiting the crack of the whip of the arriero, Luis.", "labels": [], "entities": []}, {"text": "These articles Don Johnny traded to the interior Indians for the gold dust that they washed from the Andean streams and stored in quills and bags against his coming.", "labels": [], "entities": []}, {"text": "It was a profitable business, and Se\u00f1or Armstrong expected soon to be able to purchase the coffee plantation that he coveted.", "labels": [], "entities": []}, {"text": "Armstrong stood on the narrow sidewalk, exchanging garbled Spanish with old Peralto, the rich native merchant who had just charged him four prices for half a gross of pot-metal hatchets, and abridged English with Rucker, the little German who was Consul for the United States.", "labels": [], "entities": []}, {"text": "Armstrong, waved a good-bye and took his place at the tail of the procession.", "labels": [], "entities": [{"text": "Armstrong", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.6522080302238464}]}, {"text": "Armstrong concurred, and they turned again upward toward Tacuzama.", "labels": [], "entities": []}, {"text": "Peering cautiously inside, he saw, within three feet of him, a woman of marvellous, imposing beauty, clothed in a splendid loose robe of leopard skins.", "labels": [], "entities": []}, {"text": "The hut was packed close to the small space in which she stood with the squatting figures of Indians.", "labels": [], "entities": []}, {"text": "If you need assistance tell me how I can render it.", "labels": [], "entities": []}, {"text": "[\u2026] The woman was worthy of his boldness.", "labels": [], "entities": []}, {"text": "Only by a sudden flush of her pale cheek did she acknowledge understanding of his words.", "labels": [], "entities": [{"text": "understanding of his words", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.8971983343362808}]}, {"text": "\" I am held a prisoner by these Indians.", "labels": [], "entities": []}, {"text": "God knows I need help.", "labels": [], "entities": []}, {"text": "[\u2026] look, Mr. Armstrong, there is the sea!", "labels": [], "entities": []}, {"text": "reading only the summary and then after reading the complete story.", "labels": [], "entities": []}, {"text": "The set included both factual questions (e.g. can you tell where this story takes place?) and subjective questions (e.g. how readable did you find this summary?).", "labels": [], "entities": []}, {"text": "Finally, we compare the two types of results with a surprising discovery: overlapbased measures and human judgment do not correlate well in our case.", "labels": [], "entities": []}, {"text": "This paper is organized in the following manner.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes our summarizer of short stories.", "labels": [], "entities": [{"text": "summarizer of short stories", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.8099075257778168}]}, {"text": "Section 3.1 discusses experiments comparing generated summaries to reference ones based on sentence overlap.", "labels": [], "entities": []}, {"text": "The experiments involving human judgment of the summaries are presented in Section 3.2 and the two types of experiments are compared in Section 3.3.", "labels": [], "entities": []}, {"text": "Section 4 draws conclusions and outlines possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We designed our evaluation procedure to have easily interpreted, meaningful results, and keep the amount of labour reasonable.", "labels": [], "entities": []}, {"text": "We worked with six subjects (different than the authors of this paper) who performed two separate tasks.", "labels": [], "entities": []}, {"text": "In Task 1 each subject was asked to read a story and create its summary by selecting 6% of the sentences.", "labels": [], "entities": []}, {"text": "The subjects were explained that their summaries were to raise expectations about the story, but not to reveal what happens in it.", "labels": [], "entities": []}, {"text": "In Task 2 the subjects made a number of judgments about the summaries before and after reading the original stories.", "labels": [], "entities": []}, {"text": "The subjects read a summary similar to the one shown in.", "labels": [], "entities": []}, {"text": "Next, they were asked six questions, three of which were factual in nature and three others were subjective.", "labels": [], "entities": []}, {"text": "The subjects had to answer these questions using the summary as the only source of information.", "labels": [], "entities": []}, {"text": "Subsequently, they read the original story and answered almost the same questions (see Section 4).", "labels": [], "entities": []}, {"text": "This process allowed us to understand how informative the summaries were by themselves, without access to the originals, and also whether they were misleading or incomplete.", "labels": [], "entities": []}, {"text": "The experiments were performed on a test set of 20 stories and involved six participants divided into two groups of three people.", "labels": [], "entities": []}, {"text": "Group 1 performed Task 1 on stories 1-10 of the testing set and Group 2 performed this task on stories 11-20.", "labels": [], "entities": []}, {"text": "During Task 2 Group 1 worked on stories 11-20 and Group 2 -on stories 1-10.", "labels": [], "entities": []}, {"text": "By adjusting a number of system parameters, we produced four different summaries per story.", "labels": [], "entities": []}, {"text": "All four versions were compared with human-made summaries using sentence overlap-based measures.", "labels": [], "entities": []}, {"text": "However, because the experiments are rather time consuming, it was not possible to evaluate more than one set of summaries using human judgments (Task 2).", "labels": [], "entities": []}, {"text": "That is why only summaries generated using the coarse-grained dataset and manually composed rules were evaluated in Task 2.", "labels": [], "entities": []}, {"text": "We selected this version because the differences between this set of summaries and gold-standard summaries are easiest to interpret.", "labels": [], "entities": []}, {"text": "That is to say, decisions based on a set of rules employing a smaller number of parameters are easier to track than those taken using machine learning or more elaborate rules.", "labels": [], "entities": []}, {"text": "On average, the subjects reported that completing both tasks required between 15 and 35 hours of work.", "labels": [], "entities": []}, {"text": "Four out of six subjects were native speakers of English.", "labels": [], "entities": []}, {"text": "Two others had a near-native and very good levels of English respectively.", "labels": [], "entities": []}, {"text": "The participants were given the data inform of files and had four weeks to complete the tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Inter-judge agreement.  Statistic  Group 1 Group 2 Average  Cohen (4)  0.50  0.34  0.42  Cohen (3)  0.51  0.34  0.42  PABAK (4)  0.88  0.85  0.87  PABAK (3)  0.89  0.86  0.87  ICC (4)  0.80  (0.78,  0.82)", "labels": [], "entities": [{"text": "Average  Cohen (4)  0.50  0.34  0.42  Cohen (3)  0.51  0.34  0.42", "start_pos": 61, "end_pos": 126, "type": "METRIC", "confidence": 0.7302948911984761}, {"text": "PABAK (4)  0.88  0.85  0.87  PABAK (3)  0.89  0.86  0.87  ICC (4)  0.80", "start_pos": 128, "end_pos": 199, "type": "DATASET", "confidence": 0.6881320194194191}]}, {"text": " Table 2. Sentence overlap between computer- and human-made summaries. Majority gold- standard.  Dataset  Prec.  Rec.  F  LEAD  25.09 30.49 27.53  LEAD CHAR  28.14 33.18 30.45  Rules, coarse- grained", "labels": [], "entities": [{"text": "Majority gold- standard", "start_pos": 71, "end_pos": 94, "type": "METRIC", "confidence": 0.7362911179661751}, {"text": "F", "start_pos": 119, "end_pos": 120, "type": "METRIC", "confidence": 0.8475629091262817}, {"text": "LEAD", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.5094612240791321}, {"text": "LEAD CHAR", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.8193827569484711}]}, {"text": " Table 3. Sentence overlap between computer- and human-made summaries. Union gold- standard.  Dataset  Prec.  Rec.  F  LEAD  36.53 17.97 24.09  LEAD CHAR  44.49 21.23 28.75  Rules, coarse- grained", "labels": [], "entities": [{"text": "Union gold- standard.  Dataset  Prec.  Rec.", "start_pos": 71, "end_pos": 114, "type": "DATASET", "confidence": 0.7479650139808655}, {"text": "F", "start_pos": 116, "end_pos": 117, "type": "METRIC", "confidence": 0.7906270027160645}, {"text": "LEAD", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.5205162167549133}, {"text": "LEAD CHAR", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.8150169849395752}]}, {"text": " Table 4. Sentence overlap between  computer-and human-made summaries.  Intersection gold-standard.  Dataset  Prec.  Rec.  F  LEAD  12.55 37.36 18.78  LEAD CHAR  15.97 46.14 23.73  Rules, coarse- grained", "labels": [], "entities": [{"text": "Intersection", "start_pos": 72, "end_pos": 84, "type": "METRIC", "confidence": 0.9579702019691467}, {"text": "Dataset  Prec.  Rec.", "start_pos": 101, "end_pos": 121, "type": "DATASET", "confidence": 0.903582763671875}, {"text": "F  LEAD", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.6305213868618011}, {"text": "LEAD CHAR", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.8152128159999847}]}, {"text": " Table 5. Answers to factual questions.  Id  Question  After summary  only", "labels": [], "entities": [{"text": "Id  Question  After summary", "start_pos": 41, "end_pos": 68, "type": "METRIC", "confidence": 0.8569547384977341}]}, {"text": " Table 8. ANOVA F-values between sentence  overlap measures and human judgments.  Question  Prec. Rec. F  Q1(main characters) 0.60 0.61 0.58  Q2(location)  2.58 1.94 2.36  Q3(time)  1.11 0.67 0.97  Q4(readability)  2.10 0.90 1.60  Q5(relevance)  4.55 3.75 4.28  Q10 (relevance)  6.33 3.46 5.15  Q11(completeness)  3.11 4.22 3.43", "labels": [], "entities": [{"text": "ANOVA F-values", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8122203648090363}, {"text": "Question  Prec. Rec. F  Q1(main characters) 0.60 0.61 0.58  Q2(location)  2.58 1.94 2.36  Q3(time)  1.11 0.67 0.97  Q4", "start_pos": 82, "end_pos": 200, "type": "METRIC", "confidence": 0.7207842356172102}, {"text": "relevance)  6.33 3.46 5.15  Q11(completeness)", "start_pos": 267, "end_pos": 312, "type": "METRIC", "confidence": 0.7132145762443542}]}, {"text": " Table 7. Spearman rank correlation  coefficient between sentence overlap measures  and human judgments.  Question  Prec. Rec. F  Q1(main characters)  0.09 0.29 0.17  Q2(location)  0.21 0.18 0.22  Q3(time)  0.38 0.28 0.34  Q4(readability)  0.47 0.31 0.50  Q5(relevance)  0.31 0.19 0.34  Q10(relevance)  0.60 0.40 0.59  Q11(completeness)  0.40 0.29 0.40", "labels": [], "entities": [{"text": "Question  Prec. Rec. F  Q1(main characters)  0.09 0.29 0.17  Q2(location)  0.21 0.18 0.22  Q3(time)  0.38 0.28 0.34  Q4(readability)  0.47 0.31 0.50  Q5(relevance)  0.31 0.19 0.34  Q10(relevance)  0.60 0.40 0.59  Q11(completeness)  0.40 0.29 0.40", "start_pos": 106, "end_pos": 352, "type": "METRIC", "confidence": 0.8189890017466885}]}, {"text": " Table 6. Answers to subjective questions.  Id  Question (scale: 1 to 6)  After summary  only", "labels": [], "entities": [{"text": "Id", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9708258509635925}]}]}