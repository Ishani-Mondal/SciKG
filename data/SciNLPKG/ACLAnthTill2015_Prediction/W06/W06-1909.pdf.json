{"title": [{"text": "Adapting a Semantic Question Answering System to the Web", "labels": [], "entities": [{"text": "Adapting", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9688763618469238}, {"text": "Semantic Question Answering", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.5800821483135223}]}], "abstractContent": [{"text": "This paper describes how a question answering (QA) system developed for small-sized document collections of several million sentences was modified in order to work with a monolingual subset of the web.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8474532127380371}]}, {"text": "The basic QA system relies on complete sentence parsing, inferences, and semantic representation matching.", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7444333732128143}, {"text": "semantic representation matching", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6743453145027161}]}, {"text": "The extensions and modifications needed for useful and quick answers from web documents are discussed.", "labels": [], "entities": []}, {"text": "The main extension is a two-level approach that first accesses a web search engine and downloads some of its document hits and then works similar to the basic QA system.", "labels": [], "entities": []}, {"text": "Most modifications are restrictions like a maximal number of documents and a maximal length of investigated document parts; they ensure acceptable answer times.", "labels": [], "entities": []}, {"text": "The resulting web QA system is evaluated on the German test collection from QA@CLEF 2004.", "labels": [], "entities": [{"text": "German test collection from QA@CLEF 2004", "start_pos": 48, "end_pos": 88, "type": "DATASET", "confidence": 0.9659997746348381}]}, {"text": "Several parameter settings and strategies for accessing the web search engine are investigated.", "labels": [], "entities": []}, {"text": "The main results are: precision-oriented extensions and experimentally derived parameter settings are needed to achieve similar performance on the web as on small-sized document collections that show higher homogeneity and quality of the contained texts; adapting a semantic QA system to the web is feasible, but answering a question is still expensive in terms of bandwidth and CPU time.", "labels": [], "entities": [{"text": "precision-oriented", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.9677092432975769}]}], "introductionContent": [{"text": "There are question answering (QA) systems intended for small-sized document collections (textual QA systems) and QA systems aiming at the web (web(-based) QA systems).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8562981128692627}]}, {"text": "In this paper, a system of the former type) is transformed into one of the latter type (called InSicht-W3 for short).", "labels": [], "entities": []}, {"text": "2, the textual QA system for German is presented.", "labels": [], "entities": []}, {"text": "The extensions and modifications required to get it working with the web as a virtual document collection are described and discussed in Sect.", "labels": [], "entities": []}, {"text": "3. The resulting system is evaluated on a well-known test collection and compared to the basic QA system (Sect. 4).", "labels": [], "entities": []}, {"text": "After the conclusion, some directions for further research are indicated.", "labels": [], "entities": []}], "datasetContent": [{"text": "The resulting web QA system is evaluated on an established set of test questions: QA@CLEF 2004 ( ).", "labels": [], "entities": [{"text": "QA@CLEF 2004", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.7049610316753387}]}, {"text": "Some questions in this set have an important implicit temporal context.", "labels": [], "entities": []}, {"text": "For example, the correct answer to questions like qa04 090 in example  In QA@CLEF, a supported correct answer to this question can be the name of a UNICE president only from a certain time period because the QA@CLEF document collection consists of newspaper and newswire articles from 1994 and 1995 (and because there are no documents about the history of UNICE).", "labels": [], "entities": [{"text": "QA@CLEF document collection", "start_pos": 208, "end_pos": 235, "type": "DATASET", "confidence": 0.6087608397006988}]}, {"text": "On the web, there are additional correct answers.", "labels": [], "entities": []}, {"text": "Questions with implicit time restriction (30 cases) are excluded from the evaluation so that 170 questions remain for evaluation in InSicht-W3.", "labels": [], "entities": [{"text": "InSicht-W3", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.9156031012535095}]}, {"text": "Alternatives would be to refine these questions by making the temporal restriction explicit or to extend the gold standard by answers that are to be considered correct if working on the web.", "labels": [], "entities": []}, {"text": "contains evaluation results for InSicht-W3: the percentages of right, inexact, and wrong answers (separately for non-empty answers and empty answers) and the K1-measure (see) fora definition).", "labels": [], "entities": [{"text": "InSicht-W3", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.857363760471344}]}, {"text": "For comparison, the results of the textual QA system InSicht on the QA@CLEF document collection are shown in the first row.", "labels": [], "entities": [{"text": "QA@CLEF document collection", "start_pos": 68, "end_pos": 95, "type": "DATASET", "confidence": 0.7451799154281616}]}, {"text": "The percentages for non-empty answers differ for right answers and wrong answers.", "labels": [], "entities": []}, {"text": "In both aspects, InSicht-W3 is worse than InSicht.", "labels": [], "entities": [{"text": "InSicht-W3", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.8017423152923584}, {"text": "InSicht", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9371972680091858}]}, {"text": "The main reason for these changes is that the structure and textual content of documents on the web Or just one correct answer, which differs from the one in QA@CLEF, when one interprets the present tense in the question as referring to today. are much more diverse than in the QA@CLEF collection.", "labels": [], "entities": [{"text": "QA@CLEF collection", "start_pos": 278, "end_pos": 296, "type": "DATASET", "confidence": 0.794983446598053}]}, {"text": "For example, InSicht-W3 regarded some sequences of words from unrelated link anchor texts as a sentence, which led to some wrong answers especially for definition questions.", "labels": [], "entities": [{"text": "InSicht-W3", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.9390690326690674}, {"text": "definition questions", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.9020001292228699}]}, {"text": "Also for empty answers, InSicht-W3 performs worse than InSicht.", "labels": [], "entities": []}, {"text": "This is in part due to the too optimistic assumption during answer assessment that all German questions from QA@CLEF 2004 have a correct answer on the web (therefore the column labelled right empty answers contains 0.0 for InSicht-W3).", "labels": [], "entities": [{"text": "answer assessment", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8025773465633392}, {"text": "German questions from QA@CLEF 2004", "start_pos": 87, "end_pos": 121, "type": "DATASET", "confidence": 0.7715633426393781}, {"text": "InSicht-W3", "start_pos": 223, "end_pos": 233, "type": "DATASET", "confidence": 0.8579002618789673}]}, {"text": "However, InSicht-W3 found 12 right answers that InSicht missed.", "labels": [], "entities": [{"text": "InSicht-W3", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.8692342638969421}]}, {"text": "So there is a potential fora fruitful system combination.", "labels": [], "entities": []}, {"text": "The impact of some interesting parameters was evaluated by varying parameter settings (.", "labels": [], "entities": []}, {"text": "The query network quality q min (column 2) is a value between 0 (worst) and 1 (best) that measures how faraway a derived query network is from the original semantic network of the question.", "labels": [], "entities": []}, {"text": "For example, omitting information from the semantic network for the question (like the first name of a person if also the last name is specified), leads to a reduction of the initial quality value of 1.", "labels": [], "entities": []}, {"text": "The value in column 2 indicates at what threshold variant query networks are ignored.", "labels": [], "entities": []}, {"text": "Column 3 corresponds to the parameter of word form selection (see Sect. 3.4).", "labels": [], "entities": [{"text": "word form selection", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6371971666812897}]}, {"text": "The two runs with d = 300 and q min = 0.8 show that morphological generation pays off, but the effect is smaller than in other document collections.", "labels": [], "entities": [{"text": "morphological generation", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7786853611469269}]}, {"text": "This is probably due to the fact that the large and redundant web often contains answers with the exact word forms from the question.", "labels": [], "entities": []}, {"text": "Another interesting aspect found in Table 2 is that even with d = 500 results still improve; it remains to be seen at what number of documents performance stays stable (or maybe degrades).", "labels": [], "entities": []}, {"text": "The impact of a lower quality threshold q min was not significant.", "labels": [], "entities": []}, {"text": "This might change if one operates with a larger parameter d and more inferential rules.", "labels": [], "entities": []}, {"text": "Error analysis and classification is difficult as soon as one steps to the vast web.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7303946614265442}, {"text": "classification", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.8766064047813416}]}, {"text": "One could start with a classification of error reasons for wrong empty answers.", "labels": [], "entities": []}, {"text": "The error class that can be most easily determined is that the search engine returned no results for the preselection query.", "labels": [], "entities": []}, {"text": "40 of the 170 questions (23.5%) belong to this class.", "labels": [], "entities": []}, {"text": "This might surprise users that believe that they can find nearly every bit of information on the web.", "labels": [], "entities": []}, {"text": "But there are areas where this assumption is wrong: many QA@CLEF questions relate to very specific events of the year 1994 or 1995.", "labels": [], "entities": [{"text": "QA@CLEF", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.5141323606173197}]}, {"text": "Twelve years ago, the web publishing rate was much lower than today; and even if the relevant pieces of information were on the web at that time (or in the following years), they might have been moved to archives or removed in recent years.", "labels": [], "entities": []}, {"text": "Other error reasons are very difficult and labor-intensive to separate:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results for the German questions from QA@CLEF 2004. InSicht used all 200  questions, InSicht-W3 only 170 questions (see text).  setup  answers (%)  K1", "labels": [], "entities": [{"text": "German questions from QA@CLEF 2004", "start_pos": 37, "end_pos": 71, "type": "DATASET", "confidence": 0.8539753726550511}, {"text": "InSicht", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.8993285298347473}, {"text": "InSicht-W3", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.909454882144928}]}, {"text": " Table 2: Influence of parameters on InSicht-W3 results. Parameter d is the maximal number of doc- uments used from the search engine results (see Sect. 3.4); q min is the minimal query network quality.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9916961789131165}]}]}