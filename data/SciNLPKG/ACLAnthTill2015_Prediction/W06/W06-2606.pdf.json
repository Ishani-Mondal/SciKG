{"title": [{"text": "Reranking Translation Hypotheses Using Structural Properties", "labels": [], "entities": [{"text": "Reranking Translation Hypotheses", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9497931400934855}]}], "abstractContent": [{"text": "We investigate methods that add syntactically motivated features to a statistical machine translation system in a reranking framework.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6182395219802856}]}, {"text": "The goal is to analyze whether shallow parsing techniques help in identifying ungrammatical hypotheses.", "labels": [], "entities": []}, {"text": "We show that improvements are possible by utilizing supertagging, lightweight dependency analysis, a link grammar parser and a maximum-entropy based chunk parser.", "labels": [], "entities": []}, {"text": "Adding features to n-best lists and dis-criminatively training the system on a development set increases the BLEU score up to 0.7% on the test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9828669428825378}]}], "introductionContent": [{"text": "Statistically driven machine translation systems are currently the dominant type of system in the MT community.", "labels": [], "entities": [{"text": "Statistically driven machine translation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.555563785135746}, {"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9914588928222656}]}, {"text": "Though much better than traditional rule-based approaches, these systems still make a lot of errors that seem, at least from a human point of view, illogical.", "labels": [], "entities": []}, {"text": "The main purpose of this paper is to investigate a means of identifying ungrammatical hypotheses from the output of a machine translation system by using grammatical knowledge that expresses syntactic dependencies of words or word groups.", "labels": [], "entities": []}, {"text": "We introduce several methods that try to establish this kind of linkage between the words of a hypothesis and, thus, determine its well-formedness, or \"fluency\".", "labels": [], "entities": []}, {"text": "We perform rescoring experiments that rerank n-best lists according to the presented framework.", "labels": [], "entities": []}, {"text": "As methodologies deriving well-formedness of a sentence we use supertagging () with lightweight dependency analysis (LDA) 1), link grammars () and a maximumentropy (ME) based chunk parser (.", "labels": [], "entities": []}, {"text": "The former two approaches explicitly model the syntactic dependencies between words.", "labels": [], "entities": []}, {"text": "Each hypothesis that contains irregularities, such as broken linkages or non-satisfied dependencies, should be penalized or rejected accordingly.", "labels": [], "entities": []}, {"text": "For the ME chunker, the idea is to train n-gram models on the chunk or POS sequences and directly use the log-probability as feature score.", "labels": [], "entities": [{"text": "ME chunker", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.5300858467817307}]}, {"text": "In general, these concepts and the underlying programs should be robust and fast in order to be able to cope with large amounts of data (as it is the case for n-best lists).", "labels": [], "entities": []}, {"text": "The experiments presented show a small though consistent improvement in terms of automatic evaluation measures chosen for evaluation.", "labels": [], "entities": []}, {"text": "BLEU score improvements, for instance, lie in the range from 0.3 to 0.7% on the test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9654334783554077}]}, {"text": "In the following, Section 2 gives an overview on related work in this domain.", "labels": [], "entities": []}, {"text": "In Section 3 we review our general approach to statistical machine translation (SMT) and introduce the main methodologies used for deriving syntactic dependencies on words or word groups, namely supertagging/LDA, link grammars and ME chunking.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.8027673562367758}, {"text": "ME chunking", "start_pos": 231, "end_pos": 242, "type": "TASK", "confidence": 0.7644425332546234}]}, {"text": "The corpora and the experiments are discussed in Section 4.", "labels": [], "entities": []}, {"text": "The paper is concluded in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments, we use the translation system described in ( ).", "labels": [], "entities": []}, {"text": "Our phrasebased decoder uses several models during search that are interpolated in a log-linear way (as expressed in Eq.", "labels": [], "entities": []}, {"text": "3), such as phrase-based translation models, word-based lexicon models, a language, deletion and simple reordering model and word and phrase penalties.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.6885267794132233}]}, {"text": "A word graph containing the most likely translation hypotheses is generated during the search process.", "labels": [], "entities": []}, {"text": "Out of this compact representation, we extract n-best lists as described in ).", "labels": [], "entities": []}, {"text": "These n-best lists serve as a starting point for our experiments.", "labels": [], "entities": []}, {"text": "The methods presented in Section 3 produce scores that are used as additional features for the n-best lists.", "labels": [], "entities": []}, {"text": "The use of n-best lists in machine translation has several advantages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7477942705154419}]}, {"text": "It alleviates the effects of the huge search space which is represented in word graphs by using a compact excerpt of then best hypotheses generated by the system.", "labels": [], "entities": []}, {"text": "Especially for limited domain tasks, the size of the n-best list can be rather small but still yield good oracle error rates.", "labels": [], "entities": []}, {"text": "Empirically, n-best lists should have an appropriate size such that the oracle error rate, i.e. the error rate of the best hypothesis with respect to an error measure (such as WER or PER) is approximately half the baseline error rate of the system.", "labels": [], "entities": [{"text": "PER", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.619319498538971}]}, {"text": "N -best lists are suitable for easily applying several rescoring techniques since the hypotheses are already fully generated.", "labels": [], "entities": []}, {"text": "In comparison, word graph rescoring techniques need specialized tools which can traverse the graph accordingly.", "labels": [], "entities": [{"text": "word graph rescoring", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7241517702738444}]}, {"text": "Since anode within a word graph allows for many histories, one can only apply local rescoring techniques, whereas for n-best lists, techniques can be used that consider properties of the whole sentence.", "labels": [], "entities": []}, {"text": "For the Chinese-English and Arabic-English task, we set the n-best list size ton = 1500.", "labels": [], "entities": []}, {"text": "For Japanese-English, n = 1000 produces oracle error rates that are deemed to be sufficiently low, namely 17.7% and 14.8% for WER and PER, respectively.", "labels": [], "entities": [{"text": "PER", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.8428983688354492}]}, {"text": "The single-best output for JapaneseEnglish has a word error rate of 33.3% and position-independent word error rate of 25.9%.", "labels": [], "entities": [{"text": "JapaneseEnglish", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.8749045729637146}, {"text": "word error rate", "start_pos": 49, "end_pos": 64, "type": "METRIC", "confidence": 0.8037362297375997}, {"text": "position-independent word error rate", "start_pos": 78, "end_pos": 114, "type": "METRIC", "confidence": 0.6815101951360703}]}, {"text": "For the experiments, we add additional features to the initial models of our decoder that have shown to be particularly useful in the past, such as IBM model 1 score, a clustered language model score and a word penalty that prevents the hypotheses to become too short.", "labels": [], "entities": [{"text": "IBM model 1 score", "start_pos": 148, "end_pos": 165, "type": "METRIC", "confidence": 0.7520480901002884}]}, {"text": "A detailed definition of these additional features is given in ( ).", "labels": [], "entities": []}, {"text": "Thus, the baseline we start with is   already a very strong one.", "labels": [], "entities": []}, {"text": "The log-linear interpolation weights \u03bb m from Eq.", "labels": [], "entities": []}, {"text": "3 are directly optimized using the Downhill Simplex algorithm on a linear combination of WER (word error rate), PER (position-independent word error rate), NIST and BLEU score.", "labels": [], "entities": [{"text": "Downhill Simplex", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9191440045833588}, {"text": "WER (word error rate)", "start_pos": 89, "end_pos": 110, "type": "METRIC", "confidence": 0.8720280627409617}, {"text": "PER (position-independent word error rate)", "start_pos": 112, "end_pos": 154, "type": "METRIC", "confidence": 0.8590406690325055}, {"text": "NIST", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.7212103605270386}, {"text": "BLEU score", "start_pos": 165, "end_pos": 175, "type": "METRIC", "confidence": 0.9792285263538361}]}, {"text": "In, we show the effect of adding the presented features successively to the baseline.", "labels": [], "entities": []}, {"text": "Separate entries for experiments using supertagging/LDA and link grammars show that a combination of these syntactic approaches always yields some gain in translation quality (regarding BLEU score).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 186, "end_pos": 196, "type": "METRIC", "confidence": 0.9845092594623566}]}, {"text": "The performance of the maximum-entropy based chunking is comparable.", "labels": [], "entities": []}, {"text": "A combination of all three models still yields a small improvement.", "labels": [], "entities": []}, {"text": "shows some examples for the ChineseEnglish test set.", "labels": [], "entities": [{"text": "ChineseEnglish test set", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.9854697982470194}]}, {"text": "The rescored translations are syntactically coherent, though semantical correctness cannot be guaranteed.", "labels": [], "entities": []}, {"text": "On the test data, we achieve an overall improvement of 0.7%, 0.5% and 0.3% in BLEU score for Chinese-English, JapaneseEnglish and Arabic-English, respectively (cf. Tables 4 and 5).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9774602949619293}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics after preprocessing.", "labels": [], "entities": []}, {"text": " Table 2: Effect of successively adding syntactic features to the Chinese-English n-best list for C-Star'03  (development set) and IWSLT'04 (test set).", "labels": [], "entities": [{"text": "IWSLT'04 (test set", "start_pos": 131, "end_pos": 149, "type": "DATASET", "confidence": 0.7810211181640625}]}, {"text": " Table 4: Effect of successively adding syntactic features to the Japanese-English n-best list for C-Star'03  (development set) and IWSLT'04 (test set).", "labels": [], "entities": [{"text": "IWSLT'04 (test set", "start_pos": 132, "end_pos": 150, "type": "DATASET", "confidence": 0.7738192081451416}]}, {"text": " Table 5: Effect of successively adding syntactic features to the Arabic-English n-best list for C-Star'03  (development set) and IWSLT'04 (test set).", "labels": [], "entities": [{"text": "IWSLT'04 (test set", "start_pos": 130, "end_pos": 148, "type": "DATASET", "confidence": 0.7785974442958832}]}]}