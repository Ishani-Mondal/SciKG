{"title": [{"text": "What's in a name? The automatic recognition of metonymical location names", "labels": [], "entities": [{"text": "automatic recognition of metonymical location", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.722104525566101}]}], "abstractContent": [{"text": "The correct identification of metonymies is not normally a problem for most people.", "labels": [], "entities": [{"text": "correct identification of metonymies", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.6883914694190025}]}, {"text": "For computers, things are different, however.", "labels": [], "entities": []}, {"text": "In Natural Language Processing, metonymy recognition is therefore usually addressed with complex algorithms that rely on hundreds of labelled training examples.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.946528434753418}]}, {"text": "This paper investigates two approaches to metonymy recognition that dispense with this complexity, albeit in different ways.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.9626602828502655}]}, {"text": "The first, an unsuper-vised approach to Word Sense Discrimination , does not require any labelled training instances.", "labels": [], "entities": [{"text": "Word Sense Discrimination", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6468905210494995}]}, {"text": "The second, Memory-Based Learning, replaces the complexity of current algorithms by a 'lazy' learning phase.", "labels": [], "entities": []}, {"text": "While the first approach is often able to identify a metonymical and a literal cluster in the data, it is the second in particular that produces state-of-the-art results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last few years, metonymy has emerged as an important focus of research in many areas of linguistics.", "labels": [], "entities": []}, {"text": "In Cognitive Linguistics, it is often defined as \"a cognitive process in which one conceptual entity, the vehicle, provides mental access to another conceptual entity, the target, within the same domain, or idealized cognitive model (ICM)\".", "labels": [], "entities": [{"text": "Cognitive Linguistics", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.780170202255249}]}, {"text": "In example (1), for instance, China and Taiwan provide mental access to the governments of the respective countries: (1) China has always threatened to use force if Taiwan declared independence.", "labels": [], "entities": []}, {"text": "(BNC) This paper is concerned with algorithms that automatically recognize such metonymical country names.", "labels": [], "entities": []}, {"text": "These are extremely relevant in Natural Language Processing, since any system that automatically builds semantic representations of utterances needs to be able to recognize and interpret metonymical words.", "labels": [], "entities": []}, {"text": "Early approaches to metonymy recognition, such as, identified a word as metonymical when it violated certain selectional restrictions.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.9334165751934052}]}, {"text": "Indeed, in example (1), China and Taiwan both violate the restriction that threaten and declare require an animate subject, and thus have to be interpreted metonymically.", "labels": [], "entities": []}, {"text": "This view is present in the psycholinguistic literature, too.", "labels": [], "entities": []}, {"text": "Some authors argue that a figurative interpretation of a word typically comes about when all literal interpretations fail; see for an overview.", "labels": [], "entities": []}, {"text": "This failure is often due to the violation of selectional restrictions.", "labels": [], "entities": []}, {"text": "However, in psycholinguistics as well as in computational linguistics, this approach has lost much of its appeal.", "labels": [], "entities": []}, {"text": "It has become clear to researchers in both fields that many metonymies do not violate any restrictions at all.", "labels": [], "entities": []}, {"text": "In to like Shakespeare, for instance, there is no explicit linguistic trigger for the metonymical interpretation of Shakespeare.", "labels": [], "entities": [{"text": "metonymical interpretation of Shakespeare", "start_pos": 86, "end_pos": 127, "type": "TASK", "confidence": 0.8801332265138626}]}, {"text": "Rather, it is our world knowledge that pre-empts a literal reading of the author's name.", "labels": [], "entities": []}, {"text": "Examples like this one demonstrate that metonymy recognition should not be based on rigid rules, but rather, on information about the semantic class of the target word and the semantic and grammatical context in which it occurs.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.9692087173461914}]}, {"text": "In psycholinguistics, this insight (among others) has given rise to theories claiming that a figurative interpretation does not follow the failure of a literal one, but that both processes occur in parallel).", "labels": [], "entities": []}, {"text": "In computational linguistics, it has led to the development of statisti-cal, corpus-based approaches to metonymy recognition.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.9057216048240662}]}, {"text": "This view was first put into computational practice by.", "labels": [], "entities": []}, {"text": "Their key to success was the realization that metonymy recognition is a sub-problem of Word Sense Disambiguation (WSD).", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.9160519242286682}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.7557976841926575}]}, {"text": "They found that most metonymies in the same semantic class belong to one of a limited number of metonymical patterns that can be defined a priori.", "labels": [], "entities": []}, {"text": "The task of metonymy recognition thus consists of the automatic assignment of one of these readings to a target word.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.9651941657066345}]}, {"text": "Since all words in the same semantic class may undergo the same semantic shifts, there only has to be one classifier per class (and not per word, as in classic WSD).", "labels": [], "entities": []}, {"text": "In this paper I will be concerned with the automatic identification of metonymical location names.", "labels": [], "entities": [{"text": "automatic identification of metonymical location names", "start_pos": 43, "end_pos": 97, "type": "TASK", "confidence": 0.8257727722326914}]}, {"text": "More particularly, I will test two new approaches to metonymy recognition on the basis of Markert and Nissim's (2002b) corpora of 1,000 mixed country names and 1,000 instances of the country name Hungary.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.9661825895309448}]}, {"text": "The most important metonymical patterns in these corpora are place-for-people, place-for-event and place-for-product.", "labels": [], "entities": []}, {"text": "In addition, there is a label mixed for examples that have two readings, and othermet for examples that do not belong to any of the pre-defined metonymical patterns.", "labels": [], "entities": []}, {"text": "On the mixed country data, classifiers achieved an accuracy of 87%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9997077584266663}]}, {"text": "This was the result of a combination of both grammatical and semantic information.", "labels": [], "entities": []}, {"text": "Their grammatical information included the function of a target word and its head.", "labels": [], "entities": []}, {"text": "The semantic information, in the form of Dekang Lin's (1998) thesaurus of semantically similar words, allowed the classifier to search the training set for instances whose head was similar, and not just identical, to that of a test instance. and study is the only one to approach metonymy recognition from a data-driven, statistical perspective.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 280, "end_pos": 300, "type": "TASK", "confidence": 0.9390506148338318}]}, {"text": "However, it also has a number of disadvantages.", "labels": [], "entities": []}, {"text": "First, it requires the annotation of a large number of training and test instances.", "labels": [], "entities": []}, {"text": "This compromises its possible application to a wide variety of metonymical patterns across a large num-1 This data is publicly available and can be downloaded from http://homepages.inf.ed.ac.uk/mnissim/mascara.", "labels": [], "entities": []}, {"text": "Second, its algorithms are rather complex.", "labels": [], "entities": []}, {"text": "In the training phase, they calculate smoothed probabilities on the basis of a large annotated training corpus and in the test phase, they iteratively search through a thesaurus of semantically similar words.", "labels": [], "entities": []}, {"text": "This leads to the question if this complexity is indeed necessary in metonymy recognition.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.9388363361358643}]}, {"text": "This paper investigates two approaches that each tackle one of these problems.", "labels": [], "entities": []}, {"text": "The unsupervised algorithm in section 2 has the intuitive appeal of not requiring any annotated training instances.", "labels": [], "entities": []}, {"text": "I will show that it is nevertheless often able to distinguish between two data clusters that correlate with the two target readings.", "labels": [], "entities": []}, {"text": "In section 3, I will again take recourse to a supervised learning method, but one that explicitly incorporates a much simpler learning phase than its competitors in the literature -Memory-Based Learning.", "labels": [], "entities": []}, {"text": "I will demonstrate that this algorithm of 'lazy learning' gives state-of-the-art results in metonymy recognition.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.9422470331192017}]}, {"text": "Moreover, although their psychological validity is not a focus of the present investigation, the two studied algorithms have clear links to models of human behaviour.", "labels": [], "entities": []}], "datasetContent": [{"text": "I again applied this IB1-IG algorithm to Markert and Nissim's (2002b) location corpora.", "labels": [], "entities": []}, {"text": "In order to make my results as comparable as possible to and, I made two changes in the evaluation process.", "labels": [], "entities": []}, {"text": "First, evaluation was now performed with 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "Second, in the calculation of accuracy, I made a distinction between the several metonymical labels, so that a misclassification within the metonymical category was penalized as well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9989774227142334}]}, {"text": "I conducted two rounds of experiments.", "labels": [], "entities": []}, {"text": "The first used only grammatical features: the grammatical function of the word (subj, obj, iobj, pp, gen, premod, passive subj, other), its head, the presence of a second head, and the second head (if present).", "labels": [], "entities": []}, {"text": "Such features can be expected to identify metonymies with a high precision, but since metonymies may have a wide variety of heads, performance will likely suffer from data sparseness.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9948030114173889}]}, {"text": "I therefore conducted a second round of experiments, in which I added semantic information to the feature sets, in the form of the WordNet hypernym synsets of the head's first sense.", "labels": [], "entities": [{"text": "WordNet hypernym synsets", "start_pos": 131, "end_pos": 155, "type": "DATASET", "confidence": 0.8949593305587769}]}, {"text": "WordNet is a machine-readable lexical database that, among other things, structures English verbs, nouns and adjectives in a hierarchy of so-called \"synonym sets\" or synsets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9421797394752502}]}, {"text": "Each word belongs to such a group of synonyms, and each synset \"is related to its immediately more general and more specific synsets via direct hypernym and hyponym relations\".", "labels": [], "entities": []}, {"text": "Fear, for instance, belongs to the synset fear, fearfulness, fright, which has emotion as its most immediate, and psychological fea-  ture as its highest hypernym.", "labels": [], "entities": []}, {"text": "This tree structure of synsets thus corresponds to a hierarchy of semantic classes that can be used to add semantic knowledge to a metonymy recognition system.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.7317878007888794}]}, {"text": "My experiments investigated a few constellations of semantic features.", "labels": [], "entities": []}, {"text": "The simplest of these used the highest hypernym synset of the head's first sense as an extra feature.", "labels": [], "entities": []}, {"text": "A second approach added to the feature vector the head's highest hypernym synsets, with a maximum often.", "labels": [], "entities": []}, {"text": "If the head did not have 10 hypernyms, its own synset would fill the remaining features.", "labels": [], "entities": []}, {"text": "The result of this last approach is that the MBL classifier first looks for heads within the same synset as the test head.", "labels": [], "entities": []}, {"text": "If it does not find a word that shares all hypernyms with the test instance, it gradually climbs the synset hierarchy until it finds the training instances that share as many hypernyms as possible.", "labels": [], "entities": []}, {"text": "Obviously, this approach is able to make more fine-grained semantic distinctions than the previous one.", "labels": [], "entities": []}, {"text": "The experiments with grammatical information showed that TiMBL is able to replicate results.", "labels": [], "entities": []}, {"text": "The obtained accuracy and F-scores for the mixed country names in table 4 are almost identical to Nissim and Markert's figures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.999546229839325}, {"text": "F-scores", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9979395270347595}]}, {"text": "The results for the Hungary data in table 5 lie slightly lower, but again mirror Nissim and Markert's figures closely.", "labels": [], "entities": [{"text": "Hungary data", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.8549852669239044}]}, {"text": "This is all the more promising since my results were reached without any semantic information.", "labels": [], "entities": []}, {"text": "Remember that Nissim and Markert's algorithm, in contrast, used Dekang Lin's (1998) clusters of semantically similar words in order to deal with data sparseness.", "labels": [], "entities": []}, {"text": "Memory-Based Learning does not appear to need this semantic information to arrive at state-of-theart performance.", "labels": [], "entities": []}, {"text": "Instead, it tackles possible data sparseness by its automatic back-off to the grammatical role if the target's head is not found among the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the mixed country data of four algorithms with varying context sizes and without a  stoplist.", "labels": [], "entities": []}, {"text": " Table 2: Results on the Hungary data of four algorithms with varying context sizes and without a stoplist.", "labels": [], "entities": [{"text": "Hungary data", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.7983539998531342}]}, {"text": " Table 3: Results on the Hungary data of four algorithms with varying context sizes and with a stoplist.", "labels": [], "entities": [{"text": "Hungary data", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.8079730868339539}]}, {"text": " Table 4: Results for the mixed country data.  TiMBL: TiMBL's results  N&M: Nissim and Markert's (2003) results", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8341081142425537}]}]}