{"title": [{"text": "Style & Topic Language Model Adaptation Using HMM-LDA", "labels": [], "entities": [{"text": "Style & Topic Language Model Adaptation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5596897105375925}, {"text": "HMM-LDA", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.5673183798789978}]}], "abstractContent": [{"text": "Adapting language models across styles and topics, such as for lecture transcription , involves combining generic style models with topic-specific content relevant to the target document.", "labels": [], "entities": [{"text": "lecture transcription", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7245944291353226}]}, {"text": "In this work, we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation (HMM-LDA) to obtain syntactic state and semantic topic assignments to word instances in the training corpus.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (HMM-LDA)", "start_pos": 69, "end_pos": 106, "type": "METRIC", "confidence": 0.876154234011968}]}, {"text": "From these context-dependent labels, we construct style and topic models that better model the target document, and extend the traditional bag-of-words topic models to n-grams.", "labels": [], "entities": []}, {"text": "Experiments with static model interpolation yielded a perplexity and relative word error rate (WER) reduction of 7.1% and 2.1%, respectively , over an adapted trigram base-line.", "labels": [], "entities": [{"text": "relative word error rate (WER) reduction", "start_pos": 69, "end_pos": 109, "type": "METRIC", "confidence": 0.8743735961616039}]}, {"text": "Adaptive interpolation of mixture components further reduced perplexity by 9.5% and WER by a modest 0.3%.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9945534467697144}, {"text": "WER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9976377487182617}]}], "introductionContent": [{"text": "With the rapid growth of audio-visual materials available over the web, effective language modeling of the diverse content, both in style and topic, becomes essential for efficient access and management of this information.", "labels": [], "entities": []}, {"text": "As a prime example, successful language modeling for academic lectures not only enables the initial transcription via automatic speech recognition, but also assists educators and students in the creation and navigation of these materials through annotation, retrieval, summarization, and even translation of the embedded content.", "labels": [], "entities": [{"text": "summarization", "start_pos": 269, "end_pos": 282, "type": "TASK", "confidence": 0.9641085267066956}, {"text": "translation of the embedded content", "start_pos": 293, "end_pos": 328, "type": "TASK", "confidence": 0.642901349067688}]}, {"text": "Compared with other types of audio content, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with specific terminology).", "labels": [], "entities": []}, {"text": "Unfortunately, training corpora available for language modeling rarely match the target lecture in both style and topic.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7911253273487091}]}, {"text": "While transcripts from other lectures better match the style of the target lecture than written text, it is often difficult to find transcripts on the target topic.", "labels": [], "entities": []}, {"text": "On the other hand, although topic-specific vocabulary can be gleaned from related text materials, such as the textbook and lecture slides, written language is a poor predictor of how words are actually spoken.", "labels": [], "entities": []}, {"text": "Furthermore, given that the precise topic of a target lecture is often unknown a priori and may even shift overtime, it is generally difficult to identify topically related documents.", "labels": [], "entities": []}, {"text": "Thus, an effective language model (LM) need to not only account for the casual speaking style of lectures, but also accommodate the topic-specific vocabulary of the subject matter.", "labels": [], "entities": []}, {"text": "Moreover, the ability of the language model to dynamically adapt over the course of the lecture could prove extremely useful for both increasing transcription accuracy, as well as providing evidence for lecture segmentation and information retrieval.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9717294573783875}, {"text": "lecture segmentation", "start_pos": 203, "end_pos": 223, "type": "TASK", "confidence": 0.7057615220546722}, {"text": "information retrieval", "start_pos": 228, "end_pos": 249, "type": "TASK", "confidence": 0.8207242786884308}]}, {"text": "In this paper, we investigate the application of the syntactic state and semantic topic assignments from the Hidden Markov Model with Latent Dirichlet Allocation model to the problem of language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.7360707074403763}]}, {"text": "We explore the use of these context-dependent labels to identify style and learn topics from both a large number of spoken lectures as well as written text.", "labels": [], "entities": []}, {"text": "By dynamically interpolating lecture style models with topicspecific models, we obtain language models that better describe the subtopic structure within a lecture.", "labels": [], "entities": []}, {"text": "Initial experiments demonstrate a 16.1% perplexity reduction and a 2.4% WER reduction over an adapted trigram baseline.", "labels": [], "entities": [{"text": "WER reduction", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.9899540543556213}]}, {"text": "In the following sections, we first summarize related research on adaptive and topic-mixture language models, and describe previous work on the HMM-LDA model.", "labels": [], "entities": []}, {"text": "We then examine the ability of the model to learn syntactic classes as well as topics from textbook materials and lecture transcripts.", "labels": [], "entities": []}, {"text": "Next, we describe a variety of language model experiments we performed to combine style and topic models constructed from the state and topic labels with conventional trigram models trained from both spoken and written materials.", "labels": [], "entities": []}, {"text": "We also demonstrate the use of the combined model in an on-line adaptive mode.", "labels": [], "entities": []}, {"text": "Finally, we summarize the results of this research and suggest future opportunities for related modeling techniques in spoken lecture and other content processing research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the effectiveness of models derived from the separation of syntax from content, we performed experiments that compare the perplexities and WERs of various model combinations.", "labels": [], "entities": [{"text": "WERs", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9855315685272217}]}, {"text": "For a baseline, we used an adapted model (L+T) that linearly interpolates trigram models trained on the Lectures (L) and Textbook (T) datasets.", "labels": [], "entities": [{"text": "Lectures (L) and Textbook (T) datasets", "start_pos": 104, "end_pos": 142, "type": "DATASET", "confidence": 0.6811907082796097}]}, {"text": "In all models, all interpolation weights and additional parameters are tuned on a development set consisting of the first half of the CS lectures and tested on the second half.", "labels": [], "entities": []}, {"text": "Unless otherwise noted, modified Kneser-Ney discounting) is applied with the respective training set vocabulary using the SRILM Toolkit.", "labels": [], "entities": [{"text": "SRILM Toolkit", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.9215834438800812}]}, {"text": "To compute the word error rates associated with a specific language model, we used a speaker-independent speech recognizer.", "labels": [], "entities": [{"text": "speaker-independent speech recognizer", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.5951409538586935}]}, {"text": "The lectures were pre-segmented into utterances by forced alignment of the reference transcription.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of evaluation datasets.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8235320448875427}]}]}