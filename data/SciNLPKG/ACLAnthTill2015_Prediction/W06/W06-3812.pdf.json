{"title": [{"text": "Chinese Whispers -an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges.", "labels": [], "entities": [{"text": "Chinese Whispers", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.8630810081958771}]}, {"text": "After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation.", "labels": [], "entities": [{"text": "language separation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.7407273501157761}, {"text": "word sense disambiguation", "start_pos": 266, "end_pos": 291, "type": "TASK", "confidence": 0.6948178211847941}]}, {"text": "At this, the fact is employed that the small-world property holds for many graphs in NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Clustering is the process of grouping together objects based on their similarity to each other.", "labels": [], "entities": []}, {"text": "In the field of Natural Language Processing (NLP), there area variety of applications for clustering.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.7163355549176534}]}, {"text": "The most popular ones are document clustering in applications related to retrieval and word clustering for finding sets of similar words or concept hierarchies.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7475091814994812}, {"text": "word clustering", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7688632309436798}]}, {"text": "Traditionally, language objects are characterized by a feature vector.", "labels": [], "entities": []}, {"text": "These feature vectors can be interpreted as points in a multidimensional space.", "labels": [], "entities": []}, {"text": "The clustering uses a distance metric, e.g. the cosine of the angle between two such vectors.", "labels": [], "entities": []}, {"text": "As in NLP there are often several thousand features, of which only a few correlate with each other at a time -think about the number of different words as opposed to the number of words occurring in a sentencedimensionality reduction techniques can greatly reduce complexity without considerably losing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 303, "end_pos": 311, "type": "METRIC", "confidence": 0.9955213069915771}]}, {"text": "An alternative representation that does not deal with dimensions in space is the graph representation.", "labels": [], "entities": []}, {"text": "A graph represents objects (as nodes) and their relations (as edges).", "labels": [], "entities": []}, {"text": "In NLP, there area variety of structures that can be naturally represented as graphs, e.g. lexical-semantic word nets, dependency trees, co-occurrence graphs and hyperlinked documents, just to name a few.", "labels": [], "entities": []}, {"text": "Clustering graphs is a somewhat different task than clustering objects in a multidimensional space: There is no distance metric; the similarity between objects is encoded in the edges.", "labels": [], "entities": []}, {"text": "Objects that do not share an edge cannot be compared, which gives rise to optimization techniques.", "labels": [], "entities": []}, {"text": "There is no centroid or 'average cluster member' in a graph, permitting centroid-based techniques.", "labels": [], "entities": []}, {"text": "As data sets in NLP are usually large, there is a strong need for efficient methods, i.e. of low computational complexities.", "labels": [], "entities": []}, {"text": "In this paper, a very efficient graph-clustering algorithm is introduced that is capable of partitioning very large graphs in comparatively short time.", "labels": [], "entities": []}, {"text": "Especially for smallworld graphs, high performance is reached in quality and speed.", "labels": [], "entities": [{"text": "quality", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9629810452461243}, {"text": "speed", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9715529680252075}]}, {"text": "After explaining the algorithm in the next section, experiments with synthetic graphs are reported in section 3.", "labels": [], "entities": []}, {"text": "These give an insight about the algorithm's performance.", "labels": [], "entities": []}, {"text": "In section 4, experiments on three NLP tasks are reported, section 5 concludes by discussing extensions and further application areas.", "labels": [], "entities": []}], "datasetContent": [{"text": "The analysis of the CW process is difficult due to its nonlinear nature.", "labels": [], "entities": []}, {"text": "Its run-time complexity indicates that it cannot directly optimize most global graph cluster measures because of their NPcompleteness ().", "labels": [], "entities": []}, {"text": "Therefore we perform experiments on synthetic graphs to empirically arrive at an impression of our algorithm's abilities.", "labels": [], "entities": []}, {"text": "All experiments were conducted with an implementation following figure 1.", "labels": [], "entities": []}, {"text": "For experiments with synthetic graphs, we restrict ourselves to unweighted graphs, if not stated explicitly.", "labels": [], "entities": []}, {"text": "In this section, some experiments with graphs originating from natural language data are presented.", "labels": [], "entities": []}, {"text": "First, we define the notion of cooccurrence graphs, which are used in sections 4.1 and 4.3: Two words co-occur if they can both be found in a certain unit of text, here a sentence.", "labels": [], "entities": []}, {"text": "Employing a significance measure, we determine whether their co-occurrences are significant or random.", "labels": [], "entities": []}, {"text": "In this case, we use the log-likelihood measure as described in.", "labels": [], "entities": []}, {"text": "We use the words as nodes in the graph.", "labels": [], "entities": []}, {"text": "The weight of an edge between two words is set to the significance value of their co-occurrence, if it exceeds a certain threshold.", "labels": [], "entities": []}, {"text": "In the experiments, we used significances from 15 on.", "labels": [], "entities": []}, {"text": "The entirety of words that are involved in at least one edge together with these edges is called co-occurrence graph (cf.).", "labels": [], "entities": []}, {"text": "In general, CW produces a large number of clusters on real-world graphs, of which the majority is very small.", "labels": [], "entities": []}, {"text": "For most applications, it might be advisable to define a minimum cluster size or something alike.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: normalized Mutual Information values for  three graphs and different iterations in %.", "labels": [], "entities": []}, {"text": " Table 3: Disambiguation results in % dependent on  word class (nouns, verbs, adjectives)", "labels": [], "entities": []}, {"text": " Table 4: Disambiguation results in % dependent on  frequency", "labels": [], "entities": []}]}