{"title": [{"text": "Improved Large Margin Dependency Parsing via Local Constraints and Laplacian Regularization", "labels": [], "entities": [{"text": "Improved Large Margin Dependency Parsing", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.686702984571457}, {"text": "Regularization", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.4614211916923523}]}], "abstractContent": [{"text": "We present an improved approach for learning dependency parsers from tree-bank data.", "labels": [], "entities": [{"text": "learning dependency parsers", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6701609889666239}]}, {"text": "Our technique is based on two ideas for improving large margin training in the context of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8445464074611664}]}, {"text": "First, we incorporate local constraints that enforce the correctness of each individual link, rather than just scoring the global parse tree.", "labels": [], "entities": []}, {"text": "Second, to cope with sparse data, we smooth the lexical parameters according to their underlying word similarities using Laplacian Regularization.", "labels": [], "entities": []}, {"text": "To demonstrate the benefits of our approach, we consider the problem of parsing Chi-nese treebank data using only lexical features , that is, without part-of-speech tags or grammatical categories.", "labels": [], "entities": []}, {"text": "We achieve state of the art performance, improving upon current large margin approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data).", "labels": [], "entities": []}, {"text": "Most of the early work in this area was based on postulating generative probability models of language that included parse structure.", "labels": [], "entities": []}, {"text": "Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems.", "labels": [], "entities": []}, {"text": "Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. \"maximum entropy\") to be applied).", "labels": [], "entities": []}, {"text": "In fact, recently, effective conditional parsing models have been learned using relatively straightforward \"plug-in\" estimates, augmented with similarity based smoothing ().", "labels": [], "entities": [{"text": "conditional parsing", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.6467873156070709}]}, {"text": "Currently, the work on conditional parsing models appears to have culminated in large margin training), which currently demonstrates the state of the art performance in English dependency parsing).", "labels": [], "entities": [{"text": "conditional parsing", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.48587195575237274}, {"text": "English dependency parsing", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.5825759569803873}]}, {"text": "Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches.", "labels": [], "entities": []}, {"text": "For example, smoothing methods have played a central role in probabilistic approaches, and yet they are not being used in current large margin training algorithms.", "labels": [], "entities": []}, {"text": "However, as we demonstrate, not only can smoothing be applied in a large margin training framework, it leads to generalization improvements in much the same way as probabilistic approaches.", "labels": [], "entities": []}, {"text": "The second key observation we make is somewhat more subtle.", "labels": [], "entities": []}, {"text": "It turns out that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach-the \"structured margin loss\" ()-is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.", "labels": [], "entities": []}, {"text": "In this paper, we make two contributions to the large margin approach to learning parsers from supervised data.", "labels": [], "entities": []}, {"text": "First, we show that smoothing based on lexical similarity is not only possible in the large margin framework, but more importantly, allows better generalization to new words not encountered during training.", "labels": [], "entities": []}, {"text": "Second, we show that the large margin training objective can be significantly refined to assess the error of each component of a given parse, rather than just assess a global score.", "labels": [], "entities": []}, {"text": "We show that these two extensions together lead to greater training accuracy and better generalization to novel input sentences than current large margin methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9901818633079529}]}, {"text": "To demonstrate the benefit of combining useful learning principles from both the probabilistic and large margin frameworks, we consider the problem of learning a dependency parser for Chinese.", "labels": [], "entities": []}, {"text": "This is an interesting test domain because Chinese does not have clearly defined parts-of-speech, which makes lexical smoothing one of the most natural approaches to achieving reasonable results ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our method experimentally on the Chinese Treebank (CTB) ().", "labels": [], "entities": [{"text": "Chinese Treebank (CTB)", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.9795424103736877}]}, {"text": "The parse trees in CTB are constituency structures.", "labels": [], "entities": []}, {"text": "We converted them into dependency trees using the same method and head-finding rules as in).", "labels": [], "entities": []}, {"text": "Following, we used Sections 1-270 for training, Sections 271-300 for testing and Sections 301-325 for development.", "labels": [], "entities": []}, {"text": "We experimented with two sets of data: CTB-10 and CTB-15, which contains sentences with no more than 10 and 15 words respectively., and show our experimental results trained and evaluated on Chinese Treebank sentences of length no more than 10, using the standard split.", "labels": [], "entities": [{"text": "Chinese Treebank sentences", "start_pos": 191, "end_pos": 217, "type": "DATASET", "confidence": 0.9268780946731567}]}, {"text": "For any unseen link in the new sentences, the weight is computed as the similarity weighted average of similar links seen in the training corpus.", "labels": [], "entities": [{"text": "similarity weighted average", "start_pos": 72, "end_pos": 99, "type": "METRIC", "confidence": 0.9456169009208679}]}, {"text": "The regularization parameter twas set by 5-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "We evaluate parsing accuracy by comparing the undirected dependency links in the parser outputs against the undirected links in the treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9750751852989197}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9727389812469482}]}, {"text": "We define the accuracy of the parser to be the percentage of correct dependency links among the total set of dependency links created by the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999297022819519}]}, {"text": "show that training based on the more refined local loss is far superior to training with the global loss of standard large margin training, on both the test and development sets.", "labels": [], "entities": []}, {"text": "Parsing accuracy also appears to increase with the introduction of each new feature.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8801656365394592}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9215580224990845}]}, {"text": "Notably, the pointwise mutual information and distance features significantly improve parsing accuracy-and yet we know of no other research that has investigated these features in this context.", "labels": [], "entities": [{"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9838522672653198}, {"text": "accuracy-and", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.9667937159538269}]}, {"text": "Finally, we note that Laplacian regularization improved performance as expected, but not for the global loss, where it appears to systematically degrade performance (n/a results did not complete in time).", "labels": [], "entities": []}, {"text": "It seems that the global loss model may have been over-regularized.", "labels": [], "entities": []}, {"text": "However, we have picked the t parameter which gave us the best resutls in our experiments.", "labels": [], "entities": [{"text": "resutls", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9512314796447754}]}, {"text": "One possible explanation for this phenomenon is that the interaction between the Laplician regularization in training and the similarity smoothing in parsing, since distributional word similarities are used in both cases.", "labels": [], "entities": []}, {"text": "Finally, we compared our results to the probabilistic parsing approach of (), which on this data obtained accuracies of 0.7631 on the CTB test set and 0.6104 on the development set.", "labels": [], "entities": [{"text": "CTB test set", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.979920486609141}]}, {"text": "However, we are using a much simpler feature set here.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy Results on CTB Test Set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993212223052979}, {"text": "CTB Test Set", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9575905203819275}]}, {"text": " Table 2: Accuracy Results on CTB Dev Set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992284774780273}, {"text": "CTB Dev Set", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9717573126157125}]}, {"text": " Table 3: Accuracy Results on CTB Training Set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993335604667664}, {"text": "CTB Training", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.855920821428299}]}]}