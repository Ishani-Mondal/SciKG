{"title": [{"text": "Relating WordNet Senses for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Relating WordNet Senses", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7778124014536539}, {"text": "Word Sense Disambiguation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6185367206732432}]}], "abstractContent": [{"text": "The granularity of word senses in current general purpose sense inventories is often too fine-grained, with narrow sense distinctions that are irrelevant for many NLP applications.", "labels": [], "entities": []}, {"text": "This has particularly been a problem with WordNet which is widely used for word sense disambigua-tion (WSD).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9445163607597351}, {"text": "word sense disambigua-tion (WSD)", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.8397178798913956}]}, {"text": "There have been several attempts to group WordNet senses given a number of different information sources in order to reduce granularity.", "labels": [], "entities": []}, {"text": "We propose relating senses as a matter of degree to permit a softer notion of relationships between senses compared to fixed groupings so that granularity can be varied according to the needs of the application.", "labels": [], "entities": []}, {"text": "We compare two such approaches with a gold-standard produced by humans for this work.", "labels": [], "entities": []}, {"text": "We also contrast this gold-standard and another used in previous research with the automatic methods for relating senses for use with back-off methods for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.924646258354187}]}], "introductionContent": [{"text": "It is likely that accurate word-level semantic disambiguation would benefit a number of different types of NLP application; however it is generally acknowledged byword sense disambiguation (WSD) researchers that current levels of accuracy need to be improved before WSD technology can usefully be integrated into applications.", "labels": [], "entities": [{"text": "word-level semantic disambiguation", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6319010754426321}, {"text": "sense disambiguation (WSD)", "start_pos": 168, "end_pos": 194, "type": "TASK", "confidence": 0.7537826180458069}, {"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.9953586459159851}]}, {"text": "There are at least two major problems facing researchers in this area.", "labels": [], "entities": []}, {"text": "One major problem is the lack of sufficient training data for supervised WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9763193130493164}]}, {"text": "One response to this is  to exploit the natural skew of the data and focus on finding the first (predominant) sense from a sample of text ().", "labels": [], "entities": []}, {"text": "Further contextual WSD maybe required, but the technique provides a useful unsupervised back-off method.", "labels": [], "entities": [{"text": "WSD", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7446635961532593}]}, {"text": "The other major problem for WSD is the granularity of the sense inventory since a pre-existing lexical resource is often too fine-grained, with narrow sense distinctions that are irrelevant for the intended application.", "labels": [], "entities": [{"text": "WSD", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9753456115722656}]}, {"text": "For example, WordNet) which is widely used and publicly available, has a great many subtle distinctions that may in the end not be required.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9524996280670166}]}, {"text": "For example, in we show the three senses (WNs#) for evidence from WordNet version 1.7. 1 These are all clearly related.", "labels": [], "entities": [{"text": "WordNet version 1.7. 1", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.919877678155899}]}, {"text": "One promising approach for improving accuracy is to disambiguate to a coarser-grained inventory, which groups together the related senses of a word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9976980090141296}]}, {"text": "This can be done either by defining the inventory specifically for the application, which might be most appropriate for machine translation, where correspondences across languages could determine the inventory).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7585542500019073}]}, {"text": "There are however many systems using man-made resources, particularly WordNet, which have other purposes in mind, such as entailment for applications such as question-answering and information-extraction ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9669471383094788}]}, {"text": "There have been several attempts to group WordNet senses using various different types of information sources.", "labels": [], "entities": []}, {"text": "This paper describes work to automatically relate WordNet word senses using automatically acquired thesauruses and WordNet similarity measures).", "labels": [], "entities": []}, {"text": "This work proposes using graded word sense relationships rather than fixed groupings (clusters).", "labels": [], "entities": []}, {"text": "Previous research has focused on clustering WordNet senses into groups.", "labels": [], "entities": [{"text": "clustering WordNet senses", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6857778032620748}]}, {"text": "One problem is that to do this a stopping condition is required such as the number of clusters required for each word.", "labels": [], "entities": []}, {"text": "This has been done with the numbers determined by the gold-standard for the purposes of evaluation (Agirre and Lopez) but ultimately the right number of classes for each word cannot usually be predetermined even if one knows the application, unless only a sample of words are being handled.", "labels": [], "entities": []}, {"text": "In cases where a goldstandard is provided by humans it is clear that further relationships could be drawn.", "labels": [], "entities": [{"text": "goldstandard", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.9728808999061584}]}, {"text": "For example, in the groups (hereafter referred to as SEGR) made publicly available for the SENSEVAL-2 English lexical sample) (hereafter referred to as SEVAL-2 ENG LEX) child is grouped as shown in table 1.", "labels": [], "entities": [{"text": "SENSEVAL-2 English lexical sample", "start_pos": 91, "end_pos": 124, "type": "DATASET", "confidence": 0.5538994371891022}]}, {"text": "Whilst it is perfectly reasonable the grouping decision was determined by the 'youth' vs 'descendant' distinction, the relationships between non-grouped senses, notably sense numbers 1 and 2 are apparent.", "labels": [], "entities": []}, {"text": "It is quite possible that these senses will share contextual cues useful for WSD and distinction between the two might not be relevant in a given application, for example because they are translated in the same way (ni\u00f1o/a in Spanish can mean both young boy/girl and son/daughter) or have common substitutions (boy/girl can be used as both offspring or young person).", "labels": [], "entities": [{"text": "WSD", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9859245419502258}]}, {"text": "Instead of clustering senses into groups we evaluate 2 methods that produce ranked lists of related senses for each target word sense.", "labels": [], "entities": []}, {"text": "We refer to these as RLISTs.", "labels": [], "entities": []}, {"text": "Such listings resemble nearest neighbour approaches for automatically acquired thesauruses.", "labels": [], "entities": []}, {"text": "They allow fora sense to be related to others which may not themselves be closely re-  lated.", "labels": [], "entities": []}, {"text": "Since only a fixed number of senses are defined for each word, the RLISTs include all senses of the word.", "labels": [], "entities": []}, {"text": "A cut-off can then be determined for any particular application.", "labels": [], "entities": []}, {"text": "Previous research on clustering word senses has focused on comparison to the SEGR goldstandard.", "labels": [], "entities": [{"text": "clustering word senses", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8637891809145609}, {"text": "SEGR goldstandard", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.9134960174560547}]}, {"text": "We evaluate the RLISTs against anew gold-standard produced by humans for this research since the SEGR does not have documentation with figures for inter-tagger agreement.", "labels": [], "entities": [{"text": "RLISTs", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.6299303770065308}, {"text": "SEGR", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.7648655772209167}]}, {"text": "As well as evaluating against a gold-standard, we also look at the effect of the RLISTs and the goldstandards themselves on WSD.", "labels": [], "entities": [{"text": "RLISTs", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.6423003673553467}, {"text": "WSD", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.815620481967926}]}, {"text": "Since the focus of this paper is not the WSD system, but the sense inventory, we use a simple WSD heuristic which uses the first sense of a word in all contexts, where the first sense of every word is specified by a resource.", "labels": [], "entities": []}, {"text": "While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an allwords task ().", "labels": [], "entities": [{"text": "WSD", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8838188052177429}]}, {"text": "We contrast the performance of first sense heuristics i) from SemCor ( and ii) derived automatically from the BNC following) and also iii) an upper-bound first sense heuristic extracted from the test data.", "labels": [], "entities": [{"text": "BNC", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.762147068977356}]}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "In the next section we describe some related work.", "labels": [], "entities": []}, {"text": "In section 3 we describe the two methods we will use to relate senses.", "labels": [], "entities": []}, {"text": "Our experiments are described in section 4.", "labels": [], "entities": []}, {"text": "In 4.1 we describe the construction of anew gold-standard produced using the same sense inventory used for SEGR, and give inter-annotator agreement figures for the task.", "labels": [], "entities": [{"text": "SEGR", "start_pos": 107, "end_pos": 111, "type": "TASK", "confidence": 0.45725056529045105}]}, {"text": "In section 4.2 we compare our methods to the new gold-standard and in section 4.3 we investigate how much effect coarser grained sense distinctions have on WSD using naive first sense heuristics.", "labels": [], "entities": [{"text": "WSD", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9423540830612183}]}, {"text": "We follow this with a discussion and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we use the same set of 20 nouns used by Agirre and Lopez de Lacalle.", "labels": [], "entities": []}, {"text": "The gold standard used in that work was SEGR.", "labels": [], "entities": [{"text": "SEGR", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.8452227711677551}]}, {"text": "These groupings were released for SEN-SEVAL-2 but we cannot find any documentation on how they were produced or on inter-annotator agreement.", "labels": [], "entities": [{"text": "SEN-SEVAL-2", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.5919163823127747}]}, {"text": "We have therefore produced anew gold-standard (referred to as RS) for these nouns which we describe in section 4.1.", "labels": [], "entities": [{"text": "RS", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9484634399414062}]}, {"text": "We compare the results of our methods for relating senses and SEGR to RS.", "labels": [], "entities": [{"text": "SEGR", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.5764233469963074}]}, {"text": "We then look at the performance of both the gold-standard groupings (SEGR and RS) compared to our automatic methods for coarser grained WSD of SEVAL-2 ENG LEX using some first sense heuristics.", "labels": [], "entities": [{"text": "RS", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.8383898735046387}]}], "tableCaptions": [{"text": " Table 1: SEGR for child in SEVAL-2 ENG LEX", "labels": [], "entities": [{"text": "SEGR", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8763477802276611}, {"text": "SEVAL-2 ENG LEX", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.6200027763843536}]}, {"text": " Table 3: Accuracy of Coarse-grained first sense heuristic on SEVAL-2 ENG LEX", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971370697021484}, {"text": "SEVAL-2 ENG LEX", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.4280049403508504}]}, {"text": " Table 4: RLISTs for child", "labels": [], "entities": [{"text": "RLISTs", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8931767344474792}]}]}