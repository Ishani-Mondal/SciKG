{"title": [], "abstractContent": [{"text": "A Question Answering (QA) system allows the user to ask questions in natural language and to obtain one or several answers.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.8396359860897065}]}, {"text": "If compared with a classical IR engine like Google, what kind of key benefits QA bring to users and how to measure their distinctive performances.", "labels": [], "entities": []}, {"text": "This is what we shall attempt hereto determine, specially in providing a comparative weak and strong points table of each system, along with showing how QA systems, in particular our Qristal QA system, requires up two to six timeless \"user effort\".", "labels": [], "entities": [{"text": "Qristal QA system", "start_pos": 183, "end_pos": 200, "type": "DATASET", "confidence": 0.8506664633750916}]}], "introductionContent": [{"text": "Asking questions in natural language and obtain short answers (if possible the exact answer) make QA systems the paramount of Information Retrieval.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 126, "end_pos": 147, "type": "TASK", "confidence": 0.8153983652591705}]}, {"text": "Through TREC and CLEF international campaigns, Question Answering systems are evaluated both in monolingual and multilingual use (e.g..", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8444180190563202}]}, {"text": "Nevertheless, very few comparisons of performances took place between Question Answering systems and Information Retrieval engines.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7374263405799866}, {"text": "Information Retrieval engines", "start_pos": 101, "end_pos": 130, "type": "TASK", "confidence": 0.8245129386583964}]}, {"text": "If so, they mainly focus on the quality of the supplied answers and on the time for user to obtain the answer.", "labels": [], "entities": []}, {"text": "We hereby attempt to define an evaluation method to compare performances and userfriendliness of both Question Answering systems and Information Retrieval engines.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7436360716819763}, {"text": "Information Retrieval engines", "start_pos": 133, "end_pos": 162, "type": "TASK", "confidence": 0.8070739905039469}]}, {"text": "We will apply this method to our system Qristal and to the Google Desktop Search 1 engine..", "labels": [], "entities": [{"text": "Qristal", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.8725953102111816}, {"text": "Google Desktop Search 1 engine.", "start_pos": 59, "end_pos": 90, "type": "DATASET", "confidence": 0.8647093057632447}]}, {"text": "Google Desktop Search : http://desktop.google.com/", "labels": [], "entities": []}], "datasetContent": [{"text": "We have two competing systems on the same corpora and a reference file with questions and answers.", "labels": [], "entities": []}, {"text": "We need now to define the basis of their comparison.", "labels": [], "entities": []}, {"text": "The main comparative evaluation between the Question Answering systems and the Information Retrieval engines) considered only the reading time while counting characters to be read to reach the answer.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7829310595989227}, {"text": "Information Retrieval", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.6515299081802368}]}, {"text": "Knowing the delay needed to obtain the results inmost Question Answering systems, it seems necessary to take also in account this delay if we want to measure the global user effort to obtain an answer to his question.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.6810284554958344}]}, {"text": "We consider that the user wants a correct answer to his question and we consider that the answer is correct if this answer can be found in the snippet or in the text linked to the snippet for Google (to the sentence for Qristal).", "labels": [], "entities": []}, {"text": "So we compared the quality of the systems as follows: percentage of correct answers ranked first, ranked within the five first, the ten first and the hundred first.", "labels": [], "entities": []}, {"text": "We considered both the answer as part of the snippets or part or the snippets and documents.", "labels": [], "entities": []}, {"text": "For the user, the quality of the answers returned, especially in the first results page, is paramount.", "labels": [], "entities": []}, {"text": "But, we think another item has to betaken into account: the time needed to obtain the answer.", "labels": [], "entities": []}, {"text": "This time is the compound of three elements: \u2022 the time to key in the question, \u2022 the delay before the results display, \u2022 the reading time of the snippets or sentences to reach a correct answer.", "labels": [], "entities": []}, {"text": "Looking carefully at each answer returned by Google Desktop, we discovered that it ignored some texts or, more exactly, some parts of texts, especially the end of these texts.", "labels": [], "entities": []}, {"text": "The help pages of this software point out this \"bug\" : However, if you're searching fora word within the file, please note that Google Desktop searches only about the first 10,000 words.", "labels": [], "entities": []}, {"text": "Ina few cases, Google Desktop may index slightly fewer words to save space in your search index and on your hard drive.", "labels": [], "entities": []}, {"text": "Of course this default impacted on the results and the comparisons.", "labels": [], "entities": []}, {"text": "Thus, we decided, in a second iteration of this evaluation, to consider only the 231 questions where Google Desktop found at least one correct answer.", "labels": [], "entities": []}, {"text": "Google Desktop found no answer with those 99 (330-231) removed questions for two main reasons.", "labels": [], "entities": [{"text": "Google Desktop", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9358406066894531}]}, {"text": "Firstly, as it doesn't manage a full indexation of documents.", "labels": [], "entities": []}, {"text": "Secondly, as some complex questions like \"why\" or \"how\" questions often lead it to silence on this evaluation.", "labels": [], "entities": []}, {"text": "The corpus of 231 questions is thus \"easier\" than that of 325 questions.", "labels": [], "entities": []}, {"text": "This confirms the score of Qristal for the exact answers: 73.6% versus 69.7%, and the score for the correct answer in first rank: 89.6% versus 81.8%.", "labels": [], "entities": [{"text": "Qristal", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.41604000329971313}]}, {"text": "But the results of Google are of course better: 13.9% in the first snippet against 9.7%, 43.7% in the first snippet or the first document, against 30.6%.", "labels": [], "entities": [{"text": "Google", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.9070571660995483}]}, {"text": "However the advantage of the QA system over the IR engine is clear in terms of quality, especially if we consider only the snippets.", "labels": [], "entities": []}, {"text": "This advantage is also clear for the user effort, even if any of the answer not found by Qristal penalizes this system as the reading time of this question is the consolidation of all reading times of all the snippets displayed for this question!", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Results with Google Desktop Search on  natural language and Boolean requests", "labels": [], "entities": []}, {"text": " Table 3: mean time of question entering", "labels": [], "entities": [{"text": "mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9895957112312317}]}, {"text": " Table 4: Percentage of correct answers / 330", "labels": [], "entities": []}, {"text": " Table 6: Number of answers and percentages / 231", "labels": [], "entities": [{"text": "Number of answers and percentages", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.8317172527313232}]}, {"text": " Table 10: Percentage of correct answers for questions  beginning by \"comment\" (\"how\")", "labels": [], "entities": []}]}