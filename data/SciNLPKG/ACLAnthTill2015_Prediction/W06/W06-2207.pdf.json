{"title": [{"text": "A Hybrid Approach for the Acquisition of Information Extraction Patterns", "labels": [], "entities": [{"text": "Acquisition of Information Extraction Patterns", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.6707351326942443}]}], "abstractContent": [{"text": "In this paper we present a hybrid approach for the acquisition of syntactico-semantic patterns from raw text.", "labels": [], "entities": []}, {"text": "Our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an Expectation Maximization clustering algorithm that uses the text words as attributes.", "labels": [], "entities": [{"text": "Expectation Maximization clustering", "start_pos": 126, "end_pos": 161, "type": "TASK", "confidence": 0.8345508178075155}]}, {"text": "We show that the combination of the two methods always outperforms the decision list learner alone.", "labels": [], "entities": []}, {"text": "Furthermore, using a modular architecture we investigate several algorithms for pattern ranking, the most important component of the decision list learner.", "labels": [], "entities": [{"text": "pattern ranking", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7198355495929718}]}], "introductionContent": [{"text": "Traditionally, Information Extraction (IE) identifies domain-specific events, entities, and relations among entities and/or events with the goals of: populating relational databases, providing eventlevel indexing in news stories, feeding link discovery applications, etcetera.", "labels": [], "entities": [{"text": "Information Extraction (IE) identifies domain-specific events, entities, and relations among entities and/or events", "start_pos": 15, "end_pos": 130, "type": "TASK", "confidence": 0.8522416133629648}, {"text": "eventlevel indexing in news stories", "start_pos": 193, "end_pos": 228, "type": "TASK", "confidence": 0.7455804347991943}, {"text": "link discovery", "start_pos": 238, "end_pos": 252, "type": "TASK", "confidence": 0.7103327363729477}]}, {"text": "By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns.", "labels": [], "entities": [{"text": "identification and selective extraction of relevant information", "start_pos": 17, "end_pos": 80, "type": "TASK", "confidence": 0.8057987689971924}]}, {"text": "For example, fora \"financial market change\" domain one relevant pattern is <NOUN fall MONEY to MONEY>.", "labels": [], "entities": [{"text": "NOUN fall MONEY", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.7526461482048035}, {"text": "MONEY", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.4953913390636444}]}, {"text": "When this pattern is matched on the text \"London gold fell $4.70 to $308.35\", a change of $4.70 is detected for the financial instrument \"London gold\".", "labels": [], "entities": [{"text": "London gold fell $4.70", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9510725975036621}, {"text": "change", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9425602555274963}, {"text": "London gold\"", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9780327479044596}]}, {"text": "Domain-specific patterns are either handcrafted or acquired automatically).", "labels": [], "entities": []}, {"text": "To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label.", "labels": [], "entities": []}, {"text": "The focus of this paper is to improve such lightly supervised pattern acquisition methods.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7552281320095062}]}, {"text": "Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains.", "labels": [], "entities": []}, {"text": "Although a rich literature covers bootstrapping methods applied to natural language problems) several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition.", "labels": [], "entities": [{"text": "syntactic or semantic pattern acquisition", "start_pos": 164, "end_pos": 205, "type": "TASK", "confidence": 0.6444141209125519}]}, {"text": "In this paper we answer two of these questions: (1) Can pattern acquisition be improved with text categorization techniques?", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8257196545600891}]}, {"text": "Bootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label.", "labels": [], "entities": [{"text": "Bootstrapping-based pattern acquisition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6691967944304148}, {"text": "text categorization (TC)", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.6666770577430725}]}, {"text": "Although TC is obviously not the main goal of pattern acquisition methodologies, it is nevertheless an integral part of the learning algorithm: each iteration of the acquisition algorithm depends on the previous assignments of category labels to documents.", "labels": [], "entities": [{"text": "TC", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9040465354919434}, {"text": "pattern acquisition", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7336543500423431}]}, {"text": "Hence, if the quality of the TC solution proposed is bad, the quality of the acquired patterns will suffer.", "labels": [], "entities": []}, {"text": "Motivated by this observation, we introduce a co-training-based algorithm) that uses a text categorization algorithm as reinforcement for pattern acquisition.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7390572875738144}]}, {"text": "We show, using both a direct and an indirect evaluation, that the combination of the two methodologies always improves the quality of the acquired patterns.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our evaluation procedure is to measure the quality of the acquired patterns.", "labels": [], "entities": []}, {"text": "Intuitively, We identify six categories: persons, locations, organizations, other names, temporal and numerical expressions.", "labels": [], "entities": []}, {"text": "The task of manually deciding whether an acquired pattern is relevant or not fora given domain is not trivial, mainly due to the ambiguity of the patterns.", "labels": [], "entities": []}, {"text": "Thus, this process should be carried out by more than one expert, so that the relevance of the ambiguous patterns can be agreed upon.", "labels": [], "entities": []}, {"text": "For example, the patterns s(ORG) v(score) o(goal) and s(PER) v(lead) io(with point) are clearly relevant only for the sports domain, whereas the patterns v(sign) io(as agent) and o(title) io(in DATE) might be regarded as relevant for other domains as well.", "labels": [], "entities": [{"text": "ORG", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9448366165161133}, {"text": "PER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9524691104888916}]}, {"text": "The specific procedure to manually evaluate the patterns is the following: (1) two experts separately evaluate the acquired patterns for the considered domains and collections; and (2) the results of both evaluations are compared.", "labels": [], "entities": []}, {"text": "For any disagreement, we have opted fora strict evaluation: all the occurrences of the corresponding pattern are looked up in the collection and, whenever at least one pattern occurrence belongs to a document assigned to a different domain than the domain in question, the pattern will be considered as not relevant.", "labels": [], "entities": []}, {"text": "Both the ambiguity and the high number of the extracted patterns have prevented us from performing an exhaustive direct evaluation.", "labels": [], "entities": []}, {"text": "For this reason, only the top (most relevant) 100 patterns have been evaluated for one domain per collection.", "labels": [], "entities": []}, {"text": "The results are detailed in Section 5.2.", "labels": [], "entities": []}, {"text": "For a better understanding of the proposed approach we perform an incremental evaluation: first, we evaluate only the various pattern selection criteria described in Section 2.4 by disabling the NB-EM component.", "labels": [], "entities": []}, {"text": "Second, using the best selection criteria, we evaluate the complete co-training system.", "labels": [], "entities": []}, {"text": "In both experiments we initialize the system with high-precision manually-selected seed rules which yield seed documents with a coverage of 10% of the training partitions.", "labels": [], "entities": []}, {"text": "The remaining 90% of the training documents are maintained unlabeled.", "labels": [], "entities": []}, {"text": "For all experiments we used a maximum of 400 bootstrapping iterations.", "labels": [], "entities": []}, {"text": "The acquired rules are fed to the decision list classifier which assigns category labels to the documents in the test partitions.", "labels": [], "entities": []}, {"text": "illustrates the precision/recall charts of the four algorithms as the number of patterns made available to the decision list classifier increases.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9991402626037598}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9120944142341614}]}, {"text": "All charts show precision/recall points starting after 100 learning iterations with 100-iteration increments.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.999514102935791}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.927091121673584}]}, {"text": "It is immediately obvious that the Collins selection criterion performs significantly better than the other three criteria.", "labels": [], "entities": []}, {"text": "For the same recall point, Collins yields a classification model with much higher precision, with differences ranging from 5% in the REUTERS collection to 20% in the AP collection.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9985854625701904}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9992278814315796}, {"text": "REUTERS collection", "start_pos": 133, "end_pos": 151, "type": "DATASET", "confidence": 0.8527663350105286}, {"text": "AP collection", "start_pos": 166, "end_pos": 179, "type": "DATASET", "confidence": 0.9301616847515106}]}, {"text": "Theorem 5 in) provides a theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T , then the precision of the final classifier is larger than T . Although the rule independence conditions are certainly not satisfied in our real-world evaluation, the above theorem indicates that there is a strong relation between the precision of the classifier rules on labeled data and the precision of the final classifier.", "labels": [], "entities": [{"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9935118556022644}, {"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9792650938034058}, {"text": "precision", "start_pos": 442, "end_pos": 451, "type": "METRIC", "confidence": 0.9844653010368347}, {"text": "precision", "start_pos": 500, "end_pos": 509, "type": "METRIC", "confidence": 0.9967604279518127}]}, {"text": "Our results provide the empirical proof that controling the precision of the acquired rules (i.e. the Collins criterion) is important.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9979836940765381}, {"text": "Collins criterion", "start_pos": 102, "end_pos": 119, "type": "METRIC", "confidence": 0.8498410880565643}]}, {"text": "The Collins criterion controls the recall of the learned model by favoring rules with high frequency in the collection.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9954427480697632}]}, {"text": "However, since the other two criteria do not use a high precision threshold, they will acquire more rules, which translates in better recall.", "labels": [], "entities": [{"text": "precision threshold", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.9733781218528748}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9979572296142578}]}, {"text": "For two out of the three collections, Riloff and Chi obtain a slightly better recall, about 2% higher than Collins', albeit with a much lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9995274543762207}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9965503215789795}]}, {"text": "We do not consider this an important advantage: in the next section we show that co-training with the NB-EM component further boosts the precision and recall of the Collinsbased acquisition algorithm.", "labels": [], "entities": [{"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9995300769805908}, {"text": "recall", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.999290943145752}, {"text": "Collinsbased acquisition algorithm", "start_pos": 165, "end_pos": 199, "type": "DATASET", "confidence": 0.8279836376508077}]}, {"text": "The MI criterion performs the worst of the four evaluated criteria.", "labels": [], "entities": [{"text": "MI", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9927655458450317}]}, {"text": "A clue for this behavior lies in the following equivalent form for MI: M I(p, y) = log P (p|y)\u2212log P (p).", "labels": [], "entities": [{"text": "MI", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9285435080528259}]}, {"text": "This formula indicates that, for patterns with equal conditional probabilities P (p|y), MI assigns higher scores to patterns with lower frequency.", "labels": [], "entities": [{"text": "MI", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.809679388999939}]}, {"text": "This is not the desired behavior in a TC-oriented system.", "labels": [], "entities": []}, {"text": "compares the performance of the stand-alone pattern acquisition algorithm (\"bootstrapping\") with the performance of the acquisition algorithm trained in the co-training environ- (a) AP, (b) LATIMES, and (c) REUTERS ment (\"co-training\").", "labels": [], "entities": [{"text": "AP", "start_pos": 182, "end_pos": 184, "type": "METRIC", "confidence": 0.9896370768547058}, {"text": "LATIMES", "start_pos": 190, "end_pos": 197, "type": "METRIC", "confidence": 0.9103724956512451}, {"text": "REUTERS ment", "start_pos": 207, "end_pos": 219, "type": "METRIC", "confidence": 0.9251009821891785}]}, {"text": "For both setups we used the best pattern selection criterion for pattern acquisition, i.e. the Collins criterion.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7952566146850586}, {"text": "Collins criterion", "start_pos": 95, "end_pos": 112, "type": "METRIC", "confidence": 0.8761802315711975}]}, {"text": "To put things in perspective, we also depict the performance obtained with a baseline system, i.e. the system configured to use the Riloff pattern selection criterion and without the NB-EM algorithm (\"baseline\").", "labels": [], "entities": []}, {"text": "To our knowledge, this system, or a variation of it, is the current state-of-the-art in pattern acquisition).", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.800786167383194}]}, {"text": "All algorithms were initialized with the same seed rules and had access to all documents.", "labels": [], "entities": []}, {"text": "shows that the quality of the learned patterns always improves if the pattern acquisition algorithm is \"reinforced\" with EM.", "labels": [], "entities": []}, {"text": "For the same recall point, the patterns acquired in the co-training environment yield classification models with precision (generally) much larger than the models generated by the pattern acquisition algorithm alone.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9967079162597656}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.998414158821106}]}, {"text": "When using the same pattern acquisition criterion, e.g. Collins, the differences between the co-training approach and the stand-alone pattern acquisition method (\"bootstrapping\") range from 2-3% in the REUTERS collection to 20% in the LATIMES collection.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7244881093502045}, {"text": "Collins", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9677392840385437}, {"text": "REUTERS collection", "start_pos": 202, "end_pos": 220, "type": "DATASET", "confidence": 0.6845401376485825}, {"text": "LATIMES collection", "start_pos": 235, "end_pos": 253, "type": "DATASET", "confidence": 0.7211315780878067}]}, {"text": "These results support our intuition that the sparse pattern space is insufficient to generate good classification models, which directly influences the quality of all acquired patterns.", "labels": [], "entities": []}, {"text": "Furthermore, due to the increased coverage of the lexicalized collection views, the patterns acquired in the co-training setup generally have better recall, up to 11% higher in the LATIMES collection.", "labels": [], "entities": [{"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9994572997093201}, {"text": "LATIMES collection", "start_pos": 181, "end_pos": 199, "type": "DATASET", "confidence": 0.7294702231884003}]}, {"text": "Lastly, the comparison of our best system (\"cotraining\") against the current state-of-the-art (our \"baseline\") draws an even more dramatic picture:  for the same recall point, the co-training system obtains a precision up to 35% higher for AP and LATIMES, and up to 10% higher for REUTERS.", "labels": [], "entities": [{"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9978479146957397}, {"text": "precision", "start_pos": 209, "end_pos": 218, "type": "METRIC", "confidence": 0.9991092085838318}, {"text": "REUTERS", "start_pos": 281, "end_pos": 288, "type": "METRIC", "confidence": 0.725921630859375}]}, {"text": "As stated in Section 4.2, two experts have manually evaluated the top 100 acquired patterns for one different domain in each of the three collections.", "labels": [], "entities": []}, {"text": "The three corresponding domains have been selected intending to deal with different degrees of ambiguity, which are reflected in the initial interexpert agreement.", "labels": [], "entities": []}, {"text": "Any disagreement between experts is solved using the algorithm introduced in Section 4.2.", "labels": [], "entities": []}, {"text": "shows the results of this direct evaluation.", "labels": [], "entities": []}, {"text": "The co-training approach outperforms the baseline for all three collections.", "labels": [], "entities": []}, {"text": "Concretely, improvements of 9% and 8% are achieved for the Financial and the Corporate Acquisitions domains, and 46%, by far the largest difference, is found for the Sports domain in AP.", "labels": [], "entities": [{"text": "Sports domain in AP", "start_pos": 166, "end_pos": 185, "type": "DATASET", "confidence": 0.8471995890140533}]}, {"text": "lists the top 20 patterns extracted by both approaches in the latter domain.", "labels": [], "entities": []}, {"text": "It can be observed that for the baseline, only the top 4 patterns are relevant, the rest being extremely general patterns.", "labels": [], "entities": []}, {"text": "On the other hand, the quality of the patterns acquired by our approach is much higher: all the patterns are relevant to the domain, although 7 out of the 20 might be considered ambiguous and according to the criterion defined in Section 4.2 have been evaluated as not relevant.: Top 20 patterns acquired from the Sports domain by the baseline system (Riloff) and the co-training system for the AP collection.", "labels": [], "entities": [{"text": "Riloff", "start_pos": 352, "end_pos": 358, "type": "METRIC", "confidence": 0.8335278630256653}, {"text": "AP collection", "start_pos": 395, "end_pos": 408, "type": "DATASET", "confidence": 0.8384619355201721}]}, {"text": "The correct patterns are in bold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Document collections used in the evaluation", "labels": [], "entities": []}, {"text": " Table 3: Percentage of relevant patterns for one domain per", "labels": [], "entities": []}]}