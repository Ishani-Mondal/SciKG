{"title": [], "abstractContent": [{"text": "We propose a generalization of the supervised DOP model to unsupervised learning.", "labels": [], "entities": []}, {"text": "This new model, which we call U-DOP, initially assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from (a large subset of) these binary trees to compute the most probable parse trees.", "labels": [], "entities": []}, {"text": "We show how U-DOP can be implemented by a PCFG-reduction technique and report competitive results on English (WSJ), German (NEGRA) and Chinese (CTB) data.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first paper which accurately bootstraps structure for Wall Street Journal sentences up to 40 words obtaining roughly the same accuracy as a binarized supervised PCFG.", "labels": [], "entities": [{"text": "Wall Street Journal sentences", "start_pos": 96, "end_pos": 125, "type": "DATASET", "confidence": 0.9366216212511063}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9979792237281799}]}, {"text": "We show that previous approaches to unsupervised parsing have shortcomings in that they either constrain the lexical or the structural context, or both.", "labels": [], "entities": []}], "introductionContent": [{"text": "How can we learn syntactic structure from unlabeled data in an unsupervised way?", "labels": [], "entities": []}, {"text": "The importance of unsupervised parsing is nowadays widely acknowledged.", "labels": [], "entities": []}, {"text": "While supervised parsers suffer from shortage of hand-annotated data, unsupervised parsers operate with unlabeled raw data, of which unlimited quantities are available.", "labels": [], "entities": []}, {"text": "During the last few years there has been considerable progress in unsupervised parsing.", "labels": [], "entities": []}, {"text": "To give a brief overview: van Zaanen (2000) achieved 39.2% unlabeled f-score on ATIS word strings by a sentence-aligning technique called ABL.", "labels": [], "entities": []}, {"text": "reports 42.0% unlabeled f-score on the same data using distributional clustering, and obtain 51.2% unlabeled f-score on ATIS part-of-speech strings using a constituent-context model called CCM.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.9031848907470703}]}, {"text": "Moreover, on Penn Wall Street Journal p-os-strings \u2264 10 (WSJ10), report 71.1% unlabeled f-score.", "labels": [], "entities": [{"text": "Penn Wall Street Journal p-os-strings", "start_pos": 13, "end_pos": 50, "type": "DATASET", "confidence": 0.9463539838790893}, {"text": "WSJ10)", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.8104517459869385}]}, {"text": "And the hybrid approach of, which combines a constituency and a dependency model, leads to a further increase of 77.6% f-score.", "labels": [], "entities": [{"text": "f-score", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.9856030941009521}]}, {"text": "Although there has thus been steady progress in unsupervised parsing, all these approaches have shortcomings in that they either constrain the lexical or the structural context that is taken into account, or both.", "labels": [], "entities": []}, {"text": "For example, the CCM model by is said to describe \"all contiguous subsequences of a sentence\" (.", "labels": [], "entities": []}, {"text": "While this is a very rich lexical model, it is still limited in that it neglects dependencies that are non-contiguous such as between more and than in \"BA carried more people than cargo\".", "labels": [], "entities": []}, {"text": "Moreover, by using an \"all-substrings\" approach, CCM risks to underrepresent structural context.", "labels": [], "entities": []}, {"text": "Similar shortcomings can be found in other unsupervised models.", "labels": [], "entities": []}, {"text": "In this paper we will try to directly model structural as well as lexical context without constraining any dependencies beforehand.", "labels": [], "entities": []}, {"text": "An approach that may seem apt in this respect is an allsubtrees approach (e.g. Subtrees can model both contiguous and non-contiguous lexical dependencies (see section 2) and they also model constituents in a hierarchical context.", "labels": [], "entities": []}, {"text": "Moreover, we can view the allsubtrees approach as a generalization of Klein and Manning's all-substrings approach and van Zaanen's ABL model.", "labels": [], "entities": []}, {"text": "In the current paper, we will use the allsubtrees approach as proposed in Data-Oriented Parsing or DOP).", "labels": [], "entities": [{"text": "Data-Oriented Parsing or DOP", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.7145191133022308}]}, {"text": "We will generalize the supervised version of DOP to unsupervised parsing.", "labels": [], "entities": []}, {"text": "The key idea of our approach is to initially assign all possible unlabeled binary trees to a set of given sentences, and to next use counts of all subtrees from (a large random subset of) these binary trees to compute the most probable parse trees.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, such a model has never been tried out.", "labels": [], "entities": []}, {"text": "We will refer to this unsupervised DOP model as U-DOP, while the supervised DOP model (which uses hand-annotated trees) will be referred to as S-DOP.", "labels": [], "entities": []}, {"text": "Moreover, we will continue to refer to the general approach simply as DOP.", "labels": [], "entities": [{"text": "DOP", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.7075045108795166}]}, {"text": "U-DOP is not just an engineering approach to unsupervised learning but can also be motivated from a cognitive perspective): if we don't have a clue which trees should be assigned to sentences in the initial stages of language acquisition, we can just as well assume that initially all trees are possible.", "labels": [], "entities": []}, {"text": "Only those (sub)trees that partake in computing the most probable parse trees for new sentences are actually \"learned\".", "labels": [], "entities": []}, {"text": "We have argued in that such an integration of unsupervised and supervised methods results in an integrated model for language learning and language use.", "labels": [], "entities": []}, {"text": "In the following we will first explain how U-DOP works, and how it can be approximated by a PCFG-reduction technique.", "labels": [], "entities": []}, {"text": "Next, in section 3 we discuss a number of experiments with U-DOP and compare it to previous models on English (WSJ), German (NEGRA) and Chinese (CTB) data.", "labels": [], "entities": [{"text": "WSJ), German (NEGRA) and Chinese (CTB) data", "start_pos": 111, "end_pos": 154, "type": "DATASET", "confidence": 0.5948111483683953}]}, {"text": "To the best of our knowledge, this is the first paper which bootstraps structure for WSJ sentences up to 40 words obtaining roughly the same accuracy as a binarized supervised PCFG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9984265565872192}]}, {"text": "This is remarkable since unsupervised models are clearly at a disavantage compared to supervised models which can literally reuse manually annotated data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Results of U-DOP compared to previous  models on the same data", "labels": [], "entities": []}, {"text": " Table 3. Average f-scores of U-DOP compared to a  supervised PCFG (S-PCFG) on 10 different 90-10  splits of the WSJ10", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.9232596755027771}]}]}