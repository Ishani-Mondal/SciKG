{"title": [{"text": "Experiments Adapting an Open-Domain Question Answering System to the Geographical Domain Using Scope-Based Resources", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6619760245084763}]}], "abstractContent": [{"text": "This paper describes an approach to adapt an existing multilingual Open-Domain", "labels": [], "entities": []}], "introductionContent": [{"text": "Question Answering (QA) is the task of, given a query expressed in Natural Language (NL), retrieving its correct answer (a single item, a text snippet,...).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8880445480346679}]}, {"text": "QA has become a popular task in the NL Processing (NLP) research community in the framework of different international ODQA evaluation contests such as: Text Retrieval Conference (TREC) for English, Cross-Lingual Evaluation Forum (CLEF) for European languages, and NTCIR for Asian languages.", "labels": [], "entities": [{"text": "Text Retrieval Conference (TREC)", "start_pos": 153, "end_pos": 185, "type": "TASK", "confidence": 0.8083728949228922}, {"text": "NTCIR", "start_pos": 265, "end_pos": 270, "type": "DATASET", "confidence": 0.8676906824111938}]}, {"text": "In this paper we describe our experiments in the adaptation and evaluation of an ODQA system to a Restricted Domain, the Geographical Domain.", "labels": [], "entities": []}, {"text": "GeoTALP-QA is a multilingual Geographical Domain Question Answering (GDQA) system.", "labels": [], "entities": [{"text": "GeoTALP-QA", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9345733523368835}, {"text": "multilingual Geographical Domain Question Answering (GDQA)", "start_pos": 16, "end_pos": 74, "type": "TASK", "confidence": 0.7192535810172558}]}, {"text": "This Restricted Domain Question Answering (RDQA) system has been built over an existing ODQA system, TALP-QA, a multilingual ODQA system that processes both factoid and definition questions (see) and)).", "labels": [], "entities": [{"text": "Restricted Domain Question Answering (RDQA)", "start_pos": 5, "end_pos": 48, "type": "TASK", "confidence": 0.7213676955018725}]}, {"text": "The system was evaluated for Spanish and English in the context of our participation in the conferences TREC and CLEF in 2005 and has been adapted to a multilingual GDQA system for factoid questions.", "labels": [], "entities": []}, {"text": "As pointed out in), the Geographical Domain (GD) can be considered a middle way between real Restricted Domains and open ones because many open domain texts contain a high density of geographical terms.", "labels": [], "entities": []}, {"text": "Although the basic architecture of TALP-QA has remained unchanged, a set of QA components were redesigned and modified and we had to add some specific components for the GD to our QA system.", "labels": [], "entities": []}, {"text": "The basic approach in TALP-QA consists of applying language-dependent processes on both question and passages forgetting a language independent semantic representation, and then extracting a set of Semantic Constraints (SC) for each question.", "labels": [], "entities": [{"text": "TALP-QA", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.8808439373970032}]}, {"text": "Then, an answer extraction algorithm extracts and ranks sentences that satisfy the SCs of the question.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.739082083106041}]}, {"text": "Finally, an answer selection module chooses the most appropriate answer.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.8063031435012817}]}, {"text": "We outline below the organization of the paper.", "labels": [], "entities": []}, {"text": "In the next section we present some characteristics of RDQA systems.", "labels": [], "entities": []}, {"text": "In Section 3, we present", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe how we obtained the resources needed to do experiments in the Spanish Geography domain using Spanish.", "labels": [], "entities": [{"text": "Spanish Geography domain", "start_pos": 90, "end_pos": 114, "type": "DATASET", "confidence": 0.6289651294549307}]}, {"text": "These resources were: the question corpus (validation and test), the document collection required by the offline ODQA Passage Retrieval, and the geographical scope-based resources.", "labels": [], "entities": [{"text": "ODQA Passage Retrieval", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7509007056554159}]}, {"text": "Finally, we describe the experiments performed.", "labels": [], "entities": []}, {"text": "We have designed some experiments in order to evaluate the accuracy of the GDQA system and its subsystems (QP, PR, and AE).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9992634654045105}, {"text": "AE", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.978701114654541}]}, {"text": "For PR, we evaluated the web-based snippet retrieval using Google with some variants of expansions, versus our ODQA Passage Retrieval with the corpus of the SW.", "labels": [], "entities": [{"text": "PR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9782811403274536}]}, {"text": "Then, the passages (or snippets) retrieved by the best PR approach were used by the two different Answer Extraction algorithms.", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8765485286712646}]}, {"text": "The ODQA Answer Extractor has been evaluated taking into account the answers that have a supported context in the set of passages (or snippets).", "labels": [], "entities": [{"text": "ODQA Answer Extractor", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7420162757237753}]}, {"text": "Finally, we evaluated the global results of the complete QA process with the different Answer Extractors: ODQA and Frequency-Based.", "labels": [], "entities": [{"text": "ODQA", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8216418027877808}, {"text": "Frequency-Based", "start_pos": 115, "end_pos": 130, "type": "METRIC", "confidence": 0.8327445387840271}]}, {"text": "This paper summarizes our experiments adapting an ODQA to the GD and its evaluation in Spanish in the scope of the Spanish Geography.", "labels": [], "entities": [{"text": "GD", "start_pos": 62, "end_pos": 64, "type": "DATASET", "confidence": 0.7824741005897522}, {"text": "Spanish Geography", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.8178429305553436}]}, {"text": "Out of 62 questions, our system provided the correct answer to 39 questions in the experiment with the best results.", "labels": [], "entities": []}, {"text": "Our Passage Retrieval for ODQA offers less attractive results when using the SW corpus.", "labels": [], "entities": [{"text": "Passage Retrieval", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7240269780158997}, {"text": "SW corpus", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.7869830429553986}]}, {"text": "The problem of using SW to extract the answers is that it gives few documents with the correct answer, and, it is difficult to extract the answer because the documents contain tables, lists, ill-formed sentences, etc.", "labels": [], "entities": []}, {"text": "Our ODQA AE needs a grammatically well-structured text to extract correctly the answers.", "labels": [], "entities": [{"text": "ODQA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6664513349533081}, {"text": "AE", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.37445399165153503}]}, {"text": "The QA system offers a low performance (33% of accuracy) when using this AE over the web-based retrieved passages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9989894032478333}, {"text": "AE", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9725622534751892}]}, {"text": "In some cases, the snippets are cut and we could expect a better performance retrieving the whole documents from Google.", "labels": [], "entities": []}, {"text": "On the other hand, web-based snippet retrieval, with only one query per question, gives good results in Passage Retrieval.", "labels": [], "entities": [{"text": "snippet retrieval", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.6657445579767227}, {"text": "Passage Retrieval", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.8209768235683441}]}, {"text": "The QA system with the Frequency-Based AE obtained better results than with the ODQA AE (62.9% of accuracy).", "labels": [], "entities": [{"text": "Frequency-Based AE", "start_pos": 23, "end_pos": 41, "type": "METRIC", "confidence": 0.7306160032749176}, {"text": "ODQA AE", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.5380668044090271}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9994876384735107}]}, {"text": "Finally, we conclude that our approach with Geographical scope-based resources are notably helpful to deal with multilingual Geographical Domain Question Answering.", "labels": [], "entities": [{"text": "multilingual Geographical Domain Question Answering", "start_pos": 112, "end_pos": 163, "type": "TASK", "confidence": 0.6391824007034301}]}], "tableCaptions": [{"text": " Table 5: Passage Retrieval results (refer to sec- tion 3.4.2 for detailed information of the different  query expansion acronyms).", "labels": [], "entities": [{"text": "Passage Retrieval", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.736805647611618}]}, {"text": " Table 6. We evaluated the ac- curacy taking into account the number of  correct and supported answers by the pas- sages divided by the total number of ques- tions that have a supported answer in its set  of passages. This evaluation has been done  using the results of the top-ranked retrieval  configuration over the development set: the  Google+TWJ+QB+CE configuration of the  snippet retriever.", "labels": [], "entities": [{"text": "Google+TWJ+QB+CE", "start_pos": 341, "end_pos": 357, "type": "DATASET", "confidence": 0.852694434779031}]}, {"text": " Table 7: QA results over the test set.", "labels": [], "entities": [{"text": "QA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9135373830795288}]}]}