{"title": [{"text": "Dialogue based Question Answering System in Telugu EACL 2006 Workshop on Multilingual Question Answering -MLQA06", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7476303279399872}, {"text": "Telugu EACL 2006 Workshop on Multilingual Question Answering", "start_pos": 44, "end_pos": 104, "type": "TASK", "confidence": 0.6662250608205795}, {"text": "MLQA06", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.676354169845581}]}], "abstractContent": [{"text": "A dialogue based Question Answering (QA) system for Railway information in Telugu has been described.", "labels": [], "entities": [{"text": "dialogue based Question Answering (QA)", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.7696105028901782}, {"text": "Railway information", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8237532675266266}]}, {"text": "Telugu is an important language in India belonging to the Dravidian family.", "labels": [], "entities": []}, {"text": "The main component of our QA system is the Dialogue Manager (DM), to handle the dialogues between user and system.", "labels": [], "entities": []}, {"text": "It is necessary in generating dialogue for clarifying partially understood questions, resolving Anaphora and Co-reference problems.", "labels": [], "entities": []}, {"text": "Besides, different modules have been developed for processing the query and its translation into formal database query language statement(s).", "labels": [], "entities": []}, {"text": "Based on the result from the database, a natural language answer is generated.", "labels": [], "entities": []}, {"text": "The empirical results obtained on the current system are encouraging.", "labels": [], "entities": []}, {"text": "Testing with a set of questions in Railway domain, the QA system showed 96.34% of precision and 83.96% of dialogue success rate.", "labels": [], "entities": [{"text": "Railway domain", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9181101024150848}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9997896552085876}]}, {"text": "Such a question answering system can be effectively utilized when integrated with a speech input and speech output system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8319200873374939}]}], "introductionContent": [{"text": "Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics, as reflected by the TREC (Text Retrieval Conference) QA track).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.9242873072624207}, {"text": "TREC (Text Retrieval Conference) QA", "start_pos": 208, "end_pos": 243, "type": "TASK", "confidence": 0.5796580825533185}]}, {"text": "Several QA systems have responded to these changes in the nature of the QA task by incorporating various knowledge resources (), handling of additional types of questions tapping into external data sources such as web, encyclopedia, and databases in order to find the answer candidates, which may then be located in the specific corpus being searched (.", "labels": [], "entities": []}, {"text": "The most popular classes of technique for QA are open-domain and restricted-domain ().", "labels": [], "entities": [{"text": "QA", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9438838362693787}]}, {"text": "These two domains use thesauri and lexicons in classifying documents and categorizing the questions.", "labels": [], "entities": []}, {"text": "Open domain question answering deals with questions about nearly everything and can only rely on general ontology.", "labels": [], "entities": [{"text": "Open domain question answering", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6585640013217926}]}, {"text": "It has become a very active research area over the past few years.", "labels": [], "entities": []}, {"text": "On the other hand, Restricted-domain question answering (RDQA) deals with questions under a specific domain.", "labels": [], "entities": [{"text": "Restricted-domain question answering (RDQA)", "start_pos": 19, "end_pos": 62, "type": "TASK", "confidence": 0.7905325641234716}]}, {"text": "If we create such a RDQA interface for structured e.g. relational database, we call it as Natural language interface to database system (NLIDB) (, where it allows the user to access the information stored in database by typing requests expressed in some natural language.", "labels": [], "entities": []}, {"text": "RDQA has along history, beginning with systems working over databases (e.g., BASEBALL (, and LUNAR ().", "labels": [], "entities": [{"text": "RDQA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.60711669921875}, {"text": "BASEBALL", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.8983044624328613}, {"text": "LUNAR", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.7313702702522278}]}, {"text": "In practice, current QAs can only understand limited subsets of natural language.", "labels": [], "entities": []}, {"text": "Therefore, some training is still needed to teach the end-user what kinds of questions the system can or cannot understand.", "labels": [], "entities": []}, {"text": "There are kinds of questions (e.g. questions involving negation, or quantification) that can be easily expressed in natural language, but that seem difficult (or at least tedious) to express using graphical or form based interfaces.", "labels": [], "entities": []}, {"text": "Anaphoric and elliptical expressions are also handled by the QA systems.", "labels": [], "entities": []}, {"text": "In recent years a large part of the research in QAs has been devoted to portability, i.e., to the design of QAs that can be used in different knowledge domains (Knowledge domain portability), with different underlying Database Management System (DBMS) (DBMS portability), or even with different natural languages (Natural language portability).", "labels": [], "entities": []}, {"text": "There is a growing body of research on integrating speech recognition, robust interpretation with the goal being to implement systems that engage users in spoken dialogue to help them perform certain tasks.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7370576113462448}, {"text": "robust interpretation", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7504474520683289}]}, {"text": "We expect that this line of research will have a significant influence on future QAs, giving rise to systems that will allow users to access databases by spoken dialogue, in situations for which graphic and formbased interfaces are difficult to use.", "labels": [], "entities": []}, {"text": "A practical question answering system in restricted domain) and our system handles user questions similarly.", "labels": [], "entities": [{"text": "question answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7722906470298767}]}, {"text": "However, our system extracts the information from a relational database.", "labels": [], "entities": []}, {"text": "Moreover, our system keeps track of user dialogue and handles clarifications, elaborations and confirmations needed from the user with respect to the query.", "labels": [], "entities": []}, {"text": "Along with it returns natural language answer in user-friendly format.", "labels": [], "entities": []}, {"text": "ARISE (Automatic Railway Information System for Europe) is a spoken dialogue system to provide train timetable information over the phone.", "labels": [], "entities": [{"text": "ARISE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8104764819145203}]}, {"text": "Prototypes have been developed in four languages: Dutch, French, English, and Italian.", "labels": [], "entities": []}, {"text": "ARISE uses a mixed initiative Dialogue Manager (DM).", "labels": [], "entities": [{"text": "ARISE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9463852047920227}]}, {"text": "A mix of implicit and explicit confirmation is used, based on how confident the system is in deciding whether an item has been correctly understood.", "labels": [], "entities": []}, {"text": "We relate this paper as an experiment for designing a keyword based QA system fora huge domain (i.e. for Railways), which aims at replying users questions in their native language (Telugu).", "labels": [], "entities": []}, {"text": "The system generates SQL query out of the natural language question, executes the SQL query over a relational database and then provide the answer.", "labels": [], "entities": []}, {"text": "Dialogue Manager (DM) is maintained to generate dialogues with user and to handle the anaphoric and elliptical expression in our query.", "labels": [], "entities": []}, {"text": "This system is implemented on a relatively restricted domain that includes a number of aspects of railway information system (Arrival/Departure time, Fare between for particular stations, Trains between important stations etc.).", "labels": [], "entities": [{"text": "Arrival", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9925627112388611}]}, {"text": "The precision of the information extraction stage is essential to the success of a QA system, because it places an upper bound on the precision of the entire system.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990873336791992}, {"text": "information extraction", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8308821320533752}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.995934009552002}]}, {"text": "The empirical results obtained on the current system are encouraging.", "labels": [], "entities": []}, {"text": "Testing with a set of questions in Railway domain, the QA system showed 96.34% of precision and 83.96% of dialogue success rate.", "labels": [], "entities": [{"text": "Railway domain", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9181100130081177}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9997896552085876}]}, {"text": "Section 2 deals with the System Architecture of the QA system.", "labels": [], "entities": []}, {"text": "Section 3 details about the QA system design in the Railway information domain using the Keyword based approach.", "labels": [], "entities": [{"text": "Railway information domain", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.8741403818130493}]}, {"text": "The evaluation has been carried out in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes with some directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluating our system we have taken queries from our Telugu-speaking friends.", "labels": [], "entities": []}, {"text": "We have described the Railway Information system to them.", "labels": [], "entities": [{"text": "Railway Information system", "start_pos": 22, "end_pos": 48, "type": "DATASET", "confidence": 0.9075163801511129}]}, {"text": "They have also been told about the constraints on the nature of queries in the systems.", "labels": [], "entities": []}, {"text": "They have also been shown list of example queries for the systems.", "labels": [], "entities": []}, {"text": "Here we are considering two measures for evaluating our system: Dialogue success rate and Precision.", "labels": [], "entities": [{"text": "Precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9966147541999817}]}, {"text": "The QA system was evaluated by giving 26 sets of dialogue consisting 95 natural language queries in total.", "labels": [], "entities": []}, {"text": "The two evaluation measures are defined as follows: Dialogue success rate for each set=Number of Answers or Responses generated by the system /Number of turns issued by the user.", "labels": [], "entities": []}, {"text": "Dialogue success rate = (\u2211 Dialogue success rate for each set / Number of sets of dialogues)*100.", "labels": [], "entities": []}, {"text": "Precision= (Number of correct answers given by the system/Number of answers given by the system)*100.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9955824017524719}]}, {"text": "The number of turns issued by the user in a dialogue is the total of the number of questions issued to the system and the number of responses provided by the user to the system.", "labels": [], "entities": []}, {"text": "Each set of dialogue consisted of around 3 to 5 natural language queries.", "labels": [], "entities": []}, {"text": "The total dialogue success rate for the 26 sets was obtained as 21.83.", "labels": [], "entities": []}, {"text": "The dialogue success rate for the system is calculated as Dialogue success rate= (21.83/26)*100= 83.96%.", "labels": [], "entities": [{"text": "Dialogue success rate", "start_pos": 58, "end_pos": 79, "type": "METRIC", "confidence": 0.766023576259613}]}, {"text": "Out of 95 questions, system generated answers for 82 questions of which 79 were correct answers.", "labels": [], "entities": []}, {"text": "So, the precision of the system is calculated as Precision= (79/82)*100= 96.34%.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9997597336769104}, {"text": "Precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.998958945274353}]}, {"text": "This low dialogue success rate is due to the fact that the system coverage of the domain is not extensive enough, i.e., query frames for some natural language queries were not correctly identified.", "labels": [], "entities": []}, {"text": "The information given by the user in the query was sometimes inadequate and the system was notable to identify the missing information because of the incorrect choice of the query frame.", "labels": [], "entities": []}, {"text": "Sometimes the system is unable to obtain tokens correctly from the input query even if it had identified the right query frame, thereby generating wrong answers.", "labels": [], "entities": []}, {"text": "Misinterpretation of dialogue history is also another problem.", "labels": [], "entities": [{"text": "Misinterpretation of dialogue history", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8053431510925293}]}], "tableCaptions": []}