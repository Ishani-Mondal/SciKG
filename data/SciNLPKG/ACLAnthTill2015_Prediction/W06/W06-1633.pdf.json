{"title": [{"text": "BESTCUT: A Graph Algorithm for Coreference Resolution", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9635694026947021}, {"text": "Coreference Resolution", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9694985151290894}]}], "abstractContent": [{"text": "In this paper we describe a coreference resolution method that employs a classification and a clusterization phase.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9385523796081543}]}, {"text": "Ina novel way, the clusterization is produced as a graph cutting algorithm, in which nodes of the graph correspond to the mentions of the text, whereas the edges of the graph constitute the confidences derived from the coreference classification.", "labels": [], "entities": []}, {"text": "In experiments , the graph cutting algorithm for coreference resolution, called BESTCUT, achieves state-of-the-art performance.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.9547736942768097}, {"text": "BESTCUT", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9919885396957397}]}], "introductionContent": [{"text": "Recent coreference resolution algorithms tackle the problem of identifying coreferent mentions of the same entity in text as a two step procedure: (1) a classification phase that decides whether pairs of noun phrases corefer or not; and (2) a clusterization phase that groups together all mentions that refer to the same entity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.9405893683433533}]}, {"text": "An entity is an objector a set of objects in the real world, while a mention is a textual reference to an entity 1 . Most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees () or as maximum entropy classifiers ().", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.8296374678611755}]}, {"text": "Moreover, these methods employ similar feature sets.", "labels": [], "entities": []}, {"text": "The clusterization phase is different across current approaches.", "labels": [], "entities": [{"text": "clusterization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9721363186836243}]}, {"text": "For example, there are several linking decisions for clusterization.", "labels": [], "entities": []}, {"text": "() advocate the link-first decision, which links a mention to its closest candidate referent, while) consider instead the link-best decision, which links a mention to its most confident candidate referent.", "labels": [], "entities": []}, {"text": "Both these clustering decisions are locally optimized.", "labels": [], "entities": []}, {"text": "In contrast, globally optimized clustering decisions were reported in () and, where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization (LaSO) framework () respectively, but the first search is partial and driven by heuristics and the second one only looks back in text.", "labels": [], "entities": []}, {"text": "We argue that a more adequate clusterization phase for coreference resolution can be obtained by using a graph representation.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.9781496822834015}]}, {"text": "In this paper we describe a novel representation of the coreference space as an undirected edge-weighted graph in which the nodes represent all the mentions from a text, whereas the edges between nodes constitute the confidence values derived from the coreference classification phase.", "labels": [], "entities": [{"text": "coreference classification", "start_pos": 252, "end_pos": 278, "type": "TASK", "confidence": 0.8961760103702545}]}, {"text": "In order to detect the entities referred in the text, we need to partition the graph such that all nodes in each subgraph refer to the same entity.", "labels": [], "entities": []}, {"text": "We have devised a graph partitioning method for coreference resolution, called BESTCUT, which is inspired from the well-known graph-partitioning algorithm Min-Cut (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9697183668613434}, {"text": "BESTCUT", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9882794618606567}]}, {"text": "BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 . Moreover, we have slightly modified the Min-Cut procedures.", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8284010887145996}]}, {"text": "BESTCUT replaces the bottom-up search in a tree representation (as it was performed in ()) with the top-down problem of obtaining the best partitioning of a graph.", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9119482636451721}]}, {"text": "We start by assuming that all mentions refer to a single entity; the graph cut splits the mentions into subgraphs and the split-ting continues until each subgraph corresponds to one of the entities.", "labels": [], "entities": []}, {"text": "The cut stopping decision has been implemented as an SVM-based classification.", "labels": [], "entities": [{"text": "cut stopping", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7670930624008179}]}, {"text": "The classification and clusterization phases assume that all mentions are detected.", "labels": [], "entities": []}, {"text": "In order to evaluate our coreference resolution method, we have (1) implemented a mention detection procedure that has the novelty of employing information derived from the word senses of common nouns as well as selected lexico-syntactic information; and (2) used a maximum entropy model for coreference classification.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9467770457267761}, {"text": "mention detection", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7017025947570801}, {"text": "coreference classification", "start_pos": 292, "end_pos": 318, "type": "TASK", "confidence": 0.9485628604888916}]}, {"text": "The experiments conducted on MUC and ACE data indicate state-of-the-art results when compared with the methods reported in) and ().", "labels": [], "entities": [{"text": "MUC", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8035575151443481}, {"text": "ACE data", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8153593242168427}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the coreference resolution method that uses the BESTCUT clusterization; Section 3 describes the approach we have implemented for detecting mentions in texts; Section 4 reports on the experimental results; Section 5 discusses related work; finally, Section 6 summarizes the conclusions.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9307818412780762}, {"text": "BESTCUT", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.8986597061157227}, {"text": "detecting mentions in texts", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.8546871691942215}]}], "datasetContent": [{"text": "The clusterization algorithms that we implemented to evaluate in comparison with our method are ()'s Belltree and Link-Best (best-first clusterization) from).", "labels": [], "entities": [{"text": "Belltree", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.949785053730011}]}, {"text": "The features used were described in section 2.2.", "labels": [], "entities": []}, {"text": "We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.", "labels": [], "entities": [{"text": "ACE Phase 2 (NIST, 2003)", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.7203290686011314}, {"text": "MUC6 (MUC-6, 1995) corpora", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.8436884539467948}]}, {"text": "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F () and the MUC P, Rand F scores ().", "labels": [], "entities": [{"text": "ECM-F", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.8141515254974365}, {"text": "MUC", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.6799793839454651}, {"text": "Rand F scores", "start_pos": 122, "end_pos": 135, "type": "METRIC", "confidence": 0.8750239213307699}]}, {"text": "In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types.", "labels": [], "entities": [{"text": "ACE Phase 2 corpus", "start_pos": 118, "end_pos": 136, "type": "DATASET", "confidence": 0.9082298576831818}]}, {"text": "The results obtained are tabulated in.", "labels": [], "entities": []}, {"text": "As can be observed, when it has prior knowledge of the mention types BESTCUT performs significantly better than the other two systems in the ECM-F score and slightly better in the MUC metrics.", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9824838042259216}, {"text": "ECM-F score", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.46944814920425415}, {"text": "MUC", "start_pos": 180, "end_pos": 183, "type": "DATASET", "confidence": 0.5170128345489502}]}, {"text": "The more knowledge it has about the mentions, the better it performs.", "labels": [], "entities": []}, {"text": "This is consistent with the fact that the first stage of the algorithm divides the graph into subgraphs corresponding to the five entity types.", "labels": [], "entities": []}, {"text": "If BESTCUT has no information about the mentions, its performance ranks significantly under the LinkBest and Belltree algorithms in ECM-F and MUC R.", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.723895251750946}, {"text": "LinkBest", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.8865267634391785}, {"text": "MUC R", "start_pos": 142, "end_pos": 147, "type": "TASK", "confidence": 0.61016446352005}]}, {"text": "Surprisingly enough, the Belltree algorithm, a globally optimized algorithm, performs similarly to Link-Best inmost of the scores.", "labels": [], "entities": []}, {"text": "Despite not being as dramatically affected as BESTCUT, the other two algorithms also decrease in performance with the decrease of the mention information available, which empirically proves that mention detection is a very important module for coreference resolution.", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9602790474891663}, {"text": "mention detection", "start_pos": 195, "end_pos": 212, "type": "TASK", "confidence": 0.7363273501396179}, {"text": "coreference resolution", "start_pos": 244, "end_pos": 266, "type": "TASK", "confidence": 0.9772506952285767}]}, {"text": "Even with an F-score of 77.2% for detecting entity types, our mention detection system boosts the scores of all three algorithms when compared to the case where no information is available.", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9988188147544861}, {"text": "mention detection", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.6757354140281677}]}, {"text": "It is apparent that the MUC score does not vary significantly between systems.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.6422783136367798}]}, {"text": "This only shows that none of them is particularly poor, but it is not a relevant way of comparing methods-the MUC metric has been found too indulgent by researchers), (: Comparison of results between three clusterization algorithms on ACE Phase 2.", "labels": [], "entities": [{"text": "ACE Phase 2", "start_pos": 235, "end_pos": 246, "type": "DATASET", "confidence": 0.8524882396062216}]}, {"text": "The learning algorithms are maxent for coreference and SVM for stopping the cut in BESTCUT.", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.5042556524276733}]}, {"text": "In turn, we obtain the mentions from the key files, detect them with our mention detection algorithm or do not use any information about them.", "labels": [], "entities": []}, {"text": "annotation keys and the system output, while the ECM-F metric aligns the detected entities with the key entities so that the number of common mentions is maximized.", "labels": [], "entities": []}, {"text": "The ECM-F scorer overcomes two shortcomings of the MUC scorer: not considering single mentions and treating every error as equally important (, which makes the ECM-F a more adequate measure of coreference.", "labels": [], "entities": [{"text": "MUC scorer", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.850884884595871}]}, {"text": "Our second experiment evaluates the impact that the different categories of our added features have on the performance of the BESTCUT system.", "labels": [], "entities": [{"text": "BESTCUT", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.529442310333252}]}, {"text": "The experiment was performed with a maxent classifier on the MUC6 corpus, which was priorly converted into ACE format, and employed mention information from the key annotations.", "labels": [], "entities": [{"text": "MUC6 corpus", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9852680861949921}]}, {"text": "From we can observe that the lexical features (head-match, type-pair, name-alias) have the most influence on the ECM-F and MUC scores, succeeded by the syntactic features.", "labels": [], "entities": []}, {"text": "Despite what intuition suggests, the improvement the grammatical feature gn-agree brings to the system is very small.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Comparison of results between three clusterization algorithms on ACE Phase 2. The learning  algorithms are maxent for coreference and SVM for stopping the cut in BESTCUT. In turn, we obtain  the mentions from the key files, detect them with our mention detection algorithm or do not use any  information about them.", "labels": [], "entities": [{"text": "ACE Phase", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.8383514881134033}, {"text": "maxent", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9681757688522339}, {"text": "BESTCUT", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.9274880290031433}]}, {"text": " Table 5: Impact of feature categories on BEST- CUT on MUC6. Baseline system has the (Luo et  al., 2004) features. The system was tested on key  mentions.", "labels": [], "entities": [{"text": "BEST", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.7783783078193665}, {"text": "MUC6", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.7693189382553101}]}]}