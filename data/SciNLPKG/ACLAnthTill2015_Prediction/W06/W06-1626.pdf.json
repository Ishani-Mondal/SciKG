{"title": [{"text": "Distributed Language Modeling for N -best List Re-ranking", "labels": [], "entities": [{"text": "Distributed Language Modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8683228095372518}, {"text": "Re-ranking", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.4776351749897003}]}], "abstractContent": [{"text": "In this paper we describe a novel distributed language model for N-best list re-ranking.", "labels": [], "entities": [{"text": "N-best list re-ranking", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.5874235729376475}]}, {"text": "The model is based on the client/server paradigm where each server hosts a portion of the data and provides information to the client.", "labels": [], "entities": []}, {"text": "This model allows for using an arbitrarily large corpus in a very efficient way.", "labels": [], "entities": []}, {"text": "It also provides a natural platform for relevance weighting and selection.", "labels": [], "entities": [{"text": "relevance weighting", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7628904581069946}]}, {"text": "We applied this model on a 2.97 billion-word corpus and re-ranked the N-best list from Hiero, a state-of-the-art phrase-based system.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.8834439516067505}]}, {"text": "Using BLEU as a metric, the re-ranked translation achieves a relative improvement of 4.8%, significantly better than the model-best translation .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9981264472007751}]}], "introductionContent": [{"text": "Statistical language modeling has been widely used in natural language processing applications such as Automatic Speech Recognition (ASR), Statistical Machine Translation (SMT) and Information Retrieval (IR).", "labels": [], "entities": [{"text": "Statistical language modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8313030203183492}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 103, "end_pos": 137, "type": "TASK", "confidence": 0.8015867670377096}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 139, "end_pos": 176, "type": "TASK", "confidence": 0.8054461578528086}, {"text": "Information Retrieval (IR)", "start_pos": 181, "end_pos": 207, "type": "TASK", "confidence": 0.8524929285049438}]}, {"text": "Conventional n-gram language modeling counts the frequency of all the n-grams in a corpus and calculates the conditional probabilities of a word given its history of n \u2212 1 words P (w i |w i\u22121 i\u2212n+1 ).", "labels": [], "entities": []}, {"text": "As the corpus size increases, building a high order language model offline becomes very expensive if it is still possible.", "labels": [], "entities": []}, {"text": "In this paper, we describe anew approach of language modeling using a distributed computing paradigm.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7275390475988388}]}, {"text": "Distributed language modeling can make use of arbitrarily large training corpora and provides a natural way for language model adaptation.", "labels": [], "entities": [{"text": "Distributed language modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6632642447948456}, {"text": "language model adaptation", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.6976769963900248}]}, {"text": "We applied the distributed LM to the task of reranking the N -best list in statistical machine translation and achieved significantly better translation quality when measured by the BLEU metric).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.6626503864924113}, {"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9952225089073181}]}], "datasetContent": [{"text": "We used the N -best list generated by the Hiero SMT system.", "labels": [], "entities": [{"text": "Hiero SMT system", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.7800157964229584}]}, {"text": "Hiero is a statistical phrase-based translation model that uses hierarchical phrases.", "labels": [], "entities": [{"text": "statistical phrase-based translation", "start_pos": 11, "end_pos": 47, "type": "TASK", "confidence": 0.5572550495465597}]}, {"text": "The decoder uses a trigram language model trained with modified Kneser-Ney smoothing) on a 200 million words corpus.", "labels": [], "entities": []}, {"text": "The 1000-best list was generated on 919 sentences from the MT03 ChineseEnglish evaluation set.", "labels": [], "entities": [{"text": "MT03 ChineseEnglish evaluation set", "start_pos": 59, "end_pos": 93, "type": "DATASET", "confidence": 0.856586292386055}]}, {"text": "All the data from the English Gigaword corpus plus the English side of the Chinese-English bilingual data available from LDC are used.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.8702424963315328}, {"text": "LDC", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.6656926870346069}]}, {"text": "The 2.97 billion words data is split into 150 chunks, each has about 20 million words.", "labels": [], "entities": []}, {"text": "The original order is kept so that each chunk contains data from the same news source and a certain period of time.", "labels": [], "entities": []}, {"text": "For example, chunk Xinhua2003 has all the Xinhua News data from year 2003 and NYT9499 038 has the last 20 million words from the New York Times 1994-1999 corpus.", "labels": [], "entities": [{"text": "Xinhua News data", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.7218242585659027}, {"text": "NYT9499 038", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8700337707996368}, {"text": "New York Times 1994-1999 corpus", "start_pos": 129, "end_pos": 160, "type": "DATASET", "confidence": 0.6448988497257233}]}, {"text": "One could split the data into larger(smaller) chunks which will require less(more) servers.", "labels": [], "entities": []}, {"text": "We choose 20 million words as the size for each chunk because it can be loaded by our smallest machine and it is a reasonable granularity for selection.", "labels": [], "entities": []}, {"text": "In total, 150 corpus information servers run on 26 machines connected by the standard Ethernet LAN.", "labels": [], "entities": []}, {"text": "One client sends each English hypothesis translations to all 150 servers and uses the returned information to re-rank.", "labels": [], "entities": []}, {"text": "The whole process takes about 600 seconds to finish.", "labels": [], "entities": []}, {"text": "We use BLEU scores to measure the translation accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9988420605659485}, {"text": "translation", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9419010877609253}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8636611104011536}]}, {"text": "A bootstrapping method is used to calculate the 95% confidence intervals for BLEU).", "labels": [], "entities": [{"text": "95% confidence intervals", "start_pos": 48, "end_pos": 72, "type": "METRIC", "confidence": 0.6450162753462791}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9939036965370178}]}], "tableCaptions": [{"text": " Table 2: BLEU scores of the re-ranked translations. Baseline score = 31.44", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981098175048828}, {"text": "Baseline score", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.9789675176143646}]}]}