{"title": [{"text": "Maximum Entropy Tagging with Binary and Real-Valued Features", "labels": [], "entities": [{"text": "Entropy Tagging", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6406251192092896}]}], "abstractContent": [{"text": "Recent literature on text-tagging reported successful results by applying Maximum Entropy (ME) models.", "labels": [], "entities": []}, {"text": "In general, ME taggers rely on carefully selected binary features, which try to capture discrimi-nant information from the training data.", "labels": [], "entities": [{"text": "ME taggers", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.9537211358547211}]}, {"text": "This paper introduces a standard setting of binary features, inspired by the literature on named-entity recognition and text chunking, and derives corresponding real-valued features based on smoothed log-probabilities.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7191883325576782}, {"text": "text chunking", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.6968011111021042}]}, {"text": "The resulting ME models have orders of magnitude fewer parameters.", "labels": [], "entities": []}, {"text": "Effective use of training data to estimate features and parameters is achieved by integrating a leaving-one-out method into the standard ME training algorithm.", "labels": [], "entities": []}, {"text": "Experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binary-feature models with real-valued features.", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9107118248939514}]}], "introductionContent": [{"text": "The Maximum Entropy (ME) statistical framework ( has been successfully deployed in several NLP tasks.", "labels": [], "entities": []}, {"text": "In recent evaluation campaigns, e.g. DARPA, ME models reached state-of-the-art performance on a range of text-tagging tasks.", "labels": [], "entities": []}, {"text": "With few exceptions, best ME taggers rely on carefully designed sets of features.", "labels": [], "entities": [{"text": "best ME taggers", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.5565360933542252}]}, {"text": "Features correspond to binary functions, which model events, observed in the (annotated) training data and supposed to be meaningful or discriminative for the task at hand.", "labels": [], "entities": []}, {"text": "Hence, ME models result in a loglinear combination of a large set of features, whose weights can be estimated by the well known Generalized Iterative Scaling (GIS) algorithm by.", "labels": [], "entities": [{"text": "Generalized Iterative Scaling (GIS)", "start_pos": 128, "end_pos": 163, "type": "TASK", "confidence": 0.7266245732704798}]}, {"text": "Despite ME theory and its related training algorithm do not set restrictions on the range of feature functions 1 , popular NLP text books) and research papers) seem to limit them to binary features.", "labels": [], "entities": []}, {"text": "In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.6736571987469991}]}, {"text": "This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.7337665210167567}, {"text": "Text Chuncking (TC)", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8235589444637299}]}, {"text": "By taking inspiration from the literature (), a set of standard binary features is introduced.", "labels": [], "entities": []}, {"text": "Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data.", "labels": [], "entities": []}, {"text": "A direct comparison of ME models based on binary, realvalued, and mixed features is presented.", "labels": [], "entities": []}, {"text": "Besides, performance on the tagging tasks, complexity and training time by each model are reported.", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9168960750102997}, {"text": "complexity", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9714239239692688}]}, {"text": "ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method).", "labels": [], "entities": [{"text": "ME estimation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.825666755437851}]}, {"text": "Experiments were conducted on two publicly available benchmarks for which performance levels of many systems are published on the Web.", "labels": [], "entities": []}, {"text": "Results show that better ME models for NER and TC can be developed by integrating binary and realvalued features.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents results of ME models applied to two text-tagging tasks, Named Entity Recognition (NER) and Text Chunking (TC).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.7377590586741766}, {"text": "Text Chunking (TC)", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7729295432567597}]}, {"text": "After a short introduction to the experimental framework, the detailed feature setting is presented.", "labels": [], "entities": []}, {"text": "Then, experimental results are presented for the following contrastive conditions: binary versus real-valued features, training via held-out versus leave-one-out, atomic versus complex features.", "labels": [], "entities": []}, {"text": "Named Entity Recognition English NER experiments were carried out on the CoNLL-2003 shared task 2 . This benchmark is based on texts from the Reuters Corpus which were manually annotated with parts-of-speech, chunk tags, and named entity categories.", "labels": [], "entities": [{"text": "Entity Recognition English NER", "start_pos": 6, "end_pos": 36, "type": "TASK", "confidence": 0.8230078518390656}, {"text": "CoNLL-2003 shared task 2", "start_pos": 73, "end_pos": 97, "type": "DATASET", "confidence": 0.9019725918769836}, {"text": "Reuters Corpus", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.9493038058280945}]}, {"text": "Four types of categories are defined: person, organization, location and miscellaneous, to include e.g. nations, artifacts, etc.", "labels": [], "entities": []}, {"text": "A filler class is used for the remaining words.", "labels": [], "entities": []}, {"text": "After including tags denoting the start of multiword entities, a total of 9 tags results.", "labels": [], "entities": []}, {"text": "Data are partitioned into training (200K words), development (50K words), and test (46K words) samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance of baseline models on the  NER task. Number of parameters, precision, re- call, and F-score are reported for each model.", "labels": [], "entities": [{"text": "NER task", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.921729564666748}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9992890357971191}, {"text": "re- call", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9587611158688863}, {"text": "F-score", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9993212223052979}]}, {"text": " Table 5: Performance of mixed feature models  with two different training methods.", "labels": [], "entities": []}, {"text": " Table 3: Setting used for binary and real-valued features in the reported experiments.", "labels": [], "entities": []}, {"text": " Table 8: Results with complex features on the TC  task.", "labels": [], "entities": [{"text": "TC  task", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.9155613780021667}]}]}