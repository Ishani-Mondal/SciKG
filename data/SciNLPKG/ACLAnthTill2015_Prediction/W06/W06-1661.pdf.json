{"title": [], "abstractContent": [{"text": "In this paper we describe and evaluate several statistical models for the task of realization ranking, i.e. the problem of discriminating between competing surface realizations generated fora given input semantics.", "labels": [], "entities": [{"text": "realization ranking", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8927704989910126}]}, {"text": "Three models (and several variants) are trained and tested: an \u00d2-gram language model, a discriminative maximum entropy model using structural information (and incorporating the language model as a separate feature), and finally an SVM ranker trained on the same feature set.", "labels": [], "entities": []}, {"text": "The resulting hybrid tactical generator is part of a larger, semantic transfer MT system.", "labels": [], "entities": [{"text": "semantic transfer MT", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.5782008568445841}]}], "introductionContent": [{"text": "This paper describes the application of several different statistical models for the task of realization ranking in tactical generation, i.e. the problem of choosing among multiple paraphrases that are generated fora given meaning representation.", "labels": [], "entities": [{"text": "realization ranking", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.9049367606639862}, {"text": "tactical generation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8302344083786011}]}, {"text": "The specific realization component we use is the opensource chart generator of the Linguistic.", "labels": [], "entities": []}, {"text": "Given a meaning representation in the form of Minimal Recursion Semantics (MRS; Copestake, Flickinger, Malouf,, the generator outputs English realizations in accordance with the HPSG LinGO English Resource.", "labels": [], "entities": [{"text": "HPSG LinGO English Resource", "start_pos": 178, "end_pos": 205, "type": "DATASET", "confidence": 0.9590156227350235}]}, {"text": "As an example of generator output, a sub-set of alternate realizations that are produced fora single input MRS is shown in.", "labels": [], "entities": []}, {"text": "For the two data sets considered in this paper, the average number of realizations produced by the generator is 85.7 and 102.2 (the maximum numbers are 4176 and 3408, respectively).", "labels": [], "entities": []}, {"text": "Thus, there is immediate demand fora principled way of choosing a single output among the generated candidates.", "labels": [], "entities": []}, {"text": "For this task we train and test three different statistical models: an \u00d2-gram language model, a maximum entropy model (MaxEnt) and a (linear) support vector machine (SVM).", "labels": [], "entities": []}, {"text": "These are all models that have proved popular within the NLP community, but it is usually only the first of these three that has been applied to the task of ranking in sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7091614454984665}]}, {"text": "The latter two models that we present here go beyond the surface information used by the \u00d2-gram model, and are trained on asymmetric treebank with features defined over the full HPSG analyses of competing realizations.", "labels": [], "entities": []}, {"text": "Furthermore, such discriminative models are suitable for 'on-line' use within our generator-adopting the technique of selective unpacking from a packed forest)-which means our hybrid realizer obviates the need for exhaustive enumeration of candidate outputs.", "labels": [], "entities": []}, {"text": "The present results extend our earlier work)-and the related work of Nakanishi,-to an enlarged data set, more feature types, and additional learners.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 first gives a general summary of the various statistical models we will be considering, as well as the measures used for evaluating them.", "labels": [], "entities": []}, {"text": "We then goon to define the task we are aiming to solve in terms of treebank data and feature types in Section 3.", "labels": [], "entities": []}, {"text": "By looking at different variants of the MaxEnt model we review some results for the relative contribution of individual features and the impact of frequency cutoffs for feature selection.", "labels": [], "entities": []}, {"text": "Keeping these parameters constant then, Section 4 provides an array of empirical results on the relative performance of the various approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "The models presented in this paper are evaluated with respect to two simple metrics: exact match accuracy and word accuracy.", "labels": [], "entities": [{"text": "exact match accuracy", "start_pos": 85, "end_pos": 105, "type": "METRIC", "confidence": 0.9037056962649027}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.4996235966682434}]}, {"text": "The exact match measure simply counts the number of times that the model assigns the highest score to a string that exactly matches a corresponding 'gold' or reference sentence (i.e. a sentence that is marked as preferred in the treebank).", "labels": [], "entities": [{"text": "exact match measure", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8609365423520406}]}, {"text": "This score is discounted appropriately in the case of ties between preferred and non-preferred candidates.", "labels": [], "entities": []}, {"text": "if several realizations are given the top rank by the model.", "labels": [], "entities": []}, {"text": "We also include the exact match accuracy for the five best candidates according to the models (see the \u00d2-best columns of).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.575654923915863}]}, {"text": "The simple measure of exact match accuracy offers a very intuitive and transparent view on model performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7529058456420898}]}, {"text": "However, it is also in some respects too harsh as an evaluation measure in our setting since there will often be more than just one of the candidate realizations that provides a reasonable rendering of the input semantics.", "labels": [], "entities": []}, {"text": "We therefore also include WA as similarity-based evaluation metric.", "labels": [], "entities": [{"text": "WA", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9905036091804504}]}, {"text": "This measure is based on the Levensthein distance between a candidate string and a reference, also known as edit distance.", "labels": [], "entities": []}, {"text": "This is given by the minimum number of deletions, substitutions and insertions of words that are required to transform one string into another.", "labels": [], "entities": []}, {"text": "If we let \ud97b\udf59, \u00d7 and \ud97b\udf59 represent the number of necessary deletions, substitutions and insertions respectively, and let \u00d0 be the length of the reference, then WA is defined as The scores produced by similarity measures such as WA are often difficult to interpret, but at least they provide an alternative view on the relative performance of the different models that we want to compare.", "labels": [], "entities": [{"text": "WA", "start_pos": 156, "end_pos": 158, "type": "METRIC", "confidence": 0.9091862440109253}]}, {"text": "We could also have used several other similarity measures here, such as the BLEU score which is a well-established evaluation metric within MT, but in our experience the various string similarity measures usually agree on the relative ranking of the different models.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9810213148593903}, {"text": "MT", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9069212675094604}]}], "tableCaptions": [{"text": " Table 4: The effects of frequency-based feature se- lection with respect to model size and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9988570213317871}]}, {"text": " Table 5: Performance summaries of best- performing realization rankers using various fea- ture configurations, when compared to the set-up  of", "labels": [], "entities": []}, {"text": " Table 6: Performance of the different learners. The results on the 'Jotunheimen' treebank for the discrim- inative models are averages from 10-fold cross-validation. A model trained on the entire 'Jotunheimen'  data was used when testing on 'Rondane'. Note that the training accuracy of the SVM learner on the  'Jotunheimen' training set is 91.69%, while it's 92.99% for the MaxEnt model.", "labels": [], "entities": [{"text": "Jotunheimen' treebank", "start_pos": 69, "end_pos": 90, "type": "DATASET", "confidence": 0.8953169782956442}, {"text": "Jotunheimen'  data", "start_pos": 198, "end_pos": 216, "type": "DATASET", "confidence": 0.7708726127942404}, {"text": "accuracy", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.8986008763313293}, {"text": "Jotunheimen' training set", "start_pos": 313, "end_pos": 338, "type": "DATASET", "confidence": 0.865854874253273}, {"text": "MaxEnt", "start_pos": 376, "end_pos": 382, "type": "DATASET", "confidence": 0.9383857250213623}]}, {"text": " Table 7: Exact match error counts for three mod- els, viz. the BNC LM only, the MaxEnt model by  itself (using all feature types except the LM prob- ability), and the combined MaxEnt model. The  intermediate column corresponds to ties or partial  errors, i.e. the number of items for which multiple  candidates were ranked at the top, of which some  were actually preferred and some not. Primarily  this latter error type is reduced by including the  LM feature in the MaxEnt universe.", "labels": [], "entities": [{"text": "BNC LM", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.8444707989692688}]}]}