{"title": [{"text": "Classifying Particle Semantics in English Verb-Particle Constructions", "labels": [], "entities": [{"text": "Verb-Particle Constructions", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6152504980564117}]}], "abstractContent": [{"text": "Previous computational work on learning the semantic properties of verb-particle constructions (VPCs) has focused on their compositionality, and has left unaddressed the issue of which meaning of the component words is being used in a given VPC.", "labels": [], "entities": [{"text": "learning the semantic properties of verb-particle constructions (VPCs)", "start_pos": 31, "end_pos": 101, "type": "TASK", "confidence": 0.669853737950325}]}, {"text": "We develop a feature space for use in classification of the sense contributed by the particle in a VPC, and test this on VPCs using the particle up.", "labels": [], "entities": []}, {"text": "The features that capture linguistic properties of VPCs that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test VPCs.", "labels": [], "entities": []}], "introductionContent": [{"text": "A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality-the contribution of each component word to the overall semantics of the expression.", "labels": [], "entities": [{"text": "learning the semantics of multiword expressions (MWEs)", "start_pos": 15, "end_pos": 69, "type": "TASK", "confidence": 0.695573197470771}]}, {"text": "MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof ).", "labels": [], "entities": []}, {"text": "Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g.,).", "labels": [], "entities": []}, {"text": "However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional.", "labels": [], "entities": []}, {"text": "Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning is still unknown, since the actual semantic contribution of the components is yet to be determined.", "labels": [], "entities": []}, {"text": "We address this problem in the domain of verbparticle constructions (VPCs) in English, a rich source of MWEs.", "labels": [], "entities": []}, {"text": "VPCs combine a verb with any of a finite set of particles, as in jump up, figure out, or give in.", "labels": [], "entities": []}, {"text": "Particles such as up, out, or in, with their literal meaning based in physical spatial relations, show a variety of metaphorical and aspectual meaning extensions, as exemplified here for the particle up: (1a) The sun just came up.", "labels": [], "entities": []}, {"text": "(1b) She walked up to him.", "labels": [], "entities": []}, {"text": "[movement toward a goal] (1c) Drink up your juice!", "labels": [], "entities": []}, {"text": "(1d) He curled up into a ball.", "labels": [], "entities": []}, {"text": "Cognitive linguistic analysis, as in, can provide the basis for elaborating this type of semantic variation.", "labels": [], "entities": [{"text": "Cognitive linguistic analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6470633347829183}]}, {"text": "Given such a sense inventory fora particle, our goal is to automatically determine its meaning when used with a given verb in a VPC.", "labels": [], "entities": []}, {"text": "We classify VPCs according to their particle sense, using statistical features that capture the semantic and syntactic properties of verbs and particles.", "labels": [], "entities": []}, {"text": "We contrast these with simple word co-occurrence features, which are often used to indicate the semantics of a target word.", "labels": [], "entities": []}, {"text": "In our experiments, we focus on VPCs using the particle up because it is highly frequent and has a wide range of meanings.", "labels": [], "entities": []}, {"text": "However, it is worth emphasizing that our feature space draws on general properties of VPCs, and is not specific to this particle.", "labels": [], "entities": []}, {"text": "A VPC maybe ambiguous, with its particle occurring in more than one sense; in contrast to (1a), come up may use up in a goal-oriented sense as in The deadline is coming up.", "labels": [], "entities": []}, {"text": "While our long-term goal is token classification (disambiguation) of a VPC in context, following other work on VPCs (e.g.,, we begin herewith the task of type classification.", "labels": [], "entities": [{"text": "token classification (disambiguation) of a VPC", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.8314275071024895}, {"text": "type classification", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.8039344549179077}]}, {"text": "Given our use of features which capture the statistical behaviour relevant to a VPC across a corpus, we assume that the outcome of type classification yields the predominant sense of the particle in the VPC.", "labels": [], "entities": [{"text": "type classification", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7194587290287018}]}, {"text": "Predominant sense identification is a useful component of sense disambiguation of word tokens (), and we presume our VPC type classification work will form the basis for later token disambiguation.", "labels": [], "entities": [{"text": "Predominant sense identification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6160101195176443}, {"text": "sense disambiguation of word tokens", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.8051065802574158}, {"text": "VPC type classification", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.7357020179430643}, {"text": "token disambiguation", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.7224692553281784}]}, {"text": "Section 2 continues the paper with a discussion of the features we developed for particle sense classification.", "labels": [], "entities": [{"text": "particle sense classification", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.817935049533844}]}, {"text": "Section 3 first presents some brief cognitive linguistic background, followed by the sense classes of up used in our experiments.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 discuss our experimental set-up and results, Section 6 related work, and Section 7 our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created a list of English VPCs using up, based on a list of VPCs made available by and a list of VPCs compiled by two human judges.", "labels": [], "entities": []}, {"text": "The judges then filtered this list to include only VPCs which they both agreed were valid, resulting in a final list of 389 VPCs.", "labels": [], "entities": []}, {"text": "From this list, training, verification and test sets of sixty VPCs each are randomly selected.", "labels": [], "entities": []}, {"text": "Note that the expense of manually annotating the data (as described below) prevents us from using larger datasets in this initial investigation.", "labels": [], "entities": []}, {"text": "The experimental sets are chosen such that each includes the same proportion of verbs across three frequency bands, so that the sets do not differ in frequency distribution of the verbs.", "labels": [], "entities": []}, {"text": "(We use frequency of the verbs, rather than the VPCs, since many of our features are based on the verb of the expression, and moreover, VPC frequency is approximate.)", "labels": [], "entities": []}, {"text": "The verification data is used in exploration of the feature space and selection of final features to use in testing; the test set is held out for final testing of the classifiers.", "labels": [], "entities": []}, {"text": "Each VPC in each dataset is annotated by the two human judges according to which of the four senses of up identified in Section 3.2 is contributed to the VPC.", "labels": [], "entities": []}, {"text": "As noted in Section 1, VPCs maybe ambiguous with respect to their particle sense.", "labels": [], "entities": []}, {"text": "Since our task here is type classification, the judges identify the particle sense of a VPC in its predominant usage, in their assessment.", "labels": [], "entities": [{"text": "type classification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8865855932235718}]}, {"text": "The observed inter-annotator agreement is , for the training, verification and test sets respectively.", "labels": [], "entities": []}, {"text": "Since we want to see how our features perform on differing granularities of sense classes, we run each experiment as both a 3-way and 2-way classification task.", "labels": [], "entities": []}, {"text": "In the 3-way task, the sense classes correspond to the meanings Vert-up, Goalup merged with Cmpl-up (as noted above), and Refl-up, as shown in: Frequency of items in each class for the 2-way task.", "labels": [], "entities": [{"text": "Refl-up", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9921984076499939}]}, {"text": "up/Cmpl-up with that of Refl-up, as shown in Table 3.", "labels": [], "entities": [{"text": "Cmpl-up", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.9447657465934753}]}, {"text": "We choose to merge these classes because (as illustrated in) Refl-up is a sub-sense of Goal-up, and moreover, all three of these senses contrast with Vert-up, in which increase along a vertical axis is the salient property.", "labels": [], "entities": []}, {"text": "It is worth emphasizing that the 2-way task is not simply a classification between literal and non-literal up-Vertup includes extensions of up in which the increase along a vertical axis is metaphorical.", "labels": [], "entities": []}, {"text": "The variation in the frequency of the sense classes of up across the datasets makes the true distribution of the classes difficult to estimate.", "labels": [], "entities": []}, {"text": "Furthermore, there is no obvious informed baseline for this task.", "labels": [], "entities": []}, {"text": "Therefore, we make the assumption that the true distribution of the classes is uniform, and use the chance accuracy . Accordingly, our measure of classification accuracy should weight each class evenly.", "labels": [], "entities": [{"text": "chance accuracy", "start_pos": 100, "end_pos": 115, "type": "METRIC", "confidence": 0.8743385970592499}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.815739631652832}]}, {"text": "Therefore, we report the average per class accuracy, which gives equal weight to each class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.982082724571228}]}, {"text": "For classification we use LIBSVM, an implementation of a support-vector machine.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.971236526966095}]}, {"text": "We set the input parameters, cost and gamma, using 10-fold cross-validation on the training data.", "labels": [], "entities": [{"text": "gamma", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9666790962219238}]}, {"text": "In addition, we assign a weight of to eliminate the effects of the variation in class size on the classifier.", "labels": [], "entities": []}, {"text": "Note that our choice of accuracy measure and weighting of classes in the classifier is necessary given our assumption of a uniform random baseline.", "labels": [], "entities": [{"text": "accuracy measure", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.9810363352298737}]}, {"text": "Since the accuracy values we report incorporate this weighting, these results cannot be compared to a baseline of always choosing the most frequent class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998596727848053}]}, {"text": "We present experimental results for both Ver(ification) and unseen Test data, on each set of features, individually and in combination.", "labels": [], "entities": [{"text": "Ver", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9555569291114807}]}, {"text": "All experiments are run on both the 2-way and 3-way sense classification, which have a chance baseline of 50% and 33%, respectively.: Accuracy (%) using linguistic features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9817783832550049}]}, {"text": "The results for experiments using the features that capture semantic and syntactic properties of verbs and VPCs are summarized in, and discussed in turn below.", "labels": [], "entities": []}, {"text": "Our goal was to compare the more knowledge-rich slot and particle features to an alternative feature set, the WCFs, which does not rely on linguistic analysis of the semantics and syntax of verbs and VPCs.", "labels": [], "entities": []}, {"text": "Recall that we experiment with both 200 feature words, WCF , and 500 feature words, WCF , as shown in.", "labels": [], "entities": []}, {"text": "Most of the experiments using WCFs perform worse than the corresponding experiment using all the linguistic features.", "labels": [], "entities": []}, {"text": "It appears that the linguistically motivated features are better suited to our task than simple word context features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frequency of items in each sense class.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9099193215370178}]}, {"text": " Table 2: Frequency of items in each class for the  3-way task.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.977807343006134}]}, {"text": " Table 2. In the 2-way task, we  further merge the classes corresponding to Goal-", "labels": [], "entities": []}, {"text": " Table 3: Frequency of items in each class for the  2-way task.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9758067727088928}]}, {"text": " Table 4: Accuracy (%) using linguistic features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983667731285095}]}, {"text": " Table 5: Accuracy (%) using WCFs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989805817604065}]}, {"text": " Table 6: Accuracy (%) combining linguistic fea- tures with WCFs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.993368923664093}]}]}