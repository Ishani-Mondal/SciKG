{"title": [{"text": "Multilingual Deep Lexical Acquisition for HPSGs via Supertagging", "labels": [], "entities": [{"text": "Multilingual Deep Lexical Acquisition", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6594960391521454}, {"text": "Supertagging", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.5498390197753906}]}], "abstractContent": [{"text": "We propose a conditional random field-based method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of English and Japanese.", "labels": [], "entities": []}, {"text": "Using a pseudo-likelihood approximation we are able to scale our model to hundreds of supertags and tens-of-thousands of training sentences.", "labels": [], "entities": []}, {"text": "We show that it is possible to achieve start-of-the-art results for both languages using maximally language-independent lexical features.", "labels": [], "entities": []}, {"text": "Further, we explore the performance of the models at the type-and token-level, demonstrating their superior performance when compared to a unigram-based base-line and a transformation-based learning approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over recent years, there has been a resurgence of interest in the use of precision grammars in NLP tasks, due to advances in parsing algorithm development, grammar development tools and raw computational power ).", "labels": [], "entities": [{"text": "parsing algorithm development", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.9260392586390177}]}, {"text": "Precision grammars are defined as implemented grammars of natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic \"markedness\").", "labels": [], "entities": []}, {"text": "Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string.", "labels": [], "entities": []}, {"text": "Examples include DELPH-IN grammars such as the English Resource Grammar), the various PARGRAM grammars, and the Edinburgh CCG parser ().", "labels": [], "entities": [{"text": "English Resource Grammar", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.8951263626416525}, {"text": "Edinburgh CCG parser", "start_pos": 112, "end_pos": 132, "type": "DATASET", "confidence": 0.963376005490621}]}, {"text": "Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage.", "labels": [], "entities": []}, {"text": "Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items).", "labels": [], "entities": []}, {"text": "Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition.", "labels": [], "entities": [{"text": "deep lexical acquisition", "start_pos": 203, "end_pos": 227, "type": "TASK", "confidence": 0.6790013015270233}]}, {"text": "In this, we follow in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items.", "labels": [], "entities": []}, {"text": "For the purposes of this paper, we focus specifically on supertagging as the mechanism for hypothesising new lexical items.", "labels": [], "entities": [{"text": "hypothesising new lexical items", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.8781566023826599}]}, {"text": "Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar.", "labels": [], "entities": []}, {"text": "It was first introduced as a means of reducing parser ambiguity by in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism ( ).", "labels": [], "entities": []}, {"text": "In both of these cases, supertagging provides the means to perform abeam search over the plausible lexical items fora given string context, and ideally reduces parsing complexity without sacrificing parser accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.9393051266670227}]}, {"text": "An alternate application of supertagging is in DLA, in postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage.", "labels": [], "entities": []}, {"text": "Our immediate interest in this paper is in the first of these tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line DLA.", "labels": [], "entities": []}, {"text": "In this research, we focus particularly on the Grammar Matrix-based DELPH-IN family of grammars ( ), which includes grammars of English, Japanese, Norwegian, Modern Greek, Portuguese and Korean.", "labels": [], "entities": []}, {"text": "The Grammar Matrix is a framework for streamlining and standardising HPSG-based multilingual grammar development.", "labels": [], "entities": [{"text": "standardising HPSG-based multilingual grammar development", "start_pos": 55, "end_pos": 112, "type": "TASK", "confidence": 0.6931478142738342}]}, {"text": "One property of Grammar Matrixbased grammars is that they are strongly lexicalist and adhere to a highly constrained lexicongrammar interface via a unique (terminal) lexical type for each lexical item.", "labels": [], "entities": []}, {"text": "As such, lexical item creation in any of the Grammar Matrix-based grammars, irrespective of language, consists predominantly of predicting the appropriate lexical type for each lexical item, relative to the lexical hierarchy for the corresponding grammar.", "labels": [], "entities": [{"text": "lexical item creation", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.6763551533222198}]}, {"text": "In this same spirit of standardisation and multilinguality, the aim of this research is to develop maximally language-independent supertagging methods which can be applied to any Grammar Matrixbased grammar with the minimum of effort.", "labels": [], "entities": []}, {"text": "Essentially, we hope to provide the grammar engineer with the means to semi-automatically populate the lexicon of a semi-mature grammar, hence accelerating the pace of lexicon development and producing a resource of sufficient coverage to be practically useful in NLP tasks.", "labels": [], "entities": []}, {"text": "The contributions of this paper are the development of a pseudo-likelihood conditional random field-based method of supertagging, which we then apply to the task of off-line DLA for grammars of both English and Japanese with only minor language-specific adaptation.", "labels": [], "entities": []}, {"text": "We show the supertagger to outperform previously-proposed supertagger-based DLA methods.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 outlines past work relative to this research, and Section 3 reviews the resources used in our supertagging experiments.", "labels": [], "entities": []}, {"text": "Section 4 outlines the proposed supertagger model and reviews previous research on supertaggerbased DLA.", "labels": [], "entities": []}, {"text": "Section 5 then outlines the set-up and results of our evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation is based on the treebank data associated with each grammar, and a random trainingtest split of 20,000 training sentences and 1,013 test sentences in the case of the ERG, and 40,000 training sentences and 1,095 test sentences in the case of the JACY.", "labels": [], "entities": [{"text": "JACY", "start_pos": 255, "end_pos": 259, "type": "DATASET", "confidence": 0.8736960887908936}]}, {"text": "This split is fixed for all models tested.", "labels": [], "entities": []}, {"text": "Given that the goal of this research is to acquire novel lexical items, our primary focus is on the performance of the different models at predicting the lexical type of any lexical items which occur only in the test data (which maybe either novel lexemes or previously-seen lexemes occurring with a novel lexical type).", "labels": [], "entities": []}, {"text": "As such, we identify all unknown lexical items in the test data and evaluate according to: token accuracy (the proportion of unknown lexical items which are correctly tagged: ACC ); type precision (the proportion of correctly hypothesised unknown lexical entries: PREC); type recall (the proportion of goldstandard unknown lexical entries for which we get a correct prediction: REC); and type F-score (the harmonic mean of type precision and type recall: F-SCORE).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.5539857745170593}, {"text": "type precision", "start_pos": 182, "end_pos": 196, "type": "METRIC", "confidence": 0.8355974555015564}, {"text": "recall", "start_pos": 276, "end_pos": 282, "type": "METRIC", "confidence": 0.7280512452125549}, {"text": "F-score", "start_pos": 393, "end_pos": 400, "type": "METRIC", "confidence": 0.5360729098320007}, {"text": "F-SCORE", "start_pos": 455, "end_pos": 462, "type": "METRIC", "confidence": 0.8411633968353271}]}, {"text": "We also measure the overall token accuracy (ACC) across all words in the test data, irrespective of whether they represent known or unknown lexical items.", "labels": [], "entities": [{"text": "token accuracy (ACC)", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.7855459988117218}]}], "tableCaptions": [{"text": " Table 1. Make-up of the English Resource Grammar (ERG) and JACY grammars and treebanks", "labels": [], "entities": [{"text": "English Resource Grammar (ERG)", "start_pos": 25, "end_pos": 55, "type": "DATASET", "confidence": 0.8505754967530569}]}, {"text": " Table 2. Extracted feature types for the CRF model", "labels": [], "entities": []}, {"text": " Table 3. Results of supertagging for the ERG and JACY (best result in each column in bold)", "labels": [], "entities": [{"text": "ERG", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.6186733841896057}, {"text": "JACY", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.5131273865699768}]}]}