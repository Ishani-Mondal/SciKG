{"title": [], "abstractContent": [{"text": "Lexical features are key to many approaches to sentiment analysis and opinion detection.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9679763317108154}, {"text": "opinion detection", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7930324375629425}]}, {"text": "A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexico-syntactic patterns.", "labels": [], "entities": []}, {"text": "In this paper, we use a subsumption hierarchy to formally define different types of lexical features and their relationship to one another, both in terms of representational coverage and performance.", "labels": [], "entities": []}, {"text": "We use the subsumption hierarchy in two ways: (1) as an analytic tool to automatically identify complex features that outperform simpler features, and (2) to reduce a feature set by removing unnecessary features.", "labels": [], "entities": []}, {"text": "We show that reducing the feature set improves performance on three opinion classification tasks, especially when combined with traditional feature selection.", "labels": [], "entities": [{"text": "opinion classification tasks", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.8141002655029297}]}], "introductionContent": [{"text": "Sentiment analysis and opinion recognition are active research areas that have many potential applications, including review mining, product reputation analysis, multi-document summarization, and multi-perspective question answering.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9322532415390015}, {"text": "opinion recognition", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7582609355449677}, {"text": "review mining", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.7364745587110519}, {"text": "product reputation analysis", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.7473625143369039}, {"text": "multi-document summarization", "start_pos": 162, "end_pos": 190, "type": "TASK", "confidence": 0.6387819945812225}, {"text": "multi-perspective question answering", "start_pos": 196, "end_pos": 232, "type": "TASK", "confidence": 0.6503382325172424}]}, {"text": "Lexical features are key to many approaches, and a variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexico-syntactic patterns.", "labels": [], "entities": []}, {"text": "It is common for different features to overlap representationally.", "labels": [], "entities": []}, {"text": "For example, the unigram \"happy\" will match all of the texts that the bigram \"very happy\" matches.", "labels": [], "entities": []}, {"text": "Since both features represent a positive sentiment and the bigram matches fewer contexts than the unigram, it is probably sufficient just to have the unigram.", "labels": [], "entities": []}, {"text": "However, there are many cases where a feature captures a subtlety or non-compositional meaning that a simpler feature does not.", "labels": [], "entities": []}, {"text": "For example, \"basket case\" is a highly opinionated phrase, but the words \"basket\" and \"case\" individually are not.", "labels": [], "entities": [{"text": "basket case\"", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.8054745197296143}]}, {"text": "An open question in opinion analysis is how often more complex feature representations are needed, and which types of features are most valuable.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7061943262815475}]}, {"text": "Our first goal is to devise a method to automatically identify features that are representationally subsumed by a simpler feature but that are better opinion indicators.", "labels": [], "entities": []}, {"text": "These subjective expressions could then be added to a subjectivity lexicon (, and used to gain understanding about which types of complex features capture meaningful expressions that are important for opinion recognition.", "labels": [], "entities": [{"text": "opinion recognition", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.8062013685703278}]}, {"text": "Many opinion classifiers are created by adopting a \"kitchen sink\" approach that throws together a variety of features.", "labels": [], "entities": []}, {"text": "But in many cases adding new types of features does not improve performance.", "labels": [], "entities": []}, {"text": "For example, found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams.", "labels": [], "entities": []}, {"text": "Our second goal is to automatically identify features that are unnecessary because similar features provide equal or better coverage and discriminatory value.", "labels": [], "entities": []}, {"text": "Our hypothesis is that a reduced feature set, which selectively combines unigrams with only the most valuable complex features, will perform better than a larger feature set that includes the entire \"kitchen sink\" of features.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of a subsumption hierarchy to formally define the subsumption relationships between different types of textual features.", "labels": [], "entities": []}, {"text": "We use the subsumption hierarchy in two ways.", "labels": [], "entities": []}, {"text": "First, we use subsumption as an an-alytic tool to compare features of different complexities and automatically identify complex features that substantially outperform their simpler counterparts.", "labels": [], "entities": []}, {"text": "Second, we use the subsumption hierarchy to reduce a feature set based on representational overlap and on performance.", "labels": [], "entities": []}, {"text": "We conduct experiments with three opinion data sets and show that the reduced feature sets can improve classification performance.", "labels": [], "entities": [{"text": "classification", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.9570306539535522}]}], "datasetContent": [{"text": "To see whether feature subsumption can improve classification performance, we trained an SVM classifier for each of the three opinion data sets.", "labels": [], "entities": []}, {"text": "We used the SVM light (Joachims, 1998) package with a linear kernel.", "labels": [], "entities": [{"text": "SVM light (Joachims, 1998)", "start_pos": 12, "end_pos": 38, "type": "DATASET", "confidence": 0.8382317083222526}]}, {"text": "For the Polarity and OP data we discarded all features that have frequency < 5, and for the MPQA data we discarded features that have frequency < 2 because this data set is substantially smaller.", "labels": [], "entities": [{"text": "MPQA data", "start_pos": 92, "end_pos": 101, "type": "DATASET", "confidence": 0.9736915528774261}]}, {"text": "All of our experimental results are averages over 3-fold cross-validation.", "labels": [], "entities": []}, {"text": "First, we created 4 baseline classifiers: a 1Gram classifier that uses only the unigram features; a 1+2Gram classifier that uses unigram and bigram features; a 1+EP classifier that uses unigram and extraction pattern features, and a 1+2+EP classifier that uses all three types of features.", "labels": [], "entities": []}, {"text": "Next, we created analogous 1+2Gram, 1+EP, and 1+2+EP classifiers but applied the subsumption hierarchy first to eliminate unnecessary features before training the classifier.", "labels": [], "entities": []}, {"text": "We experimented with three delta values for the subsumption process: \u03b4=.0005, .001, and .002.", "labels": [], "entities": [{"text": "\u03b4", "start_pos": 69, "end_pos": 70, "type": "METRIC", "confidence": 0.9553817510604858}]}, {"text": "The subsumption process produced small but consistent improvements on all 3 data sets.", "labels": [], "entities": []}, {"text": "For example, shows the results on the OP data, where all of the accuracy values produced after subsumption (the rightmost 3 columns) are higher than the accuracy values produced without subsumption (the Base[line] column).", "labels": [], "entities": [{"text": "OP data", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9053086340427399}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9990348815917969}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9968112111091614}]}, {"text": "For all three data sets, the best overall accuracy (shown in boldface) was always achieved after subsumption.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9898540377616882}]}, {"text": "We also observed that subsumption had a dramatic effect on the F-measure scores on the OP data, which are shown in.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9896384477615356}, {"text": "OP data", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.7694953382015228}]}, {"text": "The OP data set is fundamentally different from the other data sets because it is so highly skewed, with 91% of the documents belonging to the non-opinion class.", "labels": [], "entities": [{"text": "OP data set", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8209591110547384}]}, {"text": "Without subsumption, the classifier was conservative about assigning documents to the opinion class, achieving F-measure scores in the 82-88 range.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9966199398040771}]}, {"text": "After subsumption, the overall accuracy improved but the F-measure scores increased more dramatically.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995465874671936}, {"text": "F-measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9916400909423828}]}, {"text": "These numbers show that the subsumption process produced not only a more accurate classifier, but a more useful classifier that identifies more documents as being opinion articles.", "labels": [], "entities": []}, {"text": "We conducted a second series of experiments to determine whether a traditional feature selection approach would produce the same, or better, improvements as subsumption.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.706773117184639}]}, {"text": "For each feature, we computed its information gain (IG) and then selected the N features with the highest scores.", "labels": [], "entities": [{"text": "information gain (IG)", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.9279433012008667}]}, {"text": "We experimented with values of N ranging from 1,000 to 10,000 in increments of 1,000.", "labels": [], "entities": []}, {"text": "We hypothesized that applying subsumption before traditional feature selection might also help to identify a more diverse set of high-performing features.", "labels": [], "entities": []}, {"text": "Ina parallel set of experiments, we explored this hypothesis by first applying subsumption to reduce the size of the feature set, and then selecting the best N features using information gain.", "labels": [], "entities": []}, {"text": "show the results of these experiments for the 1+2+EP classifiers.", "labels": [], "entities": []}, {"text": "Each graph shows four lines.", "labels": [], "entities": []}, {"text": "One line corresponds to the baseline classifier with no subsumption, and another line corresponds to the baseline classifier with subsumption using the best \u03b4 value for that data set.", "labels": [], "entities": []}, {"text": "Each of these two lines corresponds to In the case of ties, we included all features with the same score as the Nth-best as well.", "labels": [], "entities": []}, {"text": "On all 3 data sets, traditional feature selection performs worse than the baseline in some cases, and it virtually never outperforms the best classifier trained after subsumption (but without feature selection).", "labels": [], "entities": []}, {"text": "Furthermore, the combination of subsumption plus feature selection generally performs best of all, and nearly always outperforms feature selection alone.", "labels": [], "entities": []}, {"text": "For all 3 data sets, our best accuracy results were achieved by performing subsumption prior to feature selection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992315769195557}]}, {"text": "The best accuracy results are 99.0% on the OP data, 83.1% on the Polarity data, and 75.4% on the MPQA data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995551705360413}, {"text": "OP data", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8272908926010132}, {"text": "Polarity data", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.7859114408493042}, {"text": "MPQA data", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.9845142066478729}]}, {"text": "For the OP data, the improvement over baseline for both accuracy and F-measure are statistically significant at the p < 0.05 level (paired t-test).", "labels": [], "entities": [{"text": "OP data", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.7470952570438385}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9994708895683289}, {"text": "F-measure", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.997632622718811}]}, {"text": "For the MPQA data, the improvement over baseline is statistically significant at the p < 0.10 level.", "labels": [], "entities": [{"text": "MPQA data", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.96453458070755}]}], "tableCaptions": []}