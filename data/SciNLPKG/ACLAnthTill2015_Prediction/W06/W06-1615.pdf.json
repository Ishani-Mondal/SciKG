{"title": [{"text": "Domain Adaptation with Structural Correspondence Learning", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7458378076553345}]}], "abstractContent": [{"text": "Discriminative learning methods are widely used in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6441641847292582}]}, {"text": "These methods work best when their training and test data are drawn from the same distribution.", "labels": [], "entities": []}, {"text": "For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent.", "labels": [], "entities": []}, {"text": "In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain.", "labels": [], "entities": []}, {"text": "We introduce structural correspondence learning to automatically induce correspondences among features from different domains.", "labels": [], "entities": [{"text": "structural correspondence learning", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.6772889097531637}]}, {"text": "We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7343956083059311}, {"text": "target domain parsing", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.666018029054006}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.8735498785972595}]}], "introductionContent": [{"text": "Discriminative learning methods are ubiquitous in natural language processing.", "labels": [], "entities": []}, {"text": "Discriminative taggers and chunkers have been the state-of-the-art for more than a decade.", "labels": [], "entities": [{"text": "Discriminative taggers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7805390954017639}]}, {"text": "Furthermore, end-to-end systems like speech recognizers) and automatic translators use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7075138688087463}]}, {"text": "However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data.", "labels": [], "entities": []}, {"text": "In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain).", "labels": [], "entities": []}, {"text": "This work focuses on using unlabeled data from both the source and target domains to learn a common feature representation that is meaningful across both domains.", "labels": [], "entities": []}, {"text": "We hypothesize that a discriminative model trained in the source domain using this common feature representation will generalize better to the target domain.", "labels": [], "entities": []}, {"text": "This representation is learned using a method we call structural correspondence learning (SCL).", "labels": [], "entities": [{"text": "structural correspondence learning (SCL)", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.7465835412343343}]}, {"text": "The key idea of SCL is to identify correspondences among features from different domains by modeling their correlations with pivot features.", "labels": [], "entities": [{"text": "SCL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9731556177139282}]}, {"text": "Pivot features are features which behave in the same way for discriminative learning in both domains.", "labels": [], "entities": []}, {"text": "Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner.", "labels": [], "entities": []}, {"text": "Even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in a compact way.", "labels": [], "entities": []}, {"text": "There are many choices for modeling co-occurrence data.", "labels": [], "entities": []}, {"text": "In this work we choose to use the technique of structural learning (.", "labels": [], "entities": []}, {"text": "Structural learning models the correlations which are most useful for semi-supervised learning.", "labels": [], "entities": []}, {"text": "We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9591672122478485}]}, {"text": "SCL is a general technique, which one can apply to feature based classifiers for any task.", "labels": [], "entities": []}, {"text": "Here, (a) Wall interfere with normal signal transduction . Figure 1: Part of speech-tagged sentences from both corpora we investigate its use in part of speech (PoS) tagging.", "labels": [], "entities": [{"text": "part of speech (PoS) tagging", "start_pos": 145, "end_pos": 173, "type": "TASK", "confidence": 0.7389662052903857}]}, {"text": "While PoS tagging has been heavily studied, many domains lack appropriate training corpora for PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.7574443519115448}, {"text": "PoS tagging", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.8568284511566162}]}, {"text": "Nevertheless, PoS tagging is an important stage in pipelined language processing systems, from information extractors to speech synthesizers.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.8472297191619873}]}, {"text": "We show how to use SCL to transfer a PoS tagger from the Wall Street Journal (financial news) to MEDLINE (biomedical abstracts), which use very different vocabularies, and we demonstrate not only improved PoS accuracy but also improved end-to-end parsing accuracy while using the improved tagger.", "labels": [], "entities": [{"text": "Wall Street Journal (financial news)", "start_pos": 57, "end_pos": 93, "type": "DATASET", "confidence": 0.7644017849649701}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9408737421035767}, {"text": "accuracy", "start_pos": 255, "end_pos": 263, "type": "METRIC", "confidence": 0.7781732678413391}]}, {"text": "An important but rarely-explored setting in domain adaptation is when we have no labeled training data for the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7777184545993805}]}, {"text": "We first demonstrate that in this situation SCL significantly improves performance over both supervised and semi-supervised taggers.", "labels": [], "entities": [{"text": "SCL", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9771584272384644}]}, {"text": "In the case when some in-domain labeled training data is available, we show how to use SCL together with the classifier combination techniques of to achieve even greater performance.", "labels": [], "entities": []}, {"text": "In the next section, we describe a motivating example involving financial news and biomedical data.", "labels": [], "entities": []}, {"text": "Section 3 describes the structural correspondence learning algorithm.", "labels": [], "entities": [{"text": "structural correspondence learning", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.6447583536307017}]}, {"text": "Sections 6 and 7 report results on adapting from the Wall Street Journal to MEDLINE.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.7904027899106344}, {"text": "MEDLINE", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.6090226769447327}]}, {"text": "We discuss related work on domain adaptation in section 8 and conclude in section 9.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8184090554714203}]}, {"text": "shows two PoS-tagged sentences, one each from the Wall Street Journal (hereafter WSJ) and MEDLINE.", "labels": [], "entities": [{"text": "Wall Street Journal (hereafter WSJ)", "start_pos": 50, "end_pos": 85, "type": "DATASET", "confidence": 0.8897545082228524}, {"text": "MEDLINE", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.5096285939216614}]}, {"text": "We chose these sentences for two reasons.", "labels": [], "entities": []}, {"text": "First, we wish to visually emphasize the difference between the two domains.", "labels": [], "entities": []}, {"text": "The vocabularies differ significantly, and PoS taggers suffer accordingly.", "labels": [], "entities": [{"text": "PoS taggers", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.8791261613368988}]}, {"text": "Second, we want to focus on the phrase \"with normal signal transduction\" from the MEDLINE sentence, depicted in(a).", "labels": [], "entities": [{"text": "MEDLINE sentence", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.7482310235500336}]}, {"text": "The word \"signal\" in this sentence is a noun, but a tagger trained on the WSJ incorrectly classifies it as an adjective.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9418408274650574}]}, {"text": "We introduce the notion of pivot features.", "labels": [], "entities": []}, {"text": "Pivot features are features which occur frequently in the two domains and behave similarly in both.(b) shows some pivot features that occur together with the word \"signal\" in our biomedical unlabeled data.", "labels": [], "entities": []}, {"text": "In this case our pivot features are all of type <the token on the right>.", "labels": [], "entities": []}, {"text": "Note that \"signal\" is unambiguously a noun in these contexts.", "labels": [], "entities": []}, {"text": "Adjectives rarely precede past tense verbs such as \"required\" or prepositions such as \"from\" and \"for\".", "labels": [], "entities": []}, {"text": "We now search for occurrences of the pivot features in the WSJ.(c) shows some words that occur together with the pivot features in the WSJ unlabeled data.", "labels": [], "entities": [{"text": "WSJ.", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9542059898376465}, {"text": "WSJ unlabeled data", "start_pos": 135, "end_pos": 153, "type": "DATASET", "confidence": 0.9054278333981832}]}, {"text": "Note that \"investment\", \"buy-outs\", and \"jail\" are all common nouns in the financial domain.", "labels": [], "entities": []}, {"text": "Furthermore, since we have labeled WSJ data, we expect to be able to label at least some of these nouns correctly.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.8091667592525482}]}], "datasetContent": [], "tableCaptions": []}