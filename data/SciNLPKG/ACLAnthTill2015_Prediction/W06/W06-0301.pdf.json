{"title": [{"text": "Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text", "labels": [], "entities": [{"text": "Extracting Opinions, Opinion Holders", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6981853723526001}]}], "abstractContent": [{"text": "This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts.", "labels": [], "entities": []}, {"text": "We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.", "labels": [], "entities": []}, {"text": "This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6606869399547577}, {"text": "FrameNet", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.9330558776855469}]}, {"text": "We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.", "labels": [], "entities": []}, {"text": "For a broader coverage, we also employ a clustering technique to predict the most probable frame fora word which is not defined in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.9358581900596619}]}, {"text": "Our experimental results show that our system performs significantly better than the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "The challenge of automatically identifying opinions in text automatically has been the focus of attention in recent years in many different domains such as news articles and product reviews.", "labels": [], "entities": [{"text": "automatically identifying opinions in text automatically", "start_pos": 17, "end_pos": 73, "type": "TASK", "confidence": 0.727659652630488}]}, {"text": "Various approaches have been adopted in subjectivity detection, semantic orientation detection, review classification and review mining.", "labels": [], "entities": [{"text": "subjectivity detection", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7535697221755981}, {"text": "semantic orientation detection", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.8366047938664755}, {"text": "review classification", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.787879467010498}, {"text": "review mining", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.780580461025238}]}, {"text": "Despite the successes in identifying opinion expressions and subjective words/phrases, there has been less achievement on the factors closely related to subjectivity and polarity, such as opinion holder, topic of opinion, and inter-topic/inter-opinion relationships.", "labels": [], "entities": []}, {"text": "This paper addresses the problem of identifying not only opinions in text but also holders and topics of opinions from online news articles.", "labels": [], "entities": []}, {"text": "Identifying opinion holders is important especially in news articles.", "labels": [], "entities": [{"text": "Identifying opinion holders", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9421322147051493}]}, {"text": "Unlike product reviews in which most opinions expressed in a review are likely to be opinions of the author of the review, news articles contain different opinions of different opinion holders (e.g. people, organizations, and countries).", "labels": [], "entities": []}, {"text": "By grouping opinion holders of different stance on diverse social and political issues, we can have a better understanding of the relationships among countries or among organizations.", "labels": [], "entities": []}, {"text": "An opinion topic can be considered as an object an opinion is about.", "labels": [], "entities": []}, {"text": "In product reviews, for example, opinion topics are often the product itself or its specific features, such as design and quality (e.g. \"I like the design of iPod video\", \"The sound quality is amazing\").", "labels": [], "entities": []}, {"text": "In news articles, opinion topics can be social issues, government's acts, new events, or someone's opinions.", "labels": [], "entities": []}, {"text": "(e.g., \"Democrats in Congress accused vice president Dick Cheney's shooting accident.\", \"Shiite leaders accused Sunnis of amass killing of Shiites in Madaen, south of Baghdad.\")", "labels": [], "entities": []}, {"text": "As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews.", "labels": [], "entities": [{"text": "opinion topic identification", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.6735647618770599}]}, {"text": "In most approaches in product review mining, given a product (e.g. mp3 player), its frequently mentioned features (e.g. sound, screen, and design) are first collected and then used as anchor points.", "labels": [], "entities": [{"text": "product review mining", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7308962345123291}]}, {"text": "In this study, we extract opinion topics from news articles.", "labels": [], "entities": []}, {"text": "Also, we do not pre-limit topics in advance.", "labels": [], "entities": []}, {"text": "We first identify an opinion and then find its holder and topic.", "labels": [], "entities": []}, {"text": "We define holder as an entity who holds an opinion, and topic, as what the opinion is about.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method that employs Semantic Role Labeling, a task of identifying semantic roles given a sentence.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7748986482620239}]}, {"text": "We de-compose the overall task into the following steps: \u2022 Identify opinions.", "labels": [], "entities": []}, {"text": "\u2022 Label semantic roles related to the opinions.", "labels": [], "entities": []}, {"text": "\u2022 Find holders and topics of opinions among the identified semantic roles.", "labels": [], "entities": []}, {"text": "\u2022 Store <opinion, holder, topic> triples into a database.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the first three subtasks.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is to present a method that identifies not only opinion holders but also opinion topics.", "labels": [], "entities": []}, {"text": "To achieve this goal, we utilize FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics, and then use them for system training.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8299995958805084}]}, {"text": "We demonstrate that investigating semantic relations between an opinion and its holder and topic is crucial in opinion holder and topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7356268912553787}]}, {"text": "This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9704645276069641}, {"text": "semantic role labeling", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7205942074457804}]}, {"text": "Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet 1 data for our task.", "labels": [], "entities": [{"text": "FrameNet 1 data", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.8571372230847677}]}, {"text": "Section 4 reports our experiments and results with discussions and finally Section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiment is first, to see how our holder and topic labeling system works on the FrameNet data, and second, to examine how it performs on online news media text.", "labels": [], "entities": [{"text": "holder and topic labeling", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.5871883481740952}, {"text": "FrameNet data", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9503351747989655}]}, {"text": "The first data set (Testset 1) consists of 10% of data described in Subsection 3.2 and the second (Testset 2) is manually annotated by 2 humans.", "labels": [], "entities": []}, {"text": "(see Subsection 4.2).", "labels": [], "entities": []}, {"text": "We report experimental results for both test sets.", "labels": [], "entities": []}, {"text": "Gold Standard: In total, Testset 1 contains 2028 annotated sentences collected from FrameNet data set.", "labels": [], "entities": [{"text": "FrameNet data set", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.9715510010719299}]}, {"text": "(834 from frames related to opinion verb and 1194 from opinion adjectives) We measure the system performance using precision (the percentage of correct holders/topics among system's labeling results), recall (the percentage of correct holders/topics that system retrieved), and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9993540644645691}, {"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9993627667427063}, {"text": "F-score", "start_pos": 278, "end_pos": 285, "type": "METRIC", "confidence": 0.9988479614257812}]}, {"text": "Baseline: For the baseline system, we applied two different algorithms for sentences which have opinion-bearing verbs as target words and for those that have opinion-bearing adjectives as target words.", "labels": [], "entities": []}, {"text": "For verbs, baseline system labeled a subject of a verb as a holder and an object as a topic.", "labels": [], "entities": []}, {"text": "(e.g. \"[ holder He] condemned [ topic the lawyer].\")", "labels": [], "entities": []}, {"text": "For adjectives, the baseline marked the subject of a predicate adjective as a holder (e.g. \" holder I] was happy\").", "labels": [], "entities": []}, {"text": "For the topics of adjectives, the baseline picks a modified word if the target adjective is a modifier (e.g. \"That was a stupid [ topic mistake]\".) and a subject word if the adjective is a predicate.", "labels": [], "entities": []}, {"text": "([ topic The view] is breathtaking in January.) show evaluation results of our system and the baseline system respectively.", "labels": [], "entities": []}, {"text": "Our system performed much better than the baseline system in identifying topic and holder for both sets of sentences with verb target words and those with adjectives.", "labels": [], "entities": []}, {"text": "Especially in recognizing topics of target opinion-bearing words, our system improved F-score from 30.4% to 66.5% for verb target words and from 38.2% to 70.3% for adjectives.", "labels": [], "entities": [{"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9996089339256287}]}, {"text": "It was interesting to see that the intuition that \"A subject of opinionbearing verb is a holder and an object is a topic\" which we applied for the baseline achieved relatively good F-score (56.9%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9997168183326721}]}, {"text": "However, our system obtained much higher F-score (78.7%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9996888637542725}]}, {"text": "Holder identification task achieved higher Fscore than topic identification which implies that identifying topics of opinion is a harder task.", "labels": [], "entities": [{"text": "Holder identification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7147023379802704}, {"text": "Fscore", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.999669075012207}, {"text": "topic identification", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.8498494625091553}]}, {"text": "Gold Standard: Two humans 8 annotated 100 sentences randomly selected from news media texts.", "labels": [], "entities": [{"text": "Gold Standard", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.5333425402641296}]}, {"text": "Those news data is collected from online news sources such as The New York Times, UN Office for the Coordination of Humanitarian Affairs, and BBC News 9 , which contain articles about various international affaires.", "labels": [], "entities": [{"text": "BBC News 9", "start_pos": 142, "end_pos": 152, "type": "DATASET", "confidence": 0.97179114818573}]}, {"text": "Annotators identified opinion-bearing sentences with marking opinion word with its holder and topic if they existed.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement in identifying opinion sentences was 82%.", "labels": [], "entities": []}, {"text": "Baseline: In order to identify opinion-bearing sentences for our baseline system, we used the opinion-bearing word set introduced in Phase 1 in Subsection 3.1.", "labels": [], "entities": []}, {"text": "If a sentence contains an opinion-bearing verb or adjective, the baseline system started looking for its holder and topic.", "labels": [], "entities": []}, {"text": "For holder and topic identification, we applied the We refer them as Human1 and Human2 for the rest of this paper.", "labels": [], "entities": [{"text": "holder and topic identification", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.617706187069416}]}, {"text": "9 www.nytimes.com, www.irinnews.org, and www.bbc.co.uk same baseline algorithm as described in Subsection 4.1 to Testset 2.", "labels": [], "entities": []}, {"text": "Result: Note that Testset 1 was collected from sentences of opinion-related frames in FrameNet and therefore all sentences in the set contained either opinion-bearing verb or adjective.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.928146243095398}]}, {"text": "(i.e. All sentences are opinion-bearing) However, sentences in Testset 2 were randomly collected from online news media pages and therefore not all of them are opinion-bearing.", "labels": [], "entities": []}, {"text": "We first evaluated the task of opinion-bearing sentence identification.", "labels": [], "entities": [{"text": "opinion-bearing sentence identification", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.6393682161966959}]}, {"text": "When we mark all sentences as opinion-bearing, it achieved 43% and 38% of accuracy for the annotation result of Human1 and Human2 respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995194673538208}, {"text": "Human1", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.9515243172645569}]}, {"text": "Our system performance (64% and 55%) is comparable with the unique assignment.", "labels": [], "entities": []}, {"text": "We measured the holder and topic identification system with precision, recall, and F-score.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.680308997631073}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9996658563613892}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9997252821922302}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9991605281829834}]}, {"text": "As we can see from, our system achieved much higher precision than the baseline system for both Topic and Holder identification tasks.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9993869066238403}, {"text": "Holder identification tasks", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.7704592049121857}]}, {"text": "However, we admit that there is still a lot of room for improvement.", "labels": [], "entities": []}, {"text": "The system achieved higher precision for topic identification, whereas it achieved higher recall for holder identification.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9994276165962219}, {"text": "topic identification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.9119458794593811}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9992747902870178}, {"text": "holder identification", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.8585341274738312}]}, {"text": "In overall, our system attained higher F-score in holder identification task, including the baseline system.", "labels": [], "entities": [{"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9991119503974915}, {"text": "holder identification task", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.8974818189938863}]}, {"text": "Based on Fscore, we believe that identifying topics of opinion is much more difficult than identifying holders.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.6391726732254028}, {"text": "identifying topics of opinion", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.8484065234661102}]}, {"text": "It was interesting to seethe same phenomenon that the baseline system mainly assuming that subject and object of a sentence are likely to be opinion holder and topic, achieved lower scores for both holder and topic identification tasks in Testset 2 as in Testset 1.", "labels": [], "entities": []}, {"text": "This implies that more sophisticated analysis of the relationship between opinion words (e.g. verbs and adjectives) and their topics and holders is crucial.", "labels": [], "entities": []}, {"text": "We observed several difficulties in evaluating holder and topic identification.", "labels": [], "entities": [{"text": "holder and topic identification", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.8545887023210526}]}, {"text": "First, the boundary of an entity of holder or topic can be flexible.", "labels": [], "entities": []}, {"text": "For example, in sentence \"Senator Titus Olupitan who sponsored the bill wants the permission.\", not only \"Senator Titus Olupitan\" but also \"Senator Titus Olupitan who sponsored the bill\" is an eligible answer.", "labels": [], "entities": []}, {"text": "Second, some correct holders and topics which our system found were evaluated wrong even if they referred the same entities in the gold standard because human annotators marked only one of them as an answer.", "labels": [], "entities": []}, {"text": "In the future, we need more annotated data for improved evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Precision (P), Recall (R), and F- score (F) of Topic and Holder identification  for opinion verbs (V) and adjectives (A) on  Testset 1.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9409061968326569}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9519103318452835}, {"text": "F- score (F)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9844564100106558}, {"text": "Topic and Holder identification", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.5033082589507103}]}, {"text": " Table 4. Baseline system on Testset 1.", "labels": [], "entities": [{"text": "Testset 1", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.9639725387096405}]}, {"text": " Table 5. Opinion-bearing sentence identifica- tion on Testset 2. (P: precision, R: recall, F:  F-score, A: Accuracy, H1: Human1, H2:  Human2)", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9366952180862427}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8945919871330261}, {"text": "F-score", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.8466874957084656}, {"text": "Accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.5927266478538513}]}, {"text": " Table 6: Results of Topic and Holder identi- fication on Testset 2. (Sys: our system, BL:  baseline)", "labels": [], "entities": [{"text": "BL", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9455059170722961}]}]}