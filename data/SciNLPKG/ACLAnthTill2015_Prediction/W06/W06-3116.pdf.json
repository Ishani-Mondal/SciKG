{"title": [{"text": "Mood at work: Ramses versus Pharaoh", "labels": [], "entities": []}], "abstractContent": [{"text": "We present here the translation system we used in this year's WMT shared task.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9709572792053223}, {"text": "WMT shared task", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.5147435168425242}]}, {"text": "The main objective of our participation was to test RAMSES, an open source phrase-based decoder.", "labels": [], "entities": [{"text": "RAMSES", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.4654769003391266}]}, {"text": "For that purpose, we used the baseline system made available by the organizers of the shared task 1 to build the necessary models.", "labels": [], "entities": []}, {"text": "We then carried out a pair-to-pair comparison of RAMSES with PHARAOH on the six different translation directions that we were asked to perform.", "labels": [], "entities": [{"text": "RAMSES", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9524493217468262}, {"text": "PHARAOH", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9128462672233582}]}, {"text": "We present this comparison in this paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "Phrase-based (PB) machine translation (MT) is now a popular paradigm, partly because of the relative ease with which we can automatically create an acceptable translation engine from a bitext.", "labels": [], "entities": [{"text": "Phrase-based (PB) machine translation (MT)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6754346059428321}]}, {"text": "As a matter of fact, deriving such an engine from a bitext consists in (more or less) gluing together dedicated software modules, often freely available.", "labels": [], "entities": []}, {"text": "Word-based models, or the so-called IBM models, can be trained using the GIZA or GIZA++ toolkits).", "labels": [], "entities": []}, {"text": "One can then train phrase-based models using the THOT toolkit).", "labels": [], "entities": [{"text": "THOT toolkit", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.7557036876678467}]}, {"text": "For their part, language models currently in use in SMT systems can be trained using packages such as SRILM) and the CMU-SLM toolkit).", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9944406151771545}, {"text": "SRILM", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.5047860145568848}]}, {"text": "Once all the models are built, one can choose to use PHARAOH, an efficient fullfledged phrase-based decoder.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.8955482244491577}]}, {"text": "We only know of one major drawback when using PHARAOH: its licensing policy.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.7406484484672546}]}, {"text": "Indeed, it is available for noncommercial use in its binary form only.", "labels": [], "entities": []}, {"text": "This severely limits its use, both commercially and scientifically (.", "labels": [], "entities": []}, {"text": "For this reason, we undertook the design of a generic architecture called MOOD (Modular ObjectOriented Decoder), especially suited for instantiating SMT decoders.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 149, "end_pos": 161, "type": "TASK", "confidence": 0.8742741048336029}]}, {"text": "Two major goals directed our design of this package: offering open source, stateof-the-art decoders and providing an architecture to easily build these decoders.", "labels": [], "entities": []}, {"text": "This effort is described in).", "labels": [], "entities": []}, {"text": "As a proof of concept that our framework (MOOD) is viable, we attempted to use its functionalities to implement a clone of PHARAOH, based on the comprehensive user manual of the latter.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.825079083442688}]}, {"text": "This clone, called RAMSES, is now part of the MOOD distribution, which can be downloaded freely from the page http://smtmood.sourceforge.net.", "labels": [], "entities": [{"text": "RAMSES", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.7555271983146667}, {"text": "MOOD distribution", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9428736567497253}]}, {"text": "We conducted a pair-to-pair comparison between the two engines that we describe in this paper.", "labels": [], "entities": []}, {"text": "We provide an overview of the MOOD architecture in Section 2.", "labels": [], "entities": [{"text": "MOOD", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9062580466270447}]}, {"text": "Then we describe briefly RAMSES in Section 3.", "labels": [], "entities": [{"text": "RAMSES", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.7672801613807678}]}, {"text": "The comparison between the two decoders in terms of automatic metrics is analyzed in Section 4.", "labels": [], "entities": []}, {"text": "We confirm this comparison by presenting a manual evaluation we conducted on an random sample of the translations produced by both decoders.", "labels": [], "entities": []}, {"text": "This is reported in Section 5.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In an effort to correlate the objective metrics with human reviews, we undertook the blind evaluation of a sample of 100 pairwise translations for the three Foreign language-to-English translation tasks.", "labels": [], "entities": [{"text": "Foreign language-to-English translation tasks", "start_pos": 157, "end_pos": 202, "type": "TASK", "confidence": 0.6928683817386627}]}, {"text": "The pairs were randomly selected from the 3064 translations produced by each engine.", "labels": [], "entities": []}, {"text": "They had to be different for each decoder and be no more than 25 words long.", "labels": [], "entities": []}, {"text": "Each evaluator was presented with a source sentence, its reference translation and the translation produced by each decoder.", "labels": [], "entities": []}, {"text": "The last two were in random order, so the evaluator did not know which engine produced the translation.", "labels": [], "entities": []}, {"text": "The evaluator's task was two-fold.", "labels": [], "entities": []}, {"text": "(1) He decided whether one translation was better than the other.", "labels": [], "entities": []}, {"text": "in test (1), he stated whether the best translation was satisfactory while the other was not.", "labels": [], "entities": []}, {"text": "Two evaluators went through the 3 \u00d7 100 sentence pairs.", "labels": [], "entities": []}, {"text": "None of them understands German; subject B understands Spanish, and both understand French and English.", "labels": [], "entities": []}, {"text": "The results of this informal, yet informative exercise are reported in.", "labels": [], "entities": []}, {"text": "Overall, in many cases (64% and 48% for subject A and B respectively), the evaluators did not prefer one translation over the other.", "labels": [], "entities": []}, {"text": "On the Spanishand French-to-English tasks, both subjects slightly preferred the translations produced by RAMSES.", "labels": [], "entities": [{"text": "RAMSES", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.8122435808181763}]}, {"text": "In about one fourth of the cases where one translation was preferred did the evaluators actually flag the selected translation as significantly better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of RAMSES and PHARAOH  on the provided test set of 2000 pairs of sentences  per language pair. P stands for PHARAOH, R for  RAMSES. All scores are percentages. p n is the n- gram precision and BP is the brevity penalty used  when computing BLEU.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.8810254335403442}, {"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.6916720271110535}, {"text": "BP", "start_pos": 215, "end_pos": 217, "type": "METRIC", "confidence": 0.998664379119873}, {"text": "BLEU", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.9939391613006592}]}, {"text": " Table 2: BLEU scores on subsets of the test corpus  filtered by sentence length ([min words, max words]  intervals), for Pharaoh and Ramses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999302864074707}]}, {"text": " Table 3: Human evaluation figures. The column  Preferred indicates the preference of the subject  (Pharaoh, Ramses or No preference). The column  Improved shows when a subject did prefer a trans- lation and also said that the preferred translation was  correct while the other one was not.", "labels": [], "entities": [{"text": "Preferred", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9716657400131226}, {"text": "Improved", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9845222234725952}]}]}