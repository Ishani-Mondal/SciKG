{"title": [{"text": "A Framework for Incorporating Alignment Information in Parsing", "labels": [], "entities": [{"text": "Incorporating Alignment Information", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.8461907704671224}, {"text": "Parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.5537427663803101}]}], "abstractContent": [{"text": "The standard PCFG approach to parsing is quite successful on certain domains, but is relatively inflexible in the type of feature information we can include in its prob-abilistic model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9780193567276001}]}, {"text": "In this work, we discuss preliminary work in developing anew probabilistic parsing model that allows us to easily incorporate many different types of features, including crosslingual information.", "labels": [], "entities": []}, {"text": "We show how this model can be used to build a successful parser fora small handmade gold-standard corpus of 188 sentences (in 3 languages) from the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 148, "end_pos": 163, "type": "DATASET", "confidence": 0.9920593202114105}]}], "introductionContent": [{"text": "Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs)).", "labels": [], "entities": [{"text": "probabilistic parsing", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.6785337626934052}]}, {"text": "For instance, consider the parse tree in.", "labels": [], "entities": []}, {"text": "One way to decompose this parse tree is to view it as a sequence of applications of CFG rules.", "labels": [], "entities": []}, {"text": "For this particular tree, we could view it as the application of rule \"NP \u2192 NP PP,\" followed by rule \"NP \u2192 DT NN,\" followed by rule \"DT \u2192 that,\" and so forth.", "labels": [], "entities": []}, {"text": "Hence instead of analyzing P (tree), we deal with the more modular: Obviously this joint distribution is just as difficult to assess and compute with as P (tree).", "labels": [], "entities": []}, {"text": "However there exist cubic time algorithms to find the most likely parse if we assume that all CFG rule applications are marginally independent of one another.", "labels": [], "entities": []}, {"text": "In other words, we need to assume that the above expression is equivalent to the following: It is straightforward to assess the probability of the factors of this expression from a corpus using relative frequency.", "labels": [], "entities": []}, {"text": "Then using these learned probabilities, we can find the most likely parse of a given sentence using the aforementioned cubic algorithms.", "labels": [], "entities": []}, {"text": "The problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption.", "labels": [], "entities": []}, {"text": "To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information.", "labels": [], "entities": []}, {"text": "For instance, it has been found to be useful to annotate nodes with their parent labels, as shown in.", "labels": [], "entities": []}, {"text": "In this case, we would be learning probabilities like: P(PP-NP \u2192 IN-PP NP-PP).", "labels": [], "entities": []}, {"text": "The choice of which annotations to use is one of the main features that distinguish parsers based on this approach.", "labels": [], "entities": []}, {"text": "Generally, this approach has proven quite effective in producing English phrase-structure grammar parsers that perform well on the Penn Treebank.", "labels": [], "entities": [{"text": "phrase-structure grammar parsers", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6725895901521047}, {"text": "Penn Treebank", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.995805025100708}]}, {"text": "One drawback of this approach is that it is somewhat inflexible.", "labels": [], "entities": []}, {"text": "Because we are adding probabilistic context by changing the data itself, we make our data increasingly sparse as we add features.", "labels": [], "entities": []}, {"text": "Thus we are constrained from adding too many features, because at some point we will not have enough data to sustain them.", "labels": [], "entities": []}, {"text": "Hence in this approach, feature selection is not merely a matter of including good features.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7923930883407593}]}, {"text": "Rather, we must strike a delicate balance between how much context we want to include versus how much we dare to partition our data set.", "labels": [], "entities": []}, {"text": "This poses a problem when we have spent time and energy to find a good set of features that work well fora given parsing task on a given domain.", "labels": [], "entities": []}, {"text": "For a different parsing task or domain, our parser may work poorly out-of-the-box, and it is no trivial matter to evaluate how we might adapt our feature set for this new task.", "labels": [], "entities": [{"text": "parsing task or domain", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8647950291633606}]}, {"text": "Furthermore, if we gain access to anew source of feature information, then it is unclear how to incorporate such information into such a parser.", "labels": [], "entities": []}, {"text": "Namely, in this paper, we are interested in seeing how the cross-lingual information contained by sentence alignments can help the performance of a parser.", "labels": [], "entities": []}, {"text": "We have a small gold-standard corpus of shallow-parsed parallel sentences (in English, French, and German) from the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.988595187664032}]}, {"text": "Because of the difficulty of testing new features using PCFG-based parsers, we propose anew probabilistic parsing framework that allows us to flexibly add features.", "labels": [], "entities": []}, {"text": "----true of our framework is the maximum-entropy parser of Ratnaparkhi(Ratnaparkhi, 1997).", "labels": [], "entities": [{"text": "Ratnaparkhi(Ratnaparkhi, 1997)", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.8104272683461508}]}, {"text": "Both frameworks are bottom-up, but while Ratnaparkhi's views parse trees as the sequence of applications of four different types of tree construction rules, our framework strives to be somewhat simpler and more general.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our parsing domain is based on a \"lean\" phrase correspondence representation for multitexts from parallel corpora (i.e., tuples of sentences that are translations of each other).", "labels": [], "entities": []}, {"text": "We defined an annotation scheme that focuses on translational correspondence of phrasal units that have a distinct, language-independent semantic status.", "labels": [], "entities": [{"text": "translational correspondence of phrasal units", "start_pos": 48, "end_pos": 93, "type": "TASK", "confidence": 0.8842729091644287}]}, {"text": "It is a hypothesis of our longer-term project that such a semantically motivated, relatively coarse phrase correspondence relation is most suitable for weakly supervised approaches to parsing of large amounts of parallel corpus data.", "labels": [], "entities": [{"text": "parsing of large amounts of parallel corpus data", "start_pos": 184, "end_pos": 232, "type": "TASK", "confidence": 0.8570010736584663}]}, {"text": "Based on this lean phrase structure format, we intend to explore an alternative to the annotation projection approach to cross-linguistic bootstrapping of parsers by).", "labels": [], "entities": []}, {"text": "They depart from a standard treebank parser for English, \"projecting\" its analyses to another language using word alignments over a parallel corpus.", "labels": [], "entities": []}, {"text": "Our planned bootstrapping approach will not start outwith a given parser for English (or any other language), but use a small set of manually annotated seed data following the lean phrase correspondence scheme, and then bootstrap consensus representations on large amounts of unannotated multitext data.", "labels": [], "entities": []}, {"text": "At the present stage, we only present experiments for training an initial system on a set of seed data.", "labels": [], "entities": []}, {"text": "The annotation scheme underlying in the gold standard annotation consists of (A) a bracketing for each language and (B) a correspondence relation of the constituents across languages.", "labels": [], "entities": []}, {"text": "Neither the constituents nor the embedding or correspondent relations were labelled.", "labels": [], "entities": []}, {"text": "The guiding principle for bracketing (A) is very simple: all and only the units that clearly play the role of a semantic argument or modifier in a larger unit are bracketed.", "labels": [], "entities": [{"text": "bracketing (A)", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8882264345884323}]}, {"text": "This means that function words, light verbs, \"bleeched\" PPs like in spite of etc. are included with the content-bearing elements.", "labels": [], "entities": []}, {"text": "This leads to a relatively flat bracketing structure.", "labels": [], "entities": []}, {"text": "Referring or quantified expressions that may include adjectives and possessive NPs or PPs are also bracketed as single constituents (e.g., [ the president of France ]), unless the semantic relations reflected by the internal embedding are part of the predication of the sentence.", "labels": [], "entities": []}, {"text": "A few more specific annotation rules were specified for cases like coordination and discontinuous constituents.", "labels": [], "entities": []}, {"text": "The correspondence relation (B) is guided by semantic correspondence of the bracketed units; the mapping need not preserve the tree structure.", "labels": [], "entities": []}, {"text": "Neither does a constituent need to have a correspondent in all (or any) of the other languages (since the content of this constituent maybe implicit in other languages, or subsumed by the content of another constituent).", "labels": [], "entities": []}, {"text": "\"Semantic correspondence\" is not restricted to truth-conditional equivalence, but is generalized to situations where two units just serve the same rhetorical function in the original text and the translation. is an annotation example.", "labels": [], "entities": [{"text": "Semantic correspondence\"", "start_pos": 1, "end_pos": 25, "type": "TASK", "confidence": 0.7939787109692892}]}, {"text": "Note that index 4 (the audience addressed by the speaker) is realized overtly only in German (Sie 'you'); in Spanish, index 3 is realized only in the verbal inflection (which is not annotated).", "labels": [], "entities": []}, {"text": "A more detailed discussion of the annotation scheme is presented in (Kuhn and Jellinghaus, to appear).", "labels": [], "entities": []}, {"text": "For the current parsing experiments, only the bracketing within each of three languages (English, French, German) is used; the crosslinguistic phrase correspondences are ignored (although we intend to include them in future experiments).", "labels": [], "entities": []}, {"text": "We automatically tagged the training and test data in English, French, and German with Schmid's decision-tree part-of-speech tagger.", "labels": [], "entities": []}, {"text": "The training data were taken from the sentencealigned Europarl corpus and consisted of 188 sentences for each of the three languages, with max- For the word alignments used as learning features, we used GIZA++, relying on the default parameters.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9270126521587372}]}, {"text": "We trained the alignments on the full Europarl corpus for both directions of each language pair.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9906076192855835}]}, {"text": "As a baseline system we trained Bikel's reimplementation of) on the gold standard (En-1 A subset of 39 sentences was annotated by two people independently, leading to an F-Score in bracketing agreement between 84 and 90 for the three languages.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 170, "end_pos": 177, "type": "METRIC", "confidence": 0.9954898953437805}]}, {"text": "Since finding an annotation scheme that works well in the bootstrapping setup is an issue on our research agenda, we postpone a more detailed analysis of the annotation process until it becomes clear that a particular scheme is indeed useful.", "labels": [], "entities": []}, {"text": "glish) training data, applying a simple additional smoothing procedure for the modifier events in order to counteract some obvious data sparseness issues.", "labels": [], "entities": []}, {"text": "Since we were attempting to learn unlabeled trees, in this experiment we only needed to learn the probabilistic model of Section 3 with no labeling schemes.", "labels": [], "entities": []}, {"text": "Hence we need only to learn the probability distribution: In other words, we need to learn the probability that a given span is a tree constituent, given some set of features of the words and preterminal tags of the sentences, as well as the previous span decisions we have made.", "labels": [], "entities": []}, {"text": "The main decision that remains, then, is which feature set to use.", "labels": [], "entities": []}, {"text": "The features we employ are very simple.", "labels": [], "entities": []}, {"text": "Namely, for span (i, j) we consider the preterminal tags of words i \u2212 1, i, j, and j + 1, as well as the French and German preterminal tags of the words to which these English words align.", "labels": [], "entities": []}, {"text": "Finally, we also use the length of the span as a feature.", "labels": [], "entities": [{"text": "length", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9770451784133911}]}, {"text": "The features considered are summarized in.", "labels": [], "entities": []}, {"text": "To learn the conditional probability distribututions, we choose to use maximum entropy models because of their popularity and the availability of software packages.", "labels": [], "entities": []}, {"text": "Specifically, we use the MEGAM package) from USC/ISI.", "labels": [], "entities": [{"text": "MEGAM package", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8233207762241364}, {"text": "USC/ISI", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8540521860122681}]}, {"text": "We did experiments fora number of different feature sets, with and without alignment features.", "labels": [], "entities": []}, {"text": "The results (precision, recall, F-score, and the percentage of sentences with no cross-bracketing) are summarized in.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996476173400879}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9995322227478027}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9990307092666626}]}, {"text": "Note that with a very simple set of features (the previous, first, last, and next preterminal tags of the sequence), our parser performs on par with the Bikel baseline.", "labels": [], "entities": [{"text": "Bikel baseline", "start_pos": 153, "end_pos": 167, "type": "DATASET", "confidence": 0.9014357328414917}]}, {"text": "Adding the length of the sequence as a feature increases the quality of the parser to a statistically significant difference over the baseline.", "labels": [], "entities": []}, {"text": "The crosslingual information provided (which is admittedly naive) does not provide a statistically significant improvement over the vanilla set of features.", "labels": [], "entities": []}, {"text": "The conclusion to be drawn is not that crosslingual information does not help (such a conclusion should not be drawn from the meager set of crosslingual features we have used here for demonstration purposes).", "labels": [], "entities": []}, {"text": "Rather, the take-away point is that such information can be easily incorporated using this framework.", "labels": [], "entities": []}], "tableCaptions": []}