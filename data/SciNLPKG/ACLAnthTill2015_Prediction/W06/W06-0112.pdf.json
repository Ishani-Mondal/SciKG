{"title": [{"text": "A Hybrid Approach to Chinese Base Noun Phrase Chunking", "labels": [], "entities": [{"text": "Chinese Base Noun Phrase Chunking", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.6862948834896088}]}], "abstractContent": [{"text": "In this paper, we propose a hybrid approach to chunking Chinese base noun phrases (base NPs), which combines SVM (Support Vector Machine) model and CRF (Conditional Random Field) model.", "labels": [], "entities": [{"text": "chunking Chinese base noun phrases", "start_pos": 47, "end_pos": 81, "type": "TASK", "confidence": 0.8677008867263794}]}, {"text": "In order to compare the result respectively from two chunkers, we use the discriminative post-processing method, whose measure criterion is the conditional probability generated from the CRF chunker.", "labels": [], "entities": []}, {"text": "With respect to the special structures of Chinese base NP and complete analyses of the first two results, we also customize some appropriate grammar rules to avoid ambiguities and prune errors.", "labels": [], "entities": []}, {"text": "According to our overall experiments, the method achieves a higher accuracy in the final results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9993879795074463}]}], "introductionContent": [{"text": "Chunking means extracting the non-overlapping segments from a stream of data.", "labels": [], "entities": []}, {"text": "These segments are called chunks.", "labels": [], "entities": []}, {"text": "The definition of base noun phrase (base NP) is simple and non-recursive noun phrase which does not contain other noun phrase descendants.", "labels": [], "entities": []}, {"text": "Base NP chunking could be used as a precursor for many elaborate natural language processing tasks, such as information retrieval, name entity extraction and text summarization and soon.", "labels": [], "entities": [{"text": "Base NP chunking", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5740961531798044}, {"text": "information retrieval", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.8158659338951111}, {"text": "name entity extraction", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7352777520815531}, {"text": "text summarization", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7345990836620331}]}, {"text": "Many other problems similar to text processing can also benefit from base NP chunking, for example, finding genes in DNA and phoneme information extraction.", "labels": [], "entities": [{"text": "text processing", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.781411200761795}, {"text": "phoneme information extraction", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.6343478262424469}]}, {"text": "The initial work on base NP chunking is focused on the grammar-based method.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.6169597059488297}]}, {"text": "introduced a transformationbased learning method which considered chunking as a kind of tagging problem.", "labels": [], "entities": []}, {"text": "Their work inspired many others to study the applications of learning methods to noun phrase chunking.) applied a scoring method to select new rules and a naive heuristic for matching rules to evaluate the results' accuracy.", "labels": [], "entities": [{"text": "noun phrase chunking.", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.6690137386322021}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9950602650642395}]}, {"text": "CoNLL-2000 proposed a shared task, which aimed at dividing a text in syntactically correlated parts of words.", "labels": [], "entities": [{"text": "CoNLL-2000", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8846076726913452}]}, {"text": "The eleven systems for the CoNLL-2000 shared task used a wide variety of machine learning methods.", "labels": [], "entities": []}, {"text": "The best system in this workshop is on the basis of Support Vector Machines used by.", "labels": [], "entities": []}, {"text": "Recently, some new statistical techniques, such as CRF () and structural learning methods () have been applied on the base NP chunking.) considered chunking as a sequence labeling task and achieved good performance by an improved training methods of CRF.", "labels": [], "entities": []}, {"text": "() presented a novel semisupervised learning method on chunking and produced performances higher than the previous best results.", "labels": [], "entities": []}, {"text": "The research on Chinese Base NP Chunking is, however, still at its developing stage.", "labels": [], "entities": [{"text": "Chinese Base NP Chunking", "start_pos": 16, "end_pos": 40, "type": "DATASET", "confidence": 0.9733385294675827}]}, {"text": "Researchers apply similar methods of English Base NP chunking to Chinese.", "labels": [], "entities": [{"text": "English Base NP chunking", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.5091581270098686}]}, {"text": "made a strict definition of Chinese base NP and put forward a quasi-dependency model to analysis the structure of Chinese base NPs.", "labels": [], "entities": []}, {"text": "There are some other methods to deal with Chinese phrase (no only base NP) chunking, such as HMM, Maximum Entropy (Zhou, Memory-Based Learning () etc.", "labels": [], "entities": []}, {"text": "However, according to our experiments over 30,000 Chinese words, the best results of Chinese base NP chunking are about 5% less than that of English chunking (Although we should admit the chunking outcomes vary among different sizes of corpus and rely on the details of experiments).", "labels": [], "entities": [{"text": "Chinese base NP chunking", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.5689884275197983}, {"text": "English chunking", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.6951602548360825}]}, {"text": "The differences between Chinese NPs and English NPs are summarized as following points: First, the flexible structure of Chinese noun phrase often results in the ambiguities during the recognition procedure.", "labels": [], "entities": []}, {"text": "For example, many English base NPs begin with the determinative, while the margin of Chinese base NPs is more uncertain.", "labels": [], "entities": [{"text": "margin", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9633474349975586}]}, {"text": "Second, the base NPs begins with more than two noun-modifiers, such as \"\u9ad8 (high)/JJ \u65b0(new)/JJ \u6280\u672f(technology)/NN\", the noun-modifiers \"\u9ad8/JJ \" cannot be completely recognized.", "labels": [], "entities": []}, {"text": "Third, the usage of Chinese word is flexible, as a Chinese word may serve with multi POS (Part-of-Speech) tags.", "labels": [], "entities": []}, {"text": "For example, a noun is used as a verbal or an adjective component in the sentence.", "labels": [], "entities": []}, {"text": "In this way the chunker is puzzled by those multi-used words.", "labels": [], "entities": []}, {"text": "Finally, there are no standard datasets and elevation systems for Chinese base NP chunking as the CoNLL-2000 shared task, which makes it difficult to compare and evaluate different Chinese base NP chunking systems.", "labels": [], "entities": [{"text": "Chinese base NP chunking", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.5127217248082161}, {"text": "CoNLL-2000 shared task", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.8670473098754883}]}, {"text": "In this paper, we propose a hybrid approach to extract the Chinese base NPs with the help of the conditional probabilities derived from the CRF algorithm and some appropriate grammar rules.", "labels": [], "entities": []}, {"text": "According to our preliminary experiments on SVM and CRF, our approach outperforms both of them.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief introduction of the data representations and methods.", "labels": [], "entities": []}, {"text": "We explain our motivations of the hybrid approach in section 3.", "labels": [], "entities": []}, {"text": "The experimental results and conclusions are introduced in section 4 and section 5 respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "The CoNLL 2000 provided the software 4 to convert Penn English Treebank II into the IOB tags form.", "labels": [], "entities": [{"text": "CoNLL 2000", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9702105522155762}, {"text": "Penn English Treebank II", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.9745063781738281}]}, {"text": "We use the Penn Chinese Treebank 5.0 5 , which is improved and involved with more POS tags, segmentation and syntactic bracketing.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5.0 5", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.9853569626808166}, {"text": "syntactic bracketing", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.7167421579360962}]}, {"text": "As the sentences in the Treebank are longer and related to more complicated structures, we modify the software with robust heuristics to cope with those new features of the Chinese Treebank and generate the training and testing data sets from the Treebank.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 173, "end_pos": 189, "type": "DATASET", "confidence": 0.9737023413181305}]}, {"text": "Afterward we also make some manual adjustments to the final data.", "labels": [], "entities": []}, {"text": "In our experiments, the SVM chunker uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and tolerance of the termination criterion, 0.01 In the base NPs chunking task, the evaluation metrics for base NP chunking include precision P, recall Rand the F \u03b2 . Usually we refer to the F \u03b2 as the creditable metric.", "labels": [], "entities": [{"text": "SVM chunker", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.8411834239959717}, {"text": "tolerance", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9740502238273621}, {"text": "NPs chunking task", "start_pos": 185, "end_pos": 202, "type": "TASK", "confidence": 0.7233425378799438}, {"text": "precision", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9921579360961914}]}, {"text": "All the experiments were performed on a Linux system with 3.2 GHz Pentium 4 and 2G memory.", "labels": [], "entities": []}, {"text": "The total size of the Penn Chinese Treebank words is 13 MB, including about 500,000 Chinese words.", "labels": [], "entities": [{"text": "Penn Chinese Treebank words", "start_pos": 22, "end_pos": 49, "type": "DATASET", "confidence": 0.9796708226203918}]}, {"text": "The quantity of training corpus amounts to 300,000 Chinese words.", "labels": [], "entities": []}, {"text": "Each word contains two Chinese characters in average.", "labels": [], "entities": []}, {"text": "We mainly use five kinds of corpus, whose sizes include 30000, 40000, 50000, 60000 and 70,000 words.", "labels": [], "entities": []}, {"text": "The corpus with an even larger size is improper according to the training corpus amount.", "labels": [], "entities": []}, {"text": "From, we can see that the results from CRF are better than that from SVM and the error-pruning performs the best.", "labels": [], "entities": []}, {"text": "Our hybrid error-pruning method achieves an obvious improvement F-scores by combining the outcome from SVM and CRF classifiers.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9918897747993469}]}, {"text": "The test F-scores are decreasing when the sizes of corpus increase.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9828470945358276}]}, {"text": "The best performance with F-score of 89.27% is achieved by using a test corpus of 30k words.", "labels": [], "entities": [{"text": "F-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9993914365768433}]}, {"text": "We get about 1.0% increase of F-score after using the hybrid approach.", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9990369081497192}]}, {"text": "The F-score is higher than F-score 87.75% of Chinese base NP chunking systems using the Maximum Entropy method in (,.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9985048770904541}, {"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9897938966751099}]}, {"text": "Which used the smaller 3 MB Penn Chinese Treebank II as the corpus.", "labels": [], "entities": [{"text": "Penn Chinese Treebank II", "start_pos": 28, "end_pos": 52, "type": "DATASET", "confidence": 0.9767438769340515}]}, {"text": "The Chinese Base NP chunkers are not superior to those for English.", "labels": [], "entities": [{"text": "Chinese Base NP chunkers", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9252565503120422}]}, {"text": "produce the best English base NP accuracy with F-score of 94.39+ (0.79), which is superior to our best results.", "labels": [], "entities": [{"text": "English base NP", "start_pos": 17, "end_pos": 32, "type": "METRIC", "confidence": 0.34313053886095685}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.7077711224555969}, {"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9995982050895691}]}, {"text": "The previous work mostly considered base NP chunking as the classification problem without special attention to the lexical information and syntactic dependence of words.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.6738245934247971}]}, {"text": "On the other hand, we add some grammar rules to strength the syntactic dependence between the words.", "labels": [], "entities": []}, {"text": "However, the syntactic structure derived from Chinese is much more flexible and complex than that from English.", "labels": [], "entities": []}, {"text": "First, some Chinese words contain abundant meanings or play different syntactic roles.", "labels": [], "entities": []}, {"text": "For example, \"\u5176\u4e2d (among which)/NN \u91cd \u5e86 (Chongqing)/NR \u5730 \u533a (district)/NN\" is recognized as abase NP.", "labels": [], "entities": []}, {"text": "Actu-ally the Chinese word \"\u5176\u4e2d/NN (among)\" refers to the content in the previous sentence and \"\u5176\u4e2d (thereinto)\" sometimes used as an adverb.", "labels": [], "entities": []}, {"text": "Second, how to deal with the conjunctions is a major problem, especially the words \"\u4e0e (and)\" can appear in the preposition structure \"\u4e0e \u2026\u2026 \u76f8 \u5173 (relate to)\", which makes it difficult to judge those types of differences.", "labels": [], "entities": []}, {"text": "Third, the chunkers cannot handle with compact sequence data of chunks with name entities and new words (especially the transliterated words) satisfactorily, such as \"\u4e2d\u56fd ( China ) /NR \u7ea2\u5341\u5b57\u4f1a( Red Cross ) /NR\u540d\u8a89 ( Honorary ) /NN \u4f1a\u957f (Chairman ) /NN \u6c5f\u6cfd\u6c11( Jiang Ze-min ) /NR\" As it points above, the English name entities sequences are connected with the conjunction such as \"of, and, in\".", "labels": [], "entities": []}, {"text": "While in Chinese there are no such connection words for name entities sequences.", "labels": [], "entities": []}, {"text": "Therefore when we use the statistical methods, those kinds of sequential chunks contribute slightly to the feature selection and classifier training, and are treated as the useless noise in the training data.", "labels": [], "entities": []}, {"text": "In the testing section, it is close the separating margin and hardly determined to be in the right category.", "labels": [], "entities": [{"text": "separating margin", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9812254905700684}]}, {"text": "What's more, some other factors such as Idiomatic and specialized expressions also account for the errors.", "labels": [], "entities": [{"text": "Idiomatic", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9476097226142883}]}, {"text": "By highlighting those kinds of words and using some rules which emphasize on those proper words, we use our error-pruning methods and useful grammar rules to correct about 60% errors.", "labels": [], "entities": []}], "tableCaptions": []}