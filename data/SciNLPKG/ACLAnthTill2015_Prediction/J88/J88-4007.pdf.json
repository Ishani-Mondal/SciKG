{"title": [{"text": "Book Reviews The Computational Analysis of English: A Corpus-based Approach THE COMPUTATIONAL ANALYSIS OF ENGLISH: A CORPUS-BASED APPROACH", "labels": [], "entities": [{"text": "Computational Analysis of English", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.7929782867431641}, {"text": "ANALYSIS", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.873129665851593}, {"text": "APPROACH", "start_pos": 130, "end_pos": 138, "type": "TASK", "confidence": 0.5057790279388428}]}], "abstractContent": [{"text": "The notion of \"information pickup\" implies a pre-established harmony of the world and the mind, disregarding the well-known arbitrariness of language.", "labels": [], "entities": [{"text": "information pickup", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7188054174184799}]}, {"text": "5. While Fodor (1986) does gives a cogent criticism of attempts to locate information \"in the world\", the alternative \"intentional\" conception that he advocates relies on questionable assumptions of an \"internal code\" wherein such information is \"encoded\".", "labels": [], "entities": []}, {"text": "The problem, of course, lies in unpacking this metaphor.", "labels": [], "entities": []}, {"text": "Falling into the custom of taking the computational metaphor of mind literally, he resuscitates our old familiar homunculus (in computational disguise as the \"executive\") to provide away out of the problem of node labels being of higher logical type than the nodes that they label.", "labels": [], "entities": []}, {"text": "A simpler resolution follows from Harris's recognition that natural language has no :separate metalanguage.", "labels": [], "entities": []}, {"text": "See also Fodor (forthcoming).", "labels": [], "entities": [{"text": "Fodor (forthcoming)", "start_pos": 9, "end_pos": 28, "type": "DATASET", "confidence": 0.9272715598344803}]}, {"text": "6. See especially Harris (1982), and Harris, Gottfried, Ryckman, et al.", "labels": [], "entities": []}, {"text": "7. This thus cuts deeper than the naive rule-counting metrics for adjudication of grammars advocated not so long ago by genera-tivists (see Ryckman 1986).", "labels": [], "entities": []}, {"text": "8. This work is reported in depth in Harris et al.", "labels": [], "entities": []}, {"text": "These science languages occupy a place between natural language and mathematics, the chief difference from the former being that operator-argument likelihoods are much more strongly defined, amounting inmost cases to simple binary selection rather than a graded scale.", "labels": [], "entities": []}, {"text": "One of the many interesting aspects of this research is determining empirically the form of argumentation in science.", "labels": [], "entities": []}, {"text": "The logical apparatus of deduction and other forms of inference are required only for various uses to which language maybe put, rather than being the semantic basis for natural language, as has sometimes been claimed.", "labels": [], "entities": []}, {"text": "9. This is a refinement of the notion of distributional meaning developed in, e.g., Harris (1954).", "labels": [], "entities": []}, {"text": "10. The case of zero likelihood is covered by the word classes of the first constraint.", "labels": [], "entities": []}, {"text": "11. An example is the elision of one of a small set of operators including appear, arrive, show up, which have high likelihood under expect, in I expect John momentarily.", "labels": [], "entities": []}, {"text": "The adverb momentarily can only modify the elided to arrive, etc., since neither expect nor John is asserted to be momentary.", "labels": [], "entities": []}, {"text": "The infinitive to, the suffix-ly, and the status of momentarily as a modifier are the results of other reductions that are described in detail in Harris (1982).", "labels": [], "entities": []}, {"text": "12. For a computer implementation, see Johnson (1987).", "labels": [], "entities": []}, {"text": "I am grateful to Tom Ryckman for helpful comments on an early draft of this review.", "labels": [], "entities": []}, {"text": "Why is it so remarkable to have a book whose analysis of language is entirely based on actual writing?", "labels": [], "entities": []}, {"text": "Professors Garside, Leech, and Sampson have the refreshing view that the analysis of language ought to be based on real language, and have presented 12 papers resulting from their studies using the Lancaster-Oslo-Bergen corpus of a million words of British English.", "labels": [], "entities": [{"text": "Lancaster-Oslo-Bergen corpus of a million words of British English", "start_pos": 198, "end_pos": 264, "type": "DATASET", "confidence": 0.9229739904403687}]}, {"text": "They present studies of spelling correction, part-of-speech assignment, parsing, and speech synthesis based on probability techniques derived from corpus studies.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.9369452595710754}, {"text": "part-of-speech assignment", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7310997843742371}, {"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.8167475461959839}, {"text": "speech synthesis", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7215833961963654}]}, {"text": "The methods here work on arbitrary texts and with reasonable efficiency.", "labels": [], "entities": []}, {"text": "English includes a great variety of constructions that pose a dilemma for any strict grammar: to include everything and face great ambiguity, or to be extremely prescriptive and reject much.", "labels": [], "entities": []}, {"text": "The authors solve this problem by using probabilities to balance both frequent and infrequent constructions, and to emphasize low-level simple algorithms over deep interpretation.", "labels": [], "entities": []}, {"text": "For anyone trying to make practical use of text, this book is extremely enlightening.", "labels": [], "entities": []}, {"text": "English is not an inferior substitute for Prolog, and treating it as such is not only a mismatch, but also unnecessary for many tasks.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.9213107228279114}]}, {"text": "The simple use of probabilities can perform many tasks that at first glance might bethought to require understanding.", "labels": [], "entities": []}, {"text": "Methods for doing these are explained clearly in the book.", "labels": [], "entities": []}, {"text": "The most detailed result described is the technique of tagging, or assigning parts of speech statistically.", "labels": [], "entities": [{"text": "tagging", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9665414094924927}, {"text": "assigning parts of speech statistically", "start_pos": 67, "end_pos": 106, "type": "TASK", "confidence": 0.8009038090705871}]}, {"text": "By using both the individual probabilities of different parts of speech fora single word, and the combined probability of sequences of two parts of speech, tagging can be done with 96-97% accuracy.", "labels": [], "entities": [{"text": "tagging", "start_pos": 156, "end_pos": 163, "type": "TASK", "confidence": 0.9751479625701904}, {"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9980620741844177}]}, {"text": "This relatively simple algorithm, relying for performance on statistical data accumulated over a large sample of English rather than upon some kind of model of language, is typical of the results presented in this book.", "labels": [], "entities": []}, {"text": "The algorithm runs on any input, from any subject area, and does a useful job without claiming to \"understand\" natural language.", "labels": [], "entities": []}, {"text": "Just as we have learned that computers can play master-level chess by exhaustive evaluation of all possible moves, without any grand strategy or even plausible move selection, it seems that many linguistic tasks do not require understanding or modeling, but merely experience, translated into probability data.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}