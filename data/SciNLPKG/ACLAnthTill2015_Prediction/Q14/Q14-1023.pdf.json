{"title": [{"text": "Multi-Modal Models for Concrete and Abstract Concept Meaning", "labels": [], "entities": [{"text": "Concrete and Abstract Concept Meaning", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.6391020774841308}]}], "abstractContent": [{"text": "Multi-modal models that learn semantic representations from both linguistic and perceptual input outperform language-only models on a range of evaluations, and better reflect human concept acquisition.", "labels": [], "entities": []}, {"text": "Most perceptual input to such models corresponds to concrete noun concepts and the superiority of the multi-modal approach has only been established when evaluating on such concepts.", "labels": [], "entities": []}, {"text": "We therefore investigate which concepts can be effectively learned by multi-modal models.", "labels": [], "entities": []}, {"text": "We show that concreteness determines both which linguistic features are most informative and the impact of perceptual input in such models.", "labels": [], "entities": []}, {"text": "We then introduce ridge regression as a means of propagating perceptual information from concrete nouns to more abstract concepts that is more robust than previous approaches.", "labels": [], "entities": [{"text": "ridge regression", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.793156772851944}]}, {"text": "Finally, we present weighted gram matrix combination, a means of combining representations from distinct modalities that outperforms alternatives when both modalities are sufficiently rich.", "labels": [], "entities": []}], "introductionContent": [{"text": "What information is needed to learn the meaning of a word?", "labels": [], "entities": [{"text": "learn the meaning of a word", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.7356865306695303}]}, {"text": "Children learning words are exposed to a diverse mix of information sources.", "labels": [], "entities": []}, {"text": "These include clues in the language itself, such as nearby words or speaker intention, but also what the child perceives about the world around it when the word is heard.", "labels": [], "entities": []}, {"text": "Learning the meaning of words requires not only a sensitivity to both linguistic and perceptual input, but also the ability to process and combine information from these modalities in a productive way.", "labels": [], "entities": []}, {"text": "Many computational semantic models represent words as real-valued vectors, encoding their relative frequency of occurrence in particular forms and contexts in linguistic corpora.", "labels": [], "entities": []}, {"text": "Motivated both by parallels with human language acquisition and by evidence that many word meanings are grounded in the perceptual system (, recent research has explored the integration into text-based models of input that approximates the visual or other sensory modalities).", "labels": [], "entities": []}, {"text": "Such models can learn higher-quality semantic representations than conventional corpusonly models, as evidenced by a range of evaluations.", "labels": [], "entities": []}, {"text": "However, the majority of perceptual input for the models in these studies corresponds directly to concrete noun concepts, such as chocolate or cheeseburger, and the superiority of the multi-modal over the corpus-only approach has only been established when evaluations include such concepts (.", "labels": [], "entities": []}, {"text": "It is thus unclear if the multi-modal approach is effective for more abstract words, such as guilt or obesity.", "labels": [], "entities": []}, {"text": "Indeed, since empirical evidence indicates differences in the representational frameworks of both concrete and abstract concepts, and verb and noun concepts), perceptual information may not fulfill the same role in the representation of the various concept types.", "labels": [], "entities": []}, {"text": "This potential challenge to the multi-modal approach is of particular practical importance since concrete nouns constitute only a small proportion of the open-class, meaning-bearing words in everyday language.", "labels": [], "entities": []}, {"text": "In light of these considerations, this paper addresses three questions: (1) Which information sources (modalities) are important for acquiring concepts of different types?", "labels": [], "entities": []}, {"text": "(2) Can perceptual input be propagated effectively from concrete to more abstract words?", "labels": [], "entities": []}, {"text": "(3) What is the best way to combine information from the different sources?", "labels": [], "entities": []}, {"text": "We construct models that acquire semantic representations for four sets of concepts: concrete nouns, abstract nouns, concrete verbs and abstract verbs.", "labels": [], "entities": []}, {"text": "The linguistic input to the models comes from the recently released Google Syntactic N-Grams Corpus (, from which a selection of linguistic features are extracted.", "labels": [], "entities": [{"text": "Google Syntactic N-Grams Corpus", "start_pos": 68, "end_pos": 99, "type": "DATASET", "confidence": 0.7263799235224724}]}, {"text": "Perceptual input is approximated by data from the norms, which encode perceptual properties of concrete nouns, and the ESPGame dataset), which contains manually generated descriptions of 100,000 images.", "labels": [], "entities": [{"text": "ESPGame dataset", "start_pos": 119, "end_pos": 134, "type": "DATASET", "confidence": 0.9239206910133362}]}, {"text": "To address (1) we extract representations for each concept type from combinations of information sources.", "labels": [], "entities": []}, {"text": "We first focus on different classes of linguistic features, before extending our models to the multi-modal context.", "labels": [], "entities": []}, {"text": "While linguistic information overall effectively reflects the meaning of all concept types, we show that features encoding syntactic patterns are only valuable for the acquisition of abstract concepts.", "labels": [], "entities": []}, {"text": "On the other hand, perceptual information, whether directly encoded or propagated through the model, plays a more important role in the representation of concrete concepts.", "labels": [], "entities": []}, {"text": "In addressing (2), we propose ridge regression as a means of propagating features from concrete nouns to more abstract concepts.", "labels": [], "entities": [{"text": "ridge regression", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.788817435503006}]}, {"text": "The regularization term in ridge regression encourages solutions that generalize well across concept types.", "labels": [], "entities": [{"text": "ridge regression", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.742308109998703}]}, {"text": "We show that ridge regression effectively propagates perceptual information to abstract nouns and concrete verbs, and is overall preferable to both linear regression and the method of Johns and Jones (2012) applied to a similar task by.", "labels": [], "entities": [{"text": "ridge regression", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8717121481895447}]}, {"text": "However, for all propagation methods, the impact of integrating perceptual information depends on the concreteness of the target concepts.", "labels": [], "entities": []}, {"text": "Indeed, for abstract verbs, the most abstract concept type in our evaluations, perceptual input actually degrades representation quality.", "labels": [], "entities": []}, {"text": "This highlights the need to consider the concreteness of the target domain when constructing multi-modal models.", "labels": [], "entities": []}, {"text": "To address (3), we present various means of combining information from different modalities.", "labels": [], "entities": []}, {"text": "We propose weighted gram matrix combination, a technique in which representations of distinct modalities are mapped to a space of common dimension where coordinates reflect proximity to other concepts.", "labels": [], "entities": [{"text": "weighted gram matrix combination", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.5895120352506638}]}, {"text": "This transformation, which has been shown to enhance semantic representations in the context of verbclustering, reduces representation sparsity and facilitates a productbased combination that results in greater inter-modal dependency.", "labels": [], "entities": []}, {"text": "Weighted gram matrix combination outperforms alternatives such as concatenation and Canonical Correlation Analysis (CCA)) when combining representations from two similarly rich information sources.", "labels": [], "entities": [{"text": "Canonical Correlation Analysis (CCA))", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.6313773492972056}]}, {"text": "In Section 3, we present experiments with linguistic features designed to address question (1).", "labels": [], "entities": []}, {"text": "These analyses are extended to multi-modal models in Section 4, where we also address (2) and (3).", "labels": [], "entities": []}, {"text": "We first discuss the relevance of concreteness and part-ofspeech (lexical function) to concept representation.", "labels": [], "entities": [{"text": "concept representation", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.7611818611621857}]}], "datasetContent": [{"text": "We create evaluation sets of abstract and concrete concepts, and introduce a complementary dichotomy between nouns and verbs, the two POS categories most fundamental to propositional meaning.", "labels": [], "entities": []}, {"text": "To construct these sets, we extract nouns and verbs from word pairs in the USF data based on their majority POS-tag in the lemmatized BNC (, excluding any word not assigned to either of the POS categories in more than 70% of instances.", "labels": [], "entities": [{"text": "USF data", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9839335381984711}]}, {"text": "For each list of concepts L = concrete nouns, concrete verbs, abstract nouns, abstract verbs, together with lists all nouns and all verbs, a corresponding set of pairs {(w 1 , w 2 ) \u2208 U SF : w 1 , w 2 \u2208 L} is defined for evaluation.", "labels": [], "entities": []}, {"text": "These details are summarized in.", "labels": [], "entities": []}, {"text": "Evaluation lists, sets of pairs and USF scores are downloadable from our website.", "labels": [], "entities": [{"text": "USF scores", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.4868542104959488}]}, {"text": "All models are evaluated by measuring correlations with the free-association scores in the USF dataset ().", "labels": [], "entities": [{"text": "USF dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9920993745326996}]}, {"text": "This dataset contains the freeassociation strength of over 150,000 word pairs.", "labels": [], "entities": []}, {"text": "3 These data reflect the cognitive proximity of concepts and have been widely used in NLP as a goldstandard for computational models.", "labels": [], "entities": []}, {"text": "For evaluation pairs (c 1 , c 2 ) we calculate the cosine similarity between our learned feature representations for c 1 and c 2 , a standard measure of the proximity of two vectors, and follow previous studies () in using Spearman's \u03c1 as a measure of correlation between these values and our goldstandard.: Spearman correlation \u03c1 of cosine similarity between vector representations derived from three feature classes with USF scores.", "labels": [], "entities": [{"text": "Spearman correlation \u03c1", "start_pos": 308, "end_pos": 330, "type": "METRIC", "confidence": 0.6853592793146769}, {"text": "USF", "start_pos": 423, "end_pos": 426, "type": "DATASET", "confidence": 0.738563060760498}]}, {"text": "* indicates statistically significant correlations (p < 0.05 ).", "labels": [], "entities": [{"text": "statistically significant correlations", "start_pos": 12, "end_pos": 50, "type": "METRIC", "confidence": 0.6548239390055338}]}], "tableCaptions": [{"text": " Table 1: Grammatical features for noun/verb concepts", "labels": [], "entities": []}, {"text": " Table 3: Spearman correlation \u03c1 of cosine similarity between vector representations derived from three feature classes  with USF scores. * indicates statistically significant correlations (p < 0.05 ).", "labels": [], "entities": [{"text": "Spearman correlation \u03c1", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8687410155932108}, {"text": "USF", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.5601924657821655}]}]}