{"title": [{"text": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to beat least as beneficial as distributional similarities for two tasks that require semantic inference.", "labels": [], "entities": []}, {"text": "To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their de-notations, based on a large corpus of 30K images and 150K descriptive captions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to draw inferences from text is a prerequisite for language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7081415355205536}]}, {"text": "These inferences are what makes it possible for even brief descriptions of everyday scenes to evoke rich mental images.", "labels": [], "entities": []}, {"text": "For example, we would expect an image of people shopping in a supermarket to depict aisles of produce or other goods, and we would expect most of these people to be customers who are either standing or walking around.", "labels": [], "entities": []}, {"text": "But such inferences require a great deal of commonsense world knowledge.", "labels": [], "entities": []}, {"text": "Standard distributional approaches to lexical similarity (Section 2.1) are very effective at identifying which words are related to the same topic, and can provide useful features for systems that perform semantic inferences (), but are not suited to capture precise entailments between complex expressions.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach for the automatic acquisition of denotational similarities between descriptions of everyday situations (Section 2).", "labels": [], "entities": [{"text": "automatic acquisition of denotational similarities between descriptions of everyday situations", "start_pos": 51, "end_pos": 145, "type": "TASK", "confidence": 0.8077618330717087}]}, {"text": "We define the (visual) denotation of a linguistic expression as the set of images it describes.", "labels": [], "entities": []}, {"text": "We create a corpus of images of everyday activities (each paired with multiple captions; Section 3) to construct a large scale visual denotation graph which associates image descriptions with their denotations (Section 4).", "labels": [], "entities": []}, {"text": "The algorithm that constructs the denotation graph uses purely syntactic and lexical rules to produce simpler captions (which have a larger denotation).", "labels": [], "entities": []}, {"text": "But since each image is originally associated with several captions, the graph can also capture similarities between syntactically and lexically unrelated descriptions.", "labels": [], "entities": []}, {"text": "We apply these similarities to two different tasks (Sections 6 and 7): an approximate entailment recognition task for our domain, where the goal is to decide whether the hypothesis (a brief image caption) refers to the same image as the premises (four longer captions), and the recently introduced Semantic Textual Similarity task, which can be viewed as a graded (rather than binary) version of paraphrase detection.", "labels": [], "entities": [{"text": "approximate entailment recognition", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.6477479835351309}, {"text": "Semantic Textual Similarity", "start_pos": 298, "end_pos": 325, "type": "TASK", "confidence": 0.7052131096522013}, {"text": "paraphrase detection", "start_pos": 396, "end_pos": 416, "type": "TASK", "confidence": 0.8440702259540558}]}, {"text": "Both tasks require semantic inference, and our results indicate that denotational similarities are at least as effective as standard approaches to similarity.", "labels": [], "entities": []}, {"text": "Our code and data set, as well as the denotation graph itself and the lexical similarities we define over it are available for research purposes at http://nlp.cs.illinois.edu/ Denotation.html.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the STS 2012 train/test data, normalized in the same way as the image captions for the denotation graph (i.e. we re-tokenize, lemmatize, and remove determiners).", "labels": [], "entities": [{"text": "STS 2012 train/test data", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9085823595523834}]}, {"text": "shows experimental results for four models: DKPro is the off-the-shelf DKProSimilarity model.", "labels": [], "entities": [{"text": "DKPro", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.8192415833473206}]}, {"text": "From our corpus, we either add additive and multiplicative compositional features (\u03a3, \u03a0) from Section 6 (img), the C-C and C-All denotational features based on nPMI , or both compositional and denotational features.", "labels": [], "entities": []}, {"text": "Systems are evaluated by the Pearson correlation (r) of their predicted similarity scores to the human-annotated ones.", "labels": [], "entities": [{"text": "Pearson correlation (r)", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.9734962344169616}]}, {"text": "We see that the denotational similarities outperform the compositional similarities, and that including compositional similarity features in addition to denotational similarity features has little effect.", "labels": [], "entities": []}, {"text": "For additional comparison, the published numbers for the TakeLab Semantic Text Similarity System ( \u02c7 Sari\u00b4c, another topperforming model from the 2012 shared task, are r = 0.880 on this dataset.", "labels": [], "entities": [{"text": "TakeLab Semantic Text Similarity", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.6128061562776566}]}], "tableCaptions": [{"text": " Table 2: Test accuracy on Approximate Entailment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9827631711959839}, {"text": "Approximate", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.909102737903595}]}, {"text": " Table 3: Accuracy on hypotheses as various additions are  made to the vector corpora. Cap is the image corpus with  caption co-occurrence. Img is the image corpus with im- age co-occurrence. +Hyp augments the image corpus  with hypernyms and uses image co-occurrence. All adds  the BNC and Gigaword corpora to +Hyp.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9627751708030701}]}, {"text": " Table 4: Accuracy on hypotheses of varying length.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9948270916938782}]}, {"text": " Table 5: Performance on the STS MSRvid task: DKPro  (B\u00e4r et al., 2013) plus compositional (\u03a3, \u03a0) and/or deno- tational similarities (nPMI ) from our corpus", "labels": [], "entities": [{"text": "DKPro", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.6573207378387451}]}]}