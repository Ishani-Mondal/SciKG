{"title": [{"text": "Automatic Detection and Language Identification of Multilingual Documents", "labels": [], "entities": [{"text": "Automatic Detection and Language Identification of Multilingual Documents", "start_pos": 0, "end_pos": 73, "type": "TASK", "confidence": 0.7455423660576344}]}], "abstractContent": [{"text": "Language identification is the task of automatically detecting the language(s) present in a document based on the content of the document.", "labels": [], "entities": [{"text": "Language identification is the task of automatically detecting the language(s) present in a document based on the content of the document", "start_pos": 0, "end_pos": 137, "type": "Description", "confidence": 0.6485414976874987}]}, {"text": "In this work, we address the problem of detecting documents that contain text from more than one language (multilingual documents).", "labels": [], "entities": []}, {"text": "We introduce a method that is able to detect that a document is multilingual, identify the languages present, and estimate their relative proportions.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our method over synthetic data, as well as real-world multilingual documents collected from the web.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language identification is the task of automatically detecting the language(s) present in a document based on the content of the document.", "labels": [], "entities": [{"text": "Language identification is the task of automatically detecting the language(s) present in a document based on the content of the document", "start_pos": 0, "end_pos": 137, "type": "Description", "confidence": 0.6485415746768316}]}, {"text": "Language identification techniques commonly assume that every document is written in one of a closed set of known languages for which there is training data, and is thus formulated as the task of selecting the most likely language from the set of training languages.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7222305536270142}]}, {"text": "In this work, we remove this monolingual assumption, and address the problem of language identification in documents that may contain text from more than one language from the candidate set.", "labels": [], "entities": [{"text": "language identification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7196141183376312}]}, {"text": "We propose a method that concurrently detects that a document is multilingual, and estimates the proportion of the document that is written in each language.", "labels": [], "entities": []}, {"text": "Detecting multilingual documents has a variety of applications.", "labels": [], "entities": [{"text": "Detecting multilingual documents", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8880488673845927}]}, {"text": "Most natural language processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (.", "labels": [], "entities": []}, {"text": "Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data.", "labels": [], "entities": []}, {"text": "Detecting multilingual documents is also important for acquiring linguistic data from the web, and has applications in mining bilingual texts for statistical machine translation from online resources.", "labels": [], "entities": [{"text": "Detecting multilingual documents", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8819215695063273}, {"text": "statistical machine translation", "start_pos": 146, "end_pos": 177, "type": "TASK", "confidence": 0.7181796034177145}]}, {"text": "There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English.) specifically mention the need for an automatic method \"to examine a multilingual document, and with high accuracy, list the languages that are present in the document\".", "labels": [], "entities": [{"text": "accuracy", "start_pos": 303, "end_pos": 311, "type": "METRIC", "confidence": 0.9926296472549438}]}, {"text": "We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the proportion of the document written in that language.", "labels": [], "entities": []}, {"text": "We achieve this with a probabilistic mixture model, using a document representation developed for monolingual language identification ().", "labels": [], "entities": [{"text": "monolingual language identification", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.6877534886201223}]}, {"text": "The model posits that each document is generated as samples from an unknown mixture of languages from the training set.", "labels": [], "entities": []}, {"text": "We introduce a Gibbs sampler to map samples to languages for any given set of languages, and use this to select the set of languages that maximizes the posterior probability of the document.", "labels": [], "entities": []}, {"text": "Our method is able to learn a language identifier for multilingual documents from monolingual training data.", "labels": [], "entities": []}, {"text": "This is an important property as there are no standard corpora of multilingual documents available, whereas corpora of monolingual documents are readily available fora reasonably large number of languages ().", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our method empirically, firstly by evaluating it on synthetic datasets drawn from Wikipedia data, and then by applying it to real-world data, showing that we are able to identify multilingual documents in targeted web crawls of minority languages.", "labels": [], "entities": []}, {"text": "Our main contributions are: (1) we present a method for identifying multilingual documents, the languages contained therein and the relative proportion of the document in each language; (2) we show that our method outperforms state-of-the-art methods for language identification in multilingual documents; (3) we show that our method is able to estimate the proportion of the document in each language to a high degree of accuracy; and (4) we show that our method is able to identify multilingual documents in real-world data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 255, "end_pos": 278, "type": "TASK", "confidence": 0.7604113519191742}, {"text": "accuracy", "start_pos": 422, "end_pos": 430, "type": "METRIC", "confidence": 0.9956284761428833}]}], "datasetContent": [{"text": "We seek to evaluate the ability of each method: (1) to correctly identify the language(s) present in each test document; and (2) for multilingual documents, to estimate the relative proportion of the document written in each language.", "labels": [], "entities": []}, {"text": "In the first instance, this is a classification problem, and the standard notions of precision (P), recall (R) and F-score (F) apply.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.9520197808742523}, {"text": "recall (R)", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9587991684675217}, {"text": "F-score (F)", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9640896320343018}]}, {"text": "Consistent with previous work in language identification, we report both the documentlevel micro-average, as well as the language-level macro-average.", "labels": [], "entities": [{"text": "language identification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7528716325759888}]}, {"text": "For consistency with, the macro-averaged F-score we report is the average of the per-class F-scores, rather than the harmonic mean of the macro-averaged precision and recall; as such, it is possible for the F-score to not fall between the precision and recall values.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.8103803992271423}, {"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.8457484245300293}, {"text": "recall", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9857930541038513}, {"text": "F-score", "start_pos": 207, "end_pos": 214, "type": "METRIC", "confidence": 0.9778664708137512}, {"text": "precision", "start_pos": 239, "end_pos": 248, "type": "METRIC", "confidence": 0.9954040050506592}, {"text": "recall", "start_pos": 253, "end_pos": 259, "type": "METRIC", "confidence": 0.8882334232330322}]}, {"text": "As is common practice, we compute the F-score for \u03b2 = 1, giving equal importance to precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9974179267883301}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9996860027313232}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9989964365959167}]}, {"text": "We tested the difference in performance for statistical significance using an approximate randomization procedure) with 10000 iterations 4), all differences between systems are statistically significant at a p < 0.05 level.", "labels": [], "entities": []}, {"text": "To evaluate the predictions of the relative proportions of a document D written in each detected language Li , we compare the topic proportion predicted by our model to the gold-standard proportion, measured as a byte ratio as follows: length of Li part of D in bytes length of D in bytes We report the correlation between predicted and actual proportions in terms of Pearson's r coefficient.", "labels": [], "entities": [{"text": "Pearson's r coefficient", "start_pos": 368, "end_pos": 391, "type": "METRIC", "confidence": 0.7806891798973083}]}, {"text": "We also report the mean absolute error (MAE) overall document-language pairs.", "labels": [], "entities": [{"text": "mean absolute error (MAE)", "start_pos": 19, "end_pos": 44, "type": "METRIC", "confidence": 0.9208361705144247}]}, {"text": "Our first experiment utilizes the ALTW2010 shared task dataset (), a synthetic dataset of 10000 bilingual documents 3 generated from Wikipedia data, introduced in the ALTW2010 shared task, The dataset is organized into training, development and test partitions.", "labels": [], "entities": [{"text": "ALTW2010 shared task dataset", "start_pos": 34, "end_pos": 62, "type": "DATASET", "confidence": 0.7611557990312576}]}, {"text": "Following standard machine learning practice, we train each system using the training partition, and tune parameters using the development partition.", "labels": [], "entities": []}, {"text": "We then report macro and micro-averaged precision, recall and F-score on the test partition, using the tuned parameters.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9754297137260437}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9995064735412598}, {"text": "F-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9986370205879211}]}, {"text": "The results on the ALTW2010 shared task dataset are summarized in.", "labels": [], "entities": [{"text": "ALTW2010 shared task dataset", "start_pos": 19, "end_pos": 47, "type": "DATASET", "confidence": 0.6344769448041916}]}, {"text": "Each of the three systems we compare was re-trained using the training data provided for the shared task, with a slight difference: in the shared task, participants were provided with multilingual training documents, but the systems targeted in this research require monolingual training data.", "labels": [], "entities": []}, {"text": "We thus split the training documents into monolingual segments using the metadata provided with the dataset.", "labels": [], "entities": []}, {"text": "The metadata was only published after completion of the task and was not available to task participants.", "labels": [], "entities": []}, {"text": "For comparison, we have included the benchmark results published by the shared task organizers, as well as the score attained by the winning entry ().", "labels": [], "entities": []}, {"text": "We tune the parameters for each system using the development partition of the dataset, and report results on the test partition.", "labels": [], "entities": []}, {"text": "For LINGUINI, there is a single parameter k to be tuned: the number of features per language.", "labels": [], "entities": []}, {"text": "We tested values between 10000 and 50000, and selected 46000 features as the optimal value.", "labels": [], "entities": []}, {"text": "For our method, there are two parameters to be tuned: (1) the number of features selected for each language, and (2) the threshold t for including a language.", "labels": [], "entities": []}, {"text": "We tested features-per-language counts between 30 and 150, and found that adding features beyond 70 per language had minimal effect.", "labels": [], "entities": []}, {"text": "We tested values of the threshold t from 0.01 to 0.15, and found the best value was 0.14.", "labels": [], "entities": []}, {"text": "For SEGLANG, we introduce a threshold ton the minimum proportion of a document (measured in bytes) that must be labeled by a language before that language is included in the output set.", "labels": [], "entities": [{"text": "SEGLANG", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9453027248382568}]}, {"text": "This was done because our initial experiments indicate that SEGLANG tends to over-produce labels.", "labels": [], "entities": [{"text": "SEGLANG", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.8713344931602478}]}, {"text": "Using the development data, we found the best value oft was 0.10.", "labels": [], "entities": []}, {"text": "We find that of the three systems tested, two outperform the winning entry to the shared task.", "labels": [], "entities": []}, {"text": "This is more evident in the macro-averaged results than in the micro-averaged results.", "labels": [], "entities": []}, {"text": "In micro-averaged terms, our method is the best performer, whereas on the macro-average, SEGLANG has the highest F-score.", "labels": [], "entities": [{"text": "SEGLANG", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.7198769450187683}, {"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.997218132019043}]}, {"text": "This suggests that our method does well on higher-density languages (relative to the ALTW2010 dataset), and poorly on lower-density languages.", "labels": [], "entities": [{"text": "ALTW2010 dataset", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.9378535151481628}]}, {"text": "This also accounts for the higher microaveraged precision but lower micro-averaged recall for our method as compared to SEGLANG.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9379284381866455}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.8893376588821411}, {"text": "SEGLANG", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.6003726124763489}]}, {"text": "The improved macro-average F-score of SEGLANG comes at a much higher computational cost, which increases dramatically as the number of languages is increased.", "labels": [], "entities": [{"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9502990245819092}, {"text": "SEGLANG", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.730675220489502}]}, {"text": "In our testing on a 16-core workstation, SEGLANG took almost 24 hours to process the ALTW2010 shared task test data, compared to 2 minutes for our method and 40 seconds for LIN-GUINI.", "labels": [], "entities": [{"text": "SEGLANG", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.6629204750061035}, {"text": "ALTW2010 shared task test data", "start_pos": 85, "end_pos": 115, "type": "DATASET", "confidence": 0.7010703682899475}]}, {"text": "As such, SEGLANG is poorly suited to detecting multilingual documents where a large number of candidate languages is considered.", "labels": [], "entities": [{"text": "SEGLANG", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.8720691204071045}]}, {"text": "The ALTW2010 dataset is an excellent starting point for this research, but it predominantly contains bilingual documents, making it difficult to assess the ability of systems to distinguish multilingual documents from monolingual ones.", "labels": [], "entities": [{"text": "ALTW2010 dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9496566951274872}]}, {"text": "Furthermore, we are unable to use it to assess the ability of systems to detect more than 2 languages in a document.", "labels": [], "entities": []}, {"text": "To address these shortcomings, we construct anew dataset in a similar vein.", "labels": [], "entities": []}, {"text": "The dataset and experiments performed on it are described in the next section.", "labels": [], "entities": []}, {"text": "To fully test the capabilities of our model, we generated WIKIPEDIAMULTI, a dataset that contains a mixture of monolingual and multilingual documents.", "labels": [], "entities": [{"text": "WIKIPEDIAMULTI", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8484671711921692}]}, {"text": "To allow for replicability of our results and to facilitate research in language identification, we have made the dataset publicly available.", "labels": [], "entities": [{"text": "replicability", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9628283381462097}, {"text": "language identification", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.784564733505249}]}, {"text": "5 WIKI-PEDIAMULTI is generated using excerpts from the mediawiki sources of Wikipedia pages downloaded from the Wikimedia foundation.", "labels": [], "entities": []}, {"text": "The dumps we used are from July-August 2010.", "labels": [], "entities": []}, {"text": "To generate WIKIPEDIAMULTI, we first normalized the raw mediawiki documents.", "labels": [], "entities": [{"text": "WIKIPEDIAMULTI", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.8386348485946655}]}, {"text": "Mediawiki documents typically contain one paragraph per line, interspersed with structural elements.", "labels": [], "entities": [{"text": "Mediawiki documents", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9108808040618896}]}, {"text": "We filtered each document to remove all structural elements, and only kept documents that exceeded 2500 bytes after normalization.", "labels": [], "entities": []}, {"text": "This yielded a collection of around 500,000 documents in 156 languages.", "labels": [], "entities": []}, {"text": "From this initial document set (hereafter referred to as WI-KICONTENT), we only retained languages that had more than 1000 documents (44 languages), and generated documents for WIKIPEDIAMULTI as follows: 1.", "labels": [], "entities": [{"text": "WIKIPEDIAMULTI", "start_pos": 177, "end_pos": 191, "type": "DATASET", "confidence": 0.8683035969734192}]}, {"text": "randomly select the number of languages K (1\u2264K\u22645) 2.", "labels": [], "entities": []}, {"text": "randomly select a set of K languages S = {L i \u2208L for i = 1\u00b7 \u00b7 \u00b7K} without replacement 3.", "labels": [], "entities": []}, {"text": "randomly select a document for each Li \u2208S from WIKICONTENT without replacement 4.", "labels": [], "entities": [{"text": "WIKICONTENT", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.6264906525611877}]}, {"text": "take the top 1 K lines of the document 5.", "labels": [], "entities": []}, {"text": "join the K sections into a single document.", "labels": [], "entities": []}, {"text": "As a result of the procedure, the relative proportion of each language in a multilingual document tends not to be uniform, as it is conditioned on the length of the original document from which it was sourced, independent of the other K \u22121 for the other languages that it was combined with.", "labels": [], "entities": []}, {"text": "Overall, the average document length is 5500 bytes (standard deviation = 3800 bytes).", "labels": [], "entities": []}, {"text": "Due to rounding up in taking: Results on the WIKIPEDIAMULTI dataset.", "labels": [], "entities": [{"text": "WIKIPEDIAMULTI dataset", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.9744456112384796}]}, {"text": "the top 1 k lines (step 4), documents with higher K tend to be longer (6200 bytes for K = 5 vs 5100 bytes for K = 1).", "labels": [], "entities": []}, {"text": "The WIKIPEDIAMULTI dataset contains training, development and test partitions.", "labels": [], "entities": [{"text": "WIKIPEDIAMULTI dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.941492885351181}]}, {"text": "The training partition consists of 5000 monolingual (i.e. K = 1) documents.", "labels": [], "entities": []}, {"text": "The development partition consists of 5000 documents, 1000 documents for each value of K where 1\u2264K\u22645.", "labels": [], "entities": []}, {"text": "The test partition contains 200 documents for each K, fora total of 1000 documents.", "labels": [], "entities": []}, {"text": "There is no overlap between any of the partitions.", "labels": [], "entities": [{"text": "overlap", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9639418125152588}]}], "tableCaptions": [{"text": " Table 1: Examples of per-language byte sequences selected by information gain.", "labels": [], "entities": []}, {"text": " Table 2: Results on the ALTW2010 dataset.  \"Benchmark\" is the benchmark system proposed by  the shared task organizers. \"Winner\" is the highest- F \u00b5 system submitted to the shared task.", "labels": [], "entities": [{"text": "ALTW2010 dataset", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9021299779415131}, {"text": "F \u00b5 system", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.923459529876709}]}, {"text": " Table 3: Results on the WIKIPEDIAMULTI dataset.", "labels": [], "entities": [{"text": "WIKIPEDIAMULTI dataset", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.9510748386383057}]}, {"text": " Table 3. In this  evaluation, our method clearly outperforms both  SEGLANG and LINGUINI. The results on WIKI- PEDIAMULTI and ALTW2010 are difficult to com- pare directly due to the different compositions of the  two datasets. ALTW2010 is predominantly bilin- gual, whereas WIKIPEDIAMULTI contains docu- ments with text in 1-5 languages. Furthermore, the  average document in ALTW2010 is half the length  of that in WIKIPEDIAMULTI. Overall, we observe  that SEGLANG has a tendency to over-label (despite  the introduction of the t parameter to reduce this ef-", "labels": [], "entities": [{"text": "LINGUINI", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8857170939445496}, {"text": "WIKIPEDIAMULTI", "start_pos": 416, "end_pos": 430, "type": "DATASET", "confidence": 0.9469202160835266}]}, {"text": " Table 4: Detection accuracy for English-language  inclusion in web documents from targeted web  crawls for low-density languages.", "labels": [], "entities": [{"text": "Detection", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8214441537857056}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9135158658027649}, {"text": "English-language  inclusion in web documents", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.799630981683731}]}]}