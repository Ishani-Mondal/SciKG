{"title": [{"text": "Joint Incremental Disfluency Detection and Dependency Parsing", "labels": [], "entities": [{"text": "Incremental Disfluency Detection and Dependency Parsing", "start_pos": 6, "end_pos": 61, "type": "TASK", "confidence": 0.7082938949267069}]}], "abstractContent": [{"text": "We present an incremental dependency parsing model that jointly performs disflu-ency detection.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7123882174491882}, {"text": "disflu-ency detection", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7128313779830933}]}, {"text": "The model handles speech repairs using a novel non-monotonic transition system, and includes several novel classes of features.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7404239177703857}]}, {"text": "For comparison, we evaluated two pipeline systems, using state-of-the-art disfluency detectors.", "labels": [], "entities": []}, {"text": "The joint model performed better on both tasks, with a parse accuracy of 90.5% and 84.0% accuracy at disfluency detection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.7415359020233154}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.998178243637085}, {"text": "disfluency detection", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7241586893796921}]}, {"text": "The model runs in expected linear time, and processes over 550 tokens a second.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most unscripted speech contains filled pauses (ums and uhs), and errors that are usually edited on-the-fly by the speaker.", "labels": [], "entities": []}, {"text": "Disfluency detection is the task of detecting these infelicities in spoken language transcripts.", "labels": [], "entities": [{"text": "Disfluency detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8542171120643616}]}, {"text": "The task has some immediate value, as disfluencies have been shown to make speech recognition output much more difficult to read), but has also been motivated as a module in a natural language understanding pipeline, because disfluencies have proven problematic for PCFG parsing models.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 266, "end_pos": 278, "type": "TASK", "confidence": 0.7852970659732819}]}, {"text": "Instead of a pipeline approach, we build on recent work in transition-based dependency parsing, to perform the two tasks jointly.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.730238676071167}]}, {"text": "There have been two small studies of dependency parsing on unscripted speech, both using entirely greedy parsing strategies, without a direct comparison against a pipeline architecture.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8079069256782532}]}, {"text": "We go substantially beyond these pilot studies, and present a system that compares favourably to a pipeline consisting of stateof-the-art components.", "labels": [], "entities": []}, {"text": "Our parser largely follows the design of.", "labels": [], "entities": []}, {"text": "We use a structured averaged perceptron model with beamsearch decoding).", "labels": [], "entities": []}, {"text": "Our feature set is based on, and our transition-system is based on the arc-eager system of.", "labels": [], "entities": []}, {"text": "We extend the transition system with a novel non-monotonic transition, Edit.", "labels": [], "entities": []}, {"text": "It allows sentences like 'Pass the pepper uh salt' to be parsed incrementally, without the need to guess early that pepper is disfluent.", "labels": [], "entities": []}, {"text": "This is achieved by reprocessing the leftward children of the word Edit marks as disfluent.", "labels": [], "entities": []}, {"text": "For instance, if the parser attaches the to pepper, but subsequently marks pepper as disfluent, the will be returned to the stack.", "labels": [], "entities": []}, {"text": "We also exploit the ease with which the model can incorporate arbitrary features, and design a set of features that capture the 'rough copy' structure of some speech repairs, which motivated the Johnson and Charniak (2004) noisy channel model.", "labels": [], "entities": []}, {"text": "Our main comparison is against two pipeline systems, which use the two current state-of-theart disfluency detection systems as pre-processors to our parser, minus the custom disfluency features and transition.", "labels": [], "entities": []}, {"text": "The joint model compared favourably to the pipeline parsers at both tasks, with an unlabelled attachment score of 90.5%, and 84.0% accuracy at detecting speech repairs.", "labels": [], "entities": [{"text": "unlabelled attachment score", "start_pos": 83, "end_pos": 110, "type": "METRIC", "confidence": 0.7343256076176962}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9991251826286316}, {"text": "detecting speech repairs", "start_pos": 143, "end_pos": 167, "type": "TASK", "confidence": 0.8698340455691019}]}, {"text": "An efficient implementation is available under an opensource license.", "labels": [], "entities": []}, {"text": "The future prospects of the system are also quite promising.", "labels": [], "entities": []}, {"text": "Because the parser is incremental, it should be well suited to unsegmented text such as the output of a speechrecognition system.", "labels": [], "entities": []}, {"text": "We consider our main contributions to be: \u2022 a novel non-monotonic transition system, for speech repairs and restarts, A flight to um and the Switchboard corpus.", "labels": [], "entities": [{"text": "speech repairs and restarts", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.8037667945027351}, {"text": "Switchboard corpus", "start_pos": 141, "end_pos": 159, "type": "DATASET", "confidence": 0.9174849390983582}]}, {"text": "FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.", "labels": [], "entities": [{"text": "FP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9733176231384277}, {"text": "Filled Pause", "start_pos": 3, "end_pos": 15, "type": "METRIC", "confidence": 0.8848710358142853}, {"text": "Reparandum", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.4077509045600891}, {"text": "IM", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9851424098014832}, {"text": "Interregnum", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.8187446594238281}, {"text": "RP", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9691875576972961}, {"text": "Repair", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.5097160935401917}]}, {"text": "We follow previous work in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9996125102043152}]}, {"text": "\u2022 several novel feature classes, \u2022 direct comparison against the two best disfluency pre-processors, and \u2022 state-of-the-art accuracy for both speech parsing and disfluency detection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9987589120864868}, {"text": "speech parsing", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7017440050840378}, {"text": "disfluency detection", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.7690247297286987}]}], "datasetContent": [{"text": "We use the Switchboard portion of the Penn Treebank (, as described in Section 2, to train our joint models and evaluate them on dependency parsing and disfluency detection.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9936469197273254}, {"text": "dependency parsing", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7714348137378693}, {"text": "disfluency detection", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.6957605928182602}]}, {"text": "The pre-processing and dependency conversion are described in Section 2.1.", "labels": [], "entities": [{"text": "dependency conversion", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.867956280708313}]}, {"text": "We use the standard train/dev/test split from Charniak and Johnson (2001): Sections 2 and 3 for training, and Section 4 divided into three held-out sections, the first of which is used for final evaluation.", "labels": [], "entities": []}, {"text": "Our parser evaluation uses the SPARSEVAL) metric.", "labels": [], "entities": [{"text": "SPARSEVAL", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.8590295910835266}]}, {"text": "However, we wanted to use the Stanford dependency converter, for the reasons described in Section 2.1, so we used our own implementation.", "labels": [], "entities": []}, {"text": "Because we do not need to deal with recognition errors, we do not need to report our parsing results using P /R/F -measures.", "labels": [], "entities": [{"text": "P /R/F -measures", "start_pos": 107, "end_pos": 123, "type": "METRIC", "confidence": 0.7133704892226628}]}, {"text": "Instead, we report an unlabelled accuracy score, which refers to the percentage of fluent words whose governors were assigned correctly.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.9819377958774567}]}, {"text": "Note that words marked as disfluent cannot have any incoming or out-going dependencies, so if a word is incorrectly marked as disfluent, all of its dependencies will be incorrect.", "labels": [], "entities": []}, {"text": "We follow and others in restricting our disfluency evaluation to speech repairs, which we identify as words that have anode labelled EDITED as an ancestor.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7164760828018188}]}, {"text": "Unlike most other disfluency detection research, we train only on the MRG files, giving us 619,236 words of training data instead of the 1,482,845 used by the pipeline systems.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.8169987201690674}, {"text": "MRG files", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.973289966583252}]}, {"text": "It maybe possible to improve our system's disfluency detection by leveraging the additional data that does not have syntactic annotation in someway.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7190189510583878}]}, {"text": "All parsing models were trained for 15 iterations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9719142317771912}]}, {"text": "We found that optimising the number of iterations on a development set led to small improvements that did not transfer to a second development set (part of Section 4, which reserved for 'future use').", "labels": [], "entities": []}, {"text": "We test for statistical significance in our results by training 20 models for each experimental configuration, using different random seeds.", "labels": [], "entities": []}, {"text": "The random seeds control how the sentences are shuffled during training, which the perceptron model is quite sensitive to.", "labels": [], "entities": []}, {"text": "We use the Wilcoxon ranksums non-parametric test.", "labels": [], "entities": []}, {"text": "The standard deviation in UAS fora sample was typically around 0.05%, and 0.5% for disfluency F -measure.", "labels": [], "entities": [{"text": "F -measure", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9136771758397421}]}, {"text": "All of our models use beam-search decoding, with abeam width of 32.", "labels": [], "entities": []}, {"text": "We found that abeam width of 64 brought a very small accuracy improvement (about 0.1%), at the cost of 50% slower run-time.", "labels": [], "entities": [{"text": "width", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9593532681465149}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9995649456977844}]}, {"text": "Wider beams than this brought no accuracy improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9994059801101685}]}, {"text": "Accuracy seems to plateau with slightly narrower beams than on newswire text.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9944064617156982}]}, {"text": "This is probably due to the shorter sentences in Switchboard.", "labels": [], "entities": []}, {"text": "The baseline and pipeline systems are configured in the same way, except that the baseline parser is modified slightly to allow it to predict disfluencies, using a special dependency label, ERASED.", "labels": [], "entities": [{"text": "ERASED", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.7974452972412109}]}, {"text": "All descendants of a word attached to its head by this label are marked as disfluent.", "labels": [], "entities": []}, {"text": "Both the baseline and pipeline/oracle parsers use the same feature set: the Zhang and Nivre (2011) features, plus our Brown cluster features.", "labels": [], "entities": []}, {"text": "The baseline system is a standard arc-eager transition-based parser with a structured averaged perceptron model and beam-search decoding.", "labels": [], "entities": []}, {"text": "The model is trained in the standard way, with a 'static' oracle and maximum-violation update, following).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development results for the joint models. For the", "labels": [], "entities": []}, {"text": " Table 2: Test-set parse and disfluency accuracies. The joint", "labels": [], "entities": []}]}