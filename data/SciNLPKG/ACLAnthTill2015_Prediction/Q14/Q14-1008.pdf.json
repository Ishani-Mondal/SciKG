{"title": [{"text": "Exploring the Role of Stress in Bayesian Word Segmentation using Adaptor Grammars", "labels": [], "entities": [{"text": "Bayesian Word Segmentation", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.5860970417658488}]}], "abstractContent": [{"text": "Stress has long been established as a major cue in word segmentation for English infants.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7158133834600449}]}, {"text": "We show that enabling a current state-of-the-art Bayesian word segmentation model to take advantage of stress cues noticeably improves its performance.", "labels": [], "entities": [{"text": "Bayesian word segmentation", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.5949988464514414}]}, {"text": "We find that the improvements range from 10 to 4%, depending on both the use of phonotactic cues and, to a lesser extent , the amount of evidence available to the learner.", "labels": [], "entities": []}, {"text": "We also find that in particular early on, stress cues are much more useful for our model than phonotactic cues by themselves, consistent with the finding that children do seem to use stress cues before they use phono-tactic cues.", "labels": [], "entities": []}, {"text": "Finally, we study how the model's knowledge about stress patterns evolves overtime.", "labels": [], "entities": []}, {"text": "We not only find that our model correctly acquires the most frequent patterns relatively quickly but also that the Unique Stress Constraint that is at the heart of a previously proposed model does not need to be builtin but can be acquired jointly with word segmen-tation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Among the first tasks a child language learner has to solve is picking out words from the fluent speech that constitutes its linguistic input.", "labels": [], "entities": []}, {"text": "For English, stress has long been claimed to be a useful cue in infant word segmentation, following the demonstra-\u02dc bborschi/ tion of its effectiveness in adult speech processing (.", "labels": [], "entities": [{"text": "infant word segmentation", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.6948974132537842}]}, {"text": "Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.75298210978508}]}, {"text": "Bayesian models of word segmentation, however, have until recently completely ignored stress.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7477946877479553}]}, {"text": "The sole exception in this respect is who added stress cues to the Bigram model, demonstrating that this leads to an improvement in segmentation performance.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 132, "end_pos": 144, "type": "TASK", "confidence": 0.9611408114433289}]}, {"text": "In this paper, we extend their work and show how to integrate stress cues into the flexible Adaptor Grammar framework.", "labels": [], "entities": []}, {"text": "This allows us to both start from a stronger baseline model and to investigate how the role of stress cues interacts with other aspects of the model.", "labels": [], "entities": []}, {"text": "In particular, we find that phonotactic cues to word-boundaries interact with stress cues, indicating synergistic effects for small inputs and partial redundancy for larger inputs.", "labels": [], "entities": []}, {"text": "Overall, we find that stress cues add roughly 6% token f-score to a model that does not account for phonotactics and 4% to a model that already incorporates phonotactics.", "labels": [], "entities": []}, {"text": "Relatedly and inline with the finding that stress cues are used by infants before phonotactic cues (), we observe that phonotactic cues require more input than stress cues to be used efficiently.", "labels": [], "entities": []}, {"text": "A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of can be acquired jointly with segmenting the input instead of having to be pre-specified; and that our models correctly identify the predominant stress pattern of the input but underestimate the frequency of iambic words, which have been found to be missegmented by infant learners.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we review prior work.", "labels": [], "entities": []}, {"text": "In Section 3 we introduce our own models.", "labels": [], "entities": []}, {"text": "In Section 4 we explain our experimental evaluation and its results.", "labels": [], "entities": []}, {"text": "Section 5 discusses our findings, and Section 6 concludes and provides some suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on several corpora of child directed speech.", "labels": [], "entities": []}, {"text": "We first describe the corpora we used, then the experimental methodology employed and finally the experimental results.", "labels": [], "entities": []}, {"text": "As the trend is comparable across all corpora, we only discuss in detail results obtained on the Alex corpus.", "labels": [], "entities": [{"text": "Alex corpus", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.8810707032680511}]}, {"text": "For completeness, however, reports the \"standard\" evaluation of performing inference overall of the three corpora.", "labels": [], "entities": [{"text": "completeness", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.7879644632339478}]}, {"text": "The aim of our experiments is to understand the contribution of stress cues to the Bayesian word segmentation models described in Section 3.", "labels": [], "entities": [{"text": "Bayesian word segmentation", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.5676409006118774}]}, {"text": "To get an idea of how input size interacts with this, we look at prefixes of the corpora with increasing sizes, 5000, and 10,000 utterances).", "labels": [], "entities": []}, {"text": "In addition, we are interested in understanding what kind of stress pattern preferences our models acquire.", "labels": [], "entities": []}, {"text": "For this, we also collect samples of the probabilities assigned to the different expansions of rule, allowing us to examine this directly.", "labels": [], "entities": []}, {"text": "The standard evaluation of segmentation models involves having them segment their input in an unsupervised manner and evaluating performance on how well they segmented that input.", "labels": [], "entities": []}, {"text": "We additionally evaluate the models on a test set for each corpus.", "labels": [], "entities": []}, {"text": "Use of a separate test set has previously been suggested as a means of testing how well the knowledge a learner acquired generalizes to novel utterances (), and is required for the kind of comparison across different sizes of input we are interested in to determine whether there the role of stress cues interacts with the input size.", "labels": [], "entities": []}, {"text": "We create the test-sets by taking the final 1000 utterances for each corpus.", "labels": [], "entities": []}, {"text": "These 1000 utterances will be segmented by the model after it has performed inference on its input, without making any further changes to the lexicon that the model has induced.", "labels": [], "entities": []}, {"text": "In other words, the model will have to segment each of the test utterances using only the lexicon (and any additional knowledge about co-occurrences, phonotactics, and stress) it has acquired from the training portion of the corpus during inference.", "labels": [], "entities": []}, {"text": "We measure segmentation performance using the standard metric of token f-score) which is the harmonic mean of token precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.869455099105835}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9955578446388245}]}, {"text": "Token f-score provides an overall impression of how accurate individual word tokens were identified.", "labels": [], "entities": []}, {"text": "To illustrate, if the gold segmentation is \"the dog\", the segmentation \"th e dog\" has a token precision of 1 3 (one out of three predicted words is correct); a token recall of 1 2 (one of the two gold words was correctly identified); and a token f-score of 0.4.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.6318607330322266}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.7889993786811829}]}, {"text": "Each of our six models is evaluated on inputs of increasing size, starting at 100 and ending at 10,000 utterances, allowing us to investigate both how performance and \"knowledge\" of the learner varies as a function of input size.", "labels": [], "entities": []}, {"text": "For completeness, we also report the \"standard\" evaluation, i.e. performance of our models on all corpora when trained on the entire input in.", "labels": [], "entities": []}, {"text": "We will focus our discussion on the results obtained on the Alex corpus, which are depicted in, where the input size is depicted on the x-axis, and the segmentation f-score for the test-set on the y-axis.", "labels": [], "entities": [{"text": "Alex corpus", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.8935569524765015}]}], "tableCaptions": [{"text": " Table 2: Relative frequencies for stress patterns for the  corpora used in our study. X  *  stands for 0 or more, X +  for one or more repetitions of X, and S for a stressed and  W for an unstressed syllable. Note the stark asymmetry  between type and token frequencies for unstressed words.  Up to two-decimal places, patterns other than the ones  given have relative frequency 0.00 (frequencies might not  sum to 1 as an artefact of rounding to 2 decimal places).", "labels": [], "entities": []}, {"text": " Table 3: Token f-scores on both train and test portions  for all three corpora when inference is performed over  the full corpus. Note that the benefit of stress is clearer  when evaluating on the test set, and that overall, perfor- mance of the different models is comparable across all  three corpora. Models are coded according to the key in", "labels": [], "entities": []}]}