{"title": [{"text": "Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence", "labels": [], "entities": [{"text": "Monolingual Alignment", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7061559110879898}, {"text": "Exploiting Word Similarity and Contextual Evidence", "start_pos": 42, "end_pos": 92, "type": "TASK", "confidence": 0.6504541685183843}]}], "abstractContent": [{"text": "We present a simple, easy-to-replicate monolin-gual aligner that demonstrates state-of-the-art performance while relying on almost no supervision and a very small number of external resources.", "labels": [], "entities": []}, {"text": "Based on the hypothesis that words with similar meanings represent potential pairs for alignment if located in similar contexts, we propose a system that operates by finding such pairs.", "labels": [], "entities": []}, {"text": "In two intrinsic evaluations on alignment test data, our system achieves F 1 scores of 88-92%, demonstrating 1-3% absolute improvement over the previous best system.", "labels": [], "entities": [{"text": "alignment", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.9680821299552917}, {"text": "F 1 scores", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9899709622065226}]}, {"text": "Moreover, in two extrinsic evaluations our aligner out-performs existing aligners, and even a naive application of the aligner approaches state-of-the-art performance in each extrinsic task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Monolingual alignment is the task of discovering and aligning similar semantic units in a pair of sentences expressed in a natural language.", "labels": [], "entities": [{"text": "Monolingual alignment is the task of discovering and aligning similar semantic units in a pair of sentences expressed in a natural language", "start_pos": 0, "end_pos": 139, "type": "Description", "confidence": 0.8175653598525308}]}, {"text": "Such alignments provide valuable information regarding how and to what extent the two sentences are related.", "labels": [], "entities": []}, {"text": "Consequently, alignment is a central component of a number of important tasks involving text comparison: textual entailment recognition, textual similarity identification, paraphrase detection, question answering and text summarization, to name a few.", "labels": [], "entities": [{"text": "text comparison", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7294662892818451}, {"text": "textual entailment recognition", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.7672903140385946}, {"text": "textual similarity identification", "start_pos": 137, "end_pos": 170, "type": "TASK", "confidence": 0.6744844317436218}, {"text": "paraphrase detection", "start_pos": 172, "end_pos": 192, "type": "TASK", "confidence": 0.8547302782535553}, {"text": "question answering", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.8751553595066071}, {"text": "text summarization", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.7288074493408203}]}, {"text": "The high utility of monolingual alignment has spawned significant research on the topic in the recent past.", "labels": [], "entities": [{"text": "monolingual alignment", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7426084280014038}]}, {"text": "Major efforts that have treated alignment as a standalone problem () are primarily supervised, thanks to the manually aligned corpus with training and test sets from Microsoft Research.", "labels": [], "entities": [{"text": "alignment", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.954215943813324}]}, {"text": "Primary concerns of such work include both quality and speed, due to the fact that alignment is frequently a component of larger NLP tasks.", "labels": [], "entities": [{"text": "speed", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9877896308898926}]}, {"text": "Driven by similar motivations, we seek to devise a lightweight, easy-to-construct aligner that produces high-quality output and is applicable to various end tasks.", "labels": [], "entities": []}, {"text": "Amid a variety of problem formulations and ingenious approaches to alignment, we take a step back and examine closely the effectiveness of two frequently made assumptions: 1) Related semantic units in two sentences must be similar or related in their meaning, and 2) Commonalities in their semantic contexts in the respective sentences provide additional evidence of their relatedness).", "labels": [], "entities": []}, {"text": "Alignment, based solely on these two assumptions, reduces to finding the best combination of pairs of similar semantic units in similar contexts.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9716111421585083}]}, {"text": "Exploiting existing resources to identify similarity of semantic units, we search for robust techniques to identify contextual commonalities.", "labels": [], "entities": []}, {"text": "Dependency trees area commonly used structure for this purpose.", "labels": [], "entities": []}, {"text": "While they remain a central part of our aligner, we expand the horizons of dependency-based alignment beyond exact matching by systematically exploiting the notion of \"type equivalence\" with a small handcrafted set of equivalent dependency types.", "labels": [], "entities": [{"text": "dependency-based alignment", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7170077562332153}]}, {"text": "In addition, we augment dependency-based alignment with surface-level text analysis.", "labels": [], "entities": []}, {"text": "While phrasal alignments are important and have been investigated in multiple studies, we focus primarily on word alignments (which have been shown to form the vast majority of alignments (\u2265 95%) in multiple human-annotated corpora (), keeping the framework flexible enough to allow incorporation of phrasal alignments in future.", "labels": [], "entities": []}, {"text": "Evaluation of our aligner on the benchmark dataset reported in shows an F 1 score of 91.7%: a 3.1% absolute improvement over the previous best system (), corresponding to a 27.2% error reduction.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9935919245084127}, {"text": "error", "start_pos": 179, "end_pos": 184, "type": "METRIC", "confidence": 0.9825535416603088}]}, {"text": "It shows superior performance also on the dataset reported in ().", "labels": [], "entities": []}, {"text": "Additionally, we present results of two extrinsic evaluations, namely textual similarity identification and paraphrase detection.", "labels": [], "entities": [{"text": "textual similarity identification", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.662697454293569}, {"text": "paraphrase detection", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.8495247960090637}]}, {"text": "Our aligner not only outperforms existing aligners in each task, but also approaches top systems for the extrinsic tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of our aligner both intrinsically and extrinsically on multiple corpora.", "labels": [], "entities": []}, {"text": "The MSR alignment dataset 2) was designed to train and directly evaluate automated aligners.", "labels": [], "entities": [{"text": "MSR alignment dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7814503312110901}]}, {"text": "Three annotators individually aligned words and phrases in 1600 pairs of premise and hypothesis sentences from the RTE2 challenge data (divided into dev and test sets, each consisting of 800 sentences).", "labels": [], "entities": [{"text": "RTE2 challenge data", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.9426227410634359}]}, {"text": "The dataset has subsequently been used to assess several top performing aligners).", "labels": [], "entities": []}, {"text": "We use the test set for evaluation in the same manner as these studies: (a) we apply majority rule to select from the three sets of annotations for each sentence and discard threeway disagreements, (b) we evaluate only on the sure links (word pairs that annotators mentioned should certainly be aligned, as opposed to possible links).", "labels": [], "entities": []}, {"text": "We test the generalizability of the aligner by evaluating it, unchanged (i.e. with identical parameter values), on a second alignment corpus: the Edin- shows the results.", "labels": [], "entities": []}, {"text": "For each corpus, it shows precision (% alignments that matched with gold annotations), recall (% gold alignments discovered by the aligner), F 1 score and the percentage of sentences that received the exact gold alignments (denoted by E) from the aligner.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9994726777076721}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9996015429496765}, {"text": "F 1 score", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9910521705945333}]}, {"text": "On the MSR test set, our aligner shows a 3.1% improvement in F 1 score over the previous best system () with a 27.2% error reduction.", "labels": [], "entities": [{"text": "MSR test set", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.823993076880773}, {"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9925956924756368}, {"text": "error", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9906169772148132}]}, {"text": "Importantly, it demonstrates a considerable increase in recall without a loss of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.999071478843689}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9981881976127625}]}, {"text": "The E score also increases as a consequence.", "labels": [], "entities": [{"text": "E score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9887246191501617}]}, {"text": "On the Edinburgh++ test set, our system achieves a 1.2% increase in F 1 score (an error reduction of 8.8%) over the previous best system (, with improvements in both precision and recall.", "labels": [], "entities": [{"text": "Edinburgh++ test set", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9728784114122391}, {"text": "F 1 score", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9921412070592245}, {"text": "error reduction", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.9433132708072662}, {"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9995918869972229}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9994089603424072}]}, {"text": "This is a remarkable result that demonstrates the general applicability of the aligner, as no parameter tuning took place.", "labels": [], "entities": []}, {"text": "We extrinsically evaluate our system on textual similarity identification and paraphrase detection.", "labels": [], "entities": [{"text": "textual similarity identification", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.6726326743761698}, {"text": "paraphrase detection", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8525764048099518}]}, {"text": "Here we discuss each task and the results of evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of intrinsic evaluation on two datasets", "labels": [], "entities": []}, {"text": " Table 3: Ablation test results", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9907581806182861}]}, {"text": " Table 4: Extrinsic evaluation on STS 2013 data", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.849333256483078}, {"text": "STS 2013 data", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8368313908576965}]}, {"text": " Table 5: Extrinsic evaluation on MSR paraphrase data", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8387653231620789}]}]}