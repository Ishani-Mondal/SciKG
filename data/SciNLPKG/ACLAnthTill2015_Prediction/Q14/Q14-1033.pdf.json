{"title": [{"text": "Building a State-of-the-Art Grammatical Error Correction System", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7009687523047129}]}], "abstractContent": [{"text": "This paper identifies and examines the key principles underlying building a state-of-the-art grammatical error correction system.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6181420981884003}]}, {"text": "We do this by analyzing the Illinois system that placed first among seventeen teams in the recent CoNLL-2013 shared task on grammatical error correction.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9148423671722412}, {"text": "CoNLL-2013 shared task on grammatical error correction", "start_pos": 98, "end_pos": 152, "type": "TASK", "confidence": 0.5595243062291827}]}, {"text": "The system focuses on five different types of errors common among non-native English writers.", "labels": [], "entities": []}, {"text": "We describe four design principles that are relevant for correcting all of these errors , analyze the system along these dimensions , and show how each of these dimensions contributes to the performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of text correction has seen an increased interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners.", "labels": [], "entities": [{"text": "text correction", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8147135078907013}, {"text": "correcting grammatical errors made by English as a Second Language (ESL) learners", "start_pos": 103, "end_pos": 184, "type": "TASK", "confidence": 0.8358336218765804}]}, {"text": "Three competitions devoted to error correction for non-native writers took place recently:), HOO-2012 (, and the CoNLL-2013 shared task ( ).", "labels": [], "entities": [{"text": "error correction", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.6572377681732178}, {"text": "HOO-2012", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.8548789024353027}]}, {"text": "The most recent and most prominent among these, the CoNLL-2013 shared task, covers several common ESL errors, including article and preposition usage mistakes, mistakes in noun number, and various verb errors, as illustrated in.", "labels": [], "entities": []}, {"text": "Seventeen teams that The CoNLL-2014 shared task that completed at the time of writing this paper was an extension of the) but addressed all types of errors.", "labels": [], "entities": []}, {"text": "The Illinois-Columbia submission, a slightly extended version of the Nowadays *phone/phones *has/have many functionalities, *included/including *\u2205/a camera and *\u2205/a Wi-Fi receiver.", "labels": [], "entities": [{"text": "Illinois-Columbia submission", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.8601211309432983}]}, {"text": "participated in the task developed a wide array of approaches that include discriminative classifiers, language models, statistical machine-translation systems, and rule-based modules.", "labels": [], "entities": []}, {"text": "Many of the systems also made use of linguistic resources such as additional annotated learner corpora, and defined highlevel features that take into account syntactic and semantic knowledge.", "labels": [], "entities": []}, {"text": "Even though the systems incorporated similar resources, the scores varied widely.", "labels": [], "entities": []}, {"text": "The top system, from the University of Illinois, obtained an F1 score of 31.20 2 , while the second team scored 25.01 and the median result was 8.48 points.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9858929216861725}]}, {"text": "These results suggest that there is not enough understanding of what works best and what elements are essential for building a state-of-the-art error correction system.", "labels": [], "entities": [{"text": "error correction", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7184741199016571}]}, {"text": "In this paper, we identify key principles for building a robust grammatical error correction system and show their importance in the context of the shared task.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.638740082581838}]}, {"text": "We do this by analyzing the Illinois system and evaluating it along several dimensions: choice Illinois CoNLL-2013 system, ranked at the top.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9275625050067902}, {"text": "Illinois CoNLL-2013 system", "start_pos": 95, "end_pos": 121, "type": "DATASET", "confidence": 0.9287891189257304}]}, {"text": "For a description of the Illinois-Columbia submission, we refer the reader to.", "labels": [], "entities": [{"text": "Illinois-Columbia submission", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.8461939692497253}]}, {"text": "The state-of-the-art performance of the Illinois system discussed here is with respect to individual components for different errors.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8657522201538086}]}, {"text": "Improvements in Rozovskaya and Roth (2013) over the Illinois system that are due to joint learning and inference are orthogonal, and the analysis in this paper still applies there.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9230562150478363}]}, {"text": "3 F1 might not be the ideal metric for this task but this was the one chosen in the evaluation.", "labels": [], "entities": [{"text": "F1", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9871434569358826}]}, {"text": "See more in of learning algorithm; choice of training data (native or annotated learner data); model adaptation to the mistakes made by the writers; and the use of linguistic knowledge.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7213833332061768}]}, {"text": "For each dimension, several implementations are compared, including, when possible, approaches chosen by other teams.", "labels": [], "entities": []}, {"text": "We also validate the obtained results on another learner corpus.", "labels": [], "entities": []}, {"text": "Overall, this paper makes two contributions: (1) we explain the success of the Illinois system, and (2) we provide an understanding and qualitative analysis of different dimensions that are essential for success in this task, with the goal of aiding future research on it.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.8320163786411285}]}, {"text": "Given that the Illinois system has been the top system in four competitive evaluations over the last few years (HOO and CoNLL), we believe that the analysis we propose will be useful for researchers in this area.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.9147041738033295}, {"text": "HOO", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.6218960285186768}]}, {"text": "In the next section, we present the CoNLL-2013 competition.", "labels": [], "entities": [{"text": "CoNLL-2013 competition", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.8817970156669617}]}, {"text": "3 gives an overview of the approaches adopted by the top five teams.", "labels": [], "entities": []}, {"text": "4 describes the Illinois system.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9426169395446777}]}, {"text": "5, the analysis of the Illinois system is presented.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.9496838450431824}]}, {"text": "6 offers a brief discussion, and Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here, we discuss the CoNLL-2013 shared task evaluation metric and provide a little bit more detail on the performance of the Illinois modules in this context.", "labels": [], "entities": [{"text": "CoNLL-2013 shared task evaluation", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.613598957657814}]}, {"text": "As shown in in Sec.", "labels": [], "entities": []}, {"text": "2, over 90% of words (about 98% in training) are used correctly.", "labels": [], "entities": []}, {"text": "The low error rates are the key reason the error correction task is so difficult: it is quite challenging fora system to improve over a writer that already performs at the level of over 90%.", "labels": [], "entities": [{"text": "error correction task", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7432784338792165}]}, {"text": "Indeed, very few NLP tasks already have systems that perform at that level.", "labels": [], "entities": []}, {"text": "The error sparsity makes it very challenging to identify mistakes accurately.", "labels": [], "entities": []}, {"text": "In fact, the highest precision of 46.45%, as calculated by the shared task evaluation metric, is achieved by the Illinois system.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9995484948158264}, {"text": "Illinois system", "start_pos": 113, "end_pos": 128, "type": "DATASET", "confidence": 0.9465174376964569}]}, {"text": "However, once the precision drops below 50%, the system introduces more mistakes than it identifies.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9995290040969849}]}, {"text": "We can look at individual modules and see whether for any type of mistake the system improves the quality of the text.", "labels": [], "entities": []}, {"text": "shows Precision/Recall curves for the system in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9974387884140015}, {"text": "Recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.787129819393158}]}, {"text": "It is interesting to note that performance varies widely by error type.", "labels": [], "entities": []}, {"text": "The easiest are noun and article usage errors: for nouns, we can do pretty well at the recall point 20% (with the corresponding precision of over 60%); for articles, the precision is around 50% at the recall value of 20%.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9956547021865845}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9951087832450867}, {"text": "precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9978893399238586}, {"text": "recall value", "start_pos": 201, "end_pos": 213, "type": "METRIC", "confidence": 0.9767562448978424}]}, {"text": "For agreement errors, we can get a precision of 55% with a very high threshold (identifying only 5% of mistakes).", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9994913339614868}]}, {"text": "Finally, on two mistakes -preposition and verb form -the system never achieves a precision over 50%..19: Features used in the article error correction system.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9989129304885864}, {"text": "article error correction", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.6534297068913778}]}, {"text": "wB and wA denote the word immediately before and after the target, respectively; and pB and pA denote the POS tag before and after the target.", "labels": [], "entities": []}, {"text": "headWord denotes the head of the NP complement.", "labels": [], "entities": [{"text": "headWord", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7291572690010071}]}, {"text": "NC stands for noun compound and is active if second to last word in the NP is tagged as a noun.", "labels": [], "entities": []}, {"text": "Verb features are active if the NP is the direct object of a verb.", "labels": [], "entities": []}, {"text": "Preposition features are active if the NP is immediately preceded by a preposition.", "labels": [], "entities": []}, {"text": "Adj feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective.", "labels": [], "entities": []}, {"text": "NpWords and npTags denote all words (POS tags) in the NP.", "labels": [], "entities": [{"text": "NpWords", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8673621416091919}]}], "tableCaptions": [{"text": " Table 6: Comparison of learning models. Web1T corpus.", "labels": [], "entities": [{"text": "Web1T corpus", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9695416986942291}]}, {"text": " Table 7: Priors confusion matrix used for adapting NB.", "labels": [], "entities": [{"text": "Priors confusion", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7660189270973206}, {"text": "adapting NB", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.741132378578186}]}, {"text": " Table 8: Adapting NB with the priors method. All models", "labels": [], "entities": [{"text": "Adapting NB", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7662379145622253}]}, {"text": " Table 9: Adapting AP using error inflation. Models are", "labels": [], "entities": [{"text": "Adapting AP", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9166248738765717}, {"text": "error inflation", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.9343301653862}]}, {"text": " Table 10: Comparison of the inflation and sampling meth-", "labels": [], "entities": [{"text": "inflation", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.970448911190033}]}, {"text": " Table 11: Feature evaluation. Models are trained on learner", "labels": [], "entities": [{"text": "Feature evaluation", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7732017636299133}]}, {"text": " Table 12: Nouns: effect of candidate identification methods", "labels": [], "entities": []}, {"text": " Table 13: Improvement due to separate training for verb", "labels": [], "entities": []}, {"text": " Table 14: Choice of training data: learner vs. native", "labels": [], "entities": []}, {"text": " Table 15: Results on CoNLL of the Illinois system (with-", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7426944375038147}, {"text": "Illinois system", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9089301526546478}]}]}