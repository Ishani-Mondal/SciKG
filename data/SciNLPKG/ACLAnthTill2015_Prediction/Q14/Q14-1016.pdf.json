{"title": [{"text": "Discriminative Lexical Semantic Segmentation with Gaps: Running the MWE Gamut", "labels": [], "entities": [{"text": "Discriminative Lexical Semantic Segmentation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.705747663974762}, {"text": "MWE Gamut", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.903555154800415}]}], "abstractContent": [{"text": "We present a novel representation, evaluation measure, and supervised models for the task of identifying the multiword expressions (MWEs) in a sentence, resulting in a lexical semantic segmentation.", "labels": [], "entities": [{"text": "identifying the multiword expressions (MWEs) in a sentence", "start_pos": 93, "end_pos": 151, "type": "TASK", "confidence": 0.67647225856781}]}, {"text": "Our approach generalizes a standard chunking representation to encode MWEs containing gaps, thereby enabling efficient sequence tagging algorithms for feature-rich discriminative models.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.6661163568496704}]}, {"text": "Experiments on anew dataset of English web text offer the first linguistically-driven evaluation of MWE identification with truly heterogeneous expression types.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.9694481790065765}]}, {"text": "Our statistical sequence model greatly outperforms a lookup-based segmentation procedure , achieving nearly 60% F 1 for MWE identification.", "labels": [], "entities": [{"text": "F 1", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9930998086929321}, {"text": "MWE identification", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.9500983953475952}]}], "introductionContent": [{"text": "Language has a knack for defying expectations when put under the microscope.", "labels": [], "entities": []}, {"text": "For example, there is the notion-sometimes referred to as compositionalitythat words will behave in predictable ways, with individual meanings that combine to form complex meanings according to general grammatical principles.", "labels": [], "entities": []}, {"text": "Yet language is awash with examples to the contrary: in particular, idiomatic expressions such as awash with NP, have a knack for VP -ing , to the contrary, and defy expectations.", "labels": [], "entities": [{"text": "VP -ing", "start_pos": 130, "end_pos": 137, "type": "TASK", "confidence": 0.7494874993960062}]}, {"text": "Thanks to processes like metaphor and grammaticalization, these are (to various degrees) semantically opaque, structurally fossilized, and/or statistically idiosyncratic.", "labels": [], "entities": []}, {"text": "In other words, idiomatic expressions maybe exceptional inform, function, or distribution.", "labels": [], "entities": []}, {"text": "They are so diverse, so unruly, so difficult to circumscribe, that entire theories of syntax are predicated on the notion that constructions with idiosyncratic form-meaning mappings or statistical properties) offer crucial evidence about the grammatical organization of language.", "labels": [], "entities": []}, {"text": "Here we focus on multiword expressions (MWEs): lexicalized combinations of two or more words that are exceptional enough to be considered as single units in the lexicon.", "labels": [], "entities": [{"text": "multiword expressions (MWEs)", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.676069688796997}]}, {"text": "As illustrates, MWEs occupy diverse syntactic and semantic functions.", "labels": [], "entities": []}, {"text": "Within MWEs, we distinguish (a) proper names and (b) lexical idioms.", "labels": [], "entities": []}, {"text": "The latter have proved themselves a \"pain in the neck for NLP\").", "labels": [], "entities": [{"text": "NLP", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8616830706596375}]}, {"text": "Automatic and efficient detection of MWEs, though far from solved, would have diverse appli-cations including machine translation, information retrieval (, opinion mining, and second language learning (.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.7750139236450195}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7580342888832092}, {"text": "information retrieval", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.8332053124904633}, {"text": "opinion mining", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.7855191230773926}]}, {"text": "It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types.", "labels": [], "entities": []}, {"text": "Consequently, the voluminous literature on MWEs in computational linguistics-see \u00a77,, and Ramisch (2012) for surveys-has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9659806489944458}]}, {"text": "To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme.", "labels": [], "entities": []}, {"text": "Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems.", "labels": [], "entities": [{"text": "general-purpose identification", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.6492258310317993}]}, {"text": "In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a prespecified MWE lexicon ( \u00a72).", "labels": [], "entities": []}, {"text": "The scheme applies to gappy (discontinuous) as well as contiguous expressions, and allows fora qualitative distinction of association strengths.", "labels": [], "entities": []}, {"text": "In we have applied this scheme to fully annotate a 55,000-word corpus of English web reviews (), a conversational genre in which colloquial idioms are highly salient.", "labels": [], "entities": []}, {"text": "This article's main contribution is to show that the representationconstrained according to linguistically motivated assumptions ( \u00a73)-can be transformed into a sequence tagging scheme that resembles standard approaches in named entity recognition and other text chunking tasks ( \u00a74).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 223, "end_pos": 247, "type": "TASK", "confidence": 0.6469774345556895}, {"text": "text chunking tasks", "start_pos": 258, "end_pos": 277, "type": "TASK", "confidence": 0.7781449357668558}]}, {"text": "Along these lines, we develop a discriminative, structured model of MWEs in context ( \u00a75) and train, evaluate, and examine it on the annotated corpus ( \u00a76).", "labels": [], "entities": []}, {"text": "Finally, in \u00a77 and \u00a78 we comment on related work and future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given that most tokens do not belong to an MWE, to evaluate MWE identification we adopt a precision/recall-based measure from the coreference resolution literature.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.9242931008338928}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9986775517463684}, {"text": "recall-based", "start_pos": 100, "end_pos": 112, "type": "METRIC", "confidence": 0.9372729659080505}, {"text": "coreference resolution", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.9333586692810059}]}, {"text": "The MUC criterion ( of links in terms of groups (units) implied by the transitive closure over those links.", "labels": [], "entities": [{"text": "MUC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.717949628829956}]}, {"text": "It can be defined as follows: Leta b denote a link between two elements in the gold standard, and a \u02c6 b denote a link in the system prediction.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.7864324748516083}, {"text": "system prediction", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.6736557334661484}]}, {"text": "Let the * operator denote the transitive closure overall links, such that a * b is 1 if a and b belong to the same (gold) set, and 0 otherwise.", "labels": [], "entities": []}, {"text": "Assuming there are no redundant 6 links within any annotation (which in our case is guaranteed by linking consecutive words in each MWE), we can write the MUC precision and recall measures as: This awards partial credit when predicted and gold expressions overlap in part.", "labels": [], "entities": [{"text": "MUC", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.5873650312423706}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.7435083985328674}, {"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9972776770591736}]}, {"text": "Requiring full MWEs to match exactly would arguably be too stringent, overpenalizing larger MWEs for minor disagreements.", "labels": [], "entities": []}, {"text": "We combine precision and recall using the standard F 1 measure of their harmonic mean.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9995642304420471}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9995111227035522}, {"text": "F 1 measure", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9383823474248251}]}, {"text": "This is the linkbased evaluation used for most of our experiments.", "labels": [], "entities": []}, {"text": "For comparison, we also report some results with a more stringent exact match evaluation where the span of the predicted MWE must be identical to the span of the gold MWE for it to count as correct.", "labels": [], "entities": [{"text": "exact match evaluation", "start_pos": 66, "end_pos": 88, "type": "METRIC", "confidence": 0.8872033357620239}]}, {"text": "Recall that the 2-level scheme ( \u00a73.1) distinguishes strong vs. weak links/ groups, where the latter category applies to reasonably compositional collocations as well as ambiguous or difficult cases.", "labels": [], "entities": []}, {"text": "If where one annotation uses a weak link the other has a strong link or no link at all, we want to penalize the disagreement less than if one had a strong link and the other had no link.", "labels": [], "entities": []}, {"text": "To accommodate the 2-level scheme, we therefore average F \u2191 1 , in which all weak links have been converted to strong links, and F \u2193 1 , in which they have been removed: If neither annotation contains any weak links, this equals the MUC As a criterion for coreference resolution, the MUC measure has perceived shortcomings which have prompted several other measures (see Recasens and Hovy, 2011 fora review).", "labels": [], "entities": [{"text": "F \u2191 1", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9505631526311239}, {"text": "MUC", "start_pos": 233, "end_pos": 236, "type": "DATASET", "confidence": 0.9331575036048889}, {"text": "coreference resolution", "start_pos": 256, "end_pos": 278, "type": "TASK", "confidence": 0.9811824560165405}, {"text": "MUC measure", "start_pos": 284, "end_pos": 295, "type": "DATASET", "confidence": 0.8117972910404205}]}, {"text": "It is not clear, however, whether any of these criticisms are relevant to MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.9791568517684937}]}, {"text": "A link between a and b is redundant if the other links already imply that a and b belong to the same set.", "labels": [], "entities": []}, {"text": "A set of N elements is expressed non-redundantly with exactly N \u2212 1 links.", "labels": [], "entities": []}, {"text": "Overall precision and recall are likewise computed by averaging \"strengthened\" and \"weakened\" measurements.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9995261430740356}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9993947744369507}]}, {"text": "score because This method applies to both the link-based and exact match evaluation criteria.", "labels": [], "entities": []}, {"text": "The corpus of web reviews described in \u00a72 is used for training and evaluation.", "labels": [], "entities": []}, {"text": "101 arbitrarily chosen documents (500 sentences, 7,171 words) were held from words appearing at least 25 times., but here evaluated on test.", "labels": [], "entities": []}, {"text": "For each configuration, the number of training iterations M and (except for the base model) the recall-oriented hyperparameter \u03c1 were tuned by cross-validation on train.", "labels": [], "entities": [{"text": "recall-oriented hyperparameter \u03c1", "start_pos": 96, "end_pos": 128, "type": "METRIC", "confidence": 0.9604692260424296}]}, {"text": "out as a final test set.", "labels": [], "entities": []}, {"text": "This left 3,312 sentences/ 48,408 words for training/development (train).", "labels": [], "entities": []}, {"text": "Feature engineering and hyperparameter tuning were conducted with 8-fold cross-validation on train.", "labels": [], "entities": []}, {"text": "The 8-tag scheme is used except where otherwise noted.", "labels": [], "entities": []}, {"text": "In learning with the structured perceptron (algorithm 1), we employ two well-known techniques that can both be viewed as regularization.", "labels": [], "entities": []}, {"text": "First, we use the average of parameters overall timesteps of learning.", "labels": [], "entities": []}, {"text": "Second, within each cross-validation fold, we determine the number of training iterations (epochs) M by early stopping-that is, after each iteration, we use the model to decode the held-out data, and when that accuracy ceases to improve, use the previous model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9970235228538513}]}, {"text": "The two hyperparameters are the number of iterations and the value of the recall cost hyperparameter (\u03c1).", "labels": [], "entities": [{"text": "recall cost hyperparameter (\u03c1)", "start_pos": 74, "end_pos": 104, "type": "METRIC", "confidence": 0.9320614735285441}]}, {"text": "Both are tuned via cross-validation on train; we use the multiple of 50 that maximizes average link-based F 1 . The chosen values are shown in table 3.", "labels": [], "entities": [{"text": "F 1", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.927186518907547}]}, {"text": "Experiments were managed with the ducttape tool.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Counts in the MWE corpus.", "labels": [], "entities": [{"text": "MWE corpus", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.7876300513744354}]}, {"text": " Table 2: Use of lexicons for lookup-based vs. statistical segmentation. Supervised learning used only basic features  and the structured perceptron, with the 8-tag scheme. Results are with the link-based matching criterion for evaluation.  Top: Comparison of preexisting lexicons. \"6 lexicons\" refers to WordNet and SemCor plus SAID, WikiMwe, Phrases.net,  and English Wiktionary; \"10 lexicons\" adds MWEs from CEDT, VNC, LVC, and Oyz. (In these lookup-based  configurations, allowing gappy MWEs never helps performance.)  Bottom:", "labels": [], "entities": [{"text": "CEDT", "start_pos": 411, "end_pos": 415, "type": "DATASET", "confidence": 0.9714549779891968}]}, {"text": " Table 3: Comparison of supervised models on test (using the 8-tag scheme). The base model corresponds to the boxed", "labels": [], "entities": []}, {"text": " Table 5: Training with different tagging schemes. Results  are cross-validation averages on train. All schemes are  evaluated against the full gold standard (8 tags).", "labels": [], "entities": []}]}