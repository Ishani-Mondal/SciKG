{"title": [{"text": "Cross-lingual Projected Expectation Regularization for Weakly Supervised Learning", "labels": [], "entities": [{"text": "Cross-lingual Projected Expectation Regularization", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6800517141819}]}], "abstractContent": [{"text": "We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages.", "labels": [], "entities": []}, {"text": "Past approaches project labels across bitext and use them as features or gold labels for training.", "labels": [], "entities": []}, {"text": "We propose anew method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries.", "labels": [], "entities": []}, {"text": "We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010).", "labels": [], "entities": []}, {"text": "Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F 1 scores of 64% and 60% when no labeled data is used.", "labels": [], "entities": [{"text": "NER datasets", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.711149126291275}, {"text": "F 1 scores", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.982571005821228}]}, {"text": "Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9990007281303406}]}, {"text": "Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and Ger-man CoNLL-03 datasets.", "labels": [], "entities": [{"text": "Ger-man CoNLL-03 datasets", "start_pos": 200, "end_pos": 225, "type": "DATASET", "confidence": 0.6832307676474253}]}], "introductionContent": [{"text": "Supervised statistical learning methods have enjoyed great popularity in Natural Language Processing (NLP) over the past decade.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.727255622545878}]}, {"text": "The success of supervised methods depends heavily upon the availability of large amounts of annotated training data.", "labels": [], "entities": []}, {"text": "Manual curation of annotated corpora is a costly and time consuming process.", "labels": [], "entities": []}, {"text": "To date, most annotated resources resides within the English language, which hinders the adoption of supervised learning methods in many multilingual environments.", "labels": [], "entities": []}, {"text": "To minimize the need for annotation, significant progress has been made in developing unsupervised and semi-supervised approaches to NLP; Goldberg 2010; inter alia) . More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (.", "labels": [], "entities": []}, {"text": "However, in a multilingual setting, coming up with effective constraints require extensive knowledge of the foreign 1 language.", "labels": [], "entities": []}, {"text": "Bilingual parallel text (bitext) lends itself as a medium to transfer knowledge from a resource-rich language to a foreign languages.", "labels": [], "entities": [{"text": "Bilingual parallel text (bitext)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7116573055585226}]}, {"text": "project labels produced by an English tagger to the foreign side of bitext, then use the projected labels to learn a HMM model.", "labels": [], "entities": []}, {"text": "More recent work applied the projection-based approach to more language-pairs, and further improved performance through the use of type-level constraints from tag dictionary and feature-rich generative or discriminative models (.", "labels": [], "entities": []}, {"text": "In our work, we propose anew projection-based method that differs in two important ways.", "labels": [], "entities": []}, {"text": "First, we never explicitly project the labels.", "labels": [], "entities": []}, {"text": "Instead, we project expectations over the labels.", "labels": [], "entities": []}, {"text": "This projection acts as a soft constraint over the labels, which allows us to transfer more information and uncertainty across language boundaries.", "labels": [], "entities": []}, {"text": "Secondly, we encode the expectations as constraints and train a model by minimizing divergence between model expectations and projected expectations in a Generalized Expectation (GE) Criteria ( framework.", "labels": [], "entities": []}, {"text": "We evaluate our approach on Named Entity Recognition (NER) tasks for English-Chinese and English-German language pairs on standard public datasets.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) tasks", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.8114699338163648}]}, {"text": "We report results in two settings: a weakly supervised setting where no labeled data or a small amount of labeled data is available, and a semisupervised settings where labeled data is available, but we can gain predictive power by learning from unlabeled bitext.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the latest version of Stanford NER Toolkit as our base CRF model in all experiments.", "labels": [], "entities": [{"text": "Stanford NER Toolkit", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.891662617524465}]}, {"text": "Features for English, Chinese and German CRFs are documented extensively in ( and and omitted here for brevity.", "labels": [], "entities": []}, {"text": "It it worth noting that the current Stanford NER models include recent improvements from semi-supervise learning approaches that induces distributional similarity features from large word clusters.", "labels": [], "entities": []}, {"text": "These models represent the current state-ofthe-art in supervised methods, and serve as a very strong baseline.", "labels": [], "entities": []}, {"text": "For Chinese NER experiments, we follow the same setup as to evaluate on the latest OntoNotes (v4.0) corpus ().", "labels": [], "entities": [{"text": "OntoNotes (v4.0) corpus", "start_pos": 83, "end_pos": 106, "type": "DATASET", "confidence": 0.8384971141815185}]}, {"text": "8 A total of 8,249 sentences from the parallel Chinese and English Penn Treebank portion 9 are reserved for evaluation.", "labels": [], "entities": [{"text": "English Penn Treebank portion 9", "start_pos": 59, "end_pos": 90, "type": "DATASET", "confidence": 0.8192975878715515}]}, {"text": "Odd-numbered documents are used as development set, and even-numbered documents are held out as blind test set.", "labels": [], "entities": []}, {"text": "The rest of OntoNotes annotated with NER tags are used to train the English and Chinese CRF base taggers.", "labels": [], "entities": [{"text": "English and Chinese CRF base taggers", "start_pos": 68, "end_pos": 104, "type": "DATASET", "confidence": 0.6752483000357946}]}, {"text": "There are about 16k and 39k labeled sentences for Chinese and English training, respectively.", "labels": [], "entities": []}, {"text": "The English CRF tagger trained on this training corpus gives F 1 score of 81.68% on the OntoNotes test set.", "labels": [], "entities": [{"text": "English CRF tagger", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.7917149861653646}, {"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9906713565190634}, {"text": "OntoNotes test set", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.9288927714029948}]}, {"text": "Four entities types are used for both Chinese and English with a IO tagging scheme.", "labels": [], "entities": [{"text": "IO tagging", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.7675848603248596}]}, {"text": "The English-Chinese bitext comes from the Foreign Broadcast Information Service corpus (FBIS).", "labels": [], "entities": [{"text": "Foreign Broadcast Information Service corpus (FBIS)", "start_pos": 42, "end_pos": 93, "type": "DATASET", "confidence": 0.8662398755550385}]}, {"text": "We randomly sampled 80k parallel sentence pairs to use as bitext in our experiments.", "labels": [], "entities": []}, {"text": "It is first sentence aligned using the Champollion Tool Kit, 13 then word aligned with the BerkeleyAligner.", "labels": [], "entities": [{"text": "Champollion Tool Kit", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.8896565635999044}, {"text": "BerkeleyAligner", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.9797337651252747}]}, {"text": "For German NER experiments, we evaluate using the standard CoNLL-03 NER corpus.", "labels": [], "entities": [{"text": "NER", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9011305570602417}, {"text": "CoNLL-03 NER corpus", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.9525323510169983}]}, {"text": "The labeled training set has 12k and 15k sentences, containing four entity types.", "labels": [], "entities": []}, {"text": "An English CRF model is also trained on the CoNLL-03 English data with the same entity types.", "labels": [], "entities": [{"text": "CoNLL-03 English data", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.9550833503405253}]}, {"text": "For bitext, we used a randomly sampled set of 40k parallel sentences from the de-en portion of the News Commentary dataset.", "labels": [], "entities": [{"text": "News Commentary dataset", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.9361340204874674}]}, {"text": "The English CRF tagger trained on CoNLL-03 English training corpus gives F 1 score of 90.4% on the CoNLL-03 test set.", "labels": [], "entities": [{"text": "English CRF tagger", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.6960078080495199}, {"text": "CoNLL-03 English training corpus", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.9491106420755386}, {"text": "F 1 score", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9903203447659811}, {"text": "CoNLL-03 test set", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.9826106429100037}]}, {"text": "We report typed entity precision (P), recall (R) and F 1 score.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9114541411399841}, {"text": "recall (R)", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9660031199455261}, {"text": "F 1 score", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.987384041150411}]}, {"text": "Statistical significance tests are done using a paired bootstrap resampling method with 1000 iterations, averaged over 5 runs.", "labels": [], "entities": []}, {"text": "We compare against three recently approaches that were introduced in Section 2.", "labels": [], "entities": []}, {"text": "They are: semi-supervised learning method using factored bilingual models with Gibbs sampling (; bilingual NER using Integer Linear Programming (ILP) with bilingual constraints, by; and constraint-driven bilingual-reranking approach).", "labels": [], "entities": []}, {"text": "The code from (Che et al., 2013) and () are publicly available.", "labels": [], "entities": []}, {"text": "Code from) is obtained through personal communications.", "labels": [], "entities": []}, {"text": "Since the objective function in Eqn.", "labels": [], "entities": []}, {"text": "2 is nonconvex, we adopted the early stopping training scheme from as the following: after each iteration in L-BFGS training, the model is evaluated against the development set; the training procedure is terminated if no improvements have been made in 20 iterations. and 2b show results of weakly supervised learning experiments.", "labels": [], "entities": []}, {"text": "Quite remarkably, on Chinese test set, our proposed method (CLiPER) achieves a F 1 score of 64.4% with 80k bitext, when no labeled training data is used.", "labels": [], "entities": [{"text": "Chinese test set", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.9541723926862081}, {"text": "F 1 score", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9930407603581747}, {"text": "bitext", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9251367449760437}]}, {"text": "In contrast, the supervised CRF baseline would require as much as 12k labeled sentences to attain the same accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9948098659515381}]}, {"text": "Results on the German test set is less striking.", "labels": [], "entities": [{"text": "German test set", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.9834084908167521}]}, {"text": "With no labeled data and 40k of bitext, CLiPER performs at F 1 of 60.0%, the equivalent of using 1.5k labeled examples in the supervised setting.", "labels": [], "entities": [{"text": "F 1", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9962626993656158}]}, {"text": "When combined with 1k labeled examples, performance of CLiPER reaches 69%, again of over 5% absolute over supervised CRF.", "labels": [], "entities": []}, {"text": "We also notice that supervised CRF model learns much faster in German than Chinese.", "labels": [], "entities": [{"text": "CRF", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9258146286010742}]}, {"text": "This result is not too surprising, since it is well recognized that Chinese NER is more challenging than German or English.", "labels": [], "entities": [{"text": "NER", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.5992236733436584}]}, {"text": "The best supervised results for Chinese is 10-20% (F 1 score) behind best German and English supervised results.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9870355526606241}]}, {"text": "Chinese NER relies more on lexicalized features, and therefore needs more labeled data to achieve good coverage.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7107921838760376}]}, {"text": "The results suggest that CLiPER seems to be very effective at transferring lexical knowledge from English to Chinese. and 2d compares soft GE projection with hard GE projection and the \"project-then-train\" style CRF training scheme (cf. Section 3.2).", "labels": [], "entities": []}, {"text": "We observe that both soft and hard GE projection significantly outperform the \"project-then-train\" style training scheme.", "labels": [], "entities": []}, {"text": "The difference is especially pronounced on the Chinese results when fewer labeled examples are available.", "labels": [], "entities": []}, {"text": "Soft projection gives better accuracy than hard projection when no labeled data is available, and also has a faster learning rate.", "labels": [], "entities": [{"text": "Soft projection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7146328985691071}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9977670907974243}]}], "tableCaptions": [{"text": " Table 1: Raw counts in the error confusion matrix of  English CRF models. Top table contains the counts  on OntoNotes test data, and bottom table contains  CoNLL-03 test data counts. Rows are the true la- bels and columns are the observed labels. For exam- ple, item at row 2, column 3 of the top table reads:  we observed 5 times where the true label should be  PERSON, but English CRF model output label LO-", "labels": [], "entities": [{"text": "OntoNotes test data", "start_pos": 109, "end_pos": 128, "type": "DATASET", "confidence": 0.8883243203163147}, {"text": "CoNLL-03 test data", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.8836185733477274}, {"text": "PERSON", "start_pos": 364, "end_pos": 370, "type": "METRIC", "confidence": 0.9707933664321899}, {"text": "English CRF model output label LO", "start_pos": 376, "end_pos": 409, "type": "DATASET", "confidence": 0.8676882783571879}]}, {"text": " Table 2: Test set Chinese, German NER results.  Best number of each column is highlighted in  bold. CRF is the supervised baseline. CRF ptt is  the \"project-then-train\" semi-supervised scheme for  CRF. BPBK10 is (", "labels": [], "entities": [{"text": "BPBK10", "start_pos": 203, "end_pos": 209, "type": "DATASET", "confidence": 0.8273850679397583}]}, {"text": " Table 3: Timing stats during model training.", "labels": [], "entities": []}]}