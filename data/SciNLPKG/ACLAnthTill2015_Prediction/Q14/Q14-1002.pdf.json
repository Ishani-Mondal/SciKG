{"title": [{"text": "FLORS: Fast and Simple Domain Adaptation for Part-of-Speech Tagging", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8937444686889648}, {"text": "Part-of-Speech Tagging", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.726424366235733}]}], "abstractContent": [{"text": "We present FLORS, anew part-of-speech tag-ger for domain adaptation.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9847666025161743}, {"text": "domain adaptation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.709623858332634}]}, {"text": "FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7952976822853088}]}, {"text": "FLORS is simpler and faster than previous domain adaptation methods, yet it has significantly better accuracy than several baselines.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7020654678344727}, {"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.725882351398468}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9990191459655762}]}], "introductionContent": [{"text": "In this paper we describe FLORS, a part-of-speech (POS) tagger that is Fast in training and tagging, uses LOcal context only (as opposed to finding the optimal tag sequence for the entire sentence), performs Robustly on target domains (TDs) in unsupervised domain adaptation (DA) and is Simple in architecture and feature representation.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9782552123069763}, {"text": "part-of-speech (POS) tagger", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.7021241068840027}, {"text": "LOcal", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9610427618026733}, {"text": "unsupervised domain adaptation (DA)", "start_pos": 244, "end_pos": 279, "type": "TASK", "confidence": 0.834175725777944}]}, {"text": "FLORS constructs a robust representation of the local context of the word v that is to be tagged.", "labels": [], "entities": []}, {"text": "This representation consists of distributional features, suffixes and word shapes of v and its local neighbors.", "labels": [], "entities": []}, {"text": "We show that it has two advantages.", "labels": [], "entities": []}, {"text": "First, since the main predictors used by FLORS are distributional features (not the word's identity), FLORS predicts unseen tags of known words better than prior work on DA for POS.", "labels": [], "entities": []}, {"text": "Second, since FLORS uses representations computed from unlabeled text, representations of unknown words are in principle of the same type as representations of known words; this property of FLORS results in better performance on unknown words compared to prior work.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.7003978490829468}]}, {"text": "These two advantages are especially beneficial for TDs that contain high rates of unseen tags of known words and high rates of unknown words.", "labels": [], "entities": []}, {"text": "We show that FLORS achieves excellent DA tagging results on the five domains of the SANCL 2012 shared task  and outperforms three state-of-the-art taggers on biomedical data.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9242882132530212}, {"text": "DA tagging", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.881689190864563}, {"text": "SANCL 2012 shared task", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.725953534245491}]}, {"text": "FLORS is also simpler and faster than other POS DA methods.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8145466446876526}, {"text": "POS DA", "start_pos": 44, "end_pos": 50, "type": "TASK", "confidence": 0.6717938780784607}]}, {"text": "It is simple in that the input representation consists of three simple types of features: distributional count features and two types of binary features, suffix and shape features.", "labels": [], "entities": []}, {"text": "Many other word representations that are used for improving generalization (e.g.,) are costly to train or have difficulty handling unknown words.", "labels": [], "entities": []}, {"text": "Our representations are fast to build and can be created on-the-fly for unknown words that occur during testing.", "labels": [], "entities": []}, {"text": "The learning architecture is simple and fast as well.", "labels": [], "entities": []}, {"text": "We train k binary one-vs-all classifiers that use local context only and no sequence information (where k is the number of tags).", "labels": [], "entities": []}, {"text": "Thus, tagging complexity is O(k).", "labels": [], "entities": [{"text": "O", "start_pos": 28, "end_pos": 29, "type": "METRIC", "confidence": 0.9938313961029053}]}, {"text": "Many other learning setups for DA are more complex; e.g., they learn representations (as opposed to just counting), they learn several classifiers for different subclasses of words (e.g., known vs. unknown) or they combine left-toright and right-to-left taggings.", "labels": [], "entities": [{"text": "DA", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9691005349159241}]}, {"text": "The next two sections describe experimental data, setup and results.", "labels": [], "entities": []}, {"text": "Results are discussed in Section 4.", "labels": [], "entities": []}, {"text": "We compare FLORS to alternative word representations in Section 5 and to related work in Section 6.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8770731091499329}]}, {"text": "Section 7 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our source domain is the Penn Treebank (  Word features.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9966423213481903}]}, {"text": "We represent each word w by four components: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features.", "labels": [], "entities": []}, {"text": "These four components are concatenated: f (w) = f left (w)\u2295f right (w)\u2295f suffix (w)\u2295f shape (w) We consider these sources of information equally important and normalize each of the four component vectors to unit length.", "labels": [], "entities": []}, {"text": "Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems.", "labels": [], "entities": [{"text": "SVM training", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8663046658039093}]}, {"text": "We follow along tradition of older ( and newer) work on creating distributional features for POS tagging based on local left and right neighbors.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.8264328241348267}]}, {"text": "Specifically, the i th entry xi off left (w) is the weighted number of times that the indicator word c i occurs immediately to the left of w: where c i is the word with frequency rank i in the corpus, freq (bigram(c i , w)) is the number of times the bigram \"c i w\" occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x).", "labels": [], "entities": [{"text": "freq", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.9864967465400696}]}, {"text": "tf-weighting has been used by other researchers ( and showed good performance in our own previous work.", "labels": [], "entities": []}, {"text": "fright (w) is defined analogously.", "labels": [], "entities": [{"text": "fright (w)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9128395915031433}]}, {"text": "We restrict the set of indicator words to then = 500 most frequent words in the corpus.", "labels": [], "entities": []}, {"text": "To avoid zero vectors, we add an entry x n+1 to each vector that counts omitted contexts:.", "labels": [], "entities": []}, {"text": "Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., does the word contain a digit, hyphen, uppercase character) and morphological (e.g., does the word end in -ed or -ing) features.", "labels": [], "entities": []}, {"text": "There are 50 unique signatures in WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.9032340049743652}]}, {"text": "We set the dimension off shape (w) that corresponds to the signature of w to 1 and all other dimensions to 0.", "labels": [], "entities": []}, {"text": "We note that the shape features we use were designed for English and probably would have to be adjusted for other languages.", "labels": [], "entities": []}, {"text": "We address the problem of unsupervised domain adaptation for POS tagging.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7603311538696289}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7781620621681213}]}, {"text": "For this problem, we consider three types of baselines: (i) high-performing publicly available systems, (ii) the taggers used at SANCL and (iii) POS DA results published for BIO.", "labels": [], "entities": [{"text": "SANCL", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.8309958577156067}, {"text": "BIO", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.83124840259552}]}, {"text": "Most of our experiments use taggers from category (i) because we can ensure that experimental conditions are directly comparable.", "labels": [], "entities": []}, {"text": "The four baselines in category (i) are shown in.", "labels": [], "entities": []}, {"text": "Three have near state-of-the-art performance on WSJ: SVMTool (), Stanford One could also compute these suffixes for _w (w prefixed by underscore) instead of for w to include words as distinguishable special suffixes.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9444513916969299}, {"text": "SVMTool", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8379383087158203}]}, {"text": "We test this alternative in, line 15.", "labels": [], "entities": []}, {"text": "() (a birectional MEMM) and C&P.) is included as a representative of fast and simple HMM taggers.", "labels": [], "entities": [{"text": "HMM taggers", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.8924117684364319}]}, {"text": "In addition, C&P is a tagger that has been extensively tested in DA scenarios with excellent results.", "labels": [], "entities": [{"text": "DA", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9462587237358093}]}, {"text": "Unless otherwise stated, we train all models using their default configuration files.", "labels": [], "entities": []}, {"text": "We use the optimized parameter configuration published by C&P for the C&P model.", "labels": [], "entities": [{"text": "C&P", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8813356558481852}, {"text": "C&P", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8611355821291605}]}, {"text": "Test set results will be compared with the SANCL taggers (category (ii)) at the end of Section 3.", "labels": [], "entities": [{"text": "SANCL taggers", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.7449547350406647}]}, {"text": "As far as category (iii) is concerned, most work on POS DA has been evaluated on BIO.", "labels": [], "entities": [{"text": "POS DA", "start_pos": 52, "end_pos": 58, "type": "TASK", "confidence": 0.6097350567579269}, {"text": "BIO", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.763565719127655}]}, {"text": "We discuss our concerns about the BIO evaluation sets in Section 4, but also show that FLORS beats previously published results on BIO as well (see).", "labels": [], "entities": [{"text": "BIO evaluation sets", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.6589190463225046}, {"text": "FLORS", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9902133345603943}, {"text": "BIO", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.7185487151145935}]}, {"text": "We train k binary SVM classifiers on the training set.", "labels": [], "entities": []}, {"text": "A token in the test set is classified by building its feature vector, running the classifiers on it and then assigning it to the POS class whose one-vs-all LIBLINEAR classifier returns the largest score.", "labels": [], "entities": []}, {"text": "Results for ALL accuracy (accuracy for all tokens) and OOV accuracy (accuracy for tokens not occurring in the labeled WSJ data) are reported in.", "labels": [], "entities": [{"text": "ALL", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.8073773384094238}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.7799873352050781}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9891713261604309}, {"text": "OOV", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9982892870903015}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.591785728931427}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9746029376983643}, {"text": "WSJ data", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.8947925269603729}]}, {"text": "Results with an asterisk are significantly worse than a column's best result using McNemar's test (p < .001).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.7310747504234314}]}, {"text": "We use the same test and p-value throughout this paper.", "labels": [], "entities": []}, {"text": "The basic FLORS model (: Tagging accuracy of four baselines and FLORS on the dev sets.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9793525338172913}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9137318730354309}, {"text": "FLORS", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.998958945274353}]}, {"text": "The table is structured as follows: baselines (lines 1-4), basic FLORS setup (lines 5-6), effect of omitting one of the three feature types if the word to be tagged is changed compared to the basic FLORS setup (lines 7-9) and if the word to be tagged is not changed compared to basic FLORS (lines 10-12), effect of three important configuration choices on tagging accuracy: window size (line 5 vs. lines 1-4).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 364, "end_pos": 372, "type": "METRIC", "confidence": 0.9800553321838379}]}, {"text": "Only in-domain on WSJ, three baselines are slightly superior.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8910847902297974}]}, {"text": "The baselines are slightly better on ALL accuracy because they were designed for tagging in-domain data and use feature sets that have been found to work well on the source domain.", "labels": [], "entities": [{"text": "ALL", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9057050943374634}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8366168141365051}, {"text": "tagging in-domain data", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8673384785652161}]}, {"text": "Generally, C&P performs best for DA among the baselines.", "labels": [], "entities": [{"text": "DA", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.8842086791992188}]}, {"text": "On answers and WSJ, however, Stanford has better overall accuracies.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.5799896121025085}, {"text": "accuracies", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9652315378189087}]}, {"text": "These results are inline with C&P.", "labels": [], "entities": [{"text": "C&P", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9356819788614908}]}, {"text": "On lines 6-15, we investigate how different modifications of the basic FLORS model affect performance.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.4949800968170166}]}, {"text": "First, we examine the effect of leaving out components of the representation: distributional features (f left (w), fright (w)), suffixes (f suffix (w)) and shape features (f shape (w)).", "labels": [], "entities": []}, {"text": "Distributional features boost performance in all domains: ALL and OOV accuracies are consistently worse for n = 0 (line 7) than for n \u2208 {250, 500} (lines 6&5).", "labels": [], "entities": [{"text": "OOV accuracies", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.8115961849689484}]}, {"text": "FLORS with n = 250 has better OOV accuracies in 5 of 6 domains.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8921858668327332}, {"text": "OOV", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9983903169631958}, {"text": "accuracies", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.5331324338912964}]}, {"text": "However, ALL accuracy for FLORS with n = 500 is better in the majority of domains.", "labels": [], "entities": [{"text": "ALL", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9072259664535522}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.942168116569519}, {"text": "FLORS", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8963819742202759}]}, {"text": "The main result of this comparison is that FLORS does not seem to be very sensitive to the value of n if n is large enough.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9651615619659424}]}, {"text": "Shape features also improve results in all domains, with one exception: emails (lines 9 vs 5).", "labels": [], "entities": []}, {"text": "For emails, shape features decrease ALL accuracy by .19 and OOV accuracy by 1.56.", "labels": [], "entities": [{"text": "ALL", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.7989662289619446}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.8215316534042358}, {"text": "OOV", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9983347058296204}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.7177525758743286}]}, {"text": "This maybe due to the fact that many OOVs are NNP/NN and that tagging conventions for NNP/NN vary between domains.", "labels": [], "entities": []}, {"text": "See Section 4 for discussion.", "labels": [], "entities": []}, {"text": "Performance benefits from suffixes in all domains but weblogs (lines 8 vs 5).", "labels": [], "entities": []}, {"text": "Weblogs contain many foreign names such as Abdul and Yasim.", "labels": [], "entities": []}, {"text": "For these words, shapes apparently provide better information for classification than suffixes.", "labels": [], "entities": []}, {"text": "ALL accuracies suffer little when leaving out suffixes, but the feature space is much smaller: about 3000 dimensions.", "labels": [], "entities": []}, {"text": "Thus, for domains where we expect few OOVs, omitting suffix features could be considered.", "labels": [], "entities": []}, {"text": "Lines 7-9 omit one of the components off (v i ) for all five words in the local context: i \u2208 {\u22122, \u22121, 0, 1, 2}.", "labels": [], "entities": []}, {"text": "Lines 10-12 omit the same components for the neighbor words only -i.e., i \u2208 {\u22122, \u22121, 1, 2} -and leave f (v 0 ) unchanged.", "labels": [], "entities": []}, {"text": "14 of the 6 \u00d7 3 ALL accuracies on lines 10-12 are worse than FLORS basic, 4 are better.", "labels": [], "entities": [{"text": "ALL accuracies", "start_pos": 16, "end_pos": 30, "type": "METRIC", "confidence": 0.9267108142375946}, {"text": "FLORS", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9915174841880798}]}, {"text": "The largest differences are .25 for newsgroups and .19 for reviews (lines 5 vs 10), but differences for the other domains are negligible.", "labels": [], "entities": []}, {"text": "This shows that the most important feature representation is that of v 0 (not surprisingly) and that the distributional features of the other words can be omitted at the cost of some loss inaccuracy if a small average number of active features is desired.", "labels": [], "entities": []}, {"text": "Another FLORS parameter is the size of the local context.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9828388094902039}]}, {"text": "Surprisingly, OOV accuracies benefit a bit in four domains if we reduce l from 2 to 1 (lines 13 vs 5).", "labels": [], "entities": []}, {"text": "However, ALL accuracy consistently drops in all six domains.", "labels": [], "entities": [{"text": "ALL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9030607342720032}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9785032868385315}]}, {"text": "This argues for using l = 2, i.e., a window size of 5.", "labels": [], "entities": []}, {"text": "Results for left-to-right (L-to-R) tagging are given online 14.", "labels": [], "entities": []}, {"text": "Similar to SVMTool and C&P, each sentence is tagged from left to right and previous tagging decisions are used for the current classification.", "labels": [], "entities": []}, {"text": "In this setting, we use the previous tag p i\u22121 as one additional feature in the feature vector of vi . The effect of left-to-right is similar to the effect of omitting suffixes: OOV accuracies go up in some domains, but ALL accuracies decrease (except for an increase of .02 for reviews).", "labels": [], "entities": [{"text": "OOV accuracies", "start_pos": 178, "end_pos": 192, "type": "METRIC", "confidence": 0.9604212939739227}, {"text": "ALL accuracies", "start_pos": 220, "end_pos": 234, "type": "METRIC", "confidence": 0.8602185547351837}]}, {"text": "This is inline with the experiments in ( where sequential information in a CRF was not robust across domains.", "labels": [], "entities": []}, {"text": "OOV tagging may benefit from correct previous tags because the larger left context that is indirectly made available by left-to-right tagging compensates partially for the lack of information about the OOV word.", "labels": [], "entities": [{"text": "OOV tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7807258367538452}]}, {"text": "In contrast to standard approaches to POS tagging, the FLORS basic representation does not contain vocabulary indices.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.8490936458110809}, {"text": "FLORS", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.6334125995635986}]}, {"text": "Line 15 shows what happens if we add them; the dimensionality of the feature vector is increased by 5|V | -where V is the training set vocabulary -and in training one binary feature is set to one for each of the five local context words.", "labels": [], "entities": []}, {"text": "Performance is almost indistinguishable from FLORS basic, suggesting that only using suffixes -which can be viewed as \"ambiguous\" vocabulary indices, e.g., \"at\" is on for \"at\", \"mat\", \"hat\", \"laundromat\" etc -is sufficient.", "labels": [], "entities": []}, {"text": "In summary, we find that distributional features, word signatures and suffixes all contribute to successful POS DA.", "labels": [], "entities": [{"text": "POS DA", "start_pos": 108, "end_pos": 114, "type": "TASK", "confidence": 0.8085921108722687}]}, {"text": "Factors with only minor impact on performance are the number of indicator words used for the distributional representations, the window size land the tagging scheme (L-to-R vs. non-L-to-R).", "labels": [], "entities": []}, {"text": "Unknown words and known words behave differently with respect to certain feature choices.", "labels": [], "entities": []}, {"text": "The different behavior of unknown and known words suggests that training and optimizing two separate models -an approach used by SVMToolwould further increase tagging accuracy.", "labels": [], "entities": [{"text": "SVMToolwould", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.822367250919342}, {"text": "tagging", "start_pos": 159, "end_pos": 166, "type": "TASK", "confidence": 0.9534904956817627}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.8932526707649231}]}, {"text": "Note that there has been at least one publication ( on optimizing a separate model for unknown words that has in some cases better performance on OOV accuracy than what we publish here.", "labels": [], "entities": [{"text": "OOV accuracy", "start_pos": 146, "end_pos": 158, "type": "METRIC", "confidence": 0.6758142411708832}]}, {"text": "However, this would complicate the architecture of FLORS.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.7307242751121521}]}, {"text": "We opted fora maximally simple model in this paper, potentially at the cost of some performance.", "labels": [], "entities": []}, {"text": "reports results on the test sets.", "labels": [], "entities": []}, {"text": "FLORS again performs significantly better on all five TDs, both on ALL and OOV.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9032225608825684}, {"text": "ALL", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.6989544034004211}, {"text": "OOV", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.4653439223766327}]}, {"text": "Only in-domain on WSJ, ALL performance is worse.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8869338631629944}]}, {"text": "Finally, we compare our results to the POS taggers for which performance was reported at SANCL 2012 (Petrov and McDonald, 2012, Table 4).", "labels": [], "entities": [{"text": "SANCL 2012", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.8601590692996979}]}, {"text": "Constituency-based parsers -which also tag words as a by-product of deriving complete parse trees -are excluded from the comparison because they are trained on a richer representation, the syntactic structure of sentences.", "labels": [], "entities": []}, {"text": "3 FLORS' results are better than the best non-parsing-based results at SANCL 2012, which were accuracies of 92.32 on newsgroups (HIT), 90.65 on reviews (HIT) and 91.07 on answers (IMS-1).", "labels": [], "entities": [{"text": "FLORS", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9754953980445862}, {"text": "SANCL 2012", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.7220047116279602}, {"text": "accuracies", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9630248546600342}, {"text": "IMS-1", "start_pos": 180, "end_pos": 185, "type": "METRIC", "confidence": 0.7753009796142578}]}], "tableCaptions": [{"text": " Table 2: Tagging accuracy of four baselines and FLORS on the dev sets. The table is structured as follows: baselines  (lines 1-4), basic FLORS setup (lines 5-6), effect of omitting one of the three feature types if the word to be tagged  is changed compared to the basic FLORS setup (lines 7-9) and if the word to be tagged is not changed compared to  basic FLORS (lines 10-12), effect of three important configuration choices on tagging accuracy: window size", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9512860774993896}, {"text": "FLORS", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.997925877571106}, {"text": "FLORS", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.8560947775840759}, {"text": "accuracy", "start_pos": 439, "end_pos": 447, "type": "METRIC", "confidence": 0.9610631465911865}]}, {"text": " Table 3: Tagging accuracy of four baselines and FLORS on the test sets.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9522485733032227}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9844144582748413}, {"text": "FLORS", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9997032284736633}]}, {"text": " Table 4: Top: Percentage of unknown tags, OOVs and unseen word+tag combinations (i.e., known words tagged with  unseen tags) in the dev sets. Bottom: Tagging accuracy on unseen word+tag.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9922379851341248}]}, {"text": " Table 5: Frequency of some tags (percent of tokens) for  bio dev and wsj train.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9924807548522949}]}, {"text": " Table 6: Tagging accuracy on bio dev. NNP\u2192NN results  were obtained by replacing NNPs with NNs.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.975625216960907}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9712780714035034}]}, {"text": " Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n:  number of indicator words. A column's best result is bold.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9656837582588196}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9755163192749023}, {"text": "FLORS", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9880732893943787}]}]}