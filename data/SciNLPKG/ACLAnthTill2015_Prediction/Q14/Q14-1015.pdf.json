{"title": [{"text": "Dynamic Language Models for Streaming Text", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features.", "labels": [], "entities": []}, {"text": "These context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself.", "labels": [], "entities": []}, {"text": "We learn our model in an efficient online fashion that is scalable for large, streaming data.", "labels": [], "entities": []}, {"text": "With five streaming datasets from two different genres-economics news articles and social media-we evaluate our model on the task of sequential language modeling.", "labels": [], "entities": [{"text": "sequential language modeling", "start_pos": 133, "end_pos": 161, "type": "TASK", "confidence": 0.7264506419499716}]}, {"text": "Our model consistently outperforms competing models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models area key component in many NLP applications, such as machine translation and exploratory corpus analysis.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.838283509016037}, {"text": "exploratory corpus analysis", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.7000818848609924}]}, {"text": "Language models are typically assumed to be static-the word-given-context distributions do not changeover time.", "labels": [], "entities": []}, {"text": "Examples include n-gram models and probabilistic topic models like latent Dirichlet allocation (; we use the term \"language model\" to refer broadly to probabilistic models of text.", "labels": [], "entities": []}, {"text": "Recently, streaming datasets (e.g., social media) have attracted much interest in NLP.", "labels": [], "entities": []}, {"text": "Since such data evolve rapidly based on events in the real world, assuming a static language model becomes unrealistic.", "labels": [], "entities": []}, {"text": "In general, more data is seen as better, but treating all past data equally runs the risk of distracting a model with irrelevant evidence.", "labels": [], "entities": []}, {"text": "On the other hand, cautiously using only the most recent data risks overfitting to short-term trends and missing important timeinsensitive effects (;.", "labels": [], "entities": []}, {"text": "Therefore, in this paper, we take steps toward methods for capturing long-range temporal dynamics in language use.", "labels": [], "entities": []}, {"text": "Our model also exploits observable context variables to capture temporal variation that is otherwise difficult to capture using only text.", "labels": [], "entities": []}, {"text": "Specifically for the applications we consider, we use stock market data as exogenous evidence on which the language model depends.", "labels": [], "entities": []}, {"text": "For example, when an important company's price moves suddenly, the language model should be based not on the very recent history, but should be similar to the language model fora day when a similar change happened, since people are likely to say similar things (either about that company, or about conditions relevant to the change).", "labels": [], "entities": []}, {"text": "Non-linguistic contexts such as stock price changes provide useful auxiliary information that might indicate the similarity of language models across different timesteps.", "labels": [], "entities": []}, {"text": "We also turn to a fully online learning framework) to deal with nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time.", "labels": [], "entities": []}, {"text": "In online learning, streaming examples are processed only when they arrive.", "labels": [], "entities": []}, {"text": "Online learning also eliminates the need to store large amounts of data in memory.", "labels": [], "entities": []}, {"text": "Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by and.", "labels": [], "entities": []}, {"text": "Those techniques are still for static modeling.", "labels": [], "entities": [{"text": "static modeling", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.8451602458953857}]}, {"text": "Language modeling for streaming datasets in the context of machine translation was considered by and.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.804067075252533}]}, {"text": "introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts.", "labels": [], "entities": [{"text": "large scale language modeling", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6862299144268036}]}, {"text": "We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions) and on-line variational algorithms.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7356665879487991}, {"text": "regret minimization", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.6681437343358994}]}, {"text": "To our knowledge, our model is the first to bring together temporal dynamics, conditioning on nonlinguistic context, and scalable online learning suitable for streaming data and extensible to include topics and n-gram histories.", "labels": [], "entities": []}, {"text": "The main idea of our model is independent of the choice of the base language model (e.g., unigrams, bigrams, topic models, etc.).", "labels": [], "entities": []}, {"text": "In this paper, we focus on unigram and bigram language models in order to evaluate the basic idea on well understood models, and to show how it can be extended to higher-order n-grams.", "labels": [], "entities": []}, {"text": "We leave extensions to topic models for future work.", "labels": [], "entities": []}, {"text": "We propose a novel task to evaluate our proposed language model.", "labels": [], "entities": []}, {"text": "The task is to predict economicsrelated text at a given time, taking into account the changes in stock prices up to the corresponding day.", "labels": [], "entities": []}, {"text": "This can be seen an inverse of the setup considered by, where news is assumed to influence stock prices.", "labels": [], "entities": []}, {"text": "We evaluate our model on economics news in various languages, as well as Twitter data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we consider the problem of predicting economy-related text appearing in news and microblogs, based on observable features that reflect current economic conditions in the world at a given time.", "labels": [], "entities": [{"text": "predicting economy-related text appearing in news and microblogs", "start_pos": 47, "end_pos": 111, "type": "TASK", "confidence": 0.8806368187069893}]}, {"text": "In the following, we describe our dataset in detail, then show experimental results on text prediction.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7993955612182617}]}, {"text": "In all experiments, we set the window size c = 7 (one week) orc = 14 (two weeks), \u03bb = 1 2|V | (V is the size of vocabulary of the dataset under consideration), and \u03d5 = 1.", "labels": [], "entities": []}, {"text": "Our data contains metadata and text corpora.", "labels": [], "entities": []}, {"text": "The metadata is used as our features, whereas the text corpora are used for learning language models and predictions.", "labels": [], "entities": []}, {"text": "The dataset (excluding Twitter) can be downloaded at http://www.ark.cs.cmu.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the datasets. Average number of documents (third column) is per day.", "labels": [], "entities": []}, {"text": " Table 2: Perplexity results for our five data streams in the unigram experiments. The base models in \"base all,\" \"base  one,\" and \"base exp\" are unigram language models. \"int. week\" is a linear interpolation of \"base one\" from the past  week. \"int. one all\" is a linear interpolation of \"base one\" and \"base all\". The rightmost two columns are versions of  our model. Best results are highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 3: Perplexity results for our five data streams in the bigram experiments. The base models in \"base all,\" \"base  one,\" and \"base exp\" are bigram language models. \"int. week\" is a linear interpolation of \"base one\" from the past  week. \"int. one all\" is a linear interpolation of \"base one\" and \"base all\". The rightmost column is a version of our  model with c = 7. Best results are highlighted in bold.", "labels": [], "entities": []}]}