{"title": [{"text": "A Large Scale Evaluation of Distributional Semantic Models: Parameters, Interactions and Model Selection", "labels": [], "entities": [{"text": "Model Selection", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.7459547221660614}]}], "abstractContent": [{"text": "This paper presents the results of a large-scale evaluation study of window-based Distribu-tional Semantic Models on a wide variety of tasks.", "labels": [], "entities": []}, {"text": "Our study combines abroad coverage of model parameters with a model selection methodology that is robust to overfitting and able to capture parameter interactions.", "labels": [], "entities": []}, {"text": "We show that our strategy allows us to identify parameter configurations that achieve good performance across different datasets and tasks 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional Semantic Models (DSMs) are employed to produce semantic representations of words from co-occurrence patterns in texts or documents.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6036843011776606}]}, {"text": "Building on the Distributional Hypothesis, DSMs quantify the amount of meaning shared by words as the degree of overlap of the sets of contexts in which they occur.", "labels": [], "entities": []}, {"text": "A widely used approach operationalizes the set of contexts as co-occurrences with other words within a certain window (e.g., 5 words).", "labels": [], "entities": []}, {"text": "A window-based DSM can be represented as a co-occurrence matrix in which rows correspond to target words, columns correspond to context words, and cells store the cooccurrence frequencies of target words and context words.", "labels": [], "entities": []}, {"text": "The co-occurrence information is usually weighted by some scoring function and the rows of the matrix are normalized.", "labels": [], "entities": []}, {"text": "Since the co-occurrence matrix tends to be very large and sparsely populated, dimensionality reduction techniques are often used to obtain a more compact representation.", "labels": [], "entities": []}, {"text": "Landauer and claim that dimensionality reduction also improves the semantic representation encoded in the co-occurrence matrix.", "labels": [], "entities": []}, {"text": "Finally, distances between the row vectors of the matrix are computed and -according to the Distributional Hypothesis -interpreted as a correlate of the semantic similarities between the corresponding target words.", "labels": [], "entities": []}, {"text": "The construction and use of a DSM involves many design choices, such as: selection of a source corpus, size of the co-occurrence window; choice of a suitable scoring function, possibly combined with an additional transformation; whether to apply dimensionality reduction, and the number of reduced dimensions; metric for measuring distances between vectors.", "labels": [], "entities": []}, {"text": "Different design choices -technically, the DSM parameters -can result in quite different similarities for the same words).", "labels": [], "entities": []}, {"text": "DSMs have already proven successful in modeling lexical meaning: they have been applied in Natural Language Processing, Information Retrieval (, and Cognitive Modeling.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.8201667368412018}, {"text": "Cognitive Modeling", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.774910181760788}]}, {"text": "Recently, the field of Distributional Semantics has moved towards new challenges, such as predicting brain activation ( and modeling meaning composition (, and references therein).", "labels": [], "entities": [{"text": "Distributional Semantics", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8466736972332001}, {"text": "predicting brain activation", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.8638864954312643}]}, {"text": "Despite such progress, a full understanding of the different parameters governing a DSM and their influence on model performance has not been achieved yet.", "labels": [], "entities": []}, {"text": "The present paper is a contribution towards this goal: it presents the results of a large-scale evaluation of window-based DSMs on a wide variety of semantic tasks.", "labels": [], "entities": []}, {"text": "More complex tasks building on distributional representations (e.g., vector composition or relational analogies) will also benefit from our findings, allowing them to choose optimal parameters for the underlying word-level DSMs.", "labels": [], "entities": []}, {"text": "At the level of parameter coverage, this work evaluates most of the relevant parameters considered in comparable state-of-the-art studies; it also introduces an additional one, which has received little attention in the literature: the index of distributional relatedness, which connects distances in the DSM space to semantic similarity.", "labels": [], "entities": []}, {"text": "We compare direct use of distance measures to neighbor rank.", "labels": [], "entities": []}, {"text": "Neighbor rank has already been successfully used to model priming effects with DSMs (; the present study extends its evaluation to standard tasks.", "labels": [], "entities": []}, {"text": "We show that neighbor rank consistently improves the performance of DSMs compared to distance, but the degree of this improvement varies from task to task.", "labels": [], "entities": []}, {"text": "At the level of task coverage, the present study includes most of the standard datasets used in comparative studies).", "labels": [], "entities": []}, {"text": "We consider three types of evaluation tasks: multiple choice (TOEFL test), correlation to human similarity ratings, and semantic clustering.", "labels": [], "entities": [{"text": "TOEFL test)", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9071639974912008}, {"text": "semantic clustering", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7369137704372406}]}, {"text": "At the level of methodology, our work adopts the approach to model selection proposed by, which is described in detail in section 4.", "labels": [], "entities": [{"text": "model selection", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.722454845905304}]}, {"text": "Our results show that parameter interactions play a crucial role in determining model performance.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reviews state-of-the-art studies on DSM evaluation.", "labels": [], "entities": [{"text": "DSM evaluation", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.6412050724029541}]}, {"text": "Section 3 describes the experimental setting in terms of tasks and evaluated parameters.", "labels": [], "entities": []}, {"text": "Section 4 outlines our methodology for model selection.", "labels": [], "entities": [{"text": "model selection", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7792977392673492}]}, {"text": "In section 5 we report the results of our evaluation study.", "labels": [], "entities": []}, {"text": "Finally, section 6 summarizes the main findings and sketches ongoing and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of DSMs has been conducted on three standard types of semantic tasks.", "labels": [], "entities": []}, {"text": "The first task is a multiple choice setting: distributional relatedness between a target word and two or more other words is used to select the best, i.e. most similar candidate.", "labels": [], "entities": []}, {"text": "Performance in this task is quantified by the decision accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9859189987182617}]}, {"text": "The evaluated dataset is the well-known TOEFL multiple-choice synonym test, which was also included in the studies of and.", "labels": [], "entities": [{"text": "TOEFL multiple-choice synonym", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.5290145576000214}]}, {"text": "In the second task, we measure the correlation between distributional relatedness and native speaker judgments of semantic similarity or relatedness.", "labels": [], "entities": []}, {"text": "Following previous studies (), performance in this task is quantified in terms of Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 82, "end_pos": 101, "type": "METRIC", "confidence": 0.9274027645587921}]}, {"text": "3 Evaluated datasets are the Rubenstein and Goodenough dataset (RG65) of 65 noun pairs, also evaluated by, and the WordSim-353 dataset (WS353) of 353 noun pairs (), included in the study of.", "labels": [], "entities": [{"text": "Goodenough dataset (RG65)", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.9195388436317444}, {"text": "WordSim-353 dataset (WS353", "start_pos": 115, "end_pos": 141, "type": "DATASET", "confidence": 0.9329280257225037}]}, {"text": "The third evaluation task is noun clustering: distributional similarity between words is used to assign them to a pre-defined number of semantic classes.", "labels": [], "entities": [{"text": "noun clustering", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.8621655702590942}]}, {"text": "Performance in this task is quantified in terms of cluster purity.", "labels": [], "entities": []}, {"text": "Clustering is performed with an algorithm based on partitioning around medoids, using the R function pam with standard settings.", "labels": [], "entities": []}, {"text": "Evaluated datasets for the clustering task are the AlmuharebPoesio set (henceforth, AP) containing 402 nouns grouped into 21 classes); the Battig set, containing 83 concrete nouns grouped into 10 classes (Van Overschelde et al., 2004); the ESS-LLI 2008 set, containing 44 concrete nouns grouped into 6 classes; 5 and the Mitchell set, containing 60 nouns grouped into 12 classes ( , also employed by Bullinaria and Levy (2012).", "labels": [], "entities": [{"text": "AP", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9377661347389221}, {"text": "ESS-LLI 2008 set", "start_pos": 240, "end_pos": 256, "type": "DATASET", "confidence": 0.9243805408477783}]}], "tableCaptions": [{"text": " Table 1: Summary of performance", "labels": [], "entities": []}, {"text": " Table 2: TOEFL task: interactions, R 2", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.6621054410934448}]}, {"text": " Table 3: Ratings datasets: interactions, R 2", "labels": [], "entities": []}, {"text": " Table 4: Clustering task: interactions, R 2", "labels": [], "entities": []}, {"text": " Table 6: General Best Settings", "labels": [], "entities": [{"text": "General Best Settings", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6006221175193787}]}, {"text": " Table 7: General best Settings -Performance", "labels": [], "entities": []}, {"text": " Table 8: TOEFL dataset -23 models tied for best  result (4 hand-picked examples shown)", "labels": [], "entities": [{"text": "TOEFL dataset", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.7267507463693619}]}, {"text": " Table 9: Ratings, RG65 dataset -19 models tied for  best result (4 hand-picked examples shown)", "labels": [], "entities": [{"text": "RG65 dataset", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.8885882496833801}]}, {"text": " Table 10: Ratings, WordSim353 dataset -best  model (3 additional hand-picked models with sim- ilar performance are shown)", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.9537148773670197}]}, {"text": " Table 11: Clustering, Almuhareb-Poesio dataset - best model (plus 3 additional hand-picked models)", "labels": [], "entities": [{"text": "Almuhareb-Poesio dataset", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.7424827814102173}]}, {"text": " Table 12: Clustering, Battig dataset -1037 models  tied for best result (4 hand-picked examples shown)", "labels": [], "entities": [{"text": "Battig dataset", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.7766008377075195}]}]}