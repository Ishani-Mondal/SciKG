{"title": [{"text": "It's All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "Annotated data is prerequisite for many NLP applications.", "labels": [], "entities": []}, {"text": "Acquiring large-scale annotated corpora is a major bottleneck, requiring significant time and resources.", "labels": [], "entities": []}, {"text": "Recent work has proposed turning annotation into a game to increase its appeal and lower its cost; however , current games are largely text-based and closely resemble traditional annotation tasks.", "labels": [], "entities": []}, {"text": "We propose anew linguistic annotation paradigm that produces annotations from playing graphical video games.", "labels": [], "entities": []}, {"text": "The effectiveness of this design is demonstrated using two video games: one to create a mapping from WordNet senses to images, and a second game that performs Word Sense Disam-biguation.", "labels": [], "entities": []}, {"text": "Both games produce accurate results.", "labels": [], "entities": []}, {"text": "The first game yields annotation quality equal to that of experts and a cost reduction of 73% over equivalent crowdsourcing; the second game provides a 16.3% improvement inaccuracy over current state-of-the-art sense disambiguation games with WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 243, "end_pos": 250, "type": "DATASET", "confidence": 0.9588713049888611}]}], "introductionContent": [{"text": "Nearly all of Natural Language Processing (NLP) depends on annotated examples, either for training systems or for evaluating their quality.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.7628085116545359}]}, {"text": "Typically, annotations are created by linguistic experts or trained annotators.", "labels": [], "entities": []}, {"text": "However, such effort is often very time-and cost-intensive, and as a result creating large-scale annotated datasets remains a longstanding bottleneck for many areas of NLP.", "labels": [], "entities": []}, {"text": "As an alternative to requiring expert-based annotations, many studies used untrained, online workers, commonly known as crowdsourcing.", "labels": [], "entities": []}, {"text": "When successful, crowdsourcing enables gathering annotations at scale; however, its performance is still limited by (1) the difficulty of expressing the annotation task as a simply-understood task suitable for the layman, (2) the cost of collecting many annotations, and (3) the tediousness of the task, which can fail to attract workers.", "labels": [], "entities": []}, {"text": "Therefore, several groups have proposed an alternate annotation method using games: an annotation task is converted into a game which, as a result of game play, produces annotations).", "labels": [], "entities": []}, {"text": "Turning an annotation task into a Game with a Purpose (GWAP) has been shown to lead to better quality results and higher worker engagement (), thanks to the annotators being stimulated by the playful component.", "labels": [], "entities": []}, {"text": "Furthermore, because games may appeal to a different group of people than crowdsourcing, they provide a complementary channel for attracting new annotators.", "labels": [], "entities": []}, {"text": "Within NLP, gamified annotation tasks include anaphora resolution, paraphrasing), term associations () and disambiguation ().", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6963760107755661}]}, {"text": "The games' interfaces typically incorporate common game elements such as scores, leaderboards, or difficulty levels.", "labels": [], "entities": []}, {"text": "However, the game itself remains largely text-based, with a strong resemblance to a traditional annotation task, and little resemblance to games most people actively play.", "labels": [], "entities": []}, {"text": "In the current work, we propose a radical shift in NLP-focused GWAP design, building graphical, dynamic games that achieve the same result as traditional annotation.", "labels": [], "entities": [{"text": "NLP-focused GWAP design", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.6487966775894165}]}, {"text": "Rather than embellish an annota-tion task with game elements, we start from a video game that is playable alone and build the task into the game as a central component.", "labels": [], "entities": []}, {"text": "By focusing on the game aspect, players are presented with a more familiar task, which leads to higher engagement.", "labels": [], "entities": []}, {"text": "Furthermore, the video game interface can potentially attract more interest from the large percentage of the populace who play video games.", "labels": [], "entities": []}, {"text": "In two video games, we demonstrate how certain linguistic annotation tasks can be effectively represented as video games.", "labels": [], "entities": []}, {"text": "The first video game, Puzzle Racer, produces a mapping between images and WordNet senses, thereby creating a large-scale library of visual analogs of concepts.", "labels": [], "entities": [{"text": "Puzzle Racer", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.7578300535678864}]}, {"text": "While resources such as ImageNet () provide a partial sense-image mapping, they are limited to only a few thousand concrete noun senses, whereas Puzzle Racer annotates all parts of speech and both concrete and abstract senses.", "labels": [], "entities": []}, {"text": "Furthermore, Puzzle Racer's output enables new visual games for tasks using word senses such as Word Sense Disambiguation, frame detection, and selectional preference acquisition.", "labels": [], "entities": [{"text": "Puzzle Racer", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.7704609930515289}, {"text": "Word Sense Disambiguation", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.6344964702924093}, {"text": "frame detection", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.8390401303768158}, {"text": "selectional preference acquisition", "start_pos": 144, "end_pos": 178, "type": "TASK", "confidence": 0.7326354384422302}]}, {"text": "The second game, Ka-boom!, performs Word Sense Disambiguation (WSD) to identify the meaning of a word in context by players interacting with pictures.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7221881200869879}, {"text": "identify the meaning of a word in context", "start_pos": 71, "end_pos": 112, "type": "TASK", "confidence": 0.7440745234489441}]}, {"text": "Sense annotation is regarded to be one of the most challenging NLP annotation tasks, so we view it as a challenging application for testing the limits of visual NLP games.", "labels": [], "entities": [{"text": "Sense annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7592390179634094}]}, {"text": "Our work provides the following four contributions.", "labels": [], "entities": []}, {"text": "First, we present anew game-centric design methodology for NLP games with a purpose.", "labels": [], "entities": []}, {"text": "Second, we demonstrate with the first game that video games can produce linguistic annotations equal in quality to those of experts and at a cost reduction from gathering the same annotations via crowdsourcing; with the second game we show that video games provide a statistically significant performance improvement over a current state-of-the-art nonvideo game with a purpose for sense annotation.", "labels": [], "entities": [{"text": "sense annotation", "start_pos": 382, "end_pos": 398, "type": "TASK", "confidence": 0.70088991522789}]}, {"text": "Third, we release both games as a platform for other researchers to use in building new games and for annotating new data.", "labels": [], "entities": []}, {"text": "Fourth, we provide multiple resources produced by the games: (1) an image library mapped to noun, verb, and adjective Word-", "labels": [], "entities": []}], "datasetContent": [{"text": "Organizers of the Wordrobe project () provided a data set of 111 recentlyannotated contexts having between one and nine games played for each (mean 3.2 games).", "labels": [], "entities": [{"text": "Wordrobe project", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9207453429698944}]}, {"text": "This data was distinct from the contexts used to evaluate Wordrobe in in which case all contexts had six games played each.", "labels": [], "entities": [{"text": "Wordrobe", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.8800826668739319}]}, {"text": "Contexts were for 74 noun and 16 verb lemmas with a total of 310 senses (mean 3.4 senses per word).", "labels": [], "entities": []}, {"text": "Contexts were assigned the most-selected sense label from the Wordrobe games.", "labels": [], "entities": [{"text": "Wordrobe games", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9518021941184998}]}, {"text": "To gather the images for each lemma used with Ka-boom!, we repeated a similar image-gathering process as done for the gold standard images in Puzzle Racer.", "labels": [], "entities": []}, {"text": "Annotators generated at least three queries for each sense, selecting three images for each query as gold standard examples of the sense.", "labels": [], "entities": []}, {"text": "During annotation, four senses could not be associated with any queries that produced high-quality images.", "labels": [], "entities": []}, {"text": "In total, 2594 images were gathered, with an average of 8.36 images per sense.", "labels": [], "entities": []}, {"text": "The query data and unrated images are included in the data set, but were not used further in Ka-boom!", "labels": [], "entities": []}, {"text": "Game players were drawn from a small group of  WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two; because all items are annotated, precision and recall are equivalent and we report performance as accuracy.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9302000999450684}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9982366561889648}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9365085959434509}, {"text": "F1 measure", "start_pos": 138, "end_pos": 148, "type": "METRIC", "confidence": 0.9799911975860596}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9982380867004395}, {"text": "recall", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.9933310151100159}, {"text": "accuracy", "start_pos": 259, "end_pos": 267, "type": "METRIC", "confidence": 0.9972754120826721}]}, {"text": "Performance is measured relative to two baselines: (1) a baseline that picks the sense of the lemma that is most frequent in SemCor), denoted as MFS, and (2) a baseline equivalent to performance if players had randomly clicked on images, denoted as Random.", "labels": [], "entities": []}, {"text": "The first experiment directly compares the image rankings produced by the game with those from an analogous crowdsourcing task.", "labels": [], "entities": []}, {"text": "Tasks were created on the CrowdFlower platform using the identical set of examples and annotation questions encountered by players.", "labels": [], "entities": []}, {"text": "In each task, workers were shown three example gold standard images (sampled from those configurations seen by players) and asked to identify the common theme among the three examples.", "labels": [], "entities": []}, {"text": "Then, five annotation questions were shown in which workers were asked to choose which of three images was most related to the theme.", "labels": [], "entities": []}, {"text": "Questions were created after the Puzzle Racer study finished in order to use the identical set of questions seen by players as mystery gates.", "labels": [], "entities": [{"text": "Puzzle Racer", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.7416099607944489}]}, {"text": "Workers were paid $0.03USD per task.", "labels": [], "entities": []}, {"text": "To compare the quality of the Puzzle Racer image rankings with those from CrowdFlower, the three highest-rated images of each sense from both rankings were compared.", "labels": [], "entities": []}, {"text": "Two annotators were shown a sense's definition and example uses, and then asked to compare the quality of three image pairs, selecting whether (a) the left image was a better depiction of the sense, (b) the right image was better, or (c) the images were approximately equal in quality.", "labels": [], "entities": []}, {"text": "In the case of disagreements, a third annotator was asked to compare the images; the majority answer was used when present or, in the case of all three ratings, images were treated as equal, the latter of which occurred for only 17% of the questions.", "labels": [], "entities": []}, {"text": "For all 396 questions, the method used to rank the image was hidden and the order in which images appeared was randomized.", "labels": [], "entities": []}, {"text": "Results During the study period, players completed 7199 races, generating 20,253 ratings across 16,479 images.", "labels": [], "entities": []}, {"text": "Ratings were balanced across senses, with a minimum and maximum of 231 and 329 ratings per sense.", "labels": [], "entities": []}, {"text": "Players accurately identified each race's theme, selecting the correct image in 83% of all golden puzzle gates shown.", "labels": [], "entities": []}, {"text": "shows example top-rated images from Puzzle Racer.", "labels": [], "entities": []}, {"text": "Experiment 1 measures differences in the quality of the three top-ranked images produced by Puzzle Racer and CrowdFlower for each sense.", "labels": [], "entities": []}, {"text": "Puzzle Racer and CrowdFlower produced similar ratings, with at least one image appearing in the top-three positions of both ranks for 55% of the senses.", "labels": [], "entities": []}, {"text": "Both annotators agreed in 72% of cases in selecting the best sense depiction, finding that in 88% of the agreed cases both images were approximately equal representations of the sense.", "labels": [], "entities": []}, {"text": "In the remaining, the Puzzle Racer image was better in 4% and activate 4 v : aerate (sewage) so as to favor the growth of organisms that decompose organic matter argument 2 n : a contentious speech act; a dispute where there is strong disagreement atmosphere 4 n : the weather or climate at someplace climb 1 v : go upward with gradual or continuous progress important 1 a : of great significance or value rule 2 v : decide with authority  The second experiment evaluates the ability of the games to produce high-quality images by measuring the difference in quality between gold standard images and top-rated images in the game.", "labels": [], "entities": [{"text": "Puzzle Racer", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.6250566989183426}]}, {"text": "CrowdFlower workers were shown a question with a sense's definition and example uses and then asked to choose which of two images was a better visual representation of the gloss.", "labels": [], "entities": []}, {"text": "Questions were created for each of the three highest-rated images for each sense, pairing each with a randomly-selected gold standard image for that sense.", "labels": [], "entities": []}, {"text": "Image order was randomized between questions.", "labels": [], "entities": [{"text": "Image order", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8349668979644775}]}, {"text": "Five questions were shown per task and workers were paid $0.05USD per task.", "labels": [], "entities": []}, {"text": "The 2670 worker responses were aggregated by selecting each question's most frequent answer.", "labels": [], "entities": []}, {"text": "Results For senses within each part of speech, workers preferred the gold standard image to the top-rated image for nouns, verbs, and adjectives 57.4%, 53.1%, and 56.2% of the time, respectively.", "labels": [], "entities": []}, {"text": "This preference is not significant at p < 0.05, indicating that the top-ranked images produced through Puzzle Racer game play are approximately equivalent in quality to images manually chosen by experts with full knowledge of the sense inventory.", "labels": [], "entities": [{"text": "Puzzle Racer game play", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7727223783731461}]}], "tableCaptions": [{"text": " Table 5: Sense disambiguation accuracies", "labels": [], "entities": [{"text": "Sense disambiguation accuracies", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.5924138724803925}]}]}