{"title": [{"text": "Joint Modeling of Opinion Expression Extraction and Attribute Classification", "labels": [], "entities": [{"text": "Joint Modeling of Opinion Expression Extraction", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6766816029946009}]}], "abstractContent": [{"text": "In this paper, we study the problems of opinion expression extraction and expression-level polarity and intensity classification.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6750636498133341}, {"text": "intensity classification", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.7710004150867462}]}, {"text": "Traditional fine-grained opinion analysis systems address these problems in isolation and thus cannot capture interactions among the tex-tual spans of opinion expressions and their opinion-related properties.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.6797769367694855}]}, {"text": "We present two types of joint approaches that can account for such interactions during 1) both learning and inference or 2) only during inference.", "labels": [], "entities": []}, {"text": "Extensive experiments on a standard dataset demonstrate that our approaches provide substantial improvements over previously published results.", "labels": [], "entities": []}, {"text": "By analyzing the results, we gain some insight into the advantages of different joint models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic extraction of opinions from text has attracted considerable attention in recent years.", "labels": [], "entities": [{"text": "Automatic extraction of opinions from text", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.876580168803533}]}, {"text": "In particular, significant research has focused on extracting detailed information for opinions at the finegrained level, e.g. identifying opinion expressions within a sentence and predicting phrase-level polarity and intensity.", "labels": [], "entities": []}, {"text": "The ability to extract finegrained opinion information is crucial in supporting many opinion-mining applications such as opinion summarization, opinion-oriented question answering and opinion retrieval.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 121, "end_pos": 142, "type": "TASK", "confidence": 0.7623952031135559}, {"text": "opinion-oriented question answering", "start_pos": 144, "end_pos": 179, "type": "TASK", "confidence": 0.6078563332557678}, {"text": "opinion retrieval", "start_pos": 184, "end_pos": 201, "type": "TASK", "confidence": 0.8028980493545532}]}, {"text": "In this paper, we focus on the problem of identifying opinion expressions and classifying their attributes.", "labels": [], "entities": []}, {"text": "We consider as an opinion expression any subjective expression that explicitly or implicitly conveys emotions, sentiment, beliefs, opinions (i.e. private states) ( ), and consider two key attributes -polarity and intensityfor characterizing the opinions.", "labels": [], "entities": [{"text": "intensityfor", "start_pos": 213, "end_pos": 225, "type": "METRIC", "confidence": 0.9492753744125366}]}, {"text": "Consider the sentence in, for example.", "labels": [], "entities": []}, {"text": "The phrases \"a bias in favor of\" and \"being severely criticized\" are opinion expressions containing positive sentiment with medium intensity and negative sentiment with high intensity, respectively.", "labels": [], "entities": []}, {"text": "Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.7627984285354614}, {"text": "attribute classification", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7191995531320572}]}, {"text": "The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.6868911683559418}]}, {"text": "The second task is usually treated as a binary or multi-class classification problem, where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence).", "labels": [], "entities": []}, {"text": "Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes.", "labels": [], "entities": []}, {"text": "However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification.", "labels": [], "entities": [{"text": "attribute classification", "start_pos": 129, "end_pos": 153, "type": "TASK", "confidence": 0.7216953039169312}]}, {"text": "Limited work has been done on the joint modeling of opinion expression extraction and attribute classification.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.7428942720095316}, {"text": "attribute classification", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.7189610153436661}]}, {"text": "first proposed a joint sequence labeling approach to extract opinion expressions and label them with polarity and intensity.", "labels": [], "entities": []}, {"text": "Their approach treats both expression extraction and attribute classification as token-level se-", "labels": [], "entities": [{"text": "expression extraction", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7775053083896637}, {"text": "attribute classification", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7282883077859879}]}], "datasetContent": [{"text": "All our experiments were conducted on the MPQA corpus ( ), a widely used corpus for fine-grained opinion analysis.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9538565576076508}, {"text": "fine-grained opinion analysis", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.6557183762391409}]}, {"text": "We used the same evaluation setting as in, where 135 documents were used for development and 10-fold cross-validation was performed on a different set of 400 documents.", "labels": [], "entities": []}, {"text": "Each training fold consists of sentences labeled with opinion expression boundaries and each expression is labeled with polarity and intensity.", "labels": [], "entities": []}, {"text": "shows some statistics of the evaluation data.", "labels": [], "entities": []}, {"text": "We used precision, recall and F1 as evaluation metrics for opinion extraction and computed them using both proportional matching and binary matching criteria.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9994712471961975}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9989965558052063}, {"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9988303780555725}, {"text": "opinion extraction", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8071868121623993}]}, {"text": "Proportional matching considers the overlapping proportion of a predicted expression s |s * | /|S * |, where Sand S * denote the set of predicted opinion expressions and the set of correct opinion expressions, respectively.", "labels": [], "entities": []}, {"text": "Binary matching is a more relaxed metric that considers a predicted opinion expression to be correct if it overlaps with a correct opinion expression.", "labels": [], "entities": [{"text": "Binary matching", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7187760770320892}]}, {"text": "We experimented with the following models: (1) PIPELINE: first extracts the spans of opinion expressions using the semi-CRF model in Section 3.1, and then assigns polarity and intensity to the extracted opinion expressions using MaxEnt models in Section 3.2.", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9958283305168152}]}, {"text": "Note that the label space of the MaxEnt models does not include \u2205 since they assume that all the opinion expressions extracted by the previous stage are correct.", "labels": [], "entities": []}, {"text": "(2) JSL: the joint sequence labeling method described in Section 3.3.1.", "labels": [], "entities": [{"text": "JSL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8621565103530884}, {"text": "joint sequence labeling", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.5483061472574869}]}, {"text": "(3) HJSL: the hierarchical joint sequence labeling method described in Section 3.3.2.", "labels": [], "entities": [{"text": "hierarchical joint sequence labeling", "start_pos": 14, "end_pos": 50, "type": "TASK", "confidence": 0.5271566435694695}]}, {"text": "(4) JI-PROB: the joint inference method using probability-based estimates (Equation 6).", "labels": [], "entities": []}, {"text": "(5) JI-LOSS: the joint inference method using loss-based estimates (Equation 7).", "labels": [], "entities": []}, {"text": "We also compare our results with previously published results from Choi and Cardie (2010) on the same task.", "labels": [], "entities": []}, {"text": "All our models are log linear models.", "labels": [], "entities": []}, {"text": "We use L-BFGS with L2 regularization for training and set the regularization parameter to 1.0.", "labels": [], "entities": []}, {"text": "We set the scaling parameter \u03b1 in JI-PROB and JI-LOSS via grid search over values between 0.1 and 1 with increments of 0.1 using the development set.", "labels": [], "entities": [{"text": "JI-PROB", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9447629451751709}, {"text": "JI-LOSS", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9202383756637573}]}, {"text": "We consider the same set of features described in Section 4 in all the models.", "labels": [], "entities": []}, {"text": "For the pipeline and joint inference models where the opinion segmentator and attribute classifiers are separately trained, we employ basic features plus segmentation-specific features in the opinion segmentator; and employ basic features plus attribute-specific features in the attribute classifiers.", "labels": [], "entities": []}, {"text": "Previous work (Johansson and Moschitti, 2011) showed that reranking is effective in improving the pipeline of opinion expression extraction and polarity classification.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 110, "end_pos": 139, "type": "TASK", "confidence": 0.6955171823501587}, {"text": "polarity classification", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.7567345201969147}]}, {"text": "We extended their approach to handle both polarity and intensity and investigated the effect of reranking on both the pipeline and joint models.", "labels": [], "entities": []}, {"text": "For the pipeline model, we generated 64-best (distinct) output with 4-best labeling at each pipeline stage; for the joint models, we generated 50-best (distinct) output using Viterbi-like dynamic programming.", "labels": [], "entities": []}, {"text": "We trained the reranker using the online PassiveAggressive algorithm) as in with 100 iterations and a regularization constant C = 0.01.", "labels": [], "entities": []}, {"text": "For features, we included the probability output by the base models, the polarity and intensity of each pair of extracted opinion expressions, and the word sequence and the POS sequence between the adjacent pairs of extracted opinion expressions.", "labels": [], "entities": [{"text": "POS sequence", "start_pos": 173, "end_pos": 185, "type": "METRIC", "confidence": 0.9651120007038116}]}, {"text": "shows the reranking performance (F1) for all subtasks.", "labels": [], "entities": [{"text": "reranking", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.978171169757843}, {"text": "F1)", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9915867447853088}]}, {"text": "We can see that after reranking, JI-LOSS still provides the best performance and HJSL achieves comparable performance to PIPELINE.", "labels": [], "entities": []}, {"text": "We also found that reranking leads to less performance gain for the joint inference approaches than for the joint learning approaches.", "labels": [], "entities": []}, {"text": "This is because the k-best output of JI-PROB and JI-LOSS present less diversity than JSL and HJSL.", "labels": [], "entities": []}, {"text": "A similar issue for reranking has also been discussed in.", "labels": [], "entities": [{"text": "reranking", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.9570657014846802}]}, {"text": "As an additional experiment, we consider a supervised sentence-level sentiment classification task using features derived from the prediction output of different opinion extraction models.", "labels": [], "entities": [{"text": "sentence-level sentiment classification task", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.7607364431023598}]}, {"text": "As a stan-: Sentence-level Sentiment Classification dard baseline, we train a MaxEnt classifier using unigrams, bigrams and opinion lexicon features extracted from the sentence.", "labels": [], "entities": []}, {"text": "Using the prediction output of an opinion extraction model, we construct features by using only words from the extracted opinion expressions, and include the predicted opinion attributes as additional features.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7355061173439026}]}, {"text": "We hypothesize that the more informative the extracted opinion expressions are, the more they can contribute to sentencelevel sentiment classification as features.", "labels": [], "entities": [{"text": "sentencelevel sentiment classification", "start_pos": 112, "end_pos": 150, "type": "TASK", "confidence": 0.7825054824352264}]}, {"text": "shows the results in terms of classification accuracy and F1 score in each sentiment category.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9737793803215027}, {"text": "F1 score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9917081594467163}]}, {"text": "BOW is the standard MaxEnt baseline.", "labels": [], "entities": [{"text": "BOW", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9348528981208801}, {"text": "MaxEnt baseline", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.8690371513366699}]}, {"text": "We can see that using features constructed from the opinion expressions always improved the performance.", "labels": [], "entities": []}, {"text": "This confirms the informativeness of the extracted opinion expressions.", "labels": [], "entities": []}, {"text": "In particular, using the opinion expressions extracted by JI-LOSS gives the best performance among all the baselines in all evaluation criteria.", "labels": [], "entities": [{"text": "JI-LOSS", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8002261519432068}]}, {"text": "This is consistent with its superior performance in our previous experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the evaluation corpus", "labels": [], "entities": []}, {"text": " Table 2: Opinion Expression Extraction (Proportional Matching). In all tables, we use bold to indicate the  highest score among all the methods; use  *  to indicate statistically significant improvements (p < 0.05) over  all the other methods under the paired-t test; use  \u2020 to denote statistically significance (p < 0.05) over the  pipeline baseline.", "labels": [], "entities": [{"text": "Opinion Expression Extraction", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6172234614690145}]}, {"text": " Table 3: Opinion Extraction with Correct Attributes (Proportional Matching)", "labels": [], "entities": [{"text": "Opinion Extraction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7471066117286682}]}, {"text": " Table 4: Opinion Extraction Results (Binary Matching)", "labels": [], "entities": [{"text": "Opinion Extraction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.702519029378891}, {"text": "Binary Matching", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.73970165848732}]}, {"text": " Table 6: Examples of mistakes that are made by the joint learning model but are corrected by the joint  inference model and vice versa. We use the same colored box notation as before, and use yellow color to  denote neutral sentiment.", "labels": [], "entities": []}, {"text": " Table 8: Sentence-level Sentiment Classification", "labels": [], "entities": [{"text": "Sentence-level Sentiment Classification", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.9272203246752421}]}]}