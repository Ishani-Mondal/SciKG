{"title": [{"text": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "labels": [], "entities": [{"text": "Finding and Describing Images with Sentences", "start_pos": 37, "end_pos": 81, "type": "TASK", "confidence": 0.8327168921629587}]}], "abstractContent": [{"text": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images.", "labels": [], "entities": [{"text": "Recursive Neural Networks (RNNs)", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.5896382729212443}]}, {"text": "However, the sentence vectors of previous models cannot accurately represent visually grounded meaning.", "labels": [], "entities": []}, {"text": "We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences.", "labels": [], "entities": []}, {"text": "Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence.", "labels": [], "entities": []}, {"text": "They are better able to abstract from the details of word order and syntactic expression.", "labels": [], "entities": []}, {"text": "DT-RNNs outperform other re-cursive and recurrent neural networks, kernel-ized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa.", "labels": [], "entities": []}, {"text": "They also give more similar representations to sentences that describe the same image.", "labels": [], "entities": []}], "introductionContent": [{"text": "Single word vector spaces are widely used) and successful at classifying single words and capturing their meaning.", "labels": [], "entities": []}, {"text": "Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (.", "labels": [], "entities": [{"text": "learning compositional meaning representations", "start_pos": 52, "end_pos": 98, "type": "TASK", "confidence": 0.7270760983228683}]}, {"text": "Similarly, classifying whole images into a fixed set of classes also achieves very high performance ().", "labels": [], "entities": []}, {"text": "However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label.", "labels": [], "entities": []}, {"text": "In this work, we introduce a model, illustrated in, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other.", "labels": [], "entities": []}, {"text": "We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space.", "labels": [], "entities": []}, {"text": "Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs)).", "labels": [], "entities": []}, {"text": "However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees.", "labels": [], "entities": []}, {"text": "The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a \"visual representation\" of the textual description.", "labels": [], "entities": []}, {"text": "DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurrent Neural Networks since they naturally focus on a sentence's action and its agents.", "labels": [], "entities": []}, {"text": "We evaluate and compare DT-RNN induced representations on their ability to use a sentence such as \"A man wearing a helmet jumps on his bike near a beach.\" to find images that show such a scene.", "labels": [], "entities": []}, {"text": "The goal is to learn sentence representations that capture A man wearing a helmet jumps on his bike near a beach.", "labels": [], "entities": []}], "datasetContent": [{"text": ". During training, we take the last hidden vector of the sentence chain and propagate the error into that.", "labels": [], "entities": []}, {"text": "It is also this vector that is used to represent the sentence.", "labels": [], "entities": []}, {"text": "Other possible comparisons are to the very different models mentioned in the related work section.", "labels": [], "entities": []}, {"text": "These models use a lot more task-specific engineering, such as running object detectors with bounding boxes, attribute classifiers, scene classifiers, CRFs for composing the sentences, etc.", "labels": [], "entities": []}, {"text": "Another line of work uses large sentence-image aligned resources (), whereas we focus on easily obtainable training data of each modality separately and a rather small multimodal corpus.", "labels": [], "entities": []}, {"text": "In our experiments we split the data into 800 training, 100 development and 100 test images.", "labels": [], "entities": []}, {"text": "Since there are 5 sentences describing each image, we have 4000 training sentences and 500 testing sentences.", "labels": [], "entities": []}, {"text": "The dataset has 3020 unique words, half of which only appear once.", "labels": [], "entities": []}, {"text": "Hence, the unsupervised, pre-trained semantic word vector representations are crucial.", "labels": [], "entities": []}, {"text": "Word vectors are not fine tuned during training.", "labels": [], "entities": [{"text": "Word vectors", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6298580318689346}]}, {"text": "Hence, the main parameters are the DT-RNN's W l\u00b7 , W r\u00b7 or the semantic matrices of which there are 141 and the image mapping WI . For both DT-RNNs the weight matrices are initialized to block identity matrices plus Gaussian noise.", "labels": [], "entities": []}, {"text": "Word vectors and hidden vectors are set o length 50.", "labels": [], "entities": []}, {"text": "Using the development split, we found \u03bb = 0.08 and the learning rate of AdaGrad to 0.0001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.9666596353054047}, {"text": "AdaGrad", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.8935791850090027}]}, {"text": "The best model uses a margin of \u2206 = 3.", "labels": [], "entities": []}, {"text": "Inspired by and we also compare to kernelized Canonical Correlation Analysis (kCCA).", "labels": [], "entities": [{"text": "kernelized Canonical Correlation Analysis", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.6268167644739151}]}, {"text": "We use the average of word vectors for describing sentences and the same powerful image vectors as before.", "labels": [], "entities": []}, {"text": "We use the code of.", "labels": [], "entities": []}, {"text": "Technically, one could combine the recently introduced deep CCA and train the recursive neural network architectures with the CCA objective.", "labels": [], "entities": []}, {"text": "We leave this to future work.", "labels": [], "entities": []}, {"text": "With linear kernels, kCCA does well for image search but is worse for sentence self similarity and describing images with sentences close-by in embedding space.", "labels": [], "entities": [{"text": "image search", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.7682291269302368}, {"text": "sentence self similarity", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.6377754410107931}]}, {"text": "All other models are trained by replacing the DT-RNN function in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 65, "end_pos": 67, "type": "DATASET", "confidence": 0.8102939128875732}]}], "tableCaptions": [{"text": " Table 1: Left: Comparison of methods for sentence similarity judgments. Lower numbers are better since they indicate  that sentences describing the same image rank more highly (are closer). The ranks are out of the 500 sentences in the  test set. Center: Comparison of methods for image search with query sentences. Shown is the average rank of the  single correct image that is being described. Right: Average rank of a correct sentence description for a query image.", "labels": [], "entities": [{"text": "sentence similarity judgments", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7419029076894125}]}, {"text": " Table 3: Evaluation comparison between mean rank of  the closest correct image or sentence (lower is better )  with recall at different thresholds (higher is better, ).  With one exception (R@5, bottom table), the SDT-RNN  outperforms the other two models and all other models  we did not include here.", "labels": [], "entities": [{"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.998540997505188}, {"text": "R@5", "start_pos": 191, "end_pos": 194, "type": "METRIC", "confidence": 0.9316936731338501}]}]}