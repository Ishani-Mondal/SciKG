{"title": [{"text": "Segmentation for Efficient Supervised Language Annotation with an Explicit Cost-Utility Tradeoff", "labels": [], "entities": [{"text": "Supervised Language Annotation", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6359160641829172}]}], "abstractContent": [{"text": "In this paper, we study the problem of manually correcting automatic annotations of natural language in as efficient a manner as possible.", "labels": [], "entities": [{"text": "manually correcting automatic annotations of natural language", "start_pos": 39, "end_pos": 100, "type": "TASK", "confidence": 0.8050173946789333}]}, {"text": "We introduce a method for automatically segmenting a corpus into chunks such that many uncertain labels are grouped into the same chunk, while human supervision can be omitted altogether for other segments.", "labels": [], "entities": []}, {"text": "A tradeoff must be found for segment sizes.", "labels": [], "entities": []}, {"text": "Choosing short segments allows us to reduce the number of highly confident labels that are supervised by the annotator, which is useful because these labels are often already correct and supervising correct labels is a waste of effort.", "labels": [], "entities": []}, {"text": "In contrast, long segments reduce the cognitive effort due to context switches.", "labels": [], "entities": []}, {"text": "Our method helps find the segmentation that optimizes supervision efficiency by defining user models to predict the cost and utility of supervising each segment and solving a constrained optimization problem balancing these contradictory objectives.", "labels": [], "entities": []}, {"text": "A user study demonstrates noticeable gains over pre-segmented, confidence-ordered baselines on two natural language processing tasks: speech transcription and word segmentation.", "labels": [], "entities": [{"text": "speech transcription", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.7088850289583206}, {"text": "word segmentation", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.747512936592102}]}], "introductionContent": [{"text": "Many natural language processing (NLP) tasks require human supervision to be useful in practice, be it to collect suitable training material or to meet some desired output quality.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.7891673445701599}]}, {"text": "Given the high cost of human intervention, how to minimize the supervision effort is an important research problem.", "labels": [], "entities": []}, {"text": "Previous works in areas such as active learning, post edit-(a) It was a bright cold (they) in (apron), and (a) clocks were striking thirteen.", "labels": [], "entities": []}, {"text": "(b) It was a bright cold (they) in (apron), and (a) clocks were striking thirteen.", "labels": [], "entities": []}, {"text": "(c) It was a bright cold (they) in (apron), and (a) clocks were striking thirteen.", "labels": [], "entities": []}, {"text": "Figure 1: Three automatic transcripts of the sentence \"It was a bright cold day in April, and the clocks were striking thirteen\", with recognition errors in parentheses.", "labels": [], "entities": []}, {"text": "The underlined parts are to be corrected by a human for (a) sentences, (b) words, or (c) the proposed segmentation.", "labels": [], "entities": []}, {"text": "ing, and interactive pattern recognition have investigated this question with notable success).", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.6946772038936615}]}, {"text": "The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system's predictions.", "labels": [], "entities": []}, {"text": "Sentences with the lowest confidence are then used as the data to be annotated (a)).", "labels": [], "entities": []}, {"text": "However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.996335506439209}]}, {"text": "In these cases, it is possible to achieve much higher benefit per annotated word by annotating sub-sentential units ( greatly across instances.", "labels": [], "entities": []}, {"text": "This is particularly important in the context of choosing segments to annotate, as human annotators heavily rely on semantics and context information to process language, and intuitively, a consecutive sequence of words can be supervised faster and more accurately than the same number of words spread out over several locations in a text.", "labels": [], "entities": []}, {"text": "This intuition can also be seen in our empirical data in, which shows that for the speech transcription and word segmentation tasks described later in Section 5, short segments had a longer annotation time per word.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.6855446994304657}]}, {"text": "Based on this fact, we argue it would be desirable to present the annotator with a segmentation of the data into easily supervisable chunks that are both large enough to reduce the number of context switches, and small enough to prevent unnecessary annotation).", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew strategy for natural language supervision tasks that attempts to optimize supervision efficiency by choosing an appropriate segmentation.", "labels": [], "entities": []}, {"text": "It relies on a user model that, given a specific segment, predicts the cost and the utility of supervising that segment.", "labels": [], "entities": []}, {"text": "Given this user model, the goal is to find a segmentation that minimizes the total predicted cost while maximizing the utility.", "labels": [], "entities": []}, {"text": "We balance these two criteria by defining a constrained optimization problem in which one criterion is the optimization objective, while the other criterion is used as a constraint.", "labels": [], "entities": []}, {"text": "Doing so allows specifying practical optimization goals such as \"remove as many errors as possible given a limited time budget,\" or \"annotate data to obtain some required classifier accuracy in as little time as possible.\"", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.8941824436187744}]}, {"text": "Solving this optimization task is computationally difficult, an NP-hard problem.", "labels": [], "entities": []}, {"text": "Nevertheless, we demonstrate that by making realistic assumptions about the segment length, an optimal solution can be found using an integer linear programming formulation for mid-sized corpora, as are common for supervised annotation tasks.", "labels": [], "entities": []}, {"text": "For larger corpora, we provide simple heuristics to obtain an approximate solution in a reasonable amount of time.", "labels": [], "entities": []}, {"text": "Experiments over two example scenarios demonstrate the usefulness of our method: Post editing for speech transcription, and active learning for Japanese word segmentation.", "labels": [], "entities": [{"text": "Post editing", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.6975678652524948}, {"text": "speech transcription", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7448441982269287}, {"text": "Japanese word segmentation", "start_pos": 144, "end_pos": 170, "type": "TASK", "confidence": 0.590755452712377}]}, {"text": "Our model predicts noticeable efficiency gains, which are confirmed in experiments with human annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present experimental results examining the effectiveness of the proposed method over two tasks: speech transcription and Japanese word segmentation.", "labels": [], "entities": [{"text": "speech transcription", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7398956269025803}, {"text": "Japanese word segmentation", "start_pos": 141, "end_pos": 167, "type": "TASK", "confidence": 0.6730332473913828}]}, {"text": "Accurate speech transcripts area much-demanded NLP product, useful by themselves, as training material for ASR, or as input for follow-up tasks like speech translation.", "labels": [], "entities": [{"text": "ASR", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9943488240242004}, {"text": "speech translation", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7462641000747681}]}, {"text": "With recognition accuracies plateauing, manually correcting (post editing) automatic speech transcripts has become popular.", "labels": [], "entities": []}, {"text": "Common approaches are to identify words () or (sub-)sentences (Sperber et al., 2013) of low confidence, and have a human editor correct these.", "labels": [], "entities": []}, {"text": "(2011) have proposed a pointwise method for Japanese word segmentation that can be trained using partially annotated sentences, which makes it attractive in combination with active learning, as well as our segmentation method.", "labels": [], "entities": [{"text": "Japanese word segmentation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6294963260491689}]}, {"text": "The authors released their method as a software package \"KyTea\" that we employed in this user study.", "labels": [], "entities": []}, {"text": "We used KyTea's active learning domain adaptation toolkit 8 as a baseline.", "labels": [], "entities": [{"text": "KyTea's active learning domain adaptation toolkit 8", "start_pos": 8, "end_pos": 59, "type": "DATASET", "confidence": 0.8225111477077007}]}, {"text": "For data, we used the Balanced Corpus of Contemporary Written Japanese (BCCWJ), created by, with the internet Q&A subcorpus as in-domain data, and the whitepaper subcorpus as background data, a domain adaptation scenario.", "labels": [], "entities": [{"text": "Balanced Corpus of Contemporary Written Japanese (BCCWJ)", "start_pos": 22, "end_pos": 78, "type": "DATASET", "confidence": 0.7676254212856293}]}, {"text": "Sentences were drawn from the in-domain corpus, and the manually annotated data was then used to train KyTea, along with the pre-annotated background data.", "labels": [], "entities": [{"text": "KyTea", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.7699183821678162}]}, {"text": "The goal (objective function) was to improve KyTea's classification accuracy on an indomain test set, given a constrained time budget of 30 minutes.", "labels": [], "entities": [{"text": "KyTea's classification", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.47175727287928265}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9608694911003113}]}, {"text": "There were again 2 supervision modes: ANNOTATE and SKIP.", "labels": [], "entities": [{"text": "ANNOTATE", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9721590876579285}]}, {"text": "Note that this is essentially a batch active learning setup with only one iteration.", "labels": [], "entities": []}, {"text": "We conducted experiments with one expert with several years of experience with Japanese word segmentation annotation, and three non-expert native speakers with no prior experience.", "labels": [], "entities": [{"text": "Japanese word segmentation annotation", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.6796808913350105}]}, {"text": "Japanese word segmentation is not a trivial task, so we provided non-experts with training, including explanation of the segmentation standard, a supervised test with immediate feedback and explanations, and hands-on training to get used to the annotation software.", "labels": [], "entities": [{"text": "Japanese word segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5525685449441274}]}, {"text": "Supervision time was predicted via GP regression (cf. Section 4.1), using the segment length and mean confidence as input features.", "labels": [], "entities": [{"text": "mean confidence", "start_pos": 97, "end_pos": 112, "type": "METRIC", "confidence": 0.9198804795742035}]}, {"text": "As before, the output variable was assumed subject to additive Gaussian noise with zero mean and 5 seconds variance.", "labels": [], "entities": []}, {"text": "To obtain training data for these models, each participant annotated about 500 example instances, drawn from the adaptation corpus, grouped into segments and balanced regarding segment length and difficulty.", "labels": [], "entities": []}, {"text": "For utility modeling (cf. Section 4.3), we first normalized KyTea's confidence scores, which are given in terms of SVM margin, using a sigmoid function).", "labels": [], "entities": [{"text": "SVM margin", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.6133882999420166}]}, {"text": "The normalization parameter was se-lected so that the mean confidence on a development set corresponded to the actual classifier accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9562694430351257}]}, {"text": "We derive our measure of classifier improvement for correcting a segment by summing over one minus the calibrated confidence for each of its tokens.", "labels": [], "entities": [{"text": "correcting a segment", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8656412363052368}]}, {"text": "To analyze how well this measure describes the actual training utility, we trained KyTea using the background data plus disjoint groups of 100 in-domain instances with similar probabilities and measured the achieved reduction of prediction errors.", "labels": [], "entities": []}, {"text": "The correlation between each group's mean utility and the achieved error reduction was 0.87.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.9785054326057434}]}, {"text": "Note that we ignore the decaying returns usually observed as more data is added to the training set.", "labels": [], "entities": []}, {"text": "Also, we did not attempt to model user errors.", "labels": [], "entities": []}, {"text": "Employing a constant base error rate, as in the transcription scenario, would change segment utilities only by a constant factor, without changing the resulting segmentation.", "labels": [], "entities": []}, {"text": "After creating the user models, we conducted the main experiment, in which each participant annotated data that was selected from a pool of 1000 in-domain sentences using two strategies.", "labels": [], "entities": []}, {"text": "The first, baseline strategy was as proposed by.", "labels": [], "entities": []}, {"text": "Queries are those instances with the lowest confidence scores.", "labels": [], "entities": []}, {"text": "Each query is then extended to the left and right, until a word boundary is predicted.", "labels": [], "entities": []}, {"text": "This strategy follows similar reasoning as was the premise to this paper: To decide whether or not a position in a text corresponds to a word boundary, the annotator has to acquire surrounding context information.", "labels": [], "entities": []}, {"text": "This context acquisition is relatively time consuming, so he might as well label the surrounding instances with little additional effort.", "labels": [], "entities": [{"text": "context acquisition", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7250809967517853}]}, {"text": "The second strategy was our proposed, more principled approach.", "labels": [], "entities": []}, {"text": "Queries of both methods were shuffled to minimize bias due to learning effects.", "labels": [], "entities": []}, {"text": "Finally, we trained KyTea using the results of both methods, and compared the achieved classifier improvement and supervision times.", "labels": [], "entities": []}, {"text": "summarizes the results of our experiment.", "labels": [], "entities": []}, {"text": "It shows that the annotations by each participant resulted in a better classifier for the proposed method than the baseline, but also took up considerably more time, a less clear improvement than for the transcription task.", "labels": [], "entities": []}, {"text": "In fact, the total error for time predictions was as high as 12.5% on average,  where the baseline method tended take less time than predicted, the proposed method more time.", "labels": [], "entities": [{"text": "error", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.5217421054840088}]}, {"text": "This is in contrast to a much lower total error (within 1%) when cross-validating our user model training data.", "labels": [], "entities": []}, {"text": "This is likely due to the fact that the data for training the user model was selected in a balanced manner, as opposed to selecting difficult examples, as our method is prone to do.", "labels": [], "entities": []}, {"text": "Thus, we may expect much better predictions when selecting user model training data that is more similar to the test case.", "labels": [], "entities": []}, {"text": "Plotting classifier accuracy over annotation time draws a clearer picture.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9472767114639282}]}, {"text": "Let us first analyze the results for the expert annotator.", "labels": [], "entities": []}, {"text": "(E.1) shows that the proposed method resulted in consistently better results, indicating that time predictions were still effective.", "labels": [], "entities": []}, {"text": "Note that this comparison may put the proposed method at a slight disadvantage by comparing intermediate results despite optimizing globally.", "labels": [], "entities": []}, {"text": "Word segmentation is the first step in NLP for languages that are commonly written without word boundaries, such as Japanese and Chinese.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6820546090602875}]}, {"text": "We apply our method to a task in which we domain-adapt a word segmentation classifier via active learning.", "labels": [], "entities": [{"text": "word segmentation classifier", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.7826095620791117}]}, {"text": "In this experiment, participants annotated whether or not a word boundary occurred at certain positions in a Japanese sentence.", "labels": [], "entities": []}, {"text": "The tokens to be grouped into segments are positions between adjacent characters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Transcription task results. For each user, the  resulting WER [%] after supervision is shown, along with  the time [min] they needed. The unsupervised WER was  19.96%.", "labels": [], "entities": [{"text": "WER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.996221661567688}, {"text": "WER", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9696983098983765}]}, {"text": " Table 2: Word segmentation task results, for our ex- pert and 3 non-expert participants. For each participant,  the resulting classifier accuracy [%] after supervision is  shown, along with the time [min] they needed. The unsu- pervised accuracy was 95.14%.", "labels": [], "entities": [{"text": "Word segmentation task", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8376752734184265}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.870823860168457}, {"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.6067081689834595}]}]}