{"title": [{"text": "Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.8220423460006714}]}], "abstractContent": [{"text": "Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.7448614835739136}]}, {"text": "In this work, we introduce a technique for learning arbitrary, rule-local, non-linear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models.", "labels": [], "entities": []}, {"text": "To demonstrate the value of our technique , we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts.", "labels": [], "entities": []}, {"text": "We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform.", "labels": [], "entities": []}, {"text": "We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9971780776977539}]}], "introductionContent": [{"text": "Linear models using log-transformed probabilities as features have emerged as the dominant model in MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9936357736587524}]}, {"text": "This practice can be traced back to the IBM noisy channel models (, which decompose decoding into the product of a translation model (TM) and a language model (LM), motivated by Bayes' Rule.", "labels": [], "entities": []}, {"text": "When introduced a log-linear model for translation (a linear sum of log-space features), they noted that the noisy channel model was a special case of their model using log probabilities.", "labels": [], "entities": [{"text": "translation", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9796725511550903}]}, {"text": "This * This work was conducted as part of the first author's Ph.D. work at Carnegie Mellon University.", "labels": [], "entities": []}, {"text": "same formulation persisted even after the introduction of MERT, which optimizes a linear model; again, using two log probability features (TM and LM) with equal weight recovered the noisy channel model.", "labels": [], "entities": [{"text": "MERT", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.7172088623046875}]}, {"text": "Yet systems now use many more features, some of which are not even probabilities.", "labels": [], "entities": []}, {"text": "We no longer believe that equal weights between the TM and LM provides optimal translation quality; the probabilities in the TM do not obey the chain rule nor Bayes' rule, nullifying several theoretical mathematical justifications for multiplying probabilities.", "labels": [], "entities": []}, {"text": "The story of multiplying probabilities may just amount to heavily penalizing small values.", "labels": [], "entities": []}, {"text": "The community has abandoned the original motivations fora linear interpolation of two logtransformed features.", "labels": [], "entities": []}, {"text": "Is there empirical evidence that we should continue using this particular transformation?", "labels": [], "entities": []}, {"text": "Do we have any reason to believe it is better than other non-linear transformations?", "labels": [], "entities": []}, {"text": "To answer these, we explore the issue of non-linearity in models for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9877646565437317}]}, {"text": "In the process, we will discuss the impact of linearity on feature engineering and develop a general mechanism for learning a class of non-linear transformations of real-valued features.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7856514155864716}]}, {"text": "Applying a non-linear transformation such as log to features is one way of achieving a non-linear response function, even if those features are aggregated in a linear model.", "labels": [], "entities": []}, {"text": "Alternatively, we could achieve a non-linear response using a natively nonlinear model such as a SVM () or RankBoost ().", "labels": [], "entities": []}, {"text": "However, MT is a structured prediction problem, in which a full hypothesis is composed of partial hypotheses.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9919402003288269}]}, {"text": "MT decoders take advantage of the fact that the model score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces ( \u00a72) -currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search.", "labels": [], "entities": [{"text": "MT decoders", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9051601886749268}]}, {"text": "Another alternative is incorporating a recurrent neural network or an additive neural network (.", "labels": [], "entities": []}, {"text": "While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features.", "labels": [], "entities": []}, {"text": "In this article, we discuss background ( \u00a72), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models ( \u00a73), present our experimental setup ( \u00a75), empirically verify the importance of non-linear feature transformations in MT and demonstrate that discretization can be used to recover non-linear transformations ( \u00a76), discuss related work ( \u00a77), and conclude ( \u00a78).", "labels": [], "entities": [{"text": "MT", "start_pos": 316, "end_pos": 318, "type": "TASK", "confidence": 0.9665411710739136}]}], "datasetContent": [{"text": "Formalism: In our experiments, we use a hierarchical phrase-based translation model.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6482428014278412}]}, {"text": "A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically.", "labels": [], "entities": [{"text": "phrase translations", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7197851538658142}]}, {"text": "In addition, hierarchical grammar rules are extracted where phrases are nested.", "labels": [], "entities": []}, {"text": "In general, our choice of formalism is rather unimportant -our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems.", "labels": [], "entities": []}, {"text": "Decoder: For decoding, we will use cdec), a multi-pass decoder that supports syntactic translation models and sparse features.", "labels": [], "entities": []}, {"text": "Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder.", "labels": [], "entities": []}, {"text": "We run PRO for 30 iterations as suggested by.", "labels": [], "entities": []}, {"text": "The PRO optimizer internally uses a L-BFGS optimizer with the default 2 regularization implemented in cdec.", "labels": [], "entities": []}, {"text": "Any additional regularization is explicitly noted.", "labels": [], "entities": []}, {"text": "Baseline Features: We use the baseline features produced by Lopez' suffix array grammar extractor (, which is distributed with cdec.", "labels": [], "entities": []}, {"text": "6 All code at http://github.com/jhclark/cdec Bidirectional lexical log-probabilities, the coherent phrasal translation log-probability, target word count, glue rule count, source OOV count, target OOV count, and target language model logprobability.", "labels": [], "entities": []}, {"text": "Note that these features maybe simplified or removed as specified in each experimental condition.", "labels": [], "entities": []}, {"text": "Zh\u2192En  Czech resources: We also construct a Czech\u2192English system based on the CzEng 1.0 data ().", "labels": [], "entities": [{"text": "CzEng 1.0 data", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.8069547414779663}]}, {"text": "First, we lowercased and performed sentence-level deduplication of the data.", "labels": [], "entities": []}, {"text": "7 Then, we uniformly sampled a training set of 1M sentences (sections 1 -97) along with a weighttuning set (section 98), hyperparameter-tuning (section 99), and test set (section 99) from the paraweb domain contained of CzEng.", "labels": [], "entities": []}, {"text": "8 Sentences less than 5 words were discarded due to noise.", "labels": [], "entities": []}, {"text": "Evaluation: We quantify increases in translation quality using case-insensitive BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9850698113441467}]}, {"text": "We control for test set variation and optimizer instability by averaging over multiple optimizer replicas: Top: Translation quality for systems with and without the typical log transform.", "labels": [], "entities": [{"text": "Translation", "start_pos": 112, "end_pos": 123, "type": "TASK", "confidence": 0.9492889046669006}]}, {"text": "Bottom: Translation quality for systems using discretization and structured regularization with probabilities P or counts C as the input of discretization.", "labels": [], "entities": [{"text": "Translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9728426337242126}]}, {"text": "MNR P consistently recovers or outperforms a state-of-the-art system, but without any assumptions about how to transform the initial features.", "labels": [], "entities": [{"text": "MNR P", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.49436041712760925}]}, {"text": "All scores are averaged over 3 end-to-end optimizer replications.", "labels": [], "entities": []}, {"text": "denotes significantly different than log probs (row2) with p(CHANCE) < 0.01 under Clark et al. and \u2020 is likewise used with regard to P (row 1).", "labels": [], "entities": [{"text": "CHANCE", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9845276474952698}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics: number of parallel sentences.", "labels": [], "entities": []}, {"text": " Table 2: Translation quality for Cz\u2192En system with  varying bits for discretization. For all other experiments,  we tune the number of bits on held-out data.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9655600190162659}]}]}