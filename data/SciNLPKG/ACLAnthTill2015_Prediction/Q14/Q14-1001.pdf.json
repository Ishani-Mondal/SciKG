{"title": [{"text": "Heterogeneous Networks and Their Applications: Scientometrics, Name Disambiguation, and Topic Modeling", "labels": [], "entities": [{"text": "Name Disambiguation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8212459087371826}, {"text": "Topic Modeling", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.8066368997097015}]}], "abstractContent": [{"text": "We present heterogeneous networks as away to unify lexical networks with relational data.", "labels": [], "entities": []}, {"text": "We build a unified ACL Anthology network, tying together the citation, author collaboration, and term-cooccurence networks with affiliation and venue relations.", "labels": [], "entities": [{"text": "ACL Anthology network", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.7615572611490885}]}, {"text": "This representation proves to be convenient and allows problems such as name disambiguation, topic modeling, and the measurement of scientific impact to be easily solved using only this network and off-the-shelf graph algorithms.", "labels": [], "entities": [{"text": "name disambiguation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8315751254558563}, {"text": "topic modeling", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.8225009441375732}]}], "introductionContent": [{"text": "Graph-based methods have been used to great effect in NLP, on problems such as word sense disambiguation (, summarization (), and dependency parsing).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.6873790621757507}, {"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9781004190444946}, {"text": "dependency parsing", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.8123447597026825}]}, {"text": "Most previous studies of networks consider networks with only a single type of node, and in some cases using a network with a single type of node can bean oversimplified view if it ignores other types of relationships.", "labels": [], "entities": []}, {"text": "In this paper we will demonstrate heterogeneous networks, networks with multiple different types of nodes and edges, along with several applications of them.", "labels": [], "entities": []}, {"text": "The applications in this paper are not presented so much as robust attempts to out-perform the current state-of-the-art, but rather attempts at being competitive against top methods with little effort beyond the construction of the heterogeneous network.", "labels": [], "entities": []}, {"text": "Throughout this paper, we will use the data from the ACL Anthology Network (AAN) (, which contains additional metadata relationships not found in the ACL Anthology, as atypical heterogeneous network.", "labels": [], "entities": [{"text": "ACL Anthology Network (AAN)", "start_pos": 53, "end_pos": 80, "type": "DATASET", "confidence": 0.9314582745234171}, {"text": "ACL Anthology", "start_pos": 150, "end_pos": 163, "type": "DATASET", "confidence": 0.8979184031486511}]}, {"text": "The results in this paper should be generally applicable to other heterogeneous networks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We provide two separate evaluations in this section, one of the topics alone, and one extrinstic evaluation of the entire paper-topic model.", "labels": [], "entities": []}, {"text": "The variants of random walk topic models are compared against LDA and the relational topic model (RTM), each with 100 topics.", "labels": [], "entities": []}, {"text": "As RTM allows only a single type of relationship between documents, we use citations as the inter-document relationships.", "labels": [], "entities": []}, {"text": "One difficulty in evaluating this random-walk topic model intrinsically against a statistical topic model like RTM is that existing evaluation measures assume certain statistical properties of the topic, for example, that the topics are generated according to a Dirichlet prior.", "labels": [], "entities": []}, {"text": "Because of this, we choose instead to evaluate this topic model extrinsically with a downstream application.", "labels": [], "entities": []}, {"text": "We choose an information retrieval application, returning a ranked list of similar documents, given a reference document.", "labels": [], "entities": []}, {"text": "We evaluate five different methods: citation-RTM, LDA, the two versions of the random-walk topic model, and a simple word vector similarity baseline.", "labels": [], "entities": [{"text": "LDA", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.796042799949646}]}, {"text": "Similarity between documents with the topic models are determined by cosine similarity between the topic vectors of the two documents.", "labels": [], "entities": []}, {"text": "Word vector similarity determines the similarity between documents by taking the cosine similarity of their word vectors.", "labels": [], "entities": []}, {"text": "From these similarity scores, a ranked list is produced.", "labels": [], "entities": [{"text": "similarity", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9556786417961121}]}, {"text": "The document set for this task is the set of all papers appearing at ACL between 2000 and 2011.", "labels": [], "entities": [{"text": "ACL between 2000 and 2011", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.9384084701538086}]}, {"text": "The top 10 results returned by each method are pooled and manually evaluated with a relevance score between 1 and 10.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.9736881256103516}]}, {"text": "Thirty such result sets were manually annotated.", "labels": [], "entities": []}, {"text": "We then evaluate each method according to its discounted cumulative gain (DCG)).", "labels": [], "entities": [{"text": "discounted cumulative gain (DCG))", "start_pos": 46, "end_pos": 79, "type": "METRIC", "confidence": 0.8514527976512909}]}, {"text": "Performance of these methods is summarized in.", "labels": [], "entities": []}, {"text": "The co-occurence-based random walk topic model performed comparably with the best performer at this task, LDA, and there was no significant difference between the two at p < 0.05.", "labels": [], "entities": [{"text": "LDA", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.5426982641220093}]}, {"text": "Going forward, an important problem is to reconcile the co-occurence-and word-similarity-based formulations of this topic model, as the two formulations perform very differently in our two evaluations.", "labels": [], "entities": []}, {"text": "Heuristically, the co-occurence model seems to create good human-readable topics, while the word-similarity model creates topics that are more mathematically-coherent, but less human-readable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Network properties of the synthetic AAN  compared with the true AAN.", "labels": [], "entities": []}, {"text": " Table 2: Agreement of various impact measures  with the true latent impact.", "labels": [], "entities": []}, {"text": " Table 4: Performance of different networks and distance measures on the author name disambiguation task.  The performance measures are averaged over the sets of two, three, and four authors. Rand index is from", "labels": [], "entities": [{"text": "author name disambiguation task", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.6826847270131111}, {"text": "Rand index", "start_pos": 192, "end_pos": 202, "type": "METRIC", "confidence": 0.7588534951210022}]}, {"text": " Table 5: Top 10 words for several topics created by the co-occurence random walk topic model. The left  column is a manual label.", "labels": [], "entities": []}, {"text": " Table 6: Examples of entities associated with selected topics.", "labels": [], "entities": []}]}