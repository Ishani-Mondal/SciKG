{"title": [{"text": "The Benefits of a Model of Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "Standard agreement measures for interannota-tor reliability are neither necessary nor sufficient to ensure a high quality corpus.", "labels": [], "entities": []}, {"text": "Ina case study of word sense annotation, conventional methods for evaluating labels from trained an-notators are contrasted with a probabilistic annotation model applied to crowdsourced data.", "labels": [], "entities": [{"text": "word sense annotation", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7382130225499471}]}, {"text": "The annotation model provides far more information , including a certainty measure for each gold standard label; the crowdsourced data was collected at less than half the cost of the conventional approach.", "labels": [], "entities": [{"text": "certainty", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9836026430130005}]}], "introductionContent": [{"text": "The quality of annotated data for computational linguistics is generally assumed to be good enough if a few annotators can be shown to be consistent with one another.", "labels": [], "entities": []}, {"text": "Standard practice relies on metrics that measure consistency, either in an absolute way, or in a chance-adjusted fashion.", "labels": [], "entities": [{"text": "consistency", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9791594743728638}]}, {"text": "Such measures, however, merely report how often annotators agree, with no direct measure of corpus quality, nor of the quality of individual items.", "labels": [], "entities": []}, {"text": "We argue that high chance-adjusted interannotator agreement is neither necessary nor sufficient to ensure high quality gold-standard labels.", "labels": [], "entities": []}, {"text": "We contrast the use of agreement metrics with the use of probabilistic models to draw inferences about annotated data where the items have been labeled by many annotators.", "labels": [], "entities": []}, {"text": "A probabilistic model to fit many annotators' observed labels produces much more information about the annotated corpus.", "labels": [], "entities": []}, {"text": "In particular, there will be a confidence estimate for each ground truth label.", "labels": [], "entities": []}, {"text": "Probabilistic models of agreement and goldstandard inference have been used in psychometrics and marketing since the 1950s (e.g., IRT models or Bradley-Terry models) and in epidemiology since the 1970s (e.g., diagnostic disease prevalence models).", "labels": [], "entities": []}, {"text": "More recently, crowdsourcing has motivated their application to data annotation for machine learning.", "labels": [], "entities": []}, {"text": "The model we apply here assumes that annotators differ from one another in their accuracy at identifying the true label values, and that these true values occur at certain rates (their prevalence).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9985514283180237}]}, {"text": "To contrast the two approaches to creation of an annotated corpus, we present a case study of word sense annotation.", "labels": [], "entities": [{"text": "word sense annotation", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.6927884022394816}]}, {"text": "The items that were annotated are occurrences of words in their sentence contexts, and each label is a WordNet sense.", "labels": [], "entities": []}, {"text": "Each item has sense labels from up to twenty-five different annotators, collected through crowdsourcing.", "labels": [], "entities": []}, {"text": "Application of an annotation model does not require this many labels per item, and crowdsourced annotation data does not require a probabilistic model.", "labels": [], "entities": []}, {"text": "The case study, however, shows how the two benefit each other. quality.", "labels": [], "entities": [{"text": "quality", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9918404817581177}]}, {"text": "Our case study shows how we created a more reliable word sense corpus fora randomly selected subset of 45 of the same words, through crowdsourcing and application of the Dawid and Skene model.", "labels": [], "entities": []}, {"text": "The model yields a certainty measure for each labeled instance.", "labels": [], "entities": [{"text": "certainty measure", "start_pos": 19, "end_pos": 36, "type": "METRIC", "confidence": 0.9694765508174896}]}, {"text": "For most instances, the certainty of the estimated true labels is high, even on words where pairwise and chance-adjusted agreement of trained annotators were both low.", "labels": [], "entities": [{"text": "certainty", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.998898983001709}]}, {"text": "The paper first summarizes the limitations of agreement metrics, then presents the Dawid and Skene model.", "labels": [], "entities": []}, {"text": "The next two sections present a case study of the crowdsourced data, and the annotation results.", "labels": [], "entities": []}, {"text": "While many of the MASC words had low agreement from trained annotators on the small proportion of the data where agreement was assessed, the same words have many instances with highly confident labels estimated from the crowdsourced annotations.", "labels": [], "entities": [{"text": "MASC words", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.5419274866580963}]}, {"text": "In the discussion section, we compare the model-based labels to the labels from the trained annotators.", "labels": [], "entities": []}, {"text": "The final sections present related work and our conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Table of annotations y ind  instance ii and annotator jj.", "labels": [], "entities": []}]}