{"title": [{"text": "TREETALK: Composition and Compression of Trees for Image Descriptions", "labels": [], "entities": [{"text": "TREETALK", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9197589159011841}]}], "abstractContent": [{"text": "We present anew tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions.", "labels": [], "entities": []}, {"text": "We investigate two related tasks: image caption generalization and generation , where the former is an optional sub-task of the latter.", "labels": [], "entities": [{"text": "image caption generalization", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.9038337667783102}]}, {"text": "The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions , then to compose anew description by selectively combining the extracted (and optionally pruned) tree fragments.", "labels": [], "entities": []}, {"text": "Key algo-rithmic components are tree composition and compression, both integrating tree structure with sequence structure.", "labels": [], "entities": []}, {"text": "Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation.", "labels": [], "entities": [{"text": "image caption generalization", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.8576096296310425}]}, {"text": "In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The web is increasingly visual, with hundreds of billions of user contributed photographs hosted online.", "labels": [], "entities": []}, {"text": "A substantial portion of these images have some sort of accompanying text, ranging from keywords, to free text on web pages, to textual descriptions directly describing depicted image content (i.e. captions).", "labels": [], "entities": []}, {"text": "We tap into the last kind of text, using naturally occuring pairs of images with natural language descriptions to compose expressive descriptions for query images via tree composition and compression.", "labels": [], "entities": []}, {"text": "Such automatic image captioning efforts could potentially be useful for many applications: from automatic organization of photo collections, to facilitating image search with complex natural language queries, to enhancing web accessibility for the visually impaired.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7491338849067688}, {"text": "automatic organization of photo collections", "start_pos": 96, "end_pos": 139, "type": "TASK", "confidence": 0.6398385047912598}, {"text": "image search", "start_pos": 157, "end_pos": 169, "type": "TASK", "confidence": 0.7225605845451355}]}, {"text": "On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities.", "labels": [], "entities": []}, {"text": "There has been a recent spike in efforts to automatically describe visual content in natural language).", "labels": [], "entities": []}, {"text": "This reflects the longstanding understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags.", "labels": [], "entities": [{"text": "encoding the complexities and subtleties of image content", "start_pos": 50, "end_pos": 107, "type": "TASK", "confidence": 0.718924954533577}]}, {"text": "Now that visual recognition algorithms are beginning to produce reliable estimates of image content (, the time seems ripe to begin exploring higher level semantic tasks.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7239685356616974}]}, {"text": "There have been two main complementary directions explored for automatic image captioning.", "labels": [], "entities": [{"text": "automatic image captioning", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6274779438972473}]}, {"text": "The first focuses on describing exactly those items (e.g., objects, attributes) that are detected by vision recognition, which subsequently confines what should be described and how).", "labels": [], "entities": []}, {"text": "Approaches in this direction could be ideal for various practical applications such as image description for the visually impaired.", "labels": [], "entities": [{"text": "image description", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.8291009962558746}]}, {"text": "However, it is not clear whether the semantic expressiveness of these approaches can eventually scale up to the casual, but highly expressive language peo-", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the 1M captioned image corpus of.", "labels": [], "entities": [{"text": "1M captioned image corpus", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.6186138018965721}]}, {"text": "We reserve 1K images as a test set, and use the rest of the corpus for phrase extraction.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.8821268379688263}]}, {"text": "We experiment with the following approaches: Proposed Approaches: \u2022 TREEPRUNING: Our tree compression approach as described in \u00a74.", "labels": [], "entities": [{"text": "TREEPRUNING", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9983429908752441}, {"text": "tree compression", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7396120131015778}]}, {"text": "\u2022 SEQ+TREE: Our tree composition approach as described in \u00a73.", "labels": [], "entities": [{"text": "SEQ", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9661459922790527}, {"text": "TREE", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.7492185831069946}]}, {"text": "\u2022 SEQ+TREE+PRUNING: SEQ+TREE using compressed captions of TREEPRUNING as building blocks.", "labels": [], "entities": [{"text": "TREE", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.7006129026412964}, {"text": "TREE", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.867050051689148}, {"text": "TREEPRUNING", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9031946063041687}]}, {"text": "We perform automatic evaluation using two measures widely used in machine translation: BLEU) and METEOR.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.6964450031518936}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9989940524101257}, {"text": "METEOR", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.960196316242218}]}, {"text": "We remove all punctuation and convert captions to lowercase.", "labels": [], "entities": []}, {"text": "We use 1K test images from the captioned image corpus, and assume the original captions as the gold standard captions to compare against.", "labels": [], "entities": []}, {"text": "The results in Method-1 Method-: Human Evaluation: posed as a binary question \"which of the two options is better?\" with respect to Relevance (Rel), Grammar (Gmar), and Overall (All).", "labels": [], "entities": [{"text": "Relevance (Rel)", "start_pos": 132, "end_pos": 147, "type": "METRIC", "confidence": 0.9259516149759293}]}, {"text": "According to Pearson's \u03c7 2 test, all results are statistically significant.", "labels": [], "entities": []}, {"text": "show that both the integration of the tree structure (+TREE) and the generalization of captions using tree compression (+PRUNING) improve the BLEU score without brevity penalty significantly, 11 while improving METEOR only moderately (due to an improvement on precision with a decrease in recall.)", "labels": [], "entities": [{"text": "TREE", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9964982271194458}, {"text": "PRUNING", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9449377059936523}, {"text": "BLEU score", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.9810263514518738}, {"text": "METEOR", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9943185448646545}, {"text": "precision", "start_pos": 260, "end_pos": 269, "type": "METRIC", "confidence": 0.9992266893386841}, {"text": "recall", "start_pos": 289, "end_pos": 295, "type": "METRIC", "confidence": 0.999152421951294}]}, {"text": "Neither BLEU nor METEOR directly measure grammatical correctness overlong distances and may not correspond perfectly to human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9980377554893494}, {"text": "METEOR", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.8421286940574646}]}, {"text": "Therefore, we supplement automatic evaluation with human evaluation.", "labels": [], "entities": []}, {"text": "For human evaluations, we present two options generated from two competing systems, and ask turkers to choose the one that is better with respect to: relevance, grammar, and overall.", "labels": [], "entities": []}, {"text": "Results are shown in with 3 turker ratings per image.", "labels": [], "entities": []}, {"text": "We filter out turkers based on a control question.", "labels": [], "entities": []}, {"text": "We then compute the selection rate (%) of preferring method-1 over method-2.", "labels": [], "entities": [{"text": "selection rate", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.9359049201011658}]}, {"text": "The agreement among turkers is a frequent concern.", "labels": [], "entities": []}, {"text": "Therefore, we vary the set of dependable users based on their Cohen's kappa score (\u03ba) against other users.", "labels": [], "entities": [{"text": "kappa score (\u03ba)", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.8981199979782104}]}, {"text": "It turns out, filtering users based on \u03ba does not make a big difference in determining the winning method.", "labels": [], "entities": []}, {"text": "As expected, tree-based systems significantly outperform sequence-based counterparts.", "labels": [], "entities": []}, {"text": "For example, Sentence Compression At the core of the image caption generalization task is sentence compression.", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.9083291590213776}, {"text": "image caption generalization", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8076714475949606}, {"text": "sentence compression", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8235425353050232}]}, {"text": "Much work has considered deletion-only edits like ours, while recent ones explore more complex edits, such as substitutions, insertions and reordering (.", "labels": [], "entities": []}, {"text": "The latter generally requires a larger training corpus.", "labels": [], "entities": []}, {"text": "We leave more expressive compression as a future research work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Human Evaluation: posed as a binary question \"which of the two options is better?\" with respect to Relevance  (Rel), Grammar (Gmar), and Overall (All). According to Pearson's \u03c7 2 test, all results are statistically significant.", "labels": [], "entities": [{"text": "Relevance  (Rel)", "start_pos": 109, "end_pos": 125, "type": "METRIC", "confidence": 0.94696144759655}, {"text": "Grammar (Gmar)", "start_pos": 127, "end_pos": 141, "type": "METRIC", "confidence": 0.9052383005619049}]}]}