{"title": [{"text": "A New Corpus and Imitation Learning Framework for Context-Dependent Semantic Parsing", "labels": [], "entities": [{"text": "Context-Dependent Semantic Parsing", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6068404614925385}]}], "abstractContent": [{"text": "Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8318253457546234}]}, {"text": "Most approaches to this task have been evaluated on a small number of existing corpora which assume that all utterances must be interpreted according to a database and typically ignore context.", "labels": [], "entities": []}, {"text": "In this paper we present anew, publicly available corpus for context-dependent semantic parsing.", "labels": [], "entities": [{"text": "context-dependent semantic parsing", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.6394285957018534}]}, {"text": "The MRL used for the annotation was designed to support a portable, interactive tourist information system.", "labels": [], "entities": [{"text": "MRL", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6538266539573669}]}, {"text": "We develop a semantic parser for this corpus by adapting the imitation learning algorithm DAGGER without requiring alignment information during training.", "labels": [], "entities": []}, {"text": "DAGGER improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6475440859794617}, {"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9991458654403687}]}], "introductionContent": [{"text": "Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8273849487304688}]}, {"text": "Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS ( and GeoQuery.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7183087170124054}, {"text": "ATIS", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.7896078824996948}]}, {"text": "As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database, as well as the development of MT systems (.", "labels": [], "entities": [{"text": "Freebase database", "start_pos": 138, "end_pos": 155, "type": "DATASET", "confidence": 0.9572755098342896}, {"text": "MT", "start_pos": 187, "end_pos": 189, "type": "TASK", "confidence": 0.9641916155815125}]}, {"text": "However, these existing corpora have some important limitations.", "labels": [], "entities": []}, {"text": "The MRs accompanying the utterances are typically restricted to some form of database query.", "labels": [], "entities": []}, {"text": "Furthermore, inmost cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignored.", "labels": [], "entities": []}, {"text": "In this paper we present anew corpus for context-dependent semantic parsing to support the development of an interactive navigation and exploration system for tourismrelated activities.", "labels": [], "entities": [{"text": "context-dependent semantic parsing", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6251769661903381}]}, {"text": "The new corpus was annotated with MRs that can handle dialog context such as coreference and can accommodate utterances that are not interpretable according to a database, e.g. repetition requests.", "labels": [], "entities": []}, {"text": "The utterances were collected in experiments with human subjects, and contain phenomena such as ellipsis and disfluency.", "labels": [], "entities": []}, {"text": "We developed guidelines and annotated 17 dialogs containing 2,374 utterances, with 82.9% exact match agreement between two annotators.", "labels": [], "entities": [{"text": "exact match agreement", "start_pos": 89, "end_pos": 110, "type": "METRIC", "confidence": 0.8292777140935262}]}, {"text": "We also develop a semantic parser for this corpus.", "labels": [], "entities": []}, {"text": "As the output MRs are rather complex, instead of adopting an approach that searches the output space exhaustively, we use the imitation learning algorithm DAGGER) that converts learning a structured prediction model into learning a set of classification models.", "labels": [], "entities": []}, {"text": "We take advantage of its ability to learn with non-decomposable loss functions and extend it to handle the absence of alignment information during training by developing a randomized expert policy.", "labels": [], "entities": []}, {"text": "Our approach improves upon independently trained classifiers by 9.0 and 4.8 F-score on the development and test sets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9980332255363464}]}], "datasetContent": [{"text": "We split the annotated dialogs into training and test sets.", "labels": [], "entities": []}, {"text": "The former consists of four dialogs from the first scenario and seven from the second, and the latter of three dialogs from each scenario.", "labels": [], "entities": []}, {"text": "All development and feature engineering was conducted using cross-validation on the training set, at the dialog level rather than the utterance level (therefore resulting in as many folds as dialogs in the training set), to ensure that each fold contains utterances from all parts of the scenario from which the dialog is taken.", "labels": [], "entities": []}, {"text": "To perform cost-sensitive classification learning we used the adaptive regularization of weight vectors (AROW) algorithm.", "labels": [], "entities": [{"text": "classification learning", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.9462431371212006}]}, {"text": "AROW is an online algorithm for linear predictors that adjusts the per-feature learning rates so that popular features do not overshadow rare but useful ones.", "labels": [], "entities": [{"text": "AROW", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6999475955963135}, {"text": "linear predictors", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6537874937057495}]}, {"text": "Given the task decomposition, each learned hypothesis consists of 59 classifiers.", "labels": [], "entities": []}, {"text": "We restricted the prediction of nodes to content words since function words are unlikely to provide useful alignments.", "labels": [], "entities": []}, {"text": "All preprocessing was performed using the Stanford CoreNLP toolkit ().", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.9492019017537435}]}, {"text": "The implementation of the semantic parser is available from http://sites.google.com/ site/andreasvlachos/resources.", "labels": [], "entities": []}, {"text": "The DAGGER parameters were set to 12 training iterations, \u03b2 = 0.3 and 3 samples for action cost assessment.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.8552417755126953}]}, {"text": "We compared our DAGGER-based imitation learning approach (henceforth Imit) against independently trained classifiers using the same classification learner and features (henceforth Indep).", "labels": [], "entities": []}, {"text": "For both systems we incorporated an alignment dictionary (+align versions) as described in Sec.", "labels": [], "entities": []}, {"text": "6.2, in order to improve node prediction performance.", "labels": [], "entities": [{"text": "node prediction", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7632917165756226}]}, {"text": "The dictionary was extracted from the training data and contains 96 tokens that commonly predict a particular node type.", "labels": [], "entities": []}, {"text": "The results from the cross-validation experiments are reported in Tbl.", "labels": [], "entities": [{"text": "Tbl.", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9173538684844971}]}, {"text": "2. Overall performance evaluated as described in Sec.", "labels": [], "entities": []}, {"text": "5 was 53.6 points in Fscore for Imit, 5.7 points higher than Indep and the difference is greater for the +align versions.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.5574355721473694}, {"text": "Indep", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.4985765218734741}]}, {"text": "These results demonstrate the advantages of training classifiers using imitation learning versus independently trained classifiers.", "labels": [], "entities": []}, {"text": "Isolating the performance for node and argument prediction stages, we observe that the main bottleneck is the former, which in the case of Imit is 60.9 points in F-score compared to 78.8 for the latter.", "labels": [], "entities": [{"text": "argument prediction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7177692949771881}, {"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9972512125968933}]}, {"text": "Accuracy for dialog acts is 78.9%.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9976261258125305}]}, {"text": "2, the alignment dictionary improved not only node prediction performance by 6: Performances using 11-fold cross-validation on the training set.", "labels": [], "entities": [{"text": "node prediction", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.6742122024297714}]}, {"text": "points in F-score, but also argument prediction by 2.5 points, thus demonstrating the benefits of learning the alignments together with the other components of the semantic parser.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9908100366592407}, {"text": "argument prediction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.757070779800415}]}, {"text": "The overall performance improved by 5.5 points in F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9977486729621887}]}, {"text": "Finally, we ran an experiment with oracle node prediction and found that the overall performance using cross-validation on the training data improved to 88.2 and 79.9 points in F-score for the Imit+align Indep+align systems.", "labels": [], "entities": [{"text": "oracle node prediction", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7112547755241394}, {"text": "F-score", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.9989455342292786}]}, {"text": "This is in agreement with the results presented by on developing a semantic parsing parser for the AMR formalism who also argue that node prediction is the main performance bottleneck.", "labels": [], "entities": [{"text": "semantic parsing parser", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8284334341684977}, {"text": "node prediction", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.8062414526939392}]}, {"text": "3 gives results on the test set.", "labels": [], "entities": []}, {"text": "The overall performance for Imit is 48.4 F-score and 47.9% for exact match.", "labels": [], "entities": [{"text": "Imit", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.404662162065506}, {"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9989368319511414}, {"text": "exact", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9437147974967957}]}, {"text": "As in the cross-validation results on the training data, training with imitation learning improved upon independently trained classifiers.", "labels": [], "entities": []}, {"text": "The performance was improved further using the alignment dictionary, reaching 53.5 points in F-score and 49.1% exact match accuracy.", "labels": [], "entities": [{"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9991406202316284}, {"text": "exact match", "start_pos": 111, "end_pos": 122, "type": "METRIC", "confidence": 0.8659071922302246}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.5631167888641357}]}, {"text": "In the experimental setup above, dialogs from the same scenarios appear in both training and testing.", "labels": [], "entities": []}, {"text": "While this is a reasonable evaluation approach also followed in ATIS evaluations, it is likely to be relatively forgiving; in practice, semantic parsers are likely to encounter entities, activities, etc.", "labels": [], "entities": []}, {"text": "Hence we conducted a second evaluation in which dialogs from one scenario are used to train a parser evaluated on the other (still respecting the train/test split from before).", "labels": [], "entities": []}, {"text": "When testing on the dialogs from the first scenario and training on the dialogs from the second, the overall performance using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7.", "labels": [], "entities": [{"text": "Imit+align", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.8761351108551025}, {"text": "F-score", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.9993724226951599}]}, {"text": "Note that direct comparisons against the performances in Tbl.", "labels": [], "entities": [{"text": "Tbl", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.9731403589248657}]}, {"text": "3 are not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MRL vocabulary used in the annotation", "labels": [], "entities": [{"text": "MRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5714356899261475}]}, {"text": " Table 2: Performances using 11-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": " Table 3: Performances on the test set.", "labels": [], "entities": []}]}