{"title": [{"text": "Online Adaptor Grammars with Hybrid Inference", "labels": [], "entities": []}], "abstractContent": [{"text": "Adaptor grammars area flexible, powerful formalism for defining nonparametric, un-supervised models of grammar productions.", "labels": [], "entities": []}, {"text": "This flexibility comes at the cost of expensive inference.", "labels": [], "entities": []}, {"text": "We address the difficulty of inference through an online algorithm which uses a hybrid of Markov chain Monte Carlo and variational inference.", "labels": [], "entities": []}, {"text": "We show that this inference strategy improves scalability without sacrificing performance on unsupervised word segmentation and topic modeling tasks.", "labels": [], "entities": [{"text": "word segmentation and topic modeling tasks", "start_pos": 106, "end_pos": 148, "type": "TASK", "confidence": 0.6995498140652975}]}], "introductionContent": [{"text": "Nonparametric Bayesian models are effective tools to discover latent structure in data).", "labels": [], "entities": []}, {"text": "These models have had great success in text analysis, especially syntax (.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.7954236268997192}]}, {"text": "Nonparametric distributions provide support over a countably infinite long-tailed distributions common in natural language).", "labels": [], "entities": []}, {"text": "We focus on adaptor grammars), syntactic nonparametric models based on probabilistic context-free grammars.", "labels": [], "entities": []}, {"text": "Adaptor grammars weaken the strong statistical independence assumptions PCFGs make (Section 2).", "labels": [], "entities": [{"text": "Adaptor grammars", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8680824935436249}]}, {"text": "The weaker statistical independence assumptions that adaptor grammars make come at the cost of expensive inference.", "labels": [], "entities": []}, {"text": "Adaptor grammars are not alone in this trade-off.", "labels": [], "entities": []}, {"text": "For example, nonparametric extensions of topic models) have substantially more expensive inference than their parametric counterparts (.", "labels": [], "entities": []}, {"text": "A common approach to address this computational bottleneck is through variational inference (.", "labels": [], "entities": []}, {"text": "One of the advantages of variational inference is that it can be easily parallelized ( or transformed into an online algorithm (, which often converges in fewer iterations than batch variational inference.", "labels": [], "entities": []}, {"text": "Past variational inference techniques for adaptor grammars assume a preprocessing step that looks at all available data to establish the support of these nonparametric distributions ().", "labels": [], "entities": []}, {"text": "Thus, these past approaches are not directly amenable to online inference.", "labels": [], "entities": []}, {"text": "Markov chain Monte Carlo (MCMC) inference, an alternative to variational inference, does not have this disadvantage.", "labels": [], "entities": [{"text": "Markov chain Monte Carlo (MCMC) inference", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5306477136909962}]}, {"text": "MCMC is easier to implement, and it discovers the support of nonparametric models during inference rather than assuming it a priori.", "labels": [], "entities": [{"text": "MCMC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8644721508026123}]}, {"text": "We apply stochastic hybrid inference () to adaptor grammars to get the best of both worlds.", "labels": [], "entities": []}, {"text": "We interleave MCMC inference inside variational inference.", "labels": [], "entities": []}, {"text": "This preserves the scalability of variational inference while adding the sparse statistics and improved exploration MCMC provides.", "labels": [], "entities": [{"text": "MCMC", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8948766589164734}]}, {"text": "Our inference algorithm for adaptor grammars starts with a variational algorithm similar to and adds hybrid sampling within variational inference (Section 3).", "labels": [], "entities": []}, {"text": "This obviates the need for expensive preprocessing and is a necessary step to create an online algorithm for adaptor grammars.", "labels": [], "entities": []}, {"text": "Our online extension (Section 4) processes examples in small batches taken from a stream of data.", "labels": [], "entities": []}, {"text": "As data arrive, the algorithm dynamically extends the underlying approximate posterior distributions as more data are observed.", "labels": [], "entities": []}, {"text": "This makes the algorithm flexible, scalable, and amenable to datasets that cannot be examined exhaustively because of their size-e.g., terabytes of social media data appear every second-or their nature-e.g., speech acquisition, where a language learner is limited to the bandwidth of the human perceptual system and cannot acquire data in a monolithic batch.", "labels": [], "entities": [{"text": "speech acquisition", "start_pos": 208, "end_pos": 226, "type": "TASK", "confidence": 0.72350013256073}]}, {"text": "We show our approach's scalability and effective-ness by applying our inference framework in Section 5 on two tasks: unsupervised word segmentation and infinite-vocabulary topic modeling.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.6848309189081192}, {"text": "infinite-vocabulary topic modeling", "start_pos": 152, "end_pos": 186, "type": "TASK", "confidence": 0.6178631782531738}]}], "datasetContent": [{"text": "We implement our online adaptor grammar model (ONLINE) in Python and compare it against both MCMC and the variational inference (, VARI-ATIONAL).", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9824658632278442}, {"text": "VARI-ATIONAL", "start_pos": 131, "end_pos": 143, "type": "METRIC", "confidence": 0.9912785887718201}]}, {"text": "We use the latest implementation of MCMC sampler for adaptor grammars and simulate the variational approach using our implementation.", "labels": [], "entities": []}, {"text": "For MCMC approach, we use the best settings reported in Johnson and Goldwater      Truncation size is set to K Word = 1.5k and K Colloc = 3k.", "labels": [], "entities": []}, {"text": "The settings are chosen from cross validation.", "labels": [], "entities": []}, {"text": "We observe similar behavior under \u03ba = {0.7, 0.9, 1.0}, \u03c4 = {32, 64, 512}, B = {10, 50} and u = {10, 20, 100}.", "labels": [], "entities": []}, {"text": "7 For ONLINE inference, we parallelize each minibatch with four threads with settings: batch size B = 100 and TNG refinement interval u = 100.", "labels": [], "entities": [{"text": "ONLINE inference", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.6887184083461761}, {"text": "batch size B", "start_pos": 87, "end_pos": 99, "type": "METRIC", "confidence": 0.6852140029271444}, {"text": "TNG refinement interval u", "start_pos": 110, "end_pos": 135, "type": "METRIC", "confidence": 0.9007868766784668}]}, {"text": "ONLINE approach runns for two passes over datasets.", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7773937582969666}]}, {"text": "VARIATIONAL runs fifty iterations, with the same truncation level as in ONLINE.", "labels": [], "entities": [{"text": "VARIATIONAL", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.5556266903877258}, {"text": "ONLINE", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.6084673404693604}]}, {"text": "For negative log-likelihood evaluation, we train the model on a random 70% of the data, and holdout the rest for testing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Word segmentation accuracy measured by word token F1 scores and negative log-likelihood on held-out test dataset in the  brackets (lower the better, on the scale of 10 6 ) for our ONLINE model against MCMC approach (Johnson et al., 2006) on various  dataset using the unigram and collocation grammar. 7", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7480984926223755}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8423951268196106}, {"text": "word token F1 scores", "start_pos": 49, "end_pos": 69, "type": "METRIC", "confidence": 0.5892781466245651}, {"text": "ONLINE", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.8204171061515808}]}, {"text": " Table 1. The horizontal axis shows the number of passes over  the entire dataset. 11", "labels": [], "entities": []}]}