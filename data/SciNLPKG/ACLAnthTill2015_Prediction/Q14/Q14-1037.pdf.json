{"title": [{"text": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking", "labels": [], "entities": [{"text": "Entity Analysis", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7607392370700836}, {"text": "Linking", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.8647111654281616}]}], "abstractContent": [{"text": "We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.9155588746070862}, {"text": "named entity recognition", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.6331225236256918}, {"text": "entity linking", "start_pos": 183, "end_pos": 197, "type": "TASK", "confidence": 0.7643247842788696}]}, {"text": "Our model is formally a structured conditional random field.", "labels": [], "entities": []}, {"text": "Unary factors encode local features from strong baselines for each task.", "labels": [], "entities": []}, {"text": "We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type.", "labels": [], "entities": []}, {"text": "On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks.", "labels": [], "entities": [{"text": "ACE 2005 and OntoNotes datasets", "start_pos": 7, "end_pos": 38, "type": "DATASET", "confidence": 0.8582909584045411}]}, {"text": "Moreover, joint modeling improves performance on each task over strong independent baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "How do we characterize the collection of entities present in a document?", "labels": [], "entities": [{"text": "characterize the collection of entities present in a document", "start_pos": 10, "end_pos": 71, "type": "TASK", "confidence": 0.7704614268408881}]}, {"text": "Two broad threads exist in the literature.", "labels": [], "entities": []}, {"text": "The first is coreference resolution (), which identifies clusters of mentions in a document referring to the same entity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.9458649158477783}]}, {"text": "This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context.", "labels": [], "entities": []}, {"text": "A separate line of work has considered the problem of entity linking or \"Wikification\", where mentions are linked to entries in a given knowledge System available at http://nlp.cs.berkeley.edu base.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7237387895584106}]}, {"text": "This is useful for grounding proper entities, but in the absence of coreference gives an incomplete picture of document content itself, in that nominal expressions and pronouns are left unresolved.", "labels": [], "entities": []}, {"text": "In this paper, we describe a joint model of coreference, entity linking, and semantic typing (named entity recognition) using a structured conditional random field.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7300256639719009}, {"text": "entity recognition", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7731530666351318}]}, {"text": "Variables in the model capture decisions about antecedence, semantic type, and entity links for each mention.", "labels": [], "entities": []}, {"text": "Unary factors on these variables incorporate features that are commonly employed when solving each task in isolation.", "labels": [], "entities": []}, {"text": "Binary and higher-order factors capture interactions between pairs of tasks.", "labels": [], "entities": []}, {"text": "For entity linking and NER, factors capture a mapping between NER's semantic types and Wikipedia's semantics as described by infoboxes, categories, and article text.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7668563723564148}]}, {"text": "Coreference interacts with the other tasks in a more complex way, via factors that softly encourage consistency of semantic types and entity links across coreference arcs, similar to the method of . shows an example of the effects such factors can capture.", "labels": [], "entities": []}, {"text": "The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7881808280944824}]}, {"text": "Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7).", "labels": [], "entities": []}, {"text": "Entity linking has been employed for coreference resolution and coreference for entity linking as part of pipelined systems.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6576703637838364}, {"text": "coreference resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9560286700725555}, {"text": "entity linking", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.7066933363676071}]}, {"text": "Past work has shown that tighter integration of coreference and entity linking is promising (; we extend these approaches and model the entire process more holistically.", "labels": [], "entities": [{"text": "coreference", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9538949131965637}, {"text": "entity linking", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.744964987039566}]}, {"text": "Named entity recognition is improved by simple coreference ( and knowledge from Wikipedia (.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7504381338755289}]}, {"text": "Joint models of coreference and NER have been proposed in and , but in neither case was supervised data used for both tasks.", "labels": [], "entities": [{"text": "coreference", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9448386430740356}, {"text": "NER", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.963455319404602}]}, {"text": "Technically, our model is most closely related to that of , who handle coreference, named entity recognition, and relation extraction.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.604525605837504}, {"text": "relation extraction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8656218945980072}]}, {"text": "Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all decisions throughout inference (rather than using a greedy approach), and the feature sets we deploy for cross-task interactions.", "labels": [], "entities": []}, {"text": "In designing a joint model, we would like to preserve the modularity, efficiency, and structural simplicity of pipelined approaches.", "labels": [], "entities": []}, {"text": "Our model's feature-based structure permits improvement of features specific to a particular task or to a pair of tasks.", "labels": [], "entities": []}, {"text": "By pruning variable domains with a coarse model and using approximate inference via belief propagation, we maintain efficiency and our model is only a factor of two slower than the union of the individual Our model could potentially be extended to handle relation extraction or mention detection, which has also been addressed in past joint modeling efforts), but that is outside the scope of the current work. models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 255, "end_pos": 274, "type": "TASK", "confidence": 0.8302850127220154}, {"text": "mention detection", "start_pos": 278, "end_pos": 295, "type": "TASK", "confidence": 0.700084388256073}]}, {"text": "Finally, as a structured CRF, it is conceptually no more complex than its component models and its behavior can be understood using the same intuition.", "labels": [], "entities": []}, {"text": "We apply our model to two datasets, ACE 2005 and OntoNotes, with different mention standards and layers of annotation.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9697391390800476}, {"text": "OntoNotes", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.8188202381134033}]}, {"text": "In both settings, our joint model outperforms our independent baseline models.", "labels": [], "entities": []}, {"text": "On ACE, we achieve state-of-the-art entity linking results, matching the performance of the system of.", "labels": [], "entities": []}, {"text": "On OntoNotes, we match the performance of the best published coreference system) and outperform two strong NER systems).", "labels": [], "entities": []}], "datasetContent": [{"text": "We present results on two corpora.", "labels": [], "entities": []}, {"text": "First, we use the ACE 2005 corpus: this corpus annotates mentions complete with coreference, semantic types (per mention), and entity links (also per mention) later added by.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.9683399796485901}]}, {"text": "We evaluate on gold mentions in this setting for comparability with prior work on entity linking; we lift this restriction in Section 6.3.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.7183045297861099}]}, {"text": "Second, we evaluate on the OntoNotes 5 corpus () as used in the CoNLL 2012 coreference shared task (.", "labels": [], "entities": [{"text": "OntoNotes 5 corpus", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.7653979460398356}, {"text": "CoNLL 2012 coreference shared task", "start_pos": 64, "end_pos": 98, "type": "DATASET", "confidence": 0.7835262775421142}]}, {"text": "This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions.", "labels": [], "entities": []}, {"text": "We will compare to prior coreference and named entity work in the system mentions setting.", "labels": [], "entities": []}, {"text": "We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile ( and parse it using the Berkeley Parser ().", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.9413935542106628}, {"text": "Reconcile", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.8908553123474121}]}, {"text": "We use the train/test split from,, and, as well as their average, the CoNLL metric, all computed from the reference implementation of the CoNLL scorer ().", "labels": [], "entities": [{"text": "CoNLL metric", "start_pos": 70, "end_pos": 82, "type": "METRIC", "confidence": 0.6544130146503448}, {"text": "CoNLL scorer", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.6296269297599792}]}, {"text": "We see that the joint model improves all three tasks compared to the individual task models in the baseline.", "labels": [], "entities": []}, {"text": "More in-depth entity linking results are shown in.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7013984471559525}]}, {"text": "We both evaluate on overall accuracy (how many mentions are correctly linked) as well as two more specific criteria: precision/recall/F 1 of non-NIL 9 predictions, and precision/recall/F 1 of NIL predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9968174695968628}, {"text": "precision/recall/F 1", "start_pos": 117, "end_pos": 137, "type": "METRIC", "confidence": 0.7791884342829386}, {"text": "precision/recall/F 1", "start_pos": 168, "end_pos": 188, "type": "METRIC", "confidence": 0.7984827756881714}]}, {"text": "This latter measure maybe important if a system designer is trying to identify new entities in a document.", "labels": [], "entities": []}, {"text": "We compare to the results of the best model from, which is a sophisticated discriminative model incorporating a latent model of mention scope.", "labels": [], "entities": []}, {"text": "Our performance is similar to that of, though the results are not exactly comparable for two reasons.", "labels": [], "entities": []}, {"text": "First, our models are trained on different datasets: Fahrni and Strube (2014) train on Wikipedia data whereas we train on the ACE training set.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.9005158543586731}, {"text": "ACE training set", "start_pos": 126, "end_pos": 142, "type": "DATASET", "confidence": 0.9264369209607443}]}, {"text": "Second, they make use of the annotated head spans in ACE whereas we only use detected heads based on automatic parses.", "labels": [], "entities": [{"text": "ACE", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8475313186645508}]}, {"text": "Note that this information is particularly beneficial for locating the right query because \"heads\" maybe multiword expressions such as West Bank as part of the phrase southern West Bank.", "labels": [], "entities": [{"text": "West Bank", "start_pos": 135, "end_pos": 144, "type": "DATASET", "confidence": 0.970840334892273}, {"text": "West Bank", "start_pos": 176, "end_pos": 185, "type": "DATASET", "confidence": 0.9529069364070892}]}, {"text": "9 NIL is a placeholder for mentions which do not link to an article in Wikipedia.", "labels": [], "entities": [{"text": "9", "start_pos": 0, "end_pos": 1, "type": "DATASET", "confidence": 0.6402051448822021}, {"text": "NIL", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.5827677845954895}]}, {"text": "On the TAC datasets, this FAHRNI model substantially outperforms and has comparable performance to: Results of model ablations on the ACE development set.", "labels": [], "entities": [{"text": "TAC datasets", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9134216904640198}, {"text": "FAHRNI", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.8896371722221375}, {"text": "ACE development set", "start_pos": 134, "end_pos": 153, "type": "DATASET", "confidence": 0.9486995339393616}]}, {"text": "We holdout each type of factor in turn from the JOINT model and add each in turnover the IN-DEP. model.", "labels": [], "entities": [{"text": "JOINT", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.5062618851661682}, {"text": "IN-DEP", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9214147925376892}]}, {"text": "We evaluate the coreference performance using the CoNLL metric, NER accuracy, and entity linking accuracy.", "labels": [], "entities": [{"text": "NER accuracy", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.5436372458934784}, {"text": "entity linking accuracy", "start_pos": 82, "end_pos": 105, "type": "METRIC", "confidence": 0.5392061571280161}]}, {"text": "The second part of our evaluation uses the datasets from the CoNLL 2012 Shared Task (, specifically the coreference and NER annotations.", "labels": [], "entities": [{"text": "CoNLL 2012 Shared Task", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.9335136562585831}]}, {"text": "All experiments use the standard automatic parses from the shared task and mentions detected according to the method of . Evaluating on OntoNotes carries with it a few complications.", "labels": [], "entities": []}, {"text": "First, gold-standard entity linking annotations are not available; we can handle this by The company ...", "labels": [], "entities": []}, {"text": "leaving thee i variables in our model latent.", "labels": [], "entities": []}, {"text": "Second, and more seriously, NER chunks are no longer the same as coreference mentions, so our assumption of fixed NER spans no longer holds.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the ACE 2005 dev and test sets for the INDEP. (task-specific factors only) and JOINT models.  Coreference metrics are computed using their reference implementations (", "labels": [], "entities": [{"text": "ACE 2005 dev", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9350343346595764}, {"text": "INDEP", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.6212301254272461}]}, {"text": " Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our INDEP. (task-specific factors  only) and JOINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We  compare overall accuracy as well as performance at predicting NILS (mentions not in the knowledge base) and non- NILS. The JOINT model roughly matches the performance of FAHRNI and gives strong gains over the INDEP. system.", "labels": [], "entities": [{"text": "ACE 2005 test set", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9772727936506271}, {"text": "INDEP", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.8282334208488464}, {"text": "accuracy", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9987928867340088}, {"text": "FAHRNI", "start_pos": 407, "end_pos": 413, "type": "DATASET", "confidence": 0.8588495850563049}, {"text": "INDEP. system", "start_pos": 446, "end_pos": 459, "type": "DATASET", "confidence": 0.9131840765476227}]}, {"text": " Table 4: CoNLL metric scores for our systems on the CoNLL 2012 blind test set, compared to Durrett and Klein  (2013) (the Berkeley system), Fernandes et al. (2012) (the winner of the CoNLL shared task), and Bj\u00f6rkelund and  Kuhn (2014) (the best reported results on the dataset to date). INDEP. and JOINT are the contributions of this work;  JOINT improves substantially over INDEP. (these improvements are statistically significant with p < 0.05 according  to a bootstrap resampling test) and achieves state-of-the-art results.", "labels": [], "entities": [{"text": "CoNLL 2012 blind test set", "start_pos": 53, "end_pos": 78, "type": "DATASET", "confidence": 0.9262966871261596}]}]}