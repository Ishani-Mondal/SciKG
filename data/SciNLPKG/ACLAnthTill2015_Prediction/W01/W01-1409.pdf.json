{"title": [{"text": "Building a Statistical Machine Translation System from Scratch: How Much Bang for the Buck Can We Expect?", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.701345831155777}]}], "abstractContent": [{"text": "We report on our experience with building a statistical MT system from scratch, including the creation of a small parallel Tamil-English corpus, and the results of a task-based pilot evaluation of statistical MT systems trained on sets of ca.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.7800723910331726}, {"text": "MT", "start_pos": 209, "end_pos": 211, "type": "TASK", "confidence": 0.8173049688339233}]}, {"text": "5000 parallel sentences of Tamil and English data.", "labels": [], "entities": []}, {"text": "Our results show that even with apparently incomprehensible system output, humans without any knowledge of Tamil can achieve performance rates as high as 86% accuracy for topic identification, 93% recall for document retrieval, and 64% recall on question answering (plus an additional 14% partially correct answers).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9972754120826721}, {"text": "topic identification", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.8202359080314636}, {"text": "recall", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.9984238147735596}, {"text": "recall", "start_pos": 236, "end_pos": 242, "type": "METRIC", "confidence": 0.9983637928962708}, {"text": "question answering", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.6346576064825058}]}], "introductionContent": [{"text": "Crises and disasters frequently attract international attention to regions of the world that have previously been largely ignored by the international community.", "labels": [], "entities": []}, {"text": "While it is possible to stock upon emergency relief supplies and, for the worst case, weapons, regardless of where exactly they are eventually going to be used, this cannot be done with multilingual information processing technology.", "labels": [], "entities": []}, {"text": "This technology will often have to be developed after the fact in a quick response to the given situation.", "labels": [], "entities": []}, {"text": "Multilingual data resources for statistical approaches, such as parallel corpora, may not always be available.", "labels": [], "entities": []}, {"text": "In the fall of 2000, we decided to put the current state of the art to the test with respect to the rapid construction of a machine translation system from scratch.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7172361314296722}]}, {"text": "Within one month, we would \u00af hire translators; \u00af translate as much text as possible; and \u00af train a statistical MT system on the data thus created.", "labels": [], "entities": [{"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9526193737983704}]}, {"text": "The language of choice was Tamil, which is spoken in Sri Lanka and in the southern part of India.", "labels": [], "entities": []}, {"text": "Tamil is a head-last language with a very rich morphology and therefore quite different from English.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given these numbers, it is obvious that one cannot expect much performance from a system that relies on models trained on only 24K tokens of data.", "labels": [], "entities": []}, {"text": "As a matter of fact, it is close to impossible to make any sense whatsoever of the output of such a system (cf..", "labels": [], "entities": []}, {"text": "To get an estimate of the performance with more training data, we augmented our corpus with a parallel corpus of international news texts in Southern Indian Tamil which was made available to us by Fred Gey of the University of California at Berkeley (henceforth: Berkeley corpus).", "labels": [], "entities": []}, {"text": "3,800 sentence pairs with 75,800 Tamil tokens after stemming (before stemming: 60,000; the difference is due to the introduction of additional markers during stemming).", "labels": [], "entities": []}, {"text": "Some of the parallel data was withheld for system evaluation; the augmented training corpus (Berkeley and TamilNet corpus; short B+TN) had a size of 85K tokens on the Tamil side.", "labels": [], "entities": [{"text": "TamilNet corpus; short B+TN", "start_pos": 106, "end_pos": 133, "type": "DATASET", "confidence": 0.7940220407077244}]}, {"text": "The augmented training corpus had a text coverage of 81% (seen at least once; 75% without augmentation), and 67% (seen at least 5 times; 60% without augmentation), respectively, for Sri Lankan Tamil.", "labels": [], "entities": []}, {"text": "We trained IBM Translation Model 4 () both on our corpus alone and on the augmented corpus, using the EGYPT toolkit ( , and then translated a number of texts using different translation models and different transfer methods, namely glossing (replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit) and Model 4 decoding ().", "labels": [], "entities": [{"text": "EGYPT", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.9196606278419495}]}, {"text": "shows the output of the different systems in comparison with the human translation.", "labels": [], "entities": []}, {"text": "We then conducted the following experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the Document Classification Task. Test subjects were asked to classify the translations of 15  documents into 4 major and 11 minor categories.", "labels": [], "entities": [{"text": "Document Classification Task", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.9391900300979614}]}, {"text": " Table 3: Accuracy on question answering. The test  sets are the same as in Table 2. Only questions con- cerning documents that were identified correctly were  considered in this evaluation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9968428611755371}, {"text": "question answering", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7754746973514557}]}]}