{"title": [{"text": "The ARC A3 Project: Terminology Acquisition Tools: Evaluation Method and Task", "labels": [], "entities": [{"text": "ARC A3 Project", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.810046394666036}, {"text": "Terminology Acquisition", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.8116037249565125}]}], "abstractContent": [{"text": "This paper describes the work achieved in the Concerted Research Project ARC A3 supported and coordinated by the AUF 1 , former Aupelf-Uref 2.", "labels": [], "entities": [{"text": "Concerted Research Project ARC A3", "start_pos": 46, "end_pos": 79, "type": "DATASET", "confidence": 0.6223882794380188}, {"text": "AUF 1", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.9271102845668793}, {"text": "Aupelf-Uref 2", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.9472856819629669}]}, {"text": "The project deals with the evaluation of term and semantic relation extraction from corpora in French.", "labels": [], "entities": [{"text": "term and semantic relation extraction from corpora", "start_pos": 41, "end_pos": 91, "type": "TASK", "confidence": 0.6812108414513725}]}, {"text": "Eight participants, both from public institutions and industrial corporations were involved in this project and were responsible for producing corpora suitable for extraction tasks and elaborating a protocol in order to evaluate objectively terminology acquisition tools.", "labels": [], "entities": [{"text": "terminology acquisition", "start_pos": 241, "end_pos": 264, "type": "TASK", "confidence": 0.7282963395118713}]}, {"text": "This expression covers respectively, term extractors, classifiers and semantic relation extractors.", "labels": [], "entities": [{"text": "semantic relation extractors", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6426332294940948}]}, {"text": "The paper also reports on the methodology used for comparing four term extractors, one classifier and three semantic relation extractors during the 2000 evaluation campaign.", "labels": [], "entities": []}, {"text": "There are also several by-products of this campaign: first, two corpora which can be used for NLP system development and evaluation as the AUF recommended; and then terminology products: for each corpus a list of terms characterizing the field is available.", "labels": [], "entities": [{"text": "NLP system development", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.8420555194218954}, {"text": "AUF", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.9531838893890381}]}, {"text": "We are not giving details about the results but rather an assessment of what the evaluation of Terminology Extraction Tools is: how was it done, what were the difficulties, which are the advantages and disadvantages of the adopted protocol, what are the limits and how should we proceed for future testing.", "labels": [], "entities": []}, {"text": "1 The Association des Universit\u00e9s Francophones 2 AUPELF is the \"Association des Universit\u00e9s Enti\u00e8rement ou Partiellement de Langue Fran\u00e7aise\", an NGO whose mission is to promote the dissemination of French as a scientific medium.).", "labels": [], "entities": []}, {"text": "The first phase of the project has been directed towards testing the systems on one corpus 4 (trial run) and towards elaborating a workable protocol based on this experience.", "labels": [], "entities": []}, {"text": "The first results were presented during the first conference of JST 5 (cf. B\u00e9guin et al., 1997, 2000).", "labels": [], "entities": [{"text": "JST 5", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.7061755061149597}]}, {"text": "This article reports on the second and final evaluation campaign.", "labels": [], "entities": []}, {"text": "2 ARC A3 Organization ARC A3 brings together four kinds of actors: a coordinator who plays an organizational role (schedule, quality control of corpora, data production, etc.), corpora providers; participants of the test and two scientific advisors.", "labels": [], "entities": [{"text": "ARC A3 Organization ARC A3", "start_pos": 2, "end_pos": 28, "type": "DATASET", "confidence": 0.5533977150917053}]}, {"text": "The action has been coordinated by the University of Lille 3.", "labels": [], "entities": []}, {"text": "The organizing team in cooperation with the discussion group made up of representatives of each participating team and two scientific 3 Ing\u00e9nierie de la Langue, Linguistique-informatique et Corpus \u00e9crits.", "labels": [], "entities": []}, {"text": "4 SPIRALE, a periodical dealing with education and pedagogy issues.", "labels": [], "entities": [{"text": "SPIRALE", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9875043034553528}]}, {"text": "Each periodical sizes around 200 pages.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Evaluation activities area corollary of the quick development of NLP tools in general and of terminology extraction in particular.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9122557640075684}]}, {"text": "It thus became necessary to evaluate these tools on objectively based criteria in order to have a clear picture of the state-of-the-art, assess the needs in this sector and hence promote research in this specific field.", "labels": [], "entities": []}, {"text": "Moreover, the principal aim of existing testing methods, as reported in the literature, is to come across software errors and then try to adapt them fora particular user environment.", "labels": [], "entities": []}, {"text": "Evaluation paradigm is basically dependant upon two major steps: (i) Creation of textual data: raw or tagged corpora and test material.", "labels": [], "entities": []}, {"text": "A corpus-based research is part of the infrastructure for the development of advanced language processing applications; (ii) Test and comparison of systems on a similar data).", "labels": [], "entities": []}, {"text": "The approach we adopted is a black-box qualitative approach The results are compared with the human performance of a task (either experts examining results or using reference lists or both).", "labels": [], "entities": []}, {"text": "Moreover comparisons are made with other systems performing the same task.", "labels": [], "entities": []}, {"text": "The results are finally calculated and translated in terms of traditional IR measures 9 . The conventional distinction between black-box and glass-box is the following: the former considers only system input-out-put relations without regard to the specific mechanisms by which the outputs were obtained while the latter examines the mechanisms linking input and output., among many others).", "labels": [], "entities": [{"text": "IR", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9118253588676453}]}, {"text": "The qualitative evaluation measures as described by Sparck-Jones 1996, pp.", "labels": [], "entities": []}, {"text": "61-122, are based on observation or interviewing and are broadly designed to obtain a more holistic, less reductive or fragmented view of the situation.", "labels": [], "entities": []}, {"text": "It is moreover more naturalistic.", "labels": [], "entities": []}, {"text": "This type of evaluation naturally fits an end-free style.", "labels": [], "entities": []}, {"text": "In our case the quality of the results is evaluated by domain experts.", "labels": [], "entities": []}, {"text": "We distinguish two types of experts: experts for the three applications tested (systematic terminology, translation and indexing); and experts in the two domains of corpora (biotechnology and pedagogy).", "labels": [], "entities": []}, {"text": "Both quantitative and qualitative approaches are goal-oriented, that is focusing on discrepancies between performance results and initial system requirements.", "labels": [], "entities": []}, {"text": "Sparck-Jones points out how the two types of measures are deeply interwoven although different in their nature: -Recall is a quantitative measure of system performance while -Declared Satisfaction is a qualitative one (i.e. such a measure is really qualitative even if the result of applying it to a set of users is a percentage.", "labels": [], "entities": [{"text": "Recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9007165431976318}]}, {"text": "The qualitative approach in the evaluation process is the easiest one for end users.", "labels": [], "entities": []}, {"text": "It means giving a value judgment on how the system globally works).", "labels": [], "entities": []}, {"text": "The dominant approach today is towards quantitative evaluations which are considered as more objective and reproducible than the qualitative approach).", "labels": [], "entities": []}, {"text": "The main attempt of these approaches is to translate the concepts of relevance and quality into numerical data.", "labels": [], "entities": []}, {"text": "Statistical approaches such as MUC 2 and TREC 3 are frequently used for this type of evaluation.).", "labels": [], "entities": [{"text": "MUC", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.4193146228790283}, {"text": "TREC", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.846944272518158}]}, {"text": "Obviously this approach has its pros and cons.", "labels": [], "entities": []}, {"text": "But it can be justified on the following basis: -Since most developers cannot provide us (as test organizers) with their systems, the only way was to send them the text corpora and let them provide us with the results.", "labels": [], "entities": []}, {"text": "A glass-box evaluation would have required an examination of the systems by the organizers which would have been impossible except for Xerox's TermFinder and Logos System's Knowledge Discovery, two commercialized systems.", "labels": [], "entities": [{"text": "Knowledge Discovery", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.6077689677476883}]}, {"text": "-Even if this approach maybe criticized on account of its subjective side, end-users like it because of its usefulness when comparing two or more systems which differ in all their parameter settings..", "labels": [], "entities": []}, {"text": "-A black-box evaluation is more oriented towards system's end-user when compared to a glass-box evaluation.", "labels": [], "entities": []}, {"text": "For the latter the test will involve analyzing the system's functioning by looking at its different components.", "labels": [], "entities": []}, {"text": "Each component is evaluated separately in itself.", "labels": [], "entities": []}, {"text": "Such an approach allows for spotting and understanding the causes of dysfunctional results.", "labels": [], "entities": [{"text": "spotting", "start_pos": 28, "end_pos": 36, "type": "TASK", "confidence": 0.9848649501800537}]}, {"text": "It is along term process which requires access to the internal parts of the system and an understanding of the architecture and global strategy of the software.", "labels": [], "entities": []}, {"text": "This is obviously a developer oriented approach and not an enduser one.", "labels": [], "entities": []}, {"text": "-In spite of its limited scope the evaluation protocol we adopted is used in more complicated NLP tools, such as MT tools.", "labels": [], "entities": [{"text": "MT tools", "start_pos": 113, "end_pos": 121, "type": "TASK", "confidence": 0.879025787115097}]}, {"text": "Evaluators examine the systems' output without considering the differences between them (cf. L').", "labels": [], "entities": []}, {"text": "Last Spring our team took part in a workshop organized by ISSCO (University of Geneva) where we and all the other participants adopted this approach.", "labels": [], "entities": [{"text": "ISSCO (University of Geneva)", "start_pos": 58, "end_pos": 86, "type": "DATASET", "confidence": 0.836890866359075}]}, {"text": "The extraction of terms, of classes and of semantic relations was necessary to test the tools performance in the three following tasks: Systematic terminology (characterizing the tested corpora); (ii) Translation; (iii) Indexing.", "labels": [], "entities": [{"text": "Translation", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.9783687591552734}, {"text": "Indexing", "start_pos": 220, "end_pos": 228, "type": "TASK", "confidence": 0.9674633741378784}]}, {"text": "This means in practice: what is the relevance of terms, classes and semantic relations provided by the systems being tested?", "labels": [], "entities": []}, {"text": "Do the terms, classes and semantic relations satisfy minimum requirements?", "labels": [], "entities": []}, {"text": "Do we need to define a minimum level of terms, classes, semantic production?", "labels": [], "entities": []}, {"text": "For example, it could be that most of the systems being tested are having qualitatively poor outputs, while only one or two produce worthwhile results.", "labels": [], "entities": []}, {"text": "Within this perspective the idea was to submit the results to specialists.", "labels": [], "entities": []}, {"text": "We distinguished for the purpose of this campaign two types of human expertise as we mentioned above.", "labels": [], "entities": []}, {"text": "Given the three tasks to be performed (indexing, systematic terminology and translation), the usual notions of recall and precision can be used to evaluate the quality of results when matched with a manually-produced reference list.", "labels": [], "entities": [{"text": "indexing", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.9541563987731934}, {"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9564666152000427}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9989339709281921}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9962579011917114}]}, {"text": "Performance failure at this level can be interpreted in terms of silence and noise (see below).", "labels": [], "entities": [{"text": "silence", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.982090413570404}]}, {"text": "Given the difficulties we listed above and the fact that it was impossible to compare Conterm with other systems performing the same task.", "labels": [], "entities": []}, {"text": "The only possible evaluation for Conterm would have been a progress evaluation for this sole classifier of the campaign 20 . This problem shows again the limits of our Protocol.", "labels": [], "entities": [{"text": "Conterm", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.8241055011749268}]}, {"text": "The Conterm lists were matched to an automatically produced untagged list of terms which corresponds to the eight texts of the INRA corpus.", "labels": [], "entities": [{"text": "Conterm lists", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8968318104743958}, {"text": "INRA corpus", "start_pos": 127, "end_pos": 138, "type": "DATASET", "confidence": 0.9668053686618805}]}, {"text": "The most important element in its evaluation is not that we matched its results with a tagged list but that the results had been matched with indexers' and/or experts lists and that we could observe the correspondence between Conterm's output and the lists.", "labels": [], "entities": []}, {"text": "It does not mean that Conterm is good for indexing but that the classes suggested by this tool embody conceptual attributes which are close to the logic underlying the human selection of candidate-terms suitable for indexing, namely its rich lexico-semantic network.", "labels": [], "entities": []}], "tableCaptions": []}