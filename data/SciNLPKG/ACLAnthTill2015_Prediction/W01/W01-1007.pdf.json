{"title": [{"text": "The Form is the Substance: Classification of Genres in Text", "labels": [], "entities": []}], "abstractContent": [{"text": "Categorization of text in IR has traditionally focused on topic.", "labels": [], "entities": [{"text": "Categorization of text in IR", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7496633529663086}]}, {"text": "As use of the Internet and e\u2212mail increases, categorization has become a key area of research as users demand methods of prioritizing documents.", "labels": [], "entities": []}, {"text": "This work investigates text classification by format style, i.e. \"genre\", and demonstrates, by complementing topic classification, that it can significantly improve retrieval of information.", "labels": [], "entities": [{"text": "text classification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7270165383815765}, {"text": "topic classification", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.7328244298696518}]}, {"text": "The paper compares use of presentation features to word features, and the combination thereof, using Na\u00efve Bayes, C4.5 and SVM classifiers.", "labels": [], "entities": []}, {"text": "Results show use of combined feature sets with SVM yields 92% classification accuracy in sorting seven genres.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9896929860115051}]}], "introductionContent": [{"text": "This paper firstly defines genre, explains the rationale for automatic genre classification, and reviews some previously published work relevant to this problem.", "labels": [], "entities": [{"text": "automatic genre classification", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.6248233318328857}]}, {"text": "It describes the features chosen to be extracted from documents for input to a classification system.", "labels": [], "entities": []}, {"text": "The paper next describes data used, experiments carried out, and the results obtained.", "labels": [], "entities": []}, {"text": "Finally the paper discusses the results and suggests ways for the research to progress.", "labels": [], "entities": []}], "datasetContent": [{"text": "Both word based features and presentation features could be calculated from samples and their use compared in classification experiments.", "labels": [], "entities": []}, {"text": "Experiments used these feature values with three different classifiers as it was thought that different classifiers might work better with one or other of the feature sets.", "labels": [], "entities": []}, {"text": "The experiments detailed here were run under the ten\u2212fold cross\u2212validation method.", "labels": [], "entities": []}, {"text": "This splits the data up into training and test sets in a 90%/10% proportion.", "labels": [], "entities": []}, {"text": "Experiments are repeated ten times with the split being made in a round\u2212 robin fashion.", "labels": [], "entities": []}, {"text": "In this way all of the data is used both in training and testing but not within the same cycle.", "labels": [], "entities": []}, {"text": "Recorded here are Recall, Precision and F1 where recall is the number of correct classifications divided by number of documents, precision is the number of correct classifications divided by the number of classifications made, and F1=2 * (Precision * Recall) / (Precision + Recall).", "labels": [], "entities": [{"text": "Recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.7272250056266785}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.999651312828064}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.999222993850708}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9995957016944885}, {"text": "F1", "start_pos": 231, "end_pos": 233, "type": "METRIC", "confidence": 0.9996020197868347}]}, {"text": "Note that in experiments where a classification is required (i.e. No \"unknown\" class) Recall, Precision and F1 are all equal.", "labels": [], "entities": [{"text": "Recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9965265393257141}, {"text": "Precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9988409876823425}, {"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9986680746078491}]}, {"text": "323 word features were selected by analysing the Information Gain over the whole corpus.", "labels": [], "entities": []}, {"text": "The selected words, therefore, are in some sense an \"ideal\" feature set.", "labels": [], "entities": []}, {"text": "The length of the vector was chosen such that no zero vector would result.", "labels": [], "entities": []}, {"text": "Word counts were measured in each document.", "labels": [], "entities": []}, {"text": "When using Na\u00efve Bayes these counts were multiplied by the log probabilities calculated.", "labels": [], "entities": [{"text": "Na\u00efve Bayes", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8720447421073914}]}, {"text": "For SVM and C4.5 logs of the counts were taken and divided by logs of the total word count in the corresponding document.", "labels": [], "entities": []}, {"text": "(Smoothing carried out by adding 1 to both numerator and denominator prior to taking logs.)", "labels": [], "entities": []}, {"text": "A misclassification matrix example is shown in.", "labels": [], "entities": []}, {"text": "The true genre tag is indicated by row and the classifier's decision is listed by column.", "labels": [], "entities": []}, {"text": "The number of correct classifications appear on the diagonal of the table, and the numbers of misclassifications are shown in the remaining column cells.", "labels": [], "entities": []}, {"text": "SVM and C4.5 experimnets used presentation feature values directly.", "labels": [], "entities": []}, {"text": "Using the Na\u00efve Bayes classifier requires that feature value ranges be defined because Bayesian classifiers usually work with features that are either present or absent.", "labels": [], "entities": [{"text": "Na\u00efve Bayes classifier", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.8440033793449402}]}, {"text": "However, Mitchell's formulation generalizes to real\u2212valued functions: The conditional probability that the genre of document dis gi, given that feature f is ind, v(f,d), is areal value V, given by: where: D(gi) is the subset of the training corpus tagged as being of genre gi.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of example queries with and without use of genre tag in a marked corpus", "labels": [], "entities": []}, {"text": " Table 2. The genres \"Television News\" and  \"Radio News\" were predominantly produced by  transcription systems and contain errors.  (Whether these two classes are truly distinct  genres is, perhaps, debatable.)", "labels": [], "entities": []}, {"text": " Table 2: Breakdown of CMU genre corpus", "labels": [], "entities": []}, {"text": " Table 3. The true genre tag is  indicated by row and the classifier's decision is  listed by column. The number of correct  classifications appear on the diagonal of the  table, and the numbers of misclassifications are  shown in the remaining column cells.  SVM  and C4.5  experimnets used  presentation feature values directly. Using the  Na\u00efve Bayes classifier requires that feature value  ranges be defined because Bayesian classifiers  usually work with features that are either present  or absent. However, Mitchell's formulation  generalizes to real\u2212valued functions: The", "labels": [], "entities": []}, {"text": " Table 4: Misclassification Matrix using SVM with presentation features from CMU dataset", "labels": [], "entities": [{"text": "Misclassification Matrix", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9057483375072479}, {"text": "CMU dataset", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.8669591248035431}]}, {"text": " Table 5: Misclassification Matrix using SVM with presentation + word features from CMU dataset", "labels": [], "entities": [{"text": "Misclassification Matrix", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9112140536308289}, {"text": "CMU dataset", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8206929266452789}]}, {"text": " Table 3: Misclassification Matrix for SVM using 323 word frequencies from CMU dataset", "labels": [], "entities": [{"text": "Misclassification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9303613901138306}, {"text": "SVM", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9454281330108643}, {"text": "CMU dataset", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.8389375507831573}]}, {"text": " Table 6: Average recall in 10\u2212fold cross validation genre identification experiment; forced decision", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9500268697738647}, {"text": "cross validation genre identification", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.5984977409243584}]}, {"text": " Table 7: Mean recall in 10\u2212fold cross validation experiment; positive identification threshold applied", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9735856652259827}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.8585370182991028}]}]}