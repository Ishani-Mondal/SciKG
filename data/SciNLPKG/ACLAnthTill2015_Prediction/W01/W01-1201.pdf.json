{"title": [{"text": "Looking Under the Hood: Tools for Diagnosing Your Question Answering Engine", "labels": [], "entities": [{"text": "Diagnosing Your Question Answering", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.693992555141449}]}], "abstractContent": [{"text": "In this paper we analyze two question answering tasks : the TREC-8 question answering task and a set of reading comprehension exams.", "labels": [], "entities": [{"text": "question answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7780858278274536}, {"text": "TREC-8 question answering task", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.6568504199385643}]}, {"text": "First, we show that Q/A systems perform better when there are multiple answer opportunities per question.", "labels": [], "entities": []}, {"text": "Next, we analyze common approaches to two subproblems: term overlap for answer sentence identification , and answer typing for short answer extraction.", "labels": [], "entities": [{"text": "answer sentence identification", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.7058063745498657}, {"text": "short answer extraction", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.6606279810269674}]}, {"text": "We present general tools for analyzing the strengths and limitations of techniques for these sub-problems.", "labels": [], "entities": []}, {"text": "Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.8017271459102631}]}], "introductionContent": [{"text": "When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation.", "labels": [], "entities": []}, {"text": "For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: and).", "labels": [], "entities": [{"text": "opendomain question answering", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.649207760890325}]}, {"text": "In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children's reading levels (;;).", "labels": [], "entities": []}, {"text": "The performance statistics have been useful for determining how well techniques work.", "labels": [], "entities": []}, {"text": "However, raw performance statistics are not enough.", "labels": [], "entities": []}, {"text": "If the score is low, we need to understand what went wrong and how to fix it.", "labels": [], "entities": []}, {"text": "If the score is high, it is important to understand why.", "labels": [], "entities": []}, {"text": "For example, performance maybe dependent on characteristics of the current test set and would not carryover to anew domain.", "labels": [], "entities": []}, {"text": "It would also be useful to know if there is a particular characteristic of the system that is central.", "labels": [], "entities": []}, {"text": "If so, then the system can be streamlined and simplified.", "labels": [], "entities": []}, {"text": "In this paper, we explore ways of gaining insight into question answering system performance.", "labels": [], "entities": [{"text": "question answering system", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.8627772529919943}]}, {"text": "First, we analyze the impact of having multiple answer opportunities fora question.", "labels": [], "entities": []}, {"text": "We found that TREC-8 Q/A systems performed better on questions that had multiple answer opportunities in the document collection.", "labels": [], "entities": []}, {"text": "Second, we present a variety of graphs to visualize and analyze functions for ranking sentences.", "labels": [], "entities": []}, {"text": "The graphs revealed that relative score instead of absolute score is paramount.", "labels": [], "entities": [{"text": "relative score", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.9348046183586121}, {"text": "absolute score", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.9406652450561523}]}, {"text": "Third, we introduce bounds on functions that use term overlap 1 to rank sentences.", "labels": [], "entities": []}, {"text": "Fourth, we compute the expected score of a hypothetical Q/A system that correctly identifies the answer type fora question and correctly identifies all entities of that type in answer sentences.", "labels": [], "entities": []}, {"text": "We found that a surprising amount of ambiguity remains because sentences often contain multiple entities of the same type.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Maximal Overlap Set Analysis for CBC data", "labels": [], "entities": [{"text": "Maximal Overlap Set Analysis", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8328837752342224}]}, {"text": " Table 3: Expected scores and frequencies for each  answer type", "labels": [], "entities": []}]}