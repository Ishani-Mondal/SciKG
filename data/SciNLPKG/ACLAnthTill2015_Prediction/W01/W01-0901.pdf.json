{"title": [], "abstractContent": [], "introductionContent": [{"text": "The aim of this workshop is to identify and to synthesize current needs for language-technology evaluation.", "labels": [], "entities": []}, {"text": "The first part of the workshop will focus on one of the most challenging current issues in language engineering: the evaluation of dialogue systems and models.", "labels": [], "entities": [{"text": "language engineering", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7325757145881653}]}, {"text": "The second part will extend the discussion to address the problem of evaluation in language engineering more broadly and on more theoretical grounds.", "labels": [], "entities": []}, {"text": "The space of possible dialogues is enormous, even for limited domains like travel information servers.", "labels": [], "entities": []}, {"text": "The generalization of evaluation methodologies across different application domains and languages is an open problem.", "labels": [], "entities": []}, {"text": "Review of published evaluations of dialogue models and systems suggests that usability techniques are the standard method.", "labels": [], "entities": []}, {"text": "Dialogue-based system are often evaluated in terms of standard, objective usability metrics, such as task-completion time and number of user actions.", "labels": [], "entities": []}, {"text": "In the past, researchers have proposed and debated theory-based methods for modifying and testing the underlying dialogue model, but the most widely used method of evaluation is usability testing, although more precise and empirical methods for evaluating the effectiveness of dialogue models have been proposed.", "labels": [], "entities": []}, {"text": "For task-based interaction, typical measures of effectiveness are time-to-completion and task outcome, but the evaluation should focus on user satisfaction rather than on arbitrary effectiveness measurements.", "labels": [], "entities": []}, {"text": "Indeed, the problems faced in current approaches to measurement of effectiveness dialogue models and systems include: 1.", "labels": [], "entities": []}, {"text": "Direct measures are unhelpful because efficient performance on the nominal task may not represent the most effective interaction 2.", "labels": [], "entities": []}, {"text": "Indirect measures usually rely on judgment and are vulnerable to weak relationships between the inputs and outputs 5.", "labels": [], "entities": []}, {"text": "We have seen before that the the evaluation of dialogue models is still unsolved, but for domains where metrics already exists, are they satisfactory and sufficient?", "labels": [], "entities": []}, {"text": "How can we take into account or abstract from the subjective factor introduced by human operators in the process?", "labels": [], "entities": []}, {"text": "6. Do similarity measures and standards offer appropriate answers to this problem?", "labels": [], "entities": []}, {"text": "Most of the efforts focus on evaluating process, but what about the issue of language resources evaluation?", "labels": [], "entities": [{"text": "language resources evaluation", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.6241299112637838}]}, {"text": "In the second part of the workshop we wish to address the problem of evaluation both from a broader perspective, including novel applications domain for evaluation, new metrics for known tasks and resource evaluation, as well as look at the problem from a more theoretical point of view, including the isssue of formal theory of evaluation and infrastructural needs of language engineering.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}