{"title": [{"text": "A Mixture-of-Experts Framework for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7550430595874786}]}], "abstractContent": [{"text": "One of the particular characteristics of text classification tasks is that they present large class imbalances.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.8729944626490275}]}, {"text": "Such a problem can easily be tackled using re-sampling methods.", "labels": [], "entities": []}, {"text": "However, although these approaches are very simple to implement , tuning them most effectively is not an easy task.", "labels": [], "entities": []}, {"text": "In particular, it is unclear whether oversampling is more effective than undersampling and which oversampling or undersampling rate should be used.", "labels": [], "entities": []}, {"text": "This paper presents a method for combining different expressions of the re-sampling approach in a mixture of experts framework.", "labels": [], "entities": []}, {"text": "The proposed combination scheme is evaluated on a very imbalanced subset of the REUTERS-21578 text collection and is shown to be very effective on this domain .", "labels": [], "entities": [{"text": "REUTERS-21578 text collection", "start_pos": 80, "end_pos": 109, "type": "DATASET", "confidence": 0.9510284066200256}]}], "introductionContent": [{"text": "A typical use of Machine Learning methods in the context of Natural Language Processing is in the domain of text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.786789059638977}]}, {"text": "Unfortunately, several characteristics specific to text data make its classification a difficult problem to handle.", "labels": [], "entities": [{"text": "classification", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.9711110591888428}]}, {"text": "In particular, the data is typically highly dimensional and it presents a large class imbalance, i.e., there, typically, are very few documents on the topic of interest while texts on unrelated subjects abound.", "labels": [], "entities": []}, {"text": "Furthermore, although large amounts of texts are available online, little of them are labeled.", "labels": [], "entities": []}, {"text": "Because the class imbalance problem is known to negatively affect typical classifiers and because unlabeled data have no place in conventional supervised learning, using off-the-shelf supervised classifiers is likely not to be very successful in the context of text data.", "labels": [], "entities": []}, {"text": "It is, instead, recommended to devise a classification method specifically tuned to the text classification problem.", "labels": [], "entities": [{"text": "text classification problem", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.8195290962855021}]}, {"text": "The purpose of this study is to target some of the characteristics of text data in the hope of improving the effectiveness of the classification process.", "labels": [], "entities": []}, {"text": "The topics of finding a good representation for text data and dealing with its high dimensionality have been investigated previously with, for example, the use of Wordnet [e.g.,] and Support Vector Machines [e.g.,, respectively.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 163, "end_pos": 170, "type": "DATASET", "confidence": 0.9428743720054626}]}, {"text": "We will not be addressing these problems here.", "labels": [], "entities": []}, {"text": "The question that we will tackle in this paper, instead, is that of dealing with the class imbalance, and, in the process of doing so, that of finding away to take advantage of the extra, albeit, unlabeled data that are often left unused in classification studies.", "labels": [], "entities": []}, {"text": "Several approaches have previously been proposed to deal with the class imbalance problem including a simple and yet quite effective method: re-sampling [e.g.,,,].", "labels": [], "entities": []}, {"text": "This paper deals with the two different types of re-sampling approaches: methods that oversample the small class in order to make it reach a size close to that of the larger class and methods that undersample the large class in order to make it reach a size close to that of the smaller class.", "labels": [], "entities": []}, {"text": "Because it is unclear whether oversampling is more effective than undersampling and which oversampling or undersampling rate should be used, we propose a method for combining a number of classifiers that oversample and undersample the data at different rates in a mixture of experts framework.", "labels": [], "entities": []}, {"text": "The mixture-of-experts is constructed in the context of a decision tree induction system: C5.0, and all resampling is done randomly.", "labels": [], "entities": []}, {"text": "This proposed combination scheme is, subsequently, evaluated on a a subset of the REUTERS-21578 text collection and is shown to be very effective in this case.", "labels": [], "entities": [{"text": "REUTERS-21578 text collection", "start_pos": 82, "end_pos": 111, "type": "DATASET", "confidence": 0.9195855061213175}]}, {"text": "The remainder of this paper is divided into four sections.", "labels": [], "entities": []}, {"text": "Section 2 describes an experimental study on a series of artificial data sets to explore the effect of oversampling and undersampling and oversampling or undersampling at different rates.", "labels": [], "entities": []}, {"text": "This study suggests a mixtureof-experts scheme which is described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 discusses the experiment conducted with that mixture-of-experts scheme on a series of text-classification tasks and discusses their results.", "labels": [], "entities": []}, {"text": "Section 5 is the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We begin this work by studying the effects of oversampling versus undersampling and oversampling or undersampling at different rates.", "labels": [], "entities": []}, {"text": "All the experiments in this part of the paper are conducted over artificial data sets defined over the domain of 4 x 7 DNF expressions, where the first number represents the number of literals present in each disjunct and the second number represents the number of disjuncts in each concept.", "labels": [], "entities": []}, {"text": "We used an alphabet of size 50.", "labels": [], "entities": []}, {"text": "For each concept, we created a training set containing 240 positive and 6000 negative examples.", "labels": [], "entities": []}, {"text": "In other words, we 2 Throughout this work, we consider a fixed imbalance ratio, a fixed number of training examples and a fixed degree of concept complexity.", "labels": [], "entities": []}, {"text": "A thorough study relating different degrees of imbalance ratios, training set sizes and concept difficulty was previously reported in).", "labels": [], "entities": []}, {"text": "3 DNF expressions were specifically chosen because of their simplicity as well as their similarity to text data whose classification accuracy we are ultimately interested in improving.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9254738688468933}]}, {"text": "In particular, like in the case of text-classification, DNF concepts of interest are, generally, represented by much fewer examples than there are counter-examples of these concepts, especially when 1) the concept at hand is fairly specific; 2) the number of disjuncts and literals per disjunct grows larger; and 3) the values assumed by the literals are drawn from a large alphabet.", "labels": [], "entities": []}, {"text": "Furthermore, an important aspect of concept complexity can be expressed in similar ways in DNF and textual concepts since adding anew subtopic to a textual concept corresponds to adding anew disjunct to a DNF concept.", "labels": [], "entities": []}, {"text": "created an imbalance ratio of 1:25 in favor of the negative class.", "labels": [], "entities": []}, {"text": "Our combination scheme was tested on a subset of the 10 top categories of the REUTERS-21578 Data Set.", "labels": [], "entities": [{"text": "REUTERS-21578 Data Set", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.9095544815063477}]}, {"text": "We first present an overview of the data, followed by the results obtained by our scheme on these data.", "labels": [], "entities": []}], "tableCaptions": []}