{"title": [], "abstractContent": [{"text": "Significant amount of work has been devoted recently to develop learning techniques that can be used to generate partial (shallow) analysis of natural language sentences rather than a full parse.", "labels": [], "entities": [{"text": "generate partial (shallow) analysis of natural language sentences", "start_pos": 104, "end_pos": 169, "type": "TASK", "confidence": 0.773749727010727}]}, {"text": "In this work we set out to evaluate whether this direction is worthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform-identifying phrases in sentences.", "labels": [], "entities": []}, {"text": "We conclude that directly learning to perform these tasks as shallow parsers do is advantageous overfull parsers both in terms of performance and robustness to new and lower quality texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Shallow parsing is studied as an alternative to full-sentence parsing.", "labels": [], "entities": [{"text": "Shallow parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7842596769332886}, {"text": "full-sentence parsing", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7167830914258957}]}, {"text": "Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text.", "labels": [], "entities": []}, {"text": "A lot of recent work on shallow parsing has been influenced by Abney's work, who has suggested to \"chunk\" sentences to base level phrases.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.6922724843025208}]}, {"text": "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September .\" would be chunked as follows): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.", "labels": [], "entities": []}, {"text": "Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship).", "labels": [], "entities": []}, {"text": "Research on shallow parsing was inspired by psycholinguistics arguments) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.6610035002231598}, {"text": "conversational) full parsing", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.656234472990036}, {"text": "sentence processing and analysis", "start_pos": 177, "end_pos": 209, "type": "TASK", "confidence": 0.7518289312720299}]}, {"text": "First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 276, "end_pos": 298, "type": "TASK", "confidence": 0.8798043429851532}, {"text": "text summarization", "start_pos": 303, "end_pos": 321, "type": "TASK", "confidence": 0.7730596661567688}]}, {"text": "Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.", "labels": [], "entities": []}, {"text": "If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.", "labels": [], "entities": []}, {"text": "This can be augmented later if more information is available.", "labels": [], "entities": []}, {"text": "Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.", "labels": [], "entities": []}, {"text": "Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.9771544337272644}]}, {"text": "However, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time.", "labels": [], "entities": [{"text": "statistical learning of full parsers", "start_pos": 112, "end_pos": 148, "type": "TASK", "confidence": 0.7975831031799316}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9991399049758911}]}, {"text": "This paper investigates the question of whether work on shallow parsing is worthwhile.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.5332342386245728}]}, {"text": "That is, we attempt to evaluate quantitatively the intuitions described above -that learning to perform shallow parsing could be more accurate and more robust than learning to generate full parses.", "labels": [], "entities": []}, {"text": "We do that by concentrating on the task of identifying the phrase structure of sentences -a byproduct of full parsers that can also be produced by shallow parsers.", "labels": [], "entities": [{"text": "identifying the phrase structure of sentences", "start_pos": 43, "end_pos": 88, "type": "TASK", "confidence": 0.7524007856845856}]}, {"text": "We investigate two instantiations of this task, \"chucking\" and identifying atomic phrases.", "labels": [], "entities": []}, {"text": "And, to study robustness, we run our experiments both on standard Penn Treebank data (part of which is used for training the parsers) and on lower quality data -the Switchboard data.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.9956576228141785}, {"text": "Switchboard data", "start_pos": 165, "end_pos": 181, "type": "DATASET", "confidence": 0.9322313070297241}]}, {"text": "Our conclusions are quite clear.", "labels": [], "entities": []}, {"text": "Indeed, shallow parsers that are specifically trained to perform the tasks of identifying the phrase structure of a sentence are more accurate and more robust than full parsers.", "labels": [], "entities": [{"text": "identifying the phrase structure of a sentence", "start_pos": 78, "end_pos": 124, "type": "TASK", "confidence": 0.8574395946093968}]}, {"text": "We believe that this finding, not only justifies work in this direction, but may even suggest that it would be worthwhile to use this methodology incrementally, to learn a more complete parser, if needed.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to run a fair comparison between full parsers and shallow parsers -which could produce quite different outputs -we have chosen the task of identifying the phrase structure of a sentence.", "labels": [], "entities": []}, {"text": "This structure can be easily extracted from the outcome of a full parser and a shallow parser can be trained specifically on this task.", "labels": [], "entities": []}, {"text": "There is no agreement on how to define phrases in sentences.", "labels": [], "entities": []}, {"text": "The definition could depend on downstream applications and could range from simple syntactic patterns to message units people use in conversations.", "labels": [], "entities": []}, {"text": "For the purpose of this study, we chose to use two different definitions.", "labels": [], "entities": []}, {"text": "Both can be formally defined and they reflect different levels of shallow parsing patterns.", "labels": [], "entities": []}, {"text": "The first is the one used in the chunking competition in).", "labels": [], "entities": [{"text": "chunking competition", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.9100581109523773}]}, {"text": "In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.", "labels": [], "entities": []}, {"text": "The goal in this case is therefore to accurately predict a collection of \u00a2 \u00a3 \u00a2 different types of phrases.", "labels": [], "entities": []}, {"text": "The chunk types are based on the syntactic category part of the bracket label in the Treebank.", "labels": [], "entities": []}, {"text": "Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.", "labels": [], "entities": []}, {"text": "The phrases are: adjective phrase (ADJP), adverb phrase (ADVP), conjunction phrase (CONJP), interjection phrase (INTJ), list marker (LST), noun phrase (NP), preposition phrase (PP), particle (PRT), subordinated clause (SBAR), unlike coordinated phrase (UCP), verb phrase (VP).", "labels": [], "entities": []}, {"text": "(See details in.)", "labels": [], "entities": []}, {"text": "The second definition used is that of atomic phrases.", "labels": [], "entities": []}, {"text": "An atomic phrase represents the most basic phrase with no nested sub-phrases.", "labels": [], "entities": []}, {"text": "For example, in the parse tree, ( (S (NP (NP Pierre Vinken) , (ADJP (NP 61 years) old) ,) (VP will (VP join (NP the board) (PP as (NP a nonexecutive director)) (NP Nov. 29))) .)) Pierre Vinken, 61 years, the board, a nonexecutive director and Nov. 29 are atomic phrases while other higher-level phrases are not.", "labels": [], "entities": []}, {"text": "That is, anatomic phrase denotes a tightly coupled message unit which is just above the level of single words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Rankings of Shallow Parsers in  CoNLL-2000. See (Tjong Kim Sang and Buch- holz, 2000) for details.", "labels": [], "entities": [{"text": "CoNLL-2000", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9108923673629761}, {"text": "Tjong Kim Sang and Buch- holz, 2000)", "start_pos": 59, "end_pos": 95, "type": "DATASET", "confidence": 0.8215323597192764}]}, {"text": " Table 2: Precision & Recall for phrase identi- fication (chunking) for the full and the shallow  parser on the WSJ data. Results are shown for an  (weighted) average of 11 types of phrases as well  as for two of the most common phrases, NP and  VP.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998309850692749}, {"text": "Recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9963687658309937}, {"text": "WSJ data", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.9804052412509918}]}, {"text": " Table 4: Switchboard data: Precision & Re- call for phrase identification (chunking) on the  Switchboard data. Results are shown for an  (weighted) average of 11 types of phrases as well  as for two of the most common phrases, NP, VP.", "labels": [], "entities": [{"text": "Precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9973020553588867}, {"text": "Re- call", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9590471784273783}, {"text": "phrase identification (chunking", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.7630848363041878}, {"text": "Switchboard data", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.860107034444809}]}, {"text": " Table 5: Robustness: Relative degradation in", "labels": [], "entities": [{"text": "Robustness", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9515904784202576}, {"text": "Relative degradation", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6462353765964508}]}]}