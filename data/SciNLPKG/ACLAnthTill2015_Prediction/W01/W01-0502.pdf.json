{"title": [{"text": "A Sequential Model for Multi-Class Classification", "labels": [], "entities": [{"text": "Multi-Class Classification", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7889204621315002}]}], "abstractContent": [{"text": "Many classification problems require decisions among a large number of competing classes.", "labels": [], "entities": [{"text": "classification", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.9640227556228638}]}, {"text": "These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion.", "labels": [], "entities": []}, {"text": "We suggest a general approach-a sequential learning model that utilizes classi-fiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set.", "labels": [], "entities": []}, {"text": "Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains.", "labels": [], "entities": []}, {"text": "The advantages of the model are illustrated in an experiment in part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7902486324310303}]}], "introductionContent": [{"text": "A large number of important natural language inferences can be viewed as problems of resolving ambiguity, either semantic or syntactic, based on properties of the surrounding context.", "labels": [], "entities": []}, {"text": "These, in turn, can all be viewed as classification problems in which the goal is to select a class label from among a collection of candidates.", "labels": [], "entities": []}, {"text": "Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers.", "labels": [], "entities": [{"text": "part-of speech tagging", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6153916915257772}, {"text": "word-sense disambiguation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.7704918086528778}, {"text": "accent restoration", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.677883118391037}, {"text": "word choice selection in machine translation", "start_pos": 88, "end_pos": 132, "type": "TASK", "confidence": 0.6611714760462443}, {"text": "context-sensitive spelling correction", "start_pos": 134, "end_pos": 171, "type": "TASK", "confidence": 0.5979286134243011}, {"text": "word selection in speech recognition", "start_pos": 173, "end_pos": 209, "type": "TASK", "confidence": 0.6590479254722595}]}, {"text": "Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success.", "labels": [], "entities": [{"text": "classification problems", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.9169838428497314}]}, {"text": "A partial list consists of Bayesian classifiers (, decision lists, Bayesian hybrids, HMMs, inductive logic methods (, memory-\u00a3 This research is supported by NSF grants IIS-9801638, IIS-0085836 and SBR-987345.", "labels": [], "entities": [{"text": "NSF", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.8477069735527039}, {"text": "IIS-9801638", "start_pos": 168, "end_pos": 179, "type": "DATASET", "confidence": 0.5953421592712402}, {"text": "IIS-0085836", "start_pos": 181, "end_pos": 192, "type": "DATASET", "confidence": 0.8498585820198059}, {"text": "SBR-987345", "start_pos": 197, "end_pos": 207, "type": "DATASET", "confidence": 0.8598755598068237}]}, {"text": "based methods (), linear classifiers) and transformationbased learning.", "labels": [], "entities": []}, {"text": "In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large -all words in words selection problems, all possible tags in tagging problems etc.", "labels": [], "entities": [{"text": "words selection", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.7196898758411407}]}, {"text": "Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, a small set of candidates (typically two) is first selected, and the classifier is trained to choose among these.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.6927979290485382}]}, {"text": "While this approach is important in that it allows the research community to develop better learning methods and evaluate them in a range of applications, it is important to realize that an important stage is missing.", "labels": [], "entities": []}, {"text": "This could be significant when the classification methods are to be embedded as part of a higher level NLP tasks such as machine translation or information extraction, where the small set of candidates the classifier can handle may not be fixed and could be hard to determine.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.830020010471344}, {"text": "information extraction", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.7928144931793213}]}, {"text": "In this work we develop a general approach to the study of multi-class classifiers.", "labels": [], "entities": []}, {"text": "We suggest a sequential learning model that utilizes (almost) general purpose classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidate set.", "labels": [], "entities": []}, {"text": "In our paradigm the sought after classifier has to choose a single class label (or a small set of labels) from among a large set of labels.", "labels": [], "entities": []}, {"text": "It works by sequentially applying simpler classifiers, each of which outputs a probability distribution over the candidate labels.", "labels": [], "entities": []}, {"text": "These distributions are multiplied and thresholded, resulting in that each classifier in the sequence needs to deal with a (significantly) smaller number of the candidate labels than the previous classifier.", "labels": [], "entities": []}, {"text": "The classifiers in the sequence are selected to be simple in the sense that they typically work only on part of the feature space where the decomposition of feature space is done so as to achieve statistical independence.", "labels": [], "entities": []}, {"text": "Simple classifier are used since they are more likely to be accurate; they are chosen so that, with high probability (w.h.p.), they have one sided error, and therefore the presence of the true label in the candidate set is maintained.", "labels": [], "entities": []}, {"text": "The order of the sequence is determined so as to maximize the rate of decreasing the size of the candidate labels set.", "labels": [], "entities": []}, {"text": "Beyond increased accuracy on multi-class classification problems , our scheme improves the computation time of these problems several orders of magnitude, relative to other standard schemes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9982190728187561}, {"text": "multi-class classification", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7336798310279846}]}, {"text": "In this work we describe the approach, discuss an experiment done in the context of part-of-speech (pos) tagging, and provide some theoretical justifications to the approach.", "labels": [], "entities": [{"text": "part-of-speech (pos) tagging", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.6461104989051819}]}, {"text": "2 provides some background on approaches to multi-class classification in machine learning and in NLP.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7755522429943085}]}, {"text": "3 we describe the sequential model proposed here and in Sec.", "labels": [], "entities": []}, {"text": "4 we describe an experiment the exhibits some of its advantages.", "labels": [], "entities": []}, {"text": "Some theoretical justifications are outlined in Sec.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}