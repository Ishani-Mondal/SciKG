{"title": [{"text": "Verification and Validation of Language Processing Systems: Is It Evaluation?", "labels": [], "entities": [{"text": "Verification and Validation of Language Processing Systems", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.8280865635190692}]}], "abstractContent": [{"text": "If Natural Language Processing (NLP) systems are viewed as intelligent systems then we should be able to make use of verification and validation (V&V) approaches and methods that have been developed in the intelligent systems community.", "labels": [], "entities": []}, {"text": "This paper addresses language engineering infrastructure issues by considering whether standard V&V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&V in the context of language processing systems.", "labels": [], "entities": []}, {"text": "We argue that evaluation, as it is performed in the NL community, can be improved by supplementing it with methods from the V&V community.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The previous section presented definitions for V&V.", "labels": [], "entities": [{"text": "V&V", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.4958738187948863}]}, {"text": "In this section the general paradigms for evaluation of NLP system is presented.", "labels": [], "entities": []}, {"text": "Our review of the evaluation literature indicates that NLP systems have largely been evaluated using a black-box, functional, approach.", "labels": [], "entities": []}, {"text": "Evaluation is often subdivided into formative evaluation and summative evaluation.", "labels": [], "entities": [{"text": "summative evaluation", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.9204785525798798}]}, {"text": "The former determines if the system meets the objectives that were set for it.", "labels": [], "entities": []}, {"text": "It can be diagnostic, indicating areas in which the system does not perform well, or predictive, indicating the performance that can be expected in actual use.", "labels": [], "entities": []}, {"text": "Summative evaluation is a comparison of different systems or approaches for solving a single problem.", "labels": [], "entities": [{"text": "Summative evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9603410959243774}]}, {"text": "Ina somewhat different taxonomy, evaluation is subdivided into \u2022 Adequacy evaluation -determination of the fitness of a system for its intended purpose.", "labels": [], "entities": []}, {"text": "Will it do what is required by the user, how well, and at what cost?", "labels": [], "entities": []}, {"text": "\u2022 Diagnostic evaluation -exposure of system failures and production of a system performance profile.", "labels": [], "entities": [{"text": "Diagnostic evaluation", "start_pos": 2, "end_pos": 23, "type": "TASK", "confidence": 0.834336131811142}]}, {"text": "\u2022 Performance evaluation -measurement of system performance in one or more specific areas.", "labels": [], "entities": [{"text": "Performance evaluation", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.6769510507583618}]}, {"text": "Can be used to compare alternative implementations or successive generations of a system.", "labels": [], "entities": []}, {"text": "We can see that performance evaluation overlaps with summative evaluation, while adequacy evaluation corresponds to formative evaluation.", "labels": [], "entities": []}, {"text": "While the evaluation process must consider the results generated by an NLP system, it also considers the usability of the system, its features, and how easily it can be enhanced.", "labels": [], "entities": []}, {"text": "For example, a translation system may appear to work well in a testbed situation, but may not function well when embedded into a larger system.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9645228981971741}]}, {"text": "Or it may perform acceptably when its output is intended fora general audience, but not when an expert uses the output.", "labels": [], "entities": []}, {"text": "Sparck Jones and Galliers (1996) discuss how the evaluation process should take into account whether the NLP task is part of a larger system with both linguistic and non-linguistic components, and determine the impact on overall performance of each of the subparts.", "labels": [], "entities": []}, {"text": "We call this component performance evaluation.", "labels": [], "entities": []}, {"text": "Additional complexity arises in the evaluation of component performance within multi-faceted systems, such as embodied conversational agents, where assessment of how well the system works is based on more than strict language aspects, considering also more subtle features such as gesture and tone).", "labels": [], "entities": []}, {"text": "Furthermore, whether or not a system response is considered to be corrector acceptable may depend on who is judging it.", "labels": [], "entities": []}, {"text": "In general, NLP systems for various kinds of tasks require differing views of the evaluation process, with different criteria, measures, and methods.", "labels": [], "entities": []}, {"text": "For example, consider the ways in which evaluation of machine translation (MT) systems is carried out.", "labels": [], "entities": [{"text": "evaluation of machine translation (MT)", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.8141176359994071}]}, {"text": "Notice that not all aspects of validation and verification, as discussed in section 2, are represented.", "labels": [], "entities": [{"text": "validation", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9771562814712524}]}, {"text": "Evaluation of machine translation (MT) systems has to consider the pre-processing of input and the post-editing of output.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.8555712699890137}]}, {"text": "Black-box evaluation of MT systems can measure the percentage of words that are incorrect in the entire output text (based on how post-editing changes raw output text to fix it).", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9775413870811462}]}, {"text": "But whether or not a word is considered incorrect in the output may depend on the task of the system.", "labels": [], "entities": []}, {"text": "So functional evaluation of an MT system may have to be augmented by a subjective determination of whether the output text carries the same information as the input text, and whether the output is intelligible.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9868862628936768}]}, {"text": "Another example is the case of speech interfaces and spoken dialogue systems.", "labels": [], "entities": []}, {"text": "the evaluation process typically focuses on the accuracy, coverage, and speed of the system, with increasing attention paid to user satisfaction (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993175268173218}, {"text": "coverage", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9085193276405334}]}, {"text": "Notice that in just these two examples, various kinds of evaluation are called into play.", "labels": [], "entities": []}, {"text": "We will argue in section 4 that V&V techniques extend these evaluation methods, providing system coverage analysis that assesses completeness and consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 146, "end_pos": 157, "type": "METRIC", "confidence": 0.975590705871582}]}, {"text": "Evaluation of NLP systems must also take into account the kinds of inputs we expect a system to work on after its testing phase is complete.", "labels": [], "entities": []}, {"text": "demonstrates the extent to which the linguistic complexity of documents is one of the factors responsible for the weakness of applications that process natural language texts.", "labels": [], "entities": []}, {"text": "The ability to categorize test data by complexity will help distinguish between a failure of an NLP system that results from extraordinary document complexity (beyond that of the data on which the system was tested) and a failure that results from inadequate testing of the NLP tool.", "labels": [], "entities": []}, {"text": "The former should be predictable, while the latter should rarely happen if a system has been adequately tested.", "labels": [], "entities": []}, {"text": "It is certainly possible that a tool maybe very well tested, functionally and with regard to consistency and completeness, on text of certain degree of complexity, but still fail on text that is more complex or from a different domain.", "labels": [], "entities": [{"text": "consistency", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9906573295593262}]}, {"text": "There are NLP evaluation methods that, although in a different problem domain, closely mirror the approach typically used with expert systems, comparing machine results to human results.", "labels": [], "entities": [{"text": "NLP evaluation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8222541809082031}]}, {"text": "For example, the TAUM-AVIATION machine translation system was evaluated in 1980, in part by comparing the raw translation produced by the system to several human translations.", "labels": [], "entities": [{"text": "TAUM-AVIATION machine translation", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.6519954999287924}]}, {"text": "Then revised and post-edited translations (human and machine) were rated and ranked by a number of potential users.", "labels": [], "entities": []}, {"text": "This is essentially the same testing method that was used for the MYCIN expert system and many additional systems.", "labels": [], "entities": [{"text": "MYCIN expert system", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.7834320267041525}]}, {"text": "However, within the expert systems area several methods have been developed in subsequent years that address the weaknesses of strictly functional evaluation approaches (e.g..", "labels": [], "entities": []}, {"text": "There are also well-known evaluation efforts such as EAGLES (Sparck Jones and) and the Paradise evaluation framework ( It is important to note that the DARPA/ARPA sponsored conferences (MUC, TIPSTER, and TREC, for example), while making considerable data available, promote functional testing by stressing black-box performance of a system.", "labels": [], "entities": [{"text": "EAGLES", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.8898945450782776}, {"text": "MUC", "start_pos": 186, "end_pos": 189, "type": "DATASET", "confidence": 0.8236827254295349}]}, {"text": "The metrics used in the MUC program are oriented toward functional testing, focusing on the number of spots in a template that are correctly filled in by a particular MUC system, along with various error-based measures.", "labels": [], "entities": []}, {"text": "For database query systems the emphasis has been on functional testing, supplemented with evaluations of the system by users, given the desire to create marketable systems.", "labels": [], "entities": []}, {"text": "An issue that arises in comparative evaluation efforts, particularly because there is so much test data available, is what it means to compare the behavior of two systems designed to carryout the same task, based on their performance on a common set of test data.", "labels": [], "entities": []}, {"text": "argues that evaluation results for individual systems, and any comparison of results across systems, should not be given much credence until they reach \"some reasonably high level of performance.\"", "labels": [], "entities": []}, {"text": "Certainly the MUC and TREC programs are based on comparing performance of multiple systems on a common task.", "labels": [], "entities": [{"text": "MUC", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.7079795598983765}, {"text": "TREC", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.5756069421768188}]}, {"text": "One of the purposes of our research is to show that without assessment of consistency and completeness, the quality of the functional testing alone may not be sufficient for predicting reliability of an NLP system and V&V methods will improve the situation.", "labels": [], "entities": [{"text": "consistency", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.994320273399353}]}, {"text": "In the previous section we outlined many different types of evaluation that are performed on NL systems.", "labels": [], "entities": []}, {"text": "Our claim at the beginning of the paper was that evaluation, as it is performed in the NL community, can be improved by adopting V&V approaches.", "labels": [], "entities": []}, {"text": "In this section we show specifically what the relationship is between V&V, as it is typically applied in software development, and evaluation as it is carreid out in the context of NLP systems.", "labels": [], "entities": []}, {"text": "In considering whether V&V and evaluation are equivalent, we need to consider whether the evaluation process achieves the goals of verification and validation.", "labels": [], "entities": []}, {"text": "That is, does the evaluation process demonstrate that \u2022 the system is correct and conforms to its specification \u2022 the knowledge inherent in the system is consistent and complete \u2022 the output is equivalent to that of human 'experts'.", "labels": [], "entities": []}, {"text": "It is apparent that summative, adequacy and diagnostic evaluation are all in someway equivalent to validation.", "labels": [], "entities": [{"text": "diagnostic", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9779582619667053}]}, {"text": "The evaluation steps involve black-box exercise of test data through the system, which then allows fora comparison of actual results to expected results.", "labels": [], "entities": []}, {"text": "This facilitates an assessment of whether the output is equivalent to that of human experts (who provide the expected results).", "labels": [], "entities": []}, {"text": "The usual evaluation processes, through formative evaluation, also facilitate one aspect of verification, in that they allow us to determine if a system conforms to its specification.", "labels": [], "entities": []}, {"text": "That is, based on the specification fora system, a domain-based test set can be constructed for evaluation which will then demonstrate whether or not a system meets the specification.", "labels": [], "entities": []}, {"text": "It is the second aspect of verification, determining whether the knowledge represented within the system is consistent and complete, that seems not to betaken into account by the evaluation processes in NLP.", "labels": [], "entities": []}, {"text": "The difficulty lies in the fact that a domain based test set can never completely test the actual system as built.", "labels": [], "entities": []}, {"text": "Rather, it tests the linguistic assumptions that motivated construction of the system.", "labels": [], "entities": []}, {"text": "A domain based test set can determine if the system behaves correctly over the test data, but may not adequately test the full system.", "labels": [], "entities": []}, {"text": "In particular, any inconsistencies in the knowledge represented within the system, or missing knowledge, may not be identified by an evaluation process that relies on domain-based test data.", "labels": [], "entities": []}, {"text": "To address this issue, we need to apply additional testing techniques, based on coverage of the actual system, in order to achieve the full breadth of verification activities on a language processing system.", "labels": [], "entities": []}, {"text": "Furthermore, we may not need larger test sets, but we may need different test cases in the test set.", "labels": [], "entities": []}], "tableCaptions": []}