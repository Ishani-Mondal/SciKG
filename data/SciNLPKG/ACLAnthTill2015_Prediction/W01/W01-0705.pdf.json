{"title": [{"text": "Automatic Verb Classiication Using Multilingual Resources", "labels": [], "entities": [{"text": "Automatic Verb Classiication", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5656342109044393}]}], "abstractContent": [{"text": "We propose the use of multilingual corpora in the automatic classiication of verbs.", "labels": [], "entities": []}, {"text": "We extend the work of Merlo and Stevenson, 2001, in which statistics oversimple syntactic features extracted from textual corpora were used to train an automatic classiier for three lexical semantic classes of English verbs.", "labels": [], "entities": []}, {"text": "We h ypoth-esize that some lexical semantic features that are diicult to detect superrcially in English may manifest themselves as easily extractable surface syntactic features in another language.", "labels": [], "entities": []}, {"text": "Our experimental results combining English and Chinese features show that a small bilingual corpus may provide a useful alternative to using a large monolingual corpus for verb classiication.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, an umber of researchers have devised corpus-based approaches for automatically learning the lexical semantic class of verbs e.g.,.", "labels": [], "entities": []}, {"text": "Automatic verb classiication yields important potential beneets for the creation of lexical resources.", "labels": [], "entities": []}, {"text": "Lexical semantic classes incorporate both syntactic and semantic information about verbs, such as the general sense of the verb e.g., change-of-state or manner-of-motion and the allowable mapping of verbal arguments to syntactic positions e.g., whether an experiencer argument can appear as the subject or the object of the verb.", "labels": [], "entities": []}, {"text": "By automatically learning the assignment of v erbs to lexical semantic classes, each verb inherits a great deal of information about its possible usage in an NLP system, without that information having to be explicitly hand-coded.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of multilingual corpora in the automatic learning of verb classiication.", "labels": [], "entities": []}, {"text": "We extend the work of, in which statistics oversimple syntactic features extracted from syntactically annotated corpora were used to train an automatic classiier fora set of sample lexical semantic classes of English verbs.", "labels": [], "entities": []}, {"text": "This work had two potential limitations: rst, only a small number ve of syntactic features that correlate with semantic class were proposed; second, a very large corpus was needed 65M words to extract suuciently discriminating statistics.", "labels": [], "entities": [{"text": "rst", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9634294509887695}]}, {"text": "We address both of these issues in the current study by exploiting the use of a parallel English-Chinese corpus.", "labels": [], "entities": []}, {"text": "Our motivating hypothesis is that some lexical semantic features that are diicult to detect superrcially in English may manifest themselves as surface syntactic features in another language.", "labels": [], "entities": []}, {"text": "If this is indeed the case, then we should be able to augment the initial set of English features with features over the translated verbs in the other language in our case, Chinese.", "labels": [], "entities": []}, {"text": "Our hypothesis that a non-English verb feature set can be useful in English verb classiication is inspired by SLA Second Language Acquisition research on learning English verbs.", "labels": [], "entities": [{"text": "SLA Second Language Acquisition", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.48883870989084244}]}, {"text": "As the name suggests, SLA research studies how h umans acquire a second language.", "labels": [], "entities": [{"text": "SLA", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9421064257621765}]}, {"text": "\ud97b\udf59Transfer eeects\"|the impact of one's native language when learning a second language Ellis, 1997|are of particular interest to us.", "labels": [], "entities": []}, {"text": "Recent research has shown that properties of a non-English native lexicon can innuence human learning of English verb class distinctions e.g.,.", "labels": [], "entities": []}, {"text": "Carrying this idea of transfer\" over to the machine learning setting, we hypothesize that features from a second language may provide an additional source of information that complements the English features, making it possible that a smaller corpus a bitext can be a useful alternative to using a large monolingual corpus for verb classiication.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use the Hong Kong Laws Parallel Text HKLaws from the Linguistic Data Consortium, a sentence-aligned bilingual corpus with 6.5M words of English, and 9M characters of Chinese.", "labels": [], "entities": [{"text": "Hong Kong Laws Parallel Text HKLaws from the Linguistic Data Consortium", "start_pos": 31, "end_pos": 102, "type": "DATASET", "confidence": 0.8501232401891188}]}, {"text": "We tagged the Chinese portion of the corpus using the CKIP tagger, and the English portion using Ratnaparkhi's tagger Ratnaparkhi, 1996.", "labels": [], "entities": [{"text": "CKIP tagger", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9442977011203766}, {"text": "Ratnaparkhi's tagger Ratnaparkhi, 1996", "start_pos": 97, "end_pos": 135, "type": "DATASET", "confidence": 0.8171485463778178}]}, {"text": "Note that the English portion of HKLaws is about 10 of the size of the corpus used by Merlo and Stevenson 2001 in their original experiments, so we are restricted to a much smaller source of data.", "labels": [], "entities": [{"text": "HKLaws", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.5317808389663696}]}, {"text": "Given the relatively small size of our corpus, and its narrow domain, we were only able to nd a sample of 16 change-of-state and 16 creationntransformation verbs in English of suucient frequency; seethe appendix for the list of verbs used.", "labels": [], "entities": []}, {"text": "The English features for these 32 verbs were automatically extracted using regular expressions over the tagged English portion of the corpus.", "labels": [], "entities": []}, {"text": "The Chinese features were calculated as follows.", "labels": [], "entities": []}, {"text": "For each English verb, we manually determined the Chinese translation in each aligned sentence to yield a collection of all aligned translations of the verb.", "labels": [], "entities": []}, {"text": "This is the aligned translation set.\"", "labels": [], "entities": []}, {"text": "We also extracted all occurrences of the Chinese verbs in the aligned translation set across the corpus, yielding the unaligned translation set\"|i.e., the possible Chinese translations of an English target verb even when they did not occur as the translation of that verb.", "labels": [], "entities": []}, {"text": "The required counts for the Chinese features were collected for these verbs partly automatically Chinese Verb POS tags, Passive Particles, Periphrastic Particles, and Morpheme Length and partly by hand Semantic Speci\ud97b\udf59city and Morpheme POS combinations.", "labels": [], "entities": []}, {"text": "The value of a Chinese feature fora given verb is the normalized frequency of occurrence of the feature across all occurrences of that verb in the given translation set.", "labels": [], "entities": []}, {"text": "The resulting frequencies for the aligned translation set form the aligned dataset, and those for the unaligned translation set form the unaligned dataset.", "labels": [], "entities": []}, {"text": "The motivation for collecting unaligned data is to examine an alternative method for combining multilingual data.", "labels": [], "entities": []}, {"text": "Note that parallel corpora, especially those that are sentencealigned, are diicult to construct.", "labels": [], "entities": []}, {"text": "Most parallel corpora we found are considerably smaller than some of the more popular monolingual ones.", "labels": [], "entities": []}, {"text": "Given that more monolingual corpora are available, we want to explore the possibility of using non-parallel texts from multiple languages hence, necessarily unaligned data, rather than solely looking at bilingual corpora.", "labels": [], "entities": []}, {"text": "In order to compare our results to the monolingual method on a large corpus as in, we also collected the 5 English features for our verbs from the 65M word WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 156, "end_pos": 166, "type": "DATASET", "confidence": 0.8666927516460419}]}, {"text": "As a result, we ha ve a total of four data sets: English HKLaws dataset, English WSJ dataset, aligned Chinese HKLaws dataset, and unaligned Chinese HKLaws dataset.", "labels": [], "entities": [{"text": "English HKLaws dataset", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.8014858961105347}, {"text": "English WSJ dataset", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.7893510858217875}, {"text": "HKLaws dataset", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.8468698263168335}, {"text": "Chinese HKLaws dataset", "start_pos": 140, "end_pos": 162, "type": "DATASET", "confidence": 0.6344634691874186}]}, {"text": "This allows us to look at four datasets individually the two English and two Chinese sets, and to pair up the English and Chinese datasets in four diierent w ays each English set paired with each Chinese set.", "labels": [], "entities": []}, {"text": "The data for each of our machine learning experiments consists of a vector of the relevant English anddor Chinese features for each v erb: Template: verb, Eng.", "labels": [], "entities": []}, {"text": "Feats., class Example: altered, 0.04, . .", "labels": [], "entities": [{"text": "altered", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9748342037200928}]}, {"text": ",1 ,c hange-of-state Combining all the English and Chinese features yields a total of 22 features.", "labels": [], "entities": [{"text": "hange-of-state", "start_pos": 6, "end_pos": 20, "type": "METRIC", "confidence": 0.993049144744873}]}, {"text": "We use the resulting vectors as the training data fora classiier using the same decision tree algorithm as in C5.0, http:::www.rulequest.com.", "labels": [], "entities": []}, {"text": "We used both 8-fold cross-validation repeated 50 times and leave-one-out training methodologies for our experiments.", "labels": [], "entities": []}, {"text": "For our 8-fold cross-validation experiments, we empirically tested the tuning options available in C5.0.", "labels": [], "entities": []}, {"text": "Except for the tree pruning percentage, we found the available options ooer little to no improvements over the default settings.", "labels": [], "entities": []}, {"text": "We set the pruning factor to 30 for the best overall performance over a variety of different combinations of features.", "labels": [], "entities": [{"text": "pruning factor", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9558007121086121}]}, {"text": "According to the manual, the default is 25.", "labels": [], "entities": []}, {"text": "A larger pruning factor results in less pruning in the decision tree.", "labels": [], "entities": []}, {"text": "The cross-validation experiments train on a large number of random subsets of the data, for which we report average accuracy and standard error.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9964467883110046}, {"text": "standard error", "start_pos": 129, "end_pos": 143, "type": "METRIC", "confidence": 0.9604246616363525}]}, {"text": "The goal of the cross-validation experiments is to evaluate the contribution of different features to learning, and if possible nd 2 An 8-fold cross-validation experiment divides the data into eight parts folds and runs eight times, each time training on a diierent 778 of the data and testing on the remaining 118.", "labels": [], "entities": []}, {"text": "We chose 8 folds simply because it evenly divides our 32 verbs.", "labels": [], "entities": []}, {"text": "In leave-one-out experiments, we leave out one vector for testing and use the remaining vectors for training, repeated 32 times once for each verb.", "labels": [], "entities": []}, {"text": "To do so, we varied the precise set of features used in each experiment.", "labels": [], "entities": []}, {"text": "Since we ha ve a total of 17 features, performing an exhaustive search of 2 131 thousand experiments is nearly impossible.", "labels": [], "entities": []}, {"text": "Instead, we analysed the performance of individual monolingual features alone, and their performance when combined with the features from the other language.", "labels": [], "entities": []}, {"text": "The leave-one-out experiments complement the cross-validation methodology: there area small number of tests, but we ha ve the result of classifying each verb rather than average performance data on random subsets.", "labels": [], "entities": []}, {"text": "Our goal for the leave-one-out experiments is to compare the precision and recall across the two classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996397495269775}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9989402890205383}]}, {"text": "A feature is selected for the leave-oneout experiments if it contributed highly to performance in the cross-validation experiments.", "labels": [], "entities": []}, {"text": "We report here the key results of our crossvalidation and leave-one-out experiments.", "labels": [], "entities": []}, {"text": "For additional results and details, see.", "labels": [], "entities": []}, {"text": "Since our task is a two-way classication with equal-sized classes, the chance accuracy is 50.", "labels": [], "entities": [{"text": "chance", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9413138628005981}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8680576682090759}]}, {"text": "Although the theoretical maximum accuracy is 100, it is worth noting that, for their three-way verb classiication task, experimentally determined a best performance of 87 among a group of human experts, indicating that a more realistic upper-bound for the machine-learning task falls well below 100.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9992019534111023}]}], "tableCaptions": [{"text": " Table 1: Accuracy Acc. and Standard Error SE in the 8-Fold Cross-Validation Experi- ments, Using English Features Only", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988701939582825}, {"text": "Acc.", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.636035144329071}, {"text": "Standard Error", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.8645201325416565}, {"text": "SE", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.6000790596008301}]}, {"text": " Table 2: Accuracy Acc. and Standard Error SE in the 8-Fold Cross-Validation Experi- ments, Using Chinese Features Only", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989411234855652}, {"text": "Acc.", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.6902946829795837}, {"text": "Standard Error", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.8602783381938934}, {"text": "SE", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.6594837307929993}]}, {"text": " Table 3: Accuracy Acc. and Standard Error SE in the 8-Fold Cross-Validation Experi- ments, Using a Combination of English and Chinese Features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987657070159912}, {"text": "Acc.", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.6887661814689636}, {"text": "Standard Error", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.8504492938518524}, {"text": "SE", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.6122145056724548}]}, {"text": " Table 4: F-measure F, Accuracy Acc., and Number of Errors E in the Leave-one-out  Experiments. 1 = CKIP Tags; 2 = Passive P articles; 3 = Periphrastic Particles", "labels": [], "entities": [{"text": "F-measure F", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8277221024036407}, {"text": "Accuracy Acc.", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.7926315367221832}, {"text": "Number of Errors E", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.9561391770839691}]}]}