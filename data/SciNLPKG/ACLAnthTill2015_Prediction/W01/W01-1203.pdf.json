{"title": [{"text": "Parsing and Question Classification for Question Answering", "labels": [], "entities": [{"text": "Question Classification", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7064591497182846}, {"text": "Question Answering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7748352289199829}]}], "abstractContent": [{"text": "This paper describes machine learning based parsing and question classification for question answering.", "labels": [], "entities": [{"text": "machine learning based parsing", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.5987118631601334}, {"text": "question classification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7554960250854492}, {"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.821511447429657}]}, {"text": "We demonstrate that for this type of application, parse trees have to be semantically richer and structurally more oriented towards semantics than what most treebanks offer.", "labels": [], "entities": []}, {"text": "We empirically show how question parsing dramatically improves when augmenting a semantically enriched Penn treebank training corpus with an additional question treebank.", "labels": [], "entities": [{"text": "question parsing", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8482180833816528}, {"text": "Penn treebank training corpus", "start_pos": 103, "end_pos": 132, "type": "DATASET", "confidence": 0.7580997496843338}]}], "introductionContent": [{"text": "There has recently been a strong increase in the research of question answering, which identifies and extracts answers from a large collection of text.", "labels": [], "entities": [{"text": "question answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.859573483467102}]}, {"text": "Unlike information retrieval systems, which return whole documents or larger sections thereof, question answering systems are designed to deliver much more focused answers, e.g. Q: Where is Ayer's Rock?", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.7049396187067032}, {"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8086116909980774}, {"text": "Q: Where is Ayer's Rock?", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.6325840875506401}]}, {"text": "A: in central Australia Q: Who was Gennady Lyachin?", "labels": [], "entities": []}, {"text": "A: captain of the Russian nuclear submarine Kursk The August 2000 TREC-9 short form Q&A track evaluations, for example, specifically limited answers to 50 bytes.", "labels": [], "entities": [{"text": "TREC-9 short form Q&A track evaluations", "start_pos": 66, "end_pos": 105, "type": "TASK", "confidence": 0.5212271697819233}]}, {"text": "The Webclopedia project at the USC Information Sciences Institute () pursues a semantics-based approach to answer pinpointing that relies heavily on parsing.", "labels": [], "entities": [{"text": "answer pinpointing", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.9224942922592163}]}, {"text": "Parsing covers both questions as well as numerous answer sentence candidates.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9404163360595703}]}, {"text": "After parsing, exact answers are extracted by matching the parse trees of answer sentence candidates against that of the parsed question.", "labels": [], "entities": []}, {"text": "This paper describes the critical challenges that a parser faces in Q&A applications and reports on a number of extensions of a deterministic machine-learning based shift-reduce parser, CONTEX), which was previously developed for machine translation applications.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.7578526437282562}]}, {"text": "In particular, section 2 describes how additional treebanking vastly improved parsing accuracy for questions; section 3 describes how the parse tree is extended to include the answer type of a question, a most critical task in question answering; section 4 presents experimental results for question parsing and QA typing; and finally, section 5 describes how the parse trees of potential answer sentences are enhanced semantically for better question-answer matching.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9353157877922058}, {"text": "question answering", "start_pos": 227, "end_pos": 245, "type": "TASK", "confidence": 0.7444611191749573}, {"text": "question parsing", "start_pos": 291, "end_pos": 307, "type": "TASK", "confidence": 0.78687784075737}, {"text": "QA typing", "start_pos": 312, "end_pos": 321, "type": "TASK", "confidence": 0.8776485621929169}]}], "datasetContent": [{"text": "In the first two test runs, the system was trained on 2000 and 3000 Wall Street Journal sentences (enriched Penn Treebank).", "labels": [], "entities": [{"text": "Wall Street Journal sentences", "start_pos": 68, "end_pos": 97, "type": "DATASET", "confidence": 0.9278403222560883}, {"text": "Penn Treebank", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9748489260673523}]}, {"text": "In runs three and four, we trained the parser with the same Wall Street Journal sentences, augmented by the 38 treebanked pre-TREC8 questions.", "labels": [], "entities": [{"text": "Wall Street Journal sentences", "start_pos": 60, "end_pos": 89, "type": "DATASET", "confidence": 0.9621990621089935}]}, {"text": "For the fifth run, we further added the 200 TREC8 questions as training sentences when testing TREC9 questions, and the first 200 TREC9 questions as training sentences when testing TREC8 questions.", "labels": [], "entities": []}, {"text": "For the final run, we divided the 893 TREC-8 and TREC-9 questions into 5 test subsets of about 179 fora five-fold cross validation experiment, in which the system was trained on 2000 WSJ sentences plus about 975 questions (all 1153 questions minus the approximately 179 test sentences held back for testing).", "labels": [], "entities": [{"text": "TREC-8", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.7687674164772034}, {"text": "TREC-9", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.7016056776046753}]}, {"text": "In each of the 5 subtests, the system was then evaluated on the test sentences that were held back, yielding a total of 893 test question sentences.", "labels": [], "entities": []}, {"text": "The Wall Street Journal sentences contain a few questions, often from quotes, but not enough and not representative enough to result in an acceptable level of question parsing accuracy.", "labels": [], "entities": [{"text": "Wall Street Journal sentences", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.9688446521759033}, {"text": "question parsing", "start_pos": 159, "end_pos": 175, "type": "TASK", "confidence": 0.7383203506469727}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.8277229070663452}]}, {"text": "While questions are typically shorter than newspaper sentences (making parsing easier), the word order is often markedly different, and constructions like preposition stranding (\"What university was Woodrow Wilson President of?\") are much more common.", "labels": [], "entities": [{"text": "preposition stranding (\"What university was Woodrow Wilson President", "start_pos": 155, "end_pos": 223, "type": "TASK", "confidence": 0.6262357698546516}]}, {"text": "The results in show how crucial it is to include additional questions when training a parser, particularly with respect to Qtarget accuracy.", "labels": [], "entities": [{"text": "Qtarget", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.47694891691207886}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.8423819541931152}]}, {"text": "With an additional 1153 treebanked questions as training input, parsing accuracy levels improve considerably for questions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9621089696884155}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9792575240135193}]}], "tableCaptions": [{"text": " Table 1: Parse tree accuracies for varying amounts and types of training data.  Total number of test questions per experiment: 1153", "labels": [], "entities": [{"text": "Parse tree accuracies", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8837930560112}]}]}