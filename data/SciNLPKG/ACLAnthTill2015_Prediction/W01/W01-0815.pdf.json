{"title": [{"text": "Evaluating text quality: judging output texts without a clear source", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider how far two attributes of text quality commonly used in MT evaluation-intelligibility and fidelity-apply within NLG.", "labels": [], "entities": [{"text": "MT evaluation-intelligibility", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.969105988740921}]}, {"text": "While the former appears to transfer directly, the latter needs to be completely re-interpreted.", "labels": [], "entities": []}, {"text": "We make a crucial distinction between the needs of symbolic authors and those of end-readers.", "labels": [], "entities": []}, {"text": "We describe a form of textual feedback, based on a controlled language used for specifying software requirements that appears well suited to authors' needs, and an approach for incrementally improving the fidelity of this feedback text to the content model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probably the most critical questions that need to be addressed when evaluating automatically generated texts are: does the text actually say what it's supposed to say and is it fluent, coherent, clear and grammatical?", "labels": [], "entities": []}, {"text": "The answers to these questions say something important about how good the target texts are and -perhaps more to the point -how good the system that generated them is.", "labels": [], "entities": []}, {"text": "There is no a priori reason why the target texts should be any better or worse when they result from natural language generation (NLG) or from machine translation (MT): indeed, they could result from the same language generator.", "labels": [], "entities": [{"text": "natural language generation (NLG) or from machine translation (MT)", "start_pos": 101, "end_pos": 167, "type": "TASK", "confidence": 0.7692725635491885}]}, {"text": "Given this, it maybe natural to assume that NLG could appropriately adopt evaluation methods developed for its more mature sister, MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.6958076357841492}]}, {"text": "However, while this holds true for issues related to intelligibility (the second critical question), it does not apply as readily to issues of fidelity (the first question).", "labels": [], "entities": []}, {"text": "We go beyond our recent experience of evaluating the AGILE system for producing multilingual versions of software user manuals and raise some open questions about how best to evaluate the faithfulness of an output text with respect to its input specification.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}