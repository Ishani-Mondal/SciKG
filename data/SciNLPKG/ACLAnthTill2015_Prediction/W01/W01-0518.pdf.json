{"title": [{"text": "Probabilistic Context-Free Grammars for Syllabiication and Grapheme-to-Phoneme Conversion", "labels": [], "entities": [{"text": "Grapheme-to-Phoneme Conversion", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.7252892553806305}]}], "abstractContent": [{"text": "We investigated the applicability of probabilis-tic context-free grammars to syllabiication and grapheme-to-phoneme conversion.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.7413698434829712}]}, {"text": "The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries.", "labels": [], "entities": [{"text": "predicting syllable boundaries", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.8865483999252319}]}, {"text": "However, our results indicate that the standard probability model does not solve grapheme-to-phoneme conversion suuciently although, we varied all free parameters of the probabilistic reestimation procedure .", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.6099356114864349}]}], "introductionContent": [{"text": "In this paper we present an approach to unsupervised learning and automatic detection of syllable boundaries as well as automatic grapheme-to-phoneme conversion using probabilistic context-free grammars (PCFGs).", "labels": [], "entities": [{"text": "automatic detection of syllable boundaries", "start_pos": 66, "end_pos": 108, "type": "TASK", "confidence": 0.7474233448505402}]}, {"text": "A text-to-speech system (TTS), like those described in, needs a module where the words are converted from graphemes to phonemes, i.e. its transcription, and one that syllabiies the obtained phoneme string before they can be further processed to speech.", "labels": [], "entities": []}, {"text": "The two tasks can be solved both with rule-based and with probabilistic methods.", "labels": [], "entities": []}, {"text": "Rule-based methods are facing the problem that they have to return one analysis.", "labels": [], "entities": []}, {"text": "If there are several possible analyses then the rule-based system has the problem of disambiguation.", "labels": [], "entities": []}, {"text": "Probabilistic methods, however, yield the most probable analysis according to the training corpus.", "labels": [], "entities": []}, {"text": "Our approach builds on two resources.", "labels": [], "entities": []}, {"text": "The \ud97b\udf59rst resource are manually constructed context-free grammars (CFGs) for both syllabiication and grapheme-to-phoneme conversion (G2P).", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.7417640388011932}]}, {"text": "The CFG generates for the G2P task all sequences of phonemes corresponding to a given orthographic input word.", "labels": [], "entities": []}, {"text": "For the syllabiication task, the CFG generates all possible syllable boundaries.", "labels": [], "entities": []}, {"text": "We use context-free grammars for generating transcriptions, and syllabiied phoneme strings, because grammars are expressive and writing grammar-rules is easy and intuitive.", "labels": [], "entities": []}, {"text": "We trained the CFGs on a training corpus that was extracted from a large newspaper corpus.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.9357094168663025}, {"text": "newspaper corpus", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.8011051416397095}]}, {"text": "The second resource consists of the inside-outside algorithm that was used for the training procedure, sustaining probabilistic context-free grammars.", "labels": [], "entities": []}, {"text": "The obtained models were evaluated on a test corpus.", "labels": [], "entities": []}, {"text": "The results of our experiments show that PCFGs are good in predicting syllable boundaries, but simple PCFGs do not yield good results for grapheme-to-phoneme conversion.", "labels": [], "entities": [{"text": "predicting syllable boundaries", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.8949235677719116}, {"text": "grapheme-to-phoneme conversion", "start_pos": 138, "end_pos": 168, "type": "TASK", "confidence": 0.736664742231369}]}, {"text": "Our method, used for the experiments described in this paper, is based on a manually constructed context-free grammar with about 50 rules which returns fora given phoneme string all possible analyses.", "labels": [], "entities": []}, {"text": "Our grammar describes how w ords are composed of syllables and syllables branch into onset, nucleus and coda.", "labels": [], "entities": []}, {"text": "These syllable parts are re-written by the grammar as sequences of natural phone classes, e.g. stops, fricatives, nasals, liquids, as well as long and short vowels, and diphtongs.", "labels": [], "entities": []}, {"text": "The phone classes are then re-interpreted as the individual phonemes that they are made up of. shows some of the rules of the context-free grammar.", "labels": [], "entities": []}, {"text": "The \ud97b\udf59rst rule (1.1) in \ud97b\udf59gure 1 describes a3) a word consisting of two and three syllables, respectively.", "labels": [], "entities": [{"text": "\ud97b\udf59rst rule", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.891308585802714}]}, {"text": "The subsequent rule (1.4) speciies a syllable without an onset and coda, whereas in rule (1.5) the coda is missing and in (1.7) the onset.", "labels": [], "entities": []}, {"text": "Rule (1.6) depicts how a syllable consists of an onset, nucleus and coda.", "labels": [], "entities": []}, {"text": "The next two rules (1.8)-(1.9) describe the complexity of the onset and the rules (1.13)-(1.14) the complexity of the coda, each consisting of one or two phonemes.", "labels": [], "entities": []}, {"text": "An onsets consists of a liquid, which is shown in rules (1.10).", "labels": [], "entities": []}, {"text": "Rule (1.12) describe a short vowel, which is re-written in rule (1.21)-  We transform the context-free grammar by a training procedure to a probabilistic CFG.", "labels": [], "entities": []}, {"text": "We then choose the analysis with the highest probability.", "labels": [], "entities": []}, {"text": "The probability of one analysis is deened as the product of the probabilities of the grammar rules appearing in the analysis.", "labels": [], "entities": []}, {"text": "In our example the correct syllable segmentation received the highest probability oped by, and generalized by, for the transformation of a context-free grammar to a PCFG, the so called training procedure.", "labels": [], "entities": [{"text": "syllable segmentation", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7450127005577087}]}, {"text": "In an initializing phase, the grammar rules are assigned random probabilities, which are reestimated during several iterations yielding the rule probabilities.", "labels": [], "entities": []}, {"text": "There are three free parameters that can be varied: (1) the training corpus, (2) the number of iterations, and (3) the start parameters.", "labels": [], "entities": []}, {"text": "We used the freely available lopar parser, implemented by. shows a fragment of the best performing PCFG with the rule probabilities used for syllabiication.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9143747091293335}]}, {"text": "Rules (2.1)-(2.3) show that the most probable word structure is a word consisting of one syllable, a two-syllabic word is less probable and the least probable structure is a three-syllabic word.", "labels": [], "entities": []}, {"text": "Almost 50% of the syllables consists of onset, nucleus and coda (rule (2.4)).", "labels": [], "entities": [{"text": "onset", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9471665620803833}]}, {"text": "Rules (2.5)-(2.6) show that syllables with empty onsets are preferred to open syllables.", "labels": [], "entities": []}, {"text": "Simple onsets consisting of one consonant are more probable than complex ones, which is also true for codas (rules (2.7)-(2.8) and (2.13)-(2.14)).", "labels": [], "entities": []}, {"text": "Rules (2.9)-(2.10) show that fricatives are more probable than liquids in the onset.", "labels": [], "entities": []}, {"text": "Moreover, it is more likely that a nasal appears in the coda than a liquid (rule (2.15)-(2.16)).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the application of the standard probability model to grapheme-tophoneme (G2P) conversion (i) using a CFG to produce all possible phonemic correspondences of a given grapheme string, (ii) predicting pronunciation by c hoosing the most probable analysis, and (iii) reading oo the transcription from the phoneme tier.", "labels": [], "entities": [{"text": "grapheme-tophoneme (G2P) conversion", "start_pos": 81, "end_pos": 116, "type": "TASK", "confidence": 0.7414300799369812}, {"text": "CFG", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.9129922986030579}, {"text": "predicting pronunciation", "start_pos": 215, "end_pos": 239, "type": "TASK", "confidence": 0.94065922498703}]}, {"text": "A fragment of the grammar is already described in section 2.", "labels": [], "entities": []}, {"text": "We employed 389000 words of the Stuttgarter Zeitungskorpus (STZ), a German newspaper corpus for our training procedure.", "labels": [], "entities": [{"text": "389000 words of the Stuttgarter Zeitungskorpus (STZ), a German newspaper corpus", "start_pos": 12, "end_pos": 91, "type": "DATASET", "confidence": 0.7557645205940519}]}, {"text": "We used the same training procedure like in section 2.1: \ud97b\udf59 we initialize the CFG 10 times with randomized rule probabilities (10 start gram- Evaluation.", "labels": [], "entities": [{"text": "CFG", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9076045155525208}]}, {"text": "shows the results of the training procedure.", "labels": [], "entities": []}, {"text": "Almost all of the 10 grammars reached the maximum of the accuracy after one iteration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9994142055511475}]}, {"text": "The best grammar yields a word accuracy of almost 40%, which is nonsatisfying.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9604361653327942}]}, {"text": "Grammatiken \"prec\" \"perpl\" This rule have to beapplied except when there is a morpheme boundary.", "labels": [], "entities": []}, {"text": "shows that for grapheme-to-phoneme conversion the modelling of preexes and suuxes in the grammar could help to improve the performance of the trained model.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6929379105567932}]}, {"text": "shows the results of the accuracy and the perplexity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9997308850288391}]}, {"text": "The accuracy is a decreasing function, whereas the perplexity did not change remarkably.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999713122844696}, {"text": "perplexity", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9596365094184875}]}, {"text": "Thus, there is no correlation between accuracy and perplexity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.999377429485321}]}, {"text": "The results correspond to the nding shown in section 2.1 for syllabiication.", "labels": [], "entities": []}, {"text": "We varied the parameter start grammarr, as the grammars are randomly initialized in the beginning of the training procedure and the inside-outside algorithm can only detect local maxima.", "labels": [], "entities": []}, {"text": "We experimented with 50 randomly initialized start grammars yielding a 3% increase inaccuracy to 42.5%.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9769747257232666}]}, {"text": "In further experiments we varied the size of the training corpus systematically: 4500, 9600, 15000, 33000, 77000, 182000, 398000 and 1000000 words.", "labels": [], "entities": []}, {"text": "We initialized 50 start grammars and trained each grammar with 10 iterations on the diierent corpora.", "labels": [], "entities": []}], "tableCaptions": []}