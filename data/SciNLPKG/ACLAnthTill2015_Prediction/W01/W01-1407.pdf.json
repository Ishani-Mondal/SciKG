{"title": [{"text": "Toward hierarchical models for statistical machine translation of inflected languages", "labels": [], "entities": [{"text": "statistical machine translation of inflected languages", "start_pos": 31, "end_pos": 85, "type": "TASK", "confidence": 0.8059149285157522}]}], "abstractContent": [{"text": "In statistical machine translation, correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6359623571236929}]}, {"text": "Existing statistical systems for MT often treat different derivatives of the same lemma as if they were independent of each other.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9936419129371643}]}, {"text": "In this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the in-terdependencies of the different derivatives.", "labels": [], "entities": []}, {"text": "We do this along two directions: Usage of hierarchical lexicon models and the introduction of equivalence classes in order to ignore information not relevant for the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.9055826663970947}]}, {"text": "The improvement of the translation results is demonstrated on a German-English corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "The statistical approach to machine translation has become widely accepted in the last few years.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8278673589229584}]}, {"text": "It has been successfully applied to realistic tasks in various national and international research programs.", "labels": [], "entities": []}, {"text": "However in many applications only small amounts of bilingual training data are available for the desired domain and language pair, and it is highly desirable to avoid at least parts of the costly data collection process.", "labels": [], "entities": []}, {"text": "Some recent publications have dealt with the problem of translation with scarce resources.", "labels": [], "entities": [{"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9815779328346252}]}, {"text": "( describe the use of dictionaries.", "labels": [], "entities": []}, {"text": "(Al-) report on an experiment of Tetun-to-English translation by different groups, including one using statistical machine translation.", "labels": [], "entities": [{"text": "Tetun-to-English translation", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.830160528421402}, {"text": "statistical machine translation", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.6261338889598846}]}, {"text": "They assume the absence of linguistic knowledge sources such as morphological analyzers and dictionaries.", "labels": [], "entities": []}, {"text": "Nevertheless, they found that human mind is very well capable of deriving dependencies such as morphology, cognates, proper names, spelling variations etc., and that this capability was finally at the basis of the better results produced by humans compared to corpus based machine translation.", "labels": [], "entities": []}, {"text": "The additional information results from complex reasoning and it is not directly accessible from the full word form representation of the data.", "labels": [], "entities": []}, {"text": "In this paper, we take a different point of view: Even if full bilingual training data is scarce, monolingual knowledge sources like morphological analyzers and data for training the target language model as well as conventional dictionaries (one word and its translation per entry) maybe available and of substantial usefulness for improving the performance of statistical translation systems.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 362, "end_pos": 385, "type": "TASK", "confidence": 0.6890098452568054}]}, {"text": "This is especially the case for highly inflected languages like German.", "labels": [], "entities": []}, {"text": "We address the question of how to achieve a better exploitation of the resources for training the parameters for statistical machine translation by taking into account explicit knowledge about the languages under consideration.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 113, "end_pos": 144, "type": "TASK", "confidence": 0.6919773320357004}]}, {"text": "In our approach we introduce equivalence classes in order to ignore information not relevant to the translation . In the experiments reported in this paper, the source language is German and the target language is English.", "labels": [], "entities": []}, {"text": "Every English string is considered as a possible translation for the input.) make use of a special way of structuring the string translation model like proposed by: The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position.", "labels": [], "entities": []}, {"text": "The lexicon probability % \u00a2 of a certain English word is assumed to depend basically only on the source word \u00a2 aligned to it.", "labels": [], "entities": []}, {"text": "The overall architecture of the statistical translation approach is depicted in.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.8097920417785645}]}, {"text": "In this figure we already anticipate the fact that we can transform the source strings in a certain manner.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were carried out on Verbmobil data, which consists of spontaneously spoken dialogs in the appointment scheduling domain.", "labels": [], "entities": [{"text": "Verbmobil data", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9223926365375519}, {"text": "appointment scheduling domain", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.734303742647171}]}, {"text": "German source sentences are translated into English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Corpus statistics: Verbmobil training.  Singletons are types occurring only once in train- ing.", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the Verbmobil test corpus  for German-to-English translation. Unknowns are  word forms not contained in the training corpus.", "labels": [], "entities": [{"text": "Verbmobil test corpus", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.8297961155573527}, {"text": "German-to-English translation", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.5861126631498337}]}, {"text": " Table 5: Effect of the introduction of equivalence  classes. For the baseline we used the original in- flected word forms.  SSER [%] ISER [%]  inflected words  37.4  26.8  equivalence classes  35.9  23.5", "labels": [], "entities": [{"text": "SSER", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9724112749099731}, {"text": "ISER", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.5572490096092224}]}, {"text": " Table 4: Effect of two-level lexicon combination. For the baseline we used the conventional one-level  full form lexicon.  ext. dictionary SSER [%] ISER [%]  baseline  yes  35.7  23.9  combined  yes  33.8  22.3  baseline  no  37.4  26.8  combined  no  36.9  25.8", "labels": [], "entities": [{"text": "SSER", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.846469521522522}, {"text": "ISER", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.5714088678359985}]}]}