{"title": [{"text": "Overcoming the customization bottleneck using example-based MT", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9356191754341125}]}], "abstractContent": [{"text": "We describe MSR-MT, a large-scale hybrid machine translation system underdevelopment for several language pairs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7451851963996887}]}, {"text": "This system's ability to acquire its primary translation knowledge automatically by parsing a bilingual corpus of hundreds of thousands of sentence pairs and aligning resulting logical forms demonstrates true promise for overcoming the so-called MT customization bottleneck.", "labels": [], "entities": [{"text": "MT customization", "start_pos": 246, "end_pos": 262, "type": "TASK", "confidence": 0.939248114824295}]}, {"text": "Trained on English and Spanish technical prose, a blind evaluation shows that MSR-MT's integration of rule-based parsers, example based processing, and statistical techniques produces translations whose quality exceeds that of uncustomized commercial MT systems in this domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Commercially available machine translation (MT) systems have long been limited in their cost effectiveness and overall utility by the need for domain customization.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8679021954536438}]}, {"text": "Such customization typically includes identifying relevant terminology (esp.", "labels": [], "entities": []}, {"text": "multi-word collocations), entering this terminology into system lexicons, and making additional tweaks to handle formatting and even some syntactic idiosyncrasies.", "labels": [], "entities": []}, {"text": "One of the goals of data-driven MT research has been to overcome this customization bottleneck through automated or semi-automated extraction of translation knowledge from bilingual corpora.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9426020979881287}]}, {"text": "To address this bottleneck, a variety of example based machine translation (EBMT) systems have been created and described in the literature.", "labels": [], "entities": [{"text": "machine translation (EBMT)", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.8329563736915588}]}, {"text": "Some of these employ parsers to produce dependency structures for the sentence pairs in aligned bilingual corpora, which are then aligned to obtain transfer rules or examples.", "labels": [], "entities": []}, {"text": "Other systems extract and use examples that are represented as linear patterns of varying complexity).", "labels": [], "entities": []}, {"text": "For some EBMT systems, substantial collections of examples are also manually crafted or at least reviewed for correctness after being identified automatically ().", "labels": [], "entities": []}, {"text": "The efforts that report accuracy results for fully automatic example extraction) do so for very modest amounts of training data (a few thousand sentence pairs).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9955703616142273}, {"text": "example extraction", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7908026278018951}]}, {"text": "Previous work in this area thus raises the possibility that manual review or crafting is required to obtain example bases of sufficient coverage and accuracy to be truly useful.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9897056818008423}]}, {"text": "Other variations of EBMT systems are hybrids that integrate an EBMT component as one of multiple sources of transfer knowledge (in addition to other transfer rule or knowledge based components) used during translation ().", "labels": [], "entities": []}, {"text": "To our knowledge, commercial quality MT has so far been achieved only through years of effort in creating hand-coded transfer rules.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9903240203857422}]}, {"text": "Systems whose primary source of translation knowledge comes from an automatically created example base have not been shown capable of matching or exceeding the quality of commercial systems.", "labels": [], "entities": [{"text": "translation knowledge", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.9023909866809845}]}, {"text": "This paper reports on MSR-MT, an MT system that attempts to break the customization bottleneck by exploiting example-based (and some statistical) techniques to automatically acquire its primary translation knowledge from a bilingual corpus of several million words.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9831624031066895}]}, {"text": "The system leverages the linguistic generality of existing rule-based parsers to enable broad coverage and to overcome some of the limitations on locality of context characteristic of data-driven approaches.", "labels": [], "entities": []}, {"text": "The ability of MSR-MT to adapt automatically to a particular domain, and to produce reasonable translations for that domain, is validated through a blind assessment by human evaluators.", "labels": [], "entities": []}, {"text": "The quality of MSR-MT's output in this one domain is shown to exceed the output quality of two highly rated (though not domain-customized) commercially available MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.9701833724975586}]}, {"text": "We believe that this demonstration is the first in the literature to show that automatic training methods can produce a commercially viable level of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.9618546962738037}]}], "datasetContent": [{"text": "For each evaluation, five to seven evaluators are asked to evaluate the same set of 200 to 250 blind test sentences.", "labels": [], "entities": []}, {"text": "For each sentence, raters are presented with a reference sentence in the target language, which is a human translation of the corresponding source sentence.", "labels": [], "entities": []}, {"text": "In order to maintain consistency among raters who may have different levels of fluency in the source language, raters are not shown the source sentence.", "labels": [], "entities": [{"text": "consistency", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9657837152481079}]}, {"text": "Instead, they are presented with two machine-generated target translations presented in random order: one translation by the system to be evaluated (the experimental system), and another translation by a comparison system (the control system).", "labels": [], "entities": []}, {"text": "The order of presentation of sentences is also randomized for each rater in order to eliminate any ordering effect.", "labels": [], "entities": []}, {"text": "Raters are asked to make a three-way choice.", "labels": [], "entities": []}, {"text": "For each sentence, raters may choose one of the two automatically translated sentences as the better translation of the (unseen) source sentence, assuming that the reference sentence represents a perfect translation, or, they may indicate that neither of the two is better.", "labels": [], "entities": []}, {"text": "Raters are instructed to use their best judgment about the relative importance of fluency/style and accuracy/content preservation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9966660141944885}, {"text": "content preservation", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.6440105736255646}]}, {"text": "We chose to use this simple three-way scale in order to avoid making any a priori judgments about the relative importance of these parameters for subjective judgments of quality.", "labels": [], "entities": []}, {"text": "The three-way scale also allows sentences to be rated on the same scale, regardless of whether the differences between output from system 1 and system 2 are substantial or negligible.", "labels": [], "entities": []}, {"text": "The scoring system is similarly simple; each judgment by a rater is represented as 1 (sentence from experimental system judged better), 0 (neither sentence judged better), or -1 (sentence from control system judged better).", "labels": [], "entities": []}, {"text": "For each sentence, the score is the mean of all raters' judgments; for each comparison, the score is the mean of the scores of all sentences.", "labels": [], "entities": []}, {"text": "Although work on MSR-MT encompasses a number of language pairs, we focus hereon the evaluation of just two, Spanish-English and English-Spanish.", "labels": [], "entities": []}, {"text": "Training data was held constant for each of these evaluations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. English/Spanish transfer mappings from  LF alignment", "labels": [], "entities": []}]}