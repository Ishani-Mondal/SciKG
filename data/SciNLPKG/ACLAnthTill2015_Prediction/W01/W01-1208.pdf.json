{"title": [{"text": "Question Answering Using Encyclopedic Knowledge Generated from the Web", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8108585774898529}]}], "abstractContent": [{"text": "We propose a question answering system which uses an encyclopedia as a knowledge base.", "labels": [], "entities": [{"text": "question answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.866084635257721}]}, {"text": "However, since existing encyclopedias lack technical/new terms, we use an encyclopedia automatically generated from the World Wide Web.", "labels": [], "entities": []}, {"text": "For this purpose, we first search the Web for pages containing a term in question.", "labels": [], "entities": []}, {"text": "Then linguistic patterns and HTML structures are used to extract text fragments describing the term.", "labels": [], "entities": []}, {"text": "Finally, extracted term descriptions are organized based on word senses and domains.", "labels": [], "entities": []}, {"text": "We also evaluate our system byway of experiments, where the Japanese Information-Technology Engineers Examination is used as a test collection.", "labels": [], "entities": [{"text": "Japanese Information-Technology Engineers Examination", "start_pos": 60, "end_pos": 113, "type": "DATASET", "confidence": 0.8930937498807907}]}], "introductionContent": [{"text": "Motivated partially by the TREC-8 QA collection), question answering has of late become one of the major topics within the natural language processing and information retrieval communities, and a number of QA systems targeting the TREC collection have been proposed (.", "labels": [], "entities": [{"text": "TREC-8 QA collection", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.7877690394719442}, {"text": "question answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9281108379364014}, {"text": "natural language processing and information retrieval", "start_pos": 123, "end_pos": 176, "type": "TASK", "confidence": 0.6125168204307556}, {"text": "TREC collection", "start_pos": 231, "end_pos": 246, "type": "DATASET", "confidence": 0.741050511598587}]}, {"text": "Although  proposed a knowledge-based QA system, most existing systems rely on conventional IR and shallow NLP methods.", "labels": [], "entities": []}, {"text": "However, question answering is inherently a more complicated procedure that usually requires explicit knowledge bases.", "labels": [], "entities": [{"text": "question answering", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9323680996894836}]}, {"text": "In this paper, we propose a question answering system which uses an encyclopedia as a knowledge base.", "labels": [], "entities": [{"text": "question answering", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8798215985298157}]}, {"text": "However, since existing (published) encyclopedias usually lack technical/new terms, we generate one based on the World Wide Web, which includes a number of technical and recent information.", "labels": [], "entities": []}, {"text": "For this purpose, we use a modified version of our method to extract term descriptions from Web pages.", "labels": [], "entities": []}, {"text": "Intuitively, our system answers interrogative questions like \"What is X?\" in which a QA system searches an encyclopedia database for one or more descriptions related to term X.", "labels": [], "entities": []}, {"text": "The performance of QA systems can be evaluated based on coverage and accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9940305352210999}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9970249533653259}]}, {"text": "Coverage is the ratio between the number of questions answered (disregarding their correctness) and the total number of questions.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8531699180603027}]}, {"text": "Accuracy is the ratio between the number of correct answers and the total number of answers made by the system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9957412481307983}]}, {"text": "While coverage can be estimated objectively and systematically, estimating accuracy relies on human subjects (because it is difficult to define the absolute description for term X), and thus is expensive.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9634922742843628}]}, {"text": "In view of this problem, we use as a test collection Information Technology Engineers Examinations 1 , which are biannual examinations necessary for candidates to qualify to be IT engineers in Japan.", "labels": [], "entities": []}, {"text": "Among a number of classes, we focus on the \"Class II\" examination, which requires funda-mental and general knowledge related to information technology.", "labels": [], "entities": []}, {"text": "Approximately half of questions are associated with IT technical terms.", "labels": [], "entities": []}, {"text": "Since past examinations and answers are open to the public, we can objectively evaluate the performance of our QA system with minimal cost.", "labels": [], "entities": []}, {"text": "Our system is not categorized into \"opendomain\" systems, where questions expressed in natural language are not limited to explicit axes including who, what, when, where, how and why.", "labels": [], "entities": []}, {"text": "However, found that each of the TREC questions can be recast as either a single axis or a combination of axes.", "labels": [], "entities": [{"text": "TREC questions", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.6164700984954834}]}, {"text": "They also found that out of the 200 TREC questions, 64 questions (approximately one third) were associated with the what axis, for which our encyclopedia-based system is expected to improve the quality of answers.", "labels": [], "entities": []}, {"text": "Section 2 analyzes the Japanese IT Engineers Examination, and Section 3 explains our question answering system.", "labels": [], "entities": [{"text": "Japanese IT Engineers Examination", "start_pos": 23, "end_pos": 56, "type": "DATASET", "confidence": 0.8798871487379074}, {"text": "question answering", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7737250030040741}]}, {"text": "Then, Sections 4 and 5 elaborate on our Web-base method for encyclopedia generation.", "labels": [], "entities": [{"text": "encyclopedia generation", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7431485950946808}]}, {"text": "Finally, Section 6 evaluates our system byway of experiments.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Coverage and accuracy (%) for different  question answering methods.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9957354068756104}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9983574748039246}, {"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7167750746011734}]}]}