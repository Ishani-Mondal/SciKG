{"title": [], "abstractContent": [{"text": "Mining the answer of a natural language open-domain question in a large collection of on-line documents is made possible by the recognition of the expected answer type in relevant text passages.", "labels": [], "entities": []}, {"text": "If the technology of retrieving texts where the answer might be found is well developed, few studies have been devoted to the recognition of the answer type.", "labels": [], "entities": []}, {"text": "This paper presents a unified model of answer types for open-domain Ques-tion/Answering that enables the discovery of exact answers.", "labels": [], "entities": []}, {"text": "The evaluation of the model, performed on real-world questions, considers both the correct-ness and the coverage of the answer types as well as their contribution to answer precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9441862106323242}]}], "introductionContent": [{"text": "Answer mining, a.k.a. textual Question/Answering (Q/A), represents the task of discovering the answer to an open-domain natural language question in large text collections.", "labels": [], "entities": [{"text": "Answer mining", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8850793838500977}, {"text": "textual Question/Answering (Q/A)", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7533561587333679}, {"text": "discovering the answer to an open-domain natural language question in large text collections", "start_pos": 79, "end_pos": 171, "type": "TASK", "confidence": 0.5148041454645303}]}, {"text": "These two requirements intentionally simplify the answer mining task, since the identification of the exact answer is left to the user.", "labels": [], "entities": [{"text": "answer mining task", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9118043184280396}]}, {"text": "However, given that the expected information is recognized by inspecting text snippets of relatively small size, the TREC Q/A task took a step closer to information retrieval rather than document retrieval.", "labels": [], "entities": [{"text": "TREC Q/A task", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.6518694221973419}, {"text": "information retrieval", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.7599042654037476}, {"text": "document retrieval", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.7077658623456955}]}, {"text": "Moreover, the techniques developed to extract text snippets where the answers might lie paved the way to a unified model for answer mining.", "labels": [], "entities": [{"text": "answer mining", "start_pos": 125, "end_pos": 138, "type": "TASK", "confidence": 0.932704508304596}]}, {"text": "To find the answer to a question several steps must betaken, as reported in () ()): \u00a2 First, the question semantics needs to be captured.", "labels": [], "entities": []}, {"text": "This translates into identifying (i) the expected answer type and (ii) the question keywords that can be used to retrieve text passages where the answer maybe found.", "labels": [], "entities": []}, {"text": "\u00a2 Secondly, the index of the document collection must be used to identify the text passages of interest.", "labels": [], "entities": []}, {"text": "The retrieval method either employs special operators or simply modifies boolean or vector retrieval.", "labels": [], "entities": []}, {"text": "Since the expected answer type is known at the time of the retrieval, the quality of the text passages is greatly improved by filtering out those passages where concepts of the same category as the answer type are not present.", "labels": [], "entities": []}, {"text": "\u00a2 Thirdly, answer extraction takes place by combining several features that take into account the expected answer type.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.9386232793331146}]}, {"text": "Since the expected answer type is the only information used in all the phases of textual Q/A, its recognition and usage is central to the performance of answer mining.", "labels": [], "entities": [{"text": "answer mining", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.8724000155925751}]}, {"text": "For an open-domain Q/A system, establishing the possible answer types is a challenging problem.", "labels": [], "entities": []}, {"text": "Currently, most of the systems recognize the answer type by associating the question stem (e.g. What, Who, Why or How) and one of the concepts from the question to a predefined general category, such as PERSON, ORGANIZATION, LOCA-TION, TIME, DATE, MONEY or NUMBER.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.9292069673538208}, {"text": "ORGANIZATION", "start_pos": 211, "end_pos": 223, "type": "METRIC", "confidence": 0.8932988047599792}, {"text": "TIME", "start_pos": 236, "end_pos": 240, "type": "METRIC", "confidence": 0.8200969099998474}, {"text": "DATE", "start_pos": 242, "end_pos": 246, "type": "METRIC", "confidence": 0.8115173578262329}, {"text": "MONEY", "start_pos": 248, "end_pos": 253, "type": "METRIC", "confidence": 0.8137989640235901}]}, {"text": "Since many of these categories are represented in texts as named entities, their recognition as possible answers is enabled by state-of-the-art Named Entity (NE) recognizers, devised to work with high precision in Information Extraction (IE) tasks.", "labels": [], "entities": [{"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9639703631401062}, {"text": "Information Extraction (IE) tasks", "start_pos": 214, "end_pos": 247, "type": "TASK", "confidence": 0.7914680540561676}]}, {"text": "To allow for NE-supported answer mining, a large number of semantic categories corresponding to various names must be considered, e.g. names of cars, names of diseases, names of dishes, names of boats, etc.", "labels": [], "entities": [{"text": "NE-supported answer mining", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.7500208417574564}]}, {"text": "Furthermore, a significant number of entities are not unique, therefore do not bear names, but are still potential answers to an opendomain question.", "labels": [], "entities": []}, {"text": "Additionally, questions do not focus only on entities and their attributes; they also ask about events and their related entities.", "labels": [], "entities": []}, {"text": "In this paper we introduce a model of answer types that accounts for answers to questions of various complexity.", "labels": [], "entities": []}, {"text": "The model enables several different formats of the exact answer to opendomain questions and considers also the situation when the answer is produced from a number of different document sources.", "labels": [], "entities": []}, {"text": "We define formally the answer types to open-domain questions and extend the recognition of answer types beyond the question processing phase, thus enabling several feed-back mechanisms derived from the processing of documents and answers.", "labels": [], "entities": []}, {"text": "The main contribution of the paper is in providing a unified model of answer mining from large collections of on-line documents that accounts for the processing of open-domain natural language questions of varied complexity.", "labels": [], "entities": [{"text": "answer mining", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9389285147190094}]}, {"text": "The hope is that a coherent model of the textual answer discovery could help developing better text mining methods, capable of acquiring and rapidly prototyping knowledge from the vast amount of on-line texts.", "labels": [], "entities": [{"text": "textual answer discovery", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7429389754931132}, {"text": "text mining", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.7789229154586792}]}, {"text": "Additionally, such a model enables the development of intelligent conversational agents that operate on open-domain tasks.", "labels": [], "entities": []}, {"text": "We first present a background of Q/A systems and then define several classes of question complexity.", "labels": [], "entities": []}, {"text": "In Section 3 we present the formal answer type model whereas in Section 4 we show how to recognize the answer type of open-domain questions and use it to mine the answer.", "labels": [], "entities": []}, {"text": "Section 5 presents the evaluation of the model and summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our answer type model we used 693 TREC test questions on which we did not train the perceptron.", "labels": [], "entities": [{"text": "TREC", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.7944255471229553}]}, {"text": "lists the breakdown of the answer type CATEGORIES recognized by our model as well as the coverage and precision of the recognition.", "labels": [], "entities": [{"text": "coverage", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9923744797706604}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9987292885780334}]}, {"text": "Currently our ANSWER TAXONOMY encodes 8707 concepts from 129 WordNet hierarchies, covering only 81% of the expected answer types.", "labels": [], "entities": [{"text": "ANSWER TAXONOMY", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.7676132321357727}]}, {"text": "This shows that we have to continue encoding more top concepts in the taxonomy and link them to more WordNet concepts.", "labels": [], "entities": []}, {"text": "The recognition mechanism had better precision than coverage in our experiments.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.998755693435669}]}, {"text": "Moreover a relationship between the coverage of answer type recognition and the overall performance of answer mining, as illustrated in.", "labels": [], "entities": [{"text": "answer type recognition", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.8092936078707377}, {"text": "answer mining", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.8483509719371796}]}, {"text": "Some of the test questions are listed in  The experiments show that open-domain natural language questions of varied degrees of complexity can be answered consistently from vast amounts of on-line texts.", "labels": [], "entities": []}, {"text": "One of the applications of a unified model of answer mining is the development of intelligent conversational agents ().", "labels": [], "entities": [{"text": "answer mining", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9420633316040039}]}], "tableCaptions": [{"text": " Table 4. Some of  the test questions are listed in", "labels": [], "entities": []}]}