{"title": [{"text": "Towards a Simple and Accurate Statistical Approach to Learning Translation Relationships among Words", "labels": [], "entities": [{"text": "Learning Translation Relationships among Words", "start_pos": 54, "end_pos": 100, "type": "TASK", "confidence": 0.79559445977211}]}], "abstractContent": [{"text": "We report on a project to derive word translation relationships automatically from parallel corpora.", "labels": [], "entities": [{"text": "derive word translation relationships", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7085888832807541}]}, {"text": "Our effort is distinguished by the use of simpler, faster models than those used in previous high-accuracy approaches.", "labels": [], "entities": []}, {"text": "Our methods achieve accuracy on single-word translations that seems comparable to any work previously reported, up to nearly 60% coverage of word types, and they perform particularly well on a class of multi-word compounds of special interest to our translation effort.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9990662932395935}]}], "introductionContent": [{"text": "This paper is a report on work in progress aimed at learning word translation relationships automatically from parallel bilingual corpora.", "labels": [], "entities": [{"text": "learning word translation relationships automatically from parallel bilingual corpora", "start_pos": 52, "end_pos": 137, "type": "TASK", "confidence": 0.808346066210005}]}, {"text": "Our effort is distinguished by the use of simple statistical models that are easier to implement and faster to run than previous high-accuracy approaches to this problem.", "labels": [], "entities": []}, {"text": "Our overall approach to machine translation is a deep-transfer approach in which the transfer relationships are learned from a parallel bilingual corpus ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7756114900112152}]}, {"text": "More specifically, the transfer component is trained by parsing both sides of the corpus to produce parallel logical forms, using lexicons and analysis grammars constructed by linguists.", "labels": [], "entities": []}, {"text": "The parallel logical forms are then aligned at the level of content word stems (lemmas), and logical-form transfer patterns are learned from the aligned logicalform corpus.", "labels": [], "entities": []}, {"text": "At run time, the source language text is parsed into logical forms employing the source language grammar and lexicon used in constructing the logical-form training corpus, and the logical-form transfer patterns are used to construct target language logical forms.", "labels": [], "entities": []}, {"text": "These logical forms are transformed into target language strings using the target-language lexicon, and a generation grammar written by a linguist.", "labels": [], "entities": []}, {"text": "The principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, fora special case described in Section 4.", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 255, "end_pos": 277, "type": "TASK", "confidence": 0.7804330488046011}]}], "datasetContent": [{"text": "Our basic method for finding translation pairs was applied to a set of approximately 200,000 French and English aligned sentence pairs, derived mainly from Microsoft technical manuals, resulting in 46,599 potential translation pairs.", "labels": [], "entities": []}, {"text": "The top 42,486 pairs were incorporated in the alignment lexicon of our end-to-end translation system.", "labels": [], "entities": []}, {"text": "The procedure for finding translations of captoids was applied to a slight superset of the training data for the basic procedure, and yielded 2561 possible translation pairs.", "labels": [], "entities": []}, {"text": "All of these were added to our end-to-end translation system, with the French multiwords being added to the lexicon of the French parser, and the translation pairs being added to the alignment lexicon.", "labels": [], "entities": []}, {"text": "The improvements in end-to-end performance due to these additions in a French-to-English translation task are described elsewhere).", "labels": [], "entities": [{"text": "French-to-English translation task", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.6830772856871287}]}, {"text": "For this report, we have evaluated our techniques for finding trans-lation pairs by soliciting judgements of translation correctness from fluent French-English bilinguals.", "labels": [], "entities": [{"text": "translation correctness", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.8648916184902191}]}, {"text": "There were too many translation pairs to obtain judgements on each one, so we randomly selected about 10% of the 42,486 general translation pairs that were actually added to the system, and about 25% of the 2561 captoid pairs.", "labels": [], "entities": []}, {"text": "The accuracy of the most strongly associated translation pairs produced by the basic method at various levels of coverage is displayed in Table 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992861151695251}]}, {"text": "We use the terms \"coverage\" and \"accuracy\" in essentially the same way as Melamed).", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9818075895309448}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9981093406677246}]}, {"text": "\"Type coverage\" means the proportion of distinct lexical types in the entire training corpus, including both French and English, for which there is at least one translation given.", "labels": [], "entities": []}, {"text": "As with the comparable results reported by Melamed, these are predominantly single lemmas for content words, but we also include occurrences multiwords as distinct types.", "labels": [], "entities": []}, {"text": "\"Mean count\" is the average number of occurrences of each type at the given level of coverage.", "labels": [], "entities": [{"text": "Mean count\"", "start_pos": 1, "end_pos": 12, "type": "METRIC", "confidence": 0.983303427696228}]}, {"text": "\"Token coverage\" is the proportion of the total number of occurrences of items in the text represented by the types included within the type coverage.", "labels": [], "entities": [{"text": "Token coverage\"", "start_pos": 1, "end_pos": 16, "type": "METRIC", "confidence": 0.6881939570109049}]}, {"text": "Since the judges were asked to evaluate the proposed translations out of context, we allowed them to give an answer of \"not sure\", as well as \"correct\" and \"incorrect\".", "labels": [], "entities": []}, {"text": "Our accuracy scores are therefore given as a range, where the low score combines answers of \"not sure\" and \"incorrect\", and the high score combines answers of \"not sure\" and \"correct\".: Results for captoids.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991033673286438}]}, {"text": "The \"total accuracy\" column gives results at different levels of coverage overall the translation pairs generated by our basic method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9337586164474487}]}, {"text": "For a more detailed analysis, the remaining columns provide a breakdown for single-word translations, translations involving multiwords given to us by the parser (\"multiword accuracy\"), and new multiwords hypothesized by our procedure (\"compound accuracy\").", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9079796671867371}, {"text": "accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.5104882717132568}]}, {"text": "As the table shows, our performance is quite good on single-word translations, with accuracy of around 80% even at our cut-off of 63% type coverage, which represents 99% of the tokens in the corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9994138479232788}, {"text": "type coverage", "start_pos": 134, "end_pos": 147, "type": "METRIC", "confidence": 0.7731463313102722}]}, {"text": "To compare our results more directly with Melamed's published results on single-word translation, we show, where both coverage and accuracy are given for single-word translations only.", "labels": [], "entities": [{"text": "single-word translation", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.6832849532365799}, {"text": "coverage", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9976769089698792}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9986687898635864}]}, {"text": "According to the standard of correctness Melamed uses that is closest to ours, he reports 92% accuracy at 36% type coverage, 89% accuracy at 46% type coverage, and 87% accuracy at 90% type coverage, on a set of 300,000 aligned sentence pairs from the French-English Hansard corpus of Candian Parliament proceedings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9984471201896667}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9972405433654785}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9983811378479004}, {"text": "Hansard corpus of Candian Parliament proceedings", "start_pos": 266, "end_pos": 314, "type": "DATASET", "confidence": 0.9453423221906027}]}, {"text": "Our accuracies at the first two of these coverage points are 88-90% and 84-87%, which is slightly lower than Melamed, but given the different corpus, different judges, and different evaluation conditions, one cannot draw any definite conclusions about which method is more accurate at these coverage levels.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.989972710609436}, {"text": "coverage", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9674486517906189}]}, {"text": "Our method, however, does not produce any result approaching 90% type coverage, and accuracy appears to start dropping rapidly below 56% type coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.5380867719650269}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9995658993721008}]}, {"text": "Nevertheless, this still represents good accuracy up to 97% token coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9992056488990784}, {"text": "coverage", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.7432665228843689}]}, {"text": "Returning to, we see that our accuracy on multiwords is much lower than on single words, especially the multiwords hypothesized by our learning procedure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9990770816802979}]}, {"text": "The results are much better, however, when we look at the results for our specialized method for finding translations of captoids, as shown in.", "labels": [], "entities": []}, {"text": "Our accuracy at nearly 20% type coverage is around 84%, which is higher than our accuracy for general translation pairs (76-80%) at the same type coverage level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993199110031128}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9988266825675964}]}, {"text": "It is lower than our single-word translation accuracy (90-91%) at this coverage level, but it is striking how close it is, given far less data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8452979922294617}]}, {"text": "At 20% type coverage of single words, there are 389 tokens per word type, while at 20% type coverage of captoids, there are fewer than 9 tokens per captoid type.", "labels": [], "entities": []}, {"text": "In fact, further analysis shows that of the 2561 captoid translation pairs, 947 have only a single example of the English captoid in the training data, yet our accuracy on these is around 82%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9996349811553955}]}, {"text": "We note, however, that our captoid learning procedure cuts off at around 20% type coverage, which is only 25% token coverage for these items.", "labels": [], "entities": [{"text": "type coverage", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.8724596202373505}]}], "tableCaptions": [{"text": " Table 1: Results for basic method.", "labels": [], "entities": []}, {"text": " Table 2: Results for single words only.", "labels": [], "entities": []}, {"text": " Table 3: Results for captoids.", "labels": [], "entities": []}]}