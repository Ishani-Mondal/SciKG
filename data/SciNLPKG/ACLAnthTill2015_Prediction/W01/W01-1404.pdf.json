{"title": [{"text": "Approximating Context-Free by Rational Transduction for Example-Based MT", "labels": [], "entities": [{"text": "Approximating", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9388535618782043}, {"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.8744565844535828}]}], "abstractContent": [{"text": "Existing studies show that a weighted context-free transduction of reasonable quality can be effectively learned from examples.", "labels": [], "entities": []}, {"text": "This paper investigates the approximation of such transduction by means of weighted rational transduc-tion.", "labels": [], "entities": []}, {"text": "The advantage is increased processing speed, which benefits real-time applications involving spoken language .", "labels": [], "entities": []}], "introductionContent": [{"text": "Several studies have investigated automatic or partly automatic learning of transductions for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7572014033794403}]}, {"text": "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (, others have chosen models closer to context-free grammars and context-free transduction, such as (;, and yet other studies cannot be comfortably assigned to either of these two frameworks, such as and.", "labels": [], "entities": []}, {"text": "In this paper we will investigate both contextfree and finite-state models.", "labels": [], "entities": []}, {"text": "The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word \u00a1 The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center for Artificial Intelligence (DFKI).", "labels": [], "entities": []}, {"text": "order between two languages, and the selection of appropriate lexical items.", "labels": [], "entities": []}, {"text": "Furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful.", "labels": [], "entities": []}, {"text": "However, practical algorithms for computing the most likely context-free derivation have a cubic time complexity, in terms of the length of the input string, or in the case of a graph output by a speech recognizer, in terms of the number of nodes in the graph.", "labels": [], "entities": []}, {"text": "For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter).", "labels": [], "entities": []}, {"text": "This may pose problems, especially for real-time speech systems.", "labels": [], "entities": []}, {"text": "Therefore, we have investigated approximation of weighted context-free transduction by means of weighted rational transduction.", "labels": [], "entities": []}, {"text": "The finite-state machinery for implementing the latter kind of transduction in general allows faster processing.", "labels": [], "entities": []}, {"text": "We can also more easily obtain robustness.", "labels": [], "entities": []}, {"text": "We hope the approximating model is able to preserve some of the accuracy of the context-free model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9993035793304443}]}, {"text": "In the next section, we discuss preliminary definitions, adapted from existing literature, making no more than small changes in presentation.", "labels": [], "entities": []}, {"text": "In Section 3 we explain how context-free transduction grammars can be represented by ordinary context-free grammars, plus a phase of postprocessing.", "labels": [], "entities": []}, {"text": "The approximation is discussed in Section 4.", "labels": [], "entities": []}, {"text": "As shown in Section 5, we may easily process input in a robust way, ensuring we always obtain output.", "labels": [], "entities": []}, {"text": "Section 6 discusses empirical results, and we end the paper with conclusions.", "labels": [], "entities": []}, {"text": "; this means that positions can only be related if their respective \"mother\" positions are related.", "labels": [], "entities": []}, {"text": "Note that this paper does not discuss how hierarchical alignments can be obtained from unannotated corpora of bitexts.", "labels": [], "entities": []}, {"text": "This is the subject of existing studies, such as ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We have investigated a corpus of English/Japanese sentence pairs, related by hierarchical alignment (see also).", "labels": [], "entities": []}, {"text": "We have taken the first 500, 1000, 1500, . .", "labels": [], "entities": []}, {"text": "aligned sentence pairs from this corpus to act as training corpora of varying sizes; we have taken 300 other sentence pairs to act as test corpus.", "labels": [], "entities": []}, {"text": "We have constructed a bilexical transduction grammar from each training corpus, in the form of a context-free grammar, and this grammar was approximated by a finite automaton.", "labels": [], "entities": []}, {"text": "The input sentences from the test corpus were then processed by context-free and finite-state machinery (in the sequel referred to by cfg and fa, respectively).", "labels": [], "entities": []}, {"text": "We have also carried out experiments with robust finite-state processing, as discussed in Section 5, which is referred to by robust fa.", "labels": [], "entities": []}, {"text": "If we append 2 after a tag, this mean (see Section 2.3).", "labels": [], "entities": []}, {"text": "The reorder operators from the resulting output strings were applied in a robust way as explained in Section 5.", "labels": [], "entities": []}, {"text": "The output strings were then compared to the reference output from the corpus, resulting in.", "labels": [], "entities": []}, {"text": "Our metric is word accuracy, which is based on edit distance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9545361995697021}]}, {"text": "For a pair of strings, the edit distance is defined as the minimum number of substitutions, insertions and deletions needed to turn one string into the other.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.8415658175945282}]}, {"text": "To allow a comparison with more established techniques (see e.g. (), we also take into consideration a simple bigram model, trained on the strings comprising both source and target sentences and reorder operators, as explained in Section 4.", "labels": [], "entities": []}, {"text": "For the purposes of predicting output symbols, a series of consecutive target symbols and reorder operators following a source symbol in the training sentences are treated as a single symbol by the bigram model, and only those maybe output after that source symbol.", "labels": [], "entities": [{"text": "predicting output symbols", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.8900664846102396}]}, {"text": "Since our construction is such that target symbols always follow source symbols they area translation of (according to the automatically obtained hierarchical alignment), this modification to the bigram model prevents output of totally unrelated target symbols that could otherwise result from a standard bigram model.", "labels": [], "entities": []}, {"text": "It also ensures that a bounded number of output symbols per input symbol are produced.", "labels": [], "entities": []}, {"text": "The fraction of sentences that were transduced (i.e. that were accepted by the grammar or the automaton), is indicated in.", "labels": [], "entities": []}, {"text": "Since robust fa(2) and bigram are able to transduce all input, they are not represented here.", "labels": [], "entities": []}, {"text": "Note that the average word accuracy is computed only with respect to the sentences that could be transduced, which explains the high accuracy for small training corpora in the cases of cfg(2) and fa(2), where the few sentences that can be transduced are mostly short and simple.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.914822518825531}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9967087507247925}]}, {"text": "presents the time consumption of transduction for the entire test corpus.", "labels": [], "entities": []}, {"text": "These data support our concerns about the high costs of context-free processing, even though our parser relies heavily on lexicalization.", "labels": [], "entities": []}, {"text": "4 shows the sizes of the automata after determinization and minimization.", "labels": [], "entities": []}, {"text": "Determinization for the largest automata indicated in the Figure took more than 24 hours for both fa(2) and robust fa(2) , which suggests these methods become unrealistic for training corpus sizes considerably larger than 10,000 bitexts.", "labels": [], "entities": []}], "tableCaptions": []}