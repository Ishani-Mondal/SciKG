{"title": [{"text": "Two levels of evaluation in a complex NL system", "labels": [], "entities": []}], "abstractContent": [{"text": "The QALC question-answering system, developed at LIMSI, has been a participant for two years in the QA track of the TREC conference.", "labels": [], "entities": [{"text": "LIMSI", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9244667887687683}, {"text": "QA track of the TREC conference", "start_pos": 100, "end_pos": 131, "type": "DATASET", "confidence": 0.6805010835329691}]}, {"text": "In this paper, we present a quantitative evaluation of various modules in our system, based on two criteria: first, the numbers of documents containing the correct answer and selected by the system; secondly, the number of answers found.", "labels": [], "entities": []}, {"text": "The first criterion is used for evaluating locally the modules in the system, which contribute in selecting documents that are likely to contain the answer.", "labels": [], "entities": []}, {"text": "The second one provides a global evaluation of the system.", "labels": [], "entities": []}, {"text": "As such, it also serves for an indirect evaluation of various modules.", "labels": [], "entities": []}], "introductionContent": [{"text": "For two years, the TREC Evaluation Conference, (Text REtrieval Conference) has been featuring a Question Answering track, in addition to those already existing.", "labels": [], "entities": [{"text": "TREC Evaluation Conference", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.4913695454597473}, {"text": "Text REtrieval Conference)", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.7158300206065178}, {"text": "Question Answering", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7726271152496338}]}, {"text": "This track involves searching for answers to a list of questions, within a collection of documents provided by NIST, the conference organizer.", "labels": [], "entities": [{"text": "NIST", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.955995500087738}]}, {"text": "Questions are factual or encyclopaedic, while documents are newspaper articles.", "labels": [], "entities": []}, {"text": "The TREC9-QA track, for instance, proposed 700 questions whose answers should be retrieved in a corpus of about one million documents.", "labels": [], "entities": [{"text": "TREC9-QA track", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7521077692508698}]}, {"text": "In addition to the evaluation, by human judges, of their systems\u00d5 results), TREC participants are also provided with an automated evaluation tool, along with a database.", "labels": [], "entities": []}, {"text": "These data consist of a list of judgements of all results sent in by all participants.", "labels": [], "entities": []}, {"text": "The evaluation tool automatically delivers a score to a set of answers given by a system to a set of questions.", "labels": [], "entities": []}, {"text": "This score is derived from the mean reciprocal rank of the first five answers.", "labels": [], "entities": []}, {"text": "For each question, the first correct answers get a mark in reverse proportion to their rank.", "labels": [], "entities": [{"text": "mark", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9706549644470215}]}, {"text": "Those evaluation tool and data are quite useful, since it gives us away of appreciating what happens when modifying our system to improve it.", "labels": [], "entities": []}, {"text": "We have been taking part to TREC for two years, with the QALC question-answering system), currently developed at LIMSI.", "labels": [], "entities": [{"text": "TREC", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.48284822702407837}, {"text": "LIMSI", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.9561266899108887}]}, {"text": "This system has following architecture: parsing of the question to find the expected type of the answer, selection of a subset of documents among the approximately one million TREC-provided items, tagging of named entities within the documents, and, finally, search for possible answers.", "labels": [], "entities": []}, {"text": "Some of the components serve to enrich both questions and documents, by adding system-readable data into them.", "labels": [], "entities": []}, {"text": "Such is the case for the modules that parse questions and tag documents.", "labels": [], "entities": []}, {"text": "Other components operate a selection among documents, using added data.", "labels": [], "entities": []}, {"text": "One example of such modules are those which select relevant documents, another is the one which extracts the answer from the documents.", "labels": [], "entities": []}, {"text": "A global evaluation of the system is based on judgement about its answers.", "labels": [], "entities": []}, {"text": "This criterion provides only indirect evaluation of each component, via the evolution of the final score when this component is modified.", "labels": [], "entities": []}, {"text": "To get a closer evaluation of our modules, we need other criteria.", "labels": [], "entities": []}, {"text": "In particular, concerning the evaluation of components for document selection, we adopted an additional criterion about selected relevant documents, that is, those that yield the correct answer.", "labels": [], "entities": [{"text": "document selection", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7596952617168427}]}, {"text": "This paper describes a quantitative evaluation of various modules in our system, based on two criteria: first, the number of selected relevant documents, and secondly, the number of found answers.", "labels": [], "entities": []}, {"text": "The first criterion is used for evaluating locally the modules in the system, which contribute in selecting documents that are likely to contain the answer.", "labels": [], "entities": []}, {"text": "The second one provides a global evaluation of the system.", "labels": [], "entities": []}, {"text": "It also serves for an indirect evaluation of various modules.", "labels": [], "entities": []}, {"text": "shows the architecture of the QALC system, made of five separate modules: Question analysis, Search engine, Re-indexing and selection of documents, Named entity recognition, and Question/sentence pairing.", "labels": [], "entities": [{"text": "Question analysis", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.8667028546333313}, {"text": "Named entity recognition", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.7197856108347574}, {"text": "Question/sentence pairing", "start_pos": 178, "end_pos": 203, "type": "TASK", "confidence": 0.7280140966176987}]}, {"text": "With a <b_numex_TYPE=\"NUMBER\"> 28 <e_numex> -power telescope you can see it on the moon <b_numex_TYPE=\"LENGTH\"> 250,000 miles <e_numex> away.", "labels": [], "entities": [{"text": "LENGTH", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9887613654136658}]}], "datasetContent": [{"text": "The second module of the QALC system deals with the selection, through a search engine, of documents that may contain an answer to a given question from the whole TREC corpus (whose size is about 3 gigabytes).", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 163, "end_pos": 174, "type": "DATASET", "confidence": 0.8155237138271332}]}, {"text": "We tested three search engines with the 200 questions that were proposed at the TREC8 QA track.", "labels": [], "entities": [{"text": "TREC8 QA track", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9226122498512268}]}, {"text": "The first one is Zprise, a vectorial search engine developed by NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.9244610071182251}]}, {"text": "The second is Indexal (de Loupy et al 1998), a pseudo-boolean search engine developed by Bertin Technologies 1 . The third search engine is ATT whose results to the TREC questions are provided by NIST in the form of ranked lists of the top 1000 documents retrieved for each question.", "labels": [], "entities": [{"text": "Indexal (de Loupy et al 1998)", "start_pos": 14, "end_pos": 43, "type": "DATASET", "confidence": 0.8651466369628906}, {"text": "ATT", "start_pos": 140, "end_pos": 143, "type": "DATASET", "confidence": 0.7131298184394836}, {"text": "NIST", "start_pos": 196, "end_pos": 200, "type": "DATASET", "confidence": 0.9613342881202698}]}, {"text": "We based our search engine tests on We are grateful to Bertin Technologies for providing us with the outputs of Indexal on the TREC collection for the TREC8-QA and TREC9-QA question set.", "labels": [], "entities": [{"text": "TREC collection", "start_pos": 127, "end_pos": 142, "type": "DATASET", "confidence": 0.8923697471618652}, {"text": "TREC9-QA question set", "start_pos": 164, "end_pos": 185, "type": "DATASET", "confidence": 0.8992916742960612}]}, {"text": "the list of relevant documents extracted from the list of correct answers provided by TREC organizers.", "labels": [], "entities": []}, {"text": "Since a search engine produces a large ranked list of relevant documents, we had to define the number of documents to retain for further processing.", "labels": [], "entities": []}, {"text": "Indeed, having too many documents leads to a question processing time that is too long, but conversely, having too few documents reduces the possibility of obtaining the correct answer.", "labels": [], "entities": [{"text": "question processing", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7122996747493744}]}, {"text": "The other goal of the tests obviously was to determine the best search engine, that is to say the one that gives the highest number of relevant documents.", "labels": [], "entities": []}, {"text": "We compared the results given by the three search engines fora threshold of 200 documents.", "labels": [], "entities": []}, {"text": "All three search engines perform quite well.", "labels": [], "entities": []}, {"text": "Nevertheless, the ATT search engine revealed itself the most efficient according to the following two criteria: the lowest number of questions for which no relevant document was retrieved, and the most relevant documents retrieved for all the 200 questions.", "labels": [], "entities": [{"text": "ATT search engine", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.7005192041397095}]}, {"text": "First, it is most essential to obtain relevant documents for as many questions as possible.", "labels": [], "entities": []}, {"text": "But the number of relevant documents for each question also counts, since having more sentences containing the answer implies a greater probability to actually find it.", "labels": [], "entities": []}, {"text": "As the processing of 200 documents by the following Natural Language Processing (NLP) modules still was too time-consuming, we needed an additional stronger selection.", "labels": [], "entities": []}, {"text": "The selection of relevant documents performed by the re-indexing and selection module relies on an NLP-based indexing composed of both single-word and phrase indices, and linguistic links between the occurrences and the original terms.", "labels": [], "entities": []}, {"text": "The original terms are extracted from the questions.", "labels": [], "entities": []}, {"text": "The tool used for extracting text sequences that correspond to occurrences or variants of these terms is FASTR (Jacquemin, 1999).", "labels": [], "entities": [{"text": "FASTR", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.8991168141365051}]}, {"text": "The ranking of the documents relies on a weighted combination of the terms and variants extracted from the documents.", "labels": [], "entities": []}, {"text": "The use of multiwords and variants for document weighting makes a finer ranking possible.", "labels": [], "entities": [{"text": "document weighting", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6785713583230972}]}, {"text": "The principle of the selection is the following: when there is a sharp drop of the documents weight curve after a given rank, we keep only those documents which occur before the drop.", "labels": [], "entities": []}, {"text": "Otherwise, we arbitrarily keep the first 100.", "labels": [], "entities": []}, {"text": "In order to evaluate the efficiency of the ranking process, we proceeded to several measures.", "labels": [], "entities": []}, {"text": "First, we apply our system on the material given for the TREC8 evaluation, onetime with the ranking process, and another time without this process.", "labels": [], "entities": [{"text": "TREC8 evaluation", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.5675858855247498}]}, {"text": "200 documents were retained for each of the 200 questions.", "labels": [], "entities": []}, {"text": "The system was scored by 0.463 in the first case, and by 0.452 in the second case.", "labels": [], "entities": []}, {"text": "These results show that document selection slightly improves the final score while much reducing the amount of text to process.", "labels": [], "entities": []}, {"text": "However, a second measurement gave us more details about how things are improved.", "labels": [], "entities": []}, {"text": "Indeed, when we compare the list of relevant documents selected by the search engine with the list of ranker-selected ones, we find that the ranker loses relevant documents.", "labels": [], "entities": []}, {"text": "For thirteen questions among the 200 in the test, the ranker did not consider relevant documents selected by the search engine.", "labels": [], "entities": []}, {"text": "What happens is: the global score improves, because found answers rank higher, but the number of found answers remains the same.", "labels": [], "entities": []}, {"text": "The interest to perform such a selection is also illustrated by the results given  We see that the selection process discards documents for 50% of the questions: 340 questions are processed from less than 100 documents.", "labels": [], "entities": []}, {"text": "For those 340 questions, the average number of selected documents is 37.", "labels": [], "entities": []}, {"text": "The document set retrieved for those questions has a weight curve with a sharp drop.", "labels": [], "entities": []}, {"text": "QALC finds more often the correct answer and in a better position for these 340 questions than for the 342 remaining ones.", "labels": [], "entities": [{"text": "QALC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6002534627914429}]}, {"text": "These results are very interesting when applying such time-consuming processes as named-entities recognition and question/sentence matching.", "labels": [], "entities": [{"text": "named-entities recognition", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.7723798751831055}, {"text": "question/sentence matching", "start_pos": 113, "end_pos": 139, "type": "TASK", "confidence": 0.5891380086541176}]}, {"text": "Document selection will also enable us to apply further sentence syntactic analysis.", "labels": [], "entities": [{"text": "Document selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.866022527217865}, {"text": "sentence syntactic analysis", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7557200292746226}]}, {"text": "We sent to TREC9 two runs which gave answers of 250 characters length, and one run which gave answers of 50 characters length.", "labels": [], "entities": [{"text": "TREC9", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8961699604988098}]}, {"text": "The first and the last runs used ATT as search engine, and the second one, Indexal.", "labels": [], "entities": [{"text": "ATT", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8934166431427002}, {"text": "Indexal", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.924936830997467}]}, {"text": "Results are consistent with our previous analysis (see Section 3.2).", "labels": [], "entities": []}, {"text": "Indeed, the run with ATT search engine gives slightly better results (0.407 strict) 2 than those obtained with the Indexal search engine (0.375 strict).", "labels": [], "entities": [{"text": "ATT search engine", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.8888019124666849}, {"text": "Indexal search engine", "start_pos": 115, "end_pos": 136, "type": "DATASET", "confidence": 0.8560042977333069}]}, {"text": "The score of the run with answers of 50 characters length was not encouraging, amounting only 0.178, with 183 correct answers retrieved 3 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows the test results.", "labels": [], "entities": []}, {"text": " Table 2. Compared performances of the  Indexal, Zprise and ATT search engines", "labels": [], "entities": [{"text": "Indexal", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.8917328715324402}, {"text": "ATT search engines", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.8048874537150065}]}, {"text": " Table 3. Evaluation of the ranking process", "labels": [], "entities": []}, {"text": " Table 4. Number of correct answers retrieved,  by rank, for the two runs at 250 characters", "labels": [], "entities": []}]}