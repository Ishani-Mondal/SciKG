{"title": [{"text": "A Flexible POS Tagger Using an Automatically Acquired Language Model*", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an algorithm that automatically learns context constraints using statistical decision trees.", "labels": [], "entities": []}, {"text": "We then use the acquired constraints in a flexible POS tag-ger.", "labels": [], "entities": []}, {"text": "The tagger is able to use information of any degree: n-grams, automatically learned context constraints, linguistically motivated manually written constraints , etc.", "labels": [], "entities": []}, {"text": "The sources and kinds of constraints are unrestricted, and the language model can be easily extended, improving the results.", "labels": [], "entities": []}, {"text": "The tagger has been tested and evaluated on the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9586619734764099}]}], "introductionContent": [{"text": "In NLP, it is necessary to model the language in a representation suitable for the task to be performed.", "labels": [], "entities": []}, {"text": "The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints.", "labels": [], "entities": []}, {"text": "Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1 , and consists of n-grams (, rules or neural nets.", "labels": [], "entities": []}, {"text": "In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc.", "labels": [], "entities": []}, {"text": "The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees ().", "labels": [], "entities": []}, {"text": "The acquisition methods range from supervised-inductivelearning-from-example algorithms *This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-C03-02 I When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised.", "labels": [], "entities": [{"text": "Spanish Research Department (CICYT)", "start_pos": 136, "end_pos": 171, "type": "DATASET", "confidence": 0.9252572158972422}]}, {"text": "to genetic algorithm strategies, through the transformation-based error-driven algorithm used in, Still another possibility are the hybrid models, which try to join the advantages of both approaches.", "labels": [], "entities": []}, {"text": "We present in this paper a hybrid approach that puts together both trends in automatic approach and the linguistic approach.", "labels": [], "entities": []}, {"text": "We describe a POS tagger based on the work described in, that is able to use bi/trigram information, automatically learned context constraints and linguistically motivated manually written constraints.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.7559829950332642}]}, {"text": "The sources and kinds of constraints are unrestricted, and the language model can be easily extended.", "labels": [], "entities": []}, {"text": "The structure of the tagger is presented in.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of some common errors commited by each model", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9582955837249756}]}, {"text": " Table 2: Tag meanings  of constraint kinds", "labels": [], "entities": []}, {"text": " Table 3: Results of the baseline taggers", "labels": [], "entities": []}]}