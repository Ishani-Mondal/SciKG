{"title": [], "abstractContent": [{"text": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies.", "labels": [], "entities": []}, {"text": "The model assigns probability to every joint sequence of words-binary-parse-structure with head-word annotation.", "labels": [], "entities": []}, {"text": "The model, its proba-bilistic parametrization, and a set of experiments meant to evaluate its predictive power are presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main goal of the proposed project is to develop a language model(LM) that uses syntactic structure.", "labels": [], "entities": []}, {"text": "The principles that guided this propo \u00a7al were: \u2022 the model will develop syntactic knowledge as a built-in feature; it will assign a probability to every joint sequence of words-binary-parse-structure; \u2022 the model should operate in a left-to-right manner so that it would be possible to decode word lattices provided by an automatic speech recognizer.", "labels": [], "entities": []}, {"text": "The model consists of two modules: a next word predictor which makes use of syntactic structure as developed by a parser.", "labels": [], "entities": []}, {"text": "The operations of these two modules are intertwined.", "labels": [], "entities": []}], "datasetContent": [{"text": "Assuming that the correct partial parse is a function of the word prefix, it makes sense to compare the word level perplexity(PP) of a standard n-gram LM with that of the P(wk/Wk-ITk-1) model.", "labels": [], "entities": [{"text": "word level perplexity(PP)", "start_pos": 104, "end_pos": 129, "type": "METRIC", "confidence": 0.788126269976298}]}, {"text": "We developed and evaluated four LMs: \u2022 2 bigram LMs P(wk/Wk-lTk-1) = P(Wk/Wk-1) referred to as W and w, respectively; wk-1 is the previous (word, POStag) pair; \u2022 2 P(wk/Wk-ITk--1) = P(wjho) models, referred to as H and h, respectively; h0 is the previous exposed (headword, POS/non-term tag) pair; the parses used in this model were those assigned manually in the Penn Treebank (Marcus95) after undergoing headword percolation and binarization.", "labels": [], "entities": [{"text": "Penn Treebank (Marcus95)", "start_pos": 364, "end_pos": 388, "type": "DATASET", "confidence": 0.9755888104438781}]}, {"text": "All four LMs predict a word wk and they were implemented using the Maximum Entropy Modeling Toolkit 1 (Ristad97).", "labels": [], "entities": []}, {"text": "The constraint templates in the {W,H} models were: 4 <= <*>_<*> <7>; P-<= <7>_<*> <7>; 2 <= <?>_<7> <?>; 8 <= <*>_<?> <7>; and in the {w,h} models they were: 4 <= <*>_<*> <7>; 2 <= <7>_<*> <7>; <.> denotes a don't care position, <7>_<7> a (word, tag) pair; for example, 4 <= <7>_<*> <7> will trigger on all ((word, any tag), predicted-word) pairs that occur more than 3 times in the training data.", "labels": [], "entities": []}, {"text": "The sentence boundary is not included in the PP calculation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows the PP results along with", "labels": [], "entities": []}]}