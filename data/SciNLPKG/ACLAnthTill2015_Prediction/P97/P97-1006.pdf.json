{"title": [{"text": "Document Classification Using a Finite Mixture Model", "labels": [], "entities": [{"text": "Document Classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8959910571575165}]}], "abstractContent": [{"text": "We propose anew method of classifying documents into categories.", "labels": [], "entities": []}, {"text": "We define for each category a finite mixture model based on soft clustering of words.", "labels": [], "entities": []}, {"text": "We treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models, and employ the EM algorithm to efficiently estimate parameters in a finite mixture model.", "labels": [], "entities": []}, {"text": "Experimental results indicate that our method outperforms existing methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are concerned herewith the issue of classifying documents into categories.", "labels": [], "entities": []}, {"text": "More precisely, we begin with a number of categories (e.g., 'tennis, soccer, skiing'), each already containing certain documents.", "labels": [], "entities": []}, {"text": "Our goal is to determine into which categories newly given documents ought to be assigned, and to do soon the basis of the distribution of each document's words.", "labels": [], "entities": []}, {"text": "1 Many methods have been proposed to address this issue, and a number of them have proved to be quite effective (e.g.,).", "labels": [], "entities": []}, {"text": "The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them.", "labels": [], "entities": []}, {"text": "In order to address this difficulty, have proposed using distributions based on what we refer to as hard 1A related issue is the retrieval, from a database, of documents which are relevant to a given query (pseudodocument) (e.g.,).", "labels": [], "entities": []}, {"text": "clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly.", "labels": [], "entities": []}, {"text": "The use of hard clustering might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories.", "labels": [], "entities": [{"text": "classification", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.9551057815551758}]}, {"text": "We propose hereto employ soft chsterinf, i.e., a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution.", "labels": [], "entities": []}, {"text": "We define for each category a finite mixture model, which is a linear combination of the word probability distributions of the clusters.", "labels": [], "entities": []}, {"text": "We thereby treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models.", "labels": [], "entities": []}, {"text": "In order to accomplish hypothesis testing, we employ the EM algorithm to efficiently and approximately calculate from training data the maximum likelihood estimates of parameters in a finite mixture model.", "labels": [], "entities": [{"text": "hypothesis testing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9426217973232269}]}, {"text": "Our method overcomes the major drawbacks of the method using word-based distributions and the method based on hard clustering, while retaining their merits; it in fact includes those two methods as special cases.", "labels": [], "entities": []}, {"text": "Experimental results indicate that our method outperforrrLs them.", "labels": [], "entities": []}, {"text": "Although the finite mixture model has already been used elsewhere in natural language processing (e.g.), this is the first work, to the best of knowledge, that uses it in the context of document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 186, "end_pos": 209, "type": "TASK", "confidence": 0.7414790987968445}]}], "datasetContent": [{"text": "In this section, we describe the results of the experiments we have conducted to compare the performance of our method with that of HCM and others.", "labels": [], "entities": []}, {"text": "As a first data set, we used a subset of the Reuters newswire data prepared by Lewis, called Reuters-21578 Distribution 1.0. 7 We selected nine overlapping categories, i.e. in which a document may berReuters-21578 is available at http://www.research.att.com/lewis.", "labels": [], "entities": [{"text": "Reuters newswire data prepared by Lewis", "start_pos": 45, "end_pos": 84, "type": "DATASET", "confidence": 0.9638576805591583}, {"text": "Reuters-21578 Distribution 1.0.", "start_pos": 93, "end_pos": 124, "type": "DATASET", "confidence": 0.9270960092544556}]}, {"text": "long to several different categories.", "labels": [], "entities": []}, {"text": "We adopted the Lewis Split in the corpus to obtain the training data and the test data.", "labels": [], "entities": [{"text": "Lewis Split in the corpus", "start_pos": 15, "end_pos": 40, "type": "DATASET", "confidence": 0.8993510127067565}]}, {"text": "12 and 13 give the details.", "labels": [], "entities": []}, {"text": "We did not conduct stemming, or use stop words s.", "labels": [], "entities": [{"text": "stemming", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.9838610887527466}]}, {"text": "We then applied FMM, HCM, WBM , and a method based on cosine-similarity, which we denote as COS 9, to conduct binary classification.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.6093389391899109}]}, {"text": "In particular, we learn the distribution for each category and that for its complement category from the training data, and then determine whether or not to classify into each category the documents in the test data.", "labels": [], "entities": []}, {"text": "When applying FMM, we used our proposed method of creating clusters in Section 4 and set 7 to be 0, 0.4, 0.5, 0.7, because these are representative values.", "labels": [], "entities": []}, {"text": "For HCM, we classified words in the same way as in FMM and set 7 to be 0.5, 0.7, 0.9, 0.95.", "labels": [], "entities": []}, {"text": "(Notice that in HCM, 7 cannot beset less than 0.5.)", "labels": [], "entities": []}, {"text": "As a second data set, we used the entire Reuters-21578 data with the Lewis Split.", "labels": [], "entities": [{"text": "Reuters-21578 data", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9885616302490234}, {"text": "Lewis Split", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9807759523391724}]}, {"text": "Again, we did not conduct stemming, or use stop words.", "labels": [], "entities": [{"text": "stemming", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.976312518119812}]}, {"text": "We then applied FMM, HCM, WBM , and COS to conduct binary classification.", "labels": [], "entities": [{"text": "FMM", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8160905838012695}, {"text": "COS", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8831398487091064}, {"text": "binary classification", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7560293972492218}]}, {"text": "When applying FMM, we used our proposed method of creating clusters and set 7 to be 0, 0.4, 0.5, 0.7.", "labels": [], "entities": []}, {"text": "For HCM, we classified words in the same way as in FMM and set 7 to be 0.5, 0.7, 0.9, 0.95.", "labels": [], "entities": []}, {"text": "We have not fully completed these experiments, however, and here we only 8'Stop words' refers to a predetermined list of words containing those which are considered not useful for document classification, such as articles and prepositions.", "labels": [], "entities": [{"text": "document classification", "start_pos": 180, "end_pos": 203, "type": "TASK", "confidence": 0.6992298513650894}]}, {"text": "9In this method, categories and documents to be classified are viewed as vectors of word frequencies, and the cosine value between the two vectors reflects similarity.", "labels": [], "entities": []}, {"text": "give the results of classifying into the ten categories having the greatest numbers of documents in the test data (see show precision-recall curves for the first data set and those for the second data set, respectively.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 124, "end_pos": 140, "type": "METRIC", "confidence": 0.9943247437477112}]}, {"text": "In these graphs, values given after FMM and HCM represent 3' in our clustering method (e.g. FMM0.5, HCM0.5,etc).", "labels": [], "entities": [{"text": "HCM", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.7693771719932556}, {"text": "FMM0.5", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.951521635055542}]}, {"text": "We adopted the break-even point as a single measure for comparison, which is the one at which precision equals recall; a higher score for the break-even point indicates better performance.", "labels": [], "entities": [{"text": "break-even point", "start_pos": 15, "end_pos": 31, "type": "METRIC", "confidence": 0.9715135395526886}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9989307522773743}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9984147548675537}]}, {"text": "16 shows the break-even point for each method for the first data set and Tab.", "labels": [], "entities": []}, {"text": "17 shows that for the second data set.", "labels": [], "entities": []}, {"text": "For the first data set, FMM0 attains the highest score at break-even point; for the second data set, FMM0.5 attains the highest.", "labels": [], "entities": [{"text": "FMM0", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.5829099416732788}, {"text": "break-even point", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.9687834084033966}, {"text": "FMM0.5", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.706026554107666}]}, {"text": "We considered the following questions: (1) The training data used in the experimentation maybe considered sparse.", "labels": [], "entities": []}, {"text": "Willa wordclustering-based method (FMM) outperform a wordbased method (WBM) here?", "labels": [], "entities": []}, {"text": "(2) Is it better to conduct soft clustering (FMM) than to do hard clustering (HCM)?", "labels": [], "entities": [{"text": "hard clustering (HCM)", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7010038256645202}]}, {"text": "(3) With our current method of creating clusters, as the threshold 7 approaches 0, FMM behaves much like WBM and it does not enjoy the effects of clustering at all (the number of parameters is as large l\u00b0In micro-averaging(, precision is defined as the percentage of classified documents in all categories which are correctly classified.", "labels": [], "entities": [{"text": "precision", "start_pos": 225, "end_pos": 234, "type": "METRIC", "confidence": 0.9980424642562866}]}, {"text": "Recall is defined as the percentage of the total documents in all categories which are correctly classified.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9142282009124756}]}, {"text": "nNotice that words which are discarded in the dustering process should not to be counted in document size.", "labels": [], "entities": [{"text": "nNotice", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9661486744880676}]}, {"text": "This is because in this case (a) a word will be assigned into all of the clusters, (b) the distribution of words in each cluster will approach that in the corresponding category in WBM, and (c) the likelihood value for each category will approach that in WBM (recall case (2) in Section 3).", "labels": [], "entities": [{"text": "WBM", "start_pos": 181, "end_pos": 184, "type": "DATASET", "confidence": 0.8515839576721191}, {"text": "WBM", "start_pos": 255, "end_pos": 258, "type": "DATASET", "confidence": 0.8927034139633179}, {"text": "recall", "start_pos": 260, "end_pos": 266, "type": "METRIC", "confidence": 0.9254701733589172}]}, {"text": "Since creating clusters in an optimal way is difficult, when clustering does not improve performance we can at least make FMM perform as well as WBM by choosing 7 = 0.", "labels": [], "entities": []}, {"text": "The question now is \"does FMM perform better than WBM when 7 is 0?\"", "labels": [], "entities": [{"text": "WBM", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.8351306319236755}]}, {"text": "In looking into these issues, we found the following: (1) When 3' >> 0, i.e., when we conduct clustering, FMM does not perform better than WBM for the first data set, but it performs better than WBM for the second data set.", "labels": [], "entities": []}, {"text": "Evaluating classification results on the basis of each individual category, we have found that for three of the nine categories in the first data set, shows the best result for each method for the category 'corn' in the first data set and that for 'grain' in the second data set.)", "labels": [], "entities": []}, {"text": "(2) When 3' >> 0, i.e., when we conduct clustering, the best of FMM almost always outperforms that of HCM.", "labels": [], "entities": []}, {"text": "(3) When 7 = 0, FMM performs better than WBM for the first data set, and that it performs as well as WBM for the second data set.", "labels": [], "entities": [{"text": "WBM", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8300125598907471}]}, {"text": "In summary, FMM always outperforms HCM; in some cases it performs better than WBM; and in general it performs at least as well as WBM.", "labels": [], "entities": [{"text": "WBM", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.8788297772407532}]}, {"text": "For both data sets, the best FMM results are superior to those of COS throughout.", "labels": [], "entities": [{"text": "COS", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.608798623085022}]}, {"text": "This indicates that the probabilistic approach is more suitable than the cosine approach for document classification based on word distributions.", "labels": [], "entities": [{"text": "document classification", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.766561895608902}]}, {"text": "Although we have not completed our experiments on the entire Reuters data set, we found that the results with FMM on the second data set are almost as good as those obtained by the other approaches reported in (.", "labels": [], "entities": [{"text": "Reuters data set", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9896415869394938}]}, {"text": "(The results are not directly comparable, because (a) the results in ( were obtained from an older version of the Reuters data; and (b) they used stop words, but we did not.)", "labels": [], "entities": [{"text": "Reuters data", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.9823984801769257}]}, {"text": "We have also conducted experiments on the Susanne corpus data t2 and confirmed the effectiveness of our method.", "labels": [], "entities": [{"text": "Susanne corpus data t2", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9642788469791412}]}, {"text": "We omit an explanation of this work here due to space limitations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frequencies of words  racket stroke shot goal kick ball  cl  4  1  2  1  0  2  c2  0  0  0  3  2  2", "labels": [], "entities": []}, {"text": " Table 2: Clusters and words (L = 5,M = 5)  ' kl racket, stroke, shot  ks kick  . k 3 goal, ball", "labels": [], "entities": []}, {"text": " Table 3: Frequencies of clusters  kl ks k3  c 1 7  0  3  c2  0  2  5", "labels": [], "entities": []}, {"text": " Table 4: Probability distributions of clusters  kl  k2  k3  cl 0.65 0.04 0.30  cs 0.06 0.29 0.65", "labels": [], "entities": []}, {"text": " Table 7: Distributed frequencies of words  racket stroke shot goal kick ball  kl  4  1  2  0  0  2  k2  0  0  0  4  2  2", "labels": [], "entities": []}, {"text": " Table 8: Probability distributions of words  racket stroke shot goal kick ball  kl  0.44  0.11  0.22  0  0  0.22  k2  0  0  0  0.50 0.25 0.25", "labels": [], "entities": []}, {"text": " Table 10: Calculating log likelihood values", "labels": [], "entities": []}, {"text": " Table 12: The first data set  Num. of doc. in training data  707  Num. of doc in test data  228  Num. of (type of) words  10902  Avg. num. of words per doe.  310.6", "labels": [], "entities": [{"text": "Avg.", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9647703766822815}]}, {"text": " Table 14: The second data set  Num. of doc. training data  13625  Num. of doc. in test data  6188  Num. of (type of) words  50301  Avg. num. of words per doc.  181.3", "labels": [], "entities": [{"text": "Avg.", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9619720578193665}]}, {"text": " Table 16: Break-even point  COS  WBM  HCM0.5  HCM0.7  HCM0.9  HCM0.95  FMM0  FMM0.4  FMM0.5  FMM0.7", "labels": [], "entities": [{"text": "Break-even point  COS", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.830476442972819}, {"text": "WBM  HCM0.5  HCM0.7  HCM0.9  HCM0.95  FMM0  FMM0.4  FMM0.5", "start_pos": 34, "end_pos": 92, "type": "DATASET", "confidence": 0.8156016692519188}, {"text": "FMM0.7", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.8382481336593628}]}, {"text": " Table 17: Break-even point for the  COS  10.52  WBM  !0.62  HCM0.5 10.47  HCM0.7 i0.51  HCM0.9 10.55  HCM0.95 0.31  FMM0  i0.62  FMM0.4  0.54  FMM0.5  0.67  FMM0.7  0.62", "labels": [], "entities": [{"text": "Break-even", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9942455887794495}, {"text": "COS  10.52  WBM  !0.62  HCM0.5 10.47  HCM0.7 i0.51  HCM0.9 10.55  HCM0.95 0.31  FMM0  i0.62  FMM0.4  0.54  FMM0.5  0.67  FMM0.7  0.62", "start_pos": 37, "end_pos": 170, "type": "DATASET", "confidence": 0.8591805724870591}]}]}