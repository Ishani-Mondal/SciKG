{"title": [{"text": "Mistake-Driven Mixture of Hierarchical Tag Context Trees", "labels": [], "entities": [{"text": "Mistake-Driven Mixture of Hierarchical Tag Context", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8460695743560791}]}], "abstractContent": [{"text": "This paper proposes a mistake-driven mixture method for learning a tag model.", "labels": [], "entities": []}, {"text": "The method iteratively performs two procedures: 1.", "labels": [], "entities": []}, {"text": "constructing a tag model based on the current data distribution and 2.", "labels": [], "entities": []}, {"text": "updating the distribution by focusing on data that are not well predicted by the constructed model.", "labels": [], "entities": []}, {"text": "The final tag model is constructed by mixing all the models according to their performance.", "labels": [], "entities": []}, {"text": "To well reflect the data distribution, we represent each tag model as a hierarchical tag (i.e.,NTT 1 < proper noun < noun) context tree.", "labels": [], "entities": []}, {"text": "By using the hierarchical tag context tree, the constituents of sequential tag models gradually change from broad coverage tags (e.g.,noun) to specific exceptional words that cannot be captured by generM tags.", "labels": [], "entities": []}, {"text": "In other words, the method incorporates not only frequent connections but also infrequent ones that are often considered to be collocationah We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models.", "labels": [], "entities": []}, {"text": "The experimental results show the proposed method significantly outper-forms both hand-crafted and conventional statistical methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The last few years have seen the great success of stochastic part-of-speech (POS) taggers.", "labels": [], "entities": [{"text": "stochastic part-of-speech (POS) taggers", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.5833007941643397}]}, {"text": "The stochastic approach generally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.998945415019989}]}, {"text": "However, 1NTT is an abbreviation of Nippon Telegraph and Telephone Corporation.", "labels": [], "entities": [{"text": "1NTT", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8001987934112549}, {"text": "Nippon Telegraph and Telephone Corporation", "start_pos": 36, "end_pos": 78, "type": "DATASET", "confidence": 0.9595214247703552}]}, {"text": "practical systems require more accuracy because POS tagging is an inevitable pre-processing step for all practical systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.995553195476532}, {"text": "POS tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.807848185300827}]}, {"text": "To derive anew stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model.", "labels": [], "entities": []}, {"text": "The word model is a set of probabilities that a word occurs with a tag (part-of-speech) when given the preceding words and their tags in a sentence.", "labels": [], "entities": []}, {"text": "On the contrary, the tag model is a set of probabilities that a tag appears after the preceding words and their tags.", "labels": [], "entities": []}, {"text": "The first option is to construct more sophisticated word models.", "labels": [], "entities": []}, {"text": "() reports that their model considers the roots and suffixes of words to greatly improve tagging accuracy for English corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.8988690972328186}]}, {"text": "However, the word model approach has the following shortcomings: \u2022 For agglutinative languages such as Japanese and Chinese, the simple Bayes transfer rule is inapplicable because the word length of a sentence is not fixed in all possible segmentations -~.", "labels": [], "entities": []}, {"text": "We can only use simpler word models in these languages.", "labels": [], "entities": []}, {"text": "\u2022 Sophisticated word models largely depend on the target language.", "labels": [], "entities": []}, {"text": "It is time-consuming to compile fine-grained word models for each language.", "labels": [], "entities": []}, {"text": "The second option is to devise anew tag model.", "labels": [], "entities": []}, {"text": "( have introduced a variable-memory-length tag model.", "labels": [], "entities": []}, {"text": "Unlike conventional bi-gram and tri-gram models, the method selects the optimal length by using the context tree which was originally introduced for use in data compression.", "labels": [], "entities": [{"text": "data compression", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.7229999899864197}]}, {"text": "Although the variable-memory length approach remarkably reduces the number of parameters, tagging accuracy is only as good as conventional methods.", "labels": [], "entities": [{"text": "tagging", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.9703200459480286}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9790098667144775}]}, {"text": "Why didn't the method have higher accuracy ? The crucial problem for current P(,,,)P(,,lu,,) P(wi) cannot be consid2In P(w,]t,) = P(t,) ' ered to be identical for ~ll segmentations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9988994598388672}]}, {"text": "tag models is the set of collocational sequences of words that cannot be captured by just their tags.", "labels": [], "entities": []}, {"text": "Because the maximal likelihood estimator (MLE) emphasizes the most frequent connections, an exceptional connection is placed in the same class as a frequent connection.", "labels": [], "entities": [{"text": "maximal likelihood estimator (MLE)", "start_pos": 12, "end_pos": 46, "type": "METRIC", "confidence": 0.8139921228090922}]}, {"text": "To tackle this problem, we introduce anew tag model based on the mistake-driven mixture of hierarchical tag context trees.", "labels": [], "entities": []}, {"text": "Compared to Schiitze and Singer's context tree (, the hierarchical tag context tree is extended in that the context is represented by a hierarchical tag set (i.e.,NTT < proper noun < noun).", "labels": [], "entities": []}, {"text": "This is extremely useful in capturing exceptional connections that can be detected only at the word level.", "labels": [], "entities": []}, {"text": "To make the best use of the hierarchical context tree, the mistake-driven mixture method imitates the process in which linguists incorporate exceptional connections into hand-crafted rules: They first construct coarse rules which seems to cover broad range of data.", "labels": [], "entities": []}, {"text": "They then try to analyze data by using the rules and extract exceptions that the rules cannot handle.", "labels": [], "entities": []}, {"text": "Next they generalize the exceptions and refine the previous rules.", "labels": [], "entities": []}, {"text": "The following two steps abstract the human algorithm for incorporating exceptional connections.", "labels": [], "entities": []}, {"text": "1. construct temporary rules which seem to well generalize given data.", "labels": [], "entities": []}, {"text": "2. try to analyze data by using the constructed rules and extract the exceptions that cannot be correctly handled, then return to the first step and focus on the exceptions.", "labels": [], "entities": []}, {"text": "To put the above idea into our learning algorithm, The mistake-driven mixture method attaches a weight vector to each example and iteratively performs the following two procedures in the training phase: 1.", "labels": [], "entities": []}, {"text": "constructing a context tree based on the current data distribution (weight vector) 2.", "labels": [], "entities": []}, {"text": "updating the distribution (weight vector) by focusing on data not well predicted by the constructed tree.", "labels": [], "entities": []}, {"text": "More precisely, the algorithm reduces the weight of examples that are correctly handled.", "labels": [], "entities": []}, {"text": "For the prediction phase, it then outputs a final tag model by mixing all the constructed models according to their performance.", "labels": [], "entities": []}, {"text": "By using the hierarchical tag context tree, the constituents of a series of tag models gradually change from broad coverage tags (e.g.,noun) to specific exceptional words that cannot be captured by general tags, In other words, the method incorporates not only frequent connections but also infrequent ones that are often considered to be exceptional.", "labels": [], "entities": []}, {"text": "The construction of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the stochastic POS tagging scheme and hierarchical tag setting.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.7626953125}]}, {"text": "Section 3 presents anew probability estimator that uses a hierarchical tag context tree and Section 4 explains the mistakedriven mixture method.", "labels": [], "entities": []}, {"text": "Section 5 reports a preliminary evaluation using Japanese newspaper articles.", "labels": [], "entities": []}, {"text": "We tested several tag models by keeping all other conditions (i.e., dictionary and word model) identical.", "labels": [], "entities": []}, {"text": "The experimental results show that the proposed method significantly outperforms both handcrafted and conventional statistical methods.", "labels": [], "entities": []}, {"text": "Section 6 concerns related works and Sections 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed an preliminary evaluation using the first 8939 Japanese sentences in a year's volume of newspaper articles.", "labels": [], "entities": []}, {"text": "We first automatically segmented and tagged these sentences and then revised them by hand.", "labels": [], "entities": []}, {"text": "The total number of words in the hand-revised corpus was 226162.", "labels": [], "entities": []}, {"text": "We trained our tag models on the corpora with every tenth sentence removed (starting with the first sentence) and then tested the removed sentences.", "labels": [], "entities": []}, {"text": "There were 22937 words in the test corpus.", "labels": [], "entities": []}, {"text": "As the first milestone of performance, we tested a hand-crafted tag model of JUMAN (, the most widely used Japanese part-ofspeech tagger.", "labels": [], "entities": []}, {"text": "The tagging accuracy of JUMAN for the test corpus was only 92.0 %.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9795962572097778}]}, {"text": "This shows that our corpus is difficult to tag because the corpus contains various genres of texts; from obituaries to poetry.", "labels": [], "entities": []}, {"text": "we compared the mixture of bi-grams and the mixture of hierarchical tag context trees.", "labels": [], "entities": []}, {"text": "In this experiment, only post-positional particles and auxiliaries were word-level elements of basic tags and all other elements were subdivision level.", "labels": [], "entities": []}, {"text": "In contrast, bi-gram was constructedby using subdivision level.", "labels": [], "entities": []}, {"text": "We set the iteration number T to 5.", "labels": [], "entities": []}, {"text": "The results of our experiments are summarized in.", "labels": [], "entities": []}, {"text": "As a singletree estimator (Number of Mixture = 1), the hierarchical tag context tree attained 94.1% accuracy, while bi-gram yielded 93.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9993182420730591}]}, {"text": "A hierarchical tag context tree offers a slight improvement, but  When we turn to the mixture estimator, a great difference is seen between hierarchical tag context trees and bi-grams.", "labels": [], "entities": []}, {"text": "The hierarchical tag context trees produced by the mistake-driven mixture method, greatly improved the accuracy and overfitting data was not serious.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.99922776222229}]}, {"text": "The best and worst performances were 96.1% (Number of Mixture = 3) and 94.1% (Number of Mixture = 1), respectively.", "labels": [], "entities": []}, {"text": "On the other hand, the performance of the bi-gram mixture was not satisfactory.", "labels": [], "entities": []}, {"text": "Tile best and worst performances were 93.8 % (Number of Mixture = 2) and 90.8 % (Number of Mixture = 5), respectively.", "labels": [], "entities": [{"text": "Tile", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9771991968154907}]}, {"text": "From the result, we may say exceptional connections are well captured by hierarchical context trees but not by bi-grams.", "labels": [], "entities": []}, {"text": "Bi-grams of subdivision are too general to selectively detect exceptions.", "labels": [], "entities": []}], "tableCaptions": []}