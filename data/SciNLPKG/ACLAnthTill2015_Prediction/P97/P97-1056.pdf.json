{"title": [{"text": "Memory-Based Learning: Using Similarity for Smoothing", "labels": [], "entities": [{"text": "Smoothing", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.9066522717475891}]}], "abstractContent": [{"text": "This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language model-ing.", "labels": [], "entities": []}, {"text": "We show that the two approaches are closely related, and we argue that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need fora large number of parameters.", "labels": [], "entities": []}, {"text": "We report two applications of this approach: PP-attachment and POS-tagging.", "labels": [], "entities": []}, {"text": "Our method achieves state-of-the-art performance in both domains, and allows the easy integration of diverse information sources, such as rich lexical representations .", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical approaches to disambiguation offer the advantage of making the most likely decision on the basis of available evidence.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9864985942840576}]}, {"text": "For this purpose a large number of probabilities has to be estimated from a training corpus.", "labels": [], "entities": []}, {"text": "However, many possible conditioning events are not present in the training data, yielding zero Maximum Likelihood (ML) estimates.", "labels": [], "entities": [{"text": "Maximum Likelihood (ML)", "start_pos": 95, "end_pos": 118, "type": "METRIC", "confidence": 0.867644202709198}]}, {"text": "This motivates the need for smoothing methods, which reestimate the probabilities of low-count events from more reliable estimates.", "labels": [], "entities": []}, {"text": "Inductive generalization from observed to new data lies at the heart of machine-learning approaches to disambiguation.", "labels": [], "entities": []}, {"text": "In Memory-Based Learning 1 (MBL) induction is based on the use of similarity.", "labels": [], "entities": [{"text": "Memory-Based Learning 1 (MBL) induction", "start_pos": 3, "end_pos": 42, "type": "TASK", "confidence": 0.5957364227090564}]}, {"text": "In this paper we describe how the use of similarity between patterns embodies a solution to the sparse data problem, how it 1The Approach is also referred to as Case-based, Instance-based or Exemplar-based.", "labels": [], "entities": []}, {"text": "relates to backed-off smoothing methods and what advantages it offers when combining diverse and rich information sources.", "labels": [], "entities": []}, {"text": "We illustrate the analysis by applying MBL to two tasks where combination of information sources promises to bring improved performance: PPattachment disambiguation and Part of Speech tagging.", "labels": [], "entities": [{"text": "PPattachment disambiguation", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.9320301115512848}, {"text": "Part of Speech tagging", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.6136251017451286}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Accuracy on the PP-attachment test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986373782157898}, {"text": "PP-attachment test set", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.7736591200033823}]}]}