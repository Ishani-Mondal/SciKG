{"title": [], "abstractContent": [{"text": "We describe a simple variant of the interpolated Markov model with non-emitting state transitions and prove that it is strictly more powerful than any Markov model.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate that the non-emitting model outperforms the interpolated model on the Brown corpus and on the Wall Street Journal under a wide range of experimental conditions.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.8780544400215149}, {"text": "Wall Street Journal", "start_pos": 124, "end_pos": 143, "type": "DATASET", "confidence": 0.947435200214386}]}, {"text": "The non-emitting model is also much less prone to overtraining.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Markov model has long been the core technology of statistical language modeling.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.8717705607414246}]}, {"text": "Many other models have been proposed, but none has offered a better combination of predictive performance, computational efficiency, and ease of implementation.", "labels": [], "entities": []}, {"text": "Here we add hierarchical non-emitting state transitions to the Markov model.", "labels": [], "entities": []}, {"text": "Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution.", "labels": [], "entities": []}, {"text": "Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model), the backoff model, and the interpolated Markov model.", "labels": [], "entities": []}, {"text": "More importantly, the non-emitting model consistently outperforms the interpolated Markov model on natural language texts, under a wide range of experimental conditions.", "labels": [], "entities": []}, {"text": "We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence.", "labels": [], "entities": []}, {"text": "Thus, the non-emitting model is better able to represent both conditional independence and long-distance dependence, ie., it is simply a better statistical model.", "labels": [], "entities": []}, {"text": "The non-emitting model is also nearly as computationally eff\u00c9cient and easy to implement as the interpolated model.", "labels": [], "entities": []}, {"text": "The remainder of our article consists of four sections.", "labels": [], "entities": []}, {"text": "In section 2, we review the interpolated Markov model and briefly demonstrate that all interpolated models are equivalent to some basic Markov model of the same model order.", "labels": [], "entities": []}, {"text": "Next, we introduce the hierarchical non-emitting Markov model in section 3, and prove that even a lowly second order non-emitting model is strictly more powerful than any basic Markov model, of any model order.", "labels": [], "entities": []}, {"text": "In section 4, we report empirical results for the interpolated model and the non-emitting model on the Brown corpus and Wall Street Journal.", "labels": [], "entities": [{"text": "Brown corpus and Wall Street Journal", "start_pos": 103, "end_pos": 139, "type": "DATASET", "confidence": 0.9148075381914774}]}, {"text": "Finally, in section 5 we conjecture that the empirical success of the non-emitting model is due to its ability to better model a point of apparent independence, such as may occur at a sentence boundary.", "labels": [], "entities": []}, {"text": "Our notation is as follows.", "labels": [], "entities": []}, {"text": "Let Abe a finite alphabet of distinct symbols, [A[ = k, and let z T 6 A T denote an arbitrary string of length T over the alphabet A.", "labels": [], "entities": []}, {"text": "Then z~ denotes the substring of z T that begins at position i and ends at position j.", "labels": [], "entities": []}, {"text": "For convenience, we abbreviate the unit length substring z~ as zi and the length t prefix of z T as z*.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}