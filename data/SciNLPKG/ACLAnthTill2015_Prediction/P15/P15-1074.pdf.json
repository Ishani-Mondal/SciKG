{"title": [{"text": "Vector-space calculation of semantic surprisal for predicting word pronunciation duration", "labels": [], "entities": [{"text": "Vector-space calculation of semantic surprisal", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7100631594657898}, {"text": "predicting word pronunciation", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.8777331908543905}]}], "abstractContent": [{"text": "In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models.", "labels": [], "entities": []}, {"text": "Here we specifically consider surprisal, a word's predictability in context.", "labels": [], "entities": []}, {"text": "Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics.", "labels": [], "entities": []}, {"text": "We build on the work by Mitchell et al.", "labels": [], "entities": []}, {"text": "(2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data.", "labels": [], "entities": [{"text": "semantic prediction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7420596778392792}]}, {"text": "An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on in-domain data, even though a better language model for our data, is notable to predict word durations, while the out-of-domain trained language model does predict word durations.", "labels": [], "entities": []}, {"text": "We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the \"language models\" of the speakers in our data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively constant rate of information transfer during speech production (e.g.,.", "labels": [], "entities": [{"text": "Uniform Information Density (UID)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7394839376211166}]}, {"text": "The rate of information transfer is thereby quantified using as each, that is, a word's negative log probability in context.", "labels": [], "entities": [{"text": "information transfer", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7712949216365814}]}, {"text": "Surprisal(w i ) = \u2212 log P (w i |w", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7898187637329102}]}], "datasetContent": [{"text": "Our goal is to test whether semantically reweighted surprisal can explain spoken word durations over and above more simple factors that are known to influence word durations, such as word length, frequency and predictability using a simpler language model.", "labels": [], "entities": []}, {"text": "Our first experiment tests whether semantic surprisal based on a model trained using in-domain data is predictive of word pronunciation duration, considering the UID hypothesis.", "labels": [], "entities": [{"text": "word pronunciation duration", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.6006900370121002}]}, {"text": "For our in-domain model, we estimate surprisal using 10-fold cross-validation over the AMI corpus: we divide the corpus into ten equally-sized segments and produce surprisal values for each word in each segment based on a model trained from the other nine segments.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.9286122918128967}]}, {"text": "We then use linear mixed effects modeling (LME) via the lme4 package in R () in order to account for word pronunciation length.", "labels": [], "entities": [{"text": "word pronunciation length", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.6729148825009664}]}, {"text": "We follow the approach of.", "labels": [], "entities": []}, {"text": "Linear mixed effects modelling is a generalization of linear regression modeling and includes both fixed effects and random effects.", "labels": [], "entities": [{"text": "Linear mixed effects modelling", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.562827855348587}, {"text": "linear regression modeling", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6602948804696401}]}, {"text": "This is particularly useful when we have a statistical units (e.g., speakers) each with their own set of repeated measures (e.g., word duration), but each such unit has its own particular characteristics (e.g., some speakers naturally speak more slowly than others).", "labels": [], "entities": []}, {"text": "These are the random effects.", "labels": [], "entities": []}, {"text": "The fixed effects are those characteristics that are expected not to vary across such units.", "labels": [], "entities": []}, {"text": "LME modeling learns coefficients for all of the predictors, defining a regression equation that should account for the data in the dependent variable (in our case, word pronunciation duration).", "labels": [], "entities": [{"text": "LME modeling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8392422795295715}]}, {"text": "The variance in the data that a model cannot explain is referred to as the residual.", "labels": [], "entities": []}, {"text": "We denote statistical significances in the following way: *** means a p-value \u2264 0.001, ** means p \u2264 0.01, * means p \u2264 0.05, and no stars means that the predictor is not significant (p > 0.05).", "labels": [], "entities": []}, {"text": "In our regression models, all the variables are centered and scaled to reduce effects of correlations between predictors.", "labels": [], "entities": []}, {"text": "Furthermore, we logtransformed the response variable (actual spoken word durations from the corpus) as well as the duration estimates from the MARY speech synthesis system to obtain more normal distributions, which are prerequisite for applying the LME models.", "labels": [], "entities": [{"text": "MARY speech synthesis", "start_pos": 143, "end_pos": 164, "type": "TASK", "confidence": 0.65299391746521}]}, {"text": "All conclusions drawn here also hold for versions of the model where no log transformation is used.", "labels": [], "entities": []}, {"text": "From the AMI corpus, we filter out data points (words) that have a pronunciation duration of zero or those that are longer than two seconds, the latter in order to avoid including such things as pauses for thought.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.838378518819809}]}, {"text": "We also remove items that are not represented in Gigaword.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9335901141166687}]}, {"text": "That leaves us with 790,061 data points for further analysis.", "labels": [], "entities": []}, {"text": "However, in our semantic model, function words are not affected by the \u2206 semantic similarity adjustment and are therefore not analyzable for the effect of semantically-weighted trigram predictability.", "labels": [], "entities": []}, {"text": "That leaves 260k data points for analysis in the models.", "labels": [], "entities": []}, {"text": "The AMI corpus contains spoken conversations, and is thus quite different from the written corpora we have available.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.740073636174202}]}, {"text": "When we train an ngram model in domain (using 10-fold cross validation), perplexities for the in-domain model (67.9) are much lower than fora language model trained on gigaword, showing that the in-domain model is a better language model for the data . In order to seethe effect of semantic surprisal estimated based on the in-domain language model and reweighted for semantic similarity within the same sentence as described in Section 3, we then expand the baseline model, adding S Semantics as a predictor.", "labels": [], "entities": []}, {"text": "shows the fixed effects of this expanded model.", "labels": [], "entities": []}, {"text": "The predictor for semantic surprisal is significant, but the coefficient is negative.", "labels": [], "entities": [{"text": "predictor", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9871214032173157}, {"text": "semantic surprisal", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9172813594341278}]}, {"text": "This apparently contradicts our hypothesis that semantic surprisal has a UID effect on pronunciation duration, so that higher S Semantics means higher D AMI . We found that these results are very stable-in particular, the same results also hold if we estimate a separate model with S Semantics as a predictor and residuals of the baseline model as a: Fixed effects of the baseline model with semantic surprisal (including also a random slope for semantic surprisal under subject).", "labels": [], "entities": [{"text": "UID", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.971668541431427}, {"text": "D AMI", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.8936234712600708}]}, {"text": "In order to understand the unexpected behaviour of S Semantics , we make use of a generalized additive model (GAM) with the R package mgcv.", "labels": [], "entities": []}, {"text": "Compared to LME models, GAMs are parameter-free and do not assume a linear form of the predictors.", "labels": [], "entities": [{"text": "GAMs", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9659391641616821}]}, {"text": "Instead, for every predictor, GAMs can fit a spline.", "labels": [], "entities": []}, {"text": "We learn a GAM using the residuals of the baseline model as a response variable and fitting semantic surprisal based on the in-domain model; see.", "labels": [], "entities": [{"text": "GAM", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9827787280082703}]}, {"text": "In, we see that S Semantics is poorly fit by a linear function.", "labels": [], "entities": []}, {"text": "In particular, there are two intervals in the curve.", "labels": [], "entities": []}, {"text": "Between surprisal values 0 and 1.5, the curve falls, but between 1.5 and 4, it rises.", "labels": [], "entities": [{"text": "surprisal", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9675517678260803}]}, {"text": "(For high surprisal values, there are too few data points from which to draw conclusions.)", "labels": [], "entities": []}, {"text": "Therefore, we decided to divide the data up into datapoints with S Semantics above 1.5 and below 1.5.", "labels": [], "entities": []}, {"text": "We then modelled the effect of S Semantics on the residuals of the baseline model, with S Semantics as a random effect.", "labels": [], "entities": []}, {"text": "This is to remove a possible effect of collinearity between S Semantics and the other predictors.", "labels": [], "entities": []}, {"text": "In order to test for the effect of possible underestimation of surprisal due to in-domain training, we also tested the semantic surprisal model when trained on more domain-general text.", "labels": [], "entities": []}, {"text": "As training data for our semantic model, we use a randomly selected 1% (by sentence) of the English Gigaword 5.0 corpus.", "labels": [], "entities": [{"text": "English Gigaword 5.0 corpus", "start_pos": 92, "end_pos": 119, "type": "DATASET", "confidence": 0.8433175981044769}]}, {"text": "This is lowercased, with hapax legomena treated as unknown words.", "labels": [], "entities": []}, {"text": "We test the model against the entire AMI corpus.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.7705739736557007}]}, {"text": "Furthermore, we also compare our semantic surprisal values to the syntactic surprisal values calculated by for the AMI corpus, which we obtained from the authors.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.9124538600444794}]}, {"text": "As noted above, the out-of-domain language model has higher perplexity on the AMI corpus-that is, it is a lowerperforming language model.", "labels": [], "entities": [{"text": "AMI corpus-that", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9391774535179138}]}, {"text": "On the other hand, it may represent overall speaker experience more accurately than the in-domain model; in other words, it maybe a better model of the speaker.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Fixed effects of a baseline model includ- ing the data points for which we could calculate  semantic surprisal.", "labels": [], "entities": []}, {"text": " Table 2: Fixed effects of the baseline model with  semantic surprisal (including also a random slope  for semantic surprisal under subject).", "labels": [], "entities": []}, {"text": " Table 4: Model of spoken word durations,  with random intercept and random slopes for  D MARY and S Semantics under speaker.", "labels": [], "entities": [{"text": "spoken word durations", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.6125679314136505}, {"text": "D", "start_pos": 88, "end_pos": 89, "type": "METRIC", "confidence": 0.8344057202339172}, {"text": "MARY", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.843681275844574}]}, {"text": " Table 5: Linear mixed effects model for spoken  word durations in the AMI corpus, for a model in- cluding both syntactic and semantic surprisal as a  predictor as well as a random intercept and slope  for D MARY and S Semantics under speaker.", "labels": [], "entities": [{"text": "spoken  word durations", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6217311124006907}, {"text": "AMI corpus", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.8600310981273651}, {"text": "MARY", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.8068238496780396}]}, {"text": " Table 6: Linear mixed effects models for spoken word durations in the AMI corpus, for native as well as  non-native speakers of English separately. The models include both syntactic and semantic surprisal as  a fixed effect, and a random intercept and slope for D MARY and S Semantics under speaker.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.834493100643158}, {"text": "MARY", "start_pos": 265, "end_pos": 269, "type": "METRIC", "confidence": 0.7870636582374573}]}]}