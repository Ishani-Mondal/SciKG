{"title": [{"text": "Harnessing Context Incongruity for Sarcasm Detection", "labels": [], "entities": [{"text": "Harnessing Context Incongruity", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8715337514877319}, {"text": "Sarcasm Detection", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9104329347610474}]}], "abstractContent": [{"text": "The relationship between context incon-gruity and sarcasm has been studied in linguistics.", "labels": [], "entities": []}, {"text": "We present a computational system that harnesses context incongruity as a basis for sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.9574003219604492}]}, {"text": "Our statistical sarcasm classifiers incorporate two kinds of incongruity features: explicit and implicit.", "labels": [], "entities": []}, {"text": "We show the benefit of our incon-gruity features for two text forms-tweets and discussion forum posts.", "labels": [], "entities": []}, {"text": "Our system also outperforms two past works (with F-score improvement of 10-20%).", "labels": [], "entities": [{"text": "F-score improvement", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.9835779368877411}]}, {"text": "We also show how our features can capture inter-sentential incongruity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcasm is defined as 'a cutting, often ironic remark intended to express contempt or ridicule' 1 . Sarcasm detection is the task of predicting a text as sarcastic or non-sarcastic.", "labels": [], "entities": [{"text": "Sarcasm detection", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.8957268297672272}]}, {"text": "The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.)", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9518449604511261}]}, {"text": "(), (b) extraction of common patterns, such as hashtag-based sentiment), a positive verb being followed by a negative situation (, or discriminative n-grams (.", "labels": [], "entities": []}, {"text": "Thus, the past work detects sarcasm with specific indicators.", "labels": [], "entities": []}, {"text": "However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.9788290858268738}]}, {"text": "In this paper, we use one such linguistic theory: context incongruity.", "labels": [], "entities": []}, {"text": "Although the past work exploits incongruity, it does so piecemeal; we take a more well-rounded view of incongruity and place it center-stage for our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use three datasets to evaluate our system: 1.", "labels": [], "entities": []}, {"text": "Tweet-A (5208 tweets, 4170 sarcastic): We download tweets with hashtags #sar-casm and #sarcastic as sarcastic tweets and #notsarcasm and #notsarcastic as nonsarcastic, using the Twitter API (https:// dev.twitter.com/).", "labels": [], "entities": []}, {"text": "A similar hashtagbased approach to create a sarcasm-annotated dataset was employed in.", "labels": [], "entities": []}, {"text": "As an additional quality check, a rough glance through the tweets is done, and the ones found to be wrong are removed.", "labels": [], "entities": []}, {"text": "The hashtags mentioned above are removed from the text so that they act as labels but not as features.", "labels": [], "entities": []}, {"text": "To extract the implicit incongruity features, we run the iterative algorithm described in Section 4.2, on a dataset of 4000 tweets (50% sarcastic) (also created using hashtag-based supervision).", "labels": [], "entities": []}, {"text": "The algorithm results in a total of 79 verb phrases and 202 noun phrases.", "labels": [], "entities": []}, {"text": "We train our classifiers for different feature combinations, using LibSVM with RBF kernel (Chang and Lin, 2011), and report average 5-fold cross-validation values.", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.6972959935665131}]}, {"text": "shows the performance of our classifiers in terms of Precision (P), Recall (R) and F-score's two rule-based algorithms: the ordered version predicts a tweet as sarcastic if it has a positive verb phrase followed by a negative situation/noun phrase, while the unordered does so if the two are present in any order.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.9283119738101959}, {"text": "Recall (R)", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9397006779909134}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.98985755443573}]}, {"text": "We see that all statistical classifiers surpass the rule-based algorithms.", "labels": [], "entities": []}, {"text": "The best F-score obtained is 0.8876 when all four kinds of features are used.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9991809725761414}]}, {"text": "This is an improvement of about 5% over the baseline, and 40% over the algorithm by. shows that even in the case of the Discussion-A dataset, our features result in an improved performance.", "labels": [], "entities": []}, {"text": "The F-score increases from 0.568 to 0.640, an improvement of about 8% in case of discussion forum posts, when all features are used.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9990911483764648}]}, {"text": "To confirm that we indeed do better, we compare our system, with their reported values.", "labels": [], "entities": []}, {"text": "This is necessary for several reasons.", "labels": [], "entities": []}, {"text": "For example, we reimplement their algorithm but do not have Our system (all features) 0.77 0.51 0.61: Comparison of our system with two past works, for Tweet-B access to their exact extracted phrases.", "labels": [], "entities": []}, {"text": "shows that we achieve a 10% higher F-score than the best reported F-score of.", "labels": [], "entities": [{"text": "F-score", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9994605183601379}, {"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9953503608703613}]}, {"text": "This value is also 20% higher than our re-implementation of that uses their hashtag retokenizer and rulebased algorithm.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Features of our sarcasm detection system", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.9799081683158875}]}, {"text": " Table 2: Comparative results for Tweet-A using  rule-based algorithm and statistical classifiers us- ing our feature combinations", "labels": [], "entities": []}, {"text": " Table 3: Comparative results for Discussion-A us- ing our feature combinations", "labels": [], "entities": []}, {"text": " Table 4: Comparison of our system with two past  works, for Tweet-B", "labels": [], "entities": [{"text": "Tweet-B", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.6897217035293579}]}]}