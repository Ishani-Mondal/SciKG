{"title": [{"text": "Learning Cross-lingual Word Embeddings via Matrix Co-factorization", "labels": [], "entities": []}], "abstractContent": [{"text": "A joint-space model for cross-lingual distributed representations generalizes language-invariant semantic features.", "labels": [], "entities": []}, {"text": "In this paper, we present a matrix co-factorization framework for learning cross-lingual word embeddings.", "labels": [], "entities": []}, {"text": "We explicitly define monolingual training objectives in the form of matrix decomposition , and induce cross-lingual constraints for simultaneously factorizing monolingual matrices.", "labels": [], "entities": []}, {"text": "The cross-lingual constraints can be derived from parallel corpora, with or without word alignments.", "labels": [], "entities": []}, {"text": "Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.6515026191870371}]}], "introductionContent": [{"text": "Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words.", "labels": [], "entities": []}, {"text": "In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (, sentiment analysis) and word sense discrimination (.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7791444957256317}, {"text": "sentiment analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.9157863259315491}, {"text": "word sense discrimination", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.7122903267542521}]}, {"text": "Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties.", "labels": [], "entities": []}, {"text": "observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible.", "labels": [], "entities": []}, {"text": "In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to transfer models across languages.", "labels": [], "entities": []}, {"text": "This is especially important for those low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density.", "labels": [], "entities": []}, {"text": "The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer.", "labels": [], "entities": [{"text": "cross-lingual model transfer", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.697137733300527}]}, {"text": "Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora.", "labels": [], "entities": []}, {"text": "Defining a cross-lingual learning objective is crucial at the core of the joint-space model. and tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs.", "labels": [], "entities": []}, {"text": "These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments.", "labels": [], "entities": [{"text": "capturing semantic information carried by high-level units (such as phrases and beyond", "start_pos": 28, "end_pos": 114, "type": "TASK", "confidence": 0.8159243281071002}]}, {"text": "However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9986262321472168}]}, {"text": "In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities, is considered to be helpful.", "labels": [], "entities": [{"text": "word translation probabilities", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.7732599576314291}]}, {"text": "Ko\u010disk` integrated word aligning process and word embedding in machine translation models.", "labels": [], "entities": [{"text": "word aligning", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.7405405938625336}, {"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7160435914993286}]}, {"text": "This method makes full use of parallel corpora and produces high-quality word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.6953096687793732}]}, {"text": "However, it is unable to exploit the richer monolingual corpora.", "labels": [], "entities": []}, {"text": "On the other hand, and learnt word embeddings of different languages in separate spaces with monolingual corpora and projected the embeddings into a joint space, but they can only capture linear transformation.", "labels": [], "entities": []}, {"text": "In this paper, we address the above challenges with a framework of matrix co-factorization.", "labels": [], "entities": []}, {"text": "We simultaneously learn word embeddings in multiple languages via matrix factorization, with induced constraints to assure cross-lingual semantic relations.", "labels": [], "entities": []}, {"text": "It provides the flexibility of constructing learning objectives from separate monolingual and cross-lingual corpora.", "labels": [], "entities": []}, {"text": "Intricate relations across languages, rather than simple linear projections, are automatically captured.", "labels": [], "entities": []}, {"text": "Additionally, our method is efficient as it learns from global statistics.", "labels": [], "entities": []}, {"text": "The cross-lingual constraints can be derived both with or without word alignments, given that there is a valid measure of cross-lingual cooccurrences or similarities.", "labels": [], "entities": []}, {"text": "We test the performance in a task of crosslingual document classification.", "labels": [], "entities": [{"text": "crosslingual document classification", "start_pos": 37, "end_pos": 73, "type": "TASK", "confidence": 0.7048376003901163}]}, {"text": "Empirical results and a visualization of the joint semantic space demonstrate the validity of our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the quality of the relatedness between words in different languages, we induce the task of cross-lingual document classification for the English-German language pair, where a classifier is trained in one language and later used to classify documents in another.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 103, "end_pos": 140, "type": "TASK", "confidence": 0.6271871129671732}]}, {"text": "We exactly replicated the experiment settings of.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy for cross-lingual classification.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9968240261077881}, {"text": "cross-lingual classification", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.8291453421115875}]}]}