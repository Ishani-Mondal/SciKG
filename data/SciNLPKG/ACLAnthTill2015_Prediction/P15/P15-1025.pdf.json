{"title": [{"text": "Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering", "labels": [], "entities": [{"text": "Learning Continuous Word Embedding", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5734947398304939}, {"text": "Question Retrieval in Community Question Answering", "start_pos": 53, "end_pos": 103, "type": "TASK", "confidence": 0.6941076914469401}]}], "abstractContent": [{"text": "Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the web.", "labels": [], "entities": [{"text": "Community question answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7649522225062052}]}, {"text": "This paper is concerned with the problem of question retrieval.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8164154291152954}]}, {"text": "Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions.", "labels": [], "entities": [{"text": "Question retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8005554676055908}, {"text": "cQA archives", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.8929657340049744}]}, {"text": "However, the lexical gap problem brings about new challenge for question retrieval in cQA.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7707996964454651}]}, {"text": "In this paper, we propose to learn continuous word embeddings with meta-data of category information within cQA pages for question retrieval.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.8192254602909088}]}, {"text": "To deal with the variable size of word embedding vectors , we employ the framework of fisher kernel to aggregated them into the fixed-length vectors.", "labels": [], "entities": []}, {"text": "Experimental results on large-scale real world cQA data set show that our approach can significantly out-perform state-of-the-art translation models and topic-based models for question retrieval in cQA.", "labels": [], "entities": [{"text": "cQA data set", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8292569319407145}, {"text": "question retrieval", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.7507323920726776}]}], "introductionContent": [{"text": "Over the past few years, a large amount of usergenerated content have become an important information resource on the web.", "labels": [], "entities": []}, {"text": "These include the traditional Frequently Asked Questions (FAQ) archives and the emerging community question answering (cQA) services, such as Yahoo!", "labels": [], "entities": [{"text": "question answering (cQA)", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.8342900395393371}]}, {"text": "Answers 1 , Live QnA 2 , and Baidu Zhidao 3 . The content in these web sites is usually organized as questions and lists of answers associated with metadata like user chosen categories to questions and askers' awards to the best answers.", "labels": [], "entities": []}, {"text": "This data made cQA archives valuable resources for various tasks like question-answering () and knowledge mining (, etc.", "labels": [], "entities": [{"text": "knowledge mining", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7949717342853546}]}, {"text": "One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA.", "labels": [], "entities": []}, {"text": "Then the best answers of these similar questions will be used to answer the queried questions.", "labels": [], "entities": []}, {"text": "Many studies have been done along this line ().", "labels": [], "entities": []}, {"text": "One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8306271135807037}, {"text": "cQA", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8371501564979553}]}, {"text": "Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions.", "labels": [], "entities": []}, {"text": "For example shown in (), we find that fora queried question \"how do I get knots out of my cats fur?\", there are good answers under an existing question \"how can I remove a tangle in my cat's fur?\" in Yahoo!", "labels": [], "entities": []}, {"text": "Although the two questions share few words in common, they have very similar meanings, it is hard for traditional retrieval models (e.g.,) to determine their similarity.", "labels": [], "entities": []}, {"text": "This lexical gap has become a major barricade preventing traditional IR models (e.g., BM25) from retrieving similar questions in cQA.", "labels": [], "entities": [{"text": "BM25", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.8930455446243286}]}, {"text": "To address the lexical gap problem in cQA, previous work in the literature can be divided into two groups.", "labels": [], "entities": []}, {"text": "The first group is the translation models, which leverage the question-answer pairs to learn the semantically related words to improve traditional IR models ().", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9662667512893677}]}, {"text": "The basic assumption is that question-answer pairs are \"parallel texts\" and relationship of words (or phrases) can be established through word-to-word (or phrase-to-phrase) translation probabilities ().", "labels": [], "entities": []}, {"text": "Experimental results show that translation models obtain stateof-the-art performance for question retrieval in cQA.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9659856557846069}, {"text": "question retrieval", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.771696925163269}]}, {"text": "However, questions and answers are far from \"parallel\" in practice, questions and answers are highly asymmetric on the information they contain ().", "labels": [], "entities": []}, {"text": "The second group is the topic-based models, which learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answers share the same topic distribution.", "labels": [], "entities": []}, {"text": "However, questions and answers are heterogeneous in many aspects, they do not share the same topic distribution in practice.", "labels": [], "entities": []}, {"text": "Inspired by the recent success of continuous space word representations in capturing the semantic similarities in various natural language processing tasks, we propose to incorporate an embedding of words in a continuous space for question representations.", "labels": [], "entities": []}, {"text": "Due to the ability of word embeddings, we firstly transform words in a question into continuous vector representations by looking up tables.", "labels": [], "entities": []}, {"text": "These word embeddings are learned in advance using a continuous skip-gram model (, or other continuous word representation learning methods.", "labels": [], "entities": []}, {"text": "Once the words are embedded in a continuous space, one can view a question as a Bag-of-Embedded-Words (BoEW).", "labels": [], "entities": []}, {"text": "Then, the variable-cardinality BoEW will be aggregated into a fixed-length vector by using the Fisher kernel (FK) framework of.", "labels": [], "entities": [{"text": "BoEW", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.8673320412635803}]}, {"text": "Through the two steps, the proposed approach can map a question into a length invariable compact vector, which can be efficiently and effectively for large-scale question retrieval task in cQA.", "labels": [], "entities": [{"text": "question retrieval task", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.7950308322906494}]}, {"text": "We test the proposed approach on large-scale Yahoo!", "labels": [], "entities": []}, {"text": "Answers data and Baidu Zhidao data.", "labels": [], "entities": [{"text": "Answers data", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.672627404332161}, {"text": "Baidu Zhidao data", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.6945733428001404}]}, {"text": "Answers and Baidu Zhidao represent the largest and most popular cQA archives in English and Chinese, respectively.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9204654097557068}, {"text": "cQA archives", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9345438182353973}]}, {"text": "We conduct both quantitative and qualitative evaluations.", "labels": [], "entities": []}, {"text": "Experimental results show that our approach can significantly outperform state-of-the-art translation models and topic-based models for question retrieval in cQA.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7680825293064117}]}, {"text": "Our contribution in this paper are three-fold: (1) we represent a question as a bag-of-embeddedwords (BoEW) in a continuous space; (2) we introduce a novel method to aggregate the variablecardinality BoEW into a fixed-length vector by using the FK.", "labels": [], "entities": []}, {"text": "The FK is just one possible way to subsequently transform this bag representation into a fixed-length vector which is more amenable to large-scale processing; (3) an empirical verification of the efficacy of the proposed framework on large-scale English and Chinese cQA data.", "labels": [], "entities": [{"text": "FK", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9844194054603577}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes the related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our proposed framework for question retrieval.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8571234345436096}]}, {"text": "Section 4 reports the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the experiments to evaluate the performance of the proposed method for question retrieval.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8199246227741241}]}, {"text": "We collect the data sets from Yahoo!", "labels": [], "entities": []}, {"text": "Answers and Baidu Zhidao represent the largest and the most popular cQA archives in English and Chinese, respectively.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9298224449157715}, {"text": "cQA archives", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9390808939933777}]}, {"text": "More specifically, we utilized the resolved questions at Yahoo!", "labels": [], "entities": []}, {"text": "The questions include 10 million items from Yahoo!", "labels": [], "entities": []}, {"text": "Answers and 8 million items from Baidu Zhidao (also called retrieval data).", "labels": [], "entities": []}, {"text": "Each resolved question consists of three fields: \"title\", \"description\" and \"answers\", as well as some metadata, such as \"category\".", "labels": [], "entities": []}, {"text": "For question retrieval, we use only the \"title\" field and \"category\" metadata.: Statistics on the manually labeled data. is assumed that the titles of questions already provide enough semantic information for understanding users' information needs (.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7622394859790802}]}, {"text": "We develop two test sets, one for \"Yahoo data\", and the other for \"Baidu data\".", "labels": [], "entities": [{"text": "Yahoo data", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9087383449077606}, {"text": "Baidu data", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9177084863185883}]}, {"text": "In order to create the test sets, we collect some extra questions that have been posted more recently than the retrieval data, and randomly sample 1, 000 questions for Yahoo!", "labels": [], "entities": []}, {"text": "Answers and Baidu Zhidao, respectively.", "labels": [], "entities": []}, {"text": "We take those questions as queries.", "labels": [], "entities": []}, {"text": "All questions are lowercased and stemmed.", "labels": [], "entities": []}, {"text": "Stopwords 5 are also removed.", "labels": [], "entities": []}, {"text": "We separately index all data from Yahoo!", "labels": [], "entities": []}, {"text": "Answers and Baidu Zhidao using an open source Lucene with the BM25 scoring function 6 . For each query from Yahoo!", "labels": [], "entities": [{"text": "BM25 scoring function", "start_pos": 62, "end_pos": 83, "type": "METRIC", "confidence": 0.9519725640614828}]}, {"text": "Answers and Baidu Zhidao, we retrieve the several candidate questions from the corresponding indexed data by using the BM25 ranking algorithm in Lucene.", "labels": [], "entities": [{"text": "BM25", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.5846066474914551}, {"text": "Lucene", "start_pos": 145, "end_pos": 151, "type": "DATASET", "confidence": 0.5117245316505432}]}, {"text": "On average, each query from Yahoo!", "labels": [], "entities": []}, {"text": "Answers has 13 candidate questions and the average number of candidate questions for Baidu Zhidao is 8.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5313668251037598}]}, {"text": "We recruit students to label the relevance of the candidate questions regarding to the queries.", "labels": [], "entities": []}, {"text": "Specifically, for each type of language, we let three native students.", "labels": [], "entities": []}, {"text": "Given a candidate question, a student is asked to label it with \"relevant\" or \"irrelevant\".", "labels": [], "entities": []}, {"text": "If a candidate question is considered semantically similar to the query, the student will label it as \"relevant\"; otherwise, the student will label it as \"irrelevant\".", "labels": [], "entities": []}, {"text": "As a result, each candidate question gets three labels and the majority of the label is taken as the final decision fora querycandidate pair.", "labels": [], "entities": []}, {"text": "We randomly split each of the two labeled data sets into a validation set and a test set with a ration 1 : 3.", "labels": [], "entities": []}, {"text": "The validation set is used for tuning parameters of different models, while the test set is used for evaluating how well the models ranked relevant candidates in contrast to irrelevant candidates.", "labels": [], "entities": []}, {"text": "presents the manually labeled data.", "labels": [], "entities": []}, {"text": "Please note that rather than evaluate both retrieval and ranking capability of different meth-ods like the existing work, we compare them in a ranking task.", "labels": [], "entities": []}, {"text": "This may lose recall for some methods, but it can enable largescale evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9995591044425964}]}, {"text": "In order to evaluate the performance of different models, we employ Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), RPrecision (R-Prec), and Precision at K (P@5) as evaluation measures.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 68, "end_pos": 96, "type": "METRIC", "confidence": 0.9703430930773417}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 98, "end_pos": 124, "type": "METRIC", "confidence": 0.9697698553403219}, {"text": "RPrecision (R-Prec)", "start_pos": 126, "end_pos": 145, "type": "METRIC", "confidence": 0.9250331223011017}, {"text": "Precision at K (P@5)", "start_pos": 151, "end_pos": 171, "type": "METRIC", "confidence": 0.9405235648155212}]}, {"text": "These measures are widely used in the literature for question retrieval in cQA ().", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7884780168533325}]}], "tableCaptions": [{"text": " Table 1: Statistics on the manually labeled data.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  \u2020 indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  \u2021 indicates the comparisons are statistically  significant with p < 0.05.", "labels": [], "entities": [{"text": "Yahoo data", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8076145350933075}, {"text": "Baidu data", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.7861437797546387}, {"text": "question retrieval", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.7280388474464417}]}]}