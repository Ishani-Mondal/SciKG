{"title": [{"text": "Generating overspecified referring expressions: the role of discrimination", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an experiment to compare a standard, minimally distinguishing algorithm for the generation of relational referring expressions with two alternatives that produce overspecified descriptions.", "labels": [], "entities": []}, {"text": "The experiment shows that discrimination-which normally plays a major role in the disambiguation task-is also a major influence in referential overspecification, even though disambiguation is in principle not relevant.", "labels": [], "entities": []}], "introductionContent": [{"text": "In Natural Language Generation (NLG) systems, Referring Expression Generation (REG) is the computational task of providing natural language descriptions of domain entities, as in 'the second street on the left', 'the money that I found in the kitchen' etc.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8222458561261495}, {"text": "Referring Expression Generation (REG)", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.725009615222613}]}, {"text": "In this paper we will focus on the issue of content selection of relational descriptions, that is, those in which the intended target is described via another object, hereby called a landmark.", "labels": [], "entities": [{"text": "content selection of relational descriptions", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.7790123105049134}]}, {"text": "Consider the example of context in.", "labels": [], "entities": []}, {"text": "Let us consider the goal of uniquely identifying the target obj1 in the context in.", "labels": [], "entities": []}, {"text": "Since the target shares most atomic properties (e.g., type, colour and size) with other distractor objects in the context (and particularly so with respect to obj4), using a relational property (near-obj2) may help prevent ambiguity.", "labels": [], "entities": []}, {"text": "The following (a)-(c) are examples of descriptions of this kind produced from the above context.", "labels": [], "entities": []}, {"text": "As in example (a), existing REG algorithms will usually pay regard to the Gricean maxim of quantity, and avoid the inclusion of properties that are not strictly required for disambiguation.", "labels": [], "entities": [{"text": "REG", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9465542435646057}]}, {"text": "In the case of relational reference, this means that both target and landmark portions of the description maybe left underspecified, and uniqueness will follow from the fact that they mutually disambiguate each other ().", "labels": [], "entities": []}, {"text": "In other words, example (a) maybe considered felicitous even though both 'cone' and 'box' are ambiguous if interpreted independently.", "labels": [], "entities": []}, {"text": "Minimally distinguishing descriptions as in (a) are the standard output of many REG algorithms that handle relational descriptions as in.", "labels": [], "entities": []}, {"text": "Human speakers, on the other hand, are largely redundant), and will often produce socalled overspecified descriptions as in (b-c) above.", "labels": [], "entities": []}, {"text": "In this paper we will focus on the issue of generating overspecified relational descriptions as in examples (b-c), discussing which properties should be selected by a REG algorithm assuming that the decision to overspecify has already been made.", "labels": [], "entities": []}, {"text": "More specifically, we will discuss whether the algorithm should include colour as in (b), size as in (c), or other alternatives, and we will assess the impact of a referential overspecification strategy that favours highly discriminatory properties over preferences that are well-established in the literature.", "labels": [], "entities": []}, {"text": "Although this may in principle seem as a narrow research topic, the generation of relational descriptions is still subject of considerable debate in the field (e.g.,) and the issue of landmark under/full-specification has a number of known consequences for referential identification (e.g.,).", "labels": [], "entities": [{"text": "referential identification", "start_pos": 257, "end_pos": 283, "type": "TASK", "confidence": 0.8241991102695465}]}], "datasetContent": [{"text": "For evaluation purposes we will make use the Stars2 corpus of referring expressions 1 . Stars2 is an obvious choice for our experiment since these data convey visual scenes in which objects will usually have one highly discriminatory property available for reference.", "labels": [], "entities": [{"text": "Stars2 corpus", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8984282910823822}]}, {"text": "Moreover, descriptions in this domain may convey up to two relations (e.g., 'the cone next to the ball, near the cone'), which gives rise to multiple opportunities for referential overspecification.", "labels": [], "entities": []}, {"text": "In addition to this, we will also make use of the subset of relational descriptions available from the GRE3D3 () and GRE3D7 (Viethen and Dale, 2011) corpora.", "labels": [], "entities": [{"text": "GRE3D3", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.9397424459457397}, {"text": "GRE3D7 (Viethen and Dale, 2011) corpora", "start_pos": 117, "end_pos": 156, "type": "DATASET", "confidence": 0.8514477014541626}]}, {"text": "Situations of reference in the GRE3D3/7 domain are in many ways simpler than those in Stars2 (i.e., by containing at most one possible relation in each scene, by not presenting any property whose discriminatory power is substantially higher than others etc.), Some of the corpus features are described in but the comparison is still useful since GRE3D3/7 are among the very few annotated relational REG corpora made publicly available for research purposes, and which have been extensively used in previous work.", "labels": [], "entities": [{"text": "GRE3D3/7 domain", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.8710712194442749}, {"text": "GRE3D3/7", "start_pos": 346, "end_pos": 354, "type": "DATASET", "confidence": 0.805742343266805}]}, {"text": "From the three domains -Stars2, GRE3D3 and GRE3D7 -we selected all instances of relational descriptions in which the landmark object was described by making use of the type attribute and exactly one additional property p.", "labels": [], "entities": [{"text": "GRE3D3", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9052070379257202}, {"text": "GRE3D7", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8988748788833618}]}, {"text": "This amounts to three Reference sets containing 725 descriptions in total: 367 descriptions from Stars2, 114 from GRE3D3 and 244 from GRE3D7.", "labels": [], "entities": [{"text": "GRE3D3", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.9556229114532471}, {"text": "GRE3D7", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.9841147661209106}]}, {"text": "In the situations of reference available from these domains, the use of p is never necessary for disambiguation, and p will never be selected by a standard REG algorithm as the Baseline strategy described in the previous section.", "labels": [], "entities": []}, {"text": "Thus, our goal is to investigate which overspecification strategyProposal or Most Frequent, cf. previous sectionwill select the correct p, and the corresponding impact of this decision on the overall results of each algorithm.", "labels": [], "entities": []}, {"text": "From the unused portion of each corpus, we estimate attribute frequencies to create the preference list P required by the algorithms.", "labels": [], "entities": []}, {"text": "The following preference orders were obtained: P (Stars2) ={type, colour, size, near, in-front-of, right, left, below, above, behind} P (GRE3D) ={type, colour, size, above, in-front-of, hpos, vpos, near, right, left} In the case of the GRE3D3/7 corpora, we notice that not all attributes appear in both data sets.", "labels": [], "entities": [{"text": "GRE3D3/7 corpora", "start_pos": 236, "end_pos": 252, "type": "DATASET", "confidence": 0.9073746651411057}]}, {"text": "Moreover, the attributes hpos and vpos were computed from the existing pos attribute, which was originally intended to model both horizontal and vertical screen coordinates as a single property in.", "labels": [], "entities": []}, {"text": "Each of the three REG strategies -Baseline, Proposal and Most Frequent -received as an input the 725 situations of reference represented in the Reference data and the corresponding P list for each domain.", "labels": [], "entities": []}, {"text": "As a result, three sets of output descriptions were obtained, hereby called System sets.", "labels": [], "entities": []}, {"text": "Evaluation was carried out by comparing each System set to the corresponding Reference corpus descriptions and measuring Dice scores and overall accuracy (that is, the number of exact matches between each System-Reference description pair).", "labels": [], "entities": [{"text": "Reference corpus descriptions", "start_pos": 77, "end_pos": 106, "type": "DATASET", "confidence": 0.8114879329999288}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9938815832138062}]}, {"text": "shows descriptive statistics for the evaluation of our three algorithms -Baseline, Proposal and Most Frequent -applied to each corpus -Stars2, GRE3D3 and GRE3D7.", "labels": [], "entities": [{"text": "GRE3D3", "start_pos": 143, "end_pos": 149, "type": "DATASET", "confidence": 0.8915968537330627}, {"text": "GRE3D7", "start_pos": 154, "end_pos": 160, "type": "DATASET", "confidence": 0.9082111716270447}]}, {"text": "Best results are highlighted in boldface.", "labels": [], "entities": []}], "tableCaptions": []}