{"title": [{"text": "Learning Word Representations from Scarce and Noisy Data with Embedding Sub-spaces", "labels": [], "entities": [{"text": "Learning Word Representations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.591642826795578}]}], "abstractContent": [{"text": "We investigate a technique to adapt unsu-pervised word embeddings to specific applications , when only small and noisy labeled datasets are available.", "labels": [], "entities": []}, {"text": "Current methods use pre-trained embeddings to initialize model parameters, and then use the labeled data to tailor them for the intended task.", "labels": [], "entities": []}, {"text": "However, this approach is prone to overfitting when the training is performed with scarce and noisy data.", "labels": [], "entities": []}, {"text": "To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity.", "labels": [], "entities": []}, {"text": "All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset.", "labels": [], "entities": []}, {"text": "This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results.", "labels": [], "entities": [{"text": "SemEval 2015 Twitter sentiment analysis challenge", "start_pos": 39, "end_pos": 88, "type": "TASK", "confidence": 0.915722111860911}]}, {"text": "Here we show results improving those of the challenge , as well as additional experiments in a Twitter Part-Of-Speech tagging task.", "labels": [], "entities": [{"text": "Twitter Part-Of-Speech tagging task", "start_pos": 95, "end_pos": 130, "type": "TASK", "confidence": 0.65890122205019}]}], "introductionContent": [{"text": "The success of supervised systems largely depends on the amount and quality of the available training data, oftentimes, even more than the particular choice of learning algorithm ().", "labels": [], "entities": []}, {"text": "Labeled data is, however, expensive to obtain, while unlabeled data is widely available.", "labels": [], "entities": []}, {"text": "In order to exploit this fact, semi-supervised learning methods can be used.", "labels": [], "entities": []}, {"text": "In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text.", "labels": [], "entities": []}, {"text": "Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models ().", "labels": [], "entities": []}, {"text": "These models work by maximizing the probability that words within a given window size are predicted correctly.", "labels": [], "entities": []}, {"text": "The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words.", "labels": [], "entities": []}, {"text": "Using these word representations, were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features.", "labels": [], "entities": []}, {"text": "However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes.", "labels": [], "entities": []}, {"text": "Ideally, word features should be adapted to the specific supervised task.", "labels": [], "entities": []}, {"text": "One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer.", "labels": [], "entities": [{"text": "word projection layer", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.7669223646322886}]}, {"text": "Then, during training, the errors made in the predictions are backpropagated to update the embeddings, so that they better predict the supervised signal).", "labels": [], "entities": []}, {"text": "However, this strategy faces an additional challenge in noisy domains, such as social media.", "labels": [], "entities": []}, {"text": "The lexical variation caused by the typos, use of slang and abbreviations leads to a great number of singletons and out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "For these words, the embeddings will be poorly reestimated.", "labels": [], "entities": []}, {"text": "Even worse, words not present on the training set will never get their embeddings updated.", "labels": [], "entities": []}, {"text": "In this paper, we describe a strategy to adapt unsupervised word embeddings when dealing with small and noisy labeled datasets.", "labels": [], "entities": []}, {"text": "The intuition behind our approach is the following.", "labels": [], "entities": []}, {"text": "For a given task, only a subset of all the latent aspects captured by the word embeddings will be useful.", "labels": [], "entities": []}, {"text": "Therefore, instead of updating the embeddings directly with the available labeled data, we estimate a projection of these embeddings into a low dimensional sub-space.", "labels": [], "entities": []}, {"text": "This simple method brings two funda-mental advantages.", "labels": [], "entities": []}, {"text": "On the one hand, we obtain low dimensional embeddings fitting the complexity of the target task.", "labels": [], "entities": []}, {"text": "On the other hand, we are able to learn new representations for all the words, even if they do not occur in the labeled dataset.", "labels": [], "entities": []}, {"text": "To estimate the low dimensional sub-space, we propose a simple non-linear model equivalent to a neural network with one single hidden layer.", "labels": [], "entities": []}, {"text": "The model is trained in supervised fashion on the labeled dataset, learning jointly the sub-space projection and a classifier for the target task.", "labels": [], "entities": []}, {"text": "Using this model, we built a system to participate in the SemEval 2015 Twitter sentiment analysis benchmark (.", "labels": [], "entities": [{"text": "SemEval 2015 Twitter sentiment analysis", "start_pos": 58, "end_pos": 97, "type": "TASK", "confidence": 0.8388595581054688}]}, {"text": "Our submission attained state-of-the-art results without hand-coded features or linguistic resources (.", "labels": [], "entities": []}, {"text": "Here, we further investigate this approach and compare it against several state-of-the-art systems for Twitter sentiment classification.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 103, "end_pos": 135, "type": "TASK", "confidence": 0.693278948465983}]}, {"text": "We also report on additional experiments to assess the adequacy of this strategy in other natural language problems.", "labels": [], "entities": []}, {"text": "To this end, we apply the embedding sub-space layer to  deep learning model for part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7664667665958405}]}, {"text": "Even though the gains were not as significant as in the sentiment polarity prediction task, the results suggest that our method is indeed generalizable to other problems.", "labels": [], "entities": [{"text": "sentiment polarity prediction task", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.9010185599327087}]}, {"text": "The rest of the paper is organized as follows: the related work is reviewed in Section 2.", "labels": [], "entities": []}, {"text": "Section 3, briefly describes the model used to pre-train the word embeddings.", "labels": [], "entities": []}, {"text": "In Section 4, we introduce the concept of embedding sub-space, as well as the the non-linear sub-space model for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.816174179315567}]}, {"text": "Section 5, details the experiments performed with the SemEval corpora.", "labels": [], "entities": [{"text": "SemEval corpora", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.7319129705429077}]}, {"text": "Section 6 describes additional experiments applying the embedding subspace method to a Part-of-Speech tagging model for Twitter data.", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6820525974035263}]}, {"text": "Finally, Section 7 draws the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first step of our approach requires a corpus of raw text for the unsupervised pre-training of the embedding matrix E.", "labels": [], "entities": []}, {"text": "We resorted to the corpus of 52 million tweets used in ( and the tokenizer described in the same work.", "labels": [], "entities": []}, {"text": "The messages were previously pre-processed as follows: lower-casing, replacing Twitter user mentions and URLs with special tokens and reducing any character repetition to at most 3 characters.", "labels": [], "entities": []}, {"text": "Words occurring less than 40 times in the corpus were discarded, resulting in a vocabulary of around 210,000 types.", "labels": [], "entities": []}, {"text": "Then, a modified version of the word2vec tool 1 was used to compute the word embeddings, as described in Section 3.", "labels": [], "entities": []}, {"text": "The window size and negative sampling rate were set to 5 and 25 words, respectively, and embeddings of 50, 200, 400 and 600 dimensions were built.", "labels": [], "entities": []}, {"text": "Our system accepts as input a sentence represented as a matrix, obtained by concatenating the one-hot vectors that represent each individual word.", "labels": [], "entities": []}, {"text": "Therefore, we first performed the aforementioned normalization and tokenization steps and then, converted each tweet into this representation.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.8728326559066772}]}, {"text": "The development set was split into 80% for parameter learning and 20% for model evaluation and selection, maintaining the original relative class proportions in each set.", "labels": [], "entities": []}, {"text": "All the weights were initialized uniformly at random, as proposed in.", "labels": [], "entities": []}, {"text": "The model was trained with conventional Stochastic Gradient Descent ( with a fixed learning rate of 0.01, and the weights were updated after each message was processed.", "labels": [], "entities": []}, {"text": "Variations of learning rate to smaller values, e.g. 0.005, were considered but did not lead to a clear pattern.", "labels": [], "entities": []}, {"text": "We explored different configurations of the hyperparameters e (embedding size) and s (sub-space size).", "labels": [], "entities": []}, {"text": "Model selection was done by early stopping, i.e., we kept the configuration with best F-measure on the evaluation set after 5-8 iterations.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9947649240493774}]}, {"text": "Tests were performed in Gimpel et al.", "labels": [], "entities": []}, {"text": "(2011) Twitter POS dataset, which uses the universal POS tag set composed by 25 different labels ().", "labels": [], "entities": [{"text": "Twitter POS dataset", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.8819766044616699}]}, {"text": "The dataset contains 1000 annotated tweets for training, 327 tweets for tuning and 500 tweets for testing.", "labels": [], "entities": []}, {"text": "The number of word tokens in these sets are 15000, 5000 and 7000, respectively.", "labels": [], "entities": []}, {"text": "There are 5000, 2000 and 3000 word types.", "labels": [], "entities": []}, {"text": "Once again, we initialized the embeddings with unsupervised pre-training using the structured skip-gram approach.", "labels": [], "entities": []}, {"text": "As for the hyperparameters of the model, we used embeddings withe = 50 dimensions, a hidden layer with h = 200 dimensions and a context of p = 2 as used in ( . Training employed mini-batch gradient descent, with mini batches of 100 sentences and a momentum of 0.95.", "labels": [], "entities": []}, {"text": "The learning rate was set to 0.2.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9443425536155701}]}, {"text": "Finally, we used early stopping by choosing the epoch with the highest accuracy in the tuning set.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.6776842772960663}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9970111846923828}]}, {"text": "As for the sub-space layer size, we tried three different hyperparameterizations: 10, 30 and 50 dimensions.", "labels": [], "entities": []}, {"text": "Using the setup that led to the best results in the sentiment prediction task (FIX), that is, fixing E and updating S, leads to lower accuracies than the baseline (TRAIN-ALL, s = 0).", "labels": [], "entities": [{"text": "sentiment prediction task", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.9201835592587789}, {"text": "FIX)", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8110471367835999}, {"text": "accuracies", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9758151769638062}, {"text": "TRAIN-ALL", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9959160685539246}]}, {"text": "We also see that different values of s do not have a very strong impact in the final results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of examples per class in each  SemEval dataset. The first row shows the training  data; the other rows are sets used for testing.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.7982888519763947}]}]}