{"title": [{"text": "Compositional Semantic Parsing on Semi-Structured Tables", "labels": [], "entities": [{"text": "Compositional Semantic Parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7246668140093485}]}], "abstractContent": [{"text": "Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7400037050247192}, {"text": "question answering", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8089065253734589}, {"text": "breadth", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9872636198997498}]}, {"text": "While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through anew task: answering complex questions on semi-structured tables using question-answer pairs as supervision.", "labels": [], "entities": [{"text": "answering complex questions on semi-structured tables", "start_pos": 130, "end_pos": 183, "type": "TASK", "confidence": 0.8313882152239481}]}, {"text": "The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations , and the deeper compositionality results in a combinatorial explosion in the space of logical forms.", "labels": [], "entities": []}, {"text": "We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines.", "labels": [], "entities": []}, {"text": "For evaluation , we created anew dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7607538998126984}, {"text": "question answering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7296039164066315}]}, {"text": "Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7522000372409821}]}, {"text": "More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase).", "labels": [], "entities": []}, {"text": "However, even these broader knowledge sources still define a x1: \"Greece held its last Summer Olympics in which year?\"", "labels": [], "entities": []}, {"text": "y1: {2004} x2: \"In which city's the first time with at least 20 nations?\"", "labels": [], "entities": []}, {"text": "y2: {Paris} x3: \"Which years have the most participating countries?\"", "labels": [], "entities": []}, {"text": "y3: {2008, 2012} x4: \"How many events were in Athens, Greece?\"", "labels": [], "entities": []}, {"text": "y4: {2} x5: \"How many more participants were therein 1900 than in the first year?\"", "labels": [], "entities": []}, {"text": "y5: {10}: Our task is to answer a highly compositional question from an HTML table.", "labels": [], "entities": []}, {"text": "We learn a semantic parser from question-table-answer triples {(x i , ti , y i )}.", "labels": [], "entities": []}, {"text": "rigid schema over entities and relation types, thus restricting the scope of answerable questions.", "labels": [], "entities": []}, {"text": "To simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose anew task (with an associated dataset): answering a question using an HTML table as the knowledge source.", "labels": [], "entities": []}, {"text": "shows several question-answer pairs and an accompanying table, which are typical of those in our dataset.", "labels": [], "entities": []}, {"text": "Note that the questions are logically quite complex, involving a variety of operations such as comparison (x 2 ), superlatives (x 3 ), aggregation (x 4 ), and arithmetic (x 5 ).", "labels": [], "entities": []}, {"text": "The HTML tables are semi-structured and not normalized.", "labels": [], "entities": []}, {"text": "For example, a cell might contain multiple parts (e.g., \"Beijing, China\" or \"200 km\").", "labels": [], "entities": []}, {"text": "Additionally, we mandate that the training and test tables are disjoint, so attest time, we will see relations (column headers; e.g., \"Nations\") and entities (table cells; e.g., \"St. Louis\") that were not observed during training.", "labels": [], "entities": []}, {"text": "This is in contrast to knowledge bases like Freebase, which have a global fixed relation schema with normalized entities and relations.", "labels": [], "entities": []}, {"text": "Our task setting produces two main challenges.", "labels": [], "entities": []}, {"text": "Firstly, the increased breadth in the knowledge source requires us to generate logical forms from novel tables with previously unseen relations and entities.", "labels": [], "entities": []}, {"text": "We therefore cannot follow the typical semantic parsing strategy of constructing or learning a lexicon that maps phrases to relations ahead of time.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7572945654392242}]}, {"text": "Secondly, the increased depth in compositionality and additional logical operations exacerbate the exponential growth of the number of possible logical forms.", "labels": [], "entities": []}, {"text": "We trained a semantic parser for this task from question-answer pairs based on the framework illustrated in.", "labels": [], "entities": []}, {"text": "First, relations and entities from the semi-structured HTML table are encoded in a graph.", "labels": [], "entities": []}, {"text": "Then, the system parses the question into candidate logical forms with a high-coverage grammar, reranks the candidates with a log-linear model, and then executes the highest-scoring logical form to produce the answer denotation.", "labels": [], "entities": []}, {"text": "We use beam search with pruning strategies based on type and denotation constraints to control the combinatorial explosion.", "labels": [], "entities": [{"text": "beam search", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.8688046932220459}]}, {"text": "To evaluate the system, we created anew dataset, WIKITABLEQUESTIONS, consisting of 2,108 HTML tables from Wikipedia and 22,033 question-answer pairs.", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.5216609239578247}]}, {"text": "When tested on unseen tables, the system achieves an accuracy of 37.1%, which is significantly higher than the information retrieval baseline of 12.7% and a simple semantic parsing baseline of 24.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.999708354473114}, {"text": "semantic parsing", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.6970123797655106}]}], "datasetContent": [{"text": "We evaluate the system on the development sets (three random 80:20 splits of the training data) and the test data.", "labels": [], "entities": []}, {"text": "In both settings, the tables we test on do not appear during training.", "labels": [], "entities": []}, {"text": "Our main metric is accuracy, which is the number of examples (x, t, y) on which the system outputs the correct answer y.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.99960857629776}]}, {"text": "We also report the oracle score, which counts the number of examples whereat least one generated candidate z \u2208 Z x executes toy.", "labels": [], "entities": []}, {"text": "We compare the system to two baselines.", "labels": [], "entities": []}, {"text": "The first baseline (IR), which simulates information retrieval, selects an answer y among the entities in the table using a log-linear model over entities (table cells) rather than logical forms.", "labels": [], "entities": []}, {"text": "The features are conjunctions between phrases in x and properties of the answers y, which coverall features in our main system that do not involve the logical form.", "labels": [], "entities": []}, {"text": "As an upper bound of this baseline,: Average accuracy and oracle scores on development data in various system settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9978535771369934}]}, {"text": "69.1% of the development examples have the answer appearing as an entity in the table.", "labels": [], "entities": []}, {"text": "In the second baseline (WQ), we only allow deduction rules that produce join and count logical forms.", "labels": [], "entities": []}, {"text": "This rule subset has the same logical coverage as, which is designed to handle the WEBQUESTIONS () and FREE917 datasets.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.7586829662322998}, {"text": "FREE917 datasets", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.9433799386024475}]}, {"text": "shows the results compared to the baselines.", "labels": [], "entities": []}, {"text": "Our system gets an accuracy of 37.1% on the test data, which is significantly higher than both baselines, while the oracle is 76.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997039437294006}]}, {"text": "The next subsections analyze the system components in more detail.", "labels": [], "entities": []}, {"text": "In this section, we analyze the breadth and depth of the WIKITABLEQUESTIONS dataset, and how the system handles them.", "labels": [], "entities": [{"text": "breadth", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9909197092056274}, {"text": "WIKITABLEQUESTIONS dataset", "start_pos": 57, "end_pos": 83, "type": "DATASET", "confidence": 0.930491030216217}]}, {"text": "With 3,929 unique column headers (relations) among 13,396 columns, the tables in the WIKITABLEQUESTIONS dataset contain many more relations than closed-domain datasets such as Geoquery (Zelle and Mooney,", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS dataset", "start_pos": 85, "end_pos": 111, "type": "DATASET", "confidence": 0.9569077789783478}]}], "tableCaptions": [{"text": " Table 5: Accuracy (acc) and oracle scores (ora)  on the development sets (3 random splits of the  training data) and the test data.", "labels": [], "entities": [{"text": "Accuracy (acc)", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9166912287473679}]}, {"text": " Table 6: Average accuracy and oracle scores on  development data in various system settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9615209698677063}]}]}