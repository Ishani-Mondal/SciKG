{"title": [{"text": "Low-Rank Tensors for Verbs in Compositional Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "Several compositional distributional semantic methods use tensors to model multi-way interactions between vectors.", "labels": [], "entities": []}, {"text": "Unfortunately, the size of the tensors can make their use impractical in large-scale implementations.", "labels": [], "entities": []}, {"text": "In this paper, we investigate whether we can match the performance of full tensors with low-rank approximations that use a fraction of the original number of parameters.", "labels": [], "entities": []}, {"text": "We investigate the effect of low-rank tensors on the transitive verb construction where the verb is a third-order tensor.", "labels": [], "entities": []}, {"text": "The results show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disam-biguation tasks.", "labels": [], "entities": [{"text": "verb disam-biguation tasks", "start_pos": 242, "end_pos": 268, "type": "TASK", "confidence": 0.7529217004776001}]}], "introductionContent": [{"text": "Distributional semantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics or by learning vector representations for words as part of a context prediction model (.", "labels": [], "entities": []}, {"text": "Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences.", "labels": [], "entities": [{"text": "compositional distributional semantics (CDS)", "start_pos": 36, "end_pos": 80, "type": "TASK", "confidence": 0.6952158510684967}]}, {"text": "One method for CDS is the Categorial framework, where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word.", "labels": [], "entities": []}, {"text": "For example, nouns are anatomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (.", "labels": [], "entities": []}, {"text": "A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space ).", "labels": [], "entities": []}, {"text": "However, a concrete implementation of the Categorial framework requires setting and storing the values, or parameters, defining these matrices and tensors.", "labels": [], "entities": []}, {"text": "These parameters can be quite numerous for even low-dimensional sentence spaces.", "labels": [], "entities": []}, {"text": "For example, a third-order tensor fora given transitive verb, mapping two 100-dimensional noun spaces to a 100-dimensional sentence space, would have 100 3 parameters in its full form.", "labels": [], "entities": []}, {"text": "All of the more complex types have corresponding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of parameters required to represent an extended vocabulary and a variety of grammatical constructions.", "labels": [], "entities": []}, {"text": "We aim to reduce the size of the models by demonstrating that reduced-rank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the full-rank tensors do.", "labels": [], "entities": []}, {"text": "We base our experiments on the transitive verb construction for which there are established tasks and datasets (.", "labels": [], "entities": []}, {"text": "Previous work on the transitive verb construction within the Categorial framework includes a two-step linear-regression method for the construction of the full verb tensors () and a multi-linear regression method combined with a two-dimensional plausibility space ( ).", "labels": [], "entities": []}, {"text": "also introduce several alternative ways of reducing the number of tensor parameters by using matrices.", "labels": [], "entities": []}, {"text": "The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions.", "labels": [], "entities": []}, {"text": "Some interaction between the subject and the object is re-introduced through a softmax layer.", "labels": [], "entities": []}, {"text": "A similar method is presented in. use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation.", "labels": [], "entities": []}, {"text": "In this paper, we use tensor rank decomposition ( to represent each verb's tensor as a sum of tensor products of vectors.", "labels": [], "entities": [{"text": "tensor rank decomposition", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6258733371893564}]}, {"text": "We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency.", "labels": [], "entities": []}, {"text": "This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing () and semantic role labelling (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8466587662696838}, {"text": "semantic role labelling", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7136306166648865}]}, {"text": "Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS.", "labels": [], "entities": []}, {"text": "Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010),, and.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the performance of the low-rank tensors against full tensors on two tasks.", "labels": [], "entities": []}, {"text": "Both tasks require the model to rank pairs of sentences each consisting of a subject, transitive verb, and object by the semantic similarity of the sentences in the pair.", "labels": [], "entities": []}, {"text": "The gold standard ranking is given by similarity scores provided by human evaluators and the scores are not averaged among the annotators.", "labels": [], "entities": []}, {"text": "The model ranking is evaluated against the ranking from the gold standard similarity judgements using Spearman's \u03c1.", "labels": [], "entities": []}, {"text": "The verb disambiguation task (GS11) (Grefenstette and Sadrzadeh, 2011) involves distinguishing between senses of an ambiguous verb, given subject and object nouns as context.", "labels": [], "entities": [{"text": "verb disambiguation task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7694817185401917}]}, {"text": "The dataset consists of 200 sentence pairs, where the two sentences in each pair have the same subject and object but differ in the verb.", "labels": [], "entities": []}, {"text": "Each of these pairs was ranked by human evaluators on a 1-7 similarity scale so that properly disambiguated pairs (e.g. author write book -author publish book) have higher similarity scores than improperly disambiguated pairs (e.g. author write book -author spell book).", "labels": [], "entities": []}, {"text": "The transitive sentence similarity dataset consists of 72 subjectverb-object sentences arranged into 108 sentence pairs.", "labels": [], "entities": []}, {"text": "As in GS11, each pair has a gold standard semantic similarity score on a 1-7 scale.", "labels": [], "entities": [{"text": "GS11", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.8695690035820007}]}, {"text": "For example, the pair medication achieve result -drug produce effect has a high similarity rating, while author write book -delegate buy land has a low rating.", "labels": [], "entities": [{"text": "similarity", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9929723739624023}]}, {"text": "In this dataset, however, the two sentences in each pair have no lexical overlap: neither subjects, objects, nor verbs are shared.: Model performance on the verb disambiguation (GS11) and sentence similarity (KS14) tasks, given by Spearman's \u03c1, and the number of parameters needed to represent each verb's tensor.", "labels": [], "entities": []}, {"text": "We show the highest tensor result for each task and vector set in bold (and also bold the baseline when it outperforms the tensor method).", "labels": [], "entities": []}, {"text": "displays correlations between the systems' scores and human SVO similarity judgements on the verb disambiguation (GS11) and sentence similarity (KS14) tasks, for both the count (SVD) and prediction vectors (PV).", "labels": [], "entities": []}, {"text": "We also give results for simple composition of word vectors using elementwise addition and multiplication) (using verb vectors produced in the same manner as for nouns).", "labels": [], "entities": []}, {"text": "As is consistent with prior work, the tensor-based models are surpassed by vector addition on the KS14 dataset (), but perform better than both addition and multiplication on the GS11 dataset.", "labels": [], "entities": [{"text": "KS14 dataset", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9754699170589447}, {"text": "GS11 dataset", "start_pos": 179, "end_pos": 191, "type": "DATASET", "confidence": 0.9892549514770508}]}, {"text": "Unsurprisingly, the rank-1 tensor has lowest performance for both tasks and vector sets, and performance generally increases as we increase the maximal rank R.", "labels": [], "entities": []}, {"text": "The full tensor achieves the best, or tied for the best, performance on both tasks when using the PV vectors.", "labels": [], "entities": []}, {"text": "However, for the SVD vectors, low-rank tensors surpass the performance of the full-rank tensor for R=40 and R=50 on GS11, and R=50 on KS14.", "labels": [], "entities": [{"text": "GS11", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.9766278266906738}, {"text": "KS14", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.92401123046875}]}], "tableCaptions": [{"text": " Table 1: Model performance on the verb disam- biguation (GS11) and sentence similarity (KS14)  tasks, given by Spearman's \u03c1, and the number of  parameters needed to represent each verb's tensor.  We show the highest tensor result for each task and  vector set in bold (and also bold the baseline when  it outperforms the tensor method).", "labels": [], "entities": [{"text": "verb disam- biguation (GS11)", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7271181174686977}]}]}