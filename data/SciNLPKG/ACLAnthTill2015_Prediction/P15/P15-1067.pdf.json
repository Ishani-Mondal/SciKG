{"title": [{"text": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "labels": [], "entities": []}], "abstractContent": [{"text": "Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness.", "labels": [], "entities": []}, {"text": "Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "In this paper , we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR.", "labels": [], "entities": []}, {"text": "In TransD, we use two vectors to represent a named symbol object (entity and relation).", "labels": [], "entities": []}, {"text": "The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically.", "labels": [], "entities": []}, {"text": "Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities.", "labels": [], "entities": []}, {"text": "TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs.", "labels": [], "entities": []}, {"text": "In Experiments , we evaluate our model on two typical tasks including triplets classification and link prediction.", "labels": [], "entities": [{"text": "triplets classification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.769358217716217}, {"text": "link prediction", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.8108382523059845}]}, {"text": "Evaluation results show that our approach outperforms state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge Graphs such as WordNet,) and) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&A), etc.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9486424326896667}, {"text": "relation extraction(RE)", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.8757880091667175}, {"text": "question answering(Q&A)", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.9063719596181598}]}, {"text": "They usually contain huge amounts of structured data as the form of triplets (head entity, relation, tail entity)(denoted as (h, r, t)), where relation models the relationship between the two entities.", "labels": [], "entities": []}, {"text": "As most knowledge graphs have been built either collaboratively or (partly) automatically, they often suffer from incompleteness.", "labels": [], "entities": []}, {"text": "Knowledge graph completion is to predict relations between entities based on existing triplets in a knowledge graph.", "labels": [], "entities": [{"text": "Knowledge graph completion", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6469241778055826}]}, {"text": "In the past decade, much work based on symbol and logic has been done for knowledge graph completion, but they are neither tractable nor enough convergence for large scale knowledge graphs.", "labels": [], "entities": [{"text": "knowledge graph completion", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.7428762316703796}]}, {"text": "Recently, a powerful approach for this task is to encode every element (entities and relations) of a knowledge graph into a low-dimensional embedding vector space.", "labels": [], "entities": []}, {"text": "These methods do reasoning over knowledge graphs through algebraic operations (see section \"Related Work\").", "labels": [], "entities": []}, {"text": "Among these methods,) is simple and effective, and also achieves state-of-the-art prediction performance.", "labels": [], "entities": []}, {"text": "It learns low-dimensional embeddings for every entity and relation in knowledge graphs.", "labels": [], "entities": []}, {"text": "These vector embeddings are denoted by the same letter in boldface.", "labels": [], "entities": []}, {"text": "The basic idea is that every relation is regarded as translation in the embedding space.", "labels": [], "entities": []}, {"text": "For a golden triplet (h, r, t), the embedding h is close to the embedding t by adding the embedding r, that is h + r \u2248 t.", "labels": [], "entities": []}, {"text": "TransE is suitable for 1-to-1 relations, but has flaws when dealing with 1-to-N, N-to-1 and N-to-N relations.", "labels": [], "entities": []}, {"text": "TransH () is proposed to solve these issues.", "labels": [], "entities": [{"text": "TransH", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8016061782836914}]}, {"text": "TransH regards a relation as a translating operation on a relation-specific hyperplane, which is characterized by a norm vector w rand a translation vector d r . The embeddings hand tare first projected to the hyperplane of relation r to obtain vectors h \u22a5 = h \u2212 w r hw rand t \u22a5 = t \u2212 w r tw r , and then h \u22a5 + d r \u2248 t \u22a5 . Both in TransE and TransH, the embeddings of entities and relations are in the same space.", "labels": [], "entities": [{"text": "TransH", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8288168907165527}]}, {"text": "However, entities and relations are different types objects, it is insufficient to model them in the same space.", "labels": [], "entities": []}, {"text": "TransR/CTransR () set a mapping matrix Mr and a vector r for every relation r.", "labels": [], "entities": []}, {"text": "In TransR, hand tare projected to the aspects that relation r focuses on through the ma- Each shape represents an entity pair appearing in a triplet of relation r.", "labels": [], "entities": []}, {"text": "M rh and M rt are mapping matrices of hand t, respectively.", "labels": [], "entities": []}, {"text": "hip , tip (i = 1, 2, 3), and r pare projection vectors.", "labels": [], "entities": []}, {"text": "h i\u22a5 and t i\u22a5 (i = 1, 2, 3) are projected vectors of entities.", "labels": [], "entities": []}, {"text": "The projected vectors satisfy h i\u22a5 + r \u2248 t i\u22a5 (i = 1, 2, 3).", "labels": [], "entities": []}, {"text": "trix Mr and then Mr h + r \u2248 Mr t.", "labels": [], "entities": []}, {"text": "CTransR is an extension of TransR by clustering diverse headtail entity pairs into groups and learning distinct relation vectors for each group.", "labels": [], "entities": [{"text": "CTransR", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8428041934967041}]}, {"text": "TransR/CTransR has significant improvements compared with previous state-of-the-art models.", "labels": [], "entities": [{"text": "TransR/CTransR", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.6594091256459554}]}, {"text": "However, it also has several flaws: (1) For atypical relation r, all entities share the same mapping matrix Mr . However, the entities linked by a relation always contains various types and attributes.", "labels": [], "entities": []}, {"text": "For example, in triplet (friedrich burklein, nationality, germany), friedrich burklein and germany are typical different types of entities.", "labels": [], "entities": []}, {"text": "These entities should be projected in different ways; (2) The projection operation is an interactive process between an entity and a relation, it is unreasonable that the mapping matrices are determined only by relations; and (3) Matrix-vector multiplication makes it has large amount of calculation, and when relation number is large, it also has much more parameters than TransE and TransH.", "labels": [], "entities": [{"text": "TransH", "start_pos": 385, "end_pos": 391, "type": "DATASET", "confidence": 0.8924111723899841}]}, {"text": "As the complexity, TransR/CTransR is difficult to apply on largescale knowledge graphs.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method named TransD to model knowledge graphs.", "labels": [], "entities": []}, {"text": "shows the basic idea of TransD.", "labels": [], "entities": []}, {"text": "In TransD, we define two vectors for each entity and relation.", "labels": [], "entities": []}, {"text": "The first vector represents the meaning of an entity or a relation, the other one (called projection vector) represents the way that how to project a entity embedding into a relation vector space and it will be used to construct mapping matrices.", "labels": [], "entities": []}, {"text": "Therefore, every entity-relation pair has an unique mapping matrix.", "labels": [], "entities": []}, {"text": "In addition, TransD has no matrixby-vector operations which can be replaced by vectors operations.", "labels": [], "entities": []}, {"text": "We evaluate TransD with the task of triplets classification and link prediction.", "labels": [], "entities": [{"text": "triplets classification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.795319676399231}, {"text": "link prediction", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7653517425060272}]}, {"text": "The experimental results show that our method has significant improvements compared with previous models.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are: (1)We propose a novel model TransD, which constructs a dynamic mapping matrix for each entity-relation pair by considering the diversity of entities and relations simultaneously.", "labels": [], "entities": []}, {"text": "It provides a flexible style to project entity representations to relation vector space; (2) Compared with TransR/CTransR, TransD has fewer parameters and has no matrixvector multiplication.", "labels": [], "entities": []}, {"text": "It is easy to be applied on large-scale knowledge graphs like In experiments, our approach outperforms previous models including TransE, TransH and TransR/CTransR in link prediction and triplets classification tasks.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 166, "end_pos": 181, "type": "TASK", "confidence": 0.7743237912654877}, {"text": "triplets classification", "start_pos": 186, "end_pos": 209, "type": "TASK", "confidence": 0.6478896290063858}]}], "datasetContent": [{"text": "We evaluate our apporach on two tasks: triplets classification and link prediction.", "labels": [], "entities": [{"text": "triplets classification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7844119369983673}, {"text": "link prediction", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7853523790836334}]}, {"text": "Then we show the experiments results and some analysis of them.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Datesets used in the experiments.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results of Triplets Classifi- cation(%). \"+E\" means that the results are com- bined with word embedding.", "labels": [], "entities": []}, {"text": " Table 4: Experimental results on link prediction.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8744701147079468}]}, {"text": " Table 5: Experimental results on FB15K by mapping properities of relations (%).", "labels": [], "entities": [{"text": "FB15K", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8237504363059998}]}, {"text": " Table 6: Entity projection vectors similarity (in descending order) computed on WN18. The similarity  scores are computed with cosine function.", "labels": [], "entities": [{"text": "similarity", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9591540694236755}, {"text": "WN18", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9867051243782043}]}, {"text": " Table 7: Relation projection vectors similarity computed on FB15k. The similarity scores are computed  with cosine function.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.9732096791267395}, {"text": "similarity", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.961439311504364}]}]}