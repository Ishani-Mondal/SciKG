{"title": [{"text": "Sentiment-Aspect Extraction based on Restricted Boltzmann Machines", "labels": [], "entities": [{"text": "Sentiment-Aspect Extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9853143095970154}]}], "abstractContent": [{"text": "Aspect extraction and sentiment analysis of reviews are both important tasks in opinion mining.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9695343673229218}, {"text": "sentiment analysis of reviews", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.9145448058843613}, {"text": "opinion mining", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.8376860916614532}]}, {"text": "We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting.", "labels": [], "entities": [{"text": "sentiment and aspect extraction", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7497949302196503}]}, {"text": "This model reflects the generation process of reviews by introducing a heterogeneous structure into the hidden layer and incorporating informative priors.", "labels": [], "entities": []}, {"text": "Experiments show that our model outper-forms previous state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays, it is commonplace for people to express their opinion about various sorts of entities, e.g., products or services, on the Internet, especially in the course of e-commerce activities.", "labels": [], "entities": []}, {"text": "Analyzing online reviews not only helps customers obtain useful product information, but also provide companies with feedback to enhance their products or service quality.", "labels": [], "entities": []}, {"text": "Aspect-based opinion mining enables people to consider much more finegrained analyses of vast quantities of online reviews, perhaps from numerous different merchant sites.", "labels": [], "entities": [{"text": "Aspect-based opinion mining", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6341442664464315}]}, {"text": "Thus, automatic identification of aspects of entities and relevant sentiment polarities in Big Data is a significant and urgent task.", "labels": [], "entities": [{"text": "automatic identification of aspects of entities and relevant sentiment polarities", "start_pos": 6, "end_pos": 87, "type": "TASK", "confidence": 0.6265069901943207}, {"text": "Big Data", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.6399528384208679}]}, {"text": "Identifying aspect and analyzing sentiment words from reviews has the ultimate goal of discerning people's opinions, attitudes, emotions, etc.", "labels": [], "entities": []}, {"text": "towards entities such as products, services, organizations, individuals, events, etc.", "labels": [], "entities": []}, {"text": "In this context, aspect-based opinion mining, also known as feature-based opinion mining, aims at extracting and summarizing particular salient aspects of entities and determining relevant sentiment polarities * Corresponding Author: Kang Liu (kliu@nlpr.ia.ac.cn) from reviews ().", "labels": [], "entities": [{"text": "aspect-based opinion mining", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.6424668331940969}, {"text": "feature-based opinion mining", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6768962939580282}, {"text": "summarizing particular salient aspects of entities", "start_pos": 113, "end_pos": 163, "type": "TASK", "confidence": 0.7758979399998983}]}, {"text": "Consider reviews of computers, for example.", "labels": [], "entities": []}, {"text": "A given computer's components (e.g., hard disk, screen) and attributes (e.g., volume, size) are viewed as aspects to be extracted from the reviews, while sentiment polarity classification consists in judging whether an opinionated review expresses an overall positive or negative opinion.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 154, "end_pos": 187, "type": "TASK", "confidence": 0.7032714188098907}]}, {"text": "Regarding aspect identification, previous methods can be divided into three main categories: rule-based, supervised, and topic model-based methods.", "labels": [], "entities": [{"text": "aspect identification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9075644612312317}]}, {"text": "For instance, association rule-based methods ( tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level.", "labels": [], "entities": []}, {"text": "Existing rule-based methods typically are notable to group the extracted aspect terms into categories.", "labels": [], "entities": []}, {"text": "Supervised ( and semisupervised learning methods were introduced to resolve certain aspect identification problems.", "labels": [], "entities": [{"text": "aspect identification", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7334914654493332}]}, {"text": "However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios.", "labels": [], "entities": []}, {"text": "Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data.", "labels": [], "entities": []}, {"text": "Latent Dirichlet Allocation, or LDA for short, () performs well in automatically extracting aspects and grouping corresponding representative words into categories.", "labels": [], "entities": []}, {"text": "Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (.", "labels": [], "entities": [{"text": "LDA-based aspect identification", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.8015862305959066}]}, {"text": "Still, these methods have several important drawbacks.", "labels": [], "entities": []}, {"text": "First, inaccurate approximations of the distribution over topics may reduce the computational accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9100747108459473}]}, {"text": "Second, mixture models are unable to exploit the co-occurrence of topics to yield high probability predictions for words that are sharper than the distributions predicted by in-dividual topics.", "labels": [], "entities": []}, {"text": "To overcome the weaknesses of existing methods and pursue the promising direction of jointly learning aspect and sentiment, we present the novel Sentiment-Aspect Extraction RBM (SERBM) model to simultaneously extract aspects of entities and relevant sentiment-bearing words.", "labels": [], "entities": [{"text": "Sentiment-Aspect Extraction RBM (SERBM)", "start_pos": 145, "end_pos": 184, "type": "TASK", "confidence": 0.7851978540420532}]}, {"text": "This two-layer structure model is inspired by conventional Restricted Boltzmann machines (RBMs).", "labels": [], "entities": [{"text": "Restricted Boltzmann machines (RBMs)", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.501330628991127}]}, {"text": "In previous work, RBMs with shared parameters (RSMs) have achieved great success in capturing distributed semantic representations from text.", "labels": [], "entities": []}, {"text": "Aiming to make the most of their ability to model latent topics while also accounting for the structured nature of aspect opinion mining, we propose replacing the standard hidden layers of RBMs with a novel heterogeneous structure.", "labels": [], "entities": [{"text": "aspect opinion mining", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.768336832523346}]}, {"text": "Three different types of hidden units are used to represent aspects, sentiments, and background words, respectively.", "labels": [], "entities": []}, {"text": "This modification better reflects the generative process for reviews, in which review words are generated not only from the aspect distribution but also affected by sentiment information.", "labels": [], "entities": []}, {"text": "Furthermore, we blend background knowledge into this model using priors and regularization to help it acquire more accurate feature representations.", "labels": [], "entities": []}, {"text": "After m-step Contrastive Divergence for parameter estimation, we can capture the required data distribution and easily compute the posterior distribution over latent aspects and sentiments from reviews.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7023776769638062}]}, {"text": "In this way, aspects and sentiments are jointly extracted from reviews, with limited computational effort.", "labels": [], "entities": []}, {"text": "This model is hence a promising alternative to more complex LDAbased models presented previously.", "labels": [], "entities": []}, {"text": "Overall, our main contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "Compared with previous LDA-based methods, our model avoids inaccurate approximations and captures latent aspects and sentiment both adequately and efficiently.", "labels": [], "entities": []}, {"text": "2. Our model exploits RBMs' advantage in properly modeling distributed semantic representations from text, but also introduces heterogeneous structure into the hidden layer to reflect the generative process for online reviews.", "labels": [], "entities": [{"text": "RBMs'", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.9710841178894043}]}, {"text": "It also uses a form of regularization to incorporate prior knowledge into the model.", "labels": [], "entities": []}, {"text": "Due these modifications, our model is very well-suited for solving aspect-based opinion mining tasks.", "labels": [], "entities": [{"text": "solving aspect-based opinion mining tasks", "start_pos": 59, "end_pos": 100, "type": "TASK", "confidence": 0.6877529799938202}]}, {"text": "3. The optimal weight matrix of this RBM model can exactly reflect individual word features toward aspects and sentiment, which is hard to achieve with LDA-based models due to the mixture model sharing mechanism.", "labels": [], "entities": []}, {"text": "4. Last but not the least, this RBM model is capable of jointly modeling aspect and sentiment information together.", "labels": [], "entities": [{"text": "RBM", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9339497685432434}]}], "datasetContent": [{"text": "We present a series of experiments to evaluate our model's performance on the aspect identification and sentiment classification tasks.", "labels": [], "entities": [{"text": "aspect identification and sentiment classification", "start_pos": 78, "end_pos": 128, "type": "TASK", "confidence": 0.7146496474742889}]}, {"text": "For the experimental setup, we use ten hidden units in our Sentiment-Aspect Extraction RBM (SERBM), where units 0-6 capture aspects, units 7-8 capture sentiment information, and unit 9 stores background information.", "labels": [], "entities": [{"text": "Sentiment-Aspect Extraction RBM", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6992247402667999}]}, {"text": "In particular, we fix hidden units 0-6 to represent the target aspects Food, Staff, Ambience, Price, Ambience, Miscellaneous, and Other Aspects, respectively.", "labels": [], "entities": []}, {"text": "Units 7-8 represent positive and negative sentiment, respectively.", "labels": [], "entities": []}, {"text": "The remaining hidden unit is intended to capture irrelevant background information.", "labels": [], "entities": []}, {"text": "Note that the structure of our model needs no modifications for new reviews.: Results in terms of perplexity dataset has a gold standard label set, then we assign one hidden unit to represent each label in the gold standard set.", "labels": [], "entities": []}, {"text": "If not, our model only obtains the priors p A,v k and p S,v k , and the aspect set can be inferred as in the work of.", "labels": [], "entities": []}, {"text": "For evaluation, following previous work, the annotated data is fed into our unsupervised model, without any of the corresponding labels.", "labels": [], "entities": []}, {"text": "The model is then evaluated in terms of how well its prediction matches the true labels.", "labels": [], "entities": []}, {"text": "As for hyperparameter optimization, we use the perplexity scores as defined in Eq.", "labels": [], "entities": [{"text": "hyperparameter optimization", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8490431308746338}]}, {"text": "10 to find the optimal hyperparameters.", "labels": [], "entities": []}, {"text": "As a baseline, we also re-implement standard RBMs and the RSM model) to process this same restaurant review dataset and identify aspects for every document in this dataset under the same experimental conditions.", "labels": [], "entities": [{"text": "restaurant review dataset", "start_pos": 90, "end_pos": 115, "type": "DATASET", "confidence": 0.6504074533780416}]}, {"text": "We recall that RSM is a similar undirected graphical model that models topics from raw text.", "labels": [], "entities": [{"text": "RSM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9017664790153503}]}, {"text": "Last but not the least, we conduct additional comparative experiments, including with LocLDA (, MaxEnt-LDA () and the SAS model (Mukherjee and Liu, 2012) to extract aspects for this restaurant review dataset under the same experimental conditions.", "labels": [], "entities": [{"text": "SAS", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.6985176205635071}, {"text": "restaurant review dataset", "start_pos": 182, "end_pos": 207, "type": "DATASET", "confidence": 0.6758085985978445}]}, {"text": "In the following, we use the abbreviated name MELDA to stand for the MaxEnt LDA method.", "labels": [], "entities": [{"text": "MELDA", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9223092198371887}]}, {"text": "Brody and Elhadad (2010) and utilize three aspects to perform a quantitative evaluation and only use sentences with a single label for evaluation to avoid ambiguity.", "labels": [], "entities": []}, {"text": "The three major aspects chosen from the gold standard labels are S = {Food, Staff, Ambience}.", "labels": [], "entities": []}, {"text": "The evaluation criterion essentially is to judge how well the prediction matches the true label, resulting in Precision, Recall, and F 1 scores.", "labels": [], "entities": [{"text": "Precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9991021156311035}, {"text": "Recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9690635204315186}, {"text": "F 1 scores", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9825381636619568}]}, {"text": "Besides these, we consider perplexity (PPL) as another evaluation metric to analyze the aspect identification quality.", "labels": [], "entities": [{"text": "aspect identification", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8342553079128265}]}, {"text": "The average test perplexity PPL over words is defined as:  where N is the number of documents, D n represents the word number, and v n stands for the wordcount of document n.", "labels": [], "entities": [{"text": "PPL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9542884826660156}]}, {"text": "Average perplexity results are reported in Table 1, while Precision, Recall, and F 1 evaluation results for aspect identification are given in Table 2.", "labels": [], "entities": [{"text": "Precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9984435439109802}, {"text": "Recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.98003089427948}, {"text": "F 1 evaluation", "start_pos": 81, "end_pos": 95, "type": "METRIC", "confidence": 0.97752712170283}, {"text": "aspect identification", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.9540759027004242}]}, {"text": "Some LDA-based methods require manual mappings for evaluation, which causes difficulties in obtaining a fair PPL result, so a few methods are only considered in.", "labels": [], "entities": []}, {"text": "To illustrate the differences, in, we list representative words for aspects identified by various models and highlight words without an obvious association or words that are rather unspecific in bold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results in terms of perplexity", "labels": [], "entities": []}, {"text": " Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset", "labels": [], "entities": [{"text": "Aspect identification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7351735383272171}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9996680021286011}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9994981288909912}, {"text": "F 1 scores", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9897534648577372}]}, {"text": " Table 5: Accuracy for SERBM and JST", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992406368255615}, {"text": "SERBM", "start_pos": 23, "end_pos": 28, "type": "TASK", "confidence": 0.5473170280456543}, {"text": "JST", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8419532179832458}]}]}