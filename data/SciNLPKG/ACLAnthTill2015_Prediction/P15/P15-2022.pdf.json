{"title": [{"text": "Recurrent Neural Network based Rule Sequence Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.8285288214683533}]}], "abstractContent": [{"text": "The inability to model long-distance dependency has been handicapping SMT for years.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9927353858947754}]}, {"text": "Specifically, the context independence assumption makes it hard to capture the dependency between translation rules.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel recurrent neural network based rule sequence model to incorporate arbitrary long contextual information during estimating probabilities of rule sequences.", "labels": [], "entities": []}, {"text": "Moreover , our model frees the translation model from keeping huge and redundant grammars, resulting in more efficient training and decoding.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9637845158576965}]}, {"text": "Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9994193315505981}]}], "introductionContent": [{"text": "Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.8156972775856653}]}, {"text": "While lots of efforts have been made in solving long-distance reordering (;), longspan n-gram matching), much less attention has been concentrated on capturing translation rule dependency, which is not explicitly modeled inmost translation systems (.", "labels": [], "entities": []}, {"text": "SMT systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.986656665802002}]}, {"text": "These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (.", "labels": [], "entities": [{"text": "translation", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.969788670539856}]}, {"text": "However, it is not an easy task to capture the rule dependency, which entails much longer context and more severe data sparsity.", "labels": [], "entities": []}, {"text": "There are two major solutions: the first one is breaking the rules into bilingual wordpairs and use a n-gram translation model to incorporate lexical dependencies that span rule boundaries ().", "labels": [], "entities": []}, {"text": "These ngram models (also known as tuple sequence model) could help phrase-based translation models to overcome the phrasal independence assumption, but they rely on word alignment to extract bilingual tuples, which brings in additional alignment error ().", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6472813785076141}]}, {"text": "The other direction lies in utilizing the rule Markov model), which directly explores dependencies in rule derivation history and achieves both good performance and slimmer translation model in syntax-based SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 207, "end_pos": 210, "type": "TASK", "confidence": 0.9266023635864258}]}, {"text": "However, the sparsity of translation rules entails aggressive pruning of the training data and constrains the model from scaling to high order grams, significantly limiting the ability of the model.", "labels": [], "entities": []}, {"text": "In this paper we follow the second line and propose a novel recurrent neural network based rule sequence model (RNN-RSM), which utilizes the representational power of recurrent neural network (RNN) to capture arbitrary distance of contextual information in estimating the probability of rule sequences, rather than constrained to n-gram local context limited by Markov assumption.", "labels": [], "entities": []}, {"text": "Compared with previous studies, our contributions are as follows: First, we lift the Markov assumption in rule sequence model and use RNN to capture arbitrarylength of contextual information, which is proven to be more accurate in estimating sequential probabilities (.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized  RNN-RSM describe in Section 3.1, fRNN-RSM st denotes for RNN-RSM factorized by source/target side in Section  3.2.", "labels": [], "entities": [{"text": "fRNN-RSM", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9251382946968079}]}]}