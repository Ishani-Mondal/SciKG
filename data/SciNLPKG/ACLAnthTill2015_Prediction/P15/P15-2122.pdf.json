{"title": [], "abstractContent": [{"text": "A question maybe asked not only to elicit information, but also to make a statement.", "labels": [], "entities": []}, {"text": "Questions serving the latter purpose , called rhetorical questions, are often lexically and syntactically indistinguishable from other types of questions.", "labels": [], "entities": []}, {"text": "Still, it is desirable to be able to identify rhetorical questions, as it is relevant for many NLP tasks, including information extraction and text summarization.", "labels": [], "entities": [{"text": "identify rhetorical questions", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.7976197203000387}, {"text": "information extraction", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8213366568088531}, {"text": "text summarization", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7586861550807953}]}, {"text": "In this paper, we explore the largely understudied problem of rhetorical question identification.", "labels": [], "entities": [{"text": "rhetorical question identification", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.8266539176305135}]}, {"text": "Specifically, we present a simple n-gram based language model to classify rhetorical questions in the Switchboard Dialogue Act Corpus.", "labels": [], "entities": [{"text": "Switchboard Dialogue Act Corpus", "start_pos": 102, "end_pos": 133, "type": "DATASET", "confidence": 0.7636249363422394}]}, {"text": "We find that a special treatment of rhetorical questions which incorporates contextual information achieves the highest performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rhetorical questions frequently appear in everyday conversations.", "labels": [], "entities": []}, {"text": "A rhetorical question is functionally different from other types of questions in that it is expressing a statement, rather than seeking information.", "labels": [], "entities": []}, {"text": "Thus, rhetorical questions must be identified to fully capture the meaning of an utterance.", "labels": [], "entities": []}, {"text": "This is not an easy task; despite their drastic functional differences, rhetorical questions are formulated like regular questions.", "labels": [], "entities": []}, {"text": "states that in principle, a given question can be interpreted as either an information seeking question or as a rhetorical question and that intonation can be used to identify the interpretation intended by the speaker.", "labels": [], "entities": []}, {"text": "For instance, consider the following example: (1) Did I tell you that writing a dissertation was easy?", "labels": [], "entities": []}, {"text": "Just from reading the text, it is difficult to tell whether the speaker is asking an informational question or whether they are implying that they did not say that writing a dissertation was easy.", "labels": [], "entities": []}, {"text": "However, according to our observation, which forms the basis of this work, there are two cases in which rhetorical questions can be identified solely based on the text.", "labels": [], "entities": []}, {"text": "Firstly, certain linguistic cues make a question obviously rhetorical, which can be seen in examples and . Secondly, the context, or neighboring utterances, often reveal the rhetorical nature of the question, as we can see in example (4).", "labels": [], "entities": []}, {"text": "(2) Who ever lifted a finger to help George?", "labels": [], "entities": []}, {"text": "(3) After all, who has anytime during the exam period?", "labels": [], "entities": []}, {"text": "(4) Who likes winter?", "labels": [], "entities": []}, {"text": "It is always cold and windy and gray and everyone feels miserable all the time.", "labels": [], "entities": []}, {"text": "There has been substantial work in the area of classifying dialog acts, within which rhetorical questions fall.", "labels": [], "entities": [{"text": "classifying dialog acts", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.8652531305948893}]}, {"text": "To our knowledge, prior work on dialog act tagging has largely ignored rhetorical questions, and there has not been any previous work specifically addressing rhetorical question identification.", "labels": [], "entities": [{"text": "dialog act tagging", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7510726650555929}, {"text": "rhetorical question identification", "start_pos": 158, "end_pos": 192, "type": "TASK", "confidence": 0.6457804044087728}]}, {"text": "Nevertheless, classification of rhetorical questions is crucial and has numerous potential applications, including questionanswering, document summarization, author identification, and opinion extraction.", "labels": [], "entities": [{"text": "classification of rhetorical questions", "start_pos": 14, "end_pos": 52, "type": "TASK", "confidence": 0.8916366100311279}, {"text": "document summarization", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.6118266731500626}, {"text": "author identification", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.8205202519893646}, {"text": "opinion extraction", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.8339098393917084}]}, {"text": "We provide an overview of related work in Section 2, discuss linguistic characteristics of rhetorical questions in Section 3, describe the experimental setup in Section 4, and present and analyze the experiment results in Section 5.", "labels": [], "entities": []}, {"text": "We find that, while the majority of the classification relies on features extracted from the question itself, adding in n-gram features from the context improves the performance.", "labels": [], "entities": []}, {"text": "An F 1 -score of 53.71% is achieved by adding features extracted from the preceding and subsequent utterances, which is about a 10% improvement from a baseline classifier using only the features from the question itself. and Reithinger and Klesen (1997) used n-gram language modeling on the Switchboard and Verbmobil corpora respectively to classify dialog acts.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.9888937473297119}, {"text": "Verbmobil corpora", "start_pos": 307, "end_pos": 324, "type": "DATASET", "confidence": 0.8900571763515472}]}, {"text": "uses a Bayesian approach with n-grams to categorize dialog acts.", "labels": [], "entities": []}, {"text": "We also employ a similar language model to achieve our results.", "labels": [], "entities": []}, {"text": "used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances.", "labels": [], "entities": [{"text": "Verbmobil corpus", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9124650955200195}]}, {"text": "utilized Hidden Markov Models on the Switchboard corpus and used word order within utterances and the order of dialog acts over utterances.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.8884340822696686}]}, {"text": "worked on automatic summarization of open-domain spoken dialogues i.e., important pieces of information are found in the back and forth of a dialogue that is absent in a written piece.", "labels": [], "entities": [{"text": "summarization of open-domain spoken dialogues", "start_pos": 20, "end_pos": 65, "type": "TASK", "confidence": 0.7804041147232056}]}, {"text": "used intra-utterance features in the Switchboard corpus and calculated n-grams for each utterance of all dialogue acts.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.9428285658359528}]}, {"text": "For each ngram, they computed the maximal predictivity i.e., its highest predictivity value within any dialogue act category.", "labels": [], "entities": []}, {"text": "We utilized a similar metric for ngram selection.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Experimental results (%)", "labels": [], "entities": []}, {"text": " Table 4. Our classifier achieves an accu- racy of 81% when trained on the questions alone  and 84% when integrating precedent and subse- quent context. Due to the reduced size of the  evenly split dataset, performing a McNemar's test  with Edwards' correction (Edwards 1948) does  not allow us to reject the null hypothesis that the  two experiments do not derive from the same dis- tribution with 95% confidence (\u03c7 2 = 1.49 giv- ing a 2-tailed p value of 0.22). However, over the  whole skewed dataset, we find \u03c7 2 = 30.74 giv- ing a 2-tailed p < 0.00001 so we have reason to  believe that with a larger evenly-split dataset inte-", "labels": [], "entities": [{"text": "accu- racy", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9673534433046976}]}, {"text": " Table 4: Experimental results (%) on evenly distributed data  (training set size: 670 & test set size: 288)", "labels": [], "entities": []}]}