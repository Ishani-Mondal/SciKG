{"title": [{"text": "Improving Distributed Representation of Word Sense via WordNet Gloss Composition and Context Clustering", "labels": [], "entities": [{"text": "Improving Distributed Representation of Word Sense", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.9177775681018829}]}], "abstractContent": [{"text": "In recent years, there has been an increasing interest in learning a distributed representation of word sense.", "labels": [], "entities": []}, {"text": "Traditional context clustering based models usually require careful tuning of model parameters , and typically perform worse on infrequent word senses.", "labels": [], "entities": []}, {"text": "This paper presents a novel approach which addresses these limitations by first initializing the word sense embeddings through learning sentence-level embeddings from WordNet glosses using a convolutional neural networks.", "labels": [], "entities": []}, {"text": "The initialized word sense embeddings are used by a context clustering based model to generate the distributed representations of word senses.", "labels": [], "entities": []}, {"text": "Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7727264960606893}, {"text": "analogical reasoning task", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.7968834638595581}]}], "introductionContent": [{"text": "With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest.", "labels": [], "entities": [{"text": "distributed representation of knowledge", "start_pos": 75, "end_pos": 114, "type": "TASK", "confidence": 0.7945510149002075}]}, {"text": "Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (), word level, phrase level (), sentence level (, discourse level ( and document level ().", "labels": [], "entities": []}, {"text": "In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information.", "labels": [], "entities": []}, {"text": "Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word.", "labels": [], "entities": []}, {"text": "However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive to the clustering algorithm which requires the setting of the sense number for each word.", "labels": [], "entities": []}, {"text": "For example, proposed two clustering based model: the Multi-Sense Skip-Gram (MSSG) model and Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) model.", "labels": [], "entities": []}, {"text": "MSSG assumes each word has the same k-sense (e.g. k = 3), i.e., the same number of possible senses.", "labels": [], "entities": [{"text": "MSSG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8665107488632202}]}, {"text": "However, the number of senses in WordNet varies from 1 such as \"ben\" to 75 such as \"break\".", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.945371150970459}]}, {"text": "As such, fixing the number of senses for all words would result in poor representations.", "labels": [], "entities": []}, {"text": "NP-MSSG can learn the number of senses for each word directly from data.", "labels": [], "entities": [{"text": "NP-MSSG", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8603469133377075}]}, {"text": "But it requires a tuning of a hyperparameter \u03bb which controls the creation of cluster centroids during training.", "labels": [], "entities": []}, {"text": "Different \u03bb needs to be tuned for different datasets.", "labels": [], "entities": []}, {"text": "(2) The initial value of sense representation is critical for most statistical clustering based approaches.", "labels": [], "entities": [{"text": "sense representation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7684425711631775}]}, {"text": "However, previous approaches usually adopted random initialization) or the mean average of candidate words in a gloss ( . As a result, they may not produce optimal clustering results for word senses.", "labels": [], "entities": []}, {"text": "Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering.", "labels": [], "entities": [{"text": "WordNet gloss composition", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.7702622810999552}, {"text": "context clustering", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7163038700819016}]}, {"text": "The basic idea is that a word sense is represented as a synonym set (synset) in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9467300772666931}]}, {"text": "In this way, instead of assigning a fixed sense number to each word as in the previous methods, different word will be assigned with different number of senses based on their corresponding entries in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 200, "end_pos": 207, "type": "DATASET", "confidence": 0.9667627215385437}]}, {"text": "Moreover, we notice that each synset has a textual definition (named as gloss).", "labels": [], "entities": []}, {"text": "Naturally, we use a convolutional neural network (CNN) to learn distributed representations of these glosses (a.k.a. sense vectors) through sentence composition.", "labels": [], "entities": []}, {"text": "Then, we modify MSSG for context clustering by initializing the sense vectors with the representations learned by our CNN-based sentence composition model.", "labels": [], "entities": [{"text": "context clustering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7182947099208832}]}, {"text": "We expect that word sense vectors initialized in this way would potentially lead to better representations of word senses generated from context clustering.", "labels": [], "entities": []}, {"text": "The obtained word sense representations are evaluated on two tasks.", "labels": [], "entities": []}, {"text": "One is word similarity task, the other is analogical reasoning task provided by WordRep ( ).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7407403886318207}, {"text": "analogical reasoning", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7022630870342255}, {"text": "WordRep", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9727514982223511}]}, {"text": "The results show that our approach attains comparable performance on learning distributed representations of word senses.", "labels": [], "entities": []}, {"text": "In specific, our learned representation outperforms publicly available embeddings on the globalSim and localSim metrics in word similarity task, and 6 in 13 subtasks in the analogical reasoning task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.763072152932485}, {"text": "analogical reasoning task", "start_pos": 173, "end_pos": 198, "type": "TASK", "confidence": 0.7716512481371561}]}], "datasetContent": [{"text": "In all experiments, we train word vectors and sense vectors on a snapshot of Wikipedia in April 2010 3, previously used in ().", "labels": [], "entities": [{"text": "Wikipedia in April 2010 3", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.9186198353767395}]}, {"text": "WordNet 3.1 is used for training the sentence composition model.", "labels": [], "entities": [{"text": "WordNet 3.1", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9078246653079987}, {"text": "sentence composition", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.764747828245163}]}, {"text": "A publicly available word vectors trained by CBOW from Google News 4 are used as pretrained word vectors for CNN.", "labels": [], "entities": [{"text": "CBOW from Google News 4", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.9276872873306274}]}, {"text": "For training CNN, we use: rectified linear units, filter windows of 3, 4, 5 with 100 feature maps each, AdaDelta decay parameter of 0.95, the dropout rate of 0.5.", "labels": [], "entities": [{"text": "AdaDelta decay parameter", "start_pos": 104, "end_pos": 128, "type": "METRIC", "confidence": 0.8464081287384033}]}, {"text": "For training VMSSG, we use MSSG-KMeans as the clustering algorithm, and CBOW for learning sense vectors.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.6479732394218445}]}, {"text": "We set the size of word vectors to 300, using boot vectors and sense vectors.", "labels": [], "entities": []}, {"text": "For other parameter, we use default parameter settings for MSSG.", "labels": [], "entities": [{"text": "MSSG", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9396601319313049}]}], "tableCaptions": [{"text": " Table 1: Experimental results in the SCWS task.", "labels": [], "entities": [{"text": "SCWS task", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.8783746063709259}]}, {"text": " Table 2: Experimental results in the analogical reasoning task.", "labels": [], "entities": [{"text": "analogical reasoning task", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.9337965846061707}]}]}