{"title": [{"text": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering", "labels": [], "entities": [{"text": "Answer Sentence Selection in Question Answering", "start_pos": 35, "end_pos": 82, "type": "TASK", "confidence": 0.778585081299146}]}], "abstractContent": [{"text": "In this paper, we present an approach that address the answer sentence selection problem for question answering.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.6295921405156454}, {"text": "question answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8533759415149689}]}, {"text": "The proposed method uses a stacked bidirectional Long-Short Term Memory (BLSTM) network to sequentially read words from question and answer sentences, and then outputs their relevance scores.", "labels": [], "entities": []}, {"text": "Unlike prior work, this approach does not require any syntactic parsing or external knowledge resources such as WordNet which may not be available in some domains or languages.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7871403992176056}, {"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9578320384025574}]}, {"text": "The full system is based on a combination of the stacked BLSTM relevance model and keywords matching.", "labels": [], "entities": [{"text": "BLSTM relevance", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.39738893508911133}, {"text": "keywords matching", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7827180922031403}]}, {"text": "The results of our experiments on a public benchmark dataset from TREC show that our system outperforms previous work which requires syntactic features and external knowledge resources.", "labels": [], "entities": [{"text": "public benchmark dataset from TREC", "start_pos": 36, "end_pos": 70, "type": "DATASET", "confidence": 0.794067108631134}]}], "introductionContent": [{"text": "A typical architecture of open-domain question answering (QA) systems is composed of three high level major steps: a) question analysis and retrieval of candidate passages; b) ranking and selecting of passages which contain the answer; and optionally c) extracting and verifying the answer.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.8477894484996795}, {"text": "question analysis", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7368287146091461}]}, {"text": "In this paper, we focus on the answer sentence selection.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.79399706919988}]}, {"text": "Being considered as a key subtask of QA, the selection is to identify the answer-bearing sentences from all candidate sentences.", "labels": [], "entities": []}, {"text": "The selected sentences should be relevant to and answer the input questions.", "labels": [], "entities": []}, {"text": "The nature of this task is to match not only the words but also the meaning between question and answer sentences.", "labels": [], "entities": []}, {"text": "For instance, although both of the following sentences contain keywords \"Capriati\" and \"play\", only the first sentence answers the question: \"What sport does Jennifer Capriati play?\"", "labels": [], "entities": []}, {"text": "Positive Sentence: \"Capriati, 19, who has not played competitive tennis since November 1994, has been given a wildcard to take part in the Paris tournament which starts on February 13.\"", "labels": [], "entities": [{"text": "Positive Sentence", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.659621924161911}]}, {"text": "Negative Sentence: \"Capriati also was playing in the U.S. Open semifinals in '91, one year before Davenport won the junior title on those same courts.\"", "labels": [], "entities": [{"text": "Negative Sentence", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.5496156811714172}]}, {"text": "Besides its application in the automated factoid QA system, another benefit of the answer sentence selection is that it can be potentially used to predict answer quality in community QA sites.", "labels": [], "entities": [{"text": "factoid QA", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.5372542440891266}, {"text": "answer sentence selection", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6671713391939799}]}, {"text": "The techniques developed from this task might also be beneficial to the emerging real-time user-oriented QA tasks such as TREC LiveQA.", "labels": [], "entities": [{"text": "TREC LiveQA", "start_pos": 122, "end_pos": 133, "type": "DATASET", "confidence": 0.8740808665752411}]}, {"text": "However, usergenerated content can be noisy and hard to parse with off-the-shelf NLP tools.", "labels": [], "entities": []}, {"text": "Therefore, methods that requires less syntactic features are desirable.", "labels": [], "entities": []}, {"text": "Recently, neural network-based distributed sentence modeling has been found successful in many natural language processing tasks such as word sense disambiguation), discourse parsing (), machine translation (), and paraphrase detection.", "labels": [], "entities": [{"text": "distributed sentence modeling", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6902528007825216}, {"text": "word sense disambiguation", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.6624836226304373}, {"text": "discourse parsing", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7461082339286804}, {"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.8190352618694305}, {"text": "paraphrase detection", "start_pos": 215, "end_pos": 235, "type": "TASK", "confidence": 0.9703201651573181}]}, {"text": "In this paper, we present an approach that leverages the power of deep neural network to address the answer sentence selection problem for question answering.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.6388540764649709}, {"text": "question answering", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.8517780005931854}]}, {"text": "Our method employs stacked bidirectional Long Short-Term Memory (BLSTM) to sequentially read the words from question and answer sentences, and then output their relevance scores.", "labels": [], "entities": [{"text": "Long Short-Term Memory (BLSTM", "start_pos": 41, "end_pos": 70, "type": "METRIC", "confidence": 0.664486688375473}]}, {"text": "The full system, when combined with keywords matching, outperforms previous approaches without using any syntactic parsing or external knowledge resources.", "labels": [], "entities": [{"text": "keywords matching", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7475586533546448}]}], "datasetContent": [{"text": "Dataset The answer sentence selection dataset used in this paper was created by based on Text REtrieval Conference (TREC) QA track (8-13) data.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.7076835234959921}, {"text": "Text REtrieval Conference (TREC) QA", "start_pos": 89, "end_pos": 124, "type": "TASK", "confidence": 0.7315801467214312}]}, {"text": "Candidate answer sentences were automatically retrieved for each question which is on average associated with 33 candidate sentences.", "labels": [], "entities": []}, {"text": "There are two sets of data provided for training.", "labels": [], "entities": []}, {"text": "One is the full training set containing 1229 questions that are automatically labeled by matching answer keys' regular expressions.", "labels": [], "entities": []}, {"text": "However, the generated labels are noisy and sometimes erroneously mark unrelated sentences as the correct answers solely because those sentences contain answer keys.", "labels": [], "entities": []}, {"text": "also provided one small training set contains 94 questions, which were manually corrected for errors.", "labels": [], "entities": []}, {"text": "In our experiments, we use the full training set because it provides significantly more question and answer sentences for learning, even though some of its labels are noisy.", "labels": [], "entities": []}, {"text": "The development and test data sets have 82 and 100 questions, respectively.", "labels": [], "entities": []}, {"text": "Following (), candidate answer sentences with over 40 words and questions with only positive or negative candidate answer sentences are removed from 1 http://nlp.stanford.edu/mengqiu/data/ qg-emnlp07-data.tgz 2 Because the original full training dataset is no longer available from the website of the lead author of (, we obtained this data re-released from: http://cs.jhu.edu/ \u02dc xuchen/packages/ jacana-qa-naacl2013-data-results.tar.bz2  Evaluation Metric Following previous works on this task, we also use Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) as evaluation metrics, which are calculated using the official trec eval evaluation scripts.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 508, "end_pos": 536, "type": "METRIC", "confidence": 0.9631957709789276}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 541, "end_pos": 567, "type": "METRIC", "confidence": 0.8875897725423177}]}, {"text": "Keywords Matching Baseline (BM25) As noted by, counting overlapped keywords, especially when re-weighted by idf value of the question word, is a fairly competitive baseline.", "labels": [], "entities": [{"text": "Keywords Matching Baseline (BM25)", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.6043398280938467}]}, {"text": "Following, our keywords matching baseline also counts the words that occurred in both questions and answer sentences, after excluding stop words and lowering the case.", "labels": [], "entities": []}, {"text": "But, instead of the tf \u00b7 idf formula used in (, word counts are re-weighted by its idf value using the Okapi BM25) formula (with constants values K 1 = 1.2 and B = 0.75).", "labels": [], "entities": []}, {"text": "Network Setup The network weights are randomly initialized using a Gaussian distribution (\u00b5 = 0 and \u03c3 = 0.1), and the network is trained with the stochastic gradient descent (SGD) with momentum 0.9.", "labels": [], "entities": []}, {"text": "We experimented single-layer unidirectional LSTM, single-layer BLSTM, and three-layer stacked BLSTM.", "labels": [], "entities": []}, {"text": "Each layer of LSTM and BLSTM has a memory size of 500.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.455185204744339}]}, {"text": "We use 300-dimensional vectors that were trained and provided by word2vec tool) using apart of the Google News dataset 4 (around 100 billion tokens) . surveys prior results on this task, and places our models in the context of the current state-of-the-art results.", "labels": [], "entities": [{"text": "Google News dataset 4", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.9180536866188049}]}, {"text": "summarizes the results of our model on the answer selection task.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.9080082972844442}]}, {"text": "According to and 2, our combined system outperforms prior works on MAP and MRR metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of prior results on the answer  sentence selection task", "labels": [], "entities": [{"text": "answer  sentence selection task", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.8231034576892853}]}, {"text": " Table 2: Overview of our results on the answer  sentence selection task. Features are keywords  matching baseline score (BM25), and pooling val- ues of single-layer unidirectional LSTM (Single- Layer LSTM), single-Layer bidirectional LSTM  (Single-Layer BLSTM) and three-Layer stacked  BLSTM's (Three-Layer BLSTM) outputs. Gra- dient boosted regression tree (GBDT) method is  used to combine features.", "labels": [], "entities": [{"text": "answer  sentence selection task", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8252286165952682}, {"text": "matching baseline score (BM25)", "start_pos": 97, "end_pos": 127, "type": "METRIC", "confidence": 0.798888956507047}, {"text": "Gra- dient boosted regression tree (GBDT)", "start_pos": 324, "end_pos": 365, "type": "METRIC", "confidence": 0.8955546286371019}]}]}