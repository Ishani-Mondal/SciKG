{"title": [{"text": "Leveraging Compounds to Improve Noun Phrase Translation from Chinese and German", "labels": [], "entities": [{"text": "Improve Noun Phrase Translation from Chinese and German", "start_pos": 24, "end_pos": 79, "type": "TASK", "confidence": 0.8120420910418034}]}], "abstractContent": [{"text": "This paper presents a method to improve the translation of polysemous nouns, when a previous occurrence of the noun as the head of a compound noun phrase is available in a text.", "labels": [], "entities": [{"text": "translation of polysemous nouns", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.889946237206459}]}, {"text": "The occurrences are identified through pattern matching rules, which detect XY compounds followed closely by a potentially coreferent occurrence of Y , such as \"Nordwand.", "labels": [], "entities": []}, {"text": "Two strategies are proposed to improve the translation of the second occurrence of Y : re-using the cached translation of Y from the XY compound, or post-editing the translation of Y using the head of the translation of XY.", "labels": [], "entities": []}, {"text": "Experiments are performed on Chinese-to-English and German-to-French statistical machine translation, over the WIT3 and Text+Berg corpora respectively, with 261 XY /Y pairs each.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6730726559956869}, {"text": "WIT3", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.9531320929527283}, {"text": "Text+Berg corpora", "start_pos": 120, "end_pos": 137, "type": "DATASET", "confidence": 0.7155898883938789}]}, {"text": "The results suggest that while the overall BLEU scores increase only slightly, the translations of the targeted polysemous nouns are significantly improved.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9985068440437317}]}], "introductionContent": [{"text": "Words tend to be less ambiguous when considered in context, which partially explains the success of phrase-based statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 100, "end_pos": 150, "type": "TASK", "confidence": 0.7275367762361254}]}, {"text": "In this paper, we take advantage of this observation, and extend the disambiguation potential of n-grams to subsequent occurrences of their individual components.", "labels": [], "entities": []}, {"text": "We assume that the translation of a noun-noun compound, noted XY , displays fewer ambiguities than the translations of its components X and Y . Therefore, on a subsequent occurrence of the head of XY , assumed to refer to the same entity as XY , we hypothesize that its previously-found translation offers a better and more coherent translation than the one proposed by an SMT system that is not aware of the compound.", "labels": [], "entities": [{"text": "SMT", "start_pos": 373, "end_pos": 376, "type": "TASK", "confidence": 0.9821529984474182}]}, {"text": "Our claim is supported by results from experiments on Chinese-to-English (ZH/EN) and German-to-French (DE/FR) translation presented in this paper.", "labels": [], "entities": [{"text": "German-to-French (DE/FR) translation", "start_pos": 85, "end_pos": 121, "type": "TASK", "confidence": 0.5575041728360313}]}, {"text": "In both source languages, noun-noun compounds are frequent, and will enable us to disambiguate subsequent occurrences of their head.", "labels": [], "entities": []}, {"text": "For instance, in the example in, the Chinese compound refers to 'high heels', and the subsequent mention of the referent using only the third character () should be translated as 'heels'.", "labels": [], "entities": []}, {"text": "However, the character by itself could also be translated as 'shoe' or 'footwear', as observed with a baseline SMT system that is not aware of the XY /Y coreference.", "labels": [], "entities": [{"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9812662601470947}, {"text": "XY /Y coreference", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.7152333706617355}]}, {"text": "Although the XY /Y configuration may not be very frequent in texts, errors in its translation are particularly detrimental to the understanding of a text, as they often conceal the coreference link between two expressions.", "labels": [], "entities": []}, {"text": "Moreover, as we will show, such issues can be quite reliably corrected, and the proposed approach can later generalize to other configurations of noun phrase coreference.", "labels": [], "entities": [{"text": "noun phrase coreference", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.6922263701756796}]}, {"text": "She thought since bought a pair of two inches high heel, but in fact it was a pair of three inches high shoes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are carried out on two different parallel corpora: the WIT 3 Chinese-English dataset ( Our baseline SMT system is the Moses phrasebased decoder ( , trained over tokenized and true-cased data.", "labels": [], "entities": [{"text": "WIT 3 Chinese-English dataset", "start_pos": 71, "end_pos": 100, "type": "DATASET", "confidence": 0.9289825707674026}, {"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9623686671257019}]}, {"text": "The language models were built using SRILM (Stolcke et al., 2011) at order 3 (i.e. up to trigrams) using the default smoothing method (i.e. Good-Turing).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.6571682691574097}]}, {"text": "Optimization was done using Minimum Error Rate Training as provided with Moses.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 28, "end_pos": 55, "type": "METRIC", "confidence": 0.7251472175121307}]}, {"text": "The effectiveness of proposed systems is measured in two ways.", "labels": [], "entities": []}, {"text": "First, we use BLEU () for overall evaluation, to verify whether our systems provide better translation for entire texts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9989548921585083}]}, {"text": "Then, we focus on the XY /Y pairs and count the number of cases in which the translations of Y match the reference or not, which can be computed automatically using the alignments.", "labels": [], "entities": []}, {"text": "However, the automatic comparison of a system's translation with the reference is not entirely informative, because even if the two differ, the system's translation can still be acceptable.", "labels": [], "entities": []}, {"text": "Therefore, we analyzed these \"undecided\" situations manually, with three human annotators (among the authors of the paper).", "labels": [], "entities": []}, {"text": "The annotators rated separately the system's translations of Y and the reference ones as 'good', 'acceptable' or 'wrong'.", "labels": [], "entities": []}, {"text": "When both the baseline and one of our systems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects.", "labels": [], "entities": []}, {"text": "This was done for the 73 such cases of the ZH/EN post-editing system.", "labels": [], "entities": [{"text": "ZH/EN post-editing system", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.875710391998291}]}, {"text": "Three of the authors, working independently, considered each translation from each system (in separate batches) with respect to the reference one, and rated its meaning on a 3-point scale: 2 (good), 1 (acceptable) or 0 (wrong).", "labels": [], "entities": []}, {"text": "To estimate the inter-rater agreement, we computed the average absolute deviation and found a value of 0.15, thus denoting very good agreement.", "labels": [], "entities": []}, {"text": "Below, we group '2' and '1' answers into one category, called \"acceptable\", and compare them to '0' answers, i.e. wrong translations.", "labels": [], "entities": []}, {"text": "When both the baseline and the post-edited translations of Y differ from the reference, they can either be identical (49 cases) or different (24).", "labels": [], "entities": []}, {"text": "In the former case, of course, neither of the systems outperforms the other.", "labels": [], "entities": []}, {"text": "The interesting observation is that the relatively high number of such cases (49) is due to situations where the reference translation of noun Y is by a pronoun (40), which the systems have currently no possibility to generate from a noun in the source sentence.", "labels": [], "entities": []}, {"text": "Manual evaluation shows that the systems' translations are correct in 36 out of 40 cases.", "labels": [], "entities": []}, {"text": "This large number shows that the \"quality\" of the systems is actually higher than what can be inferred from only.", "labels": [], "entities": []}, {"text": "Conversely, in the 9 cases when the reference translation of Y is not a pronoun, only about half of the translations are correct.", "labels": [], "entities": []}, {"text": "In the latter case, when baseline and post-edited translations differ from the reference and among themselves (24 cases), it is legitimate to ask which of the two systems is better.", "labels": [], "entities": []}, {"text": "Overall, 10 baseline translations are correct and 14 are wrong, whereas 23 post-edited translations are correct (or at least acceptable) and only one is wrong.", "labels": [], "entities": []}, {"text": "The postedited system thus clearly outperforms the baseline in this case.", "labels": [], "entities": []}, {"text": "Similarly to the observation above, we note that among the 24 cases considered here, almost all (20) involve a reference translation of Y by a pronoun.", "labels": [], "entities": []}, {"text": "In these cases, the baseline system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of SMT data sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8516789674758911}]}, {"text": " Table 2: BLEU scores of our methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994359612464905}]}, {"text": " Table 3: Comparison of each approach with the baseline, for the two language pairs, in terms of Y nouns  which are identical or different from a reference translation ('ref'). All scores are percentages of the  totals. Numbers in bold are improvements over the baseline, while those in italics are degradations.", "labels": [], "entities": []}]}