{"title": [{"text": "Structured Training for Neural Network Transition-Based Parsing", "labels": [], "entities": [{"text": "Neural Network Transition-Based Parsing", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.67237439006567}]}], "abstractContent": [{"text": "We present structured perceptron training for neural network transition-based dependency parsing.", "labels": [], "entities": [{"text": "neural network transition-based dependency parsing", "start_pos": 46, "end_pos": 96, "type": "TASK", "confidence": 0.6204214453697204}]}, {"text": "We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences.", "labels": [], "entities": []}, {"text": "Given this fixed network representation, we learn a final layer using the struc-tured perceptron with beam-search decoding.", "labels": [], "entities": []}, {"text": "On the Penn Treebank, our parser reaches 94.26% un-labeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9940741956233978}, {"text": "labeled attachment", "start_pos": 70, "end_pos": 88, "type": "METRIC", "confidence": 0.7062986493110657}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.5446577668190002}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9962672591209412}, {"text": "Stanford Dependencies", "start_pos": 146, "end_pos": 167, "type": "DATASET", "confidence": 0.9248726665973663}]}, {"text": "We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains inaccuracy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention.", "labels": [], "entities": [{"text": "Syntactic analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9562566578388214}, {"text": "language understanding", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7488269209861755}]}, {"text": "Lately, dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages ( and the efficiency of dependency parsers.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8643309772014618}]}, {"text": "Transition-based parsers) have been shown to provide a good balance between efficiency and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9976435303688049}]}, {"text": "In transition-based parsing, sentences are processed in a linear left to right pass; at each position, the parser needs to choose from a set of possible actions defined by the transition strategy.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.569002166390419}]}, {"text": "In greedy models, a classifier is used to independently decide which transition to take based on local features of the current parse configuration.", "labels": [], "entities": []}, {"text": "This classifier typically uses hand-engineered features and is trained on individual transitions extracted from the gold transition sequence.", "labels": [], "entities": []}, {"text": "While extremely fast, these greedy models typically suffer from search errors due to the inability to recover from incorrect decisions.", "labels": [], "entities": []}, {"text": "showed that a beamsearch decoding algorithm utilizing the structured perceptron training algorithm can greatly improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9975784420967102}]}, {"text": "Nonetheless, significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph-based parsers (, and only by incorporating graph-based scoring functions were able to exceed the accuracy of graph-based approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9978727102279663}, {"text": "accuracy", "start_pos": 234, "end_pos": 242, "type": "METRIC", "confidence": 0.9971957206726074}]}, {"text": "In contrast to these carefully hand-tuned approaches, recently presented a neural network version of a greedy transition-based parser.", "labels": [], "entities": []}, {"text": "In their model, a feedforward neural network with a hidden layer is used to make the transition decisions.", "labels": [], "entities": []}, {"text": "The hidden layer has the power to learn arbitrary combinations of the atomic inputs, thereby eliminating the need for hand-engineered features.", "labels": [], "entities": []}, {"text": "Furthermore, because the neural network uses a distributed representation, it is able to model lexical, part-of-speech (POS) tag, and arc label similarities in a continuous space.", "labels": [], "entities": []}, {"text": "However, although their model outperforms its greedy hand-engineered counterparts, it is not competitive with state-of-the-art dependency parsers that are trained for structured search.", "labels": [], "entities": []}, {"text": "In this work, we combine the representational power of neural networks with the superior search enabled by structured training and inference, making our parser one of the most accurate dependency parsers to date.", "labels": [], "entities": []}, {"text": "Training and testing on the Penn Treebank (, our transition-based parser achieves 93.99% unlabeled (UAS) / 92.05% labeled (LAS) attachment accuracy, outperforming the 93.22% UAS / 91.02% LAS of and.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9955011308193207}, {"text": "unlabeled (UAS) / 92.05% labeled (LAS) attachment", "start_pos": 89, "end_pos": 138, "type": "METRIC", "confidence": 0.6339011589686075}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.5416282415390015}, {"text": "UAS / 91.02% LAS", "start_pos": 174, "end_pos": 190, "type": "METRIC", "confidence": 0.5839384734630585}]}, {"text": "In addition, by incorporating unlabeled data into training, we further improve the accuracy of our model to 94.26% UAS / 92.41% LAS (93.46% UAS / 91.49% LAS for our greedy model).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9996433258056641}, {"text": "UAS / 92.41% LAS", "start_pos": 115, "end_pos": 131, "type": "METRIC", "confidence": 0.8404112458229065}]}, {"text": "In our approach we start with the basic structure of, but with a deeper architecture and improvements to the optimization procedure.", "labels": [], "entities": []}, {"text": "These modifications (Section 2) increase the performance of the greedy model by as much as 1%.", "labels": [], "entities": []}, {"text": "As in prior work, we train the neural network to model the probability of individual parse actions.", "labels": [], "entities": []}, {"text": "However, we do not use these probabilities directly for prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9583910703659058}]}, {"text": "Instead, we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates (Section 3).", "labels": [], "entities": []}, {"text": "On the Penn Treebank, this structured learning approach significantly improves parsing accuracy by 0.8%.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9957876205444336}, {"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9701607823371887}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9706459045410156}]}, {"text": "An additional contribution of this work is an effective way to leverage unlabeled data.", "labels": [], "entities": []}, {"text": "Neural networks are known to perform very well in the presence of large amounts of training data; however, obtaining more expert-annotated parse trees is very expensive.", "labels": [], "entities": []}, {"text": "To this end, we generate large quantities of high-confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees.", "labels": [], "entities": []}, {"text": "This approach is known as \"tri-training\" () and we show that it benefits our neural network parser significantly more than other approaches.", "labels": [], "entities": []}, {"text": "By adding 10 million automatically parsed tokens to the training data, we improve the accuracy of our parsers by almost \u223c1.0% on web domain data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9995158910751343}]}, {"text": "We provide an extensive exploration of our model in Section 5 through ablative analysis and other retrospective experiments.", "labels": [], "entities": []}, {"text": "One of the goals of this work is to provide guidance for future refinements and improvements on the architecture and modeling choices we introduce in this paper.", "labels": [], "entities": []}, {"text": "Finally, we also note that neural network representations have along history in syntactic parsing; however, like, our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7558985948562622}]}, {"text": "Our work is also not the first to apply structured training to neural networks (see e.g. and for Conditional Random Field (CRF) training of neural networks).", "labels": [], "entities": []}, {"text": "Our paper ex- Features Extracted early updates (section 3).", "labels": [], "entities": []}, {"text": "Structured learning reduces bias and significantly improves parsing accuracy by 0.6%.", "labels": [], "entities": [{"text": "bias", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9446161389350891}, {"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9719040393829346}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9643260836601257}]}, {"text": "We demonstrate empirically that beam search based on the scores from the neural network does notwork as well, perhaps because of the label bias problem.", "labels": [], "entities": [{"text": "beam search", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8084560632705688}]}, {"text": "A second contribution of this work is an effective way to leverage unlabeled data and other parsers.", "labels": [], "entities": []}, {"text": "Neural networks are known to perform very well in the presence of large amounts of training data.", "labels": [], "entities": []}, {"text": "It is however unlikely that the amount of hand parsed data will increase significantly because of the high cost for syntactic annotations.", "labels": [], "entities": []}, {"text": "To this end we generate large quantities of high-confidence parse trees by parsing an unlabeled corpus and selecting only the sentences on which two different parsers produced the same parse trees.", "labels": [], "entities": []}, {"text": "This idea comes from tri-training () and while applicable to other parsers as well, we show that it benefits neural network parsers more than models with discrete features.", "labels": [], "entities": []}, {"text": "Adding 10 million automatically parsed tokens to the training data improves the accuracy of our parsers further by 0.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.999647855758667}]}, {"text": "Our final greedy parser achieves an unlabeled attachment score (UAS) of 93.46% on the Penn Treebank test set, while a model with abeam of size 8 produces an UAS of 94.08% (section 4.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 36, "end_pos": 68, "type": "METRIC", "confidence": 0.8234660426775614}, {"text": "Penn Treebank test set", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.9950768202543259}, {"text": "UAS", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9984652996063232}]}, {"text": "To the best of our knowledge, these are some of the very best dependency accuracies on this corpus.", "labels": [], "entities": []}, {"text": "We provide an extensive exploration of our model in section 5.", "labels": [], "entities": []}, {"text": "In ablation experiments we tease apart our various contributions and modeling choices in order to shed some light on what matters in practice.", "labels": [], "entities": []}, {"text": "Neural network representations have been used in structured models before, and have also been used for syntactic parsing, alas with fairly complex architectures and constraints.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.6799688786268234}]}, {"text": "Our work on the other hand introduces a general approach for structured perceptron training with a neural network representation and achieves stateof-the-art parsing results for English.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental setup and the main results of our work.", "labels": [], "entities": []}, {"text": "We conduct our experiments on two English language benchmarks: (1) the standard Wall Street Journal (WSJ) part of the Penn Treebank ( and a more comprehensive union of publicly available treebanks spanning multiple domains.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) part", "start_pos": 80, "end_pos": 110, "type": "DATASET", "confidence": 0.9378175394875663}, {"text": "Penn Treebank", "start_pos": 118, "end_pos": 131, "type": "DATASET", "confidence": 0.9467000961303711}]}, {"text": "For the WSJ experiments, we follow standard practice and use sections 2-21 for training, section 22 for development and section 23 as the final test set.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.7332776188850403}]}, {"text": "Since there are many hyperparameters in our models, we additionally use section 24 for tuning.", "labels": [], "entities": []}, {"text": "We convert the constituency trees to Stanford style dependencies) using version 3.3.0 of the converter.", "labels": [], "entities": []}, {"text": "We use a CRF-based POS tagger to generate 5-fold jack-knifed POS tags on the training set and predicted tags on the dev, test and tune sets; our tagger gets comparable accuracy to the Stanford POS tagger () with 97.44% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.999015212059021}]}, {"text": "We report unlabeled attachment score (UAS) and labeled attachment score (LAS) excluding punctuation on predicted POS tags, as is standard for English.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8002006858587265}, {"text": "labeled attachment score (LAS)", "start_pos": 47, "end_pos": 77, "type": "METRIC", "confidence": 0.8846554358800253}]}, {"text": "For the second set of experiments, we follow the same procedure as above, but with a more diverse dataset for training and evaluation.", "labels": [], "entities": []}, {"text": "Following, we use (in addition to the WSJ), the OntoNotes corpus version 5 (), the English Web Treebank (, and the updated and corrected Question Treebank ().", "labels": [], "entities": [{"text": "WSJ", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.9289626479148865}, {"text": "OntoNotes corpus version 5", "start_pos": 48, "end_pos": 74, "type": "DATASET", "confidence": 0.9291640669107437}, {"text": "English Web Treebank", "start_pos": 83, "end_pos": 103, "type": "DATASET", "confidence": 0.9080955982208252}, {"text": "Question Treebank", "start_pos": 137, "end_pos": 154, "type": "DATASET", "confidence": 0.9134630262851715}]}, {"text": "We train on the union of each corpora's training set and test on each domain separately.", "labels": [], "entities": []}, {"text": "We refer to this setup as the \"Treebank Union\" setup.", "labels": [], "entities": [{"text": "Treebank Union\" setup", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.951773151755333}]}, {"text": "In our semi-supervised experiments, we use the corpus from as our source of unlabeled data.", "labels": [], "entities": []}, {"text": "We process it with the BerkeleyParser (), a latent variable constituency parser, and a reimplementation of ZPar (Zhang and Nivre, 2011), a transition-based parser with beam search.", "labels": [], "entities": []}, {"text": "Both parsers are included as baselines in our evaluation.", "labels": [], "entities": []}, {"text": "We select the first 10 7 tokens for which the two parsers agree as additional training data.", "labels": [], "entities": []}, {"text": "For our tri-training experiments, we re-train the POS tagger using the POS tags assigned on the unlabeled data from the Berkeley constituency parser.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.6360525190830231}]}], "tableCaptions": [{"text": " Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.7718481620152792}]}, {"text": " Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.8882075548171997}]}, {"text": " Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.", "labels": [], "entities": [{"text": "WSJ Dev  UAS", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.39365773399670917}]}, {"text": " Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9473752379417419}]}, {"text": " Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.", "labels": [], "entities": [{"text": "WSJ dev set", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.8732932806015015}, {"text": "B", "start_pos": 116, "end_pos": 117, "type": "METRIC", "confidence": 0.9942100048065186}]}]}