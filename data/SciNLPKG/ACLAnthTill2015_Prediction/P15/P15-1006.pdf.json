{"title": [{"text": "Text to 3D Scene Generation with Rich Lexical Grounding", "labels": [], "entities": [{"text": "3D Scene Generation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.6272898515065511}]}], "abstractContent": [{"text": "The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education , and robotics.", "labels": [], "entities": []}, {"text": "However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them.", "labels": [], "entities": [{"text": "text to 3D scene generation task", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.668418342868487}]}, {"text": "We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground tex-tual descriptions to physical objects.", "labels": [], "entities": []}, {"text": "Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods.", "labels": [], "entities": [{"text": "3D scene generation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.6564362645149231}]}, {"text": "We evaluate the fidelity and plau-sibility of 3D scenes generated with our grounding approach through human judgments.", "labels": [], "entities": []}, {"text": "To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "We examine the task of text to 3D scene generation.", "labels": [], "entities": [{"text": "text to 3D scene generation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6137692868709564}]}, {"text": "The ability to map descriptions of scenes to 3D geometric representations has a wide variety of applications; many creative industries use 3D scenes.", "labels": [], "entities": []}, {"text": "Robotics applications need to interpret commands referring to real-world environments, and the ability to visualize scenarios given highlevel descriptions is of great practical use in educational tools.", "labels": [], "entities": []}, {"text": "Unfortunately, 3D scene design user interfaces are prohibitively complex for novice users.", "labels": [], "entities": [{"text": "3D scene design", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6855378349622091}]}, {"text": "Prior work has shown the task remains challenging and time intensive for non-experts, even with simplified interfaces ( ).", "labels": [], "entities": []}, {"text": "* The first two authors are listed in alphabetical order.: We learn how to ground references such as \"L-shaped room\" to 3D models in a paired corpus of 3D scenes and natural language descriptions.", "labels": [], "entities": []}, {"text": "Sentence fragments in bold were identified as high-weighted references to the shown objects.", "labels": [], "entities": []}, {"text": "Language offers a convenient way for designers to express their creative goals.", "labels": [], "entities": []}, {"text": "Systems that can interpret natural descriptions to build a visual representation allow non-experts to visually express their thoughts with language, as was demonstrated by WordsEye, a pioneering work in text to 3D scene generation).", "labels": [], "entities": [{"text": "text to 3D scene generation", "start_pos": 203, "end_pos": 230, "type": "TASK", "confidence": 0.6445547461509704}]}, {"text": "WordsEye and other prior work in this area ( used manually chosen mappings between language and objects in scenes.", "labels": [], "entities": [{"text": "WordsEye", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9324974417686462}]}, {"text": "To our knowledge, we present the first 3D scene generation approach that learns from data how to map textual terms to objects.", "labels": [], "entities": [{"text": "3D scene generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7098682721455892}]}, {"text": "First, we collect a dataset of 3D scenes along with textual descriptions by people, which we contribute to the community.", "labels": [], "entities": []}, {"text": "We then train a classifier on a scene discrimination task and extract high-weight features that ground lexical terms to 3D models.", "labels": [], "entities": [{"text": "scene discrimination task", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.8077265620231628}]}, {"text": "We integrate our learned lexical groundings with a rule-based scene generation approach, and we show through a humanjudgment evaluation that the combination outperforms both approaches in isolation.", "labels": [], "entities": [{"text": "rule-based scene generation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6798022190729777}]}, {"text": "Finally, we introduce a scene similarity metric that correlates with human judgments.: Illustration of the text to 3D scene generation pipeline.", "labels": [], "entities": [{"text": "text to 3D scene generation", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.6970060229301452}]}, {"text": "The input is text describing a scene (left), which we parse into an abstract scene template representation capturing objects and relations (middle).", "labels": [], "entities": []}, {"text": "The scene template is then used to generate a concrete 3D scene visualizing the input description (right).", "labels": [], "entities": []}, {"text": "The 3D scene is constructed by retrieving and arranging appropriate 3D models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a human judgment experiment to compare the quality of generated scenes using the approaches we presented and baseline methods.", "labels": [], "entities": []}, {"text": "To evaluate whether lexical grounding improves scene generation, we need a method to arrange the chosen models into 3D scenes.", "labels": [], "entities": [{"text": "scene generation", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7495092153549194}]}, {"text": "Since 3D scene layout is not a focus of our work, we use an approach based on prior work in 3D scene synthesis and text to scene generation (), simplified by using sampling rather than a hill climbing strategy.", "labels": [], "entities": [{"text": "3D scene layout", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.620851198832194}, {"text": "3D scene synthesis", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.6800919572512308}, {"text": "text to scene generation", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.6536887511610985}]}, {"text": "Conditions We compare five conditions: {random, learned, rule, combo, human}.", "labels": [], "entities": []}, {"text": "The random condition represents a baseline which synthesizes a scene with randomly-selected models, while human represents scenes created by people.", "labels": [], "entities": []}, {"text": "The learned condition takes our learned lexical groundings, picks the four 3 most likely objects, and generates a scene based on them.", "labels": [], "entities": []}, {"text": "The rule and combo conditions use scenes generated by the rule-based approach and the combined model, respectively.", "labels": [], "entities": []}, {"text": "Descriptions We consider two sets of input descriptions: {Seeds, MTurk}.", "labels": [], "entities": []}, {"text": "The Seeds descriptions are 50 of the initial seed sentences from which workers were asked to create scenes.", "labels": [], "entities": []}, {"text": "These seed sentences were simple (e.g., There is a desk and a chair, There is a plate on a table) and did not have modifiers describing the objects.", "labels": [], "entities": []}, {"text": "The MTurk descriptions are much more descriptive and exhibit a wider variety in language (including misspellings and ungrammatical constructs).", "labels": [], "entities": []}, {"text": "Our hypothesis was that the rule-based system would perform well on the simple Seeds descriptions, but it would be insufficient for handling the complexities of the more varied MTurk descriptions.", "labels": [], "entities": []}, {"text": "For these more natural descriptions, we expected our combination model to perform better.", "labels": [], "entities": []}, {"text": "Our experimental results confirm this hypothesis.", "labels": [], "entities": []}, {"text": "shows a qualitative comparison of 3D scenes generated from example input descriptions using each of the four methods.", "labels": [], "entities": []}, {"text": "In the top row, the rule-based approach selects a CPU chassis for computer, while combo and learned select a more iconic monitor.", "labels": [], "entities": []}, {"text": "In the bottom row, the rule-based approach selects two newspapers and places them on the floor, while the combined approach correctly selects a coffee table with two newspapers on it.", "labels": [], "entities": []}, {"text": "The learned model is limited to four objects and does not have a notion of object identity, so it often duplicates objects.", "labels": [], "entities": []}, {"text": "We performed an experiment in which people rated the degree to which scenes match the textual descriptions from which they were generated.", "labels": [], "entities": []}, {"text": "Such ratings area natural way to evaluate how well our approach can generate scenes from text: in practical use, a person would provide an input description and then judge the suitability of the resulting scenes.", "labels": [], "entities": []}, {"text": "For the MTurk descriptions, we randomly sampled 100 descriptions from the development split of our dataset.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.6498241424560547}]}, {"text": "Procedure During the experiment, each participant was shown 30 pairs of scene descriptions and generated 3D scenes drawn randomly from all five conditions.", "labels": [], "entities": []}, {"text": "All participants provided 30 responses each fora total of 5040 scene-description ratings.", "labels": [], "entities": []}, {"text": "Participants were asked to rate how well the generated scene matched the input description on a 7-point Likert scale, with 1 indicating a poor match and 7 a very good one (see).", "labels": [], "entities": []}, {"text": "Ina separate task with the same experimental procedure, we asked other participants to rate the overall plausibility of each generated scene without a reference description.", "labels": [], "entities": []}, {"text": "This plausibility rating measures whether a method can generate plausible scenes irrespective of the degree to which the input description is matched.", "labels": [], "entities": []}, {"text": "We used Amazon Mechanical Turk to recruit 168 participants for rating the match of scenes to descriptions and 63 participants for rating scene plausibility.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.877951423327128}]}, {"text": "Design The experiment followed a withinsubjects factorial design.", "labels": [], "entities": []}, {"text": "The dependent measure was the Likert rating.", "labels": [], "entities": [{"text": "Likert rating", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9791481494903564}]}, {"text": "Since per-participant and per-scene variance on the rating is not accounted for by a standard ANOVA, we use a mixed effects model which can account for both fixed effects and random effects to determine the statistical signifi-: Average scene-description match ratings across sentence types and methods (95% C.I.).", "labels": [], "entities": []}, {"text": "We treat the participant and the specific scene as random effects of varying intercept, and the method condition as the fixed effect.", "labels": [], "entities": [{"text": "intercept", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9463768601417542}]}, {"text": "Results There was a significant effect of the method condition on the scene-description match rating: \u03c7 2 (4, N = 5040) = 1378.2, p < 0.001.", "labels": [], "entities": []}, {"text": "summarizes the average scene-description match ratings and 95% confidence intervals for all sentence type-condition pairs.", "labels": [], "entities": []}, {"text": "All pairwise differences between ratings were significant under Wilcoxon rank-sum tests with the BonferroniHolm correction (p < 0.05).", "labels": [], "entities": []}, {"text": "The scene plausibility ratings, which were obtained independent of descriptions, indicated that the only significant difference in plausibility was between scenes created by people (human) and all the other conditions.", "labels": [], "entities": []}, {"text": "We see that for the simple seed sentences both the rule-based and combined model approach the quality of human-created scenes.", "labels": [], "entities": []}, {"text": "However, all methods have significantly lower ratings for the more complex MTurk sentences.", "labels": [], "entities": [{"text": "MTurk sentences", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.8221343755722046}]}, {"text": "In this more challenging scenario, the combined model is closest to the manually created scenes and significantly outperforms both rule-based and learned models in isolation.", "labels": [], "entities": []}, {"text": "shows some common error cases in our system.", "labels": [], "entities": []}, {"text": "The top left scene was generated with the rule-based method, the top right with the learned method, and the bottom two with the combined approach.", "labels": [], "entities": []}, {"text": "At the top left, there is an erroneous selection of concrete object category (wood logs) for the four wood chairs reference in the input description, due to an incorrect head identification.", "labels": [], "entities": []}, {"text": "At top right, the learned model identifies the presence of brown desk and lamp but erroneously picks two desks and two lamps (since we always pick the top four objects).", "labels": [], "entities": []}, {"text": "The scene on the bottom right does not obey the expressed spatial constraints (in the corner of the room) since our system does not understand the grounding of room corner and that the top right side is not a good fit due to the door.", "labels": [], "entities": []}, {"text": "In the bottom left, incorrect coreference resolution results in two tables for There in the middle is a table, on the table is a cup.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8992941975593567}]}], "tableCaptions": [{"text": " Table 3: Average human ratings (out of 7) and  aligned scene template similarity scores.", "labels": [], "entities": [{"text": "aligned scene template similarity scores", "start_pos": 48, "end_pos": 88, "type": "METRIC", "confidence": 0.5994901537895203}]}]}