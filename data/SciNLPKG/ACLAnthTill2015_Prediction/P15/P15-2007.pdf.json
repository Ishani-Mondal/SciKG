{"title": [{"text": "Semantic Analysis and Helpfulness Prediction of Text for Online Product Reviews", "labels": [], "entities": [{"text": "Semantic Analysis", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8258055150508881}]}], "abstractContent": [{"text": "Predicting the helpfulness of product reviews is a key component of many e-commerce tasks such as review ranking and recommendation.", "labels": [], "entities": []}, {"text": "However, previous work mixed review helpfulness prediction with those outer layer tasks.", "labels": [], "entities": [{"text": "helpfulness prediction", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6149040311574936}]}, {"text": "Using non-text features, it leads to less transferable models.", "labels": [], "entities": []}, {"text": "This paper solves the problem from anew angle by hypothesizing that helpfulness is an internal property of text.", "labels": [], "entities": []}, {"text": "Purely using review text, we isolate review helpfulness prediction from its outer layer tasks, employ two interpretable semantic features, and use human scoring of helpfulness as ground truth.", "labels": [], "entities": [{"text": "review helpfulness prediction", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6626703639825186}]}, {"text": "Experimental results show that the two semantic features can accurately predict helpful-ness scores and greatly improve the performance compared with using features previously used.", "labels": [], "entities": []}, {"text": "Cross-category test further shows the models trained with semantic features are easier to be generalized to reviews of different product categories.", "labels": [], "entities": []}, {"text": "The models we built are also highly inter-pretable and align well with human annotations .", "labels": [], "entities": []}], "introductionContent": [{"text": "Product reviews have influential impact to online shopping as consumers tend to read product reviews when finalizing purchase decisions ().", "labels": [], "entities": []}, {"text": "However, a popular product usually has too many reviews fora consumer to read.", "labels": [], "entities": []}, {"text": "Therefore, reviews need to be ranked and recommended to consumers.", "labels": [], "entities": []}, {"text": "In particular, review helpfulness plays a critical role in review ranking and recommendation ().", "labels": [], "entities": []}, {"text": "The simple question \"Was this review helpful to you?\" increases an estimated $2.7B revenue to Amazon.com annually . However, existing literature solves helpfulness prediction together with its outer layer task, the review ranking ().", "labels": [], "entities": []}, {"text": "Those studies use features not contributing to helpfulness, such as date (, or features making the model less transferable, such as product type.", "labels": [], "entities": [{"text": "date", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9972339272499084}]}, {"text": "Models builtin these ways are also difficult to interpret from linguistic perspective.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary to isolate review helpfulness prediction from its outer layer tasks and formulate it as anew problem.", "labels": [], "entities": [{"text": "review helpfulness prediction", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.5654270847638448}]}, {"text": "In this way, models can be more robust and generalizable.", "labels": [], "entities": []}, {"text": "Beyond predicting whether a review is helpful, we can also understand why it is helpful.", "labels": [], "entities": []}, {"text": "In our approach, the results can also facilitate many other tasks, such as review summarization and sentiment extraction (.", "labels": [], "entities": [{"text": "review summarization", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.6453381180763245}, {"text": "sentiment extraction", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.9197752177715302}]}, {"text": "Recent NLP studies reveal the connection between text style and its properties, include readability (, informativeness () and trustworthiness (Pasternack and Roth, 2011) of text.", "labels": [], "entities": []}, {"text": "Hence, we hypothesize that helpfulness is also an underlying property of text.", "labels": [], "entities": []}, {"text": "To understand the essence of review text, we leverage existing linguistic and psychological dictionaries and represent reviews in semantic dimensions.", "labels": [], "entities": []}, {"text": "Two semantic features that are new to solving this problem, LIWC and INQUIRER (, are employed in this work.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9207473993301392}, {"text": "INQUIRER", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9401146173477173}]}, {"text": "The intuition behind is that people usually embed semantic meanings, such as emotion and reasoning, into text.", "labels": [], "entities": []}, {"text": "For example, the re-view \"With the incredible brightness of the main LED, this light is visible from a distance on a sunny day at noon. is more helpful than the review \"I ordered an iPad, I received an iPad.", "labels": [], "entities": []}, {"text": "I got exactly what I ordered which makes me satisfied.", "labels": [], "entities": []}, {"text": "Thanks!\" because the former mentions user experience and functionality of the product while the latter has emotional statements only.", "labels": [], "entities": []}, {"text": "Previous work approximates the ground truth of helpfulness from users' votes using \"X of Y approach\": if X of Y users think a review is helpful, then the helpfulness score of the review is the ratio X/Y . However, not many reviews have statistically abundant votes, i.e., a very small Y . Fewer than 20% of the reviews in Amazon Review Dataset) have at least 5 votes) while only 0.44% have 100+ votes.", "labels": [], "entities": [{"text": "Amazon Review Dataset", "start_pos": 322, "end_pos": 343, "type": "DATASET", "confidence": 0.9721870024998983}]}, {"text": "In addition, the review voting itself maybe biased).", "labels": [], "entities": []}, {"text": "Therefore, we proactively recruited human annotators and let them score the helpfulness of reviews in our dataset.", "labels": [], "entities": []}, {"text": "We model the problem of predicting review helpfulness score as a regression problem.", "labels": [], "entities": [{"text": "predicting review helpfulness", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.8127329349517822}]}, {"text": "Experimental results show that it is feasible to use text-only features to accurately predict helpfulness scores.", "labels": [], "entities": []}, {"text": "The two semantic features significantly outperform baseline features used in previous work.", "labels": [], "entities": []}, {"text": "In cross-category test, the two semantic features show good transferability.", "labels": [], "entities": []}, {"text": "To interpret the models, we analyze the semantic features and find that Psychological Process plays an important role in review text helpfulness.", "labels": [], "entities": []}, {"text": "Words reflecting thinking and understanding are more related to helpful reviews while emotional words are not.", "labels": [], "entities": []}, {"text": "Lastly, we validate the models trained on \"X of Y approach\" data on human annotated data and achieve highly correlated prediction.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two subsets of reviews are constructed from Amazon Review Dataset, which includes nearly 35 million reviews from Amazon.com between 1995 and 2013.", "labels": [], "entities": [{"text": "Amazon Review Dataset", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.9715071320533752}]}, {"text": "A subset of 696,696 reviews from 4 categories: Books, Home (home and kitchen), Outdoors and Electronics, are chosen in this research.", "labels": [], "entities": []}, {"text": "For each category, we select the top 100 products with the most reviews and then include all reviews related to the selected products for analysis.", "labels": [], "entities": []}, {"text": "Each review comes with users' helpfulness votes and hence helpfulness score can be approximated using \"X of Y approach.\"", "labels": [], "entities": []}, {"text": "Finally, 115,880 reviews, each of which has at least 5 votes, form the automatic labeled dataset.", "labels": [], "entities": []}, {"text": "In addition, we also create the human labeled dataset.", "labels": [], "entities": []}, {"text": "As mentioned earlier, the X of Y approach may not be a good approximation to helpfulness.", "labels": [], "entities": []}, {"text": "A better option is human scoring.", "labels": [], "entities": [{"text": "human scoring", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.7859629094600677}]}, {"text": "We randomly select 400 reviews outside of the automatic labeled dataset, 100 from each category.", "labels": [], "entities": []}, {"text": "Eight students annotated these reviews in a fashion similar to that in () by assigning real-value scores (\u2208) to each review.", "labels": [], "entities": []}, {"text": "Review text was the only information given to them.", "labels": [], "entities": [{"text": "Review text", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7855059504508972}]}, {"text": "The average helpfulness score of all valid annotations is used as the ground truth for each review.", "labels": [], "entities": []}, {"text": "We have released the human annotation data at https://sites.google.com/ site/forrestbao/acl_data.tar.bz2 .  Up to this point, we are very interested in first whether a prediction model learned for one category can be generalized to anew category, and second what elements make a review helpful.", "labels": [], "entities": []}, {"text": "In other words, we want to know the robustness of our approach and the underlying reasons.", "labels": [], "entities": []}, {"text": "In this section we will evaluate the effectiveness of each of the features as well as the combination of them.", "labels": [], "entities": []}, {"text": "For convenience, we use Fusion Semantic to denote the combination of GALC, LIWC and INQUIRER, and Fusion All to denote the combination of all features.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.8431981205940247}, {"text": "INQUIRER", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.8621320128440857}]}, {"text": "Because STR and UGR are widely used in previous work, we use them as two baselines.", "labels": [], "entities": [{"text": "STR", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.38121506571769714}, {"text": "UGR", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.7618162035942078}]}, {"text": "GALC has been introduced for this task as an emotion feature before, so we use it as the third baseline.", "labels": [], "entities": []}, {"text": "STR, URG and GALC are used as 3 baselines.", "labels": [], "entities": [{"text": "STR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5494441986083984}, {"text": "URG", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9198966026306152}, {"text": "GALC", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.775239884853363}]}, {"text": "For predicting helpfulness scores, we use SVM regressor with RBF kernel provided by.", "labels": [], "entities": []}, {"text": "Two kinds of labels are used: automatic labels obtained in \"X of Y approach\" from votes, and human labels made by human annotators.", "labels": [], "entities": []}, {"text": "Performance is evaluated by Root Mean Square Error (RMSE) and Pearson's correlation coefficients.", "labels": [], "entities": [{"text": "Root Mean Square Error (RMSE)", "start_pos": 28, "end_pos": 57, "type": "METRIC", "confidence": 0.8739915405000959}, {"text": "Pearson's correlation", "start_pos": 62, "end_pos": 83, "type": "METRIC", "confidence": 0.8765747547149658}]}, {"text": "Ten-fold cross-validation is performed for all experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of Reviews for Each Category", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9493301510810852}]}, {"text": " Table 2: RMSE (the lower the better) using auto- matic labels", "labels": [], "entities": [{"text": "RMSE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9147492051124573}]}, {"text": " Table 3: Correlation coefficients (the higher the  better) using automatic labels. All correlations are  highly significant, with p < 0.001.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9413136839866638}]}, {"text": " Table 4: Correlation coefficients between pre- dicted scores and human annotation, *: p < 0.001.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9785086512565613}]}]}