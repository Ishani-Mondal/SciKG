{"title": [{"text": "WikiKreator: Improving Wikipedia Stubs Automatically", "labels": [], "entities": [{"text": "Improving Wikipedia Stubs Automatically", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.6150370836257935}]}], "abstractContent": [{"text": "Stubs on Wikipedia often lack comprehensive information.", "labels": [], "entities": []}, {"text": "The huge cost of editing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.880389392375946}, {"text": "Wikipedia", "start_pos": 130, "end_pos": 139, "type": "DATASET", "confidence": 0.9138208627700806}]}, {"text": "In this work, we present WikiKreator, a system that is capable of generating content automatically to improve existing stubs on Wikipedia.", "labels": [], "entities": []}, {"text": "The system has two components.", "labels": [], "entities": []}, {"text": "First, a text classifier built using topic distribution vectors is used to assign content from the web to various sections on a Wikipedia article.", "labels": [], "entities": []}, {"text": "Second, we propose a novel abstractive summariza-tion technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs.", "labels": [], "entities": []}, {"text": "Experiments show that WikiKreator is capable of generating well-formed informative content.", "labels": [], "entities": []}, {"text": "Further, automatically generated content from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the effectiveness of our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Wikipedia provides comprehensive information on various topics.", "labels": [], "entities": []}, {"text": "However, a significant percentage of the articles are stubs 1 that require extensive effort in terms of adding and editing content to transform them into complete articles.", "labels": [], "entities": []}, {"text": "Ideally, we would like to create an automatic Wikipedia content generator, which can generate a comprehensive overview on any topic using available information from the web and append the generated content to the stubs.", "labels": [], "entities": []}, {"text": "Addition of automatically generated content can provide a useful start-ing point for contributors on Wikipedia, which can be improved upon later.", "labels": [], "entities": []}, {"text": "Several approaches to automatically generate Wikipedia articles have been explored).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, all the above mentioned methods identify information sources from the web using keywords and directly use the most relevant excerpts in the final article.", "labels": [], "entities": []}, {"text": "Information from the web cannot be directly copied into Wikipedia due to copyright violation issues ().", "labels": [], "entities": []}, {"text": "Further, keyword search does not always satisfy information requirements ().", "labels": [], "entities": [{"text": "keyword search", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.8601954281330109}]}, {"text": "To address the above-mentioned issues, we present WikiKreator -a system that can automatically generate content for Wikipedia stubs.", "labels": [], "entities": []}, {"text": "First, WikiKreator does not operate using keyword search.", "labels": [], "entities": []}, {"text": "Instead, we use a classifier trained using topic distribution features to identify relevant content for the stub.", "labels": [], "entities": []}, {"text": "Topic-distribution features are more effective than keyword search as they can identify relevant content based on word distributions ().", "labels": [], "entities": []}, {"text": "Second, we propose a novel abstractive summarization technique to summarize content from multiple snippets of relevant information.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.679997444152832}, {"text": "summarize content from multiple snippets of relevant information", "start_pos": 66, "end_pos": 130, "type": "TASK", "confidence": 0.8206135183572769}]}, {"text": "Figure 1 shows a stub that we attempt to improve using WikiKreator.", "labels": [], "entities": []}, {"text": "Generally, in stubs, only the introductory content is available; other sections (s 1 , ..., s r ) are absent.", "labels": [], "entities": []}, {"text": "The stub also belongs to several categories (C 1 ,C 2 , etc.", "labels": [], "entities": []}, {"text": "In this work, we address the following research question: Given the introductory content, the title of the stub and information on the categories -how can we transform the stub into a com-: Overview of our word-graph based generation (left) to populate Wikipedia template (right) prehensive Wikipedia article?", "labels": [], "entities": []}, {"text": "Our proposed approach consists of two stages.", "labels": [], "entities": []}, {"text": "First, a text classifier assigns content retrieved from the web into specific sections of the Wikipedia article.", "labels": [], "entities": [{"text": "text classifier assigns content retrieved from the web", "start_pos": 9, "end_pos": 63, "type": "TASK", "confidence": 0.8062906339764595}]}, {"text": "We train the classifier using a set of articles within the same category.", "labels": [], "entities": []}, {"text": "Currently, we limit the system to learn and assign content into the 10 most frequent sections in any given category.", "labels": [], "entities": []}, {"text": "The training set includes content from the most frequent sections as instances and their corresponding section titles as the class labels.", "labels": [], "entities": []}, {"text": "We extract topic distribution vectors using Latent Dirichlet Allocation (LDA) ( and use the features to train a Random Forest (RF) Classifier ().", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 44, "end_pos": 77, "type": "METRIC", "confidence": 0.773449495434761}]}, {"text": "To gather web content relevant to the stub, we formulate queries and retrieve top 20 search results (pages) from Google.", "labels": [], "entities": []}, {"text": "We use boilerplate detection ( to retain the important excerpts (text elements) from the pages.", "labels": [], "entities": [{"text": "boilerplate detection", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.7751224935054779}]}, {"text": "The RF classifier classifies the excerpts into one of the most frequent classes (section titles).", "labels": [], "entities": [{"text": "RF classifier classifies the excerpts", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.6271314680576324}]}, {"text": "Second, we develop a novel Integer Linear Programming (ILP) based abstractive summarization technique to generate text from the classified content.", "labels": [], "entities": []}, {"text": "Previous work only included the most informative excerpt in the article; in contrast, our abstractive summarization approach minimizes loss of information that should ideally be in an Wikipedia article by fusing content from several sentences.", "labels": [], "entities": []}, {"text": "As shown in, we construct a word-graph) using all the sentences (W G 1 ) assigned to a specific class (Epidemiology) by the classifier.", "labels": [], "entities": []}, {"text": "Multiple paths (sentences) between the start and end nodes in the graph are generated (W G 2 ).", "labels": [], "entities": []}, {"text": "We represent the generated paths as variables in the ILP problem.", "labels": [], "entities": []}, {"text": "The coefficients of each variable in the objective function of the ILP problem is obtained by combining the information score and the linguistic quality score of the path.", "labels": [], "entities": []}, {"text": "We introduce several constraints into our ILP model.", "labels": [], "entities": []}, {"text": "We limit the summary for each section to a maximum of 5 sentences.", "labels": [], "entities": [{"text": "summary", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9859064817428589}]}, {"text": "Further, we avoid redundant sentences in the summary that carry similar information.", "labels": [], "entities": []}, {"text": "The solution to the optimization problem decides the paths that are selected in the final section summary.", "labels": [], "entities": []}, {"text": "For example, in, the final paths determined by the ILP solution, -1 and 2 in W G 2 , are assigned to a section (s r ), where (s r ) is the section title Epidemiology.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this work is the first to address the issue of generating content automatically to transform Wikipedia stubs into comprehensive articles.", "labels": [], "entities": []}, {"text": "Further, we address the issue of abstractive text summarization for Wikipedia content generation.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.6290536721547445}, {"text": "Wikipedia content generation", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.6671799222628275}]}, {"text": "We evaluate our approach by generating articles in three different categories: Diseases and Disorders 3 , American Mathematicians  We also analyze reviewer reactions, by appending content into several stubs on Wikipedia, most of which (\u223c77%) have been retained by reviewers.", "labels": [], "entities": [{"text": "American Mathematicians", "start_pos": 106, "end_pos": 129, "type": "DATASET", "confidence": 0.9149036705493927}]}], "datasetContent": [{"text": "To evaluate the effectiveness of our proposed technique, we conduct several experiments.", "labels": [], "entities": []}, {"text": "First, we evaluate our content generation approach by generating content for comprehensive articles that already exist on Wikipedia.", "labels": [], "entities": []}, {"text": "Second, we analyze reviewer reactions on our system generated articles by adding content to several stubs on Wikipedia.", "labels": [], "entities": []}, {"text": "Our experiments were designed to answer the following questions: (i)What are the optimal number of topic distribution features for each category?", "labels": [], "entities": []}, {"text": "What are the classification accuracies in each domain?", "labels": [], "entities": []}, {"text": "(ii)To what extent can our technique generate the content for articles automatically?", "labels": [], "entities": []}, {"text": "(iii)What are the general reviewer reactions on Wikipedia and what percentage of automatically generated content on Wikipedia is retained?", "labels": [], "entities": []}, {"text": "Dataset Construction: As mentioned earlier in Section 3.1, we crawl Wikipedia articles by traversing the category graph.", "labels": [], "entities": [{"text": "Dataset Construction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.631613478064537}]}, {"text": "Articles that contain at least three sections were included in the training set; other articles having lesser number of sections are generally labeled as stubs and hence not used for training.", "labels": [], "entities": []}, {"text": "shows the most frequent sections in each category.", "labels": [], "entities": []}, {"text": "Further, shows the total number of articles retrieved from Wikipedia in each category.", "labels": [], "entities": []}, {"text": "The total number of instances are also shown.", "labels": [], "entities": []}, {"text": "The number of instances denotes the total number of the most frequent sections in each category.", "labels": [], "entities": []}, {"text": "As can be seen from the table, the number of instances is higher than the number of articles only in case of the category on diseases.", "labels": [], "entities": []}, {"text": "This implies that there are generally more common sections in the diseases category than the other categories.", "labels": [], "entities": []}, {"text": "In each category, the content from only the most frequent sections were used to generate a topic model.", "labels": [], "entities": []}, {"text": "The topic model is further used to infer topic distribution vectors from the training instances.", "labels": [], "entities": []}, {"text": "We used the MALLET toolkit) for generating topic distribution vectors and the WEKA package for the classification tasks.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.6916335225105286}, {"text": "WEKA package", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.7431636154651642}]}, {"text": "Optimal number of topics: The LDA model requires a pre-defined number of topics.", "labels": [], "entities": []}, {"text": "We experiment with several values of the number of topics ranging from 10 to 100.", "labels": [], "entities": []}, {"text": "The topic distribution features of the content of the instances are used to train a Random Forest Classifier with the corresponding section titles as the class labels.", "labels": [], "entities": []}, {"text": "As can be seen in the We classify web excerpts using the best performing classifiers trained using the optimal number of topic features in each category.", "labels": [], "entities": []}, {"text": "Classification performance: We use 10-fold cross validation to evaluate the accuracy of our classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9991551637649536}]}, {"text": "According to the F-Scores, our classifier (LDA-RF) performs similarly in the categories on Diseases and US Software companies.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.7734121680259705}]}, {"text": "However, the accuracy is lower in the American Mathematicians category.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9997237324714661}, {"text": "American Mathematicians category", "start_pos": 38, "end_pos": 70, "type": "DATASET", "confidence": 0.9276208082834879}]}, {"text": "We also experimented with a baseline classifier, that is trained on TFIDF features (upto trigrams).", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.7617839574813843}]}, {"text": "A Support vector machine ( classifier obtained the best performance using the TFIDF features.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.6736595034599304}]}, {"text": "The baseline system is referred to as SVM-WV.", "labels": [], "entities": [{"text": "SVM-WV", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.9367493391036987}]}, {"text": "We experimented with several other combinations of classifiers; however, we show only the best performing systems using the LDA and TFIDF features.", "labels": [], "entities": []}, {"text": "As can be seen from the, our classifier (LDA-RF) outperforms SVM-WV significantly in all the domains.", "labels": [], "entities": []}, {"text": "SVM-WV performs better in the category on diseases than the other two categories and the performance is comparable to (LDA-RF).", "labels": [], "entities": [{"text": "SVM-WV", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8860195279121399}]}, {"text": "The diseases category has more uniformity in terms of the section titles, hence specific words or phrases characterize the sections well.", "labels": [], "entities": []}, {"text": "In contrast, word distributions (LDA) work significantly better than TFIDF features in the other two categories.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of our content generation process, we generated the content of 500 randomly selected articles that already exist on Wikipedia in each of the categories.", "labels": [], "entities": []}, {"text": "We compare WikiKreator's output against the current content of those articles on Wikipedia using ROUGE).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9795529246330261}]}, {"text": "ROUGE matches N-gram sequences that exist in both the system generated articles and the original Wikipedia articles (gold standard).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.926934003829956}]}, {"text": "We also compare WikiKreator's output with an existing Wikipedia generation system of that employs a perceptron learning framework to learn topic specific extractors.", "labels": [], "entities": []}, {"text": "Queries devised using the conjunction of the document title and the section title were used to obtain excerpts from the web using a search engine, which were used in the perceptron model.", "labels": [], "entities": []}, {"text": "In Perceptron, the most important sections in the category was determined using a bisectioning algorithm to identify clusters of similar sections.", "labels": [], "entities": []}, {"text": "To understand the effectiveness of our abstractive summarizer, we design a system (Extractive) that uses an extractive summarization module.", "labels": [], "entities": []}, {"text": "In Extractive, we use LexRank () as the summarizer instead of our ILP based abstractive summarization model.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8857859969139099}]}, {"text": "We restrict the extractive summaries to 5 sentences for accurate comparison of both the systems.", "labels": [], "entities": []}, {"text": "The same content was received as input from the classifier by the Extractive as well as our ILP-based system.", "labels": [], "entities": [{"text": "Extractive", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.9129201769828796}]}, {"text": "As can be seen from the, the ROUGE scores obtained by WikiKreator is higher than that of the other comparable systems in all the categories.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9980950951576233}, {"text": "WikiKreator", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9003331065177917}]}, {"text": "The higher ROUGE scores imply that WikiKreator is generally able to retrieve useful information from the web, synthesize them and present the important information in the article.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9955400824546814}]}, {"text": "However, it may also be noted that the Extractive system outperforms the Perceptron framework.", "labels": [], "entities": [{"text": "Extractive system", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.7752542197704315}, {"text": "Perceptron framework", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.9158087074756622}]}, {"text": "Summarization from multiple sources generates more informative summaries and is more effective than 'selection' of the most informative excerpt, which is often inadequate due to potential loss of information.", "labels": [], "entities": []}, {"text": "WikiKreator performs better than the extractive system on all the categories.", "labels": [], "entities": [{"text": "WikiKreator", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8673040270805359}]}, {"text": "Our ILPbased abstractive summarization system fuses and selects content from multiple sentences, thereby aggregating information successfully from multiple sources.", "labels": [], "entities": [{"text": "ILPbased abstractive summarization", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6074673235416412}]}, {"text": "In contrast, LexRank 'extracts' the top 5 sentences that results in some information loss.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.903526246547699}]}, {"text": "Analysis of Wikipedia Reviews: To compare our method with the other techniques, it is necessary to generate content and append to Wikipedia stubs using all the techniques.", "labels": [], "entities": []}, {"text": "However, recent work on article generation ( has already shown that content directly copied from web sources cannot be used on Wikipedia.", "labels": [], "entities": [{"text": "article generation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7788175642490387}]}, {"text": "Further, bots using copyrighted content might be banned and real-users would have to read sub-standard articles due to the internal tests we perform.", "labels": [], "entities": []}, {"text": "Due to the above mentioned reasons, we appended content generated only using our abstractive summarization technique.", "labels": [], "entities": []}, {"text": "We published content generated by WikiKreator on Wikipedia and appended the content to 40 randomly selected stubs.", "labels": [], "entities": []}, {"text": "As can be seen from the, the content generated using our system was generally accepted by the reviewers.", "labels": [], "entities": []}, {"text": "Half of the articles did not require any further changes; while in 6 cases (15%) the reviewers asked us to fix grammatical issues.", "labels": [], "entities": []}, {"text": "In 9 stubs, the reliability of the cited references was questioned.", "labels": [], "entities": [{"text": "reliability", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9956417083740234}]}, {"text": "Information sources on Wikipedia need to satisfy a minimum reliability standard, which our algorithm currently cannot determine.", "labels": [], "entities": []}, {"text": "On an average, 3 edits were made to the Wikipedia articles that we generated.", "labels": [], "entities": []}, {"text": "In general, there is an average increase in the content size of the stubs that we edited showing that our method is capable of producing content that generally satisfy Wikipedia criterion.", "labels": [], "entities": []}, {"text": "Analysis of section assignment: We manually inspected generated content of 20 articles in each category.", "labels": [], "entities": [{"text": "section assignment", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8865187764167786}]}, {"text": "Generated summaries are both informative and precise.", "labels": [], "entities": []}, {"text": "However, in certain cases, the generated section title is not the same as the section title in the original Wikipedia article.", "labels": [], "entities": []}, {"text": "For example, we generated content for the section \"Causes\" for the article on Middle East Respiratory Syndrome (MERS) 12 : Milk or meat may play a role in the transmission of the virus . People should avoid drinking raw camel milk or meat that has not been properly cooked . There is growing evidence that contact with live camels or meat is causing MERS.", "labels": [], "entities": [{"text": "Middle East Respiratory Syndrome (MERS) 12", "start_pos": 78, "end_pos": 120, "type": "TASK", "confidence": 0.8058006465435028}]}, {"text": "The corresponding content on the Wikipedia is in a section labeled as \"Transmission\".", "labels": [], "entities": [{"text": "Transmission", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.9414964318275452}]}, {"text": "Section titles at the topmost level in a category might not be relevant to all the articles.", "labels": [], "entities": []}, {"text": "Instead of using a topdown approach of traversing the category-graph, we can also use a bottom-up approach where we learn from all the categories that an article belongs to.", "labels": [], "entities": []}, {"text": "For example, the article on MERS belongs to two categories: Viral respiratory tract infection and Zoonoses.", "labels": [], "entities": [{"text": "MERS", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.7016090154647827}, {"text": "Viral respiratory tract infection", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.6796384453773499}, {"text": "Zoonoses", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.7890584468841553}]}, {"text": "Training using all the categories will allow context-driven section identification.", "labels": [], "entities": [{"text": "section identification", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8450298607349396}]}, {"text": "Most frequent sections at a higher level in the category graph might not always be relevant to all the articles within a category.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data characteristics of three domains on Wikipedia", "labels": [], "entities": []}, {"text": " Table 3: Classification: Weighted F-Scores", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.7788199186325073}]}, {"text": " Table 5: Statistics of Wikipedia generation", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.9386227130889893}]}]}