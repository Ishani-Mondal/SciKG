{"title": [{"text": "Machine Comprehension with Syntax, Frames, and Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework.", "labels": [], "entities": [{"text": "MCTest question answering task", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.8213869035243988}]}, {"text": "We achieve the best results we are aware of on this dataset, outperforming concurrently-published results.", "labels": [], "entities": []}, {"text": "These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent question answering (QA) systems have focused on open-domain factoid questions, relying on knowledge bases like Freebase () or large corpora of unstructured text.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.8978487133979798}]}, {"text": "While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.6397490998109182}]}, {"text": "Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.7227693051099777}]}, {"text": "There is also recent work on QA based on synthetic text describing events in adventure games (.", "labels": [], "entities": []}, {"text": "Synthetic text provides a cleanroom environment for evaluating QA systems, and has spurred development of powerful neural architectures for complex reasoning.", "labels": [], "entities": []}, {"text": "However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression.", "labels": [], "entities": [{"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7287473976612091}]}, {"text": "In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage.", "labels": [], "entities": []}, {"text": "Several comprehension tasks have been developed, including Remedia (),, and the QA4MRE textual question answering tasks in the CLEF evaluations.", "labels": [], "entities": [{"text": "QA4MRE textual question answering", "start_pos": 80, "end_pos": 113, "type": "TASK", "confidence": 0.6527222320437431}, {"text": "CLEF evaluations", "start_pos": 127, "end_pos": 143, "type": "DATASET", "confidence": 0.8703259229660034}]}, {"text": "We consider the Machine Comprehension of Text dataset (MCTest;, a set of human-authored fictional stories with associated multiple-choice questions.", "labels": [], "entities": []}, {"text": "Knowledge bases and web corpora are not useful for this task, and answers are typically expressed just once in each story.", "labels": [], "entities": []}, {"text": "While simple baselines presented by answer over 60% of questions correctly, many of the remaining questions require deeper analysis.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of dependency syntax, frame semantics, word embeddings, and coreference for improving performance on MCTest.", "labels": [], "entities": []}, {"text": "Syntax, frame semantics, and coreference are essential for understanding who did what to whom.", "labels": [], "entities": [{"text": "coreference", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9614638686180115}, {"text": "understanding who did what to whom", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.814682811498642}]}, {"text": "Word embeddings address variation in word choice between the stories and questions.", "labels": [], "entities": []}, {"text": "Our added features achieve the best results we are aware of on this dataset, outperforming concurrently-published results).", "labels": [], "entities": []}], "datasetContent": [{"text": "MCTest splits its stories into train, development, and test sets.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8953043818473816}]}, {"text": "The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV).", "labels": [], "entities": [{"text": "MCtest DEV", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.9208356440067291}, {"text": "MC160", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.9508544206619263}, {"text": "TRAIN", "start_pos": 185, "end_pos": 190, "type": "METRIC", "confidence": 0.9413511753082275}]}, {"text": "We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer \u03bb and choose the best feature set.", "labels": [], "entities": [{"text": "DEV", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.5524938702583313}]}, {"text": "We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9639008045196533}, {"text": "MC160", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9441904425621033}]}, {"text": "We use SEMAFOR () for frame semantic parsing and the latest Stanford dependency parser) as our dependency parser.", "labels": [], "entities": [{"text": "frame semantic parsing", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6979414423306783}]}, {"text": "We use the Stanford rule-based system for coreference resolution (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9816382527351379}]}, {"text": "We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.", "labels": [], "entities": []}, {"text": "We denote the frame semantic features by F and the syntactic features by S. We use superscripts wand c to indicate the use of embeddings and coreference fora particular feature set.", "labels": [], "entities": []}, {"text": "To minimize the loss, we use the miniFunc package in MATLAB with LBFGS.", "labels": [], "entities": []}, {"text": "The accuracy of different feature sets on DEV is given in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994031190872192}, {"text": "DEV", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.919671893119812}]}, {"text": "The boldface results correspond to the best feature set combination chosen by evaluating on In this case, the feature dimensionality is 29, which includes 4 bag-of-words features, 1 distance feature, 12 frame semantic features, and with the remaining being syntactic features.", "labels": [], "entities": []}, {"text": "After choosing the best feature set on DEV, we then evaluate our system on TEST.", "labels": [], "entities": [{"text": "DEV", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9630154371261597}, {"text": "TEST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.7348564267158508}]}, {"text": "Negations: in preliminary experiments, we found that our system suffered with negation questions, so we developed a simple heuristic to deal with them.", "labels": [], "entities": []}, {"text": "If a question is identified as negation, we then negate the final score for each candidate answer.", "labels": [], "entities": []}, {"text": "The final test results are shown in.", "labels": [], "entities": []}, {"text": "We first compare to results from prior work (.", "labels": [], "entities": []}, {"text": "Their first result uses a sliding window with the bag-of-words feature B described in Sec.", "labels": [], "entities": []}, {"text": "3; this system is called \"Baseline 1\" (B1).", "labels": [], "entities": []}, {"text": "They then add the distance feature D, also described in Sec.", "labels": [], "entities": [{"text": "distance feature D", "start_pos": 18, "end_pos": 36, "type": "METRIC", "confidence": 0.9244108001391093}]}, {"text": "3. The combined system, which uses B and D, is called \"Baseline 2\" (B2).", "labels": [], "entities": []}, {"text": "Their third result adds a rich textual entailment system to B2; it is referred to as B2+RTE.", "labels": [], "entities": []}, {"text": "We also compare to concurrently-published results (.", "labels": [], "entities": []}, {"text": "We report accuracies for all questions as well as separately for the two types: those that are answerable with a single sentence from the passage (\"Single\") and those that require multiple sentences (\"Multiple\").", "labels": [], "entities": [{"text": "accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9946362376213074}]}, {"text": "We see gains inaccuracy of 6% absolute compared to the B2+RTE baseline and also outperform concurrently-published results.", "labels": [], "entities": [{"text": "B2+RTE baseline", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.6357771083712578}]}, {"text": "Even though our system only explicitly uses a single sentence from the passage when choosing an answer, we improve baseline accuracy for both single-sentence and multiplesentence questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9458122253417969}]}, {"text": "8 score for all four candidate answers, then we get partial credit of 0.25 for this question.", "labels": [], "entities": []}, {"text": "68.72: Ablation study of feature types on the dev set.", "labels": [], "entities": []}, {"text": "We also measure the contribution of each feature set by deleting it from the full feature set.", "labels": [], "entities": []}, {"text": "These ablation results are shown in.", "labels": [], "entities": [{"text": "ablation", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9808564186096191}]}, {"text": "We find that frame semantic and syntax features contribute almost equally, and using word embeddings contributes slightly more than coreference information.", "labels": [], "entities": []}, {"text": "If we delete the bag-of-words and distance features, then accuracy drops significantly, which suggests that in MCTest, simple surface-level similarity features suffice to answer a large portion of questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9995068311691284}]}], "tableCaptions": [{"text": " Table 1: Accuracy on DEV.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9924591183662415}, {"text": "DEV", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.7589338421821594}]}, {"text": " Table 2. We  first compare to results from prior work (", "labels": [], "entities": []}, {"text": " Table 2: Accuracy comparison of published results on test sets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9927878975868225}]}, {"text": " Table 3: Ablation study of feature types on the dev set.", "labels": [], "entities": []}]}