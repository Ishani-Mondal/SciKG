{"title": [], "abstractContent": [{"text": "Syntactic annotation is a hard task, but it can be made easier by allowing annotators flexibility to leave aspects of a sentence underspecified.", "labels": [], "entities": [{"text": "Syntactic annotation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9260221421718597}]}, {"text": "Unfortunately, partial annotations are not typically directly usable for training parsers.", "labels": [], "entities": []}, {"text": "We describe a method for imputing missing dependencies from sentences that have been partially annotated using the Graph Fragment Language, such that a standard dependency parser can then be trained on all annotations.", "labels": [], "entities": []}, {"text": "We show that this strategy improves performance over not using partial annotations for English, Chinese, Portuguese and Kin-yarwanda, and that performance competitive with state-of-the-art unsupervised and weakly-supervised parsers can be reached with just a few hours of annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguistically annotated data is produced for many purposes in many contexts.", "labels": [], "entities": []}, {"text": "It typically requires considerable effort, particularly for language documentation efforts in which tooling, data, and expertise in the language are scarce.", "labels": [], "entities": []}, {"text": "The challenge presented by this scarcity is compounded when doing deeper analysis, such as syntactic structure, which typically requires greater expertise and existing tooling.", "labels": [], "entities": []}, {"text": "In such scenarios, unsupervised approaches area tempting strategy.", "labels": [], "entities": []}, {"text": "While the performance of unsupervised dependency parsing has improved greatly since Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7360811531543732}]}, {"text": "Additionally, they typically require large amounts of raw data.", "labels": [], "entities": []}, {"text": "While this is not a problem for some languages, many of the world's languages do not have a clean, digitized corpus available.", "labels": [], "entities": []}, {"text": "For instance, the approach of is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.9933450520038605}]}, {"text": "The approach of requires extra unlabeled texts to estimate parameters.", "labels": [], "entities": []}, {"text": "Another strategy is to exploit small amounts of supervision or knowledge.", "labels": [], "entities": []}, {"text": "use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages.", "labels": [], "entities": []}, {"text": "use web mark-up and punctuation as additional annotations.", "labels": [], "entities": []}, {"text": "Alternatively, one could try to obtain actual dependency annotations cheaply.", "labels": [], "entities": []}, {"text": "We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices ().", "labels": [], "entities": [{"text": "Graph Fragment Language (GFL)", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.6897687713305155}]}, {"text": "GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions.", "labels": [], "entities": [{"text": "GFL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8780374526977539}]}, {"text": "The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers.", "labels": [], "entities": []}, {"text": "For example, found higher-level sentence constituents to be more informative for learning parsers than lower-level ones.", "labels": [], "entities": []}, {"text": "To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy.", "labels": [], "entities": []}, {"text": "The first stage completes the partial GFL annotations by adapting a Gibbs tree sampler).", "labels": [], "entities": []}, {"text": "The GFL annotations constrain the tree sampling space by using both dependencies and the constituent boundaries they express.", "labels": [], "entities": []}, {"text": "The system performs missing dependency arc imputation using Gibbs sampling -we refer to this approach as the Gibbs Parse Completer 2 (GPC).", "labels": [], "entities": [{"text": "Gibbs Parse Completer 2 (GPC)", "start_pos": 109, "end_pos": 138, "type": "METRIC", "confidence": 0.7475403504712241}]}, {"text": "The second stage uses the full dependencies output by the GPC to train Turbo Parser (, and evaluation is done with this trained model on unseen sentences.", "labels": [], "entities": [{"text": "GPC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9470496773719788}]}, {"text": "In simulation experiments for English, Chinese and Portuguese, we show that the method gracefully degrades when applied to training corpora with increasing percentages of the gold training dependencies removed.", "labels": [], "entities": []}, {"text": "We also do actual GFL annotations for those languages plus Kinyarwanda, and show that using the GPC to fill in the missing dependencies after two hours of annotation enables Turbo Parser to obtain 2-6% better absolute performance than when it has to throw incomplete annotations out.", "labels": [], "entities": []}, {"text": "Furthermore, the gains are even greater with less annotation time and it never hurts to use the GPC-so an annotation project can pursue a partial annotation strategy without undermining the utility of the work for parser training.", "labels": [], "entities": []}, {"text": "This strategy has the further benefit of needing only a small number of sentences-in our case, under 100 sentences annotated in a 2-4 hour window.", "labels": [], "entities": []}, {"text": "Furthermore, it relies on no outside tools or corpora other than a part-of-speech tagger; a resource that can be built with two hours of annotation time ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Two Hour GFL Annotation Statistics", "labels": [], "entities": [{"text": "GFL Annotation", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.49195294082164764}]}, {"text": " Table 3: Results with simulated partial annota- tions, GFL-GPC-X indicates X percent of depen- dencies were retained.", "labels": [], "entities": [{"text": "GFL-GPC-X", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9846685528755188}]}, {"text": " Table 4: English results compared to previous un- supervised and weakly-supervised methods.", "labels": [], "entities": []}, {"text": " Table 5: Non-English results summary", "labels": [], "entities": []}]}