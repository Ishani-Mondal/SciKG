{"title": [{"text": "Thread-Level Information for Comment Classification in Community Question Answering", "labels": [], "entities": [{"text": "Thread-Level", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8662394881248474}, {"text": "Comment Classification in Community Question Answering", "start_pos": 29, "end_pos": 83, "type": "TASK", "confidence": 0.7026757001876831}]}], "abstractContent": [{"text": "Community Question Answering (cQA) is anew application of QA in social contexts (e.g., fora).", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7348762154579163}]}, {"text": "It presents new interesting challenges and research directions, e.g., exploiting the dependencies between the different comments of a thread to select the best answer fora given question.", "labels": [], "entities": []}, {"text": "In this paper, we explored two ways of mod-eling such dependencies: (i) by designing specific features looking globally at the thread; and (ii) by applying structure prediction models.", "labels": [], "entities": []}, {"text": "We trained and evaluated our models on data from SemEval-2015 Task 3 on Answer Selection in cQA.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.8310938775539398}, {"text": "cQA", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.771379292011261}]}, {"text": "Our experiments show that: (i) the thread-level features consistently improve the performance fora variety of machine learning models, yielding state-of-the-art results; and (ii) sequential dependencies between the answer labels captured by structured prediction models are not enough to improve the results, indicating that more information is needed in the joint model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community Question Answering (cQA) is an evolution of atypical QA setting put in a Web forum context, where user interaction is enabled, without much restrictions on who can post and who can answer a question.", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7322944055000941}]}, {"text": "This is a powerful mechanism, which allows users to freely ask questions and expect some good, honest answers.", "labels": [], "entities": []}, {"text": "Unfortunately, a user has to go through all possible answers and to make sense of them.", "labels": [], "entities": []}, {"text": "It is often the case that many answers are only loosely related to the actual question, and some even change the topic.", "labels": [], "entities": []}, {"text": "This is especially common for long threads where, as the thread progresses, users start talking to each other, instead of trying to answer the initial question.", "labels": [], "entities": []}, {"text": "This is areal problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users' information needs.", "labels": [], "entities": []}, {"text": "Thus, finding the desired information in along list of answers might be very time-consuming.", "labels": [], "entities": []}, {"text": "The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking.", "labels": [], "entities": [{"text": "passage reranking", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.806885302066803}]}, {"text": "Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see () for detail.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved fora question.", "labels": [], "entities": []}, {"text": "This is mainly due to the loose dependencies between the different answer passages in standard QA.", "labels": [], "entities": []}, {"text": "In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.991230845451355}]}, {"text": "To test our hypothesis about the usefulness of thread-level information, we used a publicly available dataset, recently developed for the.", "labels": [], "entities": []}, {"text": "Subtask A in that challenge asks to identify the posts in the answer thread that answer the question well vs. those that can be potentially useful to the user vs. those that are just bad or useless.", "labels": [], "entities": []}, {"text": "We model the thread-level dependencies in two different ways: (i) by designing specific features that are able to capture the dependencies between the answers in the same thread; and (ii) by exploiting the sequential organization of the output labels for the complete thread.", "labels": [], "entities": []}, {"text": "Q: Can I obtain Driving License my QID is written Employee A1 the word employee is a general term that refers to all the staff in your company either the manager, secretary up to the lowest position or whatever positions they have.", "labels": [], "entities": []}, {"text": "you are all considered employees of your company.", "labels": [], "entities": []}, {"text": "A2 your qid should specify what is the actual profession you have.", "labels": [], "entities": [{"text": "A2", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9597895741462708}]}, {"text": "i think for me, your chances to have a drivers license is low.", "labels": [], "entities": []}, {"text": "A3 dear richard, his asking if he can obtain.", "labels": [], "entities": [{"text": "A3", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9640914797782898}]}, {"text": "means he have the driver license A4 Slim chance . .", "labels": [], "entities": [{"text": "A4", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9598433971405029}]}, {"text": "For the latter, we used the usual extensions of Logistic Regression and SVM to linear-chain models such as CRF and SVM hmm . The results clearly show that the thread-level features are important, providing consistent improvement for all our learning models.", "labels": [], "entities": []}, {"text": "In contrast, the linear-chain models fail to exploit the sequential dependencies between nearby answer labels to improve the results significantly: although the labels from the neighboring answers can affect the label of the current answer, this dependency is too loose to have impact on the selection accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 302, "end_pos": 310, "type": "METRIC", "confidence": 0.9799566268920898}]}, {"text": "In other words, labels should be used together with answers' content to account for stronger and more effective dependencies.", "labels": [], "entities": []}], "datasetContent": [{"text": "Below we first describe the data we used, then we introduce the experimental setup, and finally we present and discuss the results of our experiments.", "labels": [], "entities": []}, {"text": "Our local classifiers are support vector machines (SVM) with C = 1 (Joachims, 1999), logistic regression with a Gaussian prior with variance 10, and logistic ordinal regression.", "labels": [], "entities": []}, {"text": "In order to capture long-range sequential dependencies, we use a second-order SVM hmm () (with C = 500 and epsilon = 0.01) and a second-order linear-chain CRF, which considers dependencies between three neighboring labels in a sequence ().", "labels": [], "entities": []}, {"text": "In CRF, we perform two kinds of inference to find the most probable labels for the comments in a sequence.", "labels": [], "entities": []}, {"text": "(i) We compute the maximum a posterior (MAP) or the (jointly) most probable sequence of labels using the Viterbi algorithm.", "labels": [], "entities": [{"text": "maximum a posterior (MAP)", "start_pos": 19, "end_pos": 44, "type": "METRIC", "confidence": 0.7546396007140478}]}, {"text": "Specifically, it computes y * = argmax y 1:T P (y 1:T |x 1:T ), where T is the number of comments in the thread.", "labels": [], "entities": [{"text": "argmax y 1:T P", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9324834148089091}]}, {"text": "(ii) We use the forward-backward algorithm to find the labels by maximizing (individual) posterior marginals (MPM).", "labels": [], "entities": [{"text": "posterior marginals (MPM)", "start_pos": 89, "end_pos": 114, "type": "METRIC", "confidence": 0.7375310659408569}]}, {"text": "More formally, we comput\u00ea y = argmax y1 P (y 1 |x 1:T ), \u00b7 \u00b7 \u00b7 , argmax yT P (y T |x 1:T ) . While MAP yields a globally consistent sequence of labels, MPM can be more robust in many cases; see (Murphy, 2012, p. 613) for details.", "labels": [], "entities": []}, {"text": "CRF also uses a Gaussian prior with variance 10.", "labels": [], "entities": [{"text": "CRF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9251832962036133}]}, {"text": "In order to compare the quality of our features to the existing state of the art, we performed a first experiment aligned to the multi-class setting of the SemEval 2015 Task 3 competition.", "labels": [], "entities": [{"text": "SemEval 2015 Task 3 competition", "start_pos": 156, "end_pos": 187, "type": "TASK", "confidence": 0.776016914844513}]}, {"text": "shows our results on the official test dataset.", "labels": [], "entities": [{"text": "official test dataset", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.8443348209063212}]}, {"text": "As in the competition, the results are macroaveraged at class level.", "labels": [], "entities": []}, {"text": "The results of the top 3 systems are reported for comparison: JAIST (Tran et al., 2015), HITSZ (Hou et al., 2015) and QCRI (, where the latter refers to our old system that we used for the competition.", "labels": [], "entities": [{"text": "JAIST", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.7492208480834961}, {"text": "HITSZ", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.7869148850440979}, {"text": "QCRI", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.8621096611022949}]}, {"text": "The two main observations are (i) using threadlevel features helps significantly; and (ii) the ordinal regression model, which captures the idea that potential lies between good and bad, achieves at least as good results as the top system at SemEval, namely JAIST.", "labels": [], "entities": [{"text": "JAIST", "start_pos": 258, "end_pos": 263, "type": "DATASET", "confidence": 0.9566757082939148}]}, {"text": "For the remaining experiments, we reduce the multi-class problem to a binary one (cf. Section 2).", "labels": [], "entities": []}, {"text": "shows the results obtained on the official test dataset.", "labels": [], "entities": [{"text": "official test dataset", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.7907309730847677}]}, {"text": "Note that ordinal regression is not applicable in this binary setting.", "labels": [], "entities": []}, {"text": "The F 1 values for the baseline features suggest that using the labels in the thread sequence yields better performance with SVM hmm and CRF.", "labels": [], "entities": [{"text": "F 1", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9504320919513702}]}, {"text": "When thread-level features are used, the models using sequence labels do not outperform SVM and logistic regression anymore.", "labels": [], "entities": []}, {"text": "Regarding the two variations of CRF, the posterior marginals maximization is slightly better: maximizing on each comment pays more than on the entire thread.", "labels": [], "entities": []}, {"text": "Since the task consists in identifying good answers fora given question, further figures at the question level are necessary, i.e., we compute the target performance measure for all comments of each question and then we average the results overall threads (ta).", "labels": [], "entities": []}, {"text": "shows such the result using two measures: F 1 and accuracy, i.e., F 1,ta and A ta , for which long threads have less impact on the final outcome.", "labels": [], "entities": [{"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9927029609680176}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992431402206421}, {"text": "F 1,ta", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9872481524944305}, {"text": "A ta", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9688539206981659}]}, {"text": "The impact of the thread features is not-so-high in terms of these measures, sometimes even negatively affecting some of the models.", "labels": [], "entities": []}, {"text": "In order to better understand the mixed results obtained on the single official test set, we performed 5-fold cross validation over the entire dataset.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "When looking at the performance of the different models with the same set of features, no statistically significant differences are observed on F 1 or F 1,ta (t-test with confidence level 95%).", "labels": [], "entities": [{"text": "F 1", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.966284453868866}, {"text": "F 1,ta", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.889076828956604}]}, {"text": "The sequence of predicted labels in CRF or SVM hmm does not impact the final result.", "labels": [], "entities": []}, {"text": "In contrast, an important difference is observed when thread-level features come into play: the performance of all the models improves by approximately two F 1 points absolute, and statistically significant differences are observed for SVM and logistic regression (ttest, 95%).", "labels": [], "entities": []}, {"text": "Moreover, while on the test dataset the thread-level features do not always improve F 1,ta and A ta , on the 5-fold cross-validation using them is always beneficial: for F 1,ta statistically significant difference is observed for SVM only (t-test, 90%).", "labels": [], "entities": [{"text": "F 1,ta", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9254413545131683}, {"text": "A ta", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9479323625564575}]}, {"text": "In order to get an intuition about the effect of the thread-level features, we show two example comment threads in.", "labels": [], "entities": []}, {"text": "These comments are classified correctly when thread features are used in the classifier, and incorrectly when only basic features are used.", "labels": [], "entities": []}, {"text": "In the first case (Q u 1 ), the third comment is classified as good by models that only use basic features.", "labels": [], "entities": []}, {"text": "In contrast, thanks to the thread-level features, the classifier can consider that there is a dialogue between u 1 and u 2 , causing all the comments to be assigned to the correct class: bad.", "labels": [], "entities": []}, {"text": "In the second example (Q u 4 ), the first two comments are classified as bad when using the basic features.", "labels": [], "entities": []}, {"text": "However, the third comment -written by the same user who asked Q u 4 -includes an acknowledgment.", "labels": [], "entities": []}, {"text": "The latter is propagated to the previous comments in terms of a thread feature, which indicates that such comments are more likely to be good answers.", "labels": [], "entities": []}, {"text": "This feature provides the classifier with enough information to properly label the first two comments as good.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Macro-averaged precision, recall, F 1 - measure, and accuracy on the multi-class (good,  bad, potential) setting on the official SemEval- 2015 Task 3 test set. The top-2 systems are in- cluded for comparison. QCRI refers to our official  results, using an older version of our system.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9652016162872314}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995566010475159}, {"text": "F 1 - measure", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9892218261957169}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9995023012161255}, {"text": "SemEval- 2015 Task 3 test set", "start_pos": 139, "end_pos": 168, "type": "DATASET", "confidence": 0.6434305693422046}]}, {"text": " Table 2: Performance of the binary (good vs. bad )  classifiers on the official SemEval-2015 Task 3 test  dataset. Precision, recall, F 1 -measure and accu- racy are calculated at the comment level, while  F 1,ta and A ta are averaged at the thread level.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3 test  dataset", "start_pos": 81, "end_pos": 114, "type": "DATASET", "confidence": 0.6611841380596161}, {"text": "Precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9950030446052551}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.997016191482544}, {"text": "F 1 -measure", "start_pos": 135, "end_pos": 147, "type": "METRIC", "confidence": 0.9651614874601364}, {"text": "accu- racy", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9095733364423116}, {"text": "F 1,ta and A ta", "start_pos": 207, "end_pos": 222, "type": "METRIC", "confidence": 0.8772155046463013}]}, {"text": " Table 3: Precision, Recall, F 1 , Accuracy computed at the comment level; F 1,ta and A ta are averaged at  the thread level. Precision, Recall, F 1 , F 1,ta are computed with respect to the good classifier on 5-fold  cross-validation (mean\u00b1stand. dev.).", "labels": [], "entities": [{"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9128618836402893}, {"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9980579018592834}]}]}