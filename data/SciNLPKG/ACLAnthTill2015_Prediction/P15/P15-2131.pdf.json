{"title": [{"text": "Dialogue Management based on Sentence Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "Dialogue Management (DM) is a key issue in Spoken Dialogue System (SDS).", "labels": [], "entities": [{"text": "Dialogue Management (DM)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8563543498516083}, {"text": "Spoken Dialogue System (SDS)", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.7947763403256735}]}, {"text": "Most of the existing studies on DM use Dialogue Act (DA) to represent semantic information of sentence, which might not represent the nuanced meaning sometimes.", "labels": [], "entities": [{"text": "DM", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.958433210849762}]}, {"text": "In this paper, we model DM based on sentence clusters which have more powerful semantic representation ability than DAs.", "labels": [], "entities": [{"text": "DM", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9611634612083435}]}, {"text": "Firstly, sentences are clustered not only based on the internal information such as words and sentence structures, but also based on the external information such as context in dialogue via Recurrent Neural Networks.", "labels": [], "entities": []}, {"text": "Additionally, the DM problem is modeled as a Partially Observable Markov Decision Processes (POMD-P) with sentence clusters.", "labels": [], "entities": [{"text": "DM problem", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.9268283843994141}]}, {"text": "Finally, experimental results illustrate that the proposed DM scheme is superior to the existing one.", "labels": [], "entities": [{"text": "DM", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9218544363975525}]}], "introductionContent": [{"text": "Dialogue Management (DM) is an important issue in Spoken Dialogue Systems (SDS).)", "labels": [], "entities": [{"text": "Dialogue Management (DM)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8627302289009094}]}, {"text": "Most of the existing studies on DM use the abstract semantic representation such as Dialogue Act (DA) to represent the sentence intention.", "labels": [], "entities": [{"text": "DM", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9674630761146545}]}, {"text": "In (, authors propose a planbased, task-independent DM framework, called RavenClaw, which isolates the domain-specific aspects of the dialogue control logic from domainindependent conversational skills.", "labels": [], "entities": [{"text": "RavenClaw", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.9256771802902222}]}, {"text": "( proposes a Kalman Temporal Differences based algorithm to learn efficiently in an offpolicy manner a strategy fora large scale dialogue system.", "labels": [], "entities": []}, {"text": "In (, authors propose a scheme to utilize a socially-based reward function for reinforcement learning and use it to fit the user adaptation issue for DM.", "labels": [], "entities": [{"text": "DM", "start_pos": 150, "end_pos": 152, "type": "TASK", "confidence": 0.9661352038383484}]}, {"text": "() provides an overview of the current state of the art in the development of POMDP-based spoken dialog systems.", "labels": [], "entities": [{"text": "POMDP-based spoken dialog", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.5610355834166209}]}, {"text": "() presents a dialog manager based on a log-linear probabilistic model and uses context-free grammars to impart hierarchical structure to variables and features.", "labels": [], "entities": []}, {"text": "As we know, sentences in human-human dialogues are extremely complicated.", "labels": [], "entities": []}, {"text": "The sentences labeled with the same DA might contain different extra meanings.", "labels": [], "entities": []}, {"text": "Thus, it is difficult for DA to represent the nuanced meaning of sentence in dialogue.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel DM scheme based on sentence clustering.", "labels": [], "entities": [{"text": "DM", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9691599011421204}, {"text": "sentence clustering", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7064604312181473}]}, {"text": "The contributions of this work are as follows.", "labels": [], "entities": []}, {"text": "\u2022 Semantic representation of sentence in dialogue is defined as sentence cluster which could represent more nuanced semantic information than DA.", "labels": [], "entities": [{"text": "Semantic representation of sentence in dialogue", "start_pos": 2, "end_pos": 49, "type": "TASK", "confidence": 0.8479710419972738}]}, {"text": "Sentence similarity for clustering is calculated via internal information such as words and sentence structures and external information such as the distributed representation of sentence (vector) from Recurrent Neural Networks (RNN).", "labels": [], "entities": []}, {"text": "\u2022 The DM problem is modeled as a POMD-P, where state is defined as sequence of sentence clusters, reward is defined as slot-filling efficiency and sentence popularity, and state transition probability is calculated by the prediction model based on RNN, considering historical dialogue information sufficiently.", "labels": [], "entities": [{"text": "DM problem", "start_pos": 6, "end_pos": 16, "type": "TASK", "confidence": 0.9229631125926971}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, system model is introduced.", "labels": [], "entities": []}, {"text": "Section 3 describes sentence clustering and prediction model based on RNN, and Section 4 models the DM problem as a POMDP.", "labels": [], "entities": [{"text": "sentence clustering", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.757928878068924}]}, {"text": "Extensive experimental results are provided in Section 5 to illustrate the performance comparison, and Section 6 concludes this study.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the performances of the proposed Sentence Clustering based Dialogue Management (SCDM) scheme and the existing D-M scheme.", "labels": [], "entities": [{"text": "Sentence Clustering based Dialogue Management (SCDM)", "start_pos": 61, "end_pos": 113, "type": "TASK", "confidence": 0.8613614141941071}]}, {"text": "The existing scheme is designed according to, where DA is utilized to represent the semantic information of sentence and the dialogue policy is trained via Reinforcement Learning.", "labels": [], "entities": []}, {"text": "It is also an extrinsic (or endto-end) evaluation to compare the semantic representation ability between sentence cluster and DA.", "labels": [], "entities": []}, {"text": "In order to compare the performances of the DM schemes, we collect 171 human-human dialogues in hotel reservation and utilize 100 dialogues of them to establish a SDS.", "labels": [], "entities": []}, {"text": "The residual 71 dialogues are used to establish a simulated user for testing ().", "labels": [], "entities": []}, {"text": "We define the slots requested from machine to user as \"room type\", \"room quantity\", \"checkin time\", \"checkout time\", \"client name\" and \"client phone\".", "labels": [], "entities": []}, {"text": "We also define the slots requested from users to machine as \"hotel address = No.95 East St.\", \"room typeset = single room, double room, and deluxe room\", \"single room price = $80\", \"double room price = $100\", \"deluxe room price = $150\".", "labels": [], "entities": [{"text": "East St.", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.952851414680481}]}, {"text": "The hotel reservation task could be considered as a process of exchanging the slot information between machine and user to some extent.", "labels": [], "entities": [{"text": "hotel reservation task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7854176163673401}]}, {"text": "illustrates the dialogue turn in the DM schemes, using different training corpus.", "labels": [], "entities": []}, {"text": "Here, we vary the size of training corpus from 10 dialogues to 100 dialogues and define average turn as the average dialogue turn cost to complete the task.", "labels": [], "entities": []}, {"text": "From this picture, we find out that the SCD-M scheme has lower average turn than the existing scheme, partly because the sentence are automatically clustered into many small groups that could represent more nuanced semantic information than DAs, partly because RNN could estimate next sentence cluster according to the vector in hidden layer that contains abundant historical dialogue information.", "labels": [], "entities": [{"text": "turn", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.8934871554374695}]}, {"text": "As the number of sentence clusters is greater than number of DAs, RNN could also solve the scarcity problem and smoothing problem in the predicting process.", "labels": [], "entities": [{"text": "predicting process", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.9104236662387848}]}, {"text": "Additionally, with the increment of training dialogue size, the average turn of dialogue decreases, which ought to be ascribed to the fact that more training data could let SD-S reach more states with more times and increase the accuracy of the parameter estimation in RNN and POMDP.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9990991353988647}]}, {"text": "Furthermore, with the increment of training dialogue size, the dialogue turn improvement of the proposed scheme turns less obvious, because the number of new sentence pattern deceases with the training size increment.", "labels": [], "entities": []}], "tableCaptions": []}