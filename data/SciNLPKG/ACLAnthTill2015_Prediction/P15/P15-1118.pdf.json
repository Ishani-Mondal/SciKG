{"title": [], "abstractContent": [{"text": "Treebanks are key resources for developing accurate statistical parsers.", "labels": [], "entities": []}, {"text": "However, building treebanks is expensive and time-consuming for humans.", "labels": [], "entities": []}, {"text": "For domains requiring deep subject matter expertise such as law and medicine, treebanking is even more difficult.", "labels": [], "entities": []}, {"text": "To reduce annotation costs for these domains, we develop methods to improve cross-domain parsing inference using paraphrases.", "labels": [], "entities": [{"text": "cross-domain parsing inference", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.8080812295277914}]}, {"text": "Paraphrases are easier to obtain than full syntactic analyses as they do not require deep linguistic knowledge , only linguistic fluency.", "labels": [], "entities": []}, {"text": "A sentence and its paraphrase may have similar syntactic structures, allowing their parses to mutually inform each other.", "labels": [], "entities": []}, {"text": "We present several methods to incorporate paraphrase information by jointly parsing a sentence with its paraphrase.", "labels": [], "entities": []}, {"text": "These methods are applied to state-of-the-art constituency and dependency parsers and provide significant improvements across multiple domains .", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7348847091197968}]}], "introductionContent": [{"text": "Parsing is the task of reconstructing the syntactic structure from surface text.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9670981168746948}]}, {"text": "Many natural language processing tasks use parse trees as a basis for deeper analysis.", "labels": [], "entities": []}, {"text": "The most effective sources of supervision for training statistical parsers are treebanks.", "labels": [], "entities": []}, {"text": "Unfortunately, treebanks are expensive, time-consuming to create, and not available for most domains.", "labels": [], "entities": []}, {"text": "Compounding the problem, the accuracy of statistical parsers degrades as the domain shifts away from the supervised training corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9994224309921265}]}, {"text": "Furthermore, for * Work performed during an IBM internship.", "labels": [], "entities": []}, {"text": "domains requiring subject matter experts, e.g., law and medicine, it may not be feasible to produce large scale treebanks since subject matter experts generally don't have the necessary linguistic background.", "labels": [], "entities": []}, {"text": "It is natural to look for resources that are more easily obtained.", "labels": [], "entities": []}, {"text": "In this work, we explore using paraphrases.", "labels": [], "entities": []}, {"text": "Unlike parse trees, paraphrases can be produced quickly by humans and don't require extensive linguistic training.", "labels": [], "entities": []}, {"text": "While paraphrases are not parse trees, a sentence and its paraphrase may have similar syntactic structures for portions where they can be aligned.", "labels": [], "entities": []}, {"text": "We can improve parsers by jointly parsing a sentence with its paraphrase and encouraging certain types of overlaps in their syntactic structures.", "labels": [], "entities": []}, {"text": "As a simple example, consider replacing an unknown word in a sentence with a synonym found in the training data.", "labels": [], "entities": []}, {"text": "This may help disambiguate the sentence without changing its parse tree.", "labels": [], "entities": []}, {"text": "More disruptive forms of paraphrasing (e.g., topicalization) can also be handled by not requiring strict agreement between the parses.", "labels": [], "entities": []}, {"text": "In this paper, we use paraphrases to improve parsing inference within and across domains.", "labels": [], "entities": [{"text": "parsing inference", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.9512723982334137}]}, {"text": "We develop methods using dual-decomposition (where the parses of both sentences from a dependency parser are encouraged to agree, Section 3.2) and pair-finding (which can be applied to any nbest parser, Section 3.3).", "labels": [], "entities": []}, {"text": "Some paraphrases significantly disrupt syntactic structure.", "labels": [], "entities": []}, {"text": "To counter this, we examine relaxing agreement constraints and building classifiers to predict when joint parsing won't be beneficial (Section 3.4).", "labels": [], "entities": []}, {"text": "We show that paraphrases can be exploited to improve crossdomain parser inference for two state-of-the-art parsers, especially on domains where they perform poorly.", "labels": [], "entities": [{"text": "crossdomain parser inference", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8456589976946512}]}], "datasetContent": [{"text": "To evaluate the efficacy of the proposed methods of jointly parsing paraphrases, we built a corpus of paraphrases where one sentence in a pair of paraphrases has a gold tree.", "labels": [], "entities": []}, {"text": "We randomly sampled 4,000 sentences 5 from four gold treebanks: Brown, British National Corpus (BNC), QuestionBank 6 (QB) and Wall Street Journal (section 24).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 71, "end_pos": 100, "type": "DATASET", "confidence": 0.931896815697352}, {"text": "QuestionBank 6 (QB)", "start_pos": 102, "end_pos": 121, "type": "DATASET", "confidence": 0.87159423828125}, {"text": "Wall Street Journal", "start_pos": 126, "end_pos": 145, "type": "DATASET", "confidence": 0.9213717977205912}]}, {"text": "A linguist provided a paraphrase for each sampled sentence according to these instructions: The paraphrases should more or less convey the same information as the original sentence.", "labels": [], "entities": []}, {"text": "That is, the two sentences should logically entail each other.", "labels": [], "entities": []}, {"text": "The paraphrases should generally use most of the same words (but not necessarily in the same order).", "labels": [], "entities": []}, {"text": "Active/passive transforms, changing words with synonyms, and rephrasings of the same idea are all examples of transformations that paraphrases can use (others can be used too).", "labels": [], "entities": []}, {"text": "They can be as simple as just changing a single word in some cases (though, ideally, a variety of paraphrasing techniques would be used).", "labels": [], "entities": []}, {"text": "We also provided 10 pairs of sentences as examples.", "labels": [], "entities": []}, {"text": "We evaluate our methods only on the sampled sentences from the gold corpora because the new paraphrases do not include syntactic trees.", "labels": [], "entities": []}, {"text": "The data was divided into development and testing sets such that development and testing share the same distribution over the four corpora.", "labels": [], "entities": []}, {"text": "Paraphrases were tokenized by the BLLIP tokenizer.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.6688348650932312}]}, {"text": "See for statistics of the dataset.", "labels": [], "entities": []}, {"text": "We ran all tuning and model design experiments on the development set.", "labels": [], "entities": []}, {"text": "For the final evaluation, we tuned parameters on the development set and evaluate them on the test set.", "labels": [], "entities": []}, {"text": "Constituency trees were converted to basic non-collapsed dependency trees using Stanford Dependencies).", "labels": [], "entities": [{"text": "Stanford Dependencies", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.9327806830406189}]}, {"text": "We report unlabeled attachment scores (UAS) for all experiments and labeled attachment scores (LAS) as well in final evaluation, ignoring punctuation.", "labels": [], "entities": [{"text": "unlabeled attachment scores (UAS)", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.7993158946434656}, {"text": "labeled attachment scores (LAS)", "start_pos": 68, "end_pos": 99, "type": "METRIC", "confidence": 0.851370652516683}]}, {"text": "Averages are microaverages across all sentences.", "labels": [], "entities": []}, {"text": "We evaluate the three parsers on the test set using the tuned parameters and logistic regression models from above.", "labels": [], "entities": []}, {"text": "Joint parsing with paraphrases significantly improves accuracy for all systems (Table 7).", "labels": [], "entities": [{"text": "Joint parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7305492460727692}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9989763498306274}]}, {"text": "Self-trained BLLIP with logistic regression is the most accurate, though RBG with SDual provides the most consistent improvements.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9795389771461487}]}, {"text": "Joint parsing without logistic regression (RBG + S-Dual) is more accurate than independent parsing (RBG) overall.", "labels": [], "entities": [{"text": "Joint parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7590028047561646}, {"text": "independent parsing (RBG", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.6439669951796532}]}, {"text": "With the help of logistic regression, the methods do at least as well as their baseline counterparts on all domains with the exception of self-trained BLLIP on BNC.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.8703992962837219}]}, {"text": "We believe that the drop on BNC is largely due to noise as our BNC test set is the smallest of the four.", "labels": [], "entities": [{"text": "BNC", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.6347905397415161}, {"text": "BNC test set", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9060072898864746}]}, {"text": "As on development, logistic regression does not change the accuracy much over the RBG parser with soft dual decomposition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995284080505371}]}, {"text": "Joint parsing provides the largest gains on QuestionBank, the domain with the lowest baseline accuracies.", "labels": [], "entities": [{"text": "Joint parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6624030321836472}, {"text": "QuestionBank", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9652243852615356}]}, {"text": "This fits with our goal of using paraphrases for domain adaptation -parsing with paraphrases helps the most on domains furthest from our training data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.750150203704834}]}], "tableCaptions": [{"text": " Table 3: Statistics for the four corpora of the paraphrase dataset. Most statistics are counted from sen- tences with gold trees, including punctuation. indicates the statistic is from the paraphrased sentences.  \"Avg. aligned\" is the average number of aligned tokens from the original sentences using Meteor. OOV  is the percentage of tokens not seen in the WSJ training.", "labels": [], "entities": [{"text": "Avg. aligned\"", "start_pos": 215, "end_pos": 228, "type": "METRIC", "confidence": 0.959272027015686}, {"text": "OOV", "start_pos": 311, "end_pos": 314, "type": "METRIC", "confidence": 0.9963033199310303}, {"text": "WSJ training", "start_pos": 360, "end_pos": 372, "type": "DATASET", "confidence": 0.8769589066505432}]}, {"text": " Table 4: Comparison of hard and soft dual de- composition for joint parsing (development sec- tion, UAS).", "labels": [], "entities": [{"text": "joint parsing", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.6457456797361374}, {"text": "UAS", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9228981733322144}]}, {"text": " Table 5: UAS of joint parsing using the pair- finding scheme with various n values on the de- velopment portion. n = 1 is the baseline BLLIP  parser and n > 1 is BLLIP with pair-finding.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.3889685571193695}, {"text": "joint parsing", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.5976271629333496}, {"text": "pair- finding", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.6630318562189738}, {"text": "BLLIP", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.7781643867492676}, {"text": "BLLIP", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.708107590675354}]}, {"text": " Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.", "labels": [], "entities": [{"text": "UAS", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.6117546558380127}]}]}