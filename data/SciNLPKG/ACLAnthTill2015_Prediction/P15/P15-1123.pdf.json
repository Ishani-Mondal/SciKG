{"title": [{"text": "Discourse-sensitive Automatic Identification of Generic Expressions", "labels": [], "entities": [{"text": "Discourse-sensitive Automatic Identification of Generic Expressions", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.6632070243358612}]}], "abstractContent": [{"text": "This paper describes a novel sequence labeling method for identifying generic expressions , which refer to kinds or arbitrary members of a class, in discourse context.", "labels": [], "entities": []}, {"text": "The automatic recognition of such expressions is important for any natural language processing task that requires text understanding.", "labels": [], "entities": []}, {"text": "Prior work has focused on identifying generic noun phrases; we present anew corpus in which not only subjects but also clauses are annotated for generic-ity according to an annotation scheme motivated by semantic theory.", "labels": [], "entities": []}, {"text": "Our context-aware approach for automatically identifying generic expressions uses conditional random fields and outperforms previous work based on local decisions when evaluated on this corpus and on related data sets (ACE-2 and ACE-2005).", "labels": [], "entities": [{"text": "ACE-2", "start_pos": 219, "end_pos": 224, "type": "DATASET", "confidence": 0.915481686592102}, {"text": "ACE-2005", "start_pos": 229, "end_pos": 237, "type": "DATASET", "confidence": 0.7673783302307129}]}], "introductionContent": [{"text": "Distinguishing between statements about particular individuals or situations and generic sentences is an important part of human language understanding.", "labels": [], "entities": [{"text": "Distinguishing between statements about particular individuals or situations", "start_pos": 0, "end_pos": 76, "type": "TASK", "confidence": 0.8455941304564476}, {"text": "human language understanding", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.646665076414744}]}, {"text": "Consider example (1): sentence (a) names characteristic attributes of a kind, which are inherent to every (typical) individual, and sentence (b) describes a specific individual.", "labels": [], "entities": []}, {"text": "The above example illustrates that generic and non-generic sentences differ substantially in their semantic impact and entailment properties.", "labels": [], "entities": []}, {"text": "It can be inferred from sentence (1a) that atypical horse has a life expectancy of 25 to 30 years, and if we know that Nelly is a horse, we can infer that its life expectancy is 25 to 30 years.", "labels": [], "entities": []}, {"text": "Sentence (1b) has no such properties, it only allows inferences about the particular individual Old Billy.", "labels": [], "entities": [{"text": "Old Billy", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.96650630235672}]}, {"text": "An automatic classifier that recognizes generic expressions would be extremely valuable for various kinds of natural language processing systems: for text understanding and question answering systems, through the improvement of textual entailment methods, and for systems acquiring machine-readable knowledge from text.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7740250527858734}, {"text": "question answering", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.8013797402381897}]}, {"text": "Machinereadable knowledge bases have different representations for statements corresponding to generic knowledge about kinds and knowledge about specific individuals.", "labels": [], "entities": []}, {"text": "The non-generic sentence (1b) roughly speaking provides ABox content fora machine-readable knowledge base, i.e., knowledge about particular instances, e.g, \"A is an instance of B / has property X\".", "labels": [], "entities": []}, {"text": "In contrast, the generic sentence (1a) feeds the TBox, i.e., knowledge of the form \"All B are C / have property X\".", "labels": [], "entities": [{"text": "TBox", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.577103316783905}]}, {"text": "provide a detailed discussion of the relevance of the distinction between classes and instances for automatic ontology construction.", "labels": [], "entities": [{"text": "automatic ontology construction", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.6168693204720815}]}, {"text": "In this paper, we present anew corpus annotated in a linguistically motivated way for genericity, and a context-sensitive computational model for labeling sequences of clauses or noun phrases (NPs) with their genericity status.", "labels": [], "entities": [{"text": "genericity", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.9819728136062622}, {"text": "labeling sequences of clauses or noun phrases (NPs)", "start_pos": 146, "end_pos": 197, "type": "TASK", "confidence": 0.8303460597991943}]}, {"text": "Both manual annotation and automatic recognition of generic expressions are challenging tasks: virtually all NP types -definites, indefinites and quantified NPs, full NPs, pronouns, and even proper names (e.g. species names such as Elephas maximus) -can be found in generic and non-generic uses depending on their clausal context.", "labels": [], "entities": [{"text": "automatic recognition of generic expressions", "start_pos": 27, "end_pos": 71, "type": "TASK", "confidence": 0.6876491665840149}]}, {"text": "In this work, we call clauses generic if they provide a general characterization of entities of a certain kind, and we call mentions of NPs generic if they refer to kinds or arbitrary members of a class.", "labels": [], "entities": []}, {"text": "Although genericity on the clause-and NP-level are strongly interrelated, the concepts do not always coincide.", "labels": [], "entities": []}, {"text": "As example (2) shows, sentences describing episodic events can have a generic NP as their subject.", "labels": [], "entities": []}, {"text": "Note that references to species are kind-referring / generic on the NP level (following, seep. 65).", "labels": [], "entities": []}, {"text": "(2) In September 2013 the blobfish was voted the \"World's Ugliest Animal\".", "labels": [], "entities": []}, {"text": "(subject generic, clause non-generic) Genericity often cannot be annotated without paying attention to the wider discourse context.", "labels": [], "entities": []}, {"text": "Clearly, coreference information is needed for the genericity classification of pronouns.", "labels": [], "entities": [{"text": "genericity classification of pronouns", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.9300624579191208}]}, {"text": "Often, even genericity of full NPs or entire clauses cannot be decided in isolation, as illustrated by example (3).", "labels": [], "entities": []}, {"text": "Sentence (b) could be part of a particular narrative about a tree, or it could be a generic statement.", "labels": [], "entities": []}, {"text": "Only the context given by (a) clarifies that (b) indeed makes reference to any year's new twigs and is to be interpreted as generic.", "labels": [], "entities": []}, {"text": "In computational linguistics, most research on detecting genericity has been done in relation to the ACE corpora (), focusing on assigning genericity labels to noun phrases (, see Section 2.", "labels": [], "entities": [{"text": "detecting genericity", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8511312007904053}, {"text": "ACE corpora", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.9237211346626282}]}, {"text": "Our work is based on these approaches, most notably on the work of, and extends upon them in the following essential ways.", "labels": [], "entities": []}, {"text": "The major contributions of this work are: (1) We create anew corpus of Wikipedia articles annotated with linguistically motivated genericity labels both on the subject-and clause-level (see Section 3).", "labels": [], "entities": []}, {"text": "The corpus is balanced with respect to genericity and about 10,000 clauses in size.", "labels": [], "entities": [{"text": "genericity", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.9558987617492676}]}, {"text": "(2) We present a discourse-sensitive genericity labeler.", "labels": [], "entities": [{"text": "genericity labeler", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.849088579416275}]}, {"text": "Technically, we use conditional random fields as a sequence labeling method (Section 4).", "labels": [], "entities": []}, {"text": "We train and evaluate our method on the Wikipedia dataset and the ACE corpora, evaluating both the tasks of predicting NP genericity and the task of predicting clause-level genericity.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.9727726876735687}, {"text": "ACE corpora", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.9640442430973053}, {"text": "predicting NP genericity", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7842042048772176}, {"text": "predicting clause-level genericity", "start_pos": 149, "end_pos": 183, "type": "TASK", "confidence": 0.8363378842671713}]}, {"text": "Our labeler outperforms the state-of-the-art by a margin of 6.6-11.9% (depending on the data set) in terms of accuracy, at the same time increasing F 1 -score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.999498724937439}, {"text": "F 1 -score", "start_pos": 148, "end_pos": 158, "type": "METRIC", "confidence": 0.986901119351387}]}, {"text": "Much of the performance gain is due to the inclusion of discourse information.", "labels": [], "entities": []}, {"text": "For the discussion of our experimental results, see Section 5.", "labels": [], "entities": []}, {"text": "In this paper, we do not address the following two important aspects of genericity.", "labels": [], "entities": [{"text": "genericity", "start_pos": 72, "end_pos": 82, "type": "TASK", "confidence": 0.9825018644332886}]}, {"text": "First, habitual sentences form a class of generalizing statements which bear a close relation to generics.", "labels": [], "entities": []}, {"text": "As can be seen in example (4), they describe a characterizing property of either a specific entity or a class by generalizing over situations instead of or in addition to entities).", "labels": [], "entities": []}, {"text": "We classify habitual sentences with a generic subject as generic, and habitual sentences which describe a specific entity as non-generic, leaving the task of habituality detection for future work.", "labels": [], "entities": [{"text": "habituality detection", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.7531089782714844}]}, {"text": "(4) (a) John smokes after dinner.", "labels": [], "entities": []}, {"text": "(b) Gentlemen smoke after dinner.", "labels": [], "entities": []}, {"text": "Second, generic clauses express regularities within classes of entities, and thus are similar to universally quantified sentences in their truth conditions and entailment properties.", "labels": [], "entities": []}, {"text": "However, their truth-conditional interpretation is tricky, since they express typicality, describe stereotypes and allow exceptions, for example Dutchmen are good sailors is not false even if most Dutchmen do not sail at all.", "labels": [], "entities": []}, {"text": "We concentrate on the decision of whether a clause is generic or not, and leave the truth-conditional interpretation for further work.", "labels": [], "entities": []}, {"text": "For a detailed discussion of the semantics of generics expressions seethe comprehensive survey by; a short and instructive overview can be found in the first part of).", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports on our experiments, which we evaluate in terms of precision (P), recall (R) and F 1 -measure per class.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 71, "end_pos": 84, "type": "METRIC", "confidence": 0.9471221715211868}, {"text": "recall (R)", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9653880447149277}, {"text": "F 1 -measure", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.9798024594783783}]}, {"text": "We compute macroaverages as P macro = 1 |c| * |c| i=1 P i etc., where |c| stands for the number of classes.", "labels": [], "entities": []}, {"text": "Macro-F 1 is the harmonic mean of macro-average P and R.", "labels": [], "entities": []}, {"text": "To report on statistical significance of differences inaccuracy, we apply McNemar's test with p < 0.01.", "labels": [], "entities": []}, {"text": "We report results for cross validation (CV).", "labels": [], "entities": [{"text": "cross validation (CV)", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.747881931066513}]}, {"text": "Because we leverage contextual information by labeling sequences of clauses from entire documents, for all experiments presented in this section, if not indicated otherwise, we put all instances of one document into the same fold as one sequence.", "labels": [], "entities": []}, {"text": "Fold sizes differ slightly from each other, but folds are kept constant for all experiments.", "labels": [], "entities": [{"text": "Fold sizes", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9275436997413635}]}, {"text": "On WikiGenerics, we carryout all three prediction tasks as defined in Section 3.", "labels": [], "entities": []}, {"text": "On the ACE corpora, we only conduct Task NP because there are: Results of reimplemented baseline on ACE-2 (original, unbalanced data set), 40106 instances (annotated noun phrases).", "labels": [], "entities": [{"text": "ACE corpora", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9077110290527344}]}, {"text": "Weka's stratified 10-fold cross validation, using all features.", "labels": [], "entities": []}, {"text": "no labels corresponding to Task Cl or Task Cl+NP.", "labels": [], "entities": []}, {"text": "For the experiments on WikiGenerics, we use leave-one-document-out CV, i.e., we train on 101 of the 102 documents and test on the remaining document in each fold.", "labels": [], "entities": []}, {"text": "The total number of clauses is 10355.", "labels": [], "entities": []}, {"text": "From ACE-2005, we use the newswire and broadcast news subsections.", "labels": [], "entities": [{"text": "ACE-2005", "start_pos": 5, "end_pos": 13, "type": "DATASET", "confidence": 0.9511740803718567}]}, {"text": "Due to low frequency, we omit instances of NEG in our experiments, and apply a three-way classification task (GEN, SPC, USP).", "labels": [], "entities": []}, {"text": "We present results for all remaining 40106 mentions and for the subset of 18029 subject mentions, each time using 10-fold CV.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of reimplemented baseline on ACE-2 (original, unbalanced data set), 40106 instances  (annotated noun phrases). Weka's stratified 10-fold cross validation, using all features.", "labels": [], "entities": [{"text": "ACE-2", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8893877267837524}]}, {"text": " Table 3: Results on WikiGenerics for Task NP and Task C. * \u2020Difference statistically significant.", "labels": [], "entities": [{"text": "Difference", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9685542583465576}]}, {"text": " Table 4: Results on WikiGenerics for Task Cl+NP. *Difference statistically significant.", "labels": [], "entities": [{"text": "Difference", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.973563551902771}]}, {"text": " Table 5: Results on ACE-2 for Task NP, 10-fold CV, folds contain complete documents. *Difference  statistically significant.", "labels": [], "entities": [{"text": "Difference", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9844292402267456}]}, {"text": " Table 6: Results on ACE-2005 (bn+nw),  Task NP, 10-fold CV, 3 classes: SPC, GEN, USP.  *Difference statistically significant.", "labels": [], "entities": [{"text": "ACE-2005", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8043462038040161}, {"text": "Difference", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9755398631095886}]}]}