{"title": [{"text": "Generative Event Schema Induction with Entity Disambiguation", "labels": [], "entities": [{"text": "Generative Event Schema Induction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.803163155913353}]}], "abstractContent": [{"text": "This paper presents a generative model to event schema induction.", "labels": [], "entities": [{"text": "event schema induction", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.8339194059371948}]}, {"text": "Previous methods in the literature only use head words to represent entities.", "labels": [], "entities": []}, {"text": "However, elements other than head words contain useful information.", "labels": [], "entities": []}, {"text": "For instance, an armed man is more discriminative than man.", "labels": [], "entities": []}, {"text": "Our model takes into account this information and precisely represents it using proba-bilistic topic distributions.", "labels": [], "entities": []}, {"text": "We illustrate that such information plays an important role in parameter estimation.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.6066476553678513}]}, {"text": "Mostly, it makes topic distributions more coherent and more discriminative.", "labels": [], "entities": []}, {"text": "Experimental results on benchmark dataset empirically confirm this enhancement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information Extraction was initially defined (and is still defined) by the MUC evaluations and more specifically by the task of template filling.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7542020380496979}, {"text": "MUC", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.7826671600341797}, {"text": "template filling", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.7819090783596039}]}, {"text": "The objective of this task is to assign event roles to individual textual mentions.", "labels": [], "entities": []}, {"text": "A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, their location, date, magnitude and the damages they caused).", "labels": [], "entities": []}, {"text": "Schema induction is the task of learning these templates with no supervision from unlabeled text.", "labels": [], "entities": [{"text": "Schema induction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.87553671002388}]}, {"text": "We focus hereon event schema induction and continue the trend of generative models proposed earlier for this task.", "labels": [], "entities": [{"text": "event schema induction", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6533546845118204}]}, {"text": "The idea is to group together entities corresponding to the same role in an event template based on the similarity of the relations that these entities hold with predicates.", "labels": [], "entities": []}, {"text": "For example, in a corpus about terrorist attacks, entities that are objects of verbs to kill, to attack can be grouped together and characterized by a role named VICTIM.", "labels": [], "entities": []}, {"text": "The output of this identification operation is a set of clusters of which members are both words and relations, associated with their probability (see an example later in).", "labels": [], "entities": []}, {"text": "These clusters are not labeled but each of them represents an event slot.", "labels": [], "entities": []}, {"text": "Our approach here is to improve this initial idea by entity disambiguation.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7638649642467499}]}, {"text": "Some ambiguous entities, such as manor soldier, can match two different slots (victim or perpetrator).", "labels": [], "entities": []}, {"text": "An entity such as terrorist can be mixed up with victims when articles relate that a terrorist has been killed by police (and thus is object of to kill).", "labels": [], "entities": []}, {"text": "Our hypothesis is that the immediate context of entities is helpful for disambiguating them.", "labels": [], "entities": []}, {"text": "For example, the fact that man is associated with armed, dangerous, heroic or innocent can lead to a better attribution and definition of roles.", "labels": [], "entities": []}, {"text": "We then introduce relations between entities and their attributes in the model by means of syntactic relations.", "labels": [], "entities": []}, {"text": "The document level, which is generally a center notion in topic modeling, is not used in our generative model.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7440285682678223}]}, {"text": "This results in a simpler, more intuitive model, where observations are generated from slots, that are defined by probabilistic distributions on entities, predicates and syntactic attributes.", "labels": [], "entities": []}, {"text": "This model offers room for further extensions since multiple observations on an entity can be represented in the same manner.", "labels": [], "entities": []}, {"text": "Model parameters are estimated by Gibbs sampling.", "labels": [], "entities": []}, {"text": "We evaluate the performance of this approach by an automatic and empiric mapping between slots from the system and slots from the reference in away similar to previous work in the domain.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 briefly presents previous work; in Section 3, we detail our entity and relation representation; we describe our generative model in Section 4, before presenting our experiments and evaluations in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare with related work, we evaluated our method on the Message Understanding Conference (MUC-4) corpus using precision, recall and F-score as conventional metrics for template extraction.", "labels": [], "entities": [{"text": "Message Understanding Conference (MUC-4) corpus", "start_pos": 70, "end_pos": 117, "type": "DATASET", "confidence": 0.5693779332297189}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9993346333503723}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.998609185218811}, {"text": "F-score", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.9972680807113647}, {"text": "template extraction", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7812995910644531}]}, {"text": "In what follows, we first introduce the MUC-4 corpus (Section 5.1.1), we detail the mapping technique between learned slots and reference slots (5.1.2) as well as the hyper-parameters of our model (5.1.3).", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9387535154819489}]}, {"text": "Next, we present a first experiment (Section 5.2) showing how using attribute relations improves overall results.", "labels": [], "entities": []}, {"text": "The second experiment (Section 5.3) studies the impact of document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7208229452371597}]}, {"text": "We then compare our results with previous approaches, more particularly with Chambers (2013), from both quantitative and qualitative points of view (Section 5.4).", "labels": [], "entities": [{"text": "Chambers (2013)", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.8705276101827621}]}, {"text": "Finally, Section 5.5 is dedicated to error analysis, with a special emphasis on sources of false positives.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.8665516972541809}]}, {"text": "The MUC-4 corpus contains 1,700 news articles about terrorist incidents happening in Latin America.", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9263374507427216}]}, {"text": "The corpus is divided into 1,300 documents for the development set and four test sets, each containing 100 documents.", "labels": [], "entities": []}, {"text": "We follow the rules in the literature to guarantee comparable results).", "labels": [], "entities": []}, {"text": "The evaluation focuses on four template types -ARSON, ATTACK, BOMBING, KIDNAPPING -and four slots -Perpetrator, Instrument, Target, and Victim.", "labels": [], "entities": [{"text": "ARSON", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.7851542234420776}, {"text": "ATTACK", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9316685199737549}, {"text": "BOMBING", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9930344820022583}, {"text": "KIDNAPPING", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9875049591064453}]}, {"text": "Perpetrator is merged from Perpetrator Individual and Perpetrator Organization.", "labels": [], "entities": []}, {"text": "The matching between system answers and references is based on headword matching.", "labels": [], "entities": [{"text": "headword matching", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7212178260087967}]}, {"text": "A headword is defined as the rightmost word of the phrase or as the right-most word of the first 'of' if the phrase contains any.", "labels": [], "entities": []}, {"text": "Optional templates and slots are ignored when calculating recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9801082015037537}]}, {"text": "Template types are ignored in evaluation: this means that a perpetrator of BOMBING in the answers could be compared to a perpetrator of ARSON, ATTACK, BOMBING or KIDNAPPING in the reference.", "labels": [], "entities": [{"text": "BOMBING", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9778557419776917}, {"text": "ARSON", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.7041135430335999}, {"text": "ATTACK", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.866621196269989}, {"text": "BOMBING", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.911109209060669}]}, {"text": "In this experiment, two versions of our model are compared: HT+A uses entity heads, event trigger relations and entity attribute relations.", "labels": [], "entities": []}, {"text": "HT uses only entity heads and event triggers and omits attributes.", "labels": [], "entities": [{"text": "HT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5109872221946716}]}, {"text": "We studied the gain brought by attribute relations with a focus on their effect when coreference information was available or was missing.", "labels": [], "entities": []}, {"text": "The variations on the model input are named single, multi and coref.", "labels": [], "entities": []}, {"text": "Single input has only one event trigger for each entity.", "labels": [], "entities": []}, {"text": "A text like an armed man attacked the police station and killed a policeman results in two triples for the entity man: (armed:amod, man, attack:nsubj) and (armed:amod, man, kill:nsubj).", "labels": [], "entities": []}, {"text": "In multi input, one entity can have several event triggers, leading for the text above to the triple (armed:amod, man, [attack:nsubj, kill:nsubj]).", "labels": [], "entities": []}, {"text": "The coref input is richer than multi in that, in addition to triggers from the same sentence, triggers linked to the same corefered entity are merged together.", "labels": [], "entities": []}, {"text": "For instance, if man in the above example corefers with he in He was arrested three hours later, the merged triple becomes (armed:amod, man, [attack:nsubj, kill:nsubj, arrest:dobj]).", "labels": [], "entities": []}, {"text": "The plate notations of these model+data combinations are given in. shows a consistent improvement when using attributes, both with and without coreferences.", "labels": [], "entities": []}, {"text": "The best performance of 40.62 F-score is obtained by the full model on inputs with coref- This model is equivalent to 5b) with T=1; 5b) HT model ran on multi data; 5c) HT+A model ran on single data; 5d) HT+A model ran on multi data.: Improvement from using attributes.", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9975976347923279}]}, {"text": "In the second experiment, we evaluated our model with a post-processing step of document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.734295055270195}]}, {"text": "The MUC-4 corpus contains many \"irrelevant\" documents.", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9433532655239105}]}, {"text": "A document is irrelevant if it contains no template.", "labels": [], "entities": []}, {"text": "Among 1,300 documents in the development set, 567 are irrelevant.", "labels": [], "entities": []}, {"text": "The most challenging part is that there are many terrorist entities, e.g. bomb, force, guerrilla, occurring in irrelevant documents.", "labels": [], "entities": []}, {"text": "That makes filtering out those documents important, but difficult.", "labels": [], "entities": [{"text": "filtering out those documents", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.8838052451610565}]}, {"text": "As document classification is not explicitly performed by our model, a post-processing step is needed.", "labels": [], "entities": [{"text": "document classification", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.796766847372055}]}, {"text": "Document classification is expected to reduce false positives in irrelevant documents while not dramatically reducing recall.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9239645898342133}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9978442192077637}]}, {"text": "Given a document d with slot-assigned entities and a set of mapped slots Sm resulting from slot mapping, we have to decide whether this document is relevant or not.", "labels": [], "entities": []}, {"text": "We define the relevance score of a document as: where e is an entity in the document d; s e is the slot value assigned toe; and t is an event trigger in the list of triggers Te . The equation (4) defines the score of an entity as the sum of the conditional probabilities of triggers given a slot.", "labels": [], "entities": []}, {"text": "The relevance score of the document is proportional to the score of the entities assigned to mapped slots.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9645260870456696}]}, {"text": "If this relevance score is higher than a threshold \u03bb, then the document is considered as relevant.: Improvement from document classification as post-processing.", "labels": [], "entities": [{"text": "document classification", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.734491154551506}]}, {"text": "on the development set by maximizing the F-score of document classification.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9974225759506226}, {"text": "document classification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7037703692913055}]}, {"text": "shows the improvement when applying document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.746535450220108}]}, {"text": "The precision increases as false positives from irrelevant documents are filtered out.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999640941619873}]}, {"text": "The loss of recall comes from relevant documents that are mistakenly filtered out.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9995099306106567}]}, {"text": "However, this loss is not significant and the overall Fscore finally increases by 5%.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9969611763954163}]}, {"text": "We also compare our results to an \"oracle\" classifier that would remove all irrelevant documents while preserving all relevant ones.", "labels": [], "entities": []}, {"text": "The performance of this oracle classification shows that there are some room for further improvement from document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.7039894163608551}]}, {"text": "Irrelevant document filtering is a technique applied by most supervised and unsupervised approaches.", "labels": [], "entities": [{"text": "Irrelevant document filtering", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7488628625869751}]}, {"text": "Supervised methods prefer relevance detection at sentence or phrase-level.", "labels": [], "entities": [{"text": "relevance detection", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8663410842418671}]}, {"text": "As for several unsupervised methods, Chambers (2013) includes document classification in his topic model.", "labels": [], "entities": [{"text": "document classification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.7676228284835815}]}, {"text": "Chambers and Jurafsky (2011) and use the learned clusters to classify documents by estimating the relevance of a document with respect to a template from posthoc statistics about event triggers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Improvement from using attributes.", "labels": [], "entities": []}, {"text": " Table 2: Improvement from document classifica- tion as post-processing.", "labels": [], "entities": []}, {"text": " Table 3: Comparison to state-of-the-art unsuper- vised systems.", "labels": [], "entities": []}, {"text": " Table 4: Performance on reimplementation of  Chambers (2013).", "labels": [], "entities": []}]}