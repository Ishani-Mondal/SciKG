{"title": [{"text": "Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser", "labels": [], "entities": [{"text": "Cross-lingual Parameter Sharing", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.654966006676356}]}], "abstractContent": [{"text": "Training a high-accuracy dependency parser requires a large treebank.", "labels": [], "entities": []}, {"text": "However , these are costly and time-consuming to build.", "labels": [], "entities": []}, {"text": "We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages.", "labels": [], "entities": []}, {"text": "We exploit cues from a different source language in order to guide the learning process.", "labels": [], "entities": []}, {"text": "Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9987344145774841}]}], "introductionContent": [{"text": "Dependency parsing is a crucial component of many natural language processing systems, for tasks such as text classification, statistical machine translation (), relation extraction (, and question answering ().", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8629638254642487}, {"text": "text classification", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7990037500858307}, {"text": "statistical machine translation", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.7216836214065552}, {"text": "relation extraction", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.8741830289363861}, {"text": "question answering", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.8749280571937561}]}, {"text": "Supervised approaches to dependency parsing have been successful for languages where relatively large treebanks are available).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.851932942867279}]}, {"text": "However, for many languages, annotated treebanks are not available.", "labels": [], "entities": []}, {"text": "They are costly to create, requiring careful design, testing and subsequent refinement of annotation guidelines, along with assessment and management of annotator quality).", "labels": [], "entities": []}, {"text": "The Universal Treebank Annotation Guidelines aim at providing unified annotation for many languages enabling cross-lingual comparison (.", "labels": [], "entities": [{"text": "Universal Treebank Annotation Guidelines", "start_pos": 4, "end_pos": 44, "type": "DATASET", "confidence": 0.8370163664221764}]}, {"text": "This project provides a starting point for developing a treebank for resource-poor languages.", "labels": [], "entities": []}, {"text": "However, a mature parser requires a large treebank for training, and this is still extremely costly to create.", "labels": [], "entities": []}, {"text": "Instead, we present a method that exploits shared structure across languages to achieve a more accurate parser.", "labels": [], "entities": []}, {"text": "Structural information from the source resource-rich language is incorporated as a prior in the supervised training of a resource-poor target language parser using a small treebank.", "labels": [], "entities": []}, {"text": "When compared with a supervised model, the gain is as high as 8.7% 1 on average when trained on just 1,000 tokens.", "labels": [], "entities": []}, {"text": "As we add more training data, the gains persist, though they are more modest.", "labels": [], "entities": []}, {"text": "Even at 15,000 tokens we observe a 2.9% improvement.", "labels": [], "entities": []}, {"text": "There are two main approaches for building dependency parsers for resource-poor languages: delexicalized parsing and projection).", "labels": [], "entities": [{"text": "delexicalized parsing", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.46384434401988983}]}, {"text": "The delexicalized approach was proposed by.", "labels": [], "entities": []}, {"text": "A parser is built without any lexical features, and trained on a treebank in a resource-rich source language.", "labels": [], "entities": []}, {"text": "It is then applied directly to parse sentences in the target resource-poor languages.", "labels": [], "entities": [{"text": "parse sentences", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.8872055113315582}]}, {"text": "Delexicalized parsing relies on the fact that identical part-of-speech (POS) inventories are highly informative of dependency relations, enough to makeup for crosslingual syntactic divergence.", "labels": [], "entities": [{"text": "crosslingual syntactic divergence", "start_pos": 158, "end_pos": 191, "type": "TASK", "confidence": 0.7619348764419556}]}, {"text": "In contrast, projection approaches use parallel data to project source language dependency relations to the target language (). and exploit both delexicalized parsing and parallel data.", "labels": [], "entities": []}, {"text": "They use parallel data to constrain the model which is usually initialized by the English delexicalized parser.", "labels": [], "entities": []}, {"text": "In summary, existing work generally starts with a delexicalized parser and uses parallel data to improve it.", "labels": [], "entities": []}, {"text": "In this paper, we start with a source language parser and refine it with help from dependency annotations instead of parallel data.", "labels": [], "entities": []}, {"text": "This choice means our method can be applied in cases where linguists are dependency-annotating small amounts of field data, such as in Karuk, a nearlyextinct language of Northwest California (", "labels": [], "entities": []}], "datasetContent": [{"text": "In this part we want to see how much our crosslingual model helps to improve the supervised model, for various data sizes.", "labels": [], "entities": []}, {"text": "We experimented with the Universal Dependency Treebank collection V1.0 (Nivre et al., 2015) which contains treebanks for 10 languages.", "labels": [], "entities": [{"text": "Universal Dependency Treebank collection V1.0", "start_pos": 25, "end_pos": 70, "type": "DATASET", "confidence": 0.7373841047286988}]}, {"text": "4 These treebanks have many desirable properties for our model: the dependency types and coarse POS are the same across languages.", "labels": [], "entities": []}, {"text": "This removes the need for mapping the source and target language tagsets to a common tagset.", "labels": [], "entities": []}, {"text": "Moreover, the dependency types are also common across languages allowing LAS evaluation.", "labels": [], "entities": [{"text": "LAS evaluation", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.9581375420093536}]}, {"text": "shows the dataset size of each language in the collection.", "labels": [], "entities": []}, {"text": "Some languages have over 400k tokens such as cs, fr and es, meanwhile, hu and ga have only around 25k tokens.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tokens (\u00d7 1,000) for each lan- guage in the Universal Dependency Treebank col- lection.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9570738673210144}, {"text": "Universal Dependency Treebank col- lection", "start_pos": 64, "end_pos": 106, "type": "DATASET", "confidence": 0.8221365213394165}]}, {"text": " Table 2: Average LAS for supervised learning us- ing the modified version of the Universal POS  tagset and the fine-grained POS tagset across vari- ous training data sizes.", "labels": [], "entities": [{"text": "Average LAS", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.7843323945999146}, {"text": "Universal POS  tagset", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.8800926208496094}]}]}