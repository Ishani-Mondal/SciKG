{"title": [{"text": "Scalable Semantic Parsing with Partial Ontologies", "labels": [], "entities": [{"text": "Scalable Semantic Parsing", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6161167124907175}]}], "abstractContent": [{"text": "We consider the problem of building scal-able semantic parsers for Freebase, and present anew approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts.", "labels": [], "entities": []}, {"text": "We study this problem on two newly introduced large-scale noun phrase datasets, and present anew semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7396915853023529}]}, {"text": "Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction.", "labels": [], "entities": [{"text": "referring expression resolution", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.8358059128125509}, {"text": "entity attribute extraction", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.6574935515721639}]}, {"text": "In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9981735944747925}]}], "introductionContent": [{"text": "Recently, significant progress has been made in learning semantic parsers for large knowledge bases (KBs) such as Freebase (FB).", "labels": [], "entities": [{"text": "learning semantic parsers", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.632949580748876}, {"text": "Freebase (FB)", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.8269514441490173}]}, {"text": "Although these methods can build general purpose meaning representations, they are typically evaluated on question answering tasks and are designed to only parse questions that have complete ontological coverage, in the sense that there exists a logical form that can be executed against Freebase to get the correct answer.", "labels": [], "entities": [{"text": "question answering tasks", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.792574961980184}]}, {"text": "In this paper, we instead consider the problem of learning semantic parsers for open domain text containing \u2020 Now at Google, NY.", "labels": [], "entities": [{"text": "learning semantic parsers", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6489980220794678}]}, {"text": "1 To ensure all questions are answerable, the data is manually filtered.", "labels": [], "entities": []}, {"text": "For example, the WebQuestions dataset introduced by contains only the 7% of the originally gathered questions.", "labels": [], "entities": [{"text": "WebQuestions dataset", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.941749632358551}]}], "datasetContent": [{"text": "Knowledge base We use the Jan. 26, 2014 Freebase dump.", "labels": [], "entities": [{"text": "Jan. 26, 2014 Freebase dump", "start_pos": 26, "end_pos": 53, "type": "DATASET", "confidence": 0.6570113301277161}]}, {"text": "After pruning binary predicates taking numeric values, it contains 9351 binary predicates, 2754 unary predicates, and 1.2 billion assertions.", "labels": [], "entities": []}, {"text": "Pruning and Feature Initialization We perform beam search at each semantic parsing stage, using the Freebase search API to determine candidate named entities (10 per phrase), binary predicates (300 per phrase), and unary predicates (500 per phrase).", "labels": [], "entities": [{"text": "Feature Initialization", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7055858820676804}, {"text": "semantic parsing", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.764520674943924}]}, {"text": "The ontology matching stage considers the highest scored underspecified parse.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8356576859951019}]}, {"text": "The features are initialized to prefer well-typed logical forms.", "labels": [], "entities": []}, {"text": "Type checking features are initially set to -2 for mismatch.", "labels": [], "entities": [{"text": "Type checking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8127332627773285}]}, {"text": "Features signalling incompatible topic domains and repetition are initialized as -10.", "labels": [], "entities": [{"text": "repetition", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9948843121528625}]}, {"text": "All other initial feature weights are set to 1.", "labels": [], "entities": []}, {"text": "Datasets and Annotation We evaluate on the Wikipedia category and appositive datasets introduced in Sec.", "labels": [], "entities": []}, {"text": "3. On the Wikipedia development data, we annotated 500 logical forms, underspecified logical forms and constant mappings for ontology matching.", "labels": [], "entities": [{"text": "Wikipedia development data", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.8963818152745565}, {"text": "ontology matching", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7761836349964142}]}, {"text": "The Wikipedia test data is composed of 500 unseen categories.", "labels": [], "entities": [{"text": "Wikipedia test data", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.969733734925588}]}, {"text": "We did not train on the appositive dataset, as it contains challenges such as co-reference and parsing errors as described in Sec.", "labels": [], "entities": []}, {"text": "3. Instead, we chose 300 randomly selected examples for evaluation, and ran on the model trained on the Wikipedia development data.", "labels": [], "entities": [{"text": "Wikipedia development data", "start_pos": 104, "end_pos": 130, "type": "DATASET", "confidence": 0.8808004458745321}]}, {"text": "Evaluation Metrics We report five-fold cross validation for development but ran the final model once on the test data, manually scoring the output.", "labels": [], "entities": []}, {"text": "For evaluation on the referring expression resolution performance (as defined in Sec.", "labels": [], "entities": [{"text": "referring expression resolution", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.687572697798411}]}, {"text": "2), we include accuracy for the final logical form (Exact Match).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996614456176758}, {"text": "Exact Match)", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.7930132349332174}]}, {"text": "We also evaluate precision and recall for predicting individual literals in this logical form on the development set.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.999671220779419}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9994751811027527}]}, {"text": "To control for missing facts, we did not evaluate the set of returned entities.", "labels": [], "entities": []}, {"text": "To evaluate entity attribute extraction performance (as defined in Sec. 2), we identified three classes of predictions.", "labels": [], "entities": [{"text": "entity attribute extraction", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6803794304529825}]}, {"text": "Extractions can be correct, benign, or false.", "labels": [], "entities": []}, {"text": "Correct attributes are actually described in the phrase, benign extraction may not have been described but are still true, and false extractions are not true.", "labels": [], "entities": []}, {"text": "For example, if   the phrase \"the capital of the communist-ruled nation\" is mapped to the pair of attributes capital of administrative division(x) , location(x), the first is correct and the second is benign.", "labels": [], "entities": []}, {"text": "Other incorrect facts would be false.", "labels": [], "entities": []}, {"text": "On the development set, we report precision and recall against the union of the FB attributes in our annotations without adjusting for benign extractions or the fact that the annotations are not complete.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9996151924133301}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9994621872901917}]}, {"text": "For the test sets, we computed precision (P) where benign extractions are considered to be wrong, as well as an adjusted precision metric (P*) where benign extractions are counted as correct.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9527154713869095}, {"text": "precision metric (P*)", "start_pos": 121, "end_pos": 142, "type": "METRIC", "confidence": 0.9436859130859375}]}, {"text": "As we do not have full test set annotations, we cannot report recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9901189804077148}]}, {"text": "Finally, we report the average number of facts extracted per noun phrase (fact #).", "labels": [], "entities": []}, {"text": "Comparison Systems We compare performance to a number of ablated versions of the full system, where we have removed the open-constant ontology matching operators (NoOpenSchema), the PMI features (NoPMI), or the type checking features (NoTyping).", "labels": [], "entities": [{"text": "type checking", "start_pos": 211, "end_pos": 224, "type": "TASK", "confidence": 0.8793958723545074}]}, {"text": "For the referring expression resolution task, we excluded the named entity type feature, as this assumes typing information about the entity we are extracting attributes for.", "labels": [], "entities": [{"text": "referring expression resolution task", "start_pos": 8, "end_pos": 44, "type": "TASK", "confidence": 0.8033963143825531}]}, {"text": "We report results without the PMI features and the open schema matching operators (KCAZ13), which is a reimplementation of a recent Freebase QA model ().", "labels": [], "entities": []}, {"text": "We also learn with gold named entity linking (Gold NE).", "labels": [], "entities": []}, {"text": "For the entity attribute extraction, we built a supervised learning baseline that combines the output of two discrete SVMs, one for predicting unary relations and one for binary relations.", "labels": [], "entities": [{"text": "entity attribute extraction", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.6865583062171936}, {"text": "predicting unary relations", "start_pos": 132, "end_pos": 158, "type": "TASK", "confidence": 0.8449926773707072}]}, {"text": "Each classifier   is trained using the annotated Wikipedia categories.", "labels": [], "entities": []}, {"text": "This dataset contains hundreds of unary and binary relations, which the IE baseline can predict.", "labels": [], "entities": [{"text": "IE baseline", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.8264994919300079}]}, {"text": "Each classifier is further anchored on a specific word, and includes n-gram and POS context features around that word, following features from Mintz et al.", "labels": [], "entities": []}, {"text": "To predict binary relations, we used named entities as anchors.", "labels": [], "entities": []}, {"text": "For unary attributes we anchored on all possible nouns and adjectives.", "labels": [], "entities": []}, {"text": "The final logical form includes the best relation predicted by each classifier.", "labels": [], "entities": []}, {"text": "We use the Stanford CoreNLP 5 toolkit for tokenization, named entity recognition, and part-of-speech tagging.", "labels": [], "entities": [{"text": "Stanford CoreNLP 5 toolkit", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.9141461402177811}, {"text": "tokenization", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9787719249725342}, {"text": "named entity recognition", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.637388010819753}, {"text": "part-of-speech tagging", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7665162980556488}]}, {"text": "show performance on the referring expression resolution task.", "labels": [], "entities": [{"text": "referring expression resolution task", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.7938613742589951}]}, {"text": "show performance on the extraction task.", "labels": [], "entities": []}, {"text": "Reported precision is lower on the labeled development set than on the test set, where predicted logical forms are manually evaluated.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9528339505195618}]}, {"text": "This reflects the fact that, despite our best attempts, the development set labels are incomplete, as discussed in Section 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Wikipedia category data statistics.", "labels": [], "entities": [{"text": "Wikipedia category data statistics", "start_pos": 10, "end_pos": 44, "type": "DATASET", "confidence": 0.9206144362688065}]}, {"text": " Table 2: Appositive data statistics.", "labels": [], "entities": []}, {"text": " Table 3: Referring expression resolution perfor- mance on the development set on gold references.", "labels": [], "entities": []}, {"text": " Table 4: Manual evaluation for referring expression  resolution on the test sets.", "labels": [], "entities": [{"text": "referring expression  resolution", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.713810125986735}]}, {"text": " Table 5: Entity attribute extraction performance on  the Wikipedia category development set.", "labels": [], "entities": [{"text": "Entity attribute extraction", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7350519299507141}, {"text": "Wikipedia category development set", "start_pos": 58, "end_pos": 92, "type": "DATASET", "confidence": 0.8226617723703384}]}, {"text": " Table 6: Manual evaluation for entity attribute ex- traction on the test sets.", "labels": [], "entities": []}]}