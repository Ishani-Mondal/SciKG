{"title": [{"text": "Improving distant supervision using inference learning", "labels": [], "entities": [{"text": "Improving distant supervision", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8708029786745707}]}], "abstractContent": [{"text": "Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7861900627613068}]}, {"text": "However , this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data.", "labels": [], "entities": []}, {"text": "This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method.", "labels": [], "entities": []}, {"text": "Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8721140027046204}]}], "introductionContent": [{"text": "Distantly supervised relation extraction relies on automatically labelled data generated using information from a knowledge base.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7561163902282715}]}, {"text": "A sentence is annotated as a positive example if it contains a pair of entities that are related in the knowledge base.", "labels": [], "entities": []}, {"text": "Negative training data is often generated using a closed world assumption: pairs of entities not listed in the knowledge base are assumed to be unrelated and sentences containing them considered to be negative training examples.", "labels": [], "entities": []}, {"text": "However this assumption is violated when the knowledge base is incomplete which can lead to sentences containing instances of relations being wrongly annotated as negative examples.", "labels": [], "entities": []}, {"text": "We propose a method to improve the quality of distantly supervised data by identifying possible wrongly annotated negative instances.", "labels": [], "entities": []}, {"text": "Our proposed method includes aversion of the Path Ranking Algorithm (PRA) () which infers relation paths by combining random walks though a knowledge base.", "labels": [], "entities": [{"text": "Path Ranking Algorithm (PRA)", "start_pos": 45, "end_pos": 73, "type": "METRIC", "confidence": 0.6701218237479528}]}, {"text": "We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.7744125127792358}]}], "datasetContent": [{"text": "Relation Extraction system: The MultiR system () with features described by was used for the experiments.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.81179279088974}]}, {"text": "Datasets: Three datasets were created to train MultiR and evaluate performance.", "labels": [], "entities": [{"text": "MultiR", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.6298372745513916}]}, {"text": "The first (Unfiltered) uses the data obtained using distant supervision (Section 3.1) without removing any examples identified by PRA.", "labels": [], "entities": []}, {"text": "The overall ratio of positive to negative sentences in this dataset was 1:5.1.", "labels": [], "entities": []}, {"text": "However, this changes to 1:2.3 after removing examples identified by PRA.", "labels": [], "entities": [{"text": "PRA", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8030595183372498}]}, {"text": "Consequently the bias in the distantly supervised data was adjusted to 1:2 to increase comparability across configurations.", "labels": [], "entities": []}, {"text": "Reducing bias was also found to increase relation extraction performance, producing a strong baseline.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8698322772979736}]}, {"text": "The PRA-reduced dataset is created by applying PRA reduction (Section 3.2) to the Unfiltered dataset to remove a portion of the negative training examples.", "labels": [], "entities": [{"text": "PRA-reduced dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7275871336460114}, {"text": "PRA reduction", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.5636523962020874}, {"text": "Unfiltered dataset", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.6646831184625626}]}, {"text": "Removing these examples produces a dataset that is smaller than Unfiltered and with a different bias.", "labels": [], "entities": [{"text": "Unfiltered", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.77109694480896}]}, {"text": "Changing the bias of the training data can influence the classification results.", "labels": [], "entities": []}, {"text": "Consequently the Random-reduced dataset was created by removing randomly selected negative examples from Unfiltered to produce a dataset with the same size and bias as PRA-reduced.", "labels": [], "entities": [{"text": "Random-reduced dataset", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.8134958744049072}]}, {"text": "The Random-reduced dataset is used to show that randomly removing negative instances leads to lower results than removing those suggested by PRA.", "labels": [], "entities": []}, {"text": "Evaluation: Two approaches were used to evaluate performance.", "labels": [], "entities": []}, {"text": "The Held-out datasets consist of the Unfiltered, PRA-reduced and Random-reduced data sets.", "labels": [], "entities": [{"text": "Unfiltered", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9086294770240784}]}, {"text": "The set of entity pairs obtained from the knowledge base is split into four parts and a process similar to 4-fold cross validation applied.", "labels": [], "entities": []}, {"text": "In each fold the automatically labelled sentences obtained from the pairs in 3 of the quarters are used as training data and sentences obtained from the remaining quarter used for testing.", "labels": [], "entities": []}, {"text": "The Manually labelled dataset contains 400 examples of the relation may-prevent and 400 of may-treat which were manually labelled by two annotators who were medical experts.", "labels": [], "entities": []}, {"text": "Both relations are taken from the ND-FRT subset of UMLS.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.5734388828277588}]}, {"text": "Each annotator was asked to label every sentence and then re-examine cases where there was disagreement.", "labels": [], "entities": []}, {"text": "This process lead to inter-annotator agreement of 95.5% for may-treat and 97.3% for may-prevent.", "labels": [], "entities": [{"text": "agreement", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.6207542419433594}]}, {"text": "The annotated data set is publicly available 2 . Any sentences in the training data containing an entity pair that occurs within the manually labelled dataset are removed.", "labels": [], "entities": []}, {"text": "Although this dataset is smaller than the held-out dataset, its annotations are more reliable and it is therefore likely to be a more accurate indicator of performance accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9711186289787292}]}, {"text": "This dataset is more balanced than the held-out data with a ratio of 1:1.3 for may-treat and 1:1.8 for may-prevent.", "labels": [], "entities": []}, {"text": "Evaluation metric: Our experiments use entity level evaluation since this is the most appropriate approach to determine suitability for database population.", "labels": [], "entities": []}, {"text": "Precision and recall are computed based on the proportion of entity pairs identified.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9852077960968018}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9991499185562134}]}, {"text": "For the held-out data the set of correct entity pairs are those which occur in sentences labeled as positive examples of the relation and which are also listed as being related in UMLS.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 180, "end_pos": 184, "type": "DATASET", "confidence": 0.9221817851066589}]}, {"text": "For the manually labelled data it is simply the set of entity pairs that occur in positive examples of the relation.", "labels": [], "entities": []}, {"text": "shows the results obtained using the heldout data.", "labels": [], "entities": []}, {"text": "Overall results, averaged across all relations with maximum recall, are shown in the top portion of the table and indicate that applying PRA improves performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9971580505371094}]}, {"text": "Although the highest precision is obtained using the Unfiltered classifier, the PRA-reduced classifier leads to the best recall and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9977192282676697}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.999739944934845}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9995551705360413}]}, {"text": "Performance of the Random-reduced classifier indicates that the improvement is not simply due to a change in the bias in the data but that the examples it contains lead to an improved model.", "labels": [], "entities": []}, {"text": "The lower part of shows results for each relation.", "labels": [], "entities": []}, {"text": "The PRA-reduced classifier produces the best results for the majority of relations and always increases recall compared to Unfiltered.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9995061159133911}]}], "tableCaptions": [{"text": " Table 2: Evaluation using held-out data", "labels": [], "entities": []}, {"text": " Table 3: Evaluation using manually labelled data", "labels": [], "entities": []}]}