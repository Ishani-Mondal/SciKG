{"title": [{"text": "Environment-Driven Lexicon Induction for High-Level Instructions", "labels": [], "entities": [{"text": "Environment-Driven Lexicon Induction", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5508405963579813}]}], "abstractContent": [{"text": "We focus on the task of interpreting complex natural language instructions to a robot, in which we must ground high-level commands such as microwave the cup to low-level actions such as grasping.", "labels": [], "entities": [{"text": "interpreting complex natural language instructions", "start_pos": 24, "end_pos": 74, "type": "TASK", "confidence": 0.8634314775466919}]}, {"text": "Previous approaches that learn a lexicon during training have inadequate coverage attest time, and pure search strategies cannot handle the exponential search space.", "labels": [], "entities": []}, {"text": "We propose anew hybrid approach that leverages the environment to induce new lexical entries attest time, even for new verbs.", "labels": [], "entities": []}, {"text": "Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7350251972675323}]}, {"text": "We introduce anew dataset and show that our approach is able to successfully ground new verbs such as distribute , mix, arrange to complex logical forms, each containing up to four predicates .", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of mapping natural language instructions to actions fora robot has been gaining momentum in recent years (.", "labels": [], "entities": []}, {"text": "We are particularly interested in instructions containing verbs such as \"microwave\" denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp.", "labels": [], "entities": []}, {"text": "In this setting, it is common to find new verbs requiring new concepts attest time.", "labels": [], "entities": []}, {"text": "For example, in Figure 1, suppose that we have never seen the verb \"fill\".", "labels": [], "entities": []}, {"text": "Can we impute the correct interpretation, and moreover seize the opportunity to learn what \"fill\" means in away that generalizes to future instructions?", "labels": [], "entities": []}, {"text": "Text: \"get the cup, fill it with water and then microwave the cup\" grasping cup 3 \u2227 near(robot 1 ,cup 3 ) in cup 3 ,microwave \u2227 state(microwave 1 ,is-on) state cup 3 ,water \u2227 on(cup 3 ,sink) Unseen verb \" fill \" is grounded attest time using environment.", "labels": [], "entities": []}, {"text": "Figure 1: A lexicon learned on the training data cannot possibly coverall the verb-concept mappings needed attest time.", "labels": [], "entities": []}, {"text": "Our algorithm learns the meaning of new verbs (e.g., \"fill\") using the environment context.", "labels": [], "entities": []}, {"text": "Previous work in semantic parsing handles lexical coverage in one of two ways.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.8347437381744385}]}, {"text": "induces a highly constrained CCG lexicon capable of mapping words to complex logical forms, but it would have to skip new words (which in would lead to microwaving an empty cup).) take a freer approach by performing a search over logical forms, which can handle new words, but the logical forms there are much simpler than the ones we consider.", "labels": [], "entities": []}, {"text": "In this paper, we present an hybrid approach that uses a lexicon to represent complex concepts but also strongly leverages the environment to guide the search space.", "labels": [], "entities": []}, {"text": "The environment can provide helpful cues in several ways: \u2022 Only a few environments are likely fora given scenario-e.g., the text is unlikely to ask the robot to microwave an empty cup or put books on the floor.", "labels": [], "entities": []}, {"text": "\u2022 The logical form of one segment of text constrains that of the next segment-e.g., the text is unlikely to ask the robot to pick a cup and then put it back immediately in the same spot.", "labels": [], "entities": []}, {"text": "We show that this environment context provides: Graphical model overview: we first deterministically shallow parse the text x into a control flow graph consisting of shallow structures {c i }.", "labels": [], "entities": []}, {"text": "Given an initial environment e 1 , our semantic parsing model maps these frame nodes to logical forms {z i } representing the postconditions.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7432243824005127}]}, {"text": "From this, a planner and simulator generate the action sequences {a i } and resulting environments {e i }.", "labels": [], "entities": []}, {"text": "a signal for inducing new lexical entries that map previously unseen verbs to novel concepts.", "labels": [], "entities": []}, {"text": "In the example in, the algorithm learns that microwaving an empty cup is unlikely and this suggests that the verb \"fill\" must map to actions that end up making the cup not empty.", "labels": [], "entities": []}, {"text": "Another contribution of this paper is using postconditions as logical forms rather than actions, as in previous work . Postconditions not only reduce the search space of logical forms, but are also a more natural representation of verbs.", "labels": [], "entities": []}, {"text": "We define a conditional random field (CRF) model over postconditions, and use a planner to convert postconditions into action sequences and a simulator to generate new environments.", "labels": [], "entities": []}, {"text": "At test time, we use the lexicon induced from the training data, but also perform an environmentguided search over logical forms to induce new lexical entries on-the-fly.", "labels": [], "entities": []}, {"text": "If the predicted action sequence uses anew lexical entry generated by the search, it is added to the lexicon, where it can be reused in subsequent test examples.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm on anew corpus containing text commands fora household robot.", "labels": [], "entities": []}, {"text": "The two key findings of our experiments are: First, the environment and task context contain enough information to allow us to learn lexical entries for new verbs such as \"distribute\" and \"mix\" with complex semantics.", "labels": [], "entities": []}, {"text": "Second, using both lexical entries generated by a test-time search and those from the lexicon induced by the training data outperforms the two individual approaches.", "labels": [], "entities": []}, {"text": "This suggests that environment context can help alleviate the problem of having a limited lexicon for grounded language acquisition.", "labels": [], "entities": [{"text": "grounded language acquisition", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.6995781858762106}]}], "datasetContent": [{"text": "We collected a dataset of 500 examples from 62 people using a crowdsourcing system similar to . We consider two different 3D scenarios: a kitchen and a living room, each containing an average of 40 objects.", "labels": [], "entities": []}, {"text": "Both of these scenarios have 10 environments consisting of different sets of objects in different configurations.", "labels": [], "entities": []}, {"text": "We define 10 high-level objectives, 5 per scenario, such as clean the room, make coffee, prepare room for movie night, etc.", "labels": [], "entities": []}, {"text": "One group of users wrote natural language commands to achieve the high-level objectives.", "labels": [], "entities": []}, {"text": "Another group controlled a virtual robot to accomplish the commands given by the first group.", "labels": [], "entities": []}, {"text": "The dataset contains considerable variety, consisting of 148 different verbs, an average of 48.7 words per text, and an average of 21.5 actions per action sequence.", "labels": [], "entities": []}, {"text": "Users make spelling and grammar errors in addition to occasionally taking random actions not relevant to the text.", "labels": [], "entities": []}, {"text": "The supplementary material contains more details.", "labels": [], "entities": []}, {"text": "We filtered out 31 examples containing fewer than two action sequences.", "labels": [], "entities": []}, {"text": "Of the remaining examples, 378 were used for training and 91 were used for test.", "labels": [], "entities": []}, {"text": "Our algorithm is tested on four new environments (two from each scenario).", "labels": [], "entities": []}, {"text": "We consider two metrics, IED and END, which measure accuracy based on the action sequence and environment, respectively.", "labels": [], "entities": [{"text": "IED", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9392437934875488}, {"text": "END", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9774437546730042}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9984984397888184}]}, {"text": "Specifically, the IED metric ( ) is the edit distance between predicted and true action sequence.", "labels": [], "entities": [{"text": "IED metric", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.8755132257938385}]}, {"text": "The END metric is the Jaccard index of sets A and B, where A is the set of atoms (e.g., on(cup 1 ,table 1 )) whose truth value changed due to simulating the predicted action sequence, and B is that of the true action sequence.", "labels": [], "entities": [{"text": "END", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6657713055610657}]}, {"text": "We compare our algorithm with the following baselines:: Results on the metrics and baselines described in section 9.2.", "labels": [], "entities": []}, {"text": "The numbers are normalized to 100 with larger values being better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on the metrics and baselines de- scribed in section 9.2. The numbers are normal- ized to 100 with larger values being better.", "labels": [], "entities": []}]}