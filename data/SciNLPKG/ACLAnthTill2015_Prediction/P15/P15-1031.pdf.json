{"title": [{"text": "An Effective Neural Network Model for Graph-based Dependency Parsing", "labels": [], "entities": [{"text": "Graph-based Dependency Parsing", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.626046895980835}]}], "abstractContent": [{"text": "Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.8382843136787415}, {"text": "parsing", "start_pos": 145, "end_pos": 152, "type": "TASK", "confidence": 0.9650242924690247}]}, {"text": "In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing.", "labels": [], "entities": [{"text": "graph-based dependency parsing", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.657096395889918}]}, {"text": "Our model can automatically learn high-order feature combinations using only atomic features by exploiting a novel activation function tanh-cube.", "labels": [], "entities": []}, {"text": "Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers.", "labels": [], "entities": []}, {"text": "Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.9629393617312113}]}], "introductionContent": [{"text": "Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8017467260360718}]}, {"text": "Due to its importance, dependency parsing, has been studied for tens of years.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9377008378505707}]}, {"text": "Among a variety of dependency parsing approaches), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7677435874938965}]}, {"text": "Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs.", "labels": [], "entities": []}, {"text": "In these models, subgraphs are usually represented as a high-dimensional feature vectors * Corresponding author which are fed into a linear model to learn the feature weight for scoring the subgraphs.", "labels": [], "entities": []}, {"text": "In spite of their advantages, conventional graphbased models rely heavily on an enormous number of hand-crafted features, which brings about serious problems.", "labels": [], "entities": []}, {"text": "First, amass of features could put the models in the risk of overfitting and slowdown the parsing speed, especially in the highorder models where combinational features capturing interactions between head, modifier, siblings and (or) grandparent could easily explode the feature space.", "labels": [], "entities": [{"text": "parsing", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.9658267498016357}]}, {"text": "In addition, feature design requires domain expertise, which means useful features are likely to be neglected due to alack of domain knowledge.", "labels": [], "entities": [{"text": "feature design", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7491087317466736}]}, {"text": "As a matter of fact, these two problems exist inmost graph-based models, which have stuck the development of dependency parsing fora few years.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7874598205089569}]}, {"text": "To ease the problem of feature engineering, we propose a general and effective Neural Network model for graph-based dependency parsing in this paper.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8278037905693054}, {"text": "graph-based dependency parsing", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.6910666823387146}]}, {"text": "The main advantages of our model are as follows: \u2022 Instead of using large number of hand-crafted features, our model only uses atomic features ( ) such as word unigrams and POS-tag unigrams.", "labels": [], "entities": []}, {"text": "Feature combinations and high-order features are automatically learned with our novel activation function tanh-cube, thus alleviating the heavy burden of feature engineering in conventional graph-based models;.", "labels": [], "entities": []}, {"text": "Not only does it avoid the risk of overfitting but also it discovers useful new features that have never been used in conventional parsers.", "labels": [], "entities": []}, {"text": "\u2022 We propose to exploit phrase-level information through distributed representation for phrases (phrase embeddings).", "labels": [], "entities": []}, {"text": "It not only en- ables our model to exploit richer context information that previous work did not consider due to the curse of dimension but also captures inherent correlations between phrases.", "labels": [], "entities": []}, {"text": "\u2022 Unlike other neural network based models where an additional parser is needed for either extracting features ) or generating k-best list for reranking, both training and decoding in our model are performed based on our neural network architecture in an effective way.", "labels": [], "entities": []}, {"text": "\u2022 Our model does not impose any change to the decoding process of conventional graphbased parsing model.", "labels": [], "entities": []}, {"text": "First-order, secondorder and higher order models can be easily implemented using our model.", "labels": [], "entities": []}, {"text": "We implement three effective models with increasing expressive capabilities.", "labels": [], "entities": []}, {"text": "The first model is a simple first-order model that uses only atomic features and does not use any combinational features.", "labels": [], "entities": []}, {"text": "Despite its simpleness, it outperforms conventional first-order model) and has a faster parsing speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.9685826897621155}]}, {"text": "To further strengthen our parsing model, we incorporate phrase embeddings into the model, which significantly improves the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9725663065910339}, {"text": "parsing", "start_pos": 123, "end_pos": 130, "type": "TASK", "confidence": 0.9595925211906433}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.860602617263794}]}, {"text": "Finally, we extend our first-order model to a secondorder model that exploits interactions between two adjacent dependency edges as in thus further improves the model performance.", "labels": [], "entities": []}, {"text": "We evaluate our models on the English Penn Treebank.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.9416765968004862}]}, {"text": "Experiment results show that both our first-order and second-order models outperform the corresponding conventional models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the English Penn Treebank (PTB) to evaluate our model implementations and head rules are used to extract dependency trees.", "labels": [], "entities": [{"text": "English Penn Treebank (PTB)", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.8970707952976227}]}, {"text": "We follow the standard splits of PTB3, using section 2-21 for training, section 22 as development set and 23 as test set.", "labels": [], "entities": [{"text": "PTB3", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9766984581947327}]}, {"text": "The Stanford POS Tagger () with ten-way jackknifing of the training data is used for assigning POS tags (accuracy \u2248 97.2%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9992269277572632}]}, {"text": "Hyper-parameters of our models are tuned on the development set and their final settings are as follows: embedding size d = 50, hidden layer (Layer 2) size = 200, regularization parameter \u03bb = 10 \u22124 , discount parameter for margin loss \u03ba = 0.3, initial learning rate of AdaGrad alpha = 0.1.", "labels": [], "entities": []}, {"text": "compares our models with several conventional graph-based parsers.", "labels": [], "entities": []}, {"text": "We use MSTParser 2 for conventional first-order model and second-order model).", "labels": [], "entities": []}, {"text": "We also include the result of a third-order model of for comparison . For our models, we report the results with and without unsupervised pre-training.", "labels": [], "entities": []}, {"text": "Pretraining only trains the word-based feature embeddings on Gigaword corpus () using word2vec 4 and all other parameters are still initialized randomly.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.9422828257083893}]}, {"text": "In all experiments, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation is excluded in all evaluation metrics.", "labels": [], "entities": [{"text": "unlabeled attachment scores (UAS)", "start_pos": 30, "end_pos": 63, "type": "METRIC", "confidence": 0.7805944432814916}, {"text": "labeled attachment scores (LAS)", "start_pos": 68, "end_pos": 99, "type": "METRIC", "confidence": 0.8509272038936615}]}, {"text": "The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9762998819351196}]}, {"text": "As we can see, even with random initialization, 1-order-atomic-rand performs as well as conventional first-order model and both 1-order-phrase- rand and 2-order-phrase-rand perform better than conventional models in MSTParser.", "labels": [], "entities": []}, {"text": "Pretraining further improves the performance of all three models, which is consistent with the conclusion of previous work).", "labels": [], "entities": [{"text": "Pretraining", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9663453102111816}]}, {"text": "Moreover, 1-order-phrase performs better than 1-order-atomic, which shows that phrase embeddings do improve the model.", "labels": [], "entities": []}, {"text": "2-order-phrase further improves the performance because of the more expressive second-order factorization.", "labels": [], "entities": []}, {"text": "All three models perform significantly better than their counterparts in MSTParser where millions of features are used and 1-order-phrase works surprisingly well that it even beats the conventional second-order model.", "labels": [], "entities": []}, {"text": "With regard to parsing speed, 1-order-atomic is the fastest while other two models have similar speeds as MSTParser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9726327061653137}, {"text": "MSTParser", "start_pos": 106, "end_pos": 115, "type": "DATASET", "confidence": 0.9062813520431519}]}, {"text": "Further speedup could be achieved by using pre-computing strategy as mentioned in.", "labels": [], "entities": []}, {"text": "We did not try this strategy since parsing speed is not the main focus of this paper.", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9812929034233093}]}], "tableCaptions": [{"text": " Table 2: Comparison with conventional graph-based models.", "labels": [], "entities": []}]}