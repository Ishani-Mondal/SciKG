{"title": [{"text": "Compositional Vector Space Models for Knowledge Base Completion", "labels": [], "entities": []}], "abstractContent": [{"text": "Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y).", "labels": [], "entities": [{"text": "Knowledge base (KB) completion", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5979531904061636}]}, {"text": "Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop re-lational path treated as anatomic feature, like bornIn(X,Z) \u2192 containedIn(Z,Y).", "labels": [], "entities": []}, {"text": "This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path.", "labels": [], "entities": []}, {"text": "Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning).", "labels": [], "entities": []}, {"text": "We assemble anew dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained em-beddings by 7%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Constructing large knowledge bases (KBs) supports downstream reasoning about resolved entities and their relations, rather than the noisy textual evidence surrounding their natural language mentions.", "labels": [], "entities": []}, {"text": "For this reason KBs have been of increasing interest in both industry and academia.", "labels": [], "entities": []}, {"text": "Such KBs typically contain many millions of facts, most of them (entity1,relation,entity2) \"triples\" (also known as binary relations) such as (Barack Obama, presidentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie).", "labels": [], "entities": [{"text": "marriedTo", "start_pos": 191, "end_pos": 200, "type": "DATASET", "confidence": 0.9643779397010803}]}, {"text": "However, even the largest KBs are woefully incomplete (, missing many important facts, and therefore damaging their usefulness in downstream tasks.", "labels": [], "entities": []}, {"text": "Ironically, these missing facts can frequently be inferred from other facts already in the KB, thus representing a sort of inconsistency that can be repaired by the application of an automated process.", "labels": [], "entities": []}, {"text": "The addition of new triples by leveraging existing triples is typically known as KB completion.", "labels": [], "entities": []}, {"text": "Early work on this problem focused on learning symbolic rules.", "labels": [], "entities": []}, {"text": "For example, learns Horn clauses predictive of new binary relations by exhausitively exploring relational paths of increasing length, and selecting those surpassing an accuracy threshold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9979308843612671}]}, {"text": "(A \"path\" is a sequence of triples in which the second entity of each triple matches the first entity of the next triple.) introduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier.", "labels": [], "entities": []}, {"text": "A typical predictive feature learned by PRA is that CountryOfHeadquarters(X, Y) is implied by IsBasedIn(X,A) and StateLocatedIn(A, B) and CountryLocatedIn(B, Y).", "labels": [], "entities": []}, {"text": "Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can infer the fact CountryOfHeadquarters(Microsoft, USA) using the predictive feature.", "labels": [], "entities": [{"text": "CountryLocatedIn", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.8827389478683472}]}, {"text": "In later work, greatly increase available raw material for paths by augmenting KB-schema relations with relations defined by the text connecting mentions of entities in a large corpus (also known as OpenIE relations ().", "labels": [], "entities": []}, {"text": "However, these symbolic methods can produce many millions of distinct paths, each of which is categorically distinct, treated by PRA as a dis-tinct feature.", "labels": [], "entities": []}, {"text": "(See) Even putting aside the OpenIE relations, this limits the applicability of these methods to modern KBs that have thousands of relation types, since the number of distinct paths increases rapidly with the number of relation types.", "labels": [], "entities": []}, {"text": "If textually-defined OpenIE relations are included, the problem is obviously far more severe.", "labels": [], "entities": []}, {"text": "Better generalization can be gained by operating on embedded vector representations of relations, in which vector similarity can be interpreted as semantic similarity.", "labels": [], "entities": []}, {"text": "For example, learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations.", "labels": [], "entities": []}, {"text": "This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7565523386001587}, {"text": "KB completion", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.8312321305274963}]}, {"text": "Similarly perform KB completion by learning embeddings of relations, but based on matrices or tensors.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.8259571194648743}]}, {"text": "Universal schema () learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7456634938716888}]}, {"text": "Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations.", "labels": [], "entities": []}, {"text": "This paper proposes an approach combining the advantages of (a) reasoning about conjunctions of relations connected in a path, and (b) generalization through vector embeddings, and (c) reasoning non-atomically and compositionally about the elements of the path, for further generalization.", "labels": [], "entities": []}, {"text": "Our method uses recurrent neural networks (RNNs) to compose the semantics of relations in an arbitrary-length path.", "labels": [], "entities": []}, {"text": "At each path-step it consumes both the vector embedding of the next relation, and the vector representing the path-so-far, then outputs a composed vector (representing the extended path-so-far), which will be the input to the next step.", "labels": [], "entities": []}, {"text": "After consuming a path, the RNN should output a vector in the semantic neighborhood of the relation between the first and last entity of the path.", "labels": [], "entities": []}, {"text": "For example, after consuming the relation vectors along the path Melinda Gates \u2192 Bill Gates \u2192 Microsoft \u2192 Seattle, our method produces a vector very close to the relation livesIn.", "labels": [], "entities": []}, {"text": "Our compositional approach allow us attest time to make predictions from paths that were unseen during training, because of the generalization provided by vector neighborhoods, and because they are composed in non-atomic fashion.", "labels": [], "entities": []}, {"text": "This allows our model to seamlessly perform inference on many millions of paths in the KB graph.", "labels": [], "entities": []}, {"text": "In most of our experiments, we learn a separate RNN for predicting each relation type, but alternatively, by learning a single high-capacity composition function for all relation types, our method can perform zero-shot learning-predicting new relation types for which the composition function was never explicitly trained.", "labels": [], "entities": []}, {"text": "Related to our work, new versions of PRA () use pre-trained vector representations of relations to alleviate its feature explosion problem-but the core mechanism continues to be a classifier based on atomic-path features.", "labels": [], "entities": []}, {"text": "In the 2013 work many paths are collapsed by clustering paths according to their relations' embeddings, and substituting cluster ids for the original relation types.", "labels": [], "entities": []}, {"text": "In the 2014 work unseen paths are mapped to nearby paths seen at training time, where nearness is measured using the embeddings.", "labels": [], "entities": []}, {"text": "Neither is able to perform zero-shot learning since there must be a classifer for each predicted relation type.", "labels": [], "entities": []}, {"text": "Furthermore their pre-trained vectors do not have the opportunity to be tuned to the KB completion task because the two sub-tasks are completely disentangled.", "labels": [], "entities": [{"text": "KB completion task", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.6233603358268738}]}, {"text": "An additional contribution of our work is anew large-scale data set of over 52 million triples, and its preprocessing for purposes of path-based KB completion (can be downloaded from http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz).", "labels": [], "entities": [{"text": "KB completion", "start_pos": 145, "end_pos": 158, "type": "TASK", "confidence": 0.721335232257843}]}, {"text": "The dataset is build from the combination of Freebase () and Google's entity linking in ClueWeb (.", "labels": [], "entities": []}, {"text": "Rather than Gardner's 1000 distinct paths per relation type, we have over 2 million.", "labels": [], "entities": []}, {"text": "Rather than Gardner's 200 entity pairs, we use over 10k.", "labels": [], "entities": []}, {"text": "All experimental comparisons below are performed on this new data set.", "labels": [], "entities": []}, {"text": "On this challenging large-scale dataset our compositional method outperforms PRA (, and Cluster PRA () by 11% and 7% respectively.", "labels": [], "entities": []}, {"text": "A further contribution of our work is anew, surprisingly strong baseline method using classifiers of path bigram features, which beats PRA and Cluster PRA, and statistically ties our compositional method.", "labels": [], "entities": []}, {"text": "Our analysis shows that our method has substantially different strengths than the new baseline, and the combination of the two yields a 15% improvement over.", "labels": [], "entities": []}, {"text": "We also show that our zeroshot model is indeed capable of predicting new unseen relation types.", "labels": [], "entities": [{"text": "predicting new unseen relation types", "start_pos": 58, "end_pos": 94, "type": "TASK", "confidence": 0.8352384805679322}]}], "datasetContent": [{"text": "The hyperparameters of all the models were tuned on the same held-out development data.", "labels": [], "entities": []}, {"text": "All the neural network models are trained for 150 iterations using 50 dimensional relation vectors, and we set the L2-regularizer and learning rate to 0.0001 and 0.1 respectively.", "labels": [], "entities": []}, {"text": "We halved the learning rate after every 60 iterations and use minibatches of size 20.", "labels": [], "entities": []}, {"text": "The neural networks and the classifiers were optimized using AdaGrad).", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9462047219276428}]}], "tableCaptions": [{"text": " Table 2: Predictive paths, according to the RNN model, for 4 target relations. Two examples of seen and  unseen paths are shown for each target relation. Inverse relations are marked by \u22121 , i.e, r(x, y) =\u21d2  r \u22121 (y, x), \u2200(x, y) \u2208 r. Relations within quotes are OpenIE (textual) relation types.", "labels": [], "entities": []}, {"text": " Table 3: Results comparing different methods on 46 types. All the methods perform better when trained  using all the paths than training using the top 1, 000 paths. When training with all the paths, RNN  performs significantly (p < 0.005) better than PRA Classifier and Cluster PRA Classifier. The small  difference in performance between RNN and both PRA Classifier-b and Cluster PRA Classifier-b is not  statistically significant. The best results are obtained by combining the predictions of RNN with PRA  Classifier-b which performs significantly (p < 10 \u22125 ) better than both PRA Classifier-b and Cluster PRA  Classifier-b.", "labels": [], "entities": []}, {"text": " Table 4: Results comparing the zero-shot model  with supervised RNN and a random baseline on  10 types. RNN is the fully supervised model de- scribed in section 3 while zero-shot is the model  described in section 4. The zero-shot model with- out explicitly training for the target relation types  achieves impressive results by performing signifi- cantly (p < 0.05) better than a random baseline.", "labels": [], "entities": []}]}