{"title": [{"text": "Learning Word Reorderings for Hierarchical Phrase-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Learning Word Reorderings", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6209794481595358}, {"text": "Hierarchical Phrase-based Statistical Machine Translation", "start_pos": 30, "end_pos": 87, "type": "TASK", "confidence": 0.6238515436649322}]}], "abstractContent": [{"text": "Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 90, "end_pos": 134, "type": "TASK", "confidence": 0.5925807505846024}]}, {"text": "Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words.", "labels": [], "entities": []}, {"text": "This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality.", "labels": [], "entities": []}, {"text": "Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information .", "labels": [], "entities": [{"text": "word reordering information", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.7775826652844747}]}], "introductionContent": [{"text": "The hierarchical phrase-based model) is capable of capturing rich translation knowledge with the synchronous context-free grammar.", "labels": [], "entities": []}, {"text": "But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence.", "labels": [], "entities": []}, {"text": "used a log-linear model to compute rule weights with features similar to Pharaoh (.", "labels": [], "entities": []}, {"text": "However, to select appropriate rules, more effective criteria are required.", "labels": [], "entities": []}, {"text": "A lot of work has been done for better rule selection.  and  used maximum entropy approaches to integrate rich contextual information for target side rule selection.", "labels": [], "entities": [{"text": "rule selection.", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.9210118055343628}, {"text": "target side rule selection", "start_pos": 138, "end_pos": 164, "type": "TASK", "confidence": 0.5995705798268318}]}, {"text": "proposed a joint model to select hierarchical rules for both source and target sides.", "labels": [], "entities": []}, {"text": "demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating's word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating.", "labels": [], "entities": [{"text": "word reordering information within hierarchical phrase-based SMT", "start_pos": 40, "end_pos": 104, "type": "TASK", "confidence": 0.5764307337147849}]}, {"text": "proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with's model and still achieved significant reordering improvement over the baseline system.", "labels": [], "entities": []}, {"text": "In this paper, we incorporate word reordering information into hierarchical phrase-based SMT by training a series of separate reordering submodels for word pairs with different distances.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.7845904231071472}]}, {"text": "We will demonstrate that the translation performance achieves consistent improvement as more sub-models for longer distance reorderings being integrated, but the improvement levels off quickly.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9597188830375671}]}, {"text": "That means sub-models for reordering distance longer than a given threshold do not improve translation quality significantly.", "labels": [], "entities": []}, {"text": "Compared with previous models, our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.6877025961875916}]}, {"text": "Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost.", "labels": [], "entities": []}, {"text": "Phrase reordering models have also been integrated into hierarchical phrase-based SMT.", "labels": [], "entities": [{"text": "Phrase reordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.91648268699646}, {"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.7764876484870911}]}, {"text": "Phrase reordering models were originally developed for phrase-based SMT () and could not be used in hierarchical phrase-based model directly. and proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT.", "labels": [], "entities": [{"text": "Phrase reordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8207161128520966}, {"text": "phrase-based SMT", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.5698301792144775}, {"text": "phrasebased SMT", "start_pos": 218, "end_pos": 233, "type": "TASK", "confidence": 0.5077950656414032}]}, {"text": "However, their work limited to learning the reordering of continuous phrases.", "labels": [], "entities": []}, {"text": "For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like's work, while our model can be applied to word pairs with longer distances.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed approach for Chineseto-English (CE) and Japanese-to-English (JE) translation tasks.", "labels": [], "entities": [{"text": "Japanese-to-English (JE) translation tasks", "start_pos": 66, "end_pos": 108, "type": "TASK", "confidence": 0.6974078267812729}]}, {"text": "The official datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used.", "labels": [], "entities": [{"text": "patent machine translation task", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.64683797955513}, {"text": "NTCIR-9", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9461182355880737}]}, {"text": "The detailed statistics for training, development and test sets are given in   In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task.", "labels": [], "entities": [{"text": "NTCIR-9", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9647529721260071}, {"text": "JE task", "start_pos": 197, "end_pos": 204, "type": "TASK", "confidence": 0.6432021856307983}]}, {"text": "Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task.", "labels": [], "entities": [{"text": "NTCIR-8 JE test set", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.907752275466919}, {"text": "JE task", "start_pos": 89, "end_pos": 96, "type": "TASK", "confidence": 0.8674946427345276}]}, {"text": "The word segmentation was done by BaseSeg () for Chinese and Mecab 2 for Japanese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.673554465174675}]}, {"text": "To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ and the grow-diag-finaland heuristic ().", "labels": [], "entities": []}, {"text": "The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models.", "labels": [], "entities": []}, {"text": "Neural reordering models were trained by the toolkit NPLM (.", "labels": [], "entities": [{"text": "Neural reordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8103159964084625}, {"text": "NPLM", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.9062967300415039}]}, {"text": "For CE task, training instances extracted from all the 1M sentence pairs were used to train neural reordering models.", "labels": [], "entities": []}, {"text": "For JE task, training instances were from 1M sentence pairs that were randomly selected from all the 3.14M sentence pairs.", "labels": [], "entities": [{"text": "JE task", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.929405152797699}]}, {"text": "We  For each translation task, the recent version of the Moses hierarchical phrase-based decoder () with the training scripts was used as the baseline system Base.", "labels": [], "entities": [{"text": "translation task", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.9138813316822052}]}, {"text": "We used the default parameters for Moses.", "labels": [], "entities": []}, {"text": "A 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit 3 with the improved Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "IRST LM Toolkit 3", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.921392098069191}]}, {"text": "We integrated our reordering models into Base.", "labels": [], "entities": []}, {"text": "\"Hayashi model\" represents the method of (.", "labels": [], "entities": []}, {"text": "\"M j 1 (j = 1, 2, 3, 4)\" means that Base was augmented with the reordering scores calcuated from a series of sub-models M 1 to M j . As shown in, integrating only M 1 , which predicts reordering for two continuous source words, has already given BLEU improvement 1.8% and 1.2% over baseline on CE and JE, respectively.", "labels": [], "entities": [{"text": "Base", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9838443398475647}, {"text": "BLEU", "start_pos": 246, "end_pos": 250, "type": "METRIC", "confidence": 0.999480664730072}]}, {"text": "As more sub-models for longer distance reordering being integrated, the translation performance improved consistently, though the improvement leveled off quickly.", "labels": [], "entities": [{"text": "translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9507513046264648}]}, {"text": "For CE and JE tasks, Mn with n \u2265 3 and n \u2265 4, respectively, cannot give further performance improvement at any significant level.", "labels": [], "entities": [{"text": "JE tasks", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.5840560793876648}]}, {"text": "Why did the improvement level off quickly?: Classification accuracy (%).", "labels": [], "entities": [{"text": "Classification", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.8126118779182434}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9515591859817505}]}, {"text": "In other words, why do long distance reordering models have a much less leverage over translation performance than short ones?", "labels": [], "entities": []}, {"text": "First, the prediction accuracy decreases as the reordering distance increasing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9592784643173218}]}, {"text": "gives classification accuracies on the validation data for each sub-model.", "labels": [], "entities": []}, {"text": "The reason for accuracy decreasing is that the input size of sub-model grows as reordering distance increasing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9989380240440369}]}, {"text": "Namely, long distance reordering needs to consider more complicated context.", "labels": [], "entities": [{"text": "long distance reordering", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6494511564572653}]}, {"text": "Second, we attribute the influence decrease of the longer reordering models to the redundancy of the predictions among different reordering models.", "labels": [], "entities": []}, {"text": "For example, in, when word pairs \"(guy) (is)\" and \"(is) (James)\" are both predicted to be not reversed, the reordering for \"(guy) (James)\" can be logically determined to be not reversed without further reordering model prediction.", "labels": [], "entities": []}, {"text": "That means, sometimes, along distance word reordering can be determined by a series of shorter word reordering pairs.", "labels": [], "entities": []}, {"text": "But still, some predictions for longer reordering are useful.", "labels": [], "entities": []}, {"text": "For example, the reordering of \"(wears) (guy)\" cannot be determined when \"(wears) (glasses)\" is predicted to be not reversed and \"(glasses) (guy)\" is reversed.", "labels": [], "entities": []}, {"text": "This is the reason why translation performance improves as more sub-models being integrated.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9740288257598877}]}, {"text": "As shown in, with 4 sub-models being integrated, our model improved baseline system significantly and also outperformed Hayashi model clearly.", "labels": [], "entities": []}, {"text": "It is easy to understand, since our model was trained by feed-forward neural network on a high dimensional space and incorporated rich context information, while Hayashi model used the averaged perceptron algorithm and simple features.", "labels": [], "entities": []}, {"text": "shows the prediction accuracies of Hayashi model.", "labels": [], "entities": []}, {"text": "Note that Hayashi model predicts reorderings for all word pairs, but only prediction accuracies for word pairs with distance 4 or less are shown.", "labels": [], "entities": []}, {"text": "Compared with, the prediction accuracy of our model is much higher than Hayashi model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9272764325141907}]}, {"text": "Actually, FNN is not suitable for Hayashi model since the computation cost for Hayashi model is quite expensive.", "labels": [], "entities": [{"text": "FNN", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.4962449371814728}]}, {"text": "Using FNN to reorder all word pairs could cost nearly one minute to translate one sentence according to our experiments, while integrating 4 sub-models only cost 10 seconds . Compared with Hayashi model, our model not only speeds up decoding time but also reduces the training time.", "labels": [], "entities": []}, {"text": "Training for Hayashi model is much slower since word pairs with all different distances are used as training data.", "labels": [], "entities": []}, {"text": "By using separate sub-models, we can train each sub-model one by one and stop when translation performance cannot be improved anymore.", "labels": [], "entities": []}, {"text": "However, despite of efficiency, one unified model will theoretically have better performance than separate sub-models since separate sub-models do not share training instances and the unified model will suffer less from data sparsity.", "labels": [], "entities": []}, {"text": "So, we did some extra experiments and trained a neural network which had the same structure as M 4 to learn reorderings for all word pairs with distance 4 or less, instead of using 4 separate neural networks.", "labels": [], "entities": []}, {"text": "A specific word null was used since word pairs with distance 1,2,3 do not have enough inputs for M 4 . The significance test results showed that translation performance had no significant difference between one unified model and multiple sub-models.", "labels": [], "entities": [{"text": "translation", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.9629772901535034}]}, {"text": "This is because the training corpus for our model is quite large, so separate training sets are sufficient for each submodel to learn the reorderings well.", "labels": [], "entities": []}, {"text": "Besides, using neural networks to learn these sub-models on a continuous space can relieve the data sparsity problem to some extent.", "labels": [], "entities": []}, {"text": "Note that if we only integrate M 4 into Base, the translation quality of Base can be improved in our preliminary experiments.", "labels": [], "entities": []}, {"text": "But M 4 cannot predict reorderings for word pairs with distance less than 4.", "labels": [], "entities": []}, {"text": "So M 3 1 will be still needed for predicting reorderings of word pairs with distance 1,2,3.", "labels": [], "entities": [{"text": "M 3 1", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.8015146255493164}, {"text": "predicting reorderings of word pairs", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.8324497818946839}]}, {"text": "But after M 3 1 being integrated, M 4 will not be needed due to the redundancy of the predictions among Note that cache was used in all our experiments to reduce the expensive neural network computation cost and turned out to be very useful.", "labels": [], "entities": []}, {"text": "Without caching, integrating 4 sub-models could cost nearly 7 minutes to translate a sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Classification accuracy (%).", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7299454212188721}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9681145548820496}]}]}