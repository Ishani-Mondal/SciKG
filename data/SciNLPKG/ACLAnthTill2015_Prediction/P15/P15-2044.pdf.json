{"title": [{"text": "If all you have is a bit of the Bible: Learning POS taggers for truly low-resource language\u0161 language\u0161", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple method for learning part-of-speech taggers for languages like Akawaio, Aukan, or Cakchiquel-languages for which nothing but a translation of parts of the Bible exists.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6926647871732712}]}, {"text": "By aggre-gating over the tags from a few annotated languages and spreading them via word-alignment on the verses, we learn POS taggers for 100 languages, using the languages to bootstrap each other.", "labels": [], "entities": []}, {"text": "We evaluate our cross-lingual models on the 25 languages where test sets exist, as well as on another 10 for which we have tag dictionaries.", "labels": [], "entities": []}, {"text": "Our approach performs much better (20-30%) than state-of-the-art unsu-pervised POS taggers induced from Bible translations, and is often competitive with weakly supervised approaches that assume high-quality parallel corpora, representative monolingual corpora with perfect to-kenization, and/or tag dictionaries.", "labels": [], "entities": []}, {"text": "We make models for all 100 languages available .", "labels": [], "entities": []}], "introductionContent": [{"text": "Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages.", "labels": [], "entities": [{"text": "cross-lingual NLP", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.6777891218662262}]}, {"text": "While it has been observed repeatedly that using multiple source languages improves performance (), most available techniques work best for closely related languages.", "labels": [], "entities": []}, {"text": "In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8045313060283661}]}, {"text": "Most low-resource languages are non-Indo-European, and typically, their typological and geographic neighbors have sparse resources as well.", "labels": [], "entities": []}, {"text": "However, fora surprisingly large number of languages, translations of the Bible (or parts of it) exist.", "labels": [], "entities": []}, {"text": "Due to the canonical nature and the verse format, these translations are viable parallel data, albeit lacking annotation.", "labels": [], "entities": []}, {"text": "In our experiments, we use word alignments across all pairs of 100 parallel Bible translations to bootstrap annotation projections for those languages without any (even just weakly) supervised taggers.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7106800973415375}]}, {"text": "The projections provide both pseudo-annotated data as well as tag dictionaries for all languages.", "labels": [], "entities": []}, {"text": "We use both resources to train semi-supervised POS taggers following.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.8417151272296906}]}, {"text": "Our contributionWe present a novel approach to learning POS taggers for truly low-resource languages, where only a translation of (parts of) the Bible is available.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.7662998735904694}]}, {"text": "We obtain results competitive with approaches that assume the availability of larger volumes of more representative parallel corpora, perfectly tokenized monolingual corpora, and/or tag dictionaries for the target languages.", "labels": [], "entities": []}, {"text": "Additionally, we make the POS tagging models for 100 languages publicly available and extend the mappings in  for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.6842657923698425}]}, {"text": "The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our approach is a combination of simple techniques.", "labels": [], "entities": []}, {"text": "Part of the process is depicted in, and the algorithm is presented in Algorithm 1.", "labels": [], "entities": []}, {"text": "Assume we haven languages for which we assume the availability of m verses of the Bible.", "labels": [], "entities": []}, {"text": "We run IBM-2 1 on all n(n \u2212 1) pairs of languages.", "labels": [], "entities": []}, {"text": "Assume also manually POS-annotated training data  is available for the first k of these languages.", "labels": [], "entities": []}, {"text": "We then run taggers for these languages on the corresponding translations of the Bible to predict tags for all tokens in these translations.", "labels": [], "entities": []}, {"text": "We can think of this partially annotated multiparallel corpus as a tensor object.", "labels": [], "entities": []}, {"text": "Each column is a language l i , and each row averse v j (trivially sentence-aligned to the corresponding verses in the other columns).", "labels": [], "entities": []}, {"text": "In each cell of this matrix M (i, j, \u00b7), we have a sequence of word tokens.", "labels": [], "entities": []}, {"text": "For two languages, l 1 and l 2 , the word tokens in M (1, j, \u00b7) can be aligned (by IBM-2) to multiple word tokens in M (2, j, \u00b7), but not all words need to be aligned.", "labels": [], "entities": []}, {"text": "After running supervised POS taggers on the k languages for which we have training data, we have POS-annotated the word tokens ink columns of our tensor object.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7446473836898804}]}, {"text": "We then project the POS tag of each word token w to all other word tokens aligned tow.", "labels": [], "entities": [{"text": "POS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9751776456832886}]}, {"text": "In our experiments, k = 17 or 18 (if the target language is not one of the languages for which we have training data), which means each word token will potentially have many POS tags projected onto it.", "labels": [], "entities": []}, {"text": "Note that the number of tags can exceed 18, since many-to-many word alignments are allowed.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7213944792747498}]}, {"text": "We now use these projections to train POS taggers for the remaining n \u2212 k languages.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.7705236077308655}]}, {"text": "We use aggregated projected annotations as tokenlevel supervision.", "labels": [], "entities": []}, {"text": "We aggregate from the incoming projected POS tags by majority voting.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.8403797149658203}]}, {"text": "We also use the complete set of projections onto each word type in the target language as a type-level tag dictionary.", "labels": [], "entities": []}, {"text": "We combine the tag dictionary and the token-level projections to train discriminative, type-constrained POS taggers).", "labels": [], "entities": []}, {"text": "Below we refer to these POS taggers as using k sources (k-SRC).", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.7929565608501434}]}, {"text": "These n many POS taggers can now also be used to obtain predictions for all word tokens in our tensor object.", "labels": [], "entities": []}, {"text": "This corresponds to doing the second loop over lines 8-17 in Algorithm 1.", "labels": [], "entities": []}, {"text": "For each of our n languages, we thus complete the tensor by projecting tags into word tokens from then \u2212 1 remaining source languages.", "labels": [], "entities": []}, {"text": "For the k supervised languages, we project the tags produced by the supervised POS taggers rather than the tags obtained by projection.", "labels": [], "entities": []}, {"text": "We can then train our final POS taggers for all n languages -100, in our case -using projections from 99 languages (n-1-SRC).", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.7785020470619202}]}, {"text": "Note that we also train projected taggers for those languages for which we have annotated data.", "labels": [], "entities": []}, {"text": "This is to enable us to evaluate our methodology on more languages.", "labels": [], "entities": []}, {"text": "Algorithm 1 Train n taggers with supervision fork 1: Let M be a tensor with M (i, j, \u00b7) the word-aligned token sequence in the jth verse of the Bible in language i 2: for i \u2264 k do 3: Train TNT tagger for li using manually annotated data 4: for j \u2264 m do 5: Obtain POS predictions for M (i, j, \u00b7) 6: end for 7: end for 8: for I \u2208 {0, 1} do 9: if i > k, I = 1 then 10: Train TNT tagger for li using projected annotations in M (i, \u00b7, \u00b7) 11: end if 12: Populate M (i, \u00b7, \u00b7) by propagating tags across alignments 13: for i \u2264 n do 14: Use majority voting to obtain one tag per word 15: Obtain type-level tag dictionary from all the data 16: Train TNT/GAR tagger for li using projected annotations in M (i, \u00b7, \u00b7) and tag dictionary 17: end for 18: end for DataWe use the 100 translations of (parts of) the Bible available as part of the Edinburgh Multilingual Parallel Bible Corpus ( using k or n \u2212 1 source languages, leading to four taggers in total.", "labels": [], "entities": [{"text": "Edinburgh Multilingual Parallel Bible Corpus", "start_pos": 829, "end_pos": 873, "type": "DATASET", "confidence": 0.9532822728157043}]}, {"text": "Baselines Our baselines are two standard unsupervised POS induction algorithms: Brown clustering using the implementation by Percy Liang 4 and second-order unsupervised HMMs using logistic regression for emission probabilities, with and without our Bible tag dictionaries.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.8192562162876129}]}, {"text": "5 Upper bounds The weakly supervised system in Das and Petrov (2011) (DAS) relies on larger volumes of more representative and perfectly tokenized parallel data than we assume, as well as a representative sample of unlabeled data.", "labels": [], "entities": []}, {"text": "Such data is simply not available for many of the languages considered here.", "labels": [], "entities": []}, {"text": "The weakly supervised system in (LI) also relies on crowd-sourced type-level tag dictionaries, not available for most of the languages of concern to us.", "labels": [], "entities": []}, {"text": "We present their reported results.", "labels": [], "entities": []}, {"text": "Finally, we train the two base POS taggers (GAR and TNT) on the manually annotated data available for 17 of our languages, to be able to compare against state-of-the-art performance of supervised POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.7406804263591766}, {"text": "POS taggers", "start_pos": 196, "end_pos": 207, "type": "TASK", "confidence": 0.7468080222606659}]}], "tableCaptions": [{"text": " Table 1: Results on 25 test languages. Y=entire Bible available. N=only New Testament available.  T=manually annotated data available for training (but not used to obtain results for the language itself).  Unsupervised baselines are evaluated using optimal 1:1 mappings.", "labels": [], "entities": []}]}