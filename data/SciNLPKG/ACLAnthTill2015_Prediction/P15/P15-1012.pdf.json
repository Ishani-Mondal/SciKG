{"title": [{"text": "Joint Models of Disagreement and Stance in Online Debate", "labels": [], "entities": []}], "abstractContent": [{"text": "Online debate forums present a valuable opportunity for the understanding and modeling of dialogue.", "labels": [], "entities": [{"text": "understanding and modeling of dialogue", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.7535788178443908}]}, {"text": "To understand these debates, a key challenge is inferring the stances of the participants, all of which are interrelated and dependent.", "labels": [], "entities": []}, {"text": "While collectively modeling users' stances has been shown to be effective (Walker et al., 2012c; Hasan and Ng, 2013), there are many modeling decisions whose ramifications are not well understood.", "labels": [], "entities": []}, {"text": "To investigate these choices and their effects, we introduce a scalable unified probabilis-tic modeling framework for stance classification models that 1) are collective, 2) reason about disagreement, and 3) can model stance at either the author level or at the post level.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 118, "end_pos": 139, "type": "TASK", "confidence": 0.7983051538467407}]}, {"text": "We comprehensively evaluate the possible modeling choices on eight topics across two online debate corpora , finding accuracy improvements of up to 11.5 percentage points over a local classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9989707469940186}]}, {"text": "Our results highlight the importance of making the correct modeling choices for online dialogues, and having a unified probabilistic modeling framework that makes this possible.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding stance and opinion in dialogues can provide critical insight into the theoretical underpinnings of discourse, argumentation, and sentiment.", "labels": [], "entities": []}, {"text": "Systems for predicting the stances of individuals can potentially have positive social impact and are of practical interest to non-profits, governmental organizations, and companies.", "labels": [], "entities": [{"text": "predicting the stances", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.8792593081792196}]}], "datasetContent": [{"text": "The goals of our experiments were to validate the proposed collective modeling framework, and to make substantive conclusions about the merits of the different possible modeling options described in Section 3.", "labels": [], "entities": []}, {"text": "To this end, we evaluated the models on eight topics from 4FORUMS.COM () and CREATEDEBATE.COM, for classification tasks at both the author level and the post level.", "labels": [], "entities": [{"text": "4FORUMS.COM", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.8318887948989868}, {"text": "CREATEDEBATE.COM", "start_pos": 77, "end_pos": 93, "type": "METRIC", "confidence": 0.7926092147827148}]}, {"text": "With comparison to, our collective models (C) are essentially equivalent to their CRF, up to the form of the CRF potential function, which is not explicitly specified in the paper.", "labels": [], "entities": []}, {"text": "A further goal of our experiments was to determine whether the modeling options in our more general CRF could improve performance over models with this structure.", "labels": [], "entities": []}, {"text": "On average, each topic-wise data set contains hundreds of authors and thousands of posts.", "labels": [], "entities": []}, {"text": "The 4FORUMS data sets are annotated for stance at the author level, while CREATEDEBATE has stance labels at the post level.", "labels": [], "entities": [{"text": "4FORUMS data sets", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9185257355372111}]}, {"text": "To perform post-level evaluations on 4FORUMS we apply author labels to the posts of each author, and on CREATEDEBATE we computed author labels by selecting the majority label of their posts.", "labels": [], "entities": [{"text": "4FORUMS", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.8707601428031921}]}, {"text": "For 4FORUMS, since postlevel stance labels correspond directly to authorlevel stance labels, we use averages of post-level predictions as the local classifier output for authors.", "labels": [], "entities": []}, {"text": "Section 2 includes an overview of these debate forum data sets.", "labels": [], "entities": [{"text": "debate forum data sets", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.6826809868216515}]}, {"text": "In the experiments, classification accuracy was estimated via five repeats of 5-fold crossvalidation.", "labels": [], "entities": [{"text": "classification", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.966744601726532}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9812319874763489}]}, {"text": "In each fold, we ran logistic regression using the scikit-learn software package, 2 using the default settings, except for the L1 regularization trade-off parameter C which was tuned on a within-fold hold-out set consisting of 20% of the discussions within the fold.", "labels": [], "entities": []}, {"text": "For the collective models, weight learning was performed on the same in-fold tuning sets.", "labels": [], "entities": [{"text": "weight learning", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.8083352744579315}]}, {"text": "We trained via 700 iterations of structured perceptron, and ran the ADMM MAP inference algorithm to convergence attest time.", "labels": [], "entities": [{"text": "convergence attest time", "start_pos": 100, "end_pos": 123, "type": "METRIC", "confidence": 0.9247282942136129}]}, {"text": "On average, weight learning and inference took around 1 minute per fold.", "labels": [], "entities": [{"text": "weight learning", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.8573934435844421}]}, {"text": "The full results for author-level and post-level predictions are given in, respectively.", "labels": [], "entities": []}, {"text": "In the tables, entries in bold identify statistically significant differences from the local classifier baseline under a paired t-test with significance level \u03b1 = 0.05.", "labels": [], "entities": [{"text": "significance level \u03b1", "start_pos": 140, "end_pos": 160, "type": "METRIC", "confidence": 0.9639302094777426}]}, {"text": "These results are summarized in  A closer analysis reveals some subtleties.", "labels": [], "entities": []}, {"text": "When comparing D models with C models in, disagreement modeling makes a much bigger difference at the author level than at the post level.", "labels": [], "entities": [{"text": "disagreement modeling", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7609879374504089}]}, {"text": "This is likely impacted by the level of class imbalance for disagreement classification in the different levels of modeling.", "labels": [], "entities": [{"text": "disagreement classification", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6935892999172211}]}, {"text": "Disagreement, rather than agreement, between authors prompts many responses.", "labels": [], "entities": []}, {"text": "Thus, reply links are more likely disagreements when measured at the post level, as seen in Ta-: Post stance classification accuracy and standard deviations for 4FORUMS (left) and CREAT-EDEBATE (right), estimated via 5 repeats of 5-fold cross-validation.", "labels": [], "entities": [{"text": "Ta-: Post stance classification", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.550450806816419}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.8658574819564819}, {"text": "4FORUMS", "start_pos": 161, "end_pos": 168, "type": "METRIC", "confidence": 0.8986002802848816}]}, {"text": "Bolded figures indicate statistically significant (\u03b1 = 0.05) improvement over PL, the baseline model for the post stance classification task.", "labels": [], "entities": [{"text": "PL", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.962114691734314}, {"text": "post stance classification task", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.7654885202646255}]}, {"text": "Therefore, enforcing disagreement maybe a better assumption at the post level, and the nuanced disagreement model is not necessary in this case.", "labels": [], "entities": []}, {"text": "The overall improvements inaccuracy from disagreement modeling for post-level models were small.", "labels": [], "entities": []}, {"text": "On the other hand, the assumption that reply edges constitute disagreement is less accurate when modeling at the author level (see).", "labels": [], "entities": []}, {"text": "In this case, the full joint disagreement model is necessary to obtain good performance.", "labels": [], "entities": []}, {"text": "In an extreme example, the two datasets with the lowest disagreement rates at the author level are evolution (44.4%) and gun control (50.7%) from 4FORUMS.", "labels": [], "entities": [{"text": "evolution", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9870361685752869}, {"text": "gun control", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.7968666255474091}, {"text": "4FORUMS", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.9104979634284973}]}, {"text": "The AC classifier performed very poorly for these data sets, dropping to 46.9% accuracy in one instance, as the \"opposite stance\" assumption did not hold).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.999174177646637}]}, {"text": "The full joint disagreement model AD performed much better, in fact achieving an outstanding accuracy rates of 80.3% and 80.5% for posts on evolution and gay marriage respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9994381070137024}]}, {"text": "To illustrate the benefits of authorlevel disagreement modeling, shows a post for an author whose stance towards gun control is correctly predicted by AD but not the AC model,", "labels": [], "entities": [{"text": "authorlevel disagreement modeling", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.6776026089986166}, {"text": "AD", "start_pos": 151, "end_pos": 153, "type": "METRIC", "confidence": 0.8133682012557983}]}], "tableCaptions": [{"text": " Table 1: Structural statistics averages for 4FO- RUMS and CREATEDEBATE.", "labels": [], "entities": [{"text": "4FO- RUMS", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.44522765278816223}]}]}