{"title": [{"text": "Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning", "labels": [], "entities": [{"text": "Cross-Space Mapping", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6987313330173492}]}], "abstractContent": [{"text": "Zero-shot methods in language, vision and other domains rely on a cross-space mapping function that projects vectors from the relevant feature space (e.g., visual-feature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors.", "labels": [], "entities": []}, {"text": "Zero-shot cross-space mapping methods hold great promise as away to scale up annotation tasks well beyond the labels in the training data (e.g., recognizing objects that were never seen in training).", "labels": [], "entities": [{"text": "Zero-shot cross-space mapping", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6195456683635712}]}, {"text": "However, the current performance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications.", "labels": [], "entities": [{"text": "cross-space mapping", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6956687867641449}]}, {"text": "In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better methods to estimate it.", "labels": [], "entities": []}, {"text": "In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments.", "labels": [], "entities": [{"text": "cross-linguistic (word translation)", "start_pos": 77, "end_pos": 112, "type": "TASK", "confidence": 0.6810705542564393}, {"text": "cross-modal (image labeling) zero-shot", "start_pos": 117, "end_pos": 155, "type": "TASK", "confidence": 0.6699532965819041}]}], "introductionContent": [{"text": "In many supervised problems, the parameters of a classification function are estimated on (x, y) pairs, where x is a vector representing a training instance in some feature space, and y is the label assigned to the instance.", "labels": [], "entities": []}, {"text": "For example, in image labeling x contains visual features extracted from a picture and y is the name of the object depicted in the picture ().", "labels": [], "entities": [{"text": "image labeling x", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7661739985148112}]}, {"text": "Since each label is treated as an unanalyzed primitive, this approach requires ad-hoc annotation for each label of interest, and it will not scale up to challenges where the potential label set is vast (for example, bilingual dictionary induction, where the label set corresponds to the full vocabulary of the target language).", "labels": [], "entities": [{"text": "bilingual dictionary induction", "start_pos": 216, "end_pos": 246, "type": "TASK", "confidence": 0.6256513198216757}]}, {"text": "Zero-shot methods () address the scalability problem by building on the observation that the labels of interest are often words (or longer linguistic expressions), which stand in a semantic similarity relation to each other.", "labels": [], "entities": []}, {"text": "Moreover, distributional approaches allow us to estimate very large semantic word spaces in an efficient and unsupervised manner, using just unannotated text corpora as input.", "labels": [], "entities": []}, {"text": "Extensive evidence has shown that the similarity estimates obtained by representing words as vectors in such corpus-induced semantic spaces are extremely accurate ( ).", "labels": [], "entities": []}, {"text": "Under the assumption that the domain of interest (e.g., objects in pictures, words in a source language) exhibits comparable similarity structure to that manifested in language, we can rephrase the learning task, from inducing multiple functions from the source feature space onto independent atomic labels, to that of estimating a single crossspace mapping function from vectors in the source feature space onto vectors for the corresponding word labels in distributional semantic space.", "labels": [], "entities": []}, {"text": "The induced function can then also be applied to a data-point whose label was not used for training.", "labels": [], "entities": []}, {"text": "The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point.", "labels": [], "entities": []}, {"text": "Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (, but it has since been extensively applied in other domains, including image labeling ( and bilingual dictionary/phrase table induction (), the two applications we focus on here.", "labels": [], "entities": [{"text": "brain signal decoding", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.6420020461082458}, {"text": "image labeling", "start_pos": 179, "end_pos": 193, "type": "TASK", "confidence": 0.7308217883110046}, {"text": "bilingual dictionary/phrase table induction", "start_pos": 200, "end_pos": 243, "type": "TASK", "confidence": 0.5709617038567861}]}, {"text": "Effective zero-shot learning by cross-space mapping could get us through the manual annotation bottleneck that hampers many applications.", "labels": [], "entities": [{"text": "cross-space mapping", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7567879557609558}]}, {"text": "However, in practice, the accuracy in label retrieval with current mapping methods is still too low for practical uses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9989456534385681}, {"text": "label retrieval", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.8496084213256836}]}, {"text": "In image labeling, when a search space of realistic size is considered, accuracy is just above 1% (which is still well above chance for large search spaces).", "labels": [], "entities": [{"text": "image labeling", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.7797974348068237}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9997075200080872}]}, {"text": "In bilingual lexicon induction, accuracy reaches values around 30% (across words of varying frequency), which are definitely more encouraging, but still indicate that only 1 word in 3 will be translated correctly.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7087366183598837}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9992910623550415}]}, {"text": "In this article, we look at some general properties of the linear cross-modal mapping function standardly used for zero-shot learning, in order to achieve a better understanding of its shortcomings, and improve its quality by devising methods to overcome them.", "labels": [], "entities": [{"text": "zero-shot learning", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7694506943225861}]}, {"text": "First, when the mapping function is estimated with least-squares error techniques, we observe a systematic increase in hubness, that is, in the tendency of some vectors (\"hubs\") to appear in the top neighbour lists of many test items.", "labels": [], "entities": []}, {"text": "We connect hubness to least-squares estimation, and we show how it is greatly mitigated when the mapping function is estimated with a max-margin ranking loss instead.", "labels": [], "entities": []}, {"text": "Still, switching to max-margin greatly improves accuracy in the cross-linguistic context, but not for vision-to-language mapping.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9986028075218201}]}, {"text": "In the cross-modal setting, we observe indeed a different problem, that we name (training instance) pollution: The neighbourhoods of mapped test items are \"polluted\" by the target vectors used in training.", "labels": [], "entities": []}, {"text": "This suggests that cross-modal mapping suffers from overfitting issues, and consequently from poor generalization power.", "labels": [], "entities": [{"text": "cross-modal mapping", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7415905892848969}]}, {"text": "Taking inspiration from domain adaptation, which addresses similar generalization concerns, and self-learning, we propose a technique to augment the training data with automatically constructed examples that force the function to generalize better.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.724769338965416}]}, {"text": "Having shown the advantages of a ranking loss, our final contribution is the adaptation of some insights from the max-margin literature to our setting, in particular concerning the choice of negative examples.", "labels": [], "entities": []}, {"text": "This leads to further accuracy improvements.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9992652535438538}]}, {"text": "We thus conclude the paper by reporting zero-shot performances in both cross-modal and cross-language settings that are well above the cur-", "labels": [], "entities": []}], "datasetContent": [{"text": "Cross-linguistic experiments In the crosslinguistic experiments, we learn a mapping from the semantic space of language A to the semantic space of language B, which can then be used for translating words outside the training set.", "labels": [], "entities": []}, {"text": "Specifically, given the vector representation of a word in language A, we apply the mapping to obtain an estimate of the vector representation of its meaning in language B, returning the nearest neighbour of the mapped vector in the B space as candidate translation.", "labels": [], "entities": []}, {"text": "We focus on translating from English to Italian and adopt the setup (word vectors, training and test data) of.", "labels": [], "entities": []}, {"text": "For a set of 200K words, 300-dimensional vectors were built using the word2vec toolkit, 1 choosing the CBOW method.", "labels": [], "entities": []}, {"text": "2 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks ( ).", "labels": [], "entities": []}, {"text": "The word vectors were induced from corpora of 2.8 and 1.6 billion tokens, respectively, for English and Italian.", "labels": [], "entities": []}, {"text": "The train and test English-to-Italian translation pairs were extracted from a Europarl-derived dictionary (Tiedemann, 2012).", "labels": [], "entities": [{"text": "Europarl-derived dictionary", "start_pos": 78, "end_pos": 105, "type": "DATASET", "confidence": 0.986622154712677}]}, {"text": "The 5K most frequent translation pairs were used for training, while the test set includes 1.5K English words equally split into 5 frequency bins.", "labels": [], "entities": []}, {"text": "The search for the correct translation is performed in a semantic space of 200K Italian words.", "labels": [], "entities": []}, {"text": "5 Cross-modal experiments In the cross-modal experiments, we induce a mapping from visual to linguistic space.", "labels": [], "entities": []}, {"text": "Specifically, given an image, we apply the mapping to its visual vector representation to obtain an estimate of its representation in linguistic space, where the word associated to the nearest neighbour is retrieved as the image label.", "labels": [], "entities": []}, {"text": "Similarly to translation pairs in the crosslinguistic setup, we create a list of \"visual translation\" pairs between images and their corresponding noun labels.", "labels": [], "entities": []}, {"text": "Our starting point are the 5.1K labels in ImageNet () that occur at least 500 times in our English corpus and have concreteness score \u22655, according to.", "labels": [], "entities": []}, {"text": "For each label, we sample 100 pictures from its ImageNet entry, and associate each picture with the 4094-dimensional layer (fc7) at the top of the pre-trained convolutional neural network model of, using the Caffe toolkit ().", "labels": [], "entities": []}, {"text": "The target word space is identical to the English space used in the cross-linguistic experiment.", "labels": [], "entities": []}, {"text": "Finally, we use 75% of the labels (and the respective images) for training and the remaining 25% of the labels for testing.", "labels": [], "entities": []}, {"text": "From the 127.5K images corresponding to test labels, we sample 1K images as our test set.", "labels": [], "entities": []}, {"text": "For zero-shot evaluation purposes, the search for the correct label is performed in the space of 5.1K possible labels, unless otherwise specified.", "labels": [], "entities": []}, {"text": "However, when quantifying hubness and pollution, in order to have a setting comparable to that of crosslanguage mapping, we use the full set of 200K English words as search space.", "labels": [], "entities": []}, {"text": "Learning objectives We assume that we have cross-space \"translation\" pairs available fora set of |T r| items ( Moreover, following previous work, we assume that the mapping function is linear.", "labels": [], "entities": []}, {"text": "For estimating its parameters W \u2208 R d1\u00d7d2 , we consider two objectives.", "labels": [], "entities": []}, {"text": "The first is L2-penalized least squares Faithful to the zero-shot setup, in our experiments there is never any overlap between train and test words; however, to make the task more challenging, we include the train words in the search space, except where expressly indicated.", "labels": [], "entities": []}, {"text": "At training time, we average the 100 vectors associated to a label into a single representation, to reduce training set size while minimizing information loss.", "labels": [], "entities": []}, {"text": "At test time, as normally done, we present the model with single image visual vectors.", "labels": [], "entities": []}, {"text": "(ridge): which has an analytical solution.", "labels": [], "entities": []}, {"text": "The second objective is a margin-based ranking loss (max-margin) similar in spirit to the one used in similar cross-modal experiments with WS-ABIE (Weston et al., 2011) and DeViSE (.", "labels": [], "entities": [{"text": "WS-ABIE", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.8860839605331421}, {"text": "DeViSE", "start_pos": 173, "end_pos": 179, "type": "DATASET", "confidence": 0.910447359085083}]}, {"text": "The loss fora given pair of training items (x i , y i ) and the corresponding mappingbased prediction\u02c6yprediction\u02c6 prediction\u02c6y i = Wx i is defined as where dist is a distance measure, in our case the inverse cosine, and \u03b3 and k are tunable hyperparameters denoting the margin and the number of negative examples, respectively.", "labels": [], "entities": []}, {"text": "Intuitively, the goal of the max-margin objective is to rank the correct translation y i of xi higher than any other possible translation y j . In theory, the summation in the equation could range overall possible labels, but in practice this is too expensive (e.g., in the cross-linguistic experiments the search space contains 200K candidate labels!), and it is usually computed over just a portion of the label space.", "labels": [], "entities": []}, {"text": "In, the authors propose an efficient way of selecting negative examples, in which they randomly sample, for each training item, labels from the complete set, and pick as negative sample the first label violating the margin.", "labels": [], "entities": []}, {"text": "This guarantees that there will be exactly as many weight updates as training items.", "labels": [], "entities": []}, {"text": "Another possibility is proposed in, where negative samples are picked from a nonitem specific distribution (e.g., the uniform distribution).", "labels": [], "entities": []}, {"text": "For the experiments in Sections 3 and 4, we follow a more general setup in which the size of the margin and number of negative samples is tuned for each task.", "labels": [], "entities": []}, {"text": "In this way, fora sufficiently large margin and number of negative samples, we increase the probability of performing a weight update per training item.", "labels": [], "entities": []}, {"text": "We estimate the mapping parameters W with stochastic gradient descent and per-parameter learning rates tuned with Adagrad ().", "labels": [], "entities": [{"text": "Adagrad", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.69345623254776}]}, {"text": "The tuning of hyperparameters \u03b3 and k is performed on a random 25% subset of the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Roadmap. Proposed changes to cross- space mapping training and resulting percentage  Precision @1 in our two experimental setups.", "labels": [], "entities": [{"text": "cross- space mapping", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.725195586681366}, {"text": "Precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.983352541923523}]}, {"text": " Table 3: Ridge vs. max-margin in zero- shot experiments. Precision @N results cross- linguistically (test items: 1.5K, search space:  200K) and cross-modally (test items: 1K, search  space: 5.1K).", "labels": [], "entities": []}, {"text": " Table 4: Hubs as top predictions. Percentage of  top-1 neighbours of test vectors in zero-shot ex- periments of Table 3 with N 20 > 5.", "labels": [], "entities": [{"text": "Hubs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7863761782646179}, {"text": "Percentage", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9773291945457458}]}, {"text": " Table 5: Properties of hubs. Spearman \u03c1 of  N 20 scores with cosines to mean vector of full  search space (top) and nearest training item (bot- tom), across all search space elements. All corre- lations significant (p<0.001) except cross-modal  max-margin hubness/full-space mean.", "labels": [], "entities": [{"text": "Spearman \u03c1", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.97561115026474}]}, {"text": " Table 8: Cross-modal zero-shot experiment  with data augmentation, disjoint train/search  spaces. Same setup as Table 8, but search space  excludes training elements (1K test items, 1K  search space).", "labels": [], "entities": []}, {"text": " Table 9: Cross-modal zero-shot experiment  with data augmentation, enlarged search space.", "labels": [], "entities": []}, {"text": " Table 11: Random vs. intruding negative exam- ples. Zero-shot precision @N results when cross- space function is estimated using max-margin with  random or \"intruder\" negative examples, cross- linguistically (test items: 1.5K, search space:  200K) and cross-modally (test items: 1K, search  space: 5.1K).", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.6666286587715149}]}]}