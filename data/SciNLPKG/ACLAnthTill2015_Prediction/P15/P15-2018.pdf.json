{"title": [{"text": "A Distributed Representation Based Query Expansion Approach for Image Captioning", "labels": [], "entities": [{"text": "Distributed Representation Based Query Expansion Approach", "start_pos": 2, "end_pos": 59, "type": "TASK", "confidence": 0.666268785794576}, {"text": "Image Captioning", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7593831419944763}]}], "abstractContent": [{"text": "In this paper, we propose a novel query expansion approach for improving transfer-based automatic image captioning.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6966314166784286}, {"text": "transfer-based automatic image captioning", "start_pos": 73, "end_pos": 114, "type": "TASK", "confidence": 0.5637809261679649}]}, {"text": "The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image.", "labels": [], "entities": []}, {"text": "Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-the-art data-driven methods in terms of both automatic metrics and subjective evaluation .", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic image captioning is a fast growing area of research which lies at the intersection of computer vision and natural language processing and refers to the problem of generating natural language descriptions from images.", "labels": [], "entities": [{"text": "Automatic image captioning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7196687857309977}]}, {"text": "In the literature, there area variety of image captioning models that can be categorized into three main groups as summarized below.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7303133308887482}]}, {"text": "The first line of approaches attempts to generate novel captions directly from images).", "labels": [], "entities": []}, {"text": "Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization.", "labels": [], "entities": []}, {"text": "More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks.", "labels": [], "entities": [{"text": "generative", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.975671648979187}]}, {"text": "In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description fora given image.", "labels": [], "entities": []}, {"text": "The studies in the second group aim at learning joint representations of images and captions).", "labels": [], "entities": []}, {"text": "They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption fora given image.", "labels": [], "entities": []}, {"text": "The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem ().", "labels": [], "entities": [{"text": "image captioning", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.73262158036232}, {"text": "caption transfer", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7750435471534729}]}, {"text": "For a given image, these methods first search for visually similar images and then use the captions of the retrieved images to provide a description, which makes them much easier to implement compared to the other two classes of approaches.", "labels": [], "entities": []}, {"text": "The success of these data-driven approaches depends directly on the amount of data available and the quality of the retrieval set.", "labels": [], "entities": []}, {"text": "Clearly, the image features and the corresponding similarity measures used in retrieval play a significant role here but, as investigated in ( ), what makes this particularly difficult is that while describing an image humans do not explicitly mention every detail.", "labels": [], "entities": []}, {"text": "That is, some parts of an image are more salient than the others.", "labels": [], "entities": []}, {"text": "Hence, one also needs to bridge the semantic gap between what is therein the image and what people say when describing it.", "labels": [], "entities": []}, {"text": "As a step towards achieving this goal, in this paper, we introduce a novel automatic query expansion approach for image captioning to retrieve semantically more relevant captions.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7140666246414185}, {"text": "image captioning", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7031268328428268}]}, {"text": "As illustrated in, we swap modalities at our query expan-  Through comprehensive experiments over three benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following, we give the details about our experimental setup.", "labels": [], "entities": []}, {"text": "We estimated the distributed representation of words based on the captions of the MS COCO () dataset, containing 620K captions.", "labels": [], "entities": [{"text": "MS COCO () dataset", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.9246572703123093}]}, {"text": "As a preprocessing step, all captions in the corpus were lowercased, and stripped from punctuation.", "labels": [], "entities": []}, {"text": "In the training of word vectors, we used 500 dimensional vectors obtained with both GloVe) and word2vec () models.", "labels": [], "entities": []}, {"text": "The minimum word count was set to 5, and the window size was set to 10.", "labels": [], "entities": []}, {"text": "Although these two methods seem to produce comparable results, we found out that word2vec gives better results in our case, and thus we only report our results with word2vec model.", "labels": [], "entities": []}, {"text": "In our experiments, we used the popular Flickr8K (),), MS COCO () datasets, containing 8K, 30K and 123K images, respectively.", "labels": [], "entities": [{"text": "Flickr8K", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.8020902276039124}, {"text": "MS COCO () datasets", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.642006941139698}]}, {"text": "Each image in these datasets comes with 5 captions annotated by different people.", "labels": [], "entities": []}, {"text": "For each dataset, we utilized the corresponding validation split to optimize the parameters of our method, and used the test split for evaluation where we considered all the image-caption pairs in the training and the validation splits as our knowledge base.", "labels": [], "entities": []}, {"text": "Although Flickr8K, and Flickr30K datasets have been in use fora while, MS COCO dataset is under active development and might be subject to change.", "labels": [], "entities": [{"text": "Flickr8K", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.8825642466545105}, {"text": "Flickr30K datasets", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.9733153283596039}, {"text": "MS COCO dataset", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.892713208993276}]}, {"text": "Here, we report our results with version 1.0 of MS COCO dataset where we used the train, validation and test splits provided by . We compared our proposed approach against the adapted baseline model (VC) of im2text () which corresponds to using the caption of the nearest visually similar im-MC-KL a black and white dog is playing or fighting with a brown dog in grass a man is sitting on a blue bench with a blue blanket covering his face a man in a white shirt and sunglasses is holding hands with a woman wearing a red shirt outside one brown and black pigmented bird sitting on a tree branch MC-SB a dog looks behind itself a girl looks at a woman s face a woman and her two dogs are walking down the street a tree with many leaves around it VC a brown and white dog jumping over a red yellow and white pole age, and the word frequency-based approaches of Mason and Charniak (2014) (MC-SB and MC-KL).", "labels": [], "entities": [{"text": "MS COCO dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9117153684298197}]}, {"text": "We also provide the human agreement results (HUMAN) by comparing one groundtruth caption against the rest.", "labels": [], "entities": [{"text": "HUMAN", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9782309532165527}]}, {"text": "For a fair comparison with the MC-SB and MC-KL models) and the baseline approach VC, we used the same image similarity metric and training splits in retrieving visually similar images for all models.", "labels": [], "entities": []}, {"text": "For human agreement, we had five groundtruth image captions, thus we determine the human agreement score by following a leave-one-out strategy.", "labels": [], "entities": []}, {"text": "For display purposes, we selected one description randomly from the available five groundtruth captions in the figures.", "labels": [], "entities": []}, {"text": "We evaluated our approach with a range of existing metrics, which are thoroughly discussed in.", "labels": [], "entities": []}, {"text": "We used smoothed BLEU () for benchmarking purposes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9978770017623901}]}, {"text": "We also provided the scores of ME-TEOR () and the recently proposed CIDEr metric (, which has been shown to correlate well with the human judgments in and, respectively 3 . Human Evaluation.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.977601945400238}]}, {"text": "We designed a subjective experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of ( . In this experiment, we provided human annotators an image and a candidate description where it is rated according to a scale of 1 to 5 (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 50-70% good, 1: totally bad) for its relevancy.", "labels": [], "entities": []}, {"text": "We experimented on a randomly selected set of 100 images from our test set and evaluated our captions as well as those of the competing approaches.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.", "labels": [], "entities": []}, {"text": " Table 2: Human judgment scores on a scale of 1 to 5.", "labels": [], "entities": [{"text": "Human judgment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6034481376409531}]}]}