{"title": [{"text": "Distributional Neural Networks for Automatic Resolution of Crossword Puzzles", "labels": [], "entities": [{"text": "Automatic Resolution of Crossword Puzzles", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.836711871623993}]}], "abstractContent": [{"text": "Automatic resolution of Crossword Puzzles (CPs) heavily depends on the quality of the answer candidate lists produced by a retrieval system for each clue of the puzzle grid.", "labels": [], "entities": [{"text": "Automatic resolution of Crossword Puzzles (CPs)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6254472099244595}]}, {"text": "Previous work has shown that such lists can be generated using Information Retrieval (IR) search algorithms applied to the databases containing previously solved CPs and reranked with tree kernels (TKs) applied to a syntactic tree representation of the clues.", "labels": [], "entities": [{"text": "Information Retrieval (IR) search", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.7404205699761709}]}, {"text": "In this paper , we create a labelled dataset of 2 million clues on which we apply an innovative Distributional Neural Network (DNN) for reranking clue pairs.", "labels": [], "entities": []}, {"text": "Our DNN is com-putationally efficient and can thus take advantage of such large datasets showing a large improvement over the TK approach, when the latter uses small training data.", "labels": [], "entities": []}, {"text": "In contrast, when data is scarce, TKs outper-form DNNs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic solvers of CPs require accurate list of answer candidates to find good solutions in little time.", "labels": [], "entities": [{"text": "solvers of CPs", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8384467562039694}]}, {"text": "Candidates can be retrieved from the DBs of previously solved CPs (CPDBs) since clues are often reused, and thus querying CPDBs with the target clue allows us to recuperate the same (or similar) clues.", "labels": [], "entities": []}, {"text": "In this paper, we propose for the first time the use of Distributional Neural Networks to improve the ranking of answer candidate lists.", "labels": [], "entities": []}, {"text": "Most importantly, we build a very large dataset for clue retrieval, composed of 2,000,493 clues with their associated answers, i.e., this is a supervised corpus where large scale learning models can be developed and tested.", "labels": [], "entities": [{"text": "clue retrieval", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.8232283294200897}]}, {"text": "This dataset is an interesting * Work done when student at University of Trento resource that we make available to the research community . To assess the effectiveness of our DNN model, we compare it with the current state of the art model ( ) in reranking CP clues, where tree kernels) are used to rerank clues according to their syntactic/semantic similarity with the query clue.", "labels": [], "entities": []}, {"text": "The experimental results on our dataset demonstrate that: (i) DNNs are efficient and can greatly benefit from large amounts of data; (ii) when DNNs are applied to large-scale data, they largely outperform traditional featurebased rerankers as well as kernel-based models; and (iii) if limited training data is available for training, tree kernel-based models are more accurate than DNNs", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments compare different ranking models, i.e., BM25 as the IR baseline, and several rerankers, and our distributional neural network (DNN) for the task of clue reranking.", "labels": [], "entities": [{"text": "BM25", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.7883189916610718}, {"text": "clue reranking", "start_pos": 164, "end_pos": 178, "type": "TASK", "confidence": 0.7318909466266632}]}, {"text": "We compiled our crossword corpus combining (i) CPs downloaded from the Web 2 and (ii) the clue database provided by Otsys 3 . We removed duplicates, fill-in-the-blank clues (which are better solved by using other strategies) and clues representing anagrams or linguistic games.", "labels": [], "entities": []}, {"text": "We collected over 6.3M pairs of clue/answer and after removal of duplicates, we obtained a compressed dataset containing 2M unique and standard clues, with associated answers, which we called CPDB.", "labels": [], "entities": [{"text": "CPDB", "start_pos": 192, "end_pos": 196, "type": "DATASET", "confidence": 0.9358387589454651}]}, {"text": "We used these clues to build a Small Dataset (SD) and a Large Dataset (LD) for reranking.", "labels": [], "entities": [{"text": "reranking", "start_pos": 79, "end_pos": 88, "type": "TASK", "confidence": 0.949570894241333}]}, {"text": "The two datasets are based on pairs of clues: query and retrieved clues.", "labels": [], "entities": []}, {"text": "Such clues are retrieved using a BM25 model on CPDB.", "labels": [], "entities": [{"text": "CPDB", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.9653505682945251}]}, {"text": "For creating SD, we used 8k clues that (i) were randomly extracting from CPDB and (ii) satisfying the property that at least one correct clue (i.e., having the same answer of the query clue) is in the first retrieved 10 clues (of course the query clue is eliminated from the ranked list provided by BM25).", "labels": [], "entities": [{"text": "CPDB", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9382755160331726}, {"text": "BM25", "start_pos": 299, "end_pos": 303, "type": "DATASET", "confidence": 0.975205659866333}]}, {"text": "In total we got about 120K examples, 84,040 negative and 35,960 positive clue . For building LD, we collected 200k clues with the same property above.", "labels": [], "entities": []}, {"text": "More precisely we obtained 1,999,756 pairs (10\u00d7200k minus few problematic examples) with 599,025 positive and 140,0731 negative pairs of queries with their retrieved clues.", "labels": [], "entities": []}, {"text": "Given the large number of examples, we only used such dataset in classification modality, i.e., we did not form reranking examples (pairs of pairs).", "labels": [], "entities": []}, {"text": "We use SVM-light-TK 5 , which enables the use of structural kernels).", "labels": [], "entities": []}, {"text": "We applied structural kernels to shallow tree representations and a polynomial kernel of degree 3 to feature vectors (FV).", "labels": [], "entities": []}, {"text": "We preinitialize the word embeddings by running the word2vec tool () on the English Wikipedia dump.", "labels": [], "entities": [{"text": "English Wikipedia dump", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.8665638168652853}]}, {"text": "We opt fora skipgram model with window size 5 and filtering words with frequency less than 5.", "labels": [], "entities": []}, {"text": "The dimensionality of the embeddings is set to 50.", "labels": [], "entities": []}, {"text": "The input sentences are mapped to fixed-sized vectors by computing the average of their word embeddings.", "labels": [], "entities": []}, {"text": "We use a single non-linear hidden layer (with rectified linear (ReLU) activation function) whose size is equal to the size of the previous layer.", "labels": [], "entities": [{"text": "rectified linear (ReLU) activation function", "start_pos": 46, "end_pos": 89, "type": "METRIC", "confidence": 0.7915129406111581}]}, {"text": "The network is trained using SGD with shuffled mini-batches using the Adagrad update rule).", "labels": [], "entities": []}, {"text": "The batch size is set to 100 examples.", "labels": [], "entities": []}, {"text": "We used 25 epochs with early stopping, i.e., we stop the training if no update to the best accuracy on the dev set (we create the dev set by allocating 10% of the training set) is made for the last 5 epochs.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.8878636360168457}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9971696734428406}]}, {"text": "The accuracy computed on the dev set is the Mean Average Precision (MAP) score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993565678596497}, {"text": "Mean Average Precision (MAP) score", "start_pos": 44, "end_pos": 78, "type": "METRIC", "confidence": 0.9607531598636082}]}, {"text": "To extract the DNN features we simply take the output of the hidden layer just before the softmax.", "labels": [], "entities": []}, {"text": "We used standard metrics widely used in QA: the Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 48, "end_pos": 74, "type": "METRIC", "confidence": 0.9649012088775635}, {"text": "Mean Average Precision (MAP)", "start_pos": 79, "end_pos": 107, "type": "METRIC", "confidence": 0.9714593688646952}]}, {"text": "summarizes the results of our different reranking models trained on a small dataset (SD) of 120k examples and a large dataset (LD) with 2M examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: SVM models and DNN trained on 120k (small dataset) and 2 millions (large dataset) examples.  Feature vectors are used with all models except when indicated by \u2212FV", "labels": [], "entities": []}]}