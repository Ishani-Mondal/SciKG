{"title": [], "abstractContent": [{"text": "In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods.", "labels": [], "entities": []}, {"text": "We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets.", "labels": [], "entities": [{"text": "automatic keyword extraction", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.6808947920799255}]}, {"text": "These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet.", "labels": [], "entities": []}, {"text": "We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Keywords are frequently used in many occasions as indicators of important information contained in documents.", "labels": [], "entities": []}, {"text": "These can be used by human readers to search for their desired documents, but also in many Natural Language Processing (NLP) applications, such as Text Summarization (, Text Categorization), Information Retrieval ( and Question Answering ().", "labels": [], "entities": [{"text": "Text Summarization", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7663559019565582}, {"text": "Information Retrieval", "start_pos": 191, "end_pos": 212, "type": "TASK", "confidence": 0.8668835163116455}, {"text": "Question Answering", "start_pos": 219, "end_pos": 237, "type": "TASK", "confidence": 0.8417080044746399}]}, {"text": "Many automatic frameworks for extracting keywords have been proposed (.", "labels": [], "entities": []}, {"text": "These systems were built for more formal domains, such as news data or Web data, where the content is still produced in a controlled fashion.", "labels": [], "entities": []}, {"text": "The emergence of social media environments, such as Twitter and Facebook, has created a framework for more casual data to be posted online.", "labels": [], "entities": []}, {"text": "These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters.", "labels": [], "entities": []}, {"text": "The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts.", "labels": [], "entities": []}, {"text": "In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging), Machine Translation (, Named Entity Recognition (, Information Retrieval and Summarization (.", "labels": [], "entities": [{"text": "Part-of-Speech (POS) Tagging", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.6391132950782776}, {"text": "Machine Translation", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.8595380783081055}, {"text": "Named Entity Recognition", "start_pos": 174, "end_pos": 198, "type": "TASK", "confidence": 0.6379233996073405}, {"text": "Information Retrieval and Summarization", "start_pos": 202, "end_pos": 241, "type": "TASK", "confidence": 0.8198694288730621}]}, {"text": "As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7106242179870605}]}, {"text": "In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: 1.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7225768119096756}]}, {"text": "Provide a annotated keyword annotated dataset consisting of 1827 tweets.", "labels": [], "entities": []}, {"text": "These tweets are obtained from, and also contain POS annotations.", "labels": [], "entities": []}, {"text": "2. Improve a state-of-the-art keyword extraction system ( for this domain by learning additional features in an unsupervised fashion.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.788131445646286}]}, {"text": "The paper is organized as follows: Section 2 describes the related work; Section 3 presents the annotation process; Section 4 details the architecture of our keyword extraction system; Section 5 presents experiments using our models and we conclude in Section 6.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7161661982536316}]}], "datasetContent": [{"text": "The dataset 1 contains 1827 tweets, which are POS tagged in ().", "labels": [], "entities": []}, {"text": "We used Amazon Mechanical turk, an crowdsourcing market, to recruit eleven annotators to identify keywords in each tweet.", "labels": [], "entities": [{"text": "Amazon Mechanical turk", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.8869239091873169}]}, {"text": "Each annotator highlighted words that he would consider a keyword.", "labels": [], "entities": []}, {"text": "No specific instructions about what words can be keywords (e.g., \"urls are not keywords\"), as we wish to learn what users find important in a tweet.", "labels": [], "entities": []}, {"text": "It is also acceptable for tweets to not contain keywords, as some tweets simply do not contain important in- The corpus is submitted as supplementary material.", "labels": [], "entities": []}, {"text": "formation (e.g., retweet).", "labels": [], "entities": []}, {"text": "The annotations of each annotator are combined by selecting keywords that are chosen by at least three annotators.", "labels": [], "entities": []}, {"text": "We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in ().", "labels": [], "entities": []}, {"text": "Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3.", "labels": [], "entities": []}, {"text": "As baselines, we reported results using a TF-IDF, the default MAUI toolkit, and our own implementation of ( framework.", "labels": [], "entities": []}, {"text": "In all cases the IDF component was computed over a collection of 52 million tweets.", "labels": [], "entities": [{"text": "IDF", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.829450249671936}]}, {"text": "Results are reported on rows 1 and 2 in, respectively.", "labels": [], "entities": []}, {"text": "The parameter k (column Nr. Keywords) defines the number of keywords extracted for each tweet and is maximized on the development set.", "labels": [], "entities": []}, {"text": "Evaluation is performed using Fmeasure (column F1), where the precision (column P) is defined as the ratio of extracted keywords that are correct and the number of extracted keywords, and the recall (column R) is de-  fined as the ratio between the number of keywords correctly extracted and the total number of keywords in the dataset.", "labels": [], "entities": [{"text": "Fmeasure (column F1)", "start_pos": 30, "end_pos": 50, "type": "METRIC", "confidence": 0.8364044427871704}, {"text": "precision (column P)", "start_pos": 62, "end_pos": 82, "type": "METRIC", "confidence": 0.8348148822784424}, {"text": "recall (column R)", "start_pos": 192, "end_pos": 209, "type": "METRIC", "confidence": 0.9238922715187072}]}, {"text": "We can see that the TF-IDF, which tends to be a strong baseline for keyword/keyphrase extraction, yields poor results.", "labels": [], "entities": [{"text": "keyword/keyphrase extraction", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.5879483968019485}]}, {"text": "In fact, the best value fork is 15, which means that the system simply retrieves all words as keywords in order to maximize recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9923059344291687}]}, {"text": "This is because most keywords only occur once 3 , which makes the TF component not very informative.", "labels": [], "entities": []}, {"text": "On the other hand, the MAUI baseline performs significantly better, this is because of the usage of many hand engineered features using lists of words and Wikipedia, rather than simply relying on word counts.", "labels": [], "entities": [{"text": "MAUI baseline", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.7291786074638367}]}, {"text": "Next, we introduce features learnt using an unsupervised setup, namely, word vectors and brown clusters in rows 3 and 4, respectively.", "labels": [], "entities": []}, {"text": "These were trained on the same 52 million tweets used for computing the IDF component.", "labels": [], "entities": [{"text": "IDF component", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9019483923912048}]}, {"text": "Due to the large size of the vocabulary, word types with less than 40 occurrences were removed.", "labels": [], "entities": []}, {"text": "We observe that while both features yield improvements over the baseline model in row 2, the improvements obtained using Brown clustering are far more significant.", "labels": [], "entities": []}, {"text": "Combining both features yields slightly higher results, reported on row 5.", "labels": [], "entities": []}, {"text": "Finally, we also test training the system with all features on an out-3 6856 out of 7045 keywords are singletons of-domain keyword extraction corpus composed by news documents.", "labels": [], "entities": []}, {"text": "Results are reported on row 6, where we can observe a significant domain mismatch problem between these two domains as results drop significantly.", "labels": [], "entities": []}, {"text": "We explored different methods for choosing the number of keywords to be extracted in.", "labels": [], "entities": []}, {"text": "The simplest way is choosing a fixed number of keywords k and tune this value in the development set.", "labels": [], "entities": []}, {"text": "Next, we can also define the number of keywords as the ratio N k , where N is the number of words in the tweet, and k is the parameter that we wish to optimize.", "labels": [], "entities": []}, {"text": "Finally, the number of keywords can also be estimated using a linear regressor as y = f 1 w1, ..., f n w n , where f 1 , ..., f n denote the feature set and w 1 , ..., w n are the parameters of the model trained on the training set.", "labels": [], "entities": []}, {"text": "Once the model is trained, the number of keywords selected for each tweet is defined as y + k, where k is inserted to adjust y to maximize F-measure on the development set.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9972665309906006}]}, {"text": "Results using the best system using Brown clusters and word vectors are described in.", "labels": [], "entities": []}, {"text": "We can observe that defining the number of keywords as a fraction of the number of words in the tweet, yields better results (row 2) yields better overall results than fixing the number of extracted keywords (row 1).", "labels": [], "entities": []}, {"text": "Finally, training a predictor for the number of keywords yields further improvements (row 3) over a simple ratio of the 640 number of input words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F-measure, precision and recall results on the Twitter keyword dataset using different feature  sets.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9981701374053955}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9992892742156982}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9993053674697876}, {"text": "Twitter keyword dataset", "start_pos": 57, "end_pos": 80, "type": "DATASET", "confidence": 0.8882782260576884}]}, {"text": " Table 2: F-measure, precision and recall results on the Twitter keyword dataset using different keyword  selection methods.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985287189483643}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9993550181388855}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9993985891342163}, {"text": "Twitter keyword dataset", "start_pos": 57, "end_pos": 80, "type": "DATASET", "confidence": 0.8967216610908508}]}]}