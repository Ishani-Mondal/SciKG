{"title": [{"text": "Identifying Cascading Errors using Constraints in Dependency Parsing", "labels": [], "entities": [{"text": "Identifying", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9811686277389526}]}], "abstractContent": [{"text": "Dependency parsers are usually evaluated on attachment accuracy.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7968278527259827}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9507684111595154}]}, {"text": "Whilst easily interpreted , the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is subsequently forced to choose further incorrect arcs elsewhere in the parse.", "labels": [], "entities": []}, {"text": "We apply arc-level constraints to MST-parser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding.", "labels": [], "entities": []}, {"text": "We investigate the direct and indirect impact of applying constraints to the parser.", "labels": [], "entities": []}, {"text": "Erroneous NP and punctuation attachments cause the most cascading errors, while incorrect PP and coordination attachments are frequent but less influential.", "labels": [], "entities": []}, {"text": "Punctuation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.735007643699646}]}], "introductionContent": [{"text": "Dependency parsers are evaluated using wordlevel attachment accuracy.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7527992725372314}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.5369837284088135}]}, {"text": "Whilst comparable across systems, this does not provide insight into why the parser makes certain errors, or whether certain misattachments are caused by other errors.", "labels": [], "entities": []}, {"text": "For example, incorrectly identifying a modifier head may only introduce a single attachment error, while misplacing the root of a sentence will create substantially more errors elsewhere.", "labels": [], "entities": []}, {"text": "In projective dependency parsing, erroneous arcs can also force the parser to select other incorrect arcs.", "labels": [], "entities": [{"text": "projective dependency parsing", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6081133286158243}]}, {"text": "propose a static postparsing analysis to categorise groups of bracket errors in constituency parsing into higher level error classes such as clause attachment.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7574633359909058}, {"text": "clause attachment", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.7384230047464371}]}, {"text": "However, this cannot account for cascading changes resulting from repairing errors, or limitations which may prevent the parser from applying a repair.", "labels": [], "entities": []}, {"text": "It is unclear whether the parser will apply the repair operation in its entirety, or if it will introduce other changes in response to the repairs.", "labels": [], "entities": []}, {"text": "We develop an evaluation procedure to evaluate the influence of each error class in dependency parsing without making assumptions about how the parser will behave.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7647627294063568}]}, {"text": "We define error classes based on dependency labels, and use the dependencies in each class as arc constraints specifying the correct head and label for particular words in each sentence.", "labels": [], "entities": []}, {"text": "We adapt parsers to apply these constraints, whilst otherwise proceeding with decoding under their grammar and model.", "labels": [], "entities": []}, {"text": "By evaluating performance with and without constraints, we can directly observe the cascading impact of each error class on each the parser.", "labels": [], "entities": []}, {"text": "We implement our procedure for the graphbased MSTparser) and the transition-based ZPar () using basic Stanford dependencies over the OntoNotes 4.0 release of the WSJ Penn Treebank data.", "labels": [], "entities": [{"text": "OntoNotes 4.0 release of the WSJ Penn Treebank data", "start_pos": 133, "end_pos": 184, "type": "DATASET", "confidence": 0.8321764336691962}]}, {"text": "Our results show that erroneously attaching NPs, PPs, modifiers, and punctuation have the largest overall impact on UAS.", "labels": [], "entities": [{"text": "UAS", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9133356213569641}]}, {"text": "Of those, NPs and punctuation have the most substantial cascading impact, indicating that these errors have the most effect on the remainder of the parse.", "labels": [], "entities": []}, {"text": "Enforcing correct punctuation arcs has a particularly large impact on accuracy, even though most evaluation scripts ignore punctuation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9991759657859802}]}, {"text": "We find that punctuation arcs are commonly misplaced by large distances in the final parse, crossing over and forcing other arcs to be incorrect in the process.", "labels": [], "entities": []}, {"text": "depicts a WSJ 22 sentence as parsed by MSTparser, and the gold parse.", "labels": [], "entities": [{"text": "WSJ 22 sentence", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8052132924397787}, {"text": "MSTparser", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.9514959454536438}]}, {"text": "The UAS is 47.1%, with 8 of 17 arcs correct.", "labels": [], "entities": [{"text": "UAS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9604394435882568}, {"text": "arcs", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9708431363105774}]}, {"text": "By contrast, ZPar (parse not shown) scores 94.1%, with the sole attachment error being on LME (as with MSTparser).", "labels": [], "entities": [{"text": "attachment error", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.937706708908081}]}, {"text": "While there are nine incorrect arcs overall, MSTparser seems to have made only two underlying errors:", "labels": [], "entities": [{"text": "MSTparser", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.7794162034988403}]}], "datasetContent": [{"text": "We use the training (sections 2-21) and develop-.", "labels": [], "entities": []}, {"text": "A model trained on WSJ sections 2-21 was used to tag the development set, and 10-fold jackknife training was used to tag the training data.", "labels": [], "entities": [{"text": "WSJ sections 2-21", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.9301450451215109}]}, {"text": "We implement a custom evaluation script to facilitate a straightforward comparative analysis between the unconstrained and constrained output.", "labels": [], "entities": [{"text": "comparative analysis", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.8028692007064819}]}, {"text": "The script is based on and produces identical scores to eval06.pl, the official evaluation for the CoNLL-X Shared Task on Multilingual Dependency Parsing ().", "labels": [], "entities": [{"text": "CoNLL-X Shared Task on Multilingual Dependency Parsing", "start_pos": 99, "end_pos": 153, "type": "TASK", "confidence": 0.6271913009030479}]}, {"text": "We ignore punctuation as defined by eval06.pl in our evaluation; experiments with constraints over punctuation tokens constrain those tokens in the parse, but ignore them during evaluation.", "labels": [], "entities": []}, {"text": "We run the modified parsers over WSJ 22 with and without each set of constraints.", "labels": [], "entities": [{"text": "WSJ 22", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9404526650905609}]}, {"text": "We examine the overall unlabeled and labeled attachment scores (UAS and LAS), as well as identifying the contribution to the overall UAS improvement from directly (constrained) and indirectly corrected arcs.", "labels": [], "entities": [{"text": "attachment scores (UAS and LAS)", "start_pos": 45, "end_pos": 76, "type": "METRIC", "confidence": 0.7275617250374385}, {"text": "UAS", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.6047493815422058}]}, {"text": "MSTparser uses coarse-grained tags and finegrained POS tags in its features, both of which were provided by the CoNLL-X Shared Task.", "labels": [], "entities": [{"text": "MSTparser", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8824456930160522}]}, {"text": "We approximate the coarse-grained POS tags by taking the first character of the MXPOST-assigned POS tag, a technique also used by 3 . show the impact of applying constraints on tokens with various labels to MSTparser for the sentence in.", "labels": [], "entities": [{"text": "MXPOST-assigned POS tag", "start_pos": 80, "end_pos": 103, "type": "DATASET", "confidence": 0.7675094405810038}]}, {"text": "Enforcing the gold nn arc between decline and LME repairs that noun phrase error, but does not affect any of the other errors.", "labels": [], "entities": [{"text": "gold nn arc between decline and LME", "start_pos": 14, "end_pos": 49, "type": "METRIC", "confidence": 0.7990766763687134}]}, {"text": "Conversely, enforcing the gold root arc does not affect the noun phrase error, but repairs nearly every other error in the parse.", "labels": [], "entities": []}, {"text": "Unfortunately, the constrained root arc introduces   The UAS of constrained arcs in each experiment is the expected 100%.", "labels": [], "entities": [{"text": "UAS", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9987456798553467}]}, {"text": "Effective constraints repair an error in the baseline, and the effective constraint percentage is this figure expressed as a percentage, i.e. the error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9624351263046265}]}, {"text": "Error displacement is the average number of words that effective constraints moved an attachment point.", "labels": [], "entities": [{"text": "Error displacement", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9189223647117615}]}, {"text": "The overall \u2206UAS improvement is divided into \u2206c, the constrained impact, and \u2206u, the cascaded impact.", "labels": [], "entities": [{"text": "UAS", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6575722098350525}]}], "tableCaptions": [{"text": " Table 1: Baseline UAS and LAS scores on Stanford  dependencies over WSJ 22.", "labels": [], "entities": [{"text": "UAS", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.49783870577812195}, {"text": "LAS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9644209742546082}, {"text": "WSJ 22", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.9456267952919006}]}, {"text": " Table 3: The coverage, effective constraints and percentage, error displacement, UAS, LAS, \u2206UAS over  the corrected arcs, and the constrained and cascaded \u2206 for MSTparser over WSJ 22 (covered by ZPar).", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9965369701385498}, {"text": "error displacement", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.9198548495769501}, {"text": "UAS", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9971765279769897}, {"text": "LAS", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9946774244308472}, {"text": "UAS", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9957905411720276}, {"text": "WSJ 22", "start_pos": 177, "end_pos": 183, "type": "DATASET", "confidence": 0.9501544535160065}]}, {"text": " Table 4: The coverage, effective constraints and percentage, error displacement, UAS, LAS, \u2206UAS over  the baseline, and the constrained and cascaded \u2206 for ZPar over WSJ 22.", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9960683584213257}, {"text": "error displacement", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.9111212491989136}, {"text": "UAS", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9955152869224548}, {"text": "LAS", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.991638720035553}, {"text": "UAS", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9939795732498169}, {"text": "WSJ 22", "start_pos": 166, "end_pos": 172, "type": "DATASET", "confidence": 0.9490056037902832}]}, {"text": " Table 5: The number of unconstrained errors repaired per error class when enforcing NP attachment and  NP internal constraints for MSTparser and ZPar over WSJ 22.", "labels": [], "entities": []}, {"text": " Table 6: The number of unconstrained errors re- paired per error class when enforcing punctuation  constraints for MSTparser and ZPar.", "labels": [], "entities": []}]}