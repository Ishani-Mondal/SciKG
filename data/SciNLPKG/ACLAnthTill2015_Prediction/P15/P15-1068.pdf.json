{"title": [{"text": "How Far are We from Fully Automatic High Quality Grammatical Error Correction?", "labels": [], "entities": [{"text": "Fully Automatic High Quality Grammatical Error Correction", "start_pos": 20, "end_pos": 77, "type": "TASK", "confidence": 0.6012462505272457}]}], "abstractContent": [{"text": "In this paper, we first explore the role of inter-annotator agreement statistics in grammatical error correction and conclude that they are less informative in fields where there maybe more than one correct answer.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.5841604272524515}]}, {"text": "We next created a dataset of 50 student essays, each corrected by 10 different annotators for all error types, and investigated how both human and GEC system scores vary when different combinations of these annotations are used as the gold standard.", "labels": [], "entities": []}, {"text": "Upon learning that even humans are unable to score higher than 75% F 0.5 , we propose anew metric based on the ratio between human and system performance.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9729743897914886}]}, {"text": "We also use this method to investigate the extent to which annotators agree on certain error categories, and find that similar results can be obtained from a smaller subset of just 10 essays.", "labels": [], "entities": []}], "introductionContent": [{"text": "Interest in grammatical error correction (GEC) systems has grown considerably in the past few years, thanks mainly to the success of the recent Helping Our Own (HOO) () and Conference on Natural Language Learning (CoNLL) () shared tasks.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.7924551119407018}]}, {"text": "Despite this increasing attention, however, one of the most significant challenges facing GEC today is the lack of a robust evaluation practice.", "labels": [], "entities": [{"text": "GEC", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.6118102073669434}]}, {"text": "In fact even go as far to say that it is sometimes \"hard to draw meaningful comparisons between different approaches, even when they are evaluated on the same corpus.\"", "labels": [], "entities": []}, {"text": "One of the reasons for this is that, traditionally, system performance has only ever been evaluated against the gold standard annotations of a single native speaker (rarely, two native speakers).", "labels": [], "entities": []}, {"text": "As such, system output is not actually scored on the basis of grammatical acceptability alone, but rather is also constrained by the idiosyncrasies of the particular annotators.", "labels": [], "entities": []}, {"text": "The obvious solution to this problem would be to compare systems against the gold standard annotations of multiple annotators, in an effort to dilute the effect of individual annotator bias, however creating manual annotations is often considered too time consuming and expensive.", "labels": [], "entities": []}, {"text": "In spite of this, while other studies have instead elected to use crowdsourcing to produce multiply-corrected annotations, often concerning only a limited number of error types), one of the main contributions of this paper is the provision of a dataset of 10 human expert annotations, annotated in the tradition of, that is moreover annotated for all error types.", "labels": [], "entities": []}, {"text": "With this new dataset, we have, for the first time, been able to compare system output against the gold standard annotations of a larger group of human annotators, in a realistic grammar checking scenario, and consequently been able to quantify the extent to which additional annotators affect system performance.", "labels": [], "entities": []}, {"text": "Additionally, we also noticed that some annotators tend to agree on certain error categories more than others and so attempt to explain this.", "labels": [], "entities": []}, {"text": "In light of the results, we also explore how human annotators themselves compare against the combined annotations of the remaining annotators and thus calculate an upper bound F 0.5 score for the given dataset and number of annotators; e.g., if one human versus nine other humans is only able to score a maximum of 70% F 0.5 , then it is unreasonable to expect a machine to do better.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 176, "end_pos": 187, "type": "METRIC", "confidence": 0.9808441996574402}, {"text": "F", "start_pos": 319, "end_pos": 320, "type": "METRIC", "confidence": 0.9977009892463684}]}, {"text": "For this reason, we propose a more informative method of evaluating a system based on the ratio of that system's F 0.5 score against the equivalent human F 0.5 score.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9783185720443726}, {"text": "F 0.5 score", "start_pos": 154, "end_pos": 165, "type": "METRIC", "confidence": 0.9599740107854208}]}, {"text": "Section 2 contains an overview of some of the latest research in both GEC and SMT that makes use of IAA statistics.", "labels": [], "entities": [{"text": "GEC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.7744570970535278}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9143424034118652}, {"text": "IAA statistics", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.6856761127710342}]}, {"text": "Section 3 shows an example sentence from our dataset and qualitatively analyses how individual annotator bias affects their choice of corrections.", "labels": [], "entities": []}, {"text": "Section 4 describes the data collection process and presents some preliminary results.", "labels": [], "entities": [{"text": "data collection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.6706083565950394}]}, {"text": "Section 5 discusses the main quantitative results of the paper, formalizing the formulas used and introducing the more informative method of ratio scoring for GEC, while Section 6 summarizes the results from our additional experiments on category agreement and essay subsets.", "labels": [], "entities": [{"text": "ratio scoring", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.6952178329229355}, {"text": "GEC", "start_pos": 159, "end_pos": 162, "type": "DATASET", "confidence": 0.5682628154754639}]}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to quantify how much the F-score can vary in a realistic grammar checking scenario when there is only one gold standard annotator, we first computed the scores fora participating system vs each annotator in a pairwise fashion.", "labels": [], "entities": [{"text": "F-score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9807913303375244}, {"text": "grammar checking", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.7592955827713013}]}, {"text": "hence shows how the top team in), performed against each of the 10 human annotators individually.", "labels": [], "entities": []}, {"text": "While Tetrault and Chodorow (2008) and reported a difference of 10% precision and 5% recall between their two individual annotators in their simplified preposition correction task, shows this difference can actually be as much as almost 15% precision (A1 vs A7) and 6% recall (A1 vs A3) in a more realistic full scale correction task.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9991100430488586}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9974944591522217}, {"text": "preposition correction task", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.7556817134221395}, {"text": "precision", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.9974954128265381}, {"text": "recall", "start_pos": 269, "end_pos": 275, "type": "METRIC", "confidence": 0.9965446591377258}]}, {"text": "This equates to a differ-  ence of over 7% F 0.5 (A3 vs A7) and once again shows how varied annotator's judgments can be.", "labels": [], "entities": [{"text": "differ-  ence", "start_pos": 18, "end_pos": 31, "type": "METRIC", "confidence": 0.9692586859067281}, {"text": "F 0.5", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9673433899879456}]}], "tableCaptions": [{"text": " Table 2: Statistics for the 50 unannotated essays.", "labels": [], "entities": []}, {"text": " Table 3: Table showing how many annotations each annotator made in terms of error category. See Ng  et al. (2014) Table 1 for a more detailed description of error categories.", "labels": [], "entities": []}, {"text": " Table 4: Table showing the F 0.5 scores for the top  team in CoNLL-2014, CAMB, against each of the  10 annotators individually.", "labels": [], "entities": [{"text": "F 0.5 scores", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.97352401415507}, {"text": "CoNLL-2014", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.9220091700553894}, {"text": "CAMB", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.7392879128456116}]}, {"text": " Table 5: Table showing average human F 0.5 scores over all combinations of 1 \u2264 i < 10 gold annotators  compared to the same averages for the top 3 systems in CoNLL-2014, and the ratio percentage of each  team's average score versus the human average score.", "labels": [], "entities": [{"text": "F 0.5 scores", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.9108917911847433}, {"text": "CoNLL-2014", "start_pos": 159, "end_pos": 169, "type": "DATASET", "confidence": 0.906790554523468}]}]}