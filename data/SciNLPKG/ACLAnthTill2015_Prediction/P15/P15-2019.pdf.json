{"title": [], "abstractContent": [{"text": "We propose IMAGINET, a model of learning visually grounded representations of language from coupled textual and visual input.", "labels": [], "entities": []}, {"text": "The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence.", "labels": [], "entities": []}, {"text": "Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes.", "labels": [], "entities": [{"text": "Mimicking", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9758923649787903}]}, {"text": "Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.", "labels": [], "entities": [{"text": "semantic interpretation of multi-word phrases", "start_pos": 63, "end_pos": 108, "type": "TASK", "confidence": 0.7818052411079407}]}], "introductionContent": [{"text": "Vision is the most important sense for humans and visual sensory input plays an important role in language acquisition by grounding meanings of words and phrases in perception.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.721337080001831}]}, {"text": "Similarly, in practical applications processing multimodal data where text is accompanied by images or videos is increasingly important.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel model of learning visually-grounded representations of language from paired textual and visual input.", "labels": [], "entities": []}, {"text": "The model learns language through comprehension and production, by receiving a textual description of a scene and trying to \"imagine\" a visual representation of it, while predicting the next word at the same time.", "labels": [], "entities": []}, {"text": "The full model, which we dub IMAGINET, consists of two Gated Recurrent Unit (GRU) networks coupled via shared word embeddings.", "labels": [], "entities": []}, {"text": "IMAGINET uses a multi-task Caruana (1997) objective: both networks read the sentence word-by-word in parallel; one of them predicts the feature representation of the image depicting the described scene after reading the whole sentence, while the other one predicts the next word at each position in the word sequence.", "labels": [], "entities": [{"text": "IMAGINET", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7456541657447815}]}, {"text": "The importance of the visual and textual objectives can be traded off, and either of them can be switched off entirely, enabling us to investigate the impact of visual vs textual information on the learned language representations.", "labels": [], "entities": []}, {"text": "Our approach to modeling human language learning has connections to recent models of image captioning (see Section 2).", "labels": [], "entities": [{"text": "image captioning", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7550925612449646}]}, {"text": "Unlike in many of these models, in IMAGINET the image is the target to predict rather then the input, and the model can build a visually-grounded representation of a sentence independently of an image.", "labels": [], "entities": []}, {"text": "We can directly compare the performance of IMAGINET against a simple multivariate linear regression model with bag-of-words features and thus quantify the contribution of the added expressive power of a recurrent neural network.", "labels": [], "entities": []}, {"text": "We evaluate our model's knowledge of word meaning and sentence structure through simulating human judgments of word similarity, retrieving images corresponding to single words as well as full sentences, and retrieving paraphrases of image captions.", "labels": [], "entities": []}, {"text": "In all these tasks the model outperforms the baseline; the model significantly correlates with human ratings of word similarity, and predicts appropriate visual interpretations of single and multi-word phrases.", "labels": [], "entities": []}, {"text": "The acquired knowledge of sentence structure boosts the model's performance in both image and caption retrieval.", "labels": [], "entities": [{"text": "caption retrieval", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7045507878065109}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word similarity correlations with human  judgments measured by Spearman's \u03c1 (all correla- tions are significant at level p < 0.01).", "labels": [], "entities": []}, {"text": " Table 2: Accuracy@5 of retrieving images with  compatible labels from ImageNet.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9934787750244141}, {"text": "ImageNet", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9463739991188049}]}]}