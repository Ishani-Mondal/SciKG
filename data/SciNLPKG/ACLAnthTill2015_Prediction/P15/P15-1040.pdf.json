{"title": [{"text": "Aligning Opinions: Cross-Lingual Opinion Mining with Dependencies", "labels": [], "entities": [{"text": "Aligning Opinions", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9020566940307617}]}], "abstractContent": [{"text": "We propose a cross-lingual framework for fine-grained opinion mining using bitext projection.", "labels": [], "entities": [{"text": "fine-grained opinion mining", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7294038534164429}]}, {"text": "The only requirements area running system in a source language and word-aligned parallel data.", "labels": [], "entities": []}, {"text": "Our method projects opinion frames from the source to the target language, and then trains a system on the target language using the automatic annotations.", "labels": [], "entities": []}, {"text": "Key to our approach is a novel dependency-based model for opinion mining, which we show, as a byprod-uct, to be on par with the current state of the art for English, while avoiding the need for integer programming or rerank-ing.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.8052560389041901}]}, {"text": "In cross-lingual mode (English to Por-tuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of opinion mining is to extract opinions and sentiments from text (.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7972930073738098}]}, {"text": "With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (), tracking of newswire and blogs (), question answering (, and text-to-speech synthesis).", "labels": [], "entities": [{"text": "summarization of customer reviews", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.8604441285133362}, {"text": "tracking of newswire and blogs", "start_pos": 195, "end_pos": 225, "type": "TASK", "confidence": 0.8787264704704285}, {"text": "question answering", "start_pos": 230, "end_pos": 248, "type": "TASK", "confidence": 0.9203162789344788}, {"text": "text-to-speech synthesis", "start_pos": 256, "end_pos": 280, "type": "TASK", "confidence": 0.7138107866048813}]}, {"text": "While early work has focused on determining sentiment at document and sentence level), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (, or addressing compositionality).", "labels": [], "entities": [{"text": "finegrained opinion mining", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.6691487729549408}]}, {"text": "Since the release of the MPQA corpus 1 (, a standard corpus for fine-grained opinion mining of news documents, along string of work has been produced (reviewed in \u00a72).", "labels": [], "entities": [{"text": "MPQA corpus 1", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9557402729988098}, {"text": "fine-grained opinion mining of news documents", "start_pos": 64, "end_pos": 109, "type": "TASK", "confidence": 0.810206284125646}]}, {"text": "Despite the large volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.8563500940799713}]}, {"text": "This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages.", "labels": [], "entities": []}, {"text": "We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection.", "labels": [], "entities": [{"text": "fine-grained opinion mining", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7429211338361105}]}, {"text": "This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging, named entity recognition (), syntactic parsing (), semantic role labeling, and coreference resolution.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6392645120620728}, {"text": "named entity recognition", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.6176203191280365}, {"text": "syntactic parsing", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.783072829246521}, {"text": "semantic role labeling", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.7161720395088196}, {"text": "coreference resolution", "start_pos": 179, "end_pos": 201, "type": "TASK", "confidence": 0.9621930718421936}]}, {"text": "Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language.", "labels": [], "entities": []}, {"text": "To alleviate the complexity of the task, we start by introducing a lightweight representationcalled dependency-based opinion mining-and convert the MPQA corpus to this formalism ( \u00a73).", "labels": [], "entities": [{"text": "representationcalled dependency-based opinion mining-and", "start_pos": 79, "end_pos": 135, "type": "TASK", "confidence": 0.5939576774835587}, {"text": "MPQA corpus", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.9434676170349121}]}, {"text": "We propose a simple arc-factored model that permits easy decoding ( \u00a74) and we show that, despite its simplicity, this model is on par with state-ofthe-art opinion mining systems for English ( \u00a75).", "labels": [], "entities": []}, {"text": "Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting corpus ( \u00a76).", "labels": [], "entities": []}, {"text": "As part of this work, a validation corpus in Portuguese with subjectivity annotations was created, along with a translation of the MPQA Subjectivity lexicon of . Experimental evaluation ( \u00a77) shows that our cross-lingual approach surpasses a supervised system trained on a small corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon.", "labels": [], "entities": [{"text": "MPQA Subjectivity lexicon", "start_pos": 131, "end_pos": 156, "type": "DATASET", "confidence": 0.914350708325704}]}], "datasetContent": [{"text": "Ina first set of experiments, we evaluated the performance of our dependency-based model for opinion mining ( \u00a73) in the MPQA English corpus.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.8127767443656921}, {"text": "MPQA English corpus", "start_pos": 121, "end_pos": 140, "type": "DATASET", "confidence": 0.966650148232778}]}, {"text": "Opinion spans (Op.) are evaluated with F 1 scores, according to two matching criteria commonly used in the literature: overlap matching (OM), where a predicted span is counted as correct if it overlaps a gold one, and proportional matching (PM), proposed by.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9701714118321737}, {"text": "overlap matching (OM)", "start_pos": 119, "end_pos": 140, "type": "METRIC", "confidence": 0.9352978944778443}, {"text": "proportional matching (PM)", "start_pos": 218, "end_pos": 244, "type": "METRIC", "confidence": 0.9074575066566467}]}, {"text": "For the latter, we use the following formula for the recall, where we consider the sets of gold (G) and predicted (P) opinion spans: 7 the precision is P (G, P) = R(P, G).", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9949613809585571}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9987301230430603}]}, {"text": "We also report metrics based on ahead matching (HM) criterion, where a predicted span is considered correct if its syntactic head matches the head of the gold span.", "labels": [], "entities": [{"text": "ahead matching (HM) criterion", "start_pos": 32, "end_pos": 61, "type": "METRIC", "confidence": 0.9416058361530304}]}, {"text": "We consider that a pair opinion-agent (Op-Ag.) or opinion-target (Op-Tg.) is correctly extracted according to the OM or the HM criteria, if both the elements satisfy these criteria and the relation holds in the gold data.", "labels": [], "entities": []}, {"text": "We also compute the metric described in which measures how well agents of opinions are predicted based on a proportional matching (PM) criterion.", "labels": [], "entities": [{"text": "proportional matching (PM) criterion", "start_pos": 108, "end_pos": 144, "type": "METRIC", "confidence": 0.8036902695894241}]}, {"text": "This metric is applied to evaluate the extraction of both agents and targets.", "labels": [], "entities": []}, {"text": "Finally, to evaluate the opinions' polarities (Op-Pol.", "labels": [], "entities": []}, {"text": "metric) we consider as correct opinions where the span and polarity both match the gold ones.", "labels": [], "entities": [{"text": "span", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9805795550346375}]}, {"text": "Ina final set of experiments, we compare three systems of fine-grained opinion mining for Portuguese.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7732475697994232}]}, {"text": "All were trained as described in \u00a75.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Method comparison: F 1 scores obtained in the MPQA corpus, for our dependency based method  and the approaches in", "labels": [], "entities": [{"text": "F 1", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9827741086483002}, {"text": "MPQA corpus", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.962619960308075}]}, {"text": " Table 2: Number of documents, sentences and  opinions in the Portuguese Corpus.", "labels": [], "entities": [{"text": "Portuguese Corpus", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.957909882068634}]}, {"text": " Table 3: Inter-annotator agreement in the test par- tition (shown are F 1 scores).", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9464822212855021}]}, {"text": " Table 5: Comparison of cross-lingual approaches. F 1 scores obtained in our Portuguese validation corpus  using: a SUPERVISED system trained on the small available data, a DELEXICALIZED system trained with  universal POS tags and multilingual embeddings and our BITEXT PROJECTION OF DEPENDENCIES.  The symbol * indicates that the best system beats the other systems with statistical significance, with  p < 0.05 and according to a bootstrap resampling test", "labels": [], "entities": [{"text": "F 1", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9879455864429474}, {"text": "Portuguese validation corpus", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.8500654896100363}, {"text": "DELEXICALIZED", "start_pos": 173, "end_pos": 186, "type": "METRIC", "confidence": 0.8828657269477844}, {"text": "BITEXT", "start_pos": 263, "end_pos": 269, "type": "METRIC", "confidence": 0.9679110050201416}]}]}