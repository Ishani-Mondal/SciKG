{"title": [{"text": "A Context-Aware Topic Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.8571783502896627}]}], "abstractContent": [{"text": "Lexical selection is crucial for statistical machine translation.", "labels": [], "entities": [{"text": "Lexical selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9104815125465393}, {"text": "statistical machine translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.746630996465683}]}, {"text": "Previous studies separately exploit sentence-level contexts and document-level topics for lexical selection, neglecting their correlations.", "labels": [], "entities": [{"text": "lexical selection", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.6703561246395111}]}, {"text": "In this paper, we propose a context-aware topic model for lexical selection , which not only models local contexts and global topics but also captures their correlations.", "labels": [], "entities": [{"text": "lexical selection", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6856162548065186}]}, {"text": "The model uses target-side translations as hidden variables to connect document topics and source-side local contextual words.", "labels": [], "entities": []}, {"text": "In order to learn hidden variables and distributions from data, we introduce a Gibbs sampling algorithm for statistical estimation and inference.", "labels": [], "entities": [{"text": "statistical estimation", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.794098287820816}]}, {"text": "A new translation probability based on distributions learned by the model is integrated into a translation system for lexical selection.", "labels": [], "entities": []}, {"text": "Experiment results on NIST Chinese-English test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality.", "labels": [], "entities": [{"text": "NIST Chinese-English test sets", "start_pos": 22, "end_pos": 52, "type": "DATASET", "confidence": 0.9387199133634567}]}], "introductionContent": [{"text": "Lexical selection is a very important task in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Lexical selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9106464982032776}, {"text": "statistical machine translation (SMT)", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.814959724744161}]}, {"text": "Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge.", "labels": [], "entities": []}, {"text": "Most conventional SMT systems () exploit very limited context information contained in bilingual rules for lexical selection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9925988912582397}]}, {"text": "Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts) or 2) integrating document-level topics) into SMT.", "labels": [], "entities": [{"text": "lexical selection", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7298911809921265}, {"text": "SMT", "start_pos": 195, "end_pos": 198, "type": "TASK", "confidence": 0.9794484972953796}]}, {"text": "The methods in these two strands have shown their effectiveness on lexical selection.", "labels": [], "entities": [{"text": "lexical selection", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.6964869946241379}]}, {"text": "However, correlations between sentence-and document-level contexts have never been explored before.", "labels": [], "entities": []}, {"text": "It is clear that local contexts and global topics are often highly correlated.", "labels": [], "entities": []}, {"text": "Consider a ChineseEnglish translation example presented in.", "labels": [], "entities": []}, {"text": "On the one hand, if local contexts suggest that the source word \"\u00e1|/l` \u0131ch\u02c7ang\u0131ch\u02c7ang\" should be translated in-to \"stance\", they will also indicate that the topic of the document where the example sentence occurs is about politics.", "labels": [], "entities": []}, {"text": "The politics topic can be further used to enable the decoder to select a correct translation \"issue\" for another source word \"\u00af K/w` ent\u02c7ient\u02c7 ent\u02c7i\", which is consistent with this topic.", "labels": [], "entities": []}, {"text": "On the other hand, if we know that this document mainly focuses on the politics topic, the candiate translation \"stance\" will be more compatible with the context of \"\u00e1|/l` \u0131ch\u02c7ang\u0131ch\u02c7ang\" than the candiate translation \"attitude\".", "labels": [], "entities": []}, {"text": "This is because neighboring sourceside words \"\u00a5I/zh\u00af ongu\u00f3\" and \"\u00a5\u00e1/zh\u00afong\u00ec \u0131\" often occur in documents that are about international politics.", "labels": [], "entities": []}, {"text": "We believe that such correlations between local contextual words and global topics can be used to further improve lexical selection.", "labels": [], "entities": []}, {"text": "In this paper, we propose a unified framework to jointly model local contexts, global topics as well as their correlations for lexical selection.", "labels": [], "entities": [{"text": "lexical selection", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.68665811419487}]}, {"text": "Specifically, \u2022 First, we present a context-aware topic model (CATM) to exploit the features mentioned above for lexical selection in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9689390063285828}]}, {"text": "To the best of our knowledge, this is the first work to jointly model both local and global contexts for lexical selection in a topic model.", "labels": [], "entities": []}, {"text": "\u2022 Second, we present a Gibbs sampling algorithm to learn various distributions that are related to topics and translations from data.", "labels": [], "entities": []}, {"text": "The translation probabilities derived from our model are integrated into SMT to allow collective lexical selection with both local and global informtion.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9777795076370239}]}, {"text": "We validate the effectiveness of our model on a state-of-the-art phrase-based translation system.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.6356073617935181}]}, {"text": "Experiment results on the NIST Chinese-English translation task show that our model significantly outperforms previous lexical selection methods.", "labels": [], "entities": [{"text": "NIST Chinese-English translation task", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7845008671283722}]}], "datasetContent": [{"text": "In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.6321310251951218}]}], "tableCaptions": [{"text": " Table 2: Experiment results on the development set using  different window sizes w s .", "labels": [], "entities": []}, {"text": " Table 3: Experiment results on the test sets. Avg = average BLEU scores. WSDM (All) and TLTM (All) are models  built for all source words. \u2193: significantly worse than CATM (p<0.05), \u2193\u2193: significantly worse than CATM (p<0.01)  .", "labels": [], "entities": [{"text": "Avg", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9948599338531494}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9992092251777649}, {"text": "WSDM", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7792671918869019}, {"text": "TLTM (All)", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.8974111825227737}]}, {"text": " Table 5: Experiment results on the test sets. CATM (Log- linear) is the combination of CATM (Context) and CATM  (Topic) in a log-linear manner.", "labels": [], "entities": []}]}