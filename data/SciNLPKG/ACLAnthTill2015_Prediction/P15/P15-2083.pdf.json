{"title": [{"text": "Extended Topic Model for Word Dependency", "labels": [], "entities": [{"text": "Word Dependency", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.572159081697464}]}], "abstractContent": [{"text": "Topic Model such as Latent Dirichlet Allocation(LDA) makes assumption that topic assignment of different words are conditionally independent.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew model Extended Global Topic Random Field (EGTRF) to model non-linear dependencies between words.", "labels": [], "entities": []}, {"text": "Specifically, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words.", "labels": [], "entities": []}, {"text": "Word similarity information learned from large corpus is incorporated to enhance word topic assignment.", "labels": [], "entities": [{"text": "word topic assignment", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.745053788026174}]}, {"text": "Parameters are estimated efficiently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9050548076629639}, {"text": "EGTRF", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.6187666654586792}]}], "introductionContent": [{"text": "Probabilistic topic model such as Latent Dirichlet Allocation(LDA) () has been widely used for discovering latent topics from document collections by capturing words' cooccuring relation.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation(LDA)", "start_pos": 34, "end_pos": 66, "type": "METRIC", "confidence": 0.7731563150882721}]}, {"text": "However, the \"bag of words\" assumption is employed inmost existing topic models, it assumes the order of words can be ignored and topic assignment of each word is conditionally independent given the topic mixture of a document.", "labels": [], "entities": []}, {"text": "To relax the \"bag of words\" assumption, many extended topic models have been proposed to address the limitation of conditional independence.", "labels": [], "entities": []}, {"text": "Wallach () explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables.", "labels": [], "entities": []}, {"text": "Gruber () models the topics of words in the document as a Markov chain, and assumes all words in the same sentence are more likely to have the same topic.", "labels": [], "entities": []}, {"text": "Zhu () incorporates Markov dependency between topic assignments of neighboring words, and employs a general structure of the GLM to define a conditional distribution of latent topic assignments over words.", "labels": [], "entities": []}, {"text": "Most of the models above are limited to model linear topical dependencies between words, word topical dependencies can also be modeled by a non-linear way.", "labels": [], "entities": [{"text": "word topical dependencies", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7328783373037974}]}, {"text": "In Syntactic topic models, each word of a sentence is generated by a distribution that combines document-specific topic weights and parsetree-specific syntactic transitions.", "labels": [], "entities": [{"text": "Syntactic topic", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.8611611127853394}]}, {"text": "In Global Topic Random Field(GTRF) model (), sentences of a document are parsed into dependency trees (Marneffe et al, 2008) () ().", "labels": [], "entities": []}, {"text": "They show topics of semantically or syntactically dependent words achieve the highest similarity and are able to provide more useful information in topic modeling, which is also the basic assumption of our model.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 148, "end_pos": 162, "type": "TASK", "confidence": 0.8089977502822876}]}, {"text": "Then they propose GTRF to model non-linear topical dependencies, word topics are sampled based on graph structure instead of \"bag of words\" representation, the conditional independence of word topic assignment is thus relaxed.", "labels": [], "entities": []}, {"text": "However, GTRF assumes topic assignment of a word vertex depends on the topic mixture of the document and its neighboring word vertices, ignoring the fact that word vertex can also be influenced by the distance-2 or further word vertices.", "labels": [], "entities": []}, {"text": "In this paper, we extend GTRF model and present a novel model Extended Global Topic Random Field (EGTRF) to exploit topical dependency between words.", "labels": [], "entities": [{"text": "Extended Global Topic Random Field (EGTRF)", "start_pos": 62, "end_pos": 104, "type": "METRIC", "confidence": 0.6603338643908501}]}, {"text": "In EGTRF, the topic assignment of a word is assumed to depend on both distance-1 and distance-2 word vertices.", "labels": [], "entities": []}, {"text": "An example of a simple document that has two sentences shows in.", "labels": [], "entities": []}, {"text": "The two sentences are parsed into dependency trees respectively, and then merged into a graph.", "labels": [], "entities": []}, {"text": "Some hidden dependency relations can also be extracted by merging dependency trees.", "labels": [], "entities": []}, {"text": "For example, word \"allocation\" has anew distance-2 word \"topics\" after merging.", "labels": [], "entities": [{"text": "word \"allocation\"", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6240117028355598}]}, {"text": "Therefore, EGTRF can exploit more semantically or syntactically word dependencies.", "labels": [], "entities": []}, {"text": "Theoretically, we can also model the distance further than 2, however, it leads to more complicated computation and small increase of performance.", "labels": [], "entities": []}, {"text": "Another advantage of EGTRF is it incorporates word features.", "labels": [], "entities": []}, {"text": "The word vector representations are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns ).", "labels": [], "entities": []}, {"text": "We use the pretrained model from Google News dataset(about 100 billion words) using word2vec 1 tool to represent each word as a 300-dimensional word vector, and apply normalized word similarity as a confidence score to indicate how possible two word vertices share same topic.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.8349726796150208}]}, {"text": "We organized the paper as below: EGTRF is presented in Section 2, variational inference and parameter estimation are derived in Section 3, experiments on two datasets are showed in Section 4, we conclude the paper in Section 5.", "labels": [], "entities": [{"text": "EGTRF", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.8287038803100586}]}], "datasetContent": [{"text": "In this section we study the empirical performance of EGTRF on two datasets.", "labels": [], "entities": []}, {"text": "For each dataset, we remove very short documents, and compute a vocabulary by removing stop words, rare words, frequent words.", "labels": [], "entities": []}, {"text": "Eighty percent data are used for training, others for testing.", "labels": [], "entities": []}, {"text": "\u2022 20 News Groups: After processing, it contains 13706 documents with a vocabulary of 5164 terms.", "labels": [], "entities": []}, {"text": "\u2022 NIPS data (): Spanning from 2000 to 2005.", "labels": [], "entities": [{"text": "NIPS data", "start_pos": 2, "end_pos": 11, "type": "DATASET", "confidence": 0.8803808987140656}]}, {"text": "After processing, it contains 843 documents with a vocabulary of 6098 terms.", "labels": [], "entities": []}, {"text": "We evaluate how well a model fits the data with held-out perplexity () and predictive distribution).", "labels": [], "entities": []}, {"text": "Lower perplexity, higher log predictive probability indicate better generalization performance.", "labels": [], "entities": []}, {"text": "We implement GTRF without adding self defined edges from the original paper, and set \u03bb 2 = 0.2 to give higher reward to edges from E 1 that the two word vertices have same topic.", "labels": [], "entities": []}, {"text": "We set \u03bb 4 = 1.2 to give lower(even negative) reward to edges from E 2 that the two word vertices have same topic in EGTRF, since the distance-1 words are expected to have greater topical affects than distance-2 words.", "labels": [], "entities": [{"text": "EGTRF", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.8882389068603516}]}, {"text": "Word is represented as vector from pretrained Google News dataset, we use the word vector learned from original corpus when the word does not exist in pre-trained Google News dataset.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.8471258282661438}, {"text": "Google News dataset", "start_pos": 163, "end_pos": 182, "type": "DATASET", "confidence": 0.7955523133277893}]}, {"text": "We choose 10, 20, 30, 50 topics for 20 news dataset, 10, 15, 20, 25 topics for NIPS dataset.", "labels": [], "entities": [{"text": "NIPS dataset", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9731030762195587}]}, {"text": "shows the experimental results of four models: lda, gtrf, egtrf(EGTRF without word similarity information), and egtrf+s(EGTRF with word similarity information) on two datasets.", "labels": [], "entities": []}, {"text": "The results show EGTRF outperforms LDA and GTRF in general, and EGTRF with word similarity information achieves best performance.", "labels": [], "entities": []}, {"text": "We believe modeling distance-2 word vertices can exploit more semantically or syntactically word dependencies from document, and word similarity information obtained from large corpus can makeup the lack of sufficient information from the original corpus.", "labels": [], "entities": []}, {"text": "Therefore, adding the influence of distance-2 word vertices and word similarity information can improve performance of topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.7509324550628662}]}], "tableCaptions": []}