{"title": [{"text": "Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers", "labels": [], "entities": [{"text": "AMR Parsing", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8124306201934814}]}], "abstractContent": [{"text": "We report improved AMR parsing results by adding anew action to a transition-based AMR parser to infer abstract concepts and by incorporating richer features produced by auxiliary analyzers such as a semantic role labeler and a coreference resolver.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9497491717338562}, {"text": "AMR parser", "start_pos": 83, "end_pos": 93, "type": "TASK", "confidence": 0.7120378017425537}]}, {"text": "We report final AMR parsing results that show an improvement of 7% absolute in F 1 score over the best previously reported result.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.8725817501544952}, {"text": "absolute", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9883618950843811}, {"text": "F 1 score", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9776642918586731}]}, {"text": "Our parser is available at: https://github.com/ Juicechuan/AMRParsing", "labels": [], "entities": [{"text": "AMRParsing", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.3701232671737671}]}], "introductionContent": [{"text": "AMR parsing is the task of taking a sentence as input and producing as output an Abstract Meaning Representation (AMR) that is a rooted, directed, edge-labeled and leaf-labeled graph that is used to represent the meaning of a sentence.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8512891232967377}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.6627016216516495}]}, {"text": "AMR parsing has drawn an increasing amount of attention recently.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9060221314430237}]}, {"text": "The first published AMR parser, JAMR), performs AMR parsing in two stages: concept identification and relation identification.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.8623749613761902}, {"text": "JAMR", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8880825042724609}, {"text": "AMR parsing", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9339590072631836}, {"text": "concept identification", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7282349169254303}, {"text": "relation identification", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.8214824795722961}]}, {"text": "treat concept identification as a sequence labeling task and utilize a semiMarkov model to map spans of words in a sentence to concept graph fragments.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.7876289486885071}]}, {"text": "For relation identification, they adopt graph-based techniques similar to those used in dependency parsing).", "labels": [], "entities": [{"text": "relation identification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.973732978105545}, {"text": "dependency parsing", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7832279205322266}]}, {"text": "Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage.", "labels": [], "entities": []}, {"text": "A competitive alternative to the MSCG approach is transition-based AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.8725996613502502}]}, {"text": "Our previous work () describes a transition-based system that also involves two stages.", "labels": [], "entities": []}, {"text": "In the first step, an input sentence is parsed into a dependency tree with a dependency parser.", "labels": [], "entities": []}, {"text": "In the second step, the transition-based AMR parser transforms the dependency tree into an AMR graph by performing a series of actions.", "labels": [], "entities": []}, {"text": "Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step.", "labels": [], "entities": []}, {"text": "Unlike a dependency parse where each leaf node corresponds to a word in a sentence and there is an inherent alignment between the words in a sentence and the leaf nodes in the parse tree, the alignment between the word tokens in a sentence and the concepts in an AMR graph is non-trivial.", "labels": [], "entities": []}, {"text": "Both JAMR and our transition-based parser rely on a heuristics based aligner that can align the words in a sentence and concepts in its AMR with a 90% F 1 score, but there are some concepts in the AMR that cannot be aligned to any word in a sentence.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.8526581525802612}, {"text": "F 1 score", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9822947978973389}]}, {"text": "This is illustrated in where the concept have-org-role-91 is not aligned to any word or word sequence.", "labels": [], "entities": []}, {"text": "We refer to these concepts as abstract concepts, and existing AMR parsers do not have a systematic way of inferring such abstract concepts.", "labels": [], "entities": []}, {"text": "Current AMR parsers are in their early stages of development, and their features are not yet fully developed.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.8878958225250244}]}, {"text": "For example, the AMR makes heavy use of the framesets and semantic role labels used in the Proposition Bank (), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy.", "labels": [], "entities": [{"text": "AMR", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7117320895195007}, {"text": "Proposition Bank", "start_pos": 91, "end_pos": 107, "type": "DATASET", "confidence": 0.9003248810768127}, {"text": "PropBank", "start_pos": 206, "end_pos": 214, "type": "DATASET", "confidence": 0.9467664361000061}, {"text": "AMR parsing", "start_pos": 254, "end_pos": 265, "type": "TASK", "confidence": 0.8521346747875214}, {"text": "accuracy", "start_pos": 266, "end_pos": 274, "type": "METRIC", "confidence": 0.74997878074646}]}, {"text": "Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well.", "labels": [], "entities": []}, {"text": "In this paper, we describe an extension to our transition-based AMR parser ( by adding anew action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system () and coreference system ().", "labels": [], "entities": []}, {"text": "We also experimented with adding Brown clusters as features to the AMR parser.", "labels": [], "entities": []}, {"text": "Additionally, we experimented with using different syntactic parsers in the first stage.", "labels": [], "entities": []}, {"text": "Following our previous work, we use the averaged perceptron algorithm) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance.", "labels": [], "entities": []}, {"text": "Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding the new action significantly improves the parser performance, and (iii) semantic role information is beneficial to AMR parsing when used as features, while the Brown clusters do not make a difference and coreference information slightly hurts the AMR parsing performance.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 253, "end_pos": 264, "type": "TASK", "confidence": 0.9241026341915131}, {"text": "AMR parsing", "start_pos": 385, "end_pos": 396, "type": "TASK", "confidence": 0.8629123270511627}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly describe the transition-based parser, and in Section 3 we describe our extensions.", "labels": [], "entities": []}, {"text": "We report experimental results in Section 4 and conclude the paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first tune and evaluate our system on the newswire section of LDC2013E117 dataset.", "labels": [], "entities": [{"text": "newswire section of LDC2013E117 dataset", "start_pos": 45, "end_pos": 84, "type": "DATASET", "confidence": 0.8976494312286377}]}, {"text": "Then we show our parser's performance on the recent LDC2014T12 dataset.", "labels": [], "entities": [{"text": "LDC2014T12 dataset", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.9740370810031891}]}, {"text": "We first conduct our experiments on the newswire section of AMR annotation corpus (LDC2013E117).", "labels": [], "entities": [{"text": "newswire section of AMR annotation corpus (LDC2013E117)", "start_pos": 40, "end_pos": 95, "type": "DATASET", "confidence": 0.8769896692699857}]}, {"text": "The train/dev/test split of dataset is 4.0K/2.1K/2.1K, which is identical to the settings of JAMR.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.915229082107544}]}, {"text": "We evaluate our parser with Smatch v2.0: AMR parsing performance on the development set.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.7290022373199463}]}, {"text": "In this section, we conduct experiments on the AMR annotation release 1.0 (LDC2014T12), which contains 13,051 AMRs from newswire, weblogs and web discussion forums.", "labels": [], "entities": [{"text": "AMR annotation release 1.0 (LDC2014T12)", "start_pos": 47, "end_pos": 86, "type": "DATASET", "confidence": 0.8336013300078255}]}, {"text": "We use the training/development/test split recommended in the release: 10,312 sentences for training, 1,368 sentences for development and 1,371 sentences for testing.", "labels": [], "entities": []}, {"text": "We re-train the parser on the LDC2014T12 training set with the best parser configuration given in \u00a74.1, and test the parser on the test set.", "labels": [], "entities": [{"text": "LDC2014T12 training set", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.9760884642601013}]}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "For comparison purposes, we also include the results of the updated version of JAMR and our baseline parser in () which are also trained on the same dataset.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.8690759539604187}]}, {"text": "There is a significant drop-off in performance compared with the results on the LDC2013E117 test set for all the parsers, but our parser outperforms the other parsers by a similar margin on both test sets.", "labels": [], "entities": [{"text": "LDC2013E117 test set", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.9790133039156595}]}], "tableCaptions": [{"text": " Table 1: AMR parsing performance on develop- ment set using different syntactic parsers.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7848844230175018}]}, {"text": " Table 2: AMR parsing performance on the devel- opment set.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7098881155252457}]}, {"text": " Table 3: AMR parsing performance on the news  wire test set of LDC2013E117.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7723155319690704}, {"text": "news  wire test set of LDC2013E117", "start_pos": 41, "end_pos": 75, "type": "DATASET", "confidence": 0.9291166166464487}]}, {"text": " Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9191546440124512}, {"text": "LDC2014T12 test set", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9322438438733419}]}]}