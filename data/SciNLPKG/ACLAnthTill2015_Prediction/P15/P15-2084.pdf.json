{"title": [{"text": "Dependency Recurrent Neural Language Models for Sentence Completion", "labels": [], "entities": [{"text": "Sentence Completion", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.9076102674007416}]}], "abstractContent": [{"text": "Recent work on language modelling has shifted focus from count-based models to neural models.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7447202503681183}]}, {"text": "In these works, the words in each sentence are always considered in a left-to-right order.", "labels": [], "entities": []}, {"text": "In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points inaccuracy.", "labels": [], "entities": [{"text": "Microsoft Research Sentence Completion Challenge", "start_pos": 32, "end_pos": 80, "type": "DATASET", "confidence": 0.7365269541740418}]}, {"text": "Furthermore, we achieve results comparable with the state-of-the-art models on this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language Models (LM) are commonly used to score a sequence of tokens according to its probability of occurring in natural language.", "labels": [], "entities": []}, {"text": "They are an essential building block in a variety of applications such as machine translation, speech recognition and grammatical error correction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.812385618686676}, {"text": "speech recognition", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7980384230613708}, {"text": "grammatical error correction", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.6527995467185974}]}, {"text": "The standard way of evaluating a language model has been to calculate its perplexity on a large corpus.", "labels": [], "entities": []}, {"text": "However, this evaluation assumes the output of the language model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance.", "labels": [], "entities": []}, {"text": "For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates.", "labels": [], "entities": [{"text": "Sentence Completion Challenge", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.8899260560671488}]}, {"text": "Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g.) can be compared.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9991577863693237}]}, {"text": "Recent approaches for this task include both neural and count-based language models ().", "labels": [], "entities": []}, {"text": "Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence.", "labels": [], "entities": []}, {"text": "In this work we propose a novel neural language model that learns a recurrent neural network (RNN) () on top of the syntactic dependency parse of a sentence.", "labels": [], "entities": []}, {"text": "Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by for count-based language models.", "labels": [], "entities": []}, {"text": "Our Dependency RNN model is published simultaneously with another model, introduced in, who extend the Long-Short Term Memory (LSTM) architecture to tree-structured network topologies and evaluate it at sentence-level sentiment classification and semantic relatedness tasks, but not as a language model.", "labels": [], "entities": [{"text": "sentence-level sentiment classification", "start_pos": 203, "end_pos": 242, "type": "TASK", "confidence": 0.641342411438624}]}, {"text": "Adapting the RNN to use the syntactic dependency structure required to reset and run the network on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths.", "labels": [], "entities": []}, {"text": "Furthermore, we explain how we can incorporate the dependency labels as features.", "labels": [], "entities": []}, {"text": "Our results show that the dependency RNN language model proposed outperforms the RNN proposed by by about 10 points inaccuracy.", "labels": [], "entities": []}, {"text": "Furthermore, it improves upon the count-based dependency language model of, while achieving slightly worse than the recent state-of-the-art results by.", "labels": [], "entities": []}, {"text": "Finally, we make the code and preprocessed data available to facilitate comparisons with future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We modified the Feature-Augmented RNN toolkit 2 and adapted it to handle tree-structured data.", "labels": [], "entities": []}, {"text": "Specifically, and instead of being run sequentially on the entire training corpus, the RNN is run on all the word tokens in all unrolls of all the sentences in all the books of the corpus.", "labels": [], "entities": []}, {"text": "The RNN is reset at the beginning of each unroll of a sentence.", "labels": [], "entities": [{"text": "RNN", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7148165702819824}]}, {"text": "When calculating the log-probability of a sentence, the contribution of each word token is counted only once (and stored in a hash-table specific for that sentence).", "labels": [], "entities": []}, {"text": "Once all the unrolls of a sentence are processed, the log-probability of the sentence is the sum of the per-token logprobabilities in that hash-table.", "labels": [], "entities": []}, {"text": "We also further enhanced the RNN library by replacing some large matrix multiplication routines by calls to the CBLAS library, thus yielding a two-to three-fold speed-up in the test and training time.", "labels": [], "entities": [{"text": "CBLAS library", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.9123108386993408}]}, {"text": "The training corpus consists of 522 19th century novels from Project Gutenberg ( . All processing (sentence-splitting, PoS tagging, syntactic parsing) was performed using the Stanford CoreNLP toolkit ().", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.7666924297809601}, {"text": "syntactic parsing", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7297487556934357}, {"text": "Stanford CoreNLP toolkit", "start_pos": 175, "end_pos": 199, "type": "DATASET", "confidence": 0.8697326978047689}]}, {"text": "The test set contains 1040 sentences to be completed.", "labels": [], "entities": []}, {"text": "Each sentence consists of one ground truth and 4 impostor sentences where a specific word has been replaced with a syntactically correct but semantically incorrect impostor word.", "labels": [], "entities": []}, {"text": "Dependency trees are generated for each sentence candidate.", "labels": [], "entities": []}, {"text": "We split that set into two, using the first 520 sentences in the validation (development) set and the latter 520 sentences in the test set.", "labels": [], "entities": []}, {"text": "During training, we start annealing the learning rate \u03bb with decay factor 0.66 as soon as the classification error on the validation set starts to increase.", "labels": [], "entities": []}, {"text": "shows the accuracy (validation and test sets) obtained using a simple RNN with 50, 100, 200 and 300-dimensional hidden word representation and 250 frequency-based word classes (vocabulary size N = 72846 words appearing at least 5 times in the training corpus).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994634985923767}]}, {"text": "One notices that adding the direct word context to target word connections (using the additional matrix described in section 2), enables to jump from a poor performance of about 30% accuracy to about 40% test accuracy, essentially matching the 39% accuracy reported for Good-Turing n-gram language models in . Modelling 4-grams yields even better results, closer to the 45% accuracy reported for RNNs in ( ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9936961531639099}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9205777049064636}, {"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9636064171791077}, {"text": "accuracy", "start_pos": 374, "end_pos": 382, "type": "METRIC", "confidence": 0.9751983880996704}]}, {"text": "As shows, dependency RNNs (depRNN) enable about 10 point word accuracy improvement over sequential RNNs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9533362984657288}]}], "tableCaptions": [{"text": " Table 1: Accuracy of sequential RNN on the MSR  Sentence Completion Challenge.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9919752478599548}, {"text": "RNN", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.869926393032074}, {"text": "MSR  Sentence Completion Challenge", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.7729207575321198}]}, {"text": " Table 2: Accuracy of (un-)labeled dependency  RNN", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980700612068176}]}]}