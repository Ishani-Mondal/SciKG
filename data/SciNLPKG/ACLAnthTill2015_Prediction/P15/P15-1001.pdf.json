{"title": [{"text": "On Using Very Large Target Vocabulary for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7614590724309286}]}], "abstractContent": [{"text": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8047105669975281}, {"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7344372719526291}, {"text": "phrase-based statistical machine translation", "start_pos": 185, "end_pos": 229, "type": "TASK", "confidence": 0.6058132126927376}]}, {"text": "Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6436330576737722}]}, {"text": "In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity.", "labels": [], "entities": []}, {"text": "We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary.", "labels": [], "entities": []}, {"text": "The models trained by the proposed approach are empirically found to match, and in some cases out-perform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models.", "labels": [], "entities": [{"text": "LSTM-based neural machine translation", "start_pos": 166, "end_pos": 203, "type": "TASK", "confidence": 0.6653726994991302}]}, {"text": "Furthermore , when we use an ensemble of a few models with very large target vocabularies , we achieve performance comparable to the state of the art (measured by BLEU) on both the English\u2192German and English\u2192French translation tasks of WMT'14.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9992658495903015}, {"text": "WMT'14", "start_pos": 236, "end_pos": 242, "type": "DATASET", "confidence": 0.870167076587677}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7972430338462194}, {"text": "solving machine translation", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.6746867398420969}]}, {"text": "In neural machine translation, one builds a single neural network that reads a source sentence and generates its translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6933518648147583}]}, {"text": "The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus.", "labels": [], "entities": []}, {"text": "The NMT models have shown to perform as well as the most widely used conventional translation systems.", "labels": [], "entities": []}, {"text": "Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8225023150444031}, {"text": "statistical machine translation", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.70255974928538}]}, {"text": "First, NMT requires a minimal set of domain knowledge.", "labels": [], "entities": []}, {"text": "For instance, all of the models proposed in), ( or do not assume any linguistic property in both source and target sentences except that they are sequences of words.", "labels": [], "entities": []}, {"text": "Second, the whole system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many separately trained features whose weights are then tuned jointly.", "labels": [], "entities": []}, {"text": "Lastly, the memory footprint of the NMT model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs.", "labels": [], "entities": []}, {"text": "Despite these advantages and promising results, there is a major limitation in NMT compared to the existing phrase-based approach.", "labels": [], "entities": []}, {"text": "That is, the number of target words must be limited.", "labels": [], "entities": []}, {"text": "This is mainly because the complexity of training and using an NMT model increases as the number of target words increases.", "labels": [], "entities": []}, {"text": "A usual practice is to construct a target vocabulary of the K most frequent words (a socalled shortlist), where K is often in the range of 30k () to 80k).", "labels": [], "entities": []}, {"text": "Any word not included in this vocabulary is mapped to a special token representing an unknown word.", "labels": [], "entities": []}, {"text": "This approach works well when there are only a few unknown words in the target sentence, but it has been observed 1 that the translation performance degrades rapidly as the number of unknown words increases (;.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approximate training algorithm based on (biased) importance sampling that allows us to train an NMT model with a much larger target vocabulary.", "labels": [], "entities": []}, {"text": "The proposed algorithm effectively keeps the computational complexity during training at the level of using only a small subset of the full vocabulary.", "labels": [], "entities": []}, {"text": "Once the model with a very large target vocabulary is trained, one can choose to use either all the target words or only a subset of them.", "labels": [], "entities": []}, {"text": "We compare the proposed algorithm against the baseline shortlist-based approach in the tasks of English\u2192French and English\u2192German translation using the NMT model introduced in.", "labels": [], "entities": [{"text": "English\u2192German translation", "start_pos": 115, "end_pos": 141, "type": "TASK", "confidence": 0.6569386497139931}]}, {"text": "The empirical results demonstrate that we can potentially achieve better translation performance using larger vocabularies, and that our approach does not sacrifice too much speed for both training and decoding.", "labels": [], "entities": [{"text": "translation", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.9601091146469116}]}, {"text": "Furthermore, we show that the model trained with this algorithm gets the best translation performance yet achieved by single NMT models on the WMT'14 English\u2192French translation task.", "labels": [], "entities": [{"text": "WMT'14 English\u2192French translation task", "start_pos": 143, "end_pos": 181, "type": "TASK", "confidence": 0.722090611855189}]}], "datasetContent": [{"text": "We evaluate the proposed approach in English\u2192French and English\u2192German translation tasks.", "labels": [], "entities": [{"text": "English\u2192German translation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6330509260296822}]}, {"text": "We trained the neural machine translation models using only the bilingual, parallel corpora made available as apart of WMT'14.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7363110582033793}, {"text": "WMT'14", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.9557693600654602}]}, {"text": "For each pair, the datasets we used are: English-French English- 93: Data coverage (in %) on target-side corpora for different vocabulary sizes.", "labels": [], "entities": [{"text": "coverage", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8496721386909485}]}, {"text": "\"All\" refers to all the tokens in the training set.", "labels": [], "entities": []}, {"text": "To ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in).", "labels": [], "entities": [{"text": "English\u2192French corpus", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.6254762634634972}]}, {"text": "As for English\u2192German, the corpus was preprocessed, in a manner similar to (, in order to remove many poorly translated sentences.", "labels": [], "entities": []}, {"text": "We evaluate the models on the WMT'14 test set (news-test 2014), 3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set).", "labels": [], "entities": [{"text": "WMT'14 test set", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9893954594930013}]}, {"text": "presents data coverage w.r.t. the vocabulary size, on the target side.", "labels": [], "entities": []}, {"text": "Unless mentioned otherwise, all reported BLEU scores () are computed with the multi-bleu.perl script 4 on the cased tokenized translations.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9764091968536377}]}], "tableCaptions": [{"text": " Table 1: Data coverage (in %) on target-side cor- pora for different vocabulary sizes. \"All\" refers to  all the tokens in the training set.", "labels": [], "entities": [{"text": "coverage", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8442452549934387}]}, {"text": " Table 2: The translation performances in BLEU obtained by different models on (a) English\u2192French and  (b) English\u2192German translation tasks. RNNsearch is the model proposed in (", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9595000743865967}, {"text": "English\u2192German translation tasks", "start_pos": 107, "end_pos": 139, "type": "TASK", "confidence": 0.7247483134269714}]}]}