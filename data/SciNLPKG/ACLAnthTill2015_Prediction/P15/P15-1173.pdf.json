{"title": [{"text": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes", "labels": [], "entities": [{"text": "Lexemes", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.8051506876945496}]}], "abstractContent": [{"text": "We present AutoExtend, a system to learn embeddings for synsets and lexemes.", "labels": [], "entities": []}, {"text": "It is flexible in that it can take any word embed-dings as input and does not need an additional training corpus.", "labels": [], "entities": []}, {"text": "The synset/lexeme embeddings obtained live in the same vector space as the word embeddings.", "labels": [], "entities": []}, {"text": "A sparse tensor formalization guarantees efficiency and parallelizability.", "labels": [], "entities": []}, {"text": "We use WordNet as a lexical resource, but Auto-Extend can be easily applied to other resources like Freebase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9443360567092896}]}, {"text": "AutoExtend achieves state-of-the-art performance on word similarity and word sense disam-biguation tasks.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7046933025121689}, {"text": "word sense disam-biguation tasks", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.7179800719022751}]}], "introductionContent": [{"text": "Unsupervised methods for word embeddings (also called \"distributed word representations\") have become popular in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 113, "end_pos": 146, "type": "TASK", "confidence": 0.7637402514616648}]}, {"text": "These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space.", "labels": [], "entities": []}, {"text": "Examples for word embeddings are SENNA, the hierarchical log-bilinear model), word2vec) and GloVe ().", "labels": [], "entities": []}, {"text": "However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9495129585266113}]}, {"text": "We will simply call these resources in the rest of the paper.", "labels": [], "entities": []}, {"text": "Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9335538744926453}]}, {"text": "A synset is a set of synonyms that are interchangeable in some context.", "labels": [], "entities": []}, {"text": "A lexeme pairs a particular spelling or pronunciation with a particular meaning, i.e., a lexeme is a conjunction of a word and a synset.", "labels": [], "entities": []}, {"text": "Our premise is that many NLP applications will benefit if the non-word data types of resources -e.g., synsets in WordNet -are also available as embeddings.", "labels": [], "entities": []}, {"text": "For example, in machine translation, enriching and improving translation dictionaries (cf.) would benefit from these embeddings because they would enable us to create an enriched dictionary for word senses.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7749553620815277}]}, {"text": "Generally, our premise is that the arguments for the utility of embeddings for word forms should carryover to the utility of embeddings for other data types like synsets in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 173, "end_pos": 180, "type": "DATASET", "confidence": 0.9445522427558899}]}, {"text": "The insight underlying the method we propose is that the constraints of a resource can be formalized as constraints on embeddings and then allow us to extend word embeddings to embeddings of other data types like synsets.", "labels": [], "entities": []}, {"text": "For example, the hyponymy relation in WordNet can be formalized as such a constraint.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9548152685165405}]}, {"text": "The advantage of our approach is that it decouples embedding learning from the extension of embeddings to non-word data types in a resource.", "labels": [], "entities": []}, {"text": "If somebody comes up with a better way of learning embeddings, these embeddings become immediately usable for resources.", "labels": [], "entities": []}, {"text": "And we do not rely on any specific properties of embeddings that make them usable in some resources, but not in others.", "labels": [], "entities": []}, {"text": "An alternative to our approach is to train embeddings on annotated text, e.g., to train synset embeddings on corpora annotated with synsets.", "labels": [], "entities": []}, {"text": "However, successful embedding learning generally requires very large corpora and sense labeling is too expensive to produce corpora of such a size.", "labels": [], "entities": [{"text": "sense labeling", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.6715893298387527}]}, {"text": "Another alternative to our approach is to add up all word embedding vectors related to a particular node in a resource; e.g., to create the synset vector of lawsuit in WordNet, we can add the word vectors of the three words that are part of the synset (lawsuit, suit, case).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 168, "end_pos": 175, "type": "DATASET", "confidence": 0.9570074081420898}]}, {"text": "We will call this approach naive and use it as a baseline (S naive in).", "labels": [], "entities": []}, {"text": "We will focus on WordNet) in this paper, but our method -based on a formalization that exploits the constraints of a resource for extending embeddings from words to other data types -is broadly applicable to other resources including Wikipedia and Freebase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.9276207685470581}]}, {"text": "A word in WordNet can be viewed as a composition of several lexemes.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.9353793263435364}]}, {"text": "Lexemes from different words together can form a synset.", "labels": [], "entities": []}, {"text": "When a synset is given, it can be decomposed into its lexemes.", "labels": [], "entities": []}, {"text": "And these lexemes then join to form words.", "labels": [], "entities": []}, {"text": "These observations are the basis for the formalization of the constraints encoded in WordNet that will be presented in the next section: we view words as the sum of their lexemes and, analogously, synsets as the sum of their lexemes.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9568994045257568}]}, {"text": "Another motivation for our formalization stems from the analogy calculus developed by, which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathematical group, in particular, associativity and invertibility.", "labels": [], "entities": []}, {"text": "For example, you can take the vector of king, subtract the vector of man and add the vector of woman to get a vector near queen.", "labels": [], "entities": []}, {"text": "In other words, you remove the properties of man and add the properties of woman.", "labels": [], "entities": []}, {"text": "We can also seethe vector of king as the sum of the vector of man and the vector of a gender-neutral ruler.", "labels": [], "entities": []}, {"text": "The next thing to notice is that this does not only work for words that combine several properties, but also for words that combine several senses.", "labels": [], "entities": []}, {"text": "The vector of suit can be seen as the sum of a vector representing lawsuit and a vector representing business suit.", "labels": [], "entities": []}, {"text": "AutoExtend is designed to take word vectors as input and unravel the word vectors to the vectors of their lexemes.", "labels": [], "entities": []}, {"text": "The lexeme vectors will then give us the synset vectors.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: (i) We present AutoExtend, a flexible method that extends word embeddings to embeddings of synsets and lexemes.", "labels": [], "entities": []}, {"text": "AutoExtend is completely general in that it can be used for any set of embeddings and for any resource that imposes constraints of a certain type on the relationship between words and other data types.", "labels": [], "entities": []}, {"text": "(ii) We show that AutoExtend achieves state-of-the-art word similarity and word sense disambiguation (WSD) performance.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.7027331242958704}]}, {"text": "(iii) We publish the AutoExtend code for extending word embeddings to other data types, the lexeme and synset embeddings and the software to replicate our WSD evaluation.", "labels": [], "entities": [{"text": "WSD evaluation", "start_pos": 155, "end_pos": 169, "type": "TASK", "confidence": 0.7530910074710846}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the model, first as a general tensor formulation then as a matrix formulation making additional assumptions.", "labels": [], "entities": []}, {"text": "In Section 3, we describe data, experiments and evaluation.", "labels": [], "entities": []}, {"text": "We analyze AutoExtend in Section 4 and give a short summary on how to extend our method to other resources in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 discusses related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of \u224810 11 tokens, using word2vec CBOW ().", "labels": [], "entities": []}, {"text": "Many words in the word2vec vocabulary are not in WordNet, e.g., inflected forms (cars) and proper nouns (Tony Blair).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9670470356941223}]}, {"text": "Conversely, many WordNet lemmas are not in the word2vec vocabulary, e.g., 42 (digits were converted to 0).", "labels": [], "entities": [{"text": "WordNet lemmas", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.9064954519271851}]}, {"text": "This results in a number of empty synsets (see).", "labels": [], "entities": []}, {"text": "Note however that AutoExtend can produce embeddings for empty synsets because we use WN relation constraints in addition to synset and lexeme constraints.", "labels": [], "entities": []}, {"text": "We run AutoExtend on the word2vec vectors.", "labels": [], "entities": [{"text": "AutoExtend", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8557674288749695}]}, {"text": "As we do not know anything about a suitable weighting for the three different constraints, we set \u03b1 = \u03b2 = 0.33.", "labels": [], "entities": []}, {"text": "Our main goal is to produce compatible embeddings for lexemes and synsets.", "labels": [], "entities": []}, {"text": "Thus, we can compute nearest neighbors across all three data types as shown in.", "labels": [], "entities": []}, {"text": "We evaluate the embeddings on WSD and on similarity performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.6805943846702576}]}, {"text": "Our results depend directly on the quality of the underlying word embeddings, in our case word2vec embeddings.", "labels": [], "entities": []}, {"text": "We would expect even better evaluation results as word representation learning methods improve.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8287164171536764}]}, {"text": "Using anew and improved set of underlying embeddings is simple: it is a simple switch of the input file that contains the word embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: # of WN relations by part-of-speech", "labels": [], "entities": [{"text": "WN relations", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.9303261637687683}]}, {"text": " Table 2: # of items in WordNet and after intersection with  word2vec vectors", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9697085022926331}]}, {"text": " Table 3: WSD accuracy for different feature sets and systems.  Best result (excluding line 16) in each column in bold.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9085721969604492}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9632609486579895}]}]}