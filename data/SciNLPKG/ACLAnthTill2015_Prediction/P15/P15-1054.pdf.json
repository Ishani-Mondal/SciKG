{"title": [{"text": "Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9554961919784546}]}], "abstractContent": [{"text": "We study the problem of summarizing DAG-structured topic hierarchies over a given set of documents.", "labels": [], "entities": [{"text": "summarizing DAG-structured topic hierarchies", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.8544298559427261}]}, {"text": "Example applications include automatically generating Wikipedia disambiguation pages fora set of articles, and generating candidate multi-labels for preparing machine learning datasets (e.g., for text classification, functional genomics, and image classification).", "labels": [], "entities": [{"text": "text classification", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.793766051530838}, {"text": "image classification", "start_pos": 242, "end_pos": 262, "type": "TASK", "confidence": 0.7168471366167068}]}, {"text": "Unlike previous work, which focuses on clustering the set of documents using the topic hierarchy as features, we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features.", "labels": [], "entities": []}, {"text": "Desirable properties of the chosen topics include document coverage, specificity, topic diversity, and topic homogeneity, each of which, we show, is naturally modeled by a submodular function.", "labels": [], "entities": []}, {"text": "Other information, provided say by unsupervised approaches such as LDA and its variants, can also be utilized by defining a submodular function that expresses coherence between the chosen topics and this information.", "labels": [], "entities": []}, {"text": "We use a large-margin framework to learn convex mixtures over the set of submodular components.", "labels": [], "entities": []}, {"text": "We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth.", "labels": [], "entities": [{"text": "automatically generating Wikipedia disambiguation pages", "start_pos": 53, "end_pos": 108, "type": "TASK", "confidence": 0.6050676286220551}]}, {"text": "We find that our framework improves upon several baselines according to a variety of standard evaluation metrics including the Jaccard Index, F1 score and NMI, and moreover, can be scaled to extremely large scale problems.", "labels": [], "entities": [{"text": "Jaccard Index", "start_pos": 127, "end_pos": 140, "type": "METRIC", "confidence": 0.7559486627578735}, {"text": "F1 score", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9907625913619995}, {"text": "NMI", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.7954026460647583}]}], "introductionContent": [{"text": "Several real world machine learning applications involve hierarchy based categorization of topics fora set of objects.", "labels": [], "entities": []}, {"text": "Objects could be, e.g., a set of documents for text classification, a set of genes in functional genomics, or a set of images in computer vision.", "labels": [], "entities": [{"text": "text classification", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7269594967365265}]}, {"text": "One can often define a natural topic hierarchy to categorize these objects.", "labels": [], "entities": []}, {"text": "For example, in text and image classification problems, each document or image is assigned a hierarchy of labels -a baseball page is assigned the labels \"baseball\" and \"sports.\"", "labels": [], "entities": [{"text": "text and image classification", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6805010214447975}]}, {"text": "Moreover, many of these applications, naturally have an existing topic hierarchy generated on the entire set of objects (;; ling Zhang and hua.", "labels": [], "entities": []}, {"text": "Given a DAG-structured topic hierarchy and a subset of objects, we investigate the problem of finding a subset of DAG-structured topics that are induced by that subset (of objects).", "labels": [], "entities": []}, {"text": "This problem arises naturally in several real world applications.", "labels": [], "entities": []}, {"text": "For example, consider the problem of identifying appropriate label sets fora collection of articles.", "labels": [], "entities": []}, {"text": "Several existing text collection datasets such as 20 Newsgroup 1 , Reuters-21578 2 work with a predefined set of topics.", "labels": [], "entities": [{"text": "20 Newsgroup 1", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9223426381746928}, {"text": "Reuters-21578", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.7890787720680237}]}, {"text": "We observe that these topic names are highly abstract for the articles categorized under them.", "labels": [], "entities": []}, {"text": "On the other hand, techniques proposed by systems such as Wikipedia Miner and TAGME generate several labels for each article in the dataset that are highly specific to the article.", "labels": [], "entities": [{"text": "Wikipedia Miner", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.9459868669509888}]}, {"text": "Collating all labels from all articles to create a label: Topic Summarization overview.", "labels": [], "entities": [{"text": "Topic Summarization", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7325906753540039}]}, {"text": "On the left, we show many documents related to Apple.", "labels": [], "entities": [{"text": "Apple", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9475976228713989}]}, {"text": "In the middle, a Wikipedia category hierarchy shown as a topic DAG, links these documents at the leaf level.", "labels": [], "entities": []}, {"text": "On the right, we show the output of our summarization process, which creates a set of summary topics (Plants, Technology, Companies, Films, Music and Places in this example) with the input documents classified under them.", "labels": [], "entities": []}, {"text": "set for the dataset can result in a large number of labels and become unmanageable.", "labels": [], "entities": []}, {"text": "Our proposed techniques can summarize such large sets of labels into a smaller and more meaningful label sets using a DAG-structured topic hierarchy.", "labels": [], "entities": []}, {"text": "This also holds for image classification problems and datasets like ImageNet ().", "labels": [], "entities": [{"text": "image classification", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7983908653259277}]}, {"text": "We use the term summarize to highlight the fact that the smaller label set semantically covers the larger label set.", "labels": [], "entities": []}, {"text": "For example, the topics Physics, Chemistry, and Mathematics can be summarized into a topic Science.", "labels": [], "entities": []}, {"text": "A particularly important application of our work (and the one we use for our evaluations in Section 4) is the following: Given a collection of articles spanning different topics, but with similar titles, automatically generate a disambiguation page for those titles using the Wikipedia category hierarchy 4 as a topic DAG.", "labels": [], "entities": []}, {"text": "Disambiguation pages on Wikipedia are used to resolve conflicts in article titles that occur when a title is naturally associated with multiple articles on distinct topics.", "labels": [], "entities": []}, {"text": "Each disambiguation page organizes articles into several groups, where the articles in each group pertain only to a specific topic.", "labels": [], "entities": []}, {"text": "Disambiguations maybe seen as paths in a hierarchy leading to different articles that arguably could have the same title.", "labels": [], "entities": []}, {"text": "For example, the title Apple 6 can refer to a plant, a company, a film, a television show, a place, a technology, an album, a record label, and a newspaper daily.", "labels": [], "entities": []}, {"text": "The problem then, is to organize the articles into multiple groups where each group contains articles of similar nature (topics) and has an appropriately discerned group heading.", "labels": [], "entities": []}, {"text": "describes the topic summarization process for creation of the disambiguation page for \"Apple\".", "labels": [], "entities": []}, {"text": "All the above mentioned problems can be modeled as the problem of finding the most representative subset of topic nodes from a DAG-Structured topic hierarchy.", "labels": [], "entities": []}, {"text": "We argue that many formulations of this problem are natural instances of submodular maximization, and provide a learning framework to create submodular mixtures to solve this problem.", "labels": [], "entities": []}, {"text": "A set function f (.) is said to be submodular if for any element v and sets A \u2286 B \u2286 V \\ {v}, where V represents the ground set of elements, . This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set.", "labels": [], "entities": []}, {"text": "Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (.", "labels": [], "entities": []}, {"text": "In this paper, we investigate structured prediction methods for learn-ing weighted mixtures of submodular functions to summarize topics fora collection of objects using DAG-structured topic hierarchies.", "labels": [], "entities": []}, {"text": "Throughout this paper we use the terms \"topic\" and \"category\" interchangeably.", "labels": [], "entities": []}], "datasetContent": [{"text": "To validate our approach, we make use of Wikipedia category structure as a topic DAG and apply our technique to the task of automatic generation of Wikipedia disambiguation pages.", "labels": [], "entities": [{"text": "automatic generation of Wikipedia disambiguation pages", "start_pos": 124, "end_pos": 178, "type": "TASK", "confidence": 0.7533882309993108}]}, {"text": "We pre-processed the category graph to eliminate the cycles in order to make it a DAG.", "labels": [], "entities": []}, {"text": "Each Wikipedia disambiguation page is manually created by Wikipedia editors by grouping a collection of Wikipedia articles into several groups.", "labels": [], "entities": []}, {"text": "Each group is then assigned a name, which serves as a topic for the group.", "labels": [], "entities": []}, {"text": "Typically, a disambiguation page segregates around 20-30 articles into 5-6 groups.", "labels": [], "entities": []}, {"text": "Our goal is to measure how accurately we can recreate the groups fora disambiguation page and label them, given only the collection of articles mentioned in that disambiguation page (when actual groupings and labels are hidden).", "labels": [], "entities": []}, {"text": "We parsed the contents of Wikipedia disambiguation pages and extracted disambiguation page names, article groups and group names.", "labels": [], "entities": [{"text": "parsed the contents of Wikipedia disambiguation pages", "start_pos": 3, "end_pos": 56, "type": "TASK", "confidence": 0.6392320777688708}]}, {"text": "We collected about 8000 disambiguation pages that had at least four groups on them.", "labels": [], "entities": []}, {"text": "Wikipedia category structure is used as the topic DAG.", "labels": [], "entities": []}, {"text": "We eliminated few administrative categories such as \"Hidden Categories\", \"Articles needing cleanup\", and the like.", "labels": [], "entities": []}, {"text": "The final DAG had about 1M topics and 3M links.", "labels": [], "entities": []}, {"text": "Every group of articles on the Wikipedia disambiguation page is assigned a name by the editors.", "labels": [], "entities": []}, {"text": "Unfortunately, these names may not correspond to the Wikipedia category (topic) names.", "labels": [], "entities": []}, {"text": "For example, one of the groups on the \"Matrix\" disambiguation page has a name \"Business and government\" and there is no Wikipedia category by that name.", "labels": [], "entities": []}, {"text": "However, the group names generated by our (and baseline) method are from the Wikipedia categories (which forms our topic DAG).", "labels": [], "entities": []}, {"text": "In addition, there can be multiple relevant names fora group.", "labels": [], "entities": []}, {"text": "For example, a group on a disambiguation page maybe called \"Calculus\", but an algorithm may rightly generate \"Vector Calculus\".", "labels": [], "entities": []}, {"text": "Hence we cannot evaluate the accuracy of an algorithm just by matching the generated group names to those on the disambiguation page.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9991186261177063}]}, {"text": "To alleviate this problem, we adopt cluster-based evaluation metrics.", "labels": [], "entities": []}, {"text": "We treat every group of articles generated by an algorithm under a topic fora disambiguation page as a cluster of articles.", "labels": [], "entities": []}, {"text": "These are considered as inferred clusters fora disambiguation page.", "labels": [], "entities": []}, {"text": "We compare them against the actual grouping of articles on the Wikipedia disambiguation page by treating those groups as true clusters.", "labels": [], "entities": [{"text": "Wikipedia disambiguation page", "start_pos": 63, "end_pos": 92, "type": "DATASET", "confidence": 0.8217020432154337}]}, {"text": "We can now adopt Jaccard Index, F1-measure, and NMI (Normalized Mutual Information) based cluster evaluation metrics described in).", "labels": [], "entities": [{"text": "Jaccard Index", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.6393860131502151}, {"text": "F1-measure", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9958449006080627}, {"text": "NMI", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.8676362037658691}]}, {"text": "For each disambiguation page in the test set, we compute every metric score and then average it overall the disambiguation pages.", "labels": [], "entities": []}, {"text": "We show that the submodular mixture learning and maximization approaches, i.e., SMML cov and SMML cov+sim outperform other approaches in various metrics.", "labels": [], "entities": []}, {"text": "In all these experiments, we performed 5 fold cross validation to learn the parameters from 80% of the disambiguation pages and evaluated on the rest of the 20%, in each fold.", "labels": [], "entities": []}, {"text": "In we summarize the results of the comparison of the methods mentioned above on Jaccard Index, F1 measure and NMI.", "labels": [], "entities": [{"text": "Jaccard Index", "start_pos": 80, "end_pos": 93, "type": "METRIC", "confidence": 0.53919817507267}, {"text": "F1 measure", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9876196980476379}, {"text": "NMI", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.5777930617332458}]}, {"text": "Our proposed techniques SMML cov and SMML cov+sim outperform other techniques consistently.", "labels": [], "entities": []}, {"text": "In we measure the number of test instances (i.e., disambiguation queries) in which each of the algorithms dominate (win) in evaluation metrics.", "labels": [], "entities": []}, {"text": "In 60% of the disambiguation queries, SMML cov and SMML cov+sim approaches functions (SMML cov+sim ), but at a greatly reduced execution time, demonstrating the sufficiency of O (n) functions for our task.", "labels": [], "entities": []}, {"text": "On the average, for each disambiguation query, SMML cov took around 40 seconds (over 1M topics and 3M edges DAG) to infer the topics, whereas SMML cov+sim took around 35 minutes.", "labels": [], "entities": []}, {"text": "Both these experiments were carried on a machine with 32 GB RAM, Eight-Core AMD Opteron(tm) Processor 2427.", "labels": [], "entities": []}], "tableCaptions": []}