{"title": [{"text": "Deep Markov Neural Network for Sequential Data Classification", "labels": [], "entities": [{"text": "Sequential Data Classification", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.8416800697644552}]}], "abstractContent": [{"text": "We present a general framework for incorporating sequential data and arbitrary features into language modeling.", "labels": [], "entities": []}, {"text": "The general framework consists of two parts: a hidden Markov component and a recursive neural network component.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues.", "labels": [], "entities": [{"text": "predicting topics and sentiments in dialogues", "start_pos": 88, "end_pos": 133, "type": "TASK", "confidence": 0.8783363699913025}]}, {"text": "Experiments on real data demonstrate that our method is substantially more accurate than previous methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Processing sequential data is a significant research challenge for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6416723628838857}]}, {"text": "In the past decades, numerous studies have been conducted on modeling sequential data.", "labels": [], "entities": []}, {"text": "Hidden Markov Models (HMMs) and its variants are representative statistical models of sequential data for the purposes of classification, segmentation, and clustering.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 138, "end_pos": 150, "type": "TASK", "confidence": 0.7782912254333496}]}, {"text": "For most aforementioned methods, only the dependencies between consecutive hidden states are modeled.", "labels": [], "entities": []}, {"text": "In natural language processing, however, we find there are dependencies locally and at a distance.", "labels": [], "entities": []}, {"text": "Conservatively using the most recent history to perform prediction yields overfitting to short-term trends and missing important long-term effects.", "labels": [], "entities": []}, {"text": "Thus, it is crucial to explore in depth to capture long-term temporal dynamics in language use.", "labels": [], "entities": []}, {"text": "Numerous real world learning problems are best characterized by interactions between multiple causes or factors.", "labels": [], "entities": []}, {"text": "Taking sentiment analysis for dialogues as an example, the topic of the document and the author's identity are both valuable for mining user's opinions in the conversation.", "labels": [], "entities": []}, {"text": "Specifically, each participant in the dialogue usually has specific sentiment polarities towards different topics.", "labels": [], "entities": []}, {"text": "However, most existing sequential data modeling methods are not capable of incorporating the information from both the topic and the author's identity.", "labels": [], "entities": []}, {"text": "More generally, there is no sufficiently flexible sequential model that allows incorporating an arbitrary set of features.", "labels": [], "entities": []}, {"text": "In this paper, we present a Deep Markov Neural Network (DMNN) for incorporating sequential data and arbitrary features into language modeling.", "labels": [], "entities": []}, {"text": "Our method learns from general sequential observations.", "labels": [], "entities": []}, {"text": "It is also capable of taking the ordering of words into account, and collecting information from arbitrary features associated with the context.", "labels": [], "entities": []}, {"text": "Comparing to traditional HMM-based method, it explores deeply into the structure of sentences, and is more flexible in taking external features into account.", "labels": [], "entities": []}, {"text": "On the other hand, it doesn't suffer from the training difficulties of recurrent neural networks, such as the vanishing gradient problem.", "labels": [], "entities": []}, {"text": "The general framework consists of two parts: a hidden Markov component and a neural network component.", "labels": [], "entities": []}, {"text": "In the training phase, the hidden Markov model is trained on the sequential observation, resulting in transition probabilities and hidden states at each time step.", "labels": [], "entities": []}, {"text": "Then, the neural network is trained, taking words, features and hidden state at the previous time step as input, to predict the hidden states at the present time step.", "labels": [], "entities": []}, {"text": "The procedure is reversed in the testing phase: the neural network predicts the hidden states using words and features, then the hidden Markov model predicts the observation using hidden states.", "labels": [], "entities": []}, {"text": "A key insight of our method is to use hidden states as an intermediate representation, as abridge to connect sentences and observations.", "labels": [], "entities": []}, {"text": "By using hidden states, we can deal with arbitrary observation, without worrying about the issue of discretization and normalization.", "labels": [], "entities": []}, {"text": "Hidden states are robust with respect to the random noise in the observation.", "labels": [], "entities": []}, {"text": "Unlike recurrent neural net-32 work which connects networks between consecutive time steps, the recursive neural network in our framework connects to the previous time step by using its hidden states.", "labels": [], "entities": []}, {"text": "In the training phase, since hidden states are inferred by the hidden Markov model, the training of recursive neural networks at each time step can be performed separately, preventing the difficulty of learning an extremely deep neural network.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues.", "labels": [], "entities": [{"text": "predicting topics and sentiments in dialogues", "start_pos": 88, "end_pos": 133, "type": "TASK", "confidence": 0.8783363699913025}]}, {"text": "In this example, the sequential observation includes topics and sentiments.", "labels": [], "entities": []}, {"text": "The feature includes the identity of the author.", "labels": [], "entities": []}, {"text": "Experiments on real data demonstrate that our method is substantially more accurate than previous methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our model, we conduct experiments for sentiment analysis in conversations.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9335439503192902}]}, {"text": "We conduct experiments on both English and Chinese datasets.", "labels": [], "entities": [{"text": "English and Chinese datasets", "start_pos": 31, "end_pos": 59, "type": "DATASET", "confidence": 0.641285814344883}]}, {"text": "The detailed properties of the datasets are described as follow.", "labels": [], "entities": []}, {"text": "Twitter conversation (Twitter): The original dataset is a collection of about 1.3 million conversations drawn from Twitter by.", "labels": [], "entities": []}, {"text": "Each conversation contains between 2 and 243 posts.", "labels": [], "entities": []}, {"text": "In our experiments, we filter the data by keeping only the conversations of five or more tweets.", "labels": [], "entities": []}, {"text": "This results in 64,068 conversations containing 542,866 tweets.", "labels": [], "entities": []}, {"text": "Sina Weibo conversation (Sina): since there is no authoritative publicly available Chinese shorttext conversation corpus, we write a web crawler to grab tweets from Sina Weibo, which is the most popular Twitter-like microblogging website in China . Following the strategy used in, we crawled Sina Weibo fora 3 months period from September 2013 to November 2013.", "labels": [], "entities": []}, {"text": "Filtering the conversations that contain less than five posts, we get a Chinese conversation corpus with 5,921 conversations containing 37,282 tweets.", "labels": [], "entities": [{"text": "Chinese conversation corpus", "start_pos": 72, "end_pos": 99, "type": "DATASET", "confidence": 0.7953741351763407}]}, {"text": "For both datasets, we set the ground truth of sentiment classification of tweets by using human annotation.", "labels": [], "entities": [{"text": "sentiment classification of tweets", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.8916590809822083}]}, {"text": "Specifically, we randomly select 1000 conversations from each datasets, and then invite three researchers who work on natural language processing to label sentiment tag of each tweet (i.e., positive, negative or neutral) manually.", "labels": [], "entities": []}, {"text": "From 3 responses for each tweet, we measure the agreement as the number of people who submitted the same response.", "labels": [], "entities": []}, {"text": "We measure the performance of our framework using the tweets that satisfy at least 2 out of 3 agreement.", "labels": [], "entities": []}, {"text": "For both datasets, data preprocessing is performed.", "labels": [], "entities": []}, {"text": "The words about time, numeral words, pronoun and punctuation are removed as they are unrelated to the sentiment analysis task.: Three-way classification accuracy  In our HMMs component, the number of hidden states is 80.", "labels": [], "entities": [{"text": "sentiment analysis task.", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.9217989643414816}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.8565349578857422}]}, {"text": "We randomly initialize the matrix of state transition probabilities and the initial state distribution between 0 and 1.", "labels": [], "entities": []}, {"text": "The emission probabilities are determined by Gaussian distributions.", "labels": [], "entities": []}, {"text": "In our recursive autoencoders component, we represent each words using 100-dimensional vectors.", "labels": [], "entities": []}, {"text": "The hyperparameter used for weighing reconstruction and cross-entropy error is 0.1.", "labels": [], "entities": [{"text": "weighing reconstruction", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.9751392900943756}]}, {"text": "For each dataset, we use 800 conversations as the training data and the remaining are used for testing.", "labels": [], "entities": []}, {"text": "We summarize the experiment results in, the proposed approach significantly and consistently outperforms other methods on both datasets.", "labels": [], "entities": []}, {"text": "This verifies the effectiveness of the proposed approach.", "labels": [], "entities": []}, {"text": "For example, the overall accuracy of our algorithm is 3.2% higher than Mesnil's method and 11.0% higher than SVM on Twitter conversations dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9996552467346191}, {"text": "Twitter conversations dataset", "start_pos": 116, "end_pos": 145, "type": "DATASET", "confidence": 0.695273776849111}]}, {"text": "For the Sina Weibo dataset, we observe similar results.", "labels": [], "entities": [{"text": "Sina Weibo dataset", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.9267858266830444}]}, {"text": "The advantage of our model comes from its capability of exploring sequential information and incorporating an arbitrary number of factors of the corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Three-way classification accuracy", "labels": [], "entities": [{"text": "Three-way classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8020871877670288}]}]}