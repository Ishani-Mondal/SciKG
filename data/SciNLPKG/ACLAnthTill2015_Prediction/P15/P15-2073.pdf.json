{"title": [{"text": "\u2206BLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets", "labels": [], "entities": [{"text": "BLEU", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.9839341044425964}]}], "abstractContent": [{"text": "We introduce Discriminative BLEU (\u2206BLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs.", "labels": [], "entities": [{"text": "BLEU (\u2206BLEU)", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.7382503151893616}]}, {"text": "Reference strings are scored for quality by human raters on a scale of [\u22121, +1] to weight multi-reference BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9907525777816772}]}, {"text": "In tasks involving generation of conversational responses, \u2206BLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman's \u03c1 and Kendall's \u03c4 .", "labels": [], "entities": [{"text": "generation of conversational responses", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.8121020793914795}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.5022058486938477}, {"text": "IBM BLEU", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.8482174873352051}]}], "introductionContent": [{"text": "Many natural language processing tasks involve the generation of texts where a variety of outputs are acceptable or even desirable.", "labels": [], "entities": []}, {"text": "Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7550306916236877}, {"text": "summarization", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9770893454551697}, {"text": "sentence compression", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7367056161165237}, {"text": "paraphrase generation", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7738097012042999}]}, {"text": "A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run anew human evaluation every time anew model is built or parameters are modified.", "labels": [], "entities": []}, {"text": "In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU () and METEOR () Although BLEU is not immune from criticism (e.g.,), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8792013625303904}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9982478618621826}, {"text": "METEOR", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9941146969795227}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9891334176063538}, {"text": "BLEU", "start_pos": 233, "end_pos": 237, "type": "METRIC", "confidence": 0.9790109395980835}]}, {"text": "\u2020 Corresponding author: mgalley@microsoft.com) in SMT, and it has allowed the field to proceed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9108799695968628}]}, {"text": "BLEU has been less successfully applied to non-SMT generation tasks owing to the larger space of plausible outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9844322204589844}]}, {"text": "As a result, attempts have been made to adapt the metric.", "labels": [], "entities": []}, {"text": "To foster diversity in paraphrase generation, propose a metric called iBLEU in which the BLEU score is discounted by a BLEU score computed between the source and paraphrase.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.9281444847583771}, {"text": "BLEU score", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.977469801902771}, {"text": "BLEU score", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9715335965156555}]}, {"text": "This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase.", "labels": [], "entities": []}, {"text": "In image captioning tasks,, employ a variant of BLEU in which n-grams are weighted by tf \u00b7idf.", "labels": [], "entities": [{"text": "image captioning tasks", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.8214548428853353}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9977566599845886}]}, {"text": "This assumes the availability of a corpus with which to compute tf \u00b7idf.", "labels": [], "entities": []}, {"text": "Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9720421433448792}]}, {"text": "In this paper, we introduce Discriminative BLEU (\u2206BLEU), anew metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference BLEU.", "labels": [], "entities": [{"text": "Discriminative BLEU (\u2206BLEU)", "start_pos": 28, "end_pos": 55, "type": "METRIC", "confidence": 0.7466776490211486}]}, {"text": "In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized.", "labels": [], "entities": []}, {"text": "Our testbed for this metric is data-driven conversation, afield that has begun to attract interest ( as an alternative to conventional rule-driven or scripted dialog systems.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs.", "labels": [], "entities": [{"text": "Intrinsic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8575514256954193}]}, {"text": "Below, we describe \u2206BLEU and investigate its characteristics in comparison to standard BLEU in the context of conversational response generation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8952199220657349}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9885249733924866}, {"text": "conversational response generation", "start_pos": 110, "end_pos": 144, "type": "TASK", "confidence": 0.765108565489451}]}, {"text": "We demonstrate that \u2206BLEU correlates well with human evaluation scores in this task and thus can provide a basis for automated training and evaluation of data-driven conversation systems-and, we ultimately believe, other text generation tasks with inherently diverse targets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9130421876907349}, {"text": "text generation", "start_pos": 221, "end_pos": 236, "type": "TASK", "confidence": 0.7066407054662704}]}], "datasetContent": [{"text": "To create the multi-reference BLEU dev and test sets used in this study, we adapted and extended the methodology of.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9709752798080444}]}, {"text": "From a corpus of 29M Twitter context-message-response conversational triples, we randomly extracted approxi-: Sample reference sets created by our multi-reference extraction algorithm, along with the weights used in \u2206BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9744307398796082}]}, {"text": "Triples from which additional references are extracted are in italics.", "labels": [], "entities": []}, {"text": "Boxed sentences are in our multi-reference dev set.", "labels": [], "entities": []}, {"text": "mately 33K candidate triples that were then judged for conversational quality on a 5-point Likert-type scale by 3 crowdsourced annotators.", "labels": [], "entities": []}, {"text": "Of these, 4232 triples scored an average 4 or higher; these were randomly binned to create seed dev and test sets of 2118 triples and 2114 triples respectively.", "labels": [], "entities": []}, {"text": "Note that the dev set is not used in the experiments of this paper, since \u2206BLEU and IBM BLEU are metrics that do not require training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9806632399559021}, {"text": "IBM", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.8305372595787048}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.7484351396560669}]}, {"text": "However, the dev set is released along with a test set in the dataset release accompanying this paper.", "labels": [], "entities": []}, {"text": "We then sought to identify candidate triples in the 29M corpus for which both message and response are similar to the original messages and responses in these seed sets.", "labels": [], "entities": [{"text": "29M corpus", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9542550146579742}]}, {"text": "To this end, we employed an information retrieval algorithm with a bag-of-words BM25 similarity function (, as detailed in, to extract the top 15 responses for each messageresponse pair.", "labels": [], "entities": [{"text": "BM25 similarity function", "start_pos": 80, "end_pos": 104, "type": "METRIC", "confidence": 0.8190306226412455}]}, {"text": "Unlike, we further appended the original messages (as if parroted back).", "labels": [], "entities": []}, {"text": "The new triples were then scored for quality of the response in light of both context and message by 5 crowdsourced raters each on a 5-point Likert-type scale.", "labels": [], "entities": []}, {"text": "Crucially, and again in contradistinction to, we did not impose a score cutoff on these synthetic multireference sets.", "labels": [], "entities": []}, {"text": "Instead, we retained all candidate responses and scaled their scores into.", "labels": [], "entities": []}, {"text": "presents representative multi-reference examples (from the dev set) together with their converted scores.", "labels": [], "entities": []}, {"text": "The context and messages associated with the supplementary mined responses are also shown for illustrative purposes to demonstrate the range of conversations from which they were taken.", "labels": [], "entities": []}, {"text": "In the table, negative-weighted mined responses are semantically orthogonal to the intent of their newly assigned context and message.", "labels": [], "entities": []}, {"text": "Strongly negatively weighted responses are completely out of the ballpark (\"the weather in Russia is very cool\", \"well then!", "labels": [], "entities": []}, {"text": "Why were the biscuits needed?\"); others area little more plausible, but irrelevant or possibly topic changing (\"ohh I love that song\").", "labels": [], "entities": []}, {"text": "Higher-valued positive-weighted mined responses are typically reasonably appropriate and relevant (even though extracted from a completely unrelated conversation), and in some cases can outscore the original response, as can be seen in the third set of examples.", "labels": [], "entities": []}, {"text": "Responses generated by the 7 systems used in this study on the 2114-triple test set were hand evaluated by 5 crowdsourced raters each on a 5-point Likert-type scale.", "labels": [], "entities": [{"text": "2114-triple test set", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.8730769554773966}]}, {"text": "From these 7 systems, 12 system pairs were evaluated, fora total of about pairwise 126K ratings.", "labels": [], "entities": []}, {"text": "Here too, raters were asked to evaluate responses in terms of their relevance to both context and message.", "labels": [], "entities": []}, {"text": "Outputs from different systems were randomly interleaved for presentation to the raters.", "labels": [], "entities": []}, {"text": "We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to), whose weights have been manually tuned.", "labels": [], "entities": [{"text": "Phrase-based MT", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.46346379816532135}]}, {"text": "We also included four variants of that system, which we tuned with MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.6906489133834839}]}, {"text": "These variants differ in their number of features, and augment () with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (.", "labels": [], "entities": [{"text": "Jaccard index", "start_pos": 168, "end_pos": 181, "type": "METRIC", "confidence": 0.908792644739151}, {"text": "length ratio", "start_pos": 196, "end_pos": 208, "type": "METRIC", "confidence": 0.8942390084266663}, {"text": "DSSM score", "start_pos": 214, "end_pos": 224, "type": "METRIC", "confidence": 0.6825957894325256}]}, {"text": "RNN-based MT: the log-probability according to the RNN model of (.", "labels": [], "entities": [{"text": "RNN-based MT", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.5736416578292847}]}, {"text": "Baseline: a random baseline.", "labels": [], "entities": []}, {"text": "While \u2206BLEU relies on human qualitative judgments, it is important to note that human judgments on multi-references ( \u00a7 4.1) and those on system outputs were collected completely independently.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9276799559593201}]}, {"text": "We also note that the set of systems listed above specifically does not include a retrieval-based model, as this might have introduced spurious correlation between the two datasets ( \u00a7 4.1 and \u00a7 4.2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Human correlations for IBM BLEU, sentence-level  BLEU, and \u2206BLEU with 95% confidence intervals. This  compares 3 types of references: single only, high scoring  references (w \u2265 0.6), and all references.", "labels": [], "entities": [{"text": "IBM", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.6049731373786926}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.8703867197036743}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.7518686056137085}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.5813984870910645}]}]}