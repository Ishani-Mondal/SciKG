{"title": [{"text": "Evaluating Machine Translation Systems with Second Language Proficiency Tests", "labels": [], "entities": [{"text": "Evaluating Machine Translation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7869042158126831}]}], "abstractContent": [{"text": "A lightweight, human-in-the-loop evaluation scheme for machine translation (MT) systems is proposed.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8484948456287384}]}, {"text": "It extrinsically evaluates MT systems using human subjects' scores on second language ability test problems that are machine-translated to the subjects' native language.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9954498410224915}]}, {"text": "A large-scale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.960106611251831}]}, {"text": "An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation metrics, such as the BLEU score (), were crucial ingredients for the advances of machine translation technology in the last decade.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9856120944023132}, {"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8005562424659729}]}, {"text": "Meanwhile, the shortcomings of BLEU and similar n-gram proximitybased metrics have been pointed out by many authors including.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9951321482658386}]}, {"text": "The main criticisms include: 1) unreliability in evaluating short translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 179, "end_pos": 181, "type": "TASK", "confidence": 0.9314349293708801}]}, {"text": "Manual evaluation of translation quality is more reliable in many regards, but it is costly.", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9511234164237976}]}, {"text": "Furthermore, it is not necessarily easy to analyze the characteristics of MT systems based solely on the evaluation results such as a 5-point scale evaluation of adequacy/fluency and a ranking of the outputs of different systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9867317080497742}]}, {"text": "A remedy for some of the above-raised issues is task-based evaluation of MT systems), which measures the human performance in a task such as information extraction from a machine-translated text.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9478346705436707}, {"text": "information extraction from a machine-translated text", "start_pos": 141, "end_pos": 194, "type": "TASK", "confidence": 0.8529072999954224}]}, {"text": "The main burden of conducting task-based evaluation is also its cost; the development of a sizable amount of test materials and the gathering of appropriate human subjects is time consuming and expensive.", "labels": [], "entities": []}, {"text": "This paper proposes to utilize second-language proficiency tests (SLPTs), such as TOEIC, as the source of the specimens for extrinsic evaluation of MT systems.", "labels": [], "entities": [{"text": "TOEIC", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.749207079410553}, {"text": "MT", "start_pos": 148, "end_pos": 150, "type": "TASK", "confidence": 0.9695148468017578}]}, {"text": "For evaluating, e.g., English-toJapanese MT systems, a set of English test problems is translated by the systems and the translation qualities are evaluated by the test scores achieved by native Japanese speakers on the translated problems.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.8113968968391418}]}, {"text": "In many languages, a large collection of SLPT problems is available.", "labels": [], "entities": []}, {"text": "More than 130 standardized tests for 32 languages are listed in the English Wikipedia page for 'List of language proficiency tests' as of April 30, 2015.", "labels": [], "entities": []}, {"text": "They are carefully designed to evaluate various aspects of language ability with objective criteria.", "labels": [], "entities": []}, {"text": "We can thus obtain an easy-to-use test set that focuses on a certain aspect of MT system performance by appropriately choosing the problem types and levels.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9941135048866272}]}, {"text": "Moreover, SLPTs are primarily designed to assess the testtakers' language ability but not their general intelligence.", "labels": [], "entities": []}, {"text": "Hence, as evidenced later in the paper, the proposed scheme is expected to be robust against the heterogeneity of the subjects, as long as they are native speakers of the target language.", "labels": [], "entities": []}, {"text": "This is a desirable property for conducting a large-scale experiment, possibly with crowdsourcing.", "labels": [], "entities": []}, {"text": "In the current paper, we utilize atypical format of multiple-choice dialogue completion problems ().", "labels": [], "entities": [{"text": "multiple-choice dialogue completion", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.6274137794971466}]}, {"text": "The subjects are given a machine-  translated conversation and asked to choose an appropriate utterance from several options, which are also machine-translated, to fill in a blank in the conversation.", "labels": [], "entities": []}, {"text": "We evaluated four translation methods in the experiment including both machine-translation and manual-translation.", "labels": [], "entities": []}, {"text": "The extrinsic evaluation revealed that one of the MT systems is comparable to the human translation produced by randomly presenting the individual sentences to the translator without any context, but the translation produced by the best MT system is still far worse than that produced by a human translator working on the entire dialogue at once.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9835519194602966}]}, {"text": "Furthermore, we examined the relations between the extrinsic metric based on the subjects' scores and various intrinsic metrics including automatic scores such as the BLEU score and manual evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 167, "end_pos": 177, "type": "METRIC", "confidence": 0.982032835483551}]}, {"text": "The test material is available on request for research purposes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extrinsically evaluated four different translations of the same material, namely multiple-choice dialogue completion problems taken from second language ability tests.", "labels": [], "entities": [{"text": "multiple-choice dialogue completion", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.6053067843119303}]}, {"text": "The original problems were in English, and we translated them into Japanese.", "labels": [], "entities": []}, {"text": "Two of the translations were produced by MT systems, and the other two were produced by a human translator with and without considering the contexts of the individual sentences in the dialogues.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.7648188471794128}]}, {"text": "The human subjects solved the translated problems without knowing whether a machine or a human produced them.", "labels": [], "entities": []}, {"text": "Finally, the translation quality was evaluated based on the rate of correct answers given by the human subjects.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.96736741065979}]}, {"text": "The translation systems were evaluated by the average of the rate of correct answers made on the translated problems.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9653729200363159}]}, {"text": "Let P = {p i } be the set of original problems and M (p) be the translation of problem p produced by method M . The correct answer rate (CAR) on M (p) is defined as: CAR M (p) = # of subjects that correctly answered M(p) # of subjects who solved M(p) . The extrinsic evaluation score of translation method M is the average of CAR over P :  Automatic Evaluation Metrics We also evaluated the translation quality using BLEU, BLEU+1 (  binary relations.", "labels": [], "entities": [{"text": "correct answer rate (CAR)", "start_pos": 116, "end_pos": 141, "type": "METRIC", "confidence": 0.8516632417837778}, {"text": "BLEU", "start_pos": 417, "end_pos": 421, "type": "METRIC", "confidence": 0.9981441497802734}, {"text": "BLEU+1", "start_pos": 423, "end_pos": 429, "type": "METRIC", "confidence": 0.9462898373603821}]}, {"text": "For each relation \"A > B\" found in the broken-down relations, one point was added to system A.", "labels": [], "entities": []}, {"text": "The final ranking among the systems fora problem was determined by the total points.", "labels": [], "entities": []}, {"text": "We first present the system-level evaluation results for the four translation methods.", "labels": [], "entities": []}, {"text": "shows the min/max and the quartiles of the correct answer rates (CARs) for the 40 problems translated by each system.", "labels": [], "entities": [{"text": "correct answer rates (CARs)", "start_pos": 43, "end_pos": 70, "type": "METRIC", "confidence": 0.7798007527987162}]}, {"text": "lists the five automatic evaluation scores for each translation method measured against the two reference translation sets.", "labels": [], "entities": []}, {"text": "The averages of the CARs over the 40 problems are also listed in the bottom row of the table.", "labels": [], "entities": [{"text": "CARs", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.733128011226654}]}, {"text": "There are several noticeable facts.", "labels": [], "entities": []}, {"text": "First, despite the significantly better average CAR for Y over G, BLEU, BLEU+1, and TER prefer G to Y . Second, while the average CARs for Y and HS are almost equal, there are large differences between their automatic evaluation scores across all metrics.", "labels": [], "entities": [{"text": "CAR", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.945273756980896}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9980176687240601}, {"text": "BLEU+1", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9679229259490967}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9979526996612549}]}, {"text": "Third, a comparison of the corresponding automatic evaluation scores using Ref Sand Ref O reveals that G, Y , and HS are more similar to the manual translations that were produced without referring to the contexts of the individual sentences than those produced taking the contexts into consideration.", "labels": [], "entities": []}, {"text": "However, the large difference in the correct answer rates for HS and H O suggests that ignorance of the context in the current MT systems severely degrades the comprehensibility of the translations of texts like daily conversations.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9804964661598206}]}, {"text": "We examined how often an intrinsic metric correctly predicts the difference of the subjects' test performance on a problem.", "labels": [], "entities": []}, {"text": "Specifically, for two translation methods A and B, we say an intrinsic metric M agrees with the CAR by the subjects on problem pi iff metric M scores A's translation of and the CAR is higher on A(p i ) than on B(p i ).", "labels": [], "entities": []}, {"text": "The rate of agreements is the fraction of the problems on which M and CAR agree.", "labels": [], "entities": []}, {"text": "The agreement between two intrinsic metrics is defined similarly.", "labels": [], "entities": []}, {"text": "shows the rates of agreements between the automatic metrics and CARs and between the human evaluation and CARs.", "labels": [], "entities": []}, {"text": "As shows, all the agreement rates between the automatic metrics and CARs were less than 0.65.", "labels": [], "entities": [{"text": "agreement", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9871553182601929}, {"text": "CARs", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.7156472206115723}]}, {"text": "When considering a random baseline of 0.5, we may conclude that the automatic metrics are not very good predictors of the CARs.", "labels": [], "entities": []}, {"text": "This is unfortunate since the CARs directly indicate the comprehensibility of the translated dialogues.", "labels": [], "entities": []}, {"text": "The disagreements cannot be attributed only to the unreliability of automatic metrics on short translations.", "labels": [], "entities": []}, {"text": "shows the rate of agreements between the automatic metrics and the human evaluation.", "labels": [], "entities": []}, {"text": "shows, BLEU, BLEU+1, and TER agree with human evaluation on nearly 90% of the problems when comparing Y and HS . The human evaluation agrees with the CAR slightly better than the automatic metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9989780187606812}, {"text": "BLEU+1", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9674442013104757}, {"text": "TER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9992687106132507}]}, {"text": "However, the agreement rates are still less than 0.7 for all pairs of compared systems.", "labels": [], "entities": [{"text": "agreement", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9469752907752991}]}, {"text": "These findings suggest that there is an inherent discrepancy between the assessment of the overall translation quality of a problem and the CAR.", "labels": [], "entities": []}, {"text": "It is presumably because the CAR can be critically damaged by a subtle translation mistake that spoils a coherent understanding of a dialogue.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic Evaluation Scores and Aver- age Correct Answer Rate", "labels": [], "entities": [{"text": "Automatic Evaluation Scores", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.8594512542088827}, {"text": "Aver- age Correct Answer Rate", "start_pos": 42, "end_pos": 71, "type": "METRIC", "confidence": 0.9109643499056498}]}]}