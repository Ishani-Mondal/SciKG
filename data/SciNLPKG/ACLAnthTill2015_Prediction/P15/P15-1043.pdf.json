{"title": [{"text": "Content Models for Survey Generation: A Factoid-Based Evaluation", "labels": [], "entities": [{"text": "Survey Generation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8818914890289307}]}], "abstractContent": [{"text": "We present anew factoid-annotated dataset for evaluating content models for scientific survey article generation containing 3,425 sentences from 7 topics in natural language processing.", "labels": [], "entities": [{"text": "scientific survey article generation", "start_pos": 76, "end_pos": 112, "type": "TASK", "confidence": 0.6341220736503601}]}, {"text": "We also introduce a novel HITS-based content model for automated survey article generation called HITSUM that exploits the lexical network structure between sentences from citing and cited papers.", "labels": [], "entities": [{"text": "automated survey article generation", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.6857836544513702}]}, {"text": "Using the factoid-annotated data, we conduct a pyramid evaluation and compare HITSUM with two previous state-of-the-art content models: C-Lexrank, a network based content model, and TOPICSUM, a Bayesian content model.", "labels": [], "entities": [{"text": "TOPICSUM", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.763481855392456}]}, {"text": "Our experiments show that our new content model captures useful survey-worthy information and outper-forms C-Lexrank by 4% and TOPICSUM by 7% in pyramid evaluation.", "labels": [], "entities": [{"text": "TOPICSUM", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.885988175868988}]}], "introductionContent": [{"text": "Survey article generation is the task of automatically building informative surveys for scientific topics.", "labels": [], "entities": [{"text": "Survey article generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6184642811616262}]}, {"text": "Given the rapid growth of publications in scientific fields, the development of such systems is crucial as human-written surveys exist fora limited number of topics and get outdated quickly.", "labels": [], "entities": []}, {"text": "In this paper, we investigate content models for extracting survey-worthy information from scientific papers.", "labels": [], "entities": [{"text": "extracting survey-worthy information from scientific papers", "start_pos": 49, "end_pos": 108, "type": "TASK", "confidence": 0.8311681747436523}]}, {"text": "Such models are an essential component of any system for automatic survey article generation.", "labels": [], "entities": [{"text": "automatic survey article generation", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.5807223692536354}]}, {"text": "Earlier work in the area of survey article generation has investigated content models based on lexical networks (.", "labels": [], "entities": [{"text": "article generation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6907263398170471}]}, {"text": "These models take as input citing sentences that describe important papers on the topic and assign them a salience score based on centrality in a lexical network formed by the input citing sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluating our content models, we generated 2,000-character-long summaries using each of the systems (Lexrank, C-Lexrank, HITSUM, and TOPICSUM) for each of the topics.", "labels": [], "entities": [{"text": "Lexrank", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9333338737487793}, {"text": "HITSUM", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.757701575756073}, {"text": "TOPICSUM", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9398096203804016}]}, {"text": "The summaries are generated by ranking the input sentences using each content model and picking the top sentences till the budget of 2,000 characters is reached.", "labels": [], "entities": []}, {"text": "Each of these summaries is then given a pyramid score () computed using the factoids assigned to each sentence.", "labels": [], "entities": []}, {"text": "For the pyramid evaluation, the factoids are organized in a pyramid of order n.", "labels": [], "entities": []}, {"text": "The top tier in this pyramid contains the highest weighted factoids, the next tier contains the second highest weighted factoids, and soon.", "labels": [], "entities": []}, {"text": "The score assigned to a summary is the ratio of the sum of the weights of the factoids it contains to the sum of weights of an optimal summary with the same number of factoids.", "labels": [], "entities": []}, {"text": "Pyramid evaluation allows us to capture how each content model performs in terms of selecting sentences with the most highly weighted factoids.", "labels": [], "entities": []}, {"text": "Since the factoids have been extracted from human-written surveys and tutorials on each of the topics, the pyramid score gives us an idea of the survey-worthiness of the sentences selected by Question classification is a crucial component of modern question answering system.", "labels": [], "entities": [{"text": "Question classification", "start_pos": 192, "end_pos": 215, "type": "TASK", "confidence": 0.7648102939128876}, {"text": "question answering", "start_pos": 249, "end_pos": 267, "type": "TASK", "confidence": 0.8905147910118103}]}, {"text": "A what-type question is defined as the one whose question word is 'what', 'which', 'name' or 'list'.", "labels": [], "entities": []}, {"text": "This metaclassifier beats all published numbers on standard question classification benchmarks.", "labels": [], "entities": [{"text": "question classification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7527660727500916}]}, {"text": "Due to its challenge, this paper focuses on what-type question classification.", "labels": [], "entities": [{"text": "what-type question classification", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.8729263146718343}]}, {"text": "In this paper, we focus on fine-category classification.", "labels": [], "entities": [{"text": "fine-category classification", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8271169066429138}]}, {"text": "The promise of a machine learning approach is that the QA system builder can now focus on designing features and providing labeled data, rather than coding and maintaining complex heuristic rule bases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: List of seven NLP topics used in our ex- periments along with input size.", "labels": [], "entities": []}, {"text": " Table 3: Sample input sentences from the topic of word sense disambiguation annotated with factoids.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7459160387516022}]}, {"text": " Table 4: Fractional distribution of factoids across  various categories in citing sentences vs introduc- tion sentences.", "labels": [], "entities": []}, {"text": " Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .", "labels": [], "entities": []}]}