{"title": [{"text": "Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network", "labels": [], "entities": []}], "abstractContent": [{"text": "Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6902897755304972}]}, {"text": "The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key component named composition function.", "labels": [], "entities": []}, {"text": "Although a variety of composition functions have been proposed, the syntactic information has not been fully encoded in the composition process.", "labels": [], "entities": []}, {"text": "We propose two models, Tag Guided RNN (TG-RNN for short) which chooses a composition function according to the part-of-speech tag of a phrase, and Tag Embedded RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings together.", "labels": [], "entities": []}, {"text": "In the fine-grained sentiment classification, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counterparts .", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.7901492118835449}]}], "introductionContent": [{"text": "Among a variety of deep learning models for natural language processing, Recursive Neural Network (RNN) maybe one of the most popular models.", "labels": [], "entities": []}, {"text": "Thanks to the compositional nature of natural text, recursive neural network utilizes the recursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including semantic relationship classification, syntactic parsing (), sentiment analysis (, and machine translation (.", "labels": [], "entities": [{"text": "semantic relationship classification", "start_pos": 241, "end_pos": 277, "type": "TASK", "confidence": 0.6768875022729238}, {"text": "syntactic parsing", "start_pos": 279, "end_pos": 296, "type": "TASK", "confidence": 0.7865248322486877}, {"text": "sentiment analysis", "start_pos": 301, "end_pos": 319, "type": "TASK", "confidence": 0.9553914070129395}, {"text": "machine translation", "start_pos": 327, "end_pos": 346, "type": "TASK", "confidence": 0.7931657433509827}]}, {"text": "The key component of RNN and its variants is the composition function: how to compose the vector representation fora longer text from the vector of its child words or phrases.", "labels": [], "entities": []}, {"text": "For instance, as shown in, the vector of 'is very interesting' can be composed from the vector of the left node 'is' and that of the right node 'very interesting'.", "labels": [], "entities": []}, {"text": "It's worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learning models.   is composed from the vectors of node 'very' and node 'interesting'.", "labels": [], "entities": []}, {"text": "Similarly, the node 'is very interesting' is composed from the phrase node 'very interesting' and the word node 'is' . There are various attempts to design the composition function in RNN (or related models).", "labels": [], "entities": []}, {"text": "In RNN), a global matrix is used to linearly combine the elements of vectors.", "labels": [], "entities": []}, {"text": "In RNTN (), a global tensor is used to compute the tensor products of dimensions to favor the association between different el-ements of the vectors.", "labels": [], "entities": []}, {"text": "Sometimes it is challenging to find a single function to model the composition process.", "labels": [], "entities": []}, {"text": "As an alternative, multiple composition functions can be used.", "labels": [], "entities": []}, {"text": "For instance, in MV-RNN (), different matrices is designed for different words though the model is suffered from too much parameters.", "labels": [], "entities": []}, {"text": "In AdaMC RNN/RNTN (), a fixed number of composition functions is linearly combined and the weight for each function is adaptively learned.", "labels": [], "entities": []}, {"text": "In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models.", "labels": [], "entities": []}, {"text": "Two ideas are motivated by the example shown in: First, the composition function for the noun phrase 'the movie/NP' should be different from that for the adjective phrase 'very interesting/ADJP' since the two phrases are quite syntactically different.", "labels": [], "entities": []}, {"text": "More specifically to sentiment analysis, a noun phrase is much less likely to express sentiment than an adjective phrase.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9607599675655365}]}, {"text": "There are two notable works mentioned here:) presented to combine the parsing and composition processes, but the purpose is for parsing;) designed composition functions according to the combinatory rules and categories in CCG grammar, however, only marginal improvement against Naive Bayes was reported.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9631065726280212}, {"text": "parsing", "start_pos": 128, "end_pos": 135, "type": "TASK", "confidence": 0.9679213166236877}]}, {"text": "Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent phrase to guide the composition process from the child nodes.", "labels": [], "entities": []}, {"text": "As an example, we design a function for composing noun phrase (NP) and another one for adjective phrase (ADJP).", "labels": [], "entities": []}, {"text": "This simple strategy obtains remarkable improvements against strong baselines.", "labels": [], "entities": []}, {"text": "Second, when composing the adjective phrase 'very interesting/ADJP' from the left node 'very/RB' and the right node 'interesting/JJ', the right node is obviously more important than the left one.", "labels": [], "entities": []}, {"text": "Furthermore, the right node 'interesting/JJ' apparently contributes more to sentiment expression.", "labels": [], "entities": [{"text": "sentiment expression", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.9231694042682648}]}, {"text": "To address this issue, we propose, to learn an embedding vector for each word/phrase tag, and concatenate the tag vector with the word/phrase vector as input to the composition function.", "labels": [], "entities": []}, {"text": "For instance, we have tag vectors for DT,NN,RB,JJ,ADJP,NP, etc. and the tag vectors are then used in composing the parent's vector.", "labels": [], "entities": []}, {"text": "The proposed TE-RNTN obtain the second best result among all the top performing models but with much less parameters and complexity.", "labels": [], "entities": [{"text": "TE-RNTN", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.5656659603118896}]}, {"text": "To the best of our knowledge, this is the first time that tag embedding is proposed.", "labels": [], "entities": []}, {"text": "To summarize, the contributions of our work are as follows: \u2022 We propose tag-guided composition functions in recursive neural network, TG-RNN.", "labels": [], "entities": []}, {"text": "Tag-guided RNN allocates a composition function fora phrase according to the partof-speech tag of the phrase.", "labels": [], "entities": []}, {"text": "\u2022 We propose to learn embedding vectors for part-of-speech tags of words/phrases, and integrate the tag embeddings in RNN and RNTN respectively.", "labels": [], "entities": []}, {"text": "The two models, TE-RNN and TE-RNTN, can leverage the syntactic information of child nodes when generating the vector of parent nodes.", "labels": [], "entities": []}, {"text": "\u2022 The proposed models are efficient and effective.", "labels": [], "entities": []}, {"text": "The scale of the parameters is well controlled.", "labels": [], "entities": []}, {"text": "Experimental results on the Stanford Sentiment Treebank corpus show the effectiveness of the models.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank corpus", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.9206036031246185}]}, {"text": "TE-RNTN obtains the second best result among all publicly reported approaches, but with much less parameters and complexity.", "labels": [], "entities": [{"text": "TE-RNTN", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5067878365516663}]}, {"text": "The rest of the paper is structured as follows: in Section 2, we survey related work.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce the traditional recursive neural network as background.", "labels": [], "entities": []}, {"text": "We present our ideas in Section 4.", "labels": [], "entities": []}, {"text": "The experiments are introduced in Section 5.", "labels": [], "entities": []}, {"text": "We summarize the work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on Stanford Sentiment Treebank which contains fully labeled parse trees.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 26, "end_pos": 53, "type": "DATASET", "confidence": 0.9369899233182272}]}, {"text": "It is built upon 10,662 reviews and each sentence has sentiment labels on each node in the parse tree.", "labels": [], "entities": []}, {"text": "The sentiment label set is {0,1,2,3,4}, where the numbers mean very negative, negative, neutral, positive, and very positive, respectively.", "labels": [], "entities": []}, {"text": "We use standard split (train: 8,544 dev: 1,101, test: 2,210) on the corpus in our experiments.", "labels": [], "entities": []}, {"text": "In addition, we add the part-of-speech tag for each leaf node and phrase-type tag for each interior node using the latest version of Stanford Parser.", "labels": [], "entities": []}, {"text": "Because the newer parser generated trees different from those provided in the datasets, 74/11/11 reviews in train/dev/test datasets are ignored.", "labels": [], "entities": [{"text": "74/11/11 reviews in train/dev/test datasets", "start_pos": 88, "end_pos": 131, "type": "DATASET", "confidence": 0.6978472975584177}]}, {"text": "After removing the broken reviews, our dataset contains 10566 reviews (train: 8,470, dev: 1,090, test: 2,199).", "labels": [], "entities": []}, {"text": "The word vectors were pre-trained on an unlabeled corpus (about 100,000 movie reviews) by) as initial values and the other vectors is initialized by sampling from a uniform distribution U(\u2212\u03f5, \u03f5) where \u03f5 is 0.01 in our experiments.", "labels": [], "entities": []}, {"text": "The dimension of word vectors is 25 for RNN models and 20 for RNTN models.", "labels": [], "entities": []}, {"text": "Tanh is chosen as the nonlinearity function.", "labels": [], "entities": []}, {"text": "And after computing the output of node i with vi = f (g(v l i , v r i )), we set vi = vi ||v i || so that the resulting vector has a limited norm.", "labels": [], "entities": []}, {"text": "Backpropagation algorithm) is used to compute gradients and we use minibatch SGD with momentum as the optimization method, implemented with Theano ().", "labels": [], "entities": []}, {"text": "We trained all our models using stochastic gradient descent with a batch size of 30 examples, momentum of 0.9, L 2 -regularization weight of 0.0001 and a constant learning rate of 0.005.", "labels": [], "entities": [{"text": "momentum", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9615666270256042}, {"text": "L 2 -regularization weight", "start_pos": 111, "end_pos": 137, "type": "METRIC", "confidence": 0.9486675143241883}]}], "tableCaptions": [{"text": " Table 1: Classification accuray. Fine-grained  stands for 5-class prediction and Pos./Neg. means  binary prediction which ignores all neutral in- stances. All the accuracy is at the sentence level  (root).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9992927312850952}]}]}