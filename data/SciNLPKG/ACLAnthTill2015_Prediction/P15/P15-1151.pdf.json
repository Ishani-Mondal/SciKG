{"title": [{"text": "A Convolutional Architecture for Word Sequence Prediction", "labels": [], "entities": [{"text": "Word Sequence Prediction", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.7976496815681458}]}], "abstractContent": [{"text": "We propose a convolutional neural network , named genCNN, for word sequence prediction.", "labels": [], "entities": [{"text": "word sequence prediction", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.7723507682482401}]}, {"text": "Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector.", "labels": [], "entities": [{"text": "neural network-based language modeling and generation", "start_pos": 32, "end_pos": 85, "type": "TASK", "confidence": 0.759597177306811}]}, {"text": "Instead , we use a convolutional neural network to predict the next word with the history of words of variable length.", "labels": [], "entities": []}, {"text": "Also different from the existing feed-forward networks for language mod-eling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task.", "labels": [], "entities": []}, {"text": "We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies.", "labels": [], "entities": []}, {"text": "Our model is fast, easy to train, and readily parallelized.", "labels": [], "entities": []}, {"text": "Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-of-the-arts with big margins.", "labels": [], "entities": [{"text": "text generation", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.811004102230072}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.710451066493988}]}], "introductionContent": [{"text": "Both language modeling () and text generation) boil down to modeling the conditional probability of a word given the proceeding words.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.730985015630722}, {"text": "text generation", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7864652872085571}]}, {"text": "Previously, it is mostly done through purely memory-based approaches, such as n-grams, which cannot deal with long sequences and has to use some heuristics (called smoothing) for rare ones.", "labels": [], "entities": []}, {"text": "Another family of methods are based on distributed representations of words, which is usually tied with a neural-network (NN) architecture for estimating the conditional probabilities of words.", "labels": [], "entities": []}, {"text": "Two categories of neural networks have been used for language modeling: 1) recurrent neural networks (RNN), and 2) feedfoward network (FFN): \u2022 The RNN-based models, including its variants like LSTM, enjoy more popularity, mainly due to their flexible structures for processing word sequences of arbitrary lengths, and their recent empirical success.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.6967249065637589}]}, {"text": "We however argue that RNNs, with their power built on the recursive use of a relatively simple computation units, are forced to make greedy summarization of the history and consequently not efficient on modeling word sequences, which clearly have a bottom-up structures.", "labels": [], "entities": []}, {"text": "\u2022 The FFN-based models, on the other hand, avoid this difficulty by feeding directly on the history.", "labels": [], "entities": []}, {"text": "However, the FFNs are built on fully-connected networks, rendering them inefficient on capturing local structures of languages.", "labels": [], "entities": [{"text": "FFNs", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.8912202715873718}]}, {"text": "Moreover their \"rigid\" architectures make it futile to handle the great variety of patterns in long range correlations of words.", "labels": [], "entities": []}, {"text": "We propose a novel convolutional architecture, named genCNN, as a model that can efficiently combine local and long range structures of language for the purpose of modeling conditional probabilities.", "labels": [], "entities": []}, {"text": "genCNN can be directly used in generating a word sequence (i.e., text generation) or evaluating the likelihood of word sequences (i.e., language modeling).", "labels": [], "entities": [{"text": "text generation", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7050974071025848}, {"text": "language modeling", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.690776601433754}]}, {"text": "We also show the empirical superiority of genCNN on both tasks over traditional n-grams and its RNN or FFN counterparts.", "labels": [], "entities": []}, {"text": "Notations: We will use V to denote the vocabulary, e t (\u2208 {1, \u00b7 \u00b7 \u00b7 , |V|}) to denote the t th word in a sequence e 1:t , and e (n) t if the sequence is further indexed by n.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we randomly generate sentences by recurrently sampling e t+1 \u223c p(e t+1 |e 1:t ; \u00af \u0398), and put the newly generated word into history, until EOS (end-of-sentence) is generated.", "labels": [], "entities": [{"text": "EOS", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.9158201813697815}]}, {"text": "We consider generating two types of sentences: 1) the plain sentences, and 2) sentences with dependency parsing, which will be covered respectively in Section 5.1 and 5.2.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.6926674544811249}]}, {"text": "We evaluate our model as a language model in terms of both perplexity and its efficacy in re-ranking the n-best candidates from state-of-the-art models in statistical machine translation, with comparison to the following competitor language models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 155, "end_pos": 186, "type": "TASK", "confidence": 0.6552173097928365}]}, {"text": "Competitor Models we compare genCNN to the following competitor models \u2022 5-gram: We use SRI Language Modeling Toolkit () to train a 5-gram language model with modified Kneser-Ney smoothing; \u2022 FFN-LM: The neural language model based on feedfoward network ().", "labels": [], "entities": [{"text": "SRI Language Modeling", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.5948002139727274}]}, {"text": "We vary the input window-size from 5 to 20, while the performance stops increasing after window size 20; \u2022 RNN: we use the implementation 1 of RNN-based language model with hidden size 600; \u2022 LSTM: we adopt the code in Groundhog 2 , but vary the hyper-parameters, including the depth and word-embedding dimension, for best performance.", "labels": [], "entities": []}, {"text": "LSTM) is widely considered to be the state-of-the-art for sequence modeling.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.8733923137187958}]}], "tableCaptions": [{"text": " Table 1: Examples of sentences generated by genCNN. In the upper block (row 1-4) the underline  words are given by the human; In the middle block (row 5-8), all the sentences are generated  without any hint. The bottom block (row 9-12) shows the sentences with dependency tag generated  by genCNN trained with parsed examples.", "labels": [], "entities": []}, {"text": " Table 3:  FBIS results. The upper block  (row 1-6) compares genCNN and the competi- tor models, and the bottom block (row 7-9)  compares different variants of genCNN.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.6848710179328918}]}, {"text": " Table 4: The results for re-ranking the 1000- best of Moses. Note that the two bottom rows  are on a baseline with enhanced LM.", "labels": [], "entities": [{"text": "1000- best of Moses", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.7573906183242798}, {"text": "LM", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9911933541297913}]}]}