{"title": [{"text": "Scalable Large-Margin Structured Learning: Theory and Algorithms", "labels": [], "entities": []}], "abstractContent": [{"text": "Much of NLP tries to map structured input (sen-tences) to some form of structured output (tag sequences , parse trees, semantic graphs, or trans-lated/paraphrased/compressed sentences).", "labels": [], "entities": []}, {"text": "Thus structured prediction and its learning algorithm are of central importance to us NLP researchers.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.7789587080478668}]}, {"text": "However, when applying machine learning to structured domains, we often face scalability issues for two reasons: 1.", "labels": [], "entities": []}, {"text": "Even the fastest exact search algorithms for most NLP problems (such as parsing and translation) is too slow for repeated use on the training data, but approximate search (such as beam search) unfortunately breaks down the nice theoretical properties (such as convergence) of existing machine learning algorithms.", "labels": [], "entities": [{"text": "parsing and translation", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.6029417912165324}]}, {"text": "2. Even with inexact search, the scale of the training data in NLP still makes pure online learning (such as perceptron and MIRA) too slow on a single CPU.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.7030561566352844}]}, {"text": "This tutorial reviews recent advances that address these two challenges.", "labels": [], "entities": []}, {"text": "In particular, we will cover principled machine learning methods that are designed to work under vastly inexact search, and parallelization algorithms that speedup learning on multiple CPUs.", "labels": [], "entities": []}, {"text": "We will also extend struc-tured learning to the latent variable setting, wherein many NLP applications such as translation and semantic parsing the gold-standard derivation is hidden.", "labels": [], "entities": [{"text": "translation", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.9821211695671082}, {"text": "semantic parsing", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.6783749014139175}]}, {"text": "Overview of Structured Learning (a) key challenge 1: search efficiency (b) key challenge 2: interactions between search and learning 2.", "labels": [], "entities": []}, {"text": "Structured Perceptron (a) the basic algorithm (b) convergence proof-a purely geometric approach (updated in 2015) (c) voted and averaged perceptrons, and efficient implementation tricks (d) applications in tagging, parsing, etc.", "labels": [], "entities": [{"text": "parsing", "start_pos": 215, "end_pos": 222, "type": "TASK", "confidence": 0.7037070393562317}]}, {"text": "(e) inseparability and generalization bounds (new in 2015) 3.", "labels": [], "entities": []}, {"text": "Structured Perceptron under Inexact Search (a) convergence theory breaks under inexact search (b) early update (c) violation-fixing perceptron (d) applications in tagging, parsing, etc.-coffee break-4.", "labels": [], "entities": [{"text": "tagging, parsing", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.6265707810719808}]}, {"text": "Large-Margin Structured Learning with Latent Variables (a) examples: machine translation, semantic parsing, transliteration (b) separability condition and convergence proof (updated in 2015) (c) latent-variable perceptron under inexact search (d) applications in machine translation 5.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.843053549528122}, {"text": "semantic parsing", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7308237850666046}, {"text": "machine translation", "start_pos": 263, "end_pos": 282, "type": "TASK", "confidence": 0.775610476732254}]}, {"text": "Parallelizing Large-Margin Structured Learning (a) iterative parameter mixing (IPM) (b) minibatch perceptron and MIRA 19", "labels": [], "entities": [{"text": "MIRA", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.5323917865753174}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}