{"title": [{"text": "A Hassle-Free Unsupervised Domain Adaptation Method Using Instance Similarity Features", "labels": [], "entities": [{"text": "Hassle-Free Unsupervised Domain Adaptation", "start_pos": 2, "end_pos": 44, "type": "TASK", "confidence": 0.8606905043125153}]}], "abstractContent": [{"text": "We present a simple yet effective unsu-pervised domain adaptation method that can be generally applied for different NLP tasks.", "labels": [], "entities": []}, {"text": "Our method uses unlabeled target domain instances to induce a set of instance similarity features.", "labels": [], "entities": []}, {"text": "These features are then combined with the original features to represent labeled source domain instances.", "labels": [], "entities": []}, {"text": "Using three NLP tasks, we show that our method consistently out-performs a few baselines, including SCL, an existing general unsupervised domain adaptation method widely used in NLP.", "labels": [], "entities": []}, {"text": "More importantly, our method is very easy to implement and incurs much less computational cost than SCL.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain adaptation aims to use labeled data from a source domain to help build a system fora target domain, possibly with a small amount of labeled data from the target domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7968068420886993}]}, {"text": "The problem arises when the target domain has a different data distribution from the source domain, which is often the case.", "labels": [], "entities": []}, {"text": "In NLP, domain adaptation has been well studied in recent years.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7554737329483032}]}, {"text": "Existing work has proposed both techniques designed for specific NLP tasks) and general approaches applicable to different tasks (.", "labels": [], "entities": []}, {"text": "With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods) have also been adopted for NLP tasks.", "labels": [], "entities": []}, {"text": "There are generally two settings of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7486943900585175}]}, {"text": "We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we call it unsupervised domain adaptation.", "labels": [], "entities": []}, {"text": "Although many domain adaptation methods have been proposed, for practitioners who wish to avoid implementing or tuning sophisticated or computationally expensive methods due to either lack of enough machine learning background or limited resources, simple approaches are often more attractive.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7388188540935516}]}, {"text": "A notable example is the frustratingly easy domain adaptation method proposed by, which simply augments the feature space by duplicating features in a clever way.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7469427287578583}]}, {"text": "However, this method is only suitable for supervised domain adaptation.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6515027185281118}]}, {"text": "A later semi-supervised version of this easy adaptation method uses unlabeled data from the target domain, but it still requires some labeled data from the target domain.", "labels": [], "entities": []}, {"text": "In this paper, we propose a general unsupervised domain adaptation method that is almost equally hasslefree but does not use any labeled target data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7470326721668243}]}, {"text": "Our method uses a set of unlabeled target instances to induce anew feature space, which is then combined with the original feature space.", "labels": [], "entities": []}, {"text": "We explain analytically why the new feature space may help domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7596913278102875}]}, {"text": "Using a few different NLP tasks, we then empirically show that our method can indeed learn a better classifier for the target domain than a few baselines.", "labels": [], "entities": []}, {"text": "In particular, our method performs consistently better than or competitively with Structural Correspondence Learning (SCL)), a wellknown unsupervised domain adaptation method in NLP.", "labels": [], "entities": []}, {"text": "Furthermore, compared with SCL and other advanced methods such as the marginalized structured dropout method) and a recent feature embedding method, our method is much easier to implement.", "labels": [], "entities": []}, {"text": "In summary, our main contribution is a simple, effective and theoretically justifiable unsupervised domain adaptation method for NLP problems.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7258670330047607}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Three errors of different feature representations on", "labels": [], "entities": []}, {"text": " Table 2: Comparison of performance on three NLP tasks. For each source-target pair of each task, the performance shown", "labels": [], "entities": []}, {"text": " Table 3: Comparison between ISF and PCA.", "labels": [], "entities": []}]}