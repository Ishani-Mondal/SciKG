{"title": [{"text": "Document Level Time-anchoring for TimeLine Extraction", "labels": [], "entities": [{"text": "TimeLine Extraction", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7875813543796539}]}], "abstractContent": [{"text": "This paper investigates the contribution of document level processing of time-anchors for TimeLine event extraction.", "labels": [], "entities": [{"text": "TimeLine event extraction", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.7389445304870605}]}, {"text": "We developed and tested two different systems.", "labels": [], "entities": []}, {"text": "The first one is a baseline system that captures explicit time-anchors.", "labels": [], "entities": []}, {"text": "The second one extends the baseline system by also capturing implicit time relations.", "labels": [], "entities": []}, {"text": "We have evaluated both approaches in the SemEval 2015 task 4 TimeLine: Cross-Document Event Ordering.", "labels": [], "entities": [{"text": "SemEval 2015 task 4 TimeLine", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7962509393692017}, {"text": "Cross-Document Event Ordering", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.713712751865387}]}, {"text": "We empirically demonstrate that the document-based approach obtains a much more complete time anchoring.", "labels": [], "entities": []}, {"text": "Moreover, this approach almost doubles the performance of the systems that participated in the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Temporal relation extraction has been the topic of different SemEval tasks () and other challenges as the 6th i2b2 NLP Challenge ().", "labels": [], "entities": [{"text": "Temporal relation extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8912255167961121}, {"text": "SemEval tasks", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.8802374303340912}]}, {"text": "These tasks focused mainly on the temporal relations of the events with respect to other events or time expressions, and their goals are to discover which of them occur before, after or simultaneously to others.", "labels": [], "entities": []}, {"text": "Recently, SemEval 2015 included a novel task regarding temporal information extraction.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6474518676598867}]}, {"text": "The aim of SemEval 2015 task 4 is to order in a TimeLine the events in which a target entity is involved and presents some significant differences with respect to previous exercises.", "labels": [], "entities": [{"text": "SemEval 2015 task 4", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.9134997576475143}]}, {"text": "First, the temporal information must be recovered from different sources in a cross-document way.", "labels": [], "entities": []}, {"text": "Second, the TimeLines are focused on the events involving just a given entity.", "labels": [], "entities": []}, {"text": "Finally, unlike previous challenges, SemEval 2015 task 4 requires a quite complete time anchoring.", "labels": [], "entities": [{"text": "SemEval 2015 task 4", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.9290352463722229}, {"text": "time anchoring", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7262686789035797}]}, {"text": "This work focuses mainly on this latter point.", "labels": [], "entities": []}, {"text": "We show that the temporal relations that explicitly connect events and time expressions are not enough to obtain a full time-anchor annotation and, consequently, produce incomplete TimeLines.", "labels": [], "entities": []}, {"text": "We propose that fora complete time-anchoring the temporal analysis must be performed at a document level in order to discover implicit temporal relations.", "labels": [], "entities": []}, {"text": "We present a preliminary approach that obtains, by far, the best results on the main track of SemEval 2015 task 4.", "labels": [], "entities": [{"text": "SemEval 2015 task 4", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8107078820466995}]}], "datasetContent": [{"text": "We have evaluated our two TimeLine extractors on the main track of the SemEval 2015 task 4.", "labels": [], "entities": [{"text": "TimeLine extractors", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6324345469474792}, {"text": "SemEval 2015 task 4", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.80417600274086}]}, {"text": "Two systems participated in this track, WHUNLP and SPINOZAVU, with three runs in total.", "labels": [], "entities": [{"text": "WHUNLP", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.6788407564163208}]}, {"text": "Their performances in terms of Precision (P), Recall (R) and F1-score (F1) are presented in.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9577519297599792}, {"text": "Recall (R)", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9665775448083878}, {"text": "F1-score (F1)", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.9175580441951752}]}, {"text": "We also present in italics additional results of both systems.", "labels": [], "entities": []}, {"text": "On the one hand, the results of a corrected run of the WHUNLP system provided by the SemEval organizers.", "labels": [], "entities": [{"text": "WHUNLP system", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.7467192709445953}, {"text": "SemEval organizers", "start_pos": 85, "end_pos": 103, "type": "DATASET", "confidence": 0.7196892499923706}]}, {"text": "On the other hand, the results of an out of the competition version of the SPINOZAVU team explained in.", "labels": [], "entities": [{"text": "SPINOZAVU team", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.7144761681556702}]}, {"text": "The best run is obtained by the corrected version of WHUNLP 1 with an F1 of 7.85%.", "labels": [], "entities": [{"text": "WHUNLP 1", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8723287582397461}, {"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9991874098777771}]}, {"text": "The low figures obtained show the intrinsic difficulty of the task, specially in terms of Recall.", "labels": [], "entities": [{"text": "Recall", "start_pos": 90, "end_pos": 96, "type": "TASK", "confidence": 0.714949905872345}]}, {"text": "also contains the results obtained by our systems.", "labels": [], "entities": []}, {"text": "We present two different runs.", "labels": [], "entities": []}, {"text": "On the one hand, we present the results obtained using just the explicit time-anchors provided by BTE.", "labels": [], "entities": [{"text": "BTE", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.9483446478843689}]}, {"text": "As it can be seen, the results obtained by this run are similar to those obtained by WHUNLP 1.", "labels": [], "entities": [{"text": "WHUNLP 1", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.8789456784725189}]}, {"text": "On the other hand, the results of the implicit time-anchoring approach (DLT) outperforms by far our baseline and all previous systems applied to the task.", "labels": [], "entities": []}, {"text": "To check that these results are not biased by the time-relation extractor we use in our pipeline (TimePro), we reproduce the performances of BTE and DLT using another system to obtain the time-relations.", "labels": [], "entities": [{"text": "BTE", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.6848927140235901}]}, {"text": "For this purpose we have used CAEVO by).", "labels": [], "entities": [{"text": "CAEVO", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9290401339530945}]}, {"text": "The results obtained in this case show that the improvement obtained by our proposal is quite similar, regardless of the time-relation extractor chosen.", "labels": [], "entities": []}, {"text": "The figures in seem to prove our hypothesis.", "labels": [], "entities": []}, {"text": "In order to obtain a full time-anchoring annotation, the temporal analysis must be carried out at a document level.", "labels": [], "entities": []}, {"text": "The TimeLine extractor almost doubles the performance by just including a straightforward strategy as the one described in Section 5.", "labels": [], "entities": [{"text": "TimeLine extractor", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.5993461310863495}]}, {"text": "As expected, shows that this improvement is much more significant in terms of Recall.", "labels": [], "entities": [{"text": "Recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9676184058189392}]}], "tableCaptions": [{"text": " Table 1: Results on the SemEval-2015 task", "labels": [], "entities": [{"text": "SemEval-2015 task", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.7702459394931793}]}]}