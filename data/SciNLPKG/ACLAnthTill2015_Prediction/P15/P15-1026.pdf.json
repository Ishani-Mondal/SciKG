{"title": [{"text": "Question Answering over Freebase with Multi-Column Convolutional Neural Networks", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8055310547351837}]}], "abstractContent": [{"text": "Answering natural language questions over a knowledge base is an important and challenging task.", "labels": [], "entities": [{"text": "Answering natural language questions", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8943924754858017}]}, {"text": "Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking.", "labels": [], "entities": [{"text": "question understanding", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7799805998802185}, {"text": "answer ranking", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.8480436503887177}]}, {"text": "In this paper , we introduce multi-column convolu-tional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context , and answer type) and learn their distributed representations.", "labels": [], "entities": []}, {"text": "Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base.", "labels": [], "entities": []}, {"text": "Question-answer pairs are used to train the model to rank candidate answers.", "labels": [], "entities": []}, {"text": "We also leverage question paraphrases to train the column networks in a multi-task learning manner.", "labels": [], "entities": []}, {"text": "We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset.", "labels": [], "entities": [{"text": "FREEBASE", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.7048271894454956}, {"text": "WEBQUESTIONS dataset", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.8921374082565308}]}, {"text": "Experimental results show that our method achieves better or comparable performance compared with baseline systems.", "labels": [], "entities": []}, {"text": "In addition, we develop a method to compute the salience scores of question words in different column networks.", "labels": [], "entities": []}, {"text": "The results help us intuitively understand what MCCNNs learn.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic question answering systems return the direct and exact answers to natural language questions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7703632712364197}]}, {"text": "In recent years, the development of largescale knowledge bases, such as FREEBASE (, provides a rich resource to answer open-domain questions.", "labels": [], "entities": [{"text": "FREEBASE", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.505593478679657}]}, {"text": "However, how * Contribution during internship at Microsoft Research.", "labels": [], "entities": []}, {"text": "to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging.", "labels": [], "entities": []}, {"text": "Up to now, there are two mainstream methods for this task.", "labels": [], "entities": []}, {"text": "The first one is based on semantic parsing) and the other relies on information extraction over the structured knowledge base).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7411324977874756}, {"text": "information extraction", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7077532559633255}]}, {"text": "The semantic parsers learn to understand natural language questions by converting them into logical forms.", "labels": [], "entities": []}, {"text": "Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers.", "labels": [], "entities": []}, {"text": "Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals ( to reduce annotation costs.", "labels": [], "entities": []}, {"text": "However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability.", "labels": [], "entities": []}, {"text": "In addition, they need to manually design features for semantic parsers.", "labels": [], "entities": []}, {"text": "The second approach uses information extraction techniques for open question answering.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7933673560619354}, {"text": "open question answering", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.6286042630672455}]}, {"text": "These methods retrieve a set of candidate answers from the knowledge base, and the extract features for the question and these candidates to rank them.", "labels": [], "entities": []}, {"text": "However, the method proposed by Yao and Van Durme (2014) relies on rules and dependency parse results to extract hand-crafted features for questions.", "labels": [], "entities": []}, {"text": "Moreover, some methods () use the summation of question word embeddings to represent questions, which ignores word order information and cannot process complicated questions.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the multi-column convolutional neural networks to automatically analyze questions from multiple aspects.", "labels": [], "entities": []}, {"text": "Specifically, the model shares the same word embeddings to represent question words.", "labels": [], "entities": []}, {"text": "MCCNNs use different column networks to extract answer types, relations, and context information from the input questions.", "labels": [], "entities": []}, {"text": "The entities and relations in the knowledge base (namely FREE-BASE in our experiments) are also represented as low-dimensional vectors.", "labels": [], "entities": [{"text": "FREE-BASE", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9802075028419495}]}, {"text": "Then, a score layer is employed to rank candidate answers according to the representations of questions and candidate answers.", "labels": [], "entities": []}, {"text": "The proposed information extraction based method utilizes question-answer pairs to automatically learn the model without relying on manually annotated logical forms and hand-crafted features.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7886892855167389}]}, {"text": "We also do not use any pre-defined lexical triggers and rules.", "labels": [], "entities": []}, {"text": "In addition, the question paraphrases are also used to train networks and generalize for the unseen words in a multi-task learning manner.", "labels": [], "entities": []}, {"text": "We have conducted extensive experiments on WE-BQUESTIONS.", "labels": [], "entities": [{"text": "WE-BQUESTIONS", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.608659565448761}]}, {"text": "Experimental results illustrate that our method outperforms several baseline systems.", "labels": [], "entities": []}, {"text": "The contributions of this paper are three-fold: \u2022 We introduce multi-column convolutional neural networks for question understanding without relying on hand-crafted features and rules, and use question paraphrases to train the column networks and word vectors in a multi-task learning manner; \u2022 We jointly learn low-dimensional embeddings for the entities and relations in FREE-BASE with question-answer pairs as supervision signals; \u2022 We conduct extensive experiments on the WEBQUESTIONS dataset, and provide some intuitive interpretations for MCCNNs by developing a method to detect salient question words in the different column networks.", "labels": [], "entities": [{"text": "question understanding", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7951138019561768}, {"text": "WEBQUESTIONS dataset", "start_pos": 476, "end_pos": 496, "type": "DATASET", "confidence": 0.9405273199081421}]}], "datasetContent": [{"text": "In order to evaluate the model, we use the dataset WEBQUESTIONS (Section 3) to conduct experiments.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.8379366993904114}]}, {"text": "Settings The development set is used to select hyper-parameters in the experiments.", "labels": [], "entities": []}, {"text": "The nonlinearity function f = tanh is employed.", "labels": [], "entities": []}, {"text": "The dimension of word vectors is set to 25.", "labels": [], "entities": []}, {"text": "They are initialized by the pre-trained word embeddings provided in).", "labels": [], "entities": []}, {"text": "The window size of MCCNNs is 5.", "labels": [], "entities": [{"text": "MCCNNs", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.6403719782829285}]}, {"text": "The dimension of the pooling layers and the dimension of answer embeddings are set to 64.", "labels": [], "entities": []}, {"text": "The parameters are initialized by the techniques described in).", "labels": [], "entities": []}, {"text": "The max value used for max-norm regularization is 3.", "labels": [], "entities": []}, {"text": "The initial learning rate used in AdaGrad is set to 0.01.", "labels": [], "entities": []}, {"text": "A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set.", "labels": [], "entities": []}, {"text": "The margin values in Equation and Equation is set tom = 0.5 and mp = 0.1.", "labels": [], "entities": [{"text": "margin", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9749457836151123}, {"text": "Equation", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8617416024208069}, {"text": "Equation", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9443053603172302}]}, {"text": "Method F1 P@1 ( 31.4 -( 39.9 -( 37.5 -(  The evaluation metrics macro F1 score) and precision @ 1 (Bordes et al., 2014a) are reported.", "labels": [], "entities": [{"text": "F1 P", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9687846004962921}, {"text": "F1 score)", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9278974334398905}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9991018772125244}]}, {"text": "We use the official evaluation script provided by to compute the F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9833707809448242}]}, {"text": "Notably, the F1 score defined in ) is slightly different from others (how to compute scores for the questions without predicted results).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9811532199382782}]}, {"text": "We instead use the original definition in experiments.", "labels": [], "entities": []}, {"text": "As shown in, our method achieves better or comparable results than baseline methods on WEBQUESTIONS.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9369237422943115}]}, {"text": "To be more specific, the first three rows are semantic parsing based methods, and the other baselines are information extraction based methods.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7154055088758469}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7300797998905182}]}, {"text": "These approaches except () rely on handcrafted features and predefined rules.", "labels": [], "entities": []}, {"text": "The results show that automatically question understanding can be as good as the models using manually designed features.", "labels": [], "entities": [{"text": "automatically question understanding", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.5638310809930166}]}, {"text": "Besides, our multi-column convolutional neural networks based model outperforms the methods that use the sum of word embeddings as question representations ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results on the test split of WE-", "labels": [], "entities": [{"text": "WE-", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.48681579530239105}]}, {"text": " Table 2: Evaluation results of different set- tings on the test split of WEBQUESTIONS. w/o  path/type/context: without using the specific col- umn. w/o multi-column: tying parameters of mul- tiple columns. w/o paraphrase: without using  question paraphrases for training. 1-hop: using 1- hop paths to generate candidate answers.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9199256896972656}]}]}