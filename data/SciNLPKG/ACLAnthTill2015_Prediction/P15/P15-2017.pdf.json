{"title": [{"text": "Language Models for Image Captioning: The Quirks and What Works", "labels": [], "entities": [{"text": "Image Captioning", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7546128034591675}]}], "abstractContent": [{"text": "Two recent approaches have achieved state-of-the-art results in image caption-ing.", "labels": [], "entities": [{"text": "image caption-ing", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7522356510162354}]}, {"text": "The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence.", "labels": [], "entities": []}, {"text": "The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence.", "labels": [], "entities": []}, {"text": "In this paper , we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input.", "labels": [], "entities": []}, {"text": "We examine issues in the different approaches, including linguistic irregularities, caption repetition , and data set overlap.", "labels": [], "entities": [{"text": "caption repetition", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9132394790649414}]}, {"text": "By combining key aspects of the ME and RNN methods, we achieve anew record performance over previously published results on the benchmark COCO dataset.", "labels": [], "entities": [{"text": "ME", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.7578963041305542}, {"text": "COCO dataset", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9168614149093628}]}, {"text": "However, the gains we see in BLEU do not translate to human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.989845871925354}]}], "introductionContent": [{"text": "Recent progress in automatic image captioning has shown that an image-conditioned language model can be very effective at generating captions.", "labels": [], "entities": [{"text": "automatic image captioning", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6629952689011892}]}, {"text": "Two leading approaches have been explored for this task.", "labels": [], "entities": []}, {"text": "The first decomposes the problem into an initial step that uses a convolutional neural network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words.", "labels": [], "entities": []}, {"text": "The second approach uses the activations from final hidden layer of an object detection CNN as the input to a recurrent neural network language model.", "labels": [], "entities": []}, {"text": "This is referred to as a Multimodal Recurrent Neural Network (MRNN).", "labels": [], "entities": []}, {"text": "Similar in spirit is the the log-bilinear (LBL) LM of.", "labels": [], "entities": []}, {"text": "In this paper, we study the relative merits of these approaches.", "labels": [], "entities": []}, {"text": "By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components.", "labels": [], "entities": []}, {"text": "We find that the approach of directly generating the text with an MRNN 1 outperforms the ME LM when measured by BLEU on the COCO dataset (), 2 but this recurrent model tends to reproduce captions in the training set.", "labels": [], "entities": [{"text": "ME LM", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9351017773151398}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9986194372177124}, {"text": "COCO dataset", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9706970751285553}]}, {"text": "In fact, a simple k-nearest neighbor approach, which is common in earlier related work), performs similarly to the MRNN.", "labels": [], "entities": [{"text": "MRNN", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9182001948356628}]}, {"text": "In contrast, the ME LM generates the most novel captions, and does the best at captioning images for which there is no close match in the training data.", "labels": [], "entities": [{"text": "ME LM", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.7540004551410675}]}, {"text": "With a Deep Multimodal Similarity Model (DMSM) incorporated, 3 the ME LM significantly outperforms other methods according to human judgments.", "labels": [], "entities": []}, {"text": "In sum, the contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "We compare the use of discrete detections and continuous valued CNN activations as the conditioning information for language models trained to generate image captions.", "labels": [], "entities": []}, {"text": "2. We show that a simple k-nearest neighbor retrieval method performs at near state-of-theart for this task and dataset.", "labels": [], "entities": []}, {"text": "3. We demonstrate that a state-of-the-art MRNN-based approach tends to reconstruct previously seen captions; in contrast, the two stage ME LM approach achieves similar or better performance while generating relatively novel captions.", "labels": [], "entities": []}, {"text": "4. We advance the state-of-the-art BLEU scores on the COCO dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9971791505813599}, {"text": "COCO dataset", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9545240998268127}]}, {"text": "5. We present human evaluation results on the systems with the best performance as measured by automatic metrics.", "labels": [], "entities": []}, {"text": "6. We explore several issues with the statistical models and the underlying COCO dataset, including linguistic irregularities, caption repetition, and data set overlap.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.8982129395008087}, {"text": "caption repetition", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.818889707326889}]}], "datasetContent": [{"text": "Because automatic metrics do not always correlate with human judgments, we also performed human evaluations using the same procedure as in.", "labels": [], "entities": []}, {"text": "Here, human judges were presented with an image, a system generated caption, and a human generated caption, and were asked which caption was \"better\".", "labels": [], "entities": []}, {"text": "For each condition, 5 judgments were obtained for 1000 images from the testval set.", "labels": [], "entities": []}, {"text": "The D-ME+DMSM outperforms the MRNN by 5 percentage points for the \"Better Or Equal to Human\" judgment, despite both systems achieving the same BLEU score.", "labels": [], "entities": [{"text": "MRNN", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.5452967882156372}, {"text": "BLEU score", "start_pos": 143, "end_pos": 153, "type": "METRIC", "confidence": 0.977721244096756}]}, {"text": "The k-Nearest Neighbor system performs 1.4 percentage points worse than the MRNN, despite achieving a slightly higher BLEU score.", "labels": [], "entities": [{"text": "MRNN", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8171024322509766}, {"text": "BLEU score", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.982220470905304}]}, {"text": "Finally, the combined model does not outperform the D-ME+DMSM in terms of human judgments despite a 1.6 BLEU improvement.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.999345600605011}]}, {"text": "Although we cannot pinpoint the exact reason for this mismatch between automated scores and human evaluation, a more detailed analysis of the difference between systems is performed in Sections 4 and 5.: Results when comparing produced captions to those written by humans, as judged by humans.", "labels": [], "entities": []}, {"text": "These are the percent of captions judged to be \"better than\" or \"better than or equal to\" a caption written by a human.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model performance on testval.  \u2020: From (Fang et al.,  2015).", "labels": [], "entities": []}, {"text": " Table 3: Model performance on testval after re-ranking.   \u2020: previously reported and reconfirmed BLEU scores from  (Fang et al., 2015). +DMSM had resulted in the highest score  yet reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9972496628761292}, {"text": "DMSM", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9870718121528625}]}, {"text": " Table 4: Results when comparing produced captions to those  written by humans, as judged by humans. These are the per- cent of captions judged to be \"better than\" or \"better than or  equal to\" a caption written by a human.", "labels": [], "entities": []}, {"text": " Table 5: Example errors in the two basic approaches.", "labels": [], "entities": []}, {"text": " Table 6: Percentage unique (Unique Captions) and novel  (Seen In Training) captions for testval images. For example,  28.5% unique means 5,776 unique strings were generated for  all 20,244 images.", "labels": [], "entities": []}, {"text": " Table 7: Performance for different portions of testval, based  on visual overlap with the training.", "labels": [], "entities": []}]}