{"title": [{"text": "Semantic Representations for Domain Adaptation: A Case Study on the Tree Kernel-based Method for Relation Extraction", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.694897472858429}, {"text": "Relation Extraction", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.9227868616580963}]}], "abstractContent": [{"text": "We study the application of word embed-dings to generate semantic representations for the domain adaptation problem of relation extraction (RE) in the tree kernel-based method.", "labels": [], "entities": [{"text": "domain adaptation problem of relation extraction (RE)", "start_pos": 90, "end_pos": 143, "type": "TASK", "confidence": 0.7560378577974107}]}, {"text": "We systematically evaluate various techniques to generate the semantic representations and demonstrate that they are effective to improve the generalization performance of a tree kernel-based relation extractor across domains (up to 7% relative improvement).", "labels": [], "entities": []}, {"text": "In addition, we compare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings , to gain insights into which kind of system is more robust to domain changes.", "labels": [], "entities": [{"text": "RE", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.8942531943321228}]}, {"text": "Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8560614943504333}, {"text": "information extraction", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7369547039270401}]}, {"text": "Previous research on RE has followed either the kernelbased approach () or the feature-based approach;).", "labels": [], "entities": [{"text": "RE", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.961847186088562}]}, {"text": "Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution.", "labels": [], "entities": []}, {"text": "This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case.", "labels": [], "entities": [{"text": "RE", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9469642639160156}]}, {"text": "To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target domains.", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.850379753112793}]}, {"text": "We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (, i.e., building a single system that is able to cope with different, yet related target domains.", "labels": [], "entities": []}, {"text": "While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent.", "labels": [], "entities": [{"text": "DA", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9377892017364502}, {"text": "RE", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9520694613456726}]}, {"text": "To the best of our knowledge, there have been only three studies on DA for RE).", "labels": [], "entities": [{"text": "DA", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.5980138182640076}, {"text": "RE", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.7558988332748413}]}, {"text": "Of these,  follow the supervised DA paradigm and assume some labeled data in the target domains.", "labels": [], "entities": []}, {"text": "In contrast, and work on the unsupervised DA.", "labels": [], "entities": []}, {"text": "In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains.", "labels": [], "entities": [{"text": "DA", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.8217383027076721}, {"text": "RE", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9795178771018982}]}, {"text": "Our current work therefore focuses on the single-system unsupervised DA.", "labels": [], "entities": [{"text": "single-system unsupervised DA", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.49565522869427997}]}, {"text": "Besides, note that this setting tries to construct a single system that can work robustly with different but related domains (multiple target domains), thus being different from most previous studies on DA () which have attempted to design a specialized system for every specific target domain.", "labels": [], "entities": []}, {"text": "propose to embed word clusters and latent semantic analysis (LSA) of words into tree kernels for DA of RE, while Nguyen and Grishman (2014) studies the appli-cation of word clusters and word embeddings for DA of RE on the feature-based method.", "labels": [], "entities": [{"text": "latent semantic analysis (LSA)", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.7383451908826828}]}, {"text": "Although word clusters) have been employed by both studies to improve the performance of relation extractors across domains, the application of word embeddings () for DA of RE is only examined in the feature-based method and never explored in the tree kernelbased method so far, giving rise to the first question we want to address in this paper: (i) Can word embeddings help the tree kernelbased methods on DA for RE and more importantly, in which way can we do it effectively?", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7275649309158325}]}, {"text": "This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mismatches of concrete labels in the parse trees to compute the kernels.", "labels": [], "entities": []}, {"text": "It is unclear at the first glance how to encode word embeddings into the tree kernels effectively so that word embeddings could help to improve the generalization performance of RE.", "labels": [], "entities": []}, {"text": "One way is to use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e.g., by resembling the method of that exploited LSA (in the semantic syntactic tree kernel (SSTK), cf. \u00a72.1).", "labels": [], "entities": []}, {"text": "We explore various methods to apply word embeddings to generate the semantic representations for DA of RE and demonstrate that semantic representations are very effective to significantly improve the portability of the relation extractors based on the tree kernels, bringing us to the second question: (ii) Between the feature-based method in and the SSTK method in, which method is better for DA of RE, given the recent discovery of word embeddings for both methods?", "labels": [], "entities": []}, {"text": "It is worth noting that besides the approach difference, these two works employ rather different resources and settings in their evaluation, making it impossible to directly compare their performance.", "labels": [], "entities": []}, {"text": "In particular, while only use the path-enclosed trees induced from the constituent parse trees as the representation for relation mentions, Nguyen and Grishman (2014) include a rich set of features extracted from multiple resources such as constituent trees, dependency trees, gazetteers, semantic resources in the representation.", "labels": [], "entities": []}, {"text": "consider the direction of relations in their evaluation (i.e, distinguishing between relation classes and their inverses) but disregard this relation direction.", "labels": [], "entities": []}, {"text": "Finally, we note that although both studies evaluate their systems on the ACE 2005 dataset, they actually have different dataset partitions.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9872948129971822}]}, {"text": "In order to overcome this limitation, we conduct an evaluation in which the two methods are directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an insight on their effectiveness for DA of RE.", "labels": [], "entities": [{"text": "DA of RE", "start_pos": 235, "end_pos": 243, "type": "TASK", "confidence": 0.6959427793820699}]}, {"text": "In fact, the problem of incompatible comparison is unfortunately very common in the RE literature and we believe there is a need to tackle this increasing confusion in this line of research.", "labels": [], "entities": [{"text": "RE", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.8874858021736145}]}, {"text": "Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings.", "labels": [], "entities": []}, {"text": "To ease the comparison for future work and circumvent the Zigglebottom pitfall, the entire setup and package is available.", "labels": [], "entities": [{"text": "Zigglebottom pitfall", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.8094110488891602}]}], "datasetContent": [{"text": "We use the word clusters trained by on the ukWaC corpus () with 2 billion words, and the C&W word embeddings from Turian el al.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9924944639205933}, {"text": "C&W word embeddings from Turian el al", "start_pos": 89, "end_pos": 126, "type": "DATASET", "confidence": 0.7819738255606757}]}, {"text": "(2010) 2 with 50 dimensions following.", "labels": [], "entities": []}, {"text": "In order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by, which uses the Charniak parser to obtain the constituent trees, the SVM-light-TK for the SSTK kernel in SVM, the directional relation classes, etc.", "labels": [], "entities": []}, {"text": "We utilize the default vector kernel in the SVM-light-TK package (d=3).", "labels": [], "entities": []}, {"text": "For the feature-based method, we apply the MaxEnt classifier in the MALLET 3 package with the L2 regularizer on the hierarchical architecture for relation extraction as in.", "labels": [], "entities": [{"text": "MALLET 3 package", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.7192199726899465}, {"text": "relation extraction", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.8107363283634186}]}, {"text": "Following prior work, we evaluate the systems on the ACE 2005 dataset which involves 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un).", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9784032106399536}]}, {"text": "The union of bn and nw (news) is used as the source domain while bc, cts and wl play the role of the target domains.", "labels": [], "entities": []}, {"text": "We take half of bc as the only target development set, and use the remaining data and domains for testing.", "labels": [], "entities": []}, {"text": "The dataset partition is exactly the same as in.", "labels": [], "entities": []}, {"text": "As described in their paper, the target domains quite differ from the source domain in the relation distributions and vocabulary.", "labels": [], "entities": []}, {"text": "In this section, we examine the semantic representation for DA of RE in the tree kernelbased method.", "labels": [], "entities": []}, {"text": "In particular, we take the systems using the PET trees, word clusters and LSA in as the baselines and augment them with the embeddings WED = HEAD+PHRASE.", "labels": [], "entities": [{"text": "PHRASE", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.7093619704246521}]}, {"text": "We report the performance of these augmented systems in for the two scenarios: (i) in-domain: both training and testing are performed on the source domain via 5-fold cross validation and (ii) out-of-domain: models are trained on the source domain but evaluated on the three target domains.", "labels": [], "entities": []}, {"text": "To summarize, we find: First, word embeddings seem to subsume word clusters in the tree kernel-based method (comparing rows 2 and 4, and except domain cts) while word embeddings and LSA actually encode different information (comparing rows 2 and 6 for the out-of-domain experiments) and their combination would be helpful for DA of RE.", "labels": [], "entities": []}, {"text": "Second, regarding composite kernels, given word embeddings, the addition of the baseline kernel (PET) is in general useful for the augmented kernels PET WC and PET LSA (comparing rows 4 and 8, rows 6 and 10) although it is less pronounced for PET LSA.", "labels": [], "entities": []}, {"text": "Third and most importantly, for all the systems in Plank and Moschitti (2013) (the baselines) and for all the target domains, whether word clusters and LSA are utilized or not, we consistently witness the performance improvement of the baselines when combined with word embedding (comparing systems X and X+WED where X is some baseline system).", "labels": [], "entities": []}, {"text": "The best out-of-domain performance is achieved when word embeddings are employed in conjunction with the composite kernels (PET+PET WC+PET LSA for the target domains bc and wl, and PET+PET WC for the target domain cts).", "labels": [], "entities": []}, {"text": "To be more concrete, the best system with word embeddings (row 12 in) significantly outperforms the best system in Plank and Moschitti (2013) with p < 0.05, an improvement of 3.7%, 1.1% and 2.7% on the target domains bc, cts and wl respectively, demonstrating the benefit of word embeddings for DA of RE in the tree kernel-based method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the bc dev set for PET. Best com-", "labels": [], "entities": [{"text": "bc dev set", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.685738205909729}, {"text": "PET", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.5562995076179504}]}, {"text": " Table 2: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. Systems of the rows", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9691961705684662}]}, {"text": " Table 3: Tree kernel-based in Plank and Moschitti (2013) vs feature-based in", "labels": [], "entities": []}, {"text": " Table 4: Performance of the feature-based method (dev).", "labels": [], "entities": []}]}