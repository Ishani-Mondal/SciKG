{"title": [], "abstractContent": [{"text": "1 Tutorial Overview Statistical natural language processing relies on probabilistic models of linguistic structure.", "labels": [], "entities": [{"text": "Statistical natural language processing", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.6189457848668098}]}, {"text": "More complex models can help capture our intuitions about language, by adding linguistically meaningful interactions and latent variables.", "labels": [], "entities": []}, {"text": "However, inference and learning in the models we want often poses a serious computational challenge.", "labels": [], "entities": []}, {"text": "Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods.", "labels": [], "entities": [{"text": "Belief propagation (BP)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.895856523513794}]}, {"text": "These approaches can handle joint models of interacting components , are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing , phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.8593586087226868}, {"text": "CCG parsing", "start_pos": 239, "end_pos": 250, "type": "TASK", "confidence": 0.7888565063476562}, {"text": "phrase extraction", "start_pos": 253, "end_pos": 270, "type": "TASK", "confidence": 0.8508889377117157}, {"text": "semantic role labeling", "start_pos": 272, "end_pos": 294, "type": "TASK", "confidence": 0.6475910743077596}, {"text": "information extraction", "start_pos": 300, "end_pos": 322, "type": "TASK", "confidence": 0.8203368782997131}]}, {"text": "This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks.", "labels": [], "entities": [{"text": "BP", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9492377042770386}]}, {"text": "Our goal is to elucidate how these approaches can easily be applied to new problems.", "labels": [], "entities": []}, {"text": "We also cover the theory underlying them.", "labels": [], "entities": []}, {"text": "Our target audience is researchers inhuman language technologies; we do not assume familiarity with BP.", "labels": [], "entities": []}, {"text": "In the first three sections, we discuss applications of BP to NLP problems, the basics of mod-eling with factor graphs and message passing, and the theoretical underpinnings of \"what BP is do-ing\" and how it relates to other inference techniques.", "labels": [], "entities": [{"text": "message passing", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.730036735534668}]}, {"text": "In the second three sections, we cover key extensions to the standard BP algorithm to enable modeling of linguistic structure, efficient inference , and approximation-aware training.", "labels": [], "entities": []}, {"text": "We survey a variety of software tools and introduce anew software framework that incorporates many of the modern approaches covered in this tutorial.", "labels": [], "entities": []}, {"text": "Probabilistic Modeling [15 min., Eisner] \u2022 Intro: Modeling with factor graphs \u2022 Constituency and dependency parsing \u2022 Joint CCG Parsing and supertagging \u2022 Transliteration; Morphology \u2022 Alignment; Phrase extraction \u2022 Joint models for NLP; Semantic role labeling ; Targeted sentiment \u2022 Variable-centric view of the world 2.", "labels": [], "entities": [{"text": "Constituency and dependency parsing", "start_pos": 80, "end_pos": 115, "type": "TASK", "confidence": 0.578648753464222}, {"text": "Phrase extraction", "start_pos": 196, "end_pos": 213, "type": "TASK", "confidence": 0.8609652817249298}, {"text": "Semantic role labeling", "start_pos": 238, "end_pos": 260, "type": "TASK", "confidence": 0.7264282902081808}]}, {"text": "Belief Propagation Basics [40 min., Eisner] \u2022 Messages and beliefs \u2022 Sum-product algorithm \u2022 Relation to the forward-backward and Viterbi algorithms \u2022 BP as dynamic programming \u2022 Acyclic vs. loopy graphs 3.", "labels": [], "entities": []}, {"text": "Theory [25 min., Gormley] \u2022 From sum-product to max-product \u2022 From arc consistency to BP \u2022 From Gibbs sampling to particle BP to BP \u2022 Convergence properties \u2022 Bethe free energy 4.", "labels": [], "entities": [{"text": "BP", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9865748286247253}]}, {"text": "Incorporating Structure into Factors and Variables [30 min., Gormley] \u2022 Embedding dynamic programs (e.g. inside-outside) within factors \u2022 String-valued variables and finite state machines 5.", "labels": [], "entities": []}, {"text": "Message approximation and scheduling [20 min., Eisner] \u2022 Computing fewer messages \u2022 Pruning messages \u2022 Expectation Propagation and Penalized EP 6.", "labels": [], "entities": [{"text": "Message approximation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8501392304897308}]}, {"text": "Approximation-aware Training [30 min., Gorm-ley] \u2022 Empirical risk minimization under approximations (ERMA) \u2022 BP as a computational expression graph \u2022 Automatic differentiation (AD) 7.", "labels": [], "entities": [{"text": "Approximation-aware", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9525405168533325}, {"text": "Gorm-ley", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9543156027793884}, {"text": "Empirical risk minimization", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7030877073605856}, {"text": "Automatic differentiation (AD)", "start_pos": 150, "end_pos": 180, "type": "TASK", "confidence": 0.7326703906059265}]}, {"text": "Software [10 min., Gormley] 5", "labels": [], "entities": [{"text": "Gormley", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9211245179176331}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}