{"title": [{"text": "A Strategic Reasoning Model for Generating Alternative Answers", "labels": [], "entities": [{"text": "Generating Alternative Answers", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.6355006098747253}]}], "abstractContent": [{"text": "We characterize a class of indirect answers to yes/no questions, alternative answers , where information is given that is not directly asked about, but which might nonetheless address the underlying motivation for the question.", "labels": [], "entities": []}, {"text": "We develop a model rooted in game theory that generates these answers via strategic reasoning about possible unobserved domain-level user requirements.", "labels": [], "entities": []}, {"text": "We implement the model within an interactive question answering system simulating real estate dialogue.", "labels": [], "entities": []}, {"text": "The system learns a prior probability distribution over possible user requirements by analyzing training dialogues, which it uses to make strategic decisions about answer selection.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.8620148301124573}]}, {"text": "The system generates pragmatically natural and inter-pretable answers which make for more efficient interactions compared to a baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language dialogue, questions are often answered indirectly.", "labels": [], "entities": [{"text": "natural language dialogue", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.667666107416153}]}, {"text": "This is particularly apparent for yes/no questions, where a wide range of responses beyond literal \"yes\" and \"no\" answers is available.", "labels": [], "entities": []}, {"text": "Sometimes indirect answers serve to anticipate the next step of the hearer's plan, as in (1), where the literal answer is entailed by the supplied answer, and sometimes indirect answers leave it to the hearer to infer the literal answer from common contextual assumptions, as in (2)).", "labels": [], "entities": []}, {"text": "(1) Q: Has the train to Windsor left yet?", "labels": [], "entities": [{"text": "Q", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9709269404411316}]}, {"text": "A: It's leaving soon from gate 7.", "labels": [], "entities": []}, {"text": "(2) Q: Is Sue at work?", "labels": [], "entities": []}, {"text": "A: She's sick with the flu.", "labels": [], "entities": []}, {"text": "But other times there is no semantic link between the question and the supplied answer.", "labels": [], "entities": []}, {"text": "Rather, the answer must be interpreted in light of the taskspecific goals of the interlocutors.", "labels": [], "entities": []}, {"text": "Consider in a context where a customer is posing questions to areal estate agent with the aim of renting an apartment.", "labels": [], "entities": []}, {"text": "(3) Q: Does the apartment have a garden?", "labels": [], "entities": []}, {"text": "A: Well, it has a large balcony.", "labels": [], "entities": []}, {"text": "Whether there is a balcony has no logical bearing on whether there is a garden.", "labels": [], "entities": []}, {"text": "Intuitively, the realtor is inferring that the customer's question might have been motivated by a more general requirement (perhaps the customer wants a place to grow flowers) and supplying an alternative attribute to satisfy that requirement.", "labels": [], "entities": []}, {"text": "In this case the answerer must reason about which attributes of an apartment might satisfy a customer who would ask about a garden.", "labels": [], "entities": []}, {"text": "Note that multiple motivating requirements are possible (perhaps the customer just wants to relax outside), such that the answerer might just as easily have said, \"It has a large balcony, and there is a park close by.\"", "labels": [], "entities": []}, {"text": "In either case, the hearer can infer from the lack of a direct answer that the apartment must not have a garden, because if it did, to say so would have been more obviously helpful.", "labels": [], "entities": []}, {"text": "This paper focuses on this class of answers, which we call alternative answers.", "labels": [], "entities": []}, {"text": "We characterize these as indirect answers to yes/no questions that offer attributes of an object under discussion which might satisfy an unobserved domain-level requirement of the questioner.", "labels": [], "entities": []}, {"text": "We conceive of a requirement as a set of satisfying conditions, such that a particular domain-related need would be met by anyone member of the set.", "labels": [], "entities": []}, {"text": "For example, in the context of (3) we can encode a possible customer requirement of a place to grow flowers in an apartment, FLOWERS = {GARDEN, BALCONY}, such that either GARDEN or BALCONY would suffice to satisfy the requirement.", "labels": [], "entities": [{"text": "FLOWERS", "start_pos": 125, "end_pos": 132, "type": "METRIC", "confidence": 0.9947354197502136}, {"text": "BALCONY", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.892988383769989}, {"text": "BALCONY", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9760221838951111}]}, {"text": "In order to generate alternative answers automatically, we must first solve two problems: (i) how does one learn and represent a space of likely user requirements?, and (ii) how does one use such a space to select indirect answers?", "labels": [], "entities": []}, {"text": "To do this in a natural, pragmatically interpretable way, we must not only derive answers like in (3), but crucially, also rule out infelicitous responses like the following, where a logically possible alternative leads to incoherence due to the low probability of an appropriate requirement like {GARDEN, BASE-MENT}.", "labels": [], "entities": [{"text": "GARDEN", "start_pos": 298, "end_pos": 304, "type": "METRIC", "confidence": 0.7915077209472656}, {"text": "BASE-MENT", "start_pos": 306, "end_pos": 315, "type": "METRIC", "confidence": 0.9867433309555054}]}, {"text": "(In other words, wanting a garden has little effect on the probability of wanting a basement.)", "labels": [], "entities": []}, {"text": "(4) Q: Does the apartment have a garden?", "labels": [], "entities": []}, {"text": "A: #Well, it has a large basement.", "labels": [], "entities": []}, {"text": "To solve these problems, we propose an approach rooted in decision-theoretic and game-theoretic analyses of indirectness in natural language) whereby a system uses strategic reasoning to derive an optimal response to a yes/no question given certain domain assumptions.", "labels": [], "entities": []}, {"text": "The model operates by assuming that both the questioner and the answerer are rational, i.e. that both participants want to further their own goals, and will behave so as to maximize the probability of success at doing so.", "labels": [], "entities": []}, {"text": "One appeal of the strategic approach is its relative simplicity: the model utilizes a learned probability distribution over possible domain-level requirements of the questioner and applies simple probabilistic reasoning to feed content selection during online answer generation.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 260, "end_pos": 277, "type": "TASK", "confidence": 0.708460196852684}]}, {"text": "Unlike plan inference approaches, we do not need to represent any complex taxonomies of stimulus conditions or coherence relations.", "labels": [], "entities": []}, {"text": "By implementing the strategic reasoning model within a simple interactive question answering system (, simulating real estate dialogues with exchanges like in (3), we are able to evaluate the current approach quantitatively in terms of dialogue efficiency, perceived coherence of the supplied answers, and ability of users to draw natural pragmatic inferences.", "labels": [], "entities": []}, {"text": "We conclude that strategic reasoning provides a promising framework for developing answer generation methods by starting with principled theoretical analyses of human dialogue.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.9169585704803467}]}, {"text": "The following section presents the model, including a concrete content selection algorithm used for producing answers to questions, and then walks through a simple illustrative example.", "labels": [], "entities": []}, {"text": "Section 3 describes our implementation, addresses the problem of learning requirement probabilities, and presents the results of our evaluation, providing quantitative support for our approach.", "labels": [], "entities": []}, {"text": "Section 4 concludes with a general summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "We obtained data from a total of 115 subjects via Amazon Mechanical Turk; 65 subjects interacted with the literal comparison model, and 50 subjects interacted with the strategic model.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.9462117552757263}]}, {"text": "We excluded a total of 13 outliers across both conditions who asked too few or too many questions (1.5 interquartile ranges below the 1st or above the 3rd quartile).", "labels": [], "entities": []}, {"text": "These subjects either quit the task early or simply asked all available questions even for apartments that were obviously not a good fit for their requirements.", "labels": [], "entities": []}, {"text": "Two subjects were excluded for not filling out the post-experiment questionnaire.", "labels": [], "entities": []}, {"text": "This left 100 subjects (59 literal/41 strategic), of which 86 (49/37) successfully completed the task, accepting the object which met all assigned requirements.", "labels": [], "entities": []}, {"text": "There was no statistically significant difference between the literal and strategic models with respect to task success.", "labels": [], "entities": []}, {"text": "We first compare the literal and strategic models with regard to dialogue length, looking only at the subjects who successfully completed the task.", "labels": [], "entities": [{"text": "dialogue length", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7088226079940796}]}, {"text": "Due to the highly structured nature of the experiment it was always the case that a successful dialogue consisted of 10 apartment proposals, some number of QA pairs, where each question was given a single answer, 9 rejections and, finally, one acceptance.", "labels": [], "entities": []}, {"text": "This allows us to use the number of questions asked as a proxy for dialogue length.", "labels": [], "entities": [{"text": "dialogue length", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8040353953838348}]}, {"text": "The strategic model yields 27.4 questions on average, more than four fewer than the literal model's 31.6.", "labels": [], "entities": []}, {"text": "Standard statistical tests show the effect to be highly significant, with a one-way ANOVA yielding F=16.2, p = 0.0001, and a mixed effects regression model with a random slope for item (the items in this case being the set of requirements assigned to the sub- We now ask whether the observed effect is due only to the presence of helpful alternatives which preclude the need for follow-up questions, or whether the ability of users to draw pragmatic inferences from unhelpful alternatives (i.e. alternatives that don't actually satisfy the user's requirement) also contributes to dialogue efficiency., taken from areal dialogue with our system, illustrates such an inference.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9936916828155518}, {"text": "F", "start_pos": 99, "end_pos": 100, "type": "METRIC", "confidence": 0.9972667694091797}]}, {"text": "The subject specifically wants a caf\u00e9 nearby, and infers from the alternative answer that this requirement cannot be satisfied, and therefore rejects.", "labels": [], "entities": []}, {"text": "The subject could have asked the question again to get a direct answer, which would have had a negative effect on dialogue efficiency, but this did not happen.", "labels": [], "entities": []}, {"text": "We want to know if subjects' aggregate behavior reflects this example.", "labels": [], "entities": []}, {"text": "First, take the null hypothesis to be that subjects do not reliably draw such negative implicatures.", "labels": [], "entities": []}, {"text": "In that case we would expect a certain proportion of questions to be repeated.", "labels": [], "entities": []}, {"text": "Subjects are allowed to ask questions multiple times, and alternatives are never presented twice, such that repeating questions will ultimately lead to a direct yes/no answer.", "labels": [], "entities": []}, {"text": "We do see some instances of this behavior in the proportion repeated questions dialogues.", "labels": [], "entities": []}, {"text": "If this is indicative of an overall difficulty in drawing pragmatic inferences from an online dialogue system, then we expect the number of such repetitions to reflect the number of unhelpful alternatives that are offered.", "labels": [], "entities": []}, {"text": "Instead, we find that when we plot a linear regression of repeated questions vs. unhelpful alternatives, we get a flat line with no observable correlation.", "labels": [], "entities": []}, {"text": "Moreover, we also find no effect of unhelpful alternatives on whether the task was successfully completed.", "labels": [], "entities": []}, {"text": "This suggests that the correct inferences are being drawn, as in.", "labels": [], "entities": []}, {"text": "We now look at the perceived coherence of the dialogues as assessed by our post-experiment questionnaire.", "labels": [], "entities": []}, {"text": "We obtain a composite coherence score from all coherence-related items on the seven point Likert scale by summing all per-item scores for each subject and normalizing them to a unit interval, where 1 signifies the upper bound of perceived coherence.", "labels": [], "entities": []}, {"text": "Although there is a difference in mean coherence score between the strategic and literal models, with the strategic model exhibiting 88% perceived coherence and the literal model 93%, the difference is not statistically significant.", "labels": [], "entities": []}, {"text": "Moreover, we can rule out the possibility that the strategic model is judged to be coherent only when the number of alternative answers is low.", "labels": [], "entities": []}, {"text": "To rule this out, we calculate the expected coherence score under the null hypothesis that coherence is directly proportional to the proportion of literal answers.", "labels": [], "entities": []}, {"text": "Taking the literal model's average score of 0.93 as a ceiling, we multiply this by the proportion of literal answers to obtain a null hypothesis expected score of about 0.75 for the strategic model.", "labels": [], "entities": []}, {"text": "This null hypothesis is disconfirmed (F=12.5, t=30.6, p<0.01).", "labels": [], "entities": [{"text": "F", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9985581040382385}]}, {"text": "The strategic model is judged, by the criteria assessed by our post-experiment questionnaire, to be pragmatically coherent independently of the rate of indirect answers given.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of best-case output with  respect to potential benefit of alternative answer  types to subjects. Precision = hits / hits+misses,  and Recall = hits / possible hits. A \"hit\" is a QA  pair which is a possible output of the model, such  that A could be a beneficial answer to a customer  asking Q, and a \"miss\" is such a QA pair such that  A is irrelevant to Q.", "labels": [], "entities": [{"text": "Precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9950289130210876}, {"text": "Recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9978895783424377}]}]}