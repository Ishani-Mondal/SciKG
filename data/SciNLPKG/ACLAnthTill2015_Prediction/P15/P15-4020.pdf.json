{"title": [{"text": "Multi-level Translation Quality Prediction with QUEST++", "labels": [], "entities": [{"text": "Translation Quality Prediction", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.6841410100460052}]}], "abstractContent": [{"text": "This paper presents QUEST++ , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level.", "labels": [], "entities": []}, {"text": "It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences).", "labels": [], "entities": []}, {"text": "QUEST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models.", "labels": [], "entities": []}, {"text": "Results on recent datasets show that QUEST++ achieves state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade.", "labels": [], "entities": [{"text": "Quality Estimation (QE) of Machine Translation (MT)", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6775455556132577}]}, {"text": "With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency, selecting high quality segments, selecting the best translation), and highlighting words or phrases that need revision ().", "labels": [], "entities": [{"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9878455996513367}]}, {"text": "Most recent work focuses on sentence-level QE.", "labels": [], "entities": [{"text": "sentence-level QE", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.5434002429246902}]}, {"text": "This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores).", "labels": [], "entities": []}, {"text": "Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012.", "labels": [], "entities": [{"text": "Sentence-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.85201096534729}, {"text": "Statistical Machine Translation (WMT)", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.7830792019764582}]}, {"text": "While standard algorithms can be used to build prediction models, key to this task is work of feature engineering.", "labels": [], "entities": []}, {"text": "Two open source feature extraction toolkits are available for that: ASIYA and QUEST 2 ( ).", "labels": [], "entities": []}, {"text": "The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years).", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.8582647244135538}]}, {"text": "QE at other textual levels have received much less attention.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.6861922740936279}]}, {"text": "Word-level QE () is seemingly a more challenging task where a quality label is to be produced for each target word.", "labels": [], "entities": [{"text": "Word-level QE", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.4159283936023712}]}, {"text": "An additional challenge is the acquisition of sizable training sets.", "labels": [], "entities": []}, {"text": "Although significant efforts have been made, there is considerable room for improvement.", "labels": [], "entities": []}, {"text": "In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline.", "labels": [], "entities": [{"text": "WMT13-14 QE shared task submissions", "start_pos": 14, "end_pos": 49, "type": "TASK", "confidence": 0.6174698233604431}]}, {"text": "Document-level QE consists in predicting a single label for entire documents, be it an absolute score) or a relative ranking of translations by one or more MT systems.", "labels": [], "entities": [{"text": "Document-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6467384696006775}]}, {"text": "While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document.", "labels": [], "entities": []}, {"text": "Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document.", "labels": [], "entities": []}, {"text": "Feature engineering is a challenge given the little availability of tools to extract discourse-wide information.", "labels": [], "entities": [{"text": "Feature engineering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.882818728685379}]}, {"text": "In addition, no datasets with human-created labels are available and thus scores produced by automatic metrics have to be used as approximation.", "labels": [], "entities": []}, {"text": "Some applications require fine-grained, wordlevel information on quality.", "labels": [], "entities": []}, {"text": "For example, one may want to highlight words that need fixing.", "labels": [], "entities": []}, {"text": "Document-level QE is needed particularly for gisting purposes where post-editing is not an option.", "labels": [], "entities": [{"text": "Document-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.4860471785068512}]}, {"text": "For example, for predictions on translations of product reviews in order to decide whether or not they are understandable by readers.", "labels": [], "entities": [{"text": "predictions on translations of product reviews", "start_pos": 17, "end_pos": 63, "type": "TASK", "confidence": 0.8403890530268351}]}, {"text": "We believe that the limited progress in word and documentlevel QE research is partially due to lack of a basic framework that one can be build upon and extend.", "labels": [], "entities": [{"text": "word and documentlevel QE", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.4928146153688431}]}, {"text": "QUEST++ is a significantly refactored and expanded version of an existing open source sentence-level toolkit, QUEST.", "labels": [], "entities": []}, {"text": "Feature extraction modules for both word and document-level QE were added and the three levels of prediction were unified into a single pipeline, allowing for interactions between word, sentence and documentlevel QE.", "labels": [], "entities": []}, {"text": "For example, word-level predictions can be used as features for sentence-level QE.", "labels": [], "entities": []}, {"text": "Finally, sequence-labelling learning algorithms for wordlevel QE were added.", "labels": [], "entities": [{"text": "wordlevel QE", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.6027237474918365}]}, {"text": "QUEST++ can be easily extended with new features at any textual level.", "labels": [], "entities": [{"text": "QUEST", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.850141167640686}]}, {"text": "The architecture of the system is described in Section 2.", "labels": [], "entities": []}, {"text": "Its main component, the feature extractor, is presented in Section 3.", "labels": [], "entities": [{"text": "feature extractor", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6834041178226471}]}, {"text": "Section 4 presents experiments using the framework with various datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "In what follows, we evaluate QUEST++'s performance for the three prediction levels and various datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F-1 for the WMT14 English-Spanish task", "labels": [], "entities": [{"text": "F-1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9291104078292847}, {"text": "WMT14 English-Spanish task", "start_pos": 22, "end_pos": 48, "type": "DATASET", "confidence": 0.9173559149106344}]}, {"text": " Table 2: F-1 for the WMT14 Spanish-English task", "labels": [], "entities": [{"text": "F-1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9518327713012695}, {"text": "WMT14 Spanish-English task", "start_pos": 22, "end_pos": 48, "type": "DATASET", "confidence": 0.9280070265134176}]}, {"text": " Table 3: F-1 for the WMT14 English-German task", "labels": [], "entities": [{"text": "F-1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.891096830368042}, {"text": "WMT14 English-German task", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.9201119343439738}]}, {"text": " Table 4: F-1 for the WMT14 German-English task", "labels": [], "entities": [{"text": "F-1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.897594153881073}, {"text": "WMT14 German-English task", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.9021516839663187}]}, {"text": " Table 5: F-1 for the WMT15 English-Spanish task", "labels": [], "entities": [{"text": "F-1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9332139492034912}, {"text": "WMT15 English-Spanish task", "start_pos": 22, "end_pos": 48, "type": "DATASET", "confidence": 0.900007168451945}]}, {"text": " Table 7: MAE values for document-level QE", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9844692349433899}, {"text": "QE", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.49392592906951904}]}]}