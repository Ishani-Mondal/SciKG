{"title": [{"text": "A Deeper Exploration of the Standard PB-SMT Approach to Text Simplification and its Evaluation", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7459722459316254}]}], "abstractContent": [{"text": "In the last few years, there has been a growing number of studies addressing the Text Simplification (TS) task as a mono-lingual machine translation (MT) problem which translates from 'original' to 'sim-ple' language.", "labels": [], "entities": [{"text": "Text Simplification (TS) task", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.8649093310038248}, {"text": "mono-lingual machine translation (MT)", "start_pos": 116, "end_pos": 153, "type": "TASK", "confidence": 0.8419188857078552}]}, {"text": "Motivated by those results , we investigate the influence of quality vs quantity of the training data on the effectiveness of such a MT approach to text simplification.", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9933516383171082}, {"text": "text simplification", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7839352786540985}]}, {"text": "We conduct 40 experiments on the aligned sentences from English Wikipedia and Simple English Wikipedia, controlling for: (1) the similarity between the original and simplified sentences in the training and development datasets, and (2) the sizes of those datasets.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.856129507223765}]}, {"text": "The results suggest that in the standard PB-SMT approach to text simplification the quality of the datasets has a greater impact on the system performance.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8022691011428833}]}, {"text": "Additionally, we point out several important differences between cross-lingual MT and monolingual MT used in text simplification , and show that BLEU is not a good measure of system performance in text simplification task.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7900388240814209}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9991784691810608}, {"text": "text simplification task", "start_pos": 197, "end_pos": 221, "type": "TASK", "confidence": 0.8238749106725057}]}], "introductionContent": [{"text": "In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from 'original' to 'simple' language.", "labels": [], "entities": [{"text": "text simplification (TS)", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.864215886592865}, {"text": "monolingual machine translation (MT)", "start_pos": 105, "end_pos": 141, "type": "TASK", "confidence": 0.836152603228887}, {"text": "translating sentences from 'original' to 'simple' language", "start_pos": 153, "end_pos": 211, "type": "TASK", "confidence": 0.6638195785609159}]}, {"text": "Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task), but made no attempt to explain the reasons behind the success of their systems.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 58, "end_pos": 102, "type": "TASK", "confidence": 0.5881468877196312}]}, {"text": "obtained reasonably good results (BLEU = 60.75) despite the small size of the datasets used (4,483 original sentences and their corresponding simplifications).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9996813535690308}]}, {"text": "Her results indicated that in this specific monolingual MT task, we do not need such large datasets (as in cross-lingual MT) in order to achieve good results.", "labels": [], "entities": [{"text": "MT task", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.9188080430030823}, {"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.8284329175949097}]}, {"text": "At the moment, the scarcity and very limited sizes of the available TS datasets (usually only up to 1,000 sentence pairs) are the main factors which impede the use of data-driven approaches to text simplification for all languages except English (for which English Wikipedia and Simple English Wikipedia offer a large comparable TS dataset).", "labels": [], "entities": [{"text": "text simplification", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.7074706554412842}]}, {"text": "Therefore, in this paper, we decided to investigate several important issues in MT-based text simplification: 1.", "labels": [], "entities": [{"text": "MT-based text simplification", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.9195606509844462}]}, {"text": "The impact of the size of the training and development datasets; 2.", "labels": [], "entities": []}, {"text": "The impact of the similarity between the original and simplified sentences in the training and development datasets; and 3.", "labels": [], "entities": []}, {"text": "The suitability of using the BLEU score for the automatic evaluation of system's performance.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9719499349594116}]}, {"text": "To the best of our knowledge, there have been no studies which address those important questions.", "labels": [], "entities": []}, {"text": "In order to explore the first two issues, we conduct 40 translation experiments using the aligned sentence pairs from the largest existing TS corpus (Wikipedia TS corpus), controlling the training and development datasets for: (1) sentence similarity (in terms of the S-BLEU score), and (2) size.", "labels": [], "entities": [{"text": "TS corpus (Wikipedia TS corpus", "start_pos": 139, "end_pos": 169, "type": "DATASET", "confidence": 0.7885723561048508}, {"text": "size", "start_pos": 291, "end_pos": 295, "type": "METRIC", "confidence": 0.9572008848190308}]}, {"text": "Our results indicate that only the former can influence the MT output significantly.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9826046824455261}]}, {"text": "In order to explore the last issue, we test our models on two different test sets and perform human evaluation of the output of several systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all experiments, we use the same standard PB-SMT model (, the GIZA++ implementation of IBM word alignment model 4 (, and the refinement and phrase-extraction heuristics described further by.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.6302080899477005}]}, {"text": "We tune the systems using minimum error rate training (MERT).", "labels": [], "entities": [{"text": "minimum error rate training (MERT", "start_pos": 26, "end_pos": 59, "type": "METRIC", "confidence": 0.8316127061843872}]}, {"text": "For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.4811449348926544}]}, {"text": "We limit our stack size to 500 hypotheses during decoding.", "labels": [], "entities": []}, {"text": "We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9682281315326691}]}, {"text": "Using the simplified sentences as references and the original sentences as hypotheses, In women, the larger mammary glands within the breast produce the milk.", "labels": [], "entities": []}, {"text": "The breast contains mammary glands.", "labels": [], "entities": []}, {"text": "We test our models on two different test sets: The sizes of both test sets and their BLEU scores (calculated using the original sentences as simplification/translation hypotheses and the corresponding manually simplified sentences as simplification/translation references) are given in Table 2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9989203214645386}]}, {"text": "Note that those BLEU scores can be regarded as the baselines for the translation experiments, as they correspond to the BLEU score obtained when the systems do not perform any changes to the input.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9972266554832458}, {"text": "BLEU score obtained", "start_pos": 120, "end_pos": 139, "type": "METRIC", "confidence": 0.9708412289619446}]}, {"text": "The BLEU scores for all 40 experiments tested on the WikiTest dataset, are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9992098808288574}, {"text": "WikiTest dataset", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9876393973827362}]}, {"text": "The baseline BLEU score (when no simplification is performed) for this test set is 62.27).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9759008884429932}]}, {"text": "As shown in, none of the 40 experiments have even reached that baseline.", "labels": [], "entities": []}, {"text": "We compare S-BLEU scores for each pair of experiments (240 reference sentences in the test set and their corresponding automatically simplified sentences) using the paired t-test in SPSS in order to check whether the differences in the obtained results are significant.", "labels": [], "entities": [{"text": "SPSS", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.7213200926780701}]}, {"text": "The only results that are significantly lower than the rest are those obtained for the experiments in which the training and development datasets consist only of the sentence pairs with S-BLEU scores between 0 and 0.3.", "labels": [], "entities": []}, {"text": "The results sug- The rows represent intervals of the S-BLEU scores on the training and development datasets, while the columns represent the number of the sentence pairs used for training.", "labels": [], "entities": []}, {"text": "The highest score is presented in bold; the baseline (no simplification performed) is gest that the sizes of the training and development datasets do not influence the translation results significantly on any type of sentence pairs used.", "labels": [], "entities": []}, {"text": "The results of the experiments tested on EncBritTest () again show that the quantity of the training data does not influence system performance.", "labels": [], "entities": [{"text": "EncBritTest", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9547734260559082}]}, {"text": "There are no statistically significant differences (measured by the paired t-test on S-BLEU scores on all 601 reference sentences and the corresponding automatic simplifications) among experiments which differ only in the size of the training and development datasets.", "labels": [], "entities": []}, {"text": "However, the models trained and tuned on the datasets consisting of the sentence pairs with the highest and the lowest S-BLEU scores ([0,0.3] and (0.9,1]) perform significantly worse than the models trained and tuned on the sentence pairs with S-BLEU scores belonging to other intervals.", "labels": [], "entities": []}, {"text": "The results presented in indicate that the BLEU score, in MT-based text simplification, mostly reflects the surface similarity of the original and simplified sentences in the test set and does not give an informative evaluation of the systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9987309575080872}, {"text": "MT-based text simplification", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.8831678032875061}]}, {"text": "Therefore, we conducted a human assessment of the generated sentences.", "labels": [], "entities": []}, {"text": "Following the standard procedure for human evaluation of TS systems used in previous studies, three human evaluators were asked to assess the generated sentences on a 1-5 scale (where the higher mark always denotes better output) according to three cri- The rows represent intervals of the S-BLEU scores on the training and development datasets, while the columns represent the number of the sentence pairs used for training.", "labels": [], "entities": []}, {"text": "The highest score is presented in bold; the baseline (no simplification performed) is 12.40.", "labels": [], "entities": [{"text": "baseline", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9732289910316467}]}, {"text": "teria: grammaticality (G), meaning preservation (M), and simplicity (S).", "labels": [], "entities": [{"text": "simplicity (S)", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.929687887430191}]}, {"text": "We decided that the same person has to rate all simplified versions of the same original sentence (shown always in a random order), in order to make a fairer comparison among the systems.", "labels": [], "entities": []}, {"text": "That decision, however, limited the number of systems we can evaluate.", "labels": [], "entities": []}, {"text": "Therefore, we focused only on six out of 40 trained systems.", "labels": [], "entities": []}, {"text": "Several examples of the automatically simplified sentences and their scores are presented in.", "labels": [], "entities": []}, {"text": "The results of the human evaluation are given in.", "labels": [], "entities": []}, {"text": "It seems that the use of the sentence pairs with the S-BLEU score between 0.5 and 0.6 leads to the best system performances in terms of grammaticality and meaning preservation, while at the same time improving the simplicity of the sentences.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.714124009013176}, {"text": "simplicity", "start_pos": 214, "end_pos": 224, "type": "METRIC", "confidence": 0.9784185290336609}]}, {"text": "5 Furthermore, the differences inhuman scores between the systems differing only in size of the datasets used were not statistically significant.", "labels": [], "entities": []}, {"text": "At the same time, the differences inhuman S-03-1000 Madrid was occupied by French troops during the Napoleonic Wars, and Napoleon's brother Joseph was put on the throne.", "labels": [], "entities": [{"text": "S-03-1000 Madrid", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.7152040898799896}]}, {"text": "5 5 5 S-10-1000 Madrid was occupied by French troops during the Napoleonic Wars, and Napoleon's brother Joseph was -RRB-installed on them on the throne.", "labels": [], "entities": [{"text": "RRB-installed", "start_pos": 116, "end_pos": 129, "type": "METRIC", "confidence": 0.9262902736663818}]}, {"text": "The columns G, M, S contain the mean value of the human scores for grammaticality, meaning preservation, and simplicity, respectively.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7258549332618713}, {"text": "simplicity", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.997809112071991}]}, {"text": "Differences to the original versions are shown in italics.", "labels": [], "entities": []}, {"text": "Systems which are not presented did not make any changes to these two original sentences.", "labels": [], "entities": []}, {"text": "The mean value of the human scores for grammaticality (G), meaning preservation (M), and simplicity (S).", "labels": [], "entities": [{"text": "meaning preservation (M)", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.6563409447669983}, {"text": "simplicity (S)", "start_pos": 89, "end_pos": 103, "type": "METRIC", "confidence": 0.9337756931781769}]}, {"text": "The highest achieved scores (excluding the scores for original sentences) on each aspect (G, M, and S) are presented in bold.", "labels": [], "entities": []}, {"text": "scores between the systems differing only in similarity of the sentence pairs (the interval of the S-BLEU score) used were statistically significant.", "labels": [], "entities": [{"text": "similarity", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.96357661485672}]}], "tableCaptions": [{"text": " Table 2: Test sets for all translation experiments", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores on the WikiTest dataset", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993237257003784}, {"text": "WikiTest dataset", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9536333978176117}]}, {"text": " Table 4: BLEU scores on the EncBritTest dataset", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990264177322388}, {"text": "EncBritTest dataset", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9803280234336853}]}, {"text": " Table 6: Outputs of different systems and their human evaluation scores", "labels": [], "entities": []}, {"text": " Table 7: Results of the human evaluation  System  G  M  S  Original  4.85  /  2.60  S-03-200  4.03  3.95  2.57  S-03-1000  4.20  4.03  2.85  S-06-200  4.50  4.45  2.68  S-06-1000  4.43  4.48  2.72  S-10-200  3.25  2.92  2.45  S-10-1000  2.92  2.95  2.53", "labels": [], "entities": []}]}