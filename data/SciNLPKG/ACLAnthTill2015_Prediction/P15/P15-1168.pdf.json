{"title": [{"text": "Gated Recursive Neural Network for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6029611627260844}]}], "abstractContent": [{"text": "Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering.", "labels": [], "entities": []}, {"text": "However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features.", "labels": [], "entities": []}, {"text": "In this paper, we propose a gated recursive neural network (GRNN) for Chinese word segmen-tation, which contains reset and update gates to incorporate the complicated combinations of the context characters.", "labels": [], "entities": []}, {"text": "Since GRNN is relative deep, we also use a supervised layer-wise training method to avoid the problem of gradient diffusion.", "labels": [], "entities": [{"text": "gradient diffusion", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7215263843536377}]}, {"text": "Experiments on the benchmark datasets show that our model outperforms the previous neural network models as well as the state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unlike English and other western languages, Chinese do not delimit words by white-space.", "labels": [], "entities": []}, {"text": "Therefore, word segmentation is a preliminary and important pre-process for Chinese language processing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7596756815910339}, {"text": "Chinese language processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.694512665271759}]}, {"text": "Most previous systems address this problem by treating this task as a sequence labeling problem and have achieved great success.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7392136553923289}]}, {"text": "Due to the nature of supervised learning, the performance of these models is greatly affected by the design of features.", "labels": [], "entities": []}, {"text": "These features are explicitly represented by the different combinations of context characters, which are based on linguistic intuition and statistical information.", "labels": [], "entities": []}, {"text": "However, the number of features could be so large that the result models are too large to use in practice and prone to overfit on training corpus.: Illustration of our model for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 178, "end_pos": 203, "type": "TASK", "confidence": 0.5949373344580332}]}, {"text": "The solid nodes indicate the active neurons, while the hollow ones indicate the suppressed neurons.", "labels": [], "entities": []}, {"text": "Specifically, the links denote the information flow, where the solid edges denote the acceptation of the combinations while the dashed edges means rejection of that.", "labels": [], "entities": []}, {"text": "As shown in the right figure, we receive a score vector for tagging target character \"\u5730\" by incorporating all the combination information.", "labels": [], "entities": []}, {"text": "Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.791367381811142}]}, {"text": "developed a general neural network architecture for sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.8083249926567078}]}, {"text": "Following this work, many methods () applied the neural network to Chinese word segmentation and achieved a performance that approaches the state-of-the-art methods.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6000610788663229}]}, {"text": "However, these neural models just concatenate the embeddings of the context characters, and feed them into neural network.", "labels": [], "entities": []}, {"text": "Since the concatenation operation is relatively simple, it is difficult to model the complicated features as the traditional discrete feature based models.", "labels": [], "entities": []}, {"text": "Although the complicated interactions of inputs can be modeled by the deep neural network, the previous neural model shows that the deep model cannot outperform the one with a single non-linear model.", "labels": [], "entities": []}, {"text": "Therefore, the neural model only captures the interactions by the simple transition matrix and the single non-linear transformation . These dense features extracted via these simple interactions are not nearly as good as the substantial discrete features in the traditional methods.", "labels": [], "entities": []}, {"text": "In this paper, we propose a gated recursive neural network (GRNN) to model the complicated combinations of characters, and apply it to Chinese word segmentation task.", "labels": [], "entities": [{"text": "Chinese word segmentation task", "start_pos": 135, "end_pos": 165, "type": "TASK", "confidence": 0.6865686550736427}]}, {"text": "Inspired by the success of gated recurrent neural network), we introduce two kinds of gates to control the combinations in recursive structure.", "labels": [], "entities": []}, {"text": "We also use the layer-wise training method to avoid the problem of gradient diffusion, and the dropout strategy to avoid the overfitting problem.", "labels": [], "entities": [{"text": "gradient diffusion", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7310918271541595}]}, {"text": "gives an illustration of how our approach models the complicated combinations of the context characters.", "labels": [], "entities": []}, {"text": "Given a sentence \"\u96e8 (Rainy) \u5929 (Day) \u5730\u9762 (Ground) \u79ef\u6c34 (Accumulated water)\", the target character is \"\u5730\".", "labels": [], "entities": []}, {"text": "This sentence is very complicated because each consecutive two characters can be combined as a word.", "labels": [], "entities": []}, {"text": "To predict the label of the target character \"\u5730\" under the given context, GRNN detects the combinations recursively from the bottom layer to the top.", "labels": [], "entities": [{"text": "GRNN", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.6980477571487427}]}, {"text": "Then, we receive a score vector of tags by incorporating all the combination information in network.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summarized as follows: \u2022 We propose a novel GRNN architecture to model the complicated combinations of the context characters.", "labels": [], "entities": []}, {"text": "GRNN can select and preserve the useful combinations via reset and update gates.", "labels": [], "entities": [{"text": "GRNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8314124345779419}]}, {"text": "These combinations play a similar role in the feature engineering of the traditional methods with discrete features.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.754319429397583}]}, {"text": "\u2022 We evaluate the performance of Chinese word segmentation on PKU, MSRA and CTB6 benchmark datasets which are commonly used for evaluation of Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.5904131829738617}, {"text": "PKU", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9235118627548218}, {"text": "MSRA", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8602383136749268}, {"text": "CTB6 benchmark datasets", "start_pos": 76, "end_pos": 99, "type": "DATASET", "confidence": 0.8692103226979574}, {"text": "Chinese word segmentation", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.5987253685792288}]}, {"text": "Experiment results show that our model outperforms other neural network models, and achieves state-of-the-art performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on two different kinds of texts: newswire texts and micro-blog texts.", "labels": [], "entities": []}, {"text": "For evaluation, we use the standard Bakeoff scoring program to calculate precision, recall, F1-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.99949049949646}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.999433696269989}, {"text": "F1-score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9981871247291565}]}, {"text": "We use three popular datasets, PKU, MSRA and CTB6, to evaluate our model on newswire texts.", "labels": [], "entities": [{"text": "PKU", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.857745885848999}, {"text": "MSRA", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.6168740391731262}, {"text": "CTB6", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7521905899047852}]}, {"text": "The PKU and MSRA data are provided by the second International Chinese Word Segmentation Bakeoff (, and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36)), which is a segmented, part-of-speech tagged, and fully bracketed corpus in the constituency formalism.", "labels": [], "entities": [{"text": "MSRA data", "start_pos": 12, "end_pos": 21, "type": "DATASET", "confidence": 0.8364408612251282}, {"text": "International Chinese Word Segmentation Bakeoff", "start_pos": 49, "end_pos": 96, "type": "DATASET", "confidence": 0.8602889180183411}, {"text": "Chinese TreeBank 6.0 (LDC2007T36", "start_pos": 117, "end_pos": 149, "type": "DATASET", "confidence": 0.9160999774932861}]}, {"text": "These datasets are commonly used by previous state-of-the-art models and neural network models.", "labels": [], "entities": []}, {"text": "In addition, we use the first 90% sentences of the training data as training set and the rest 10% sentences as development set for PKU and MSRA datasets, and we divide the training, development and test sets according to) for the CTB6 dataset.", "labels": [], "entities": [{"text": "PKU", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.8751893639564514}, {"text": "MSRA datasets", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.8214269280433655}, {"text": "CTB6 dataset", "start_pos": 230, "end_pos": 242, "type": "DATASET", "confidence": 0.9840584397315979}]}, {"text": "All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of different models with or without layer-wise training strategy on PKU test set.", "labels": [], "entities": [{"text": "PKU test set", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9389258225758871}]}, {"text": " Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.", "labels": [], "entities": [{"text": "PKU", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9243267178535461}, {"text": "MSRA and CTB6 test sets", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.7575952351093292}]}, {"text": " Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.", "labels": [], "entities": [{"text": "PKU", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9508621692657471}, {"text": "MSRA and CTB6 test sets", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.7819501280784606}]}, {"text": " Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.", "labels": [], "entities": [{"text": "GRNN", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.8522412180900574}, {"text": "PKU", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.947957456111908}, {"text": "MSRA and CTB6 test  sets", "start_pos": 72, "end_pos": 96, "type": "DATASET", "confidence": 0.7758714199066162}]}, {"text": " Table 6: Statistical information of NLPCC 2015 dataset.", "labels": [], "entities": [{"text": "NLPCC 2015 dataset", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.9384859800338745}]}, {"text": " Table 7: Templates of CRF++ and FNLP.", "labels": [], "entities": [{"text": "FNLP", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8469996452331543}]}, {"text": " Table 8: Performances on NLPCC 2015 dataset.", "labels": [], "entities": [{"text": "NLPCC 2015 dataset", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.9620268940925598}]}]}