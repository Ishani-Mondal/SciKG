{"title": [{"text": "Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding", "labels": [], "entities": []}], "abstractContent": [{"text": "Spoken dialogue systems (SDS) typically require a predefined semantic ontology to train a spoken language understanding (SLU) module.", "labels": [], "entities": [{"text": "Spoken dialogue systems (SDS)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7889913121859232}]}, {"text": "In addition to the annotation cost, a key challenge for designing such an ontology is to define a coherent slot set while considering their complex relations.", "labels": [], "entities": []}, {"text": "This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utterances and semantic elements without the need of corpus annotations.", "labels": [], "entities": []}, {"text": "Specifically, our model learns the semantic slots fora domain-specific SDS in an unsupervised fashion, and carries out semantic parsing using latent MF techniques.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.7137209922075272}]}, {"text": "To further consider the global semantic structure , such as inter-word and inter-slot relations , we augment the latent MF-based model with a knowledge graph propagation model based on a slot-based semantic graph and a word-based lexical graph.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner.", "labels": [], "entities": [{"text": "MF", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9476956725120544}]}], "introductionContent": [{"text": "A key component of a spoken dialogue system (SDS) is the spoken language understanding (SLU) module-it parses the users' utterances into semantic representations; for example, the utterance \"find a cheap restaurant\" can be parsed into (price=cheap, target=restaurant) (.", "labels": [], "entities": [{"text": "spoken language understanding (SLU) module-it parses the users' utterances into semantic representations", "start_pos": 57, "end_pos": 161, "type": "TASK", "confidence": 0.7706119631017957}]}, {"text": "To design the SLU module of a SDS, most previous studies relied on predefined slots 1 for training the decoder).", "labels": [], "entities": []}, {"text": "However, these predefined semantic slots may bias the subsequent data collection process, and the cost of manually labeling utterances for updating the ontology is expensive ( . In recent years, this problem led to the development of unsupervised SLU techniques).", "labels": [], "entities": []}, {"text": "In particular,  proposed a frame-semantics based framework for automatically inducing semantic slots given raw audios.", "labels": [], "entities": []}, {"text": "However, these approaches generally do not explicitly learn the latent factor representations to model the measurement errors), nor do they jointly consider the complex lexical, syntactic, and semantic relations among words, slots, and utterances.", "labels": [], "entities": []}, {"text": "Another challenge of SLU is the inference of the hidden semantics.", "labels": [], "entities": []}, {"text": "Considering the user utterance \"can i have a cheap restaurant\", from its surface patterns, we can see that it includes explicit semantic information about \"price (cheap)\" and \"target (restaurant)\"; however, it also includes hidden semantic information, such as \"food\" and \"seeking\", since the SDS needs to infer that the user wants to \"find\" some cheap \"food\", even though they are not directly observed in the surface patterns.", "labels": [], "entities": []}, {"text": "Nonetheless, these implicit semantics are important semantic concepts for domainspecific SDSs.", "labels": [], "entities": []}, {"text": "Traditional SLU models use discriminative classifiers) to predict whether the predefined slots occur in the utterances or not, ignoring the unobserved concepts and the hidden semantic information.", "labels": [], "entities": []}, {"text": "In this paper, we take a rather radical approach: we propose a novel matrix factorization (MF) model for learning latent features for SLU, taking account of additional information such as the word relations, the induced slots, and the slot relations.", "labels": [], "entities": []}, {"text": "To further consider the global coherence of induced slots, we combine the MF model with a knowledge graph propagation based model, fusing both a word-based lexical knowledge graph and a slot-based semantic graph.", "labels": [], "entities": []}, {"text": "In fact, as it is shown in the Netflix challenge, MF is credited as the most useful technique for recommendation systems ().", "labels": [], "entities": [{"text": "MF", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9633381366729736}]}, {"text": "Also, the MF model considers the unobserved patterns and estimates their probabilities instead of viewing them as negative examples.", "labels": [], "entities": [{"text": "MF", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.81471848487854}]}, {"text": "However, to the best of our knowledge, the MF technique is not yet well understood in the SLU and SDS communities, and it is not very straight-forward to use MF methods to learn latent feature representations for semantic parsing in SLU.", "labels": [], "entities": [{"text": "MF", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9561482667922974}, {"text": "semantic parsing", "start_pos": 213, "end_pos": 229, "type": "TASK", "confidence": 0.7342541515827179}]}, {"text": "To evaluate the performance of our model, we compare it to standard discriminative SLU baselines, and show that our MF-based model is able to produce strong results in semantic decoding, and the knowledge graph propagation model further improves the performance.", "labels": [], "entities": [{"text": "knowledge graph propagation", "start_pos": 195, "end_pos": 222, "type": "TASK", "confidence": 0.6206652124722799}]}, {"text": "Our contributions are three-fold: \u2022 We are among the first to study matrix factorization techniques for unsupervised SLU, taking account of additional information; \u2022 We augment the MF model with a knowledge graph propagation model, increasing the global coherence of semantic decoding using induced slots; \u2022 Our experimental results show that the MFbased unsupervised SLU outperforms strong discriminative baselines, obtaining promising results.", "labels": [], "entities": [{"text": "knowledge graph propagation", "start_pos": 197, "end_pos": 224, "type": "TASK", "confidence": 0.6522978643576304}]}, {"text": "In the next section, we outline the related work in unsupervised SLU and latent variable modeling for spoken language processing.", "labels": [], "entities": [{"text": "latent variable modeling", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6471658249696096}, {"text": "spoken language processing", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6067213813463846}]}, {"text": "Section 3 introduces our framework.", "labels": [], "entities": []}, {"text": "The detailed MF approach is explained in Section 4.", "labels": [], "entities": [{"text": "MF", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9915812611579895}]}, {"text": "We then introduce the global knowledge graphs for MF in Section 5.", "labels": [], "entities": [{"text": "MF", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9501554369926453}]}, {"text": "Section 6 shows the experimental results, and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we used the Cambridge University SLU corpus, previously used on several other SLU tasks ().", "labels": [], "entities": [{"text": "Cambridge University SLU corpus", "start_pos": 32, "end_pos": 63, "type": "DATASET", "confidence": 0.9600711464881897}]}, {"text": "The domain of the corpus is about restaurant recommendation in Cambridge; subjects were asked to interact with multiple SDSs in an in-car setting.", "labels": [], "entities": [{"text": "restaurant recommendation in Cambridge", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.7157479524612427}]}, {"text": "The corpus contains a total number of 2,166 dialogues, including 15,453 utterances (10,571 for self-training and 4,882 for  testing).", "labels": [], "entities": []}, {"text": "The data is gender-balanced, with slightly more native than non-native speakers.", "labels": [], "entities": []}, {"text": "The vocabulary size is 1868.", "labels": [], "entities": []}, {"text": "An ASR system was used to transcribe the speech; the word error rate was reported as 37%.", "labels": [], "entities": [{"text": "ASR", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.7093944549560547}, {"text": "word error rate", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.8244756460189819}]}, {"text": "There are 10 slots created by domain experts: addr, area, food, name, phone, postcode, price range, signature, task, and type.", "labels": [], "entities": []}, {"text": "For parameter setting, the weights for balancing feature models and propagation models, \u03b1 and \u03b2, are set as 0.5 to give the same influence, and the threshold for defining the unobserved facts \u03b4 is set as 0.5 for all experiments.", "labels": [], "entities": []}, {"text": "We use the Stanford Parser to obtain the collapsed typed syntactic dependencies and set the dimensionality of embeddings d = 300 in all experiments.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.86549112200737}]}, {"text": "To evaluate the accuracy of the automatically decoded slots, we measure their quality as the proximity between predicted slots and reference slots.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9988161325454712}]}, {"text": "shows the mappings that indicate semantically related induced slots and reference slots . To eliminate the influence of threshold selection when predicting semantic slots, in the following http://nlp.stanford.edu/software/lex-parser.", "labels": [], "entities": []}, {"text": "shtml metrics, we take the whole ranking list into account and evaluate the performance by the metrics that are independent of the selected threshold.", "labels": [], "entities": []}, {"text": "For each utterance, with the predicted probabilities of all slot candidates, we can compute an average precision (AP) to evaluate the performance of SLU by treating the slots with mappings as positive.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 95, "end_pos": 117, "type": "METRIC", "confidence": 0.8249283194541931}]}, {"text": "AP scores the ranking result higher if the correct slots are ranked higher, which also approximates to the area under the precision-recall curve.", "labels": [], "entities": [{"text": "AP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5354300737380981}, {"text": "precision-recall", "start_pos": 122, "end_pos": 138, "type": "METRIC", "confidence": 0.9935368895530701}]}, {"text": "Mean average precision (MAP) is the metric for evaluating all utterances.", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.9595605532328287}]}, {"text": "For all experiments, we perform a paired t-test on the AP scores of the results to test the significance.", "labels": [], "entities": [{"text": "AP", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9670981168746948}]}, {"text": "shows the MAP performance of predicted slots for all experiments on ASR and manual transcripts.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8487585783004761}, {"text": "ASR", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9779279232025146}]}, {"text": "For the first baseline using explicit semantics, we use the observed data to self-train models for predicting the probability of each semantic slot by support vector machine (SVM) with a linear kernel and multinomial logistic regression (MLR) (row (a)-(b)).", "labels": [], "entities": []}, {"text": "It is shown that SVM and MLR perform similarly, and MLR is slightly better than SVM because it has better capability of estimating probabilities.", "labels": [], "entities": []}, {"text": "For modeling implicit semantics, two baselines are performed as references, Random (row (c)) and Majority (row (d)), where the former assigns random probabilities for all slots, and the later assigns probabilities for the slots based on their frequency distribution.", "labels": [], "entities": []}, {"text": "To improve probability estimation, we further integrate the results from implicit semantics with the better result from explicit approaches, MLR (row (b)), by averaging the probability distribution from two results.", "labels": [], "entities": [{"text": "probability estimation", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7415631711483002}, {"text": "MLR", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.6509072780609131}]}, {"text": "Two baselines, Random and Majority, cannot model the implicit semantics, producing poor results.", "labels": [], "entities": []}, {"text": "The results of Random integrated with MLR significantly degrades the performance of: The MAP of predicted slots using different types of relation models in M R (%); \u2020 indicates that the result is significantly better than the feature model (column (a)) with p < 0.05 in t-test.", "labels": [], "entities": [{"text": "MAP", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9990010857582092}]}], "tableCaptions": [{"text": " Table 2: The MAP of predicted slots (%);  \u2020 indicates that the result is significantly better than the Logistic  Regression (row (b)) with p < 0.05 in t-test.", "labels": [], "entities": [{"text": "MAP", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9989713430404663}, {"text": "Regression", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.46474704146385193}]}]}