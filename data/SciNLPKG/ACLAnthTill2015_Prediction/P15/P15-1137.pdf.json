{"title": [{"text": "Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution", "labels": [], "entities": [{"text": "Antecedent", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9752033948898315}, {"text": "Coreference Resolution", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.9613452851772308}]}], "abstractContent": [{"text": "We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.977689653635025}, {"text": "anaphoricity detection", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7749422490596771}]}, {"text": "Although we use only simple, unconjoined features, the model is able to learn useful representations , and we report the best overall score on the CoNLL 2012 English test set to date.", "labels": [], "entities": [{"text": "CoNLL 2012 English test", "start_pos": 147, "end_pos": 170, "type": "DATASET", "confidence": 0.9356992542743683}]}], "introductionContent": [{"text": "One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric -that is, that are not coreferent with any previous mention -far exceeds the number of mentions that are anaphoric.", "labels": [], "entities": [{"text": "resolving coreference", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7949322462081909}]}, {"text": "This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9525197446346283}]}, {"text": "Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features () and, as a result, state-of-the-art systems tend to use linear models with complicated feature conjunction schemes in order to capture more fine-grained interactions.", "labels": [], "entities": []}, {"text": "While this approach has shown success, it is not obvious which additional feature conjunctions will lead to improved performance, which is problematic as systems attempt to scale with new data and features.", "labels": [], "entities": []}, {"text": "In this work, we propose a data-driven model for coreference that does not require prespecifying any feature relationships.", "labels": [], "entities": [{"text": "coreference", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9753845930099487}]}, {"text": "Inspired by recent work in learning representations for natural language tasks), we explore neural network models which take only raw, unconjoined features as input, and attempt to learn intermediate representations automatically.", "labels": [], "entities": []}, {"text": "In particular, the model we describe attempts to create independent feature representations useful for both detecting the anaphoricity of a mention (that is, whether or not a mention is anaphoric) and ranking the potential antecedents of an anaphoric mention.", "labels": [], "entities": []}, {"text": "Adequately capturing anaphoricity information has long been thought to bean important aspect of the coreference task (see and Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent fora non-anaphoric mention even in the presence of a misleading head match.", "labels": [], "entities": [{"text": "coreference task", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.9010727405548096}]}, {"text": "We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model's weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions.", "labels": [], "entities": [{"text": "anaphoricity detection", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.666136771440506}]}, {"text": "Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test set (), and of over 1.5 points over the state-of-the-art coreference system.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.5896017253398895}, {"text": "CoNLL 2012 English test set", "start_pos": 133, "end_pos": 160, "type": "DATASET", "confidence": 0.9670291423797608}]}, {"text": "Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments examine performance as compared with other coreference systems, as well as the effect of features, pre-training, and model architecture.", "labels": [], "entities": []}, {"text": "We also perform a qualitative comparison of our model with the analogous linear model on some challenging non-anaphoric cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the two subtasks on the CoNLL 2012  development set by feature set and model type. \"Conj.\" indi- cates whether conjunctions are used. The linear anaphoric  system is an SVM (LibLinear implementation (Fan et al.,  2008)), and the linear antecedent system is a linear model  with the margin-based objective.", "labels": [], "entities": [{"text": "CoNLL 2012  development set", "start_pos": 49, "end_pos": 76, "type": "DATASET", "confidence": 0.9613673985004425}]}, {"text": " Table 2: Results on CoNLL 2012 English test set. We compare against recent state-of-the-art systems, including (in order)  Durrett and Klein (2013), Ma et al. (2014), Bj\u00f6rkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01  scorer). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&K and  D&K for all metrics.", "labels": [], "entities": [{"text": "CoNLL 2012 English test set", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.9606005787849426}, {"text": "F1", "start_pos": 260, "end_pos": 262, "type": "METRIC", "confidence": 0.9991801381111145}, {"text": "B", "start_pos": 363, "end_pos": 364, "type": "METRIC", "confidence": 0.9551938772201538}]}, {"text": " Table 3: F1 performance comparison between state-of-the-art  linear mention-ranking model (Durrett and Klein, 2013) and  our full models on CoNLL 2012 development set for different  feature sets.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9879827499389648}, {"text": "CoNLL 2012 development set", "start_pos": 141, "end_pos": 167, "type": "DATASET", "confidence": 0.9528817236423492}]}, {"text": " Table 4: Comparison of performance (in F1 score) of vari- ous models on CoNLL 2012 development set using BASIC+  features. \"PT\" and \"RI\" refer to pretraining and random ini- tialization respectively. \"Fully Conn.\" refers to baseline fully  connected networks. See text for further model descriptions.", "labels": [], "entities": [{"text": "F1 score)", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9694351355234782}, {"text": "CoNLL 2012 development set", "start_pos": 73, "end_pos": 99, "type": "DATASET", "confidence": 0.9682454019784927}, {"text": "PT", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9932860732078552}, {"text": "RI", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.5151254534721375}]}, {"text": " Table 5: Absolute error counts from the coreference analysis  tool of Kummerfeld and Klein (2013). The upper set roughly  corresponds to the precision and the lower to the recall of the  coreference clusters produced by the model.", "labels": [], "entities": [{"text": "Absolute error counts", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9226058721542358}, {"text": "coreference analysis", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.9189726114273071}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.99903404712677}, {"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9982602000236511}]}, {"text": " Table 6: Errors made by NN (g 1 ) (top) and NN (g 2 ) (bottom)  on CoNLL 2012 English development data. Rows correspond  to", "labels": [], "entities": [{"text": "Errors", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9877968430519104}, {"text": "CoNLL 2012 English development data", "start_pos": 68, "end_pos": 103, "type": "DATASET", "confidence": 0.970452880859375}]}]}