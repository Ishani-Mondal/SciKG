{"title": [{"text": "Efficient Methods for Inferring Large Sparse Topic Hierarchies", "labels": [], "entities": [{"text": "Inferring Large Sparse Topic Hierarchies", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.7426180362701416}]}], "abstractContent": [{"text": "Latent variable topic models such as Latent Dirichlet Allocation (LDA) can discover topics from text in an unsupervised fashion.", "labels": [], "entities": []}, {"text": "However, scaling the models up to the many distinct topics exhibited in modern corpora is challenging.", "labels": [], "entities": []}, {"text": "\"Flat\" topic models like LDA have difficulty modeling sparsely expressed topics, and richer hierarchical models become compu-tationally intractable as the number of topics increases.", "labels": [], "entities": []}, {"text": "In this paper, we introduce efficient methods for inferring large topic hierarchies.", "labels": [], "entities": []}, {"text": "Our approach is built upon the Sparse Backoff Tree (SBT), anew prior for latent topic distributions that organizes the latent topics as leaves in a tree.", "labels": [], "entities": []}, {"text": "We show how a document model based on SBTs can effectively infer accurate topic spaces of over a million topics.", "labels": [], "entities": []}, {"text": "We introduce a collapsed sampler for the model that exploits sparsity and the tree structure in order to make inference efficient.", "labels": [], "entities": []}, {"text": "In experiments with multiple data sets, we show that scaling to large topic spaces results in much more accurate models, and that SBT document models make use of large topic spaces more effectively than flat LDA.", "labels": [], "entities": []}], "introductionContent": [{"text": "Latent variable topic models, such as Latent Dirichlet Allocation (, are popular approaches for automatically discovering topics in document collections.", "labels": [], "entities": []}, {"text": "However, learning models that capture the large numbers of distinct topics expressed in today's corpora is challenging.", "labels": [], "entities": []}, {"text": "While efficient methods for learning large topic models have been developed (, these methods have focused on \"flat\" topic models such as LDA.", "labels": [], "entities": []}, {"text": "Flat topic models overlarge topic spaces are prone to overfitting: even in a Web-scale corpus, some words are expressed rarely, and many documents are brief.", "labels": [], "entities": []}, {"text": "Inferring a large topic distribution for each word and document given such sparse data is challenging.", "labels": [], "entities": []}, {"text": "As a result, LDA models in practice tend to consider a few thousand topics at most, even when training on billions of words (.", "labels": [], "entities": []}, {"text": "A promising alternative to flat topic models is found in recent hierarchical topic models;).", "labels": [], "entities": []}, {"text": "Topics of words and documents can be naturally arranged into hierarchies.", "labels": [], "entities": []}, {"text": "For example, an article on the topic of the Chicago Bulls is also relevant to the more general topics of NBA, Basketball, and Sports.", "labels": [], "entities": []}, {"text": "Hierarchies can combat data sparsity: if data is too sparse to place the term \"Pau Gasol\" within the Chicago Bulls topic, perhaps it can be appropriately modeled at somewhat less precision within the Basketball topic.", "labels": [], "entities": []}, {"text": "A hierarchical model can make fine-grained distinctions where data is plentiful, and back-off to more coarse-grained distinctions where data is sparse.", "labels": [], "entities": []}, {"text": "However, current hierarchical models are hindered by computational complexity.", "labels": [], "entities": []}, {"text": "The existing inference methods for the models have runtimes that increase at least linearly with the number of topics, making them intractable on large corpora with large numbers of topics.", "labels": [], "entities": []}, {"text": "In this paper, we present a hierarchical topic model that can scale to large numbers of distinct topics.", "labels": [], "entities": []}, {"text": "Our approach is built upon anew prior for latent topic distributions called a Sparse Backoff Tree (SBT).", "labels": [], "entities": []}, {"text": "SBTs organize the latent topics as leaves in a tree, and smooth the distributions for each topic with those of similar topics nearby in the tree.", "labels": [], "entities": []}, {"text": "SBT priors use absolute discounting and learned backoff distributions for smoothing sparse observation counts, rather than the fixed additive discounting utilized in Dirichlet and Chinese Restaurant Process models.", "labels": [], "entities": [{"text": "SBT priors", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.827547013759613}]}, {"text": "We show how the SBT's characteristics enable a novel collapsed sampler that exploits the tree structure for efficiency, allowing SBT-based document models (SBTDMs) that scale to hierarchies of over a million topics.", "labels": [], "entities": []}, {"text": "We perform experiments in text modeling and hyperlink prediction, and find that SBTDM is more accurate compared to LDA and the recent nested Hierarchical Dirichlet Process (nHDP) (.", "labels": [], "entities": [{"text": "text modeling", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.7890717089176178}, {"text": "hyperlink prediction", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7852955758571625}]}, {"text": "For example, SBTDMs with a hundred thousand topics achieve perplexities 28-52% lower when compared with a standard LDA configuration using 1,000 topics.", "labels": [], "entities": []}, {"text": "We verify that the empirical time complexity of inference in SBTDM increases sub-linearly in the number of topics, and show that for large topic spaces SBTDM is more than an order of magnitude more efficient than the hierarchical Pachinko Allocation Model ( and nHDP.", "labels": [], "entities": []}, {"text": "Lastly, we release an implementation of SBTDM as open-source software.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate the efficiency and accuracy of SBTDM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9996659755706787}]}, {"text": "We evaluate SBTs on two data sets, the RCV1 Reuters corpus of newswire text (), and a distinct data set of Wikipedia links, WPL.", "labels": [], "entities": [{"text": "SBTs", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9493072032928467}, {"text": "RCV1 Reuters corpus of newswire text", "start_pos": 39, "end_pos": 75, "type": "DATASET", "confidence": 0.9540498654047648}, {"text": "WPL", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.9135494828224182}]}, {"text": "We consider two disjoint subsets of RCV1, a small training set (RCV1s) and a larger training set (RCV1).", "labels": [], "entities": []}, {"text": "We compare the accuracy and efficiency of SBTDM against flat LDA and two existing hierarchical models, the Pachinko Allocation Model (PAM) and nested Hierarchical Dirichlet Process (nHDP).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9992654919624329}, {"text": "SBTDM", "start_pos": 42, "end_pos": 47, "type": "TASK", "confidence": 0.8898670673370361}]}, {"text": "To explore how the SBT structure impacts modeling performance, we experiment with two different SBTDM configurations.", "labels": [], "entities": []}, {"text": "SBTDM-wide is a shallow tree in which the branching factor increases from the root downward in the sequence 3, 6, 6, 9, 9, 12, 12.", "labels": [], "entities": []}, {"text": "Thus, the largest model we consider has 3 \u00b7 6 \u00b7 6 \u00b7 9 \u00b7 9 \u00b7 12 \u00b7 12 = 1,259,712 distinct latent topics.", "labels": [], "entities": []}, {"text": "SBTDM-tall has lower branching factors of either 2 or 3 (so in our evaluation its depth ranges from 3 to 15).", "labels": [], "entities": []}, {"text": "As in SBTDM-wide, in SBTDM-tall the lower branching factors occur toward the root of the tree.", "labels": [], "entities": []}, {"text": "We vary the number of topics by considering balanced subtrees of each model.", "labels": [], "entities": []}, {"text": "For nHDP, we use the same tree structures as in SBT-wide.", "labels": [], "entities": []}, {"text": "In preliminary experiments, using the tall structure in nHDP yielded similar accuracy but was somewhat slower.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993200302124023}]}, {"text": "Similar to our LDA implementation, SBTDM optimizes hyperparameter settings as sampling proceeds.", "labels": [], "entities": []}, {"text": "We use local beam search to choose new hyperparameters that maximize leave-oneout likelihood for the distributions P (Z|d) and P (Z|w) on the training data, evaluating the parameters against the current state of the sampler.", "labels": [], "entities": []}, {"text": "We trained all models by performing 100 sampling passes through the full training corpus (i.e., approximately 10 billion samples for RCV1, and 8 billion samples for WPL).", "labels": [], "entities": [{"text": "RCV1", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.9212815165519714}, {"text": "WPL", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.8022956848144531}]}, {"text": "We evaluate performance on held-out test sets of 998 documents for RCV1 (122,646 tokens), and 200 documents for WPL (84,610 tokens).", "labels": [], "entities": [{"text": "RCV1", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9605815410614014}, {"text": "WPL", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.8913977146148682}]}, {"text": "We use the left-to-right algorithm () over a randomized word order with 20 particles to compute perplexity.", "labels": [], "entities": []}, {"text": "We re-optimize the LDA hyperparameters at regular intervals during sampling.", "labels": [], "entities": []}, {"text": "We begin with experiments over a small corpus to highlight the efficiency advantages of SBTDM.: Statistics of the three training corpora.", "labels": [], "entities": []}, {"text": "As argued above, existing hierarchical models require inference that becomes expensive as the topic space increases in size.", "labels": [], "entities": []}, {"text": "We illustrate this by comparing our model with PAM and nHDP.", "labels": [], "entities": []}, {"text": "We also compare against a fast \"flat\" LDA implementation, SparseLDA, from the MALLET software package).", "labels": [], "entities": [{"text": "SparseLDA", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.7277804017066956}, {"text": "MALLET software package", "start_pos": 78, "end_pos": 101, "type": "DATASET", "confidence": 0.8659055630366007}]}, {"text": "For SBTDM we utilize a parallel inference approach, sampling all variables using a fixed estimate of the counts n, and then updating the counts after each full sampling pass (as in ().", "labels": [], "entities": [{"text": "SBTDM", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.8971679210662842}]}, {"text": "The SparseLDA and nHDP implementations are also parallel.", "labels": [], "entities": [{"text": "SparseLDA", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8802815675735474}]}, {"text": "All parallel methods use 15 threads.", "labels": [], "entities": []}, {"text": "The PAM implementation provided in MALLET is single-threaded.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.7401120066642761}]}, {"text": "Our efficiency measurements are shown in.", "labels": [], "entities": []}, {"text": "We plot the mean wall-clock time to perform 100 sampling passes over the RCV1s corpus, starting from randomly initialized models (i.e. without expansion in SBTDM).", "labels": [], "entities": [{"text": "RCV1s corpus", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9472655653953552}]}, {"text": "For the largest plotted topic sizes for PAM and nHDP, we estimate total runtime using a small number of iterations.", "labels": [], "entities": []}, {"text": "The results show that SBTDM's time to sample increases well below linear in the number of topics.", "labels": [], "entities": [{"text": "SBTDM", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.8130068778991699}]}, {"text": "Both SBTDM methods have runtimes that increase at a rate substantially below that of the square root of the number of topics (plotted as a blue line in the figure for reference).", "labels": [], "entities": []}, {"text": "For the largest numbers of topics in the plot, when we increase the number of topics by a factor of 12, the time to sample increases by less than a factor of 1.7 for both SBT configurations.", "labels": [], "entities": []}, {"text": "We also evaluate the accuracy of the models on the small corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.999504804611206}]}, {"text": "We do not compare against PAM, as the MALLET implementation lacks a method for computing perplexity fora PAM model.", "labels": [], "entities": [{"text": "PAM", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8739718198776245}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The SBT models tend to achieve lower perplexity than LDA, and SBTDM-tall performs slightly better than SBTDM-wide for most topic sizes.", "labels": [], "entities": []}, {"text": "The best model, SBT-wide with 8,748 topics, achieves perplexity 14% lower than the best LDA model and 2% lower than the best SBTDM-tall model.", "labels": [], "entities": []}, {"text": "The LDA model overfits for the largest topic configuration, whereas at that size both SBT models remain at least as accurate as any of the LDA models in.", "labels": [], "entities": []}, {"text": "We also evaluated using the topic coherence measure from , which reflects how well the learned topics reflect word cooccurrence statistics in the training data.", "labels": [], "entities": []}, {"text": "Follow- ing recent experiments with the measure (), we use = 10 \u221212 pseudo-cooccurrences of each word pair and we evaluate the average coherence using the top 10 words for each topic.: Average topic coherence on the small RCV1s corpus.", "labels": [], "entities": [{"text": "RCV1s corpus", "start_pos": 222, "end_pos": 234, "type": "DATASET", "confidence": 0.9631391167640686}]}], "tableCaptions": [{"text": " Table 1: Statistics of the three training corpora.", "labels": [], "entities": []}, {"text": " Table 2: Average topic coherence on the small  RCV1s corpus.", "labels": [], "entities": [{"text": "RCV1s corpus", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9503628313541412}]}, {"text": " Table 4: Model accuracy on large corpora (cor- pus perplexity measure). The SBT model utilizes  larger numbers of topics more effectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9965131878852844}]}, {"text": " Table 3: Small training corpus (RCV1s) performance. Shown is perplexity averaged over three runs for  each method and number of topics, with standard deviation in parens. Both SBTDM models achieve  lower perplexity than LDA and nHDP for the larger numbers of topics.", "labels": [], "entities": []}]}