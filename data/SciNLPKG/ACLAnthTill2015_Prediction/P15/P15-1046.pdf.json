{"title": [{"text": "New Transfer Learning Techniques for Disparate Label Sets", "labels": [], "entities": []}], "abstractContent": [{"text": "In natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar).", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.8253380656242371}]}, {"text": "Standard domain adaptation techniques are not directly applicable to take advantage of the existing annotations because they assume that the label set is invariant.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7602322399616241}]}, {"text": "We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a number of transfer learning techniques.", "labels": [], "entities": []}, {"text": "We also introduce anew transfer learning technique based on pretrain-ing of hidden-unit CRFs (HUCRFs).", "labels": [], "entities": []}, {"text": "We perform extensive experiments on slot tagging on eight personal digital assistant domains and demonstrate that the proposed methods are superior to strong baselines.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9120098650455475}]}], "introductionContent": [{"text": "The main goal of NLU is to automatically extract the meaning of spoken or typed queries.", "labels": [], "entities": []}, {"text": "In recent years, this task has become increasingly important as more and more speech-based applications have emerged.", "labels": [], "entities": []}, {"text": "Recent releases of personal digital assistants such as Siri, Google Now, Dragon Go and Cortana in smart phones provide natural language based interface fora variety of domains (e.g. places, weather, communications, reminders).", "labels": [], "entities": []}, {"text": "The NLU in these domains are based on statistical machine learned models which require annotated training data.", "labels": [], "entities": []}, {"text": "Typically each domain has its own schema to annotate the words and queries.", "labels": [], "entities": []}, {"text": "However the meaning of words and utterances could be different in each domain.", "labels": [], "entities": []}, {"text": "For example, \"sunny\" is considered a weather condition in the weather domain but it maybe a song title in a music domain.", "labels": [], "entities": []}, {"text": "Thus every time anew application is developed or anew domain is built, a significant amount of resources is invested in creating annotations specific to that application or domain.", "labels": [], "entities": []}, {"text": "One might attempt to apply existing techniques) in domain adaption to this problem, but a straightforward application is not possible because these techniques assume that the label set is invariant.", "labels": [], "entities": [{"text": "domain adaption", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7123664766550064}]}, {"text": "In this work, we provide a simple and effective solution to this problem by abstracting the label types using the canonical correlation analysis (CCA) by) a powerful and flexible statistical technique for dimensionality reduction.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 205, "end_pos": 229, "type": "TASK", "confidence": 0.7342962175607681}]}, {"text": "We derive a low dimensional representation for each label type that is maximally correlated to the average context of that label via CCA.", "labels": [], "entities": []}, {"text": "These shared label representations, or label embeddings, allow us to map label types across different domains and reduce the setting to a standard domain adaptation problem.", "labels": [], "entities": []}, {"text": "After the mapping, we can apply the standard transfer learning techniques to solve the problem.", "labels": [], "entities": []}, {"text": "Additionally, we introduce a novel pretraining technique for hidden-unit CRFs (HUCRFs) to effectively transfer knowledge from one domain to another.", "labels": [], "entities": []}, {"text": "In our experiments, we find that our pretraining method is almost always superior to strong baselines such as the popular domain adaptation method of Daum\u00e9 III (2007).", "labels": [], "entities": [{"text": "Daum\u00e9 III (2007)", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.9020801305770874}]}], "datasetContent": [{"text": "In this section, we turn to experimental findings to provide empirical support for our proposed methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Size of number of label, labeled data set size and description for Alarm, Calendar, Communica- tion, Note, Ondevice, Places, Reminder and Weather domains partitioned into training and test set.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of slot F1 scores using  the proposed CCA-derived mapping versus other  mapping methods combined with different adap- tation techniques.", "labels": [], "entities": [{"text": "slot F1 scores", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.6753833691279093}]}, {"text": " Table 4: Slot F1 scores on each target domain using adapted models from the nearest source domain.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.6077005863189697}]}, {"text": " Table 5: Slot F1 scores of using Union, Daume, Coarse-to-Fine and pretraining on all pairs of source and  target data. The numbers in boldface are the best performing adaptation technique in each pair.", "labels": [], "entities": [{"text": "Slot F1 scores", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.7808494170506796}, {"text": "Union", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.992957353591919}, {"text": "pretraining", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9766895771026611}]}]}