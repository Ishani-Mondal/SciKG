{"title": [{"text": "Statistical Machine Translation Features with Multitask Tensor Networks", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7325918475786845}]}], "abstractContent": [{"text": "We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.8738862971464793}, {"text": "SMT", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9874140024185181}]}, {"text": "First, we propose new features based on neural networks to model various non-local translation phenomena.", "labels": [], "entities": []}, {"text": "Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units.", "labels": [], "entities": []}, {"text": "Third, we apply multitask learning to estimate the neural network parameters jointly.", "labels": [], "entities": []}, {"text": "Each of our proposed methods results in significant improvements that are complementary.", "labels": [], "entities": []}, {"text": "The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese-English translation over a state-of-the-art system that already includes neural network features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9982326030731201}]}], "introductionContent": [{"text": "Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.8355725010236105}]}, {"text": "They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (), or they implement the whole translation process as a single neural network ().", "labels": [], "entities": []}, {"text": "The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated * * Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.7256982525189718}, {"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9923338890075684}, {"text": "SMT", "start_pos": 160, "end_pos": 163, "type": "TASK", "confidence": 0.9736860394477844}, {"text": "BBN.", "start_pos": 318, "end_pos": 322, "type": "DATASET", "confidence": 0.9838966131210327}]}, {"text": "This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often suggest that these systems can still be significantly improved with better features.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.988976001739502}]}, {"text": "We build on) who proposed a simple yet powerful feedforward neural network model that estimates the translation probability conditioned on the target history and a large window of source word context.", "labels": [], "entities": []}, {"text": "We take advantage of neural networks' ability to handle sparsity, and to infer useful abstract representations automatically.", "labels": [], "entities": []}, {"text": "At the same time, we address the challenge of learning the large set of neural network parameters.", "labels": [], "entities": []}, {"text": "In particular, \u2022 We develop new Neural Network Features to model non-local translation phenomena related to word reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.7284894287586212}]}, {"text": "Large fullylexicalized contexts are used to model these phenomena effectively, making the use of neural networks essential.", "labels": [], "entities": []}, {"text": "All of the features are useful individually, and their combination results in significant improvements (Section 2).", "labels": [], "entities": []}, {"text": "\u2022 We use a Tensor Neural Network Architecture () to automatically learn complex pairwise interactions between the network nodes.", "labels": [], "entities": []}, {"text": "The introduction of the tensor hidden layer results in more powerful features with lower model perplexity and significantly improved MT performance for all of neural network features (Section 3).", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9081438183784485}]}, {"text": "\u2022 We apply Multitask Learning (MTL)) to jointly train related neural network features by sharing parameters.", "labels": [], "entities": []}, {"text": "This allows parameters learned for one feature to benefit the learning of the other features.", "labels": [], "entities": []}, {"text": "This results in better trained models and achieves additional MT improvements (Section 4).", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9239711165428162}]}, {"text": "We apply the resulting Multitask Tensor Networks to the new features and to existing ones, obtaining strong experimental results over the strongest previous results of).", "labels": [], "entities": []}, {"text": "We obtain improvements of +2.5 BLEU points for Arabic-English and +1.8 BLEU points for Chinese-English on the DARPA BOLT Web Forum condition.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9991434812545776}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9979918003082275}, {"text": "DARPA BOLT Web Forum condition", "start_pos": 110, "end_pos": 140, "type": "DATASET", "confidence": 0.8317936539649964}]}, {"text": "We also obtain improvements of +2.7 BLEU point for Arabic-English and +1.9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously published results in).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9990010857582092}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9968247413635254}, {"text": "NIST Open12 test sets", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.9565354883670807}]}, {"text": "Both the tensor architecture and multitask learning are general techniques that are likely to benefit other neural network features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9831843972206116}, {"text": "DARPA BOLT Web Forum", "start_pos": 126, "end_pos": 146, "type": "DATASET", "confidence": 0.8296428918838501}, {"text": "NIST OpenMT12 conditions", "start_pos": 155, "end_pos": 179, "type": "DATASET", "confidence": 0.9036510785420736}]}], "tableCaptions": [{"text": " Table 1: MT results of various model combination  in BLEU and in TER.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8332500457763672}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9910823106765747}, {"text": "TER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9072928428649902}]}, {"text": " Table 3: Experimental results to investigate the effects of the new features, DTN and MTL. The top  part shows the BOLT results, while the bottom part shows the NIST results. The best results for each  conditions and each language-pair are in bold. The baselines are in italics. .", "labels": [], "entities": [{"text": "BOLT", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9943681359291077}, {"text": "NIST", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.8897618651390076}]}, {"text": " Table 4: Experimental results for the NIST condi- tion. Mixed-case scores are also reported. Base- lines are in italics.", "labels": [], "entities": [{"text": "NIST condi- tion", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7072391957044601}]}]}