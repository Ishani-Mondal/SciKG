{"title": [{"text": "Sieve-Based Entity Linking for the Biomedical Domain", "labels": [], "entities": [{"text": "Sieve-Based Entity Linking", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8496673901875814}]}], "abstractContent": [{"text": "We examine a key task in biomedical text processing, normalization of disorder mentions.", "labels": [], "entities": [{"text": "biomedical text processing", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6854894359906515}, {"text": "normalization of disorder mentions", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.8642501533031464}]}, {"text": "We present a multi-pass sieve approach to this task, which has the advantage of simplicity and modularity.", "labels": [], "entities": []}, {"text": "Our approach is evaluated on two datasets, one comprising clinical reports and the other comprising biomedical abstracts, achieving state-of-the-art results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity linking is the task of mapping an entity mention in a text document to an entity in a knowledge base.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7825084626674652}]}, {"text": "This task is challenging because (1) the same word or phrase can be used to refer to different entities, and (2) the same entity can be referred to by different words or phrases.", "labels": [], "entities": []}, {"text": "In the biomedical text processing community, the task is more commonly known as normalization, where the goal is to map a word or phrase in a document to a unique concept in an ontology (based on the description of that concept in the ontology) after disambiguating potential ambiguous surface words or phrases.", "labels": [], "entities": []}, {"text": "Unlike in the news domain, in the biomedical domain it is rare for the same word or phrase to refer to multiple different concepts.", "labels": [], "entities": []}, {"text": "However, different words or phrases often refer to the same concept.", "labels": [], "entities": []}, {"text": "Given that mentions in biomedical text are relatively unambiguous, normalizing them involves addressing primarily the second challenge mentioned above.", "labels": [], "entities": [{"text": "normalizing them", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.9043651223182678}]}, {"text": "The goal of this paper is to advance the state of the art in normalizing disorder mentions in documents from two genres, clinical reports and biomedical abstracts.", "labels": [], "entities": [{"text": "normalizing disorder mentions in documents", "start_pos": 61, "end_pos": 103, "type": "TASK", "confidence": 0.9019903063774108}]}, {"text": "For example, given the disorder mention swelling of abdomen, a normalization system should map it to the concept in the ontology associated with the term abdominal distention.", "labels": [], "entities": []}, {"text": "Not all disorder mentions can be mapped  to a given ontology, however.", "labels": [], "entities": []}, {"text": "The reason is that the ontology may not include all of the possible concepts.", "labels": [], "entities": []}, {"text": "Hence, determining whether a disorder mention can be mapped to a concept in the given ontology is part of the normalization task.", "labels": [], "entities": []}, {"text": "Note that disorders have been the target of many research initiatives in the biomedical domain, as one of its major goals is to alleviate health disorders.", "labels": [], "entities": []}, {"text": "First, we propose a simpler and more modular approach to normalization than existing approaches: a multipass sieve approach.", "labels": [], "entities": [{"text": "normalization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9750990271568298}]}, {"text": "Second, our system achieves state-of-the-art results on datasets from two genres, clinical reports and biomedical abstracts.", "labels": [], "entities": []}, {"text": "To our knowledge, we are the first to present normalization results on two genres.", "labels": [], "entities": []}, {"text": "Finally, to facilitate comparison with future work on this task, we release the source code of our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our multi-pass sieve approach to normalization of disorder mentions.", "labels": [], "entities": [{"text": "normalization of disorder mentions", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.8862677961587906}]}, {"text": "Results on normalizing gold disorder mentions are shown in, where performance is reported in terms of accuracy (i.e., the percentage of gold disorder mentions correctly normalized).", "labels": [], "entities": [{"text": "normalizing gold disorder mentions", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.7521232068538666}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9987492561340332}]}, {"text": "Row 1 shows the baseline results, which are the best results reported to date on the ShARe and NCBI datasets by and, respectively.", "labels": [], "entities": [{"text": "ShARe and NCBI datasets", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.7195461690425873}]}, {"text": "As we can see, the baselines achieve accuracies of 89.5 and 82.2 on ShARe and NCBI, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.99897301197052}, {"text": "ShARe", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.45729860663414}, {"text": "NCBI", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9251807928085327}]}, {"text": "The subsequent rows show the results of our approach when our ten sieves are added incrementally.", "labels": [], "entities": []}, {"text": "In other words, each row shows the results obtained after adding a sieve to the sieves in the previous rows.", "labels": [], "entities": []}, {"text": "Our best system results, highlighted in bold in, are obtained when all our ten sieves are employed.", "labels": [], "entities": []}, {"text": "These results are significantly better than the baseline results (paired t-tests, p < 0.05).", "labels": [], "entities": []}, {"text": "To better understand the usefulness of each sieve, we apply paired t-tests on the results in adjacent rows.", "labels": [], "entities": []}, {"text": "We find that among the ten sieves,  Sieve 2 improves the results on both datasets at the lowest significance level (p < 0.02), while Sieves 6, 7, 8, and 10 improve results on both datasets at a slightly higher significance level (p < 0.05).", "labels": [], "entities": []}, {"text": "Among the remaining four sieves (3, 4, 5, 9), Sieve 3 improves results only on the clinical reports (p < 0.04), Sieve 4 improves results only on the biomedical abstracts dataset (p < 0.02), and Sieves 5 and 9 do not have any significant impact on either dataset (p > 0.05).", "labels": [], "entities": []}, {"text": "The last finding can be ascribed to the low proportions of hyphenated (Sieve 5) and composite (Sieve 9) disorder mentions found in the test datasets.", "labels": [], "entities": [{"text": "Sieve 5) and composite (Sieve 9) disorder mentions", "start_pos": 71, "end_pos": 121, "type": "METRIC", "confidence": 0.5438542528585955}]}, {"text": "After removing Sieves 5 and 9, accuracies drop insignificantly (p > 0.05) by 0.3% and 1.14% on the clinical reports and biomedical abstracts, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9992628693580627}]}], "tableCaptions": [{"text": " Table 2: Normalization accuracies on the test data  from the ShARe corpus and the NCBI corpus.", "labels": [], "entities": [{"text": "ShARe corpus", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.8654606640338898}, {"text": "NCBI corpus", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.9803888499736786}]}]}