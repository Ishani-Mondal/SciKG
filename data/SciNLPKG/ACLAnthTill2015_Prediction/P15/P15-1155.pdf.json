{"title": [{"text": "Predicting Salient Updates for Disaster Summarization", "labels": [], "entities": [{"text": "Predicting Salient Updates", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9120248357454935}, {"text": "Disaster Summarization", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7494527995586395}]}], "abstractContent": [{"text": "During crises such as natural disasters or other human tragedies, information needs of both civilians and responders often require urgent, specialized treatment.", "labels": [], "entities": []}, {"text": "Monitoring and summarizing a text stream during such an event remains a difficult problem.", "labels": [], "entities": [{"text": "summarizing a text stream", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.8595347851514816}]}, {"text": "We present a system for update sum-marization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection , increasing the quality of the updates.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.6936103254556656}]}, {"text": "We use novel, disaster-specific features for salience prediction, including geo-locations and language models representing the language of disaster.", "labels": [], "entities": [{"text": "salience prediction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.9012083411216736}]}, {"text": "Our evaluation on a standard set of retrospective events using ROUGE shows that salience prediction provides a significant improvement over other approaches.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9470332264900208}, {"text": "salience prediction", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.9179961979389191}]}], "introductionContent": [{"text": "During crises, information is critical for first responders, crisis management organizations, and those caught in the event.", "labels": [], "entities": []}, {"text": "When the event is significant, as in the case of Hurricane Sandy, the amount of content produced by traditional news outlets, government agencies, relief organizations, and social media can vastly overwhelm those trying to monitor the situation.", "labels": [], "entities": []}, {"text": "Crisis informatics () is dedicated to finding methods for sharing the right information in a timely fashion during such an event.", "labels": [], "entities": [{"text": "Crisis informatics", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6784894317388535}]}, {"text": "Research in this field has focused on human-in-the-loop approaches ranging from on the ground information gathering to crowdsourced reporting and disaster management.", "labels": [], "entities": [{"text": "disaster management", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7063076347112656}]}, {"text": "Multi-document summarization has the potential to assist the crisis informatics community.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6312107741832733}]}, {"text": "Automatic summarization could deliver relevant and salient information at regular intervals, even when human volunteers are unable to.", "labels": [], "entities": [{"text": "summarization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.969822883605957}]}, {"text": "Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large.", "labels": [], "entities": []}, {"text": "While methods for identifying, tracking, and summarizing events from text based input have been explored extensively (, these experiments were not developed to handle streaming data from a heterogeneous environment at web scale.", "labels": [], "entities": [{"text": "identifying, tracking, and summarizing events from text based input", "start_pos": 18, "end_pos": 85, "type": "TASK", "confidence": 0.704424424604936}]}, {"text": "These methods also rely heavily on redundancy which is suboptimal for time sensitive domains where there is a high cost in delaying information.", "labels": [], "entities": []}, {"text": "In this paper, we present an update summarization system to track events across time.", "labels": [], "entities": []}, {"text": "Our system predicts sentence salience in the context of a large-scale event, such as a disaster, and integrates these predictions into a clustering based multidocument summarization system.", "labels": [], "entities": []}, {"text": "We demonstrate that combining salience with clustering produces more relevant summaries compared to baselines using clustering or relevance alone.", "labels": [], "entities": []}, {"text": "Our experiments suggest that this is because our system is better able to adapt to dynamic changes in input volume that adversely affect methods that use redundancy as a proxy for salience.", "labels": [], "entities": []}, {"text": "In addition to the tight integration between clustering and salience prediction, our approach also exploits knowledge about the event to determine salience.", "labels": [], "entities": [{"text": "salience prediction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8425293862819672}]}, {"text": "Thus, salience represents both how typical a sentence is of the event type (e.g., industrial accident, hurricane, riot) and whether it specifies information about this particular event.", "labels": [], "entities": []}, {"text": "Our feature representation includes a set of language models, one for each event type, to measure the typicality of the sentence with regard to the current event, the distance of mentioned locations from the center of the event, and the change in word frequencies over the time of the event.", "labels": [], "entities": []}, {"text": "While we evaluate these features in the domain of disasters, this approach is generally applicable to many update summarization tasks.", "labels": [], "entities": []}, {"text": "Our approach achieves a statistically significant improvement in ROUGE scores compared to multiple baselines.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 65, "end_pos": 77, "type": "METRIC", "confidence": 0.9655941128730774}]}, {"text": "Additionally, we introduce novel methods for estimating the average information gain each update provides and how completely the update summary covers the event it is tracking; our system's updates contain more relevant information on average than the competing baselines.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We begin with a review of related work in the information retrieval and multi-document summarization literature.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7819448113441467}, {"text": "multi-document summarization", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.6855668723583221}]}, {"text": "Section 3 outlines the details of our salience and summarization models.", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9721522927284241}]}, {"text": "Next we describe our data (Section 4) and experiments (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we discuss our results (Section 6) and conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system on two metrics: ROUGE), an automatic summarization method and an evaluation of system expected gain and comprehensiveness-metrics adapted from the TREC TS track).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9957659244537354}, {"text": "TREC TS track", "start_pos": 170, "end_pos": 183, "type": "DATASET", "confidence": 0.8555723428726196}]}, {"text": "ROUGE measures the ngram overlap between a model summary and an automatically generated system summary.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9537637829780579}, {"text": "ngram overlap", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.8889079093933105}]}, {"text": "Model summaries for each event were constructed by concatenating the event's nuggets.", "labels": [], "entities": []}, {"text": "Generally, ROUGE evaluation assumes both model and system summaries are of a bounded length.", "labels": [], "entities": [{"text": "ROUGE evaluation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7562642097473145}]}, {"text": "Since our systems are summarizing events over a span of two weeks time, the total length of our system output is much longer than the model.", "labels": [], "entities": []}, {"text": "To address this, for each system/event pair, we sample with replacement 1000 random summaries of length less than or equal to the model summary (truncating the last sentence when neccessary).", "labels": [], "entities": []}, {"text": "The final ROUGE scores for the system are the average scores from these 1000 samples.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.993186354637146}]}, {"text": "Because we are interested in system performance overtime, we also evaluate systems at 12 hour intervals using the same regime as above.", "labels": [], "entities": []}, {"text": "The model summaries in this case are retrospective, and this evaluation reveals how quickly systems can cover information in the model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System ROUGE performance.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.6942071914672852}]}, {"text": " Table 2: Feature ablation ROUGE performance.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.6310189366340637}]}]}