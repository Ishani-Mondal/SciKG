{"title": [{"text": "SACRY: Syntax-based Automatic Crossword puzzle Resolution sYstem", "labels": [], "entities": [{"text": "SACRY", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7798945903778076}, {"text": "Crossword puzzle Resolution sYstem", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.7793579399585724}]}], "abstractContent": [{"text": "In this paper, we present our Crossword Puzzle Resolution System (SACRY), which exploits syntactic structures for clue reranking and answer extraction.", "labels": [], "entities": [{"text": "Crossword Puzzle Resolution", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.611078550418218}, {"text": "clue reranking", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.769988089799881}, {"text": "answer extraction", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.8934683799743652}]}, {"text": "SACRY uses a database (DB) containing previously solved CPs in order to generate the list of candidate answers.", "labels": [], "entities": []}, {"text": "Additionally, it uses innovative features, such as the answer position in the rank and aggregated information such as the min, max and average clue reranking scores.", "labels": [], "entities": [{"text": "min", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9512497186660767}, {"text": "average clue reranking scores", "start_pos": 135, "end_pos": 164, "type": "METRIC", "confidence": 0.752638466656208}]}, {"text": "Our system is based on WebCrow, one of the most advanced systems for automatic crossword puzzle resolution.", "labels": [], "entities": [{"text": "crossword puzzle resolution", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.8512276609738668}]}, {"text": "Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete CP resolution tasks, i.e., accuracy of 99.17%.", "labels": [], "entities": [{"text": "CP resolution tasks", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.8137674927711487}, {"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.9995207786560059}]}], "introductionContent": [{"text": "Crossword Puzzles (CPs) are the most famous language games played around the world.", "labels": [], "entities": [{"text": "Crossword Puzzles (CPs)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7666346848011016}]}, {"text": "The automatic resolution of CPs is an open challenge for the artificial intelligence (AI) community, which mainly employs AI techniques for filling the puzzle grid with candidate answers.", "labels": [], "entities": [{"text": "automatic resolution of CPs", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7751756906509399}]}, {"text": "Basic approaches try to optimize the overall probability of correctly filling the grid by exploiting the likelihood of each candidate answer, while satisfying the grid constraints.", "labels": [], "entities": []}, {"text": "Previous work () clearly suggests that providing the solver with an accurate list of answer candidates is an important step for the CP resolution task.", "labels": [], "entities": [{"text": "solver", "start_pos": 53, "end_pos": 59, "type": "TASK", "confidence": 0.9699912071228027}, {"text": "CP resolution task", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.9423267841339111}]}, {"text": "These can be retrieved using (i) the Web, (ii) Wikipedia, (iii) dictionaries or lexical databases like WordNet or, (iv) most importantly, recuperated from the DBs of previously solved CP.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9594799876213074}]}, {"text": "Indeed, CPs are often created reusing the same clues of past CPs, and thus querying the DB with the target clue allows for recuperating the same (or similar) clues of the target one.", "labels": [], "entities": []}, {"text": "It is interesting to note that, for this purpose, all previous automatic CP solvers use standard DB techniques, e.g., SQL Full-Text query.", "labels": [], "entities": [{"text": "CP solvers", "start_pos": 73, "end_pos": 83, "type": "TASK", "confidence": 0.7219229340553284}]}, {"text": "Existing systems for automatic CP resolution, such as Proverb () and Dr. Fill), use several different modules for generating candidate answer lists.", "labels": [], "entities": [{"text": "CP resolution", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.792334258556366}]}, {"text": "These are merged and used for defining a Constraint Satisfaction Problem, resolved by the CP solver.", "labels": [], "entities": [{"text": "Constraint Satisfaction Problem", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8576561411221822}, {"text": "CP solver", "start_pos": 90, "end_pos": 99, "type": "TASK", "confidence": 0.5821197628974915}]}, {"text": "Our CP system, SACRY, is based on innovative QA methods for answering CP clues.", "labels": [], "entities": [{"text": "SACRY", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.5207746028900146}, {"text": "answering CP clues", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8450926343599955}]}, {"text": "We employ (i) state-of-the-art IR techniques to retrieve the correct answer by querying the DB of previously solved CPs, (ii) learning to rank methods based on syntactic structure of clues and structural kernels to improve the ranking of clues that can potentially contain the answers and (iii) an aggregation algorithm for generating the final list containing unique candidate answers.", "labels": [], "entities": [{"text": "IR", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.951738178730011}]}, {"text": "We implemented a specific module based on these approaches and we plugged it into an automatic CP solver, namely WebCrow ().", "labels": [], "entities": [{"text": "CP solver", "start_pos": 95, "end_pos": 104, "type": "TASK", "confidence": 0.7214176952838898}]}, {"text": "The latter is one of the best systems for CP resolution and it has been kindly made available by the authors.", "labels": [], "entities": [{"text": "CP resolution", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9572460055351257}]}, {"text": "We tested our models on a dataset containing more than two million clues and their associated answers.", "labels": [], "entities": []}, {"text": "This dataset is an interesting resource that we will make available to the research community.", "labels": [], "entities": []}, {"text": "It can be used for tasks such as: (i) similar clue retrieval/reranking, which focuses on improving the rank of clues c i retrieved by a search engine, and (ii) answer reranking, which targets the list of ac i , i.e., their aggregated clues.", "labels": [], "entities": [{"text": "clue retrieval", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.6998037099838257}, {"text": "answer reranking", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.7550225853919983}]}, {"text": "We tested SACRY on an end-to-end task by solving ten crossword puzzles provided by two of the most famous CP editors from The New York Times and the Washington Post.", "labels": [], "entities": []}, {"text": "SACRY obtained an impressive CP resolution accuracy of 99.17%.", "labels": [], "entities": [{"text": "SACRY", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7788015007972717}, {"text": "CP resolution", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.7103031873703003}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.7261085510253906}]}, {"text": "In the reminder of this paper, Sec.", "labels": [], "entities": []}, {"text": "2 introduces WebCrow and its architecture.", "labels": [], "entities": []}, {"text": "Our models for similar clues retrieval and answers reranking are described in Sec.", "labels": [], "entities": [{"text": "clues retrieval", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.699016883969307}, {"text": "answers reranking", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7125040292739868}]}, {"text": "Finally, the conclusions and directions for future work are presented in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we compare our approach with WebCrow both on ranking candidate answers and on the end-to-end CP resolution.", "labels": [], "entities": [{"text": "CP resolution", "start_pos": 112, "end_pos": 125, "type": "TASK", "confidence": 0.5106698423624039}]}, {"text": "To train our models, we adopted SVM-light-TK 5 , which enables the use of the Partial Tree Kernel (PTK)) in SVM-light, with default parameters.", "labels": [], "entities": []}, {"text": "We applied a polynomial kernel of degree 3 to the explicit feature vectors (FV).", "labels": [], "entities": [{"text": "FV", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.8018606305122375}]}, {"text": "To measure the impact of the rerankers as well as the CWDB module, we used well-known metrics for assessing the accuracy of QA and retrieval systems, i.e.: Recall at different ranks (R@1, 5, 20, 50, 100), Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9983422756195068}, {"text": "Recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9778637290000916}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 205, "end_pos": 231, "type": "METRIC", "confidence": 0.9574461976687113}, {"text": "Mean Average Precision (MAP)", "start_pos": 236, "end_pos": 264, "type": "METRIC", "confidence": 0.9763467113176981}]}, {"text": "R@1 is the percentage of questions with a correct answer ranked at the first position.", "labels": [], "entities": [{"text": "R@1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9734435876210531}]}, {"text": "MRR is computed as follows: , where rank(q) is the position of the first correct answer in the candidate list.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.4039941132068634}]}, {"text": "For a set of queries Q, MAP is the mean over the average precision scores for each query: 1 Q Q q=1 AveP (q).", "labels": [], "entities": [{"text": "MAP", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9960596561431885}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9917148947715759}, {"text": "AveP", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9333517551422119}]}, {"text": "To measure the complete CP resolution task, we use the accuracy over the entire words filling a CP grid (one wrong letter causes the entire definition to be incorrect).", "labels": [], "entities": [{"text": "CP resolution", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9202210903167725}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9992777705192566}]}, {"text": "Given an input clue BM25 retrieves a list of 100 clues.", "labels": [], "entities": []}, {"text": "On the latter, we tested our different models for clue reranking.", "labels": [], "entities": [{"text": "clue reranking", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9107713103294373}]}, {"text": "For space constraints, we only report a short summary of our experiments: kernel-based rerankers combined with traditional features (PTK+FV) relatively improve standard IR by 16%.", "labels": [], "entities": [{"text": "IR", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.9388547539710999}]}, {"text": "This is an interesting result as in (), the authors showed that standard IR greatly improves on the DB methods for clue retrieval, i.e., they showed that BM25 relatively improves on SQL by about 40% in MRR.", "labels": [], "entities": [{"text": "clue retrieval", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.8058728277683258}, {"text": "BM25", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.7266356945037842}]}, {"text": "In order to test the effectiveness of our method, we evaluated the resolution of full CP.", "labels": [], "entities": [{"text": "resolution", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9915394186973572}]}, {"text": "We selected five crosswords from The New York Times newspaper and other five from the Washington Post.", "labels": [], "entities": [{"text": "The New York Times newspaper", "start_pos": 33, "end_pos": 61, "type": "DATASET", "confidence": 0.9023101806640625}, {"text": "Washington Post", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.8769735395908356}]}, {"text": "shows the average resolution accuracy over the ten CP of the original WebCrow compared to WebCrow using our reranked lists.", "labels": [], "entities": [{"text": "resolution", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9873088002204895}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.550413966178894}]}, {"text": "We ran the solver by providing it with lists of different size.", "labels": [], "entities": [{"text": "solver", "start_pos": 11, "end_pos": 17, "type": "TASK", "confidence": 0.9912803769111633}]}, {"text": "We note that our model consistently outperforms WebCrow.", "labels": [], "entities": []}, {"text": "This means that the lists of candidate answers generated by our models help the solver, which in turn fills the grid with higher accuracy.", "labels": [], "entities": [{"text": "solver", "start_pos": 80, "end_pos": 86, "type": "TASK", "confidence": 0.9833390712738037}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9962794184684753}]}, {"text": "In particular, our CP system achieves an average accuracy of 99.17%, which makes it competitive with international CP resolution challenges.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9993602633476257}, {"text": "CP resolution", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.8254028260707855}]}, {"text": "Additionally, WebCrow achieves the highest accuracy when uses the largest candidate lists (both original or reranked) but a large list size negatively impacts on the speed of the solver, which in a CP competition is critical to beat the other competitors (if participants obtain the same score, the solving time decides who is ranked first).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9986584186553955}, {"text": "solver", "start_pos": 179, "end_pos": 185, "type": "TASK", "confidence": 0.9659414887428284}]}, {"text": "Thus, our approach also provide a speedup as the best accuracy is reached for just 50 candidates (in contrast with the 100 candidates needed by the original WebCrow).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9988681077957153}]}], "tableCaptions": [{"text": " Table 1: Answer reranking on the dev. set.", "labels": [], "entities": [{"text": "Answer reranking", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6066637933254242}]}]}