{"title": [{"text": "Non-Linear Text Regression with a Deep Convolutional Neural Network", "labels": [], "entities": [{"text": "Non-Linear Text Regression", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5847150782744089}]}], "abstractContent": [{"text": "Text regression has traditionally been tackled using linear models.", "labels": [], "entities": [{"text": "Text regression", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.80528724193573}]}, {"text": "Here we present a non-linear method based on a deep convolutional neural network.", "labels": [], "entities": []}, {"text": "We show that despite having millions of parameters , this model can be trained on only a thousand documents, resulting in a 40% relative improvement over sparse linear models, the previous state of the art.", "labels": [], "entities": []}, {"text": "Further, this method is flexible allowing for easy incorporation of side information such as document meta-data.", "labels": [], "entities": []}, {"text": "Finally we present a novel technique for interpreting the effect of different text inputs on this complex non-linear model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text regression involves predicting areal world phenomenon from textual inputs, and has been shown to be effective in domains including election results (, financial risk ( and public health.", "labels": [], "entities": [{"text": "Text regression", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7921782433986664}]}, {"text": "Almost universally, the text regression problem has been framed as linear regression, with the modelling innovation focussed on effective regression, e.g., using Lasso penalties to promote feature sparsity.", "labels": [], "entities": [{"text": "text regression", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7506202161312103}]}, {"text": "Despite their successes, linear models are limiting: text regression problems will often involve complex interactions between textual inputs, thus requiring a non-linear approach to properly capture such phenomena.", "labels": [], "entities": [{"text": "text regression", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7744671404361725}]}, {"text": "For instance, in modelling movie revenue conjunctions of features are likely to be important, e.g., a movie described as 'scary' is likely to have different effects for children's versus adult movies.", "labels": [], "entities": []}, {"text": "While these kinds of features can be captured using explicit feature engineering, this process is tedious, limited in scope (e.g., to conjunctions) and -as we show herecan be dramatically improved by representational learning as part of a non-linear model.", "labels": [], "entities": []}, {"text": "In this paper, we propose an artificial neural network (ANN) for modelling text regression.", "labels": [], "entities": [{"text": "text regression", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.7437838613986969}]}, {"text": "In language processing, ANNs were first proposed for probabilistic language modelling, followed by models of sentences () and parsing) inter alia.", "labels": [], "entities": [{"text": "language processing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7079519182443619}, {"text": "probabilistic language modelling", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.6535195608933767}]}, {"text": "These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics.", "labels": [], "entities": []}, {"text": "In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing () and recent applications to language processing (.", "labels": [], "entities": [{"text": "image processing", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.7910129427909851}]}, {"text": "These works have mainly focused on 'big data' problems with plentiful training examples.", "labels": [], "entities": []}, {"text": "Given their large numbers of parameters, often in the millions, one would expect that such models can only be effectively learned on very large datasets.", "labels": [], "entities": []}, {"text": "However we show here that a complex deep convolution network can be trained on about a thousand training examples, although careful model design and regularisation is paramount.", "labels": [], "entities": []}, {"text": "We consider the problem of predicting the future box-office takings of movies based on reviews by movie critics and movie attributes.", "labels": [], "entities": []}, {"text": "Our approach is based on the method and dataset of, who presented a linear regression model over uni-, bi-, and tri-gram term frequency counts extracted from reviews, as well as movie and reviewer metadata.", "labels": [], "entities": []}, {"text": "This problem is especially interesting, as comparatively few instances are available for training (see) while each in- our model also operates over ngrams, 1 \u2264 n \u2264 3, and movie metadata, albeit using an ANN in place of their linear model.", "labels": [], "entities": []}, {"text": "We use word embeddings to represent words in a low dimensional space, a convolutional network with max-pooling to represent documents in terms of n-grams, and several fully connected hidden layers to allow for learning of complex non-linear interactions.", "labels": [], "entities": []}, {"text": "We show that including non-linearities in the model is crucial for accurate modelling, providing a relative error reduction of 40% (MAE) over their best linear model.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 99, "end_pos": 123, "type": "METRIC", "confidence": 0.7820718685785929}, {"text": "MAE)", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9900351464748383}]}, {"text": "Our final contribution is a novel means of model interpretation.", "labels": [], "entities": [{"text": "model interpretation", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7481262981891632}]}, {"text": "Although it is notoriously difficult to interpret the parameters of an ANN, we show a simple method of quantifying the effect of text n-grams on the prediction output.", "labels": [], "entities": []}, {"text": "This allows for identification of the most important textual inputs, and investigation of non-linear interactions between these words and phrases in different data instances.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Movie review dataset (Joshi et al., 2010).", "labels": [], "entities": [{"text": "Movie review dataset", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.8188043832778931}]}, {"text": " Table 2: Experiment results on test set. Linear  models by (Joshi et al., 2010).", "labels": [], "entities": []}, {"text": " Table 3: Various alternative configurations, based  on the ANN Text+Meta model. The asterisk ( * )  denotes the settings in the ANN Text+Meta model.", "labels": [], "entities": [{"text": "ANN Text+Meta model", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.8366563081741333}]}, {"text": " Table 4: Selected phrase impacts on the predic- tions in $ USD(K) in the test set, showing min,  max and avg change in prediction value and num- ber of occurrences (denoted #). Periods denote ab- breviations (language, accompanying).", "labels": [], "entities": []}]}