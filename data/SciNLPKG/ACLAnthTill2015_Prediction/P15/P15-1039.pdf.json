{"title": [{"text": "Generating High Quality Proposition Banks for Multilingual Semantic Role Labeling", "labels": [], "entities": [{"text": "Multilingual Semantic Role Labeling", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.5568608120083809}]}], "abstractContent": [{"text": "Semantic role labeling (SRL) is crucial to natural language understanding as it identifies the predicate-argument structure in text with semantic labels.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8347224394480387}, {"text": "natural language understanding", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6780926783879598}]}, {"text": "Unfortunately, resources required to construct SRL models are expensive to obtain and simply do not exist for most languages.", "labels": [], "entities": [{"text": "SRL", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9741721153259277}]}, {"text": "In this paper, we present a two-stage method to enable the construction of SRL models for resource-poor languages by exploiting monolingual SRL and multilingual parallel data.", "labels": [], "entities": [{"text": "SRL", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.981860339641571}]}, {"text": "Experimental results show that our method out-performs existing methods.", "labels": [], "entities": []}, {"text": "We use our method to generate Proposition Banks with high to reasonable quality for 7 languages in three language families and release these resources to the research community.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (SRL) is the task of automatically labeling predicates and arguments in a sentence with shallow semantic labels.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8401179214318594}, {"text": "automatically labeling predicates and arguments in a sentence with shallow semantic labels", "start_pos": 44, "end_pos": 134, "type": "TASK", "confidence": 0.5122970466812452}]}, {"text": "This level of analysis provides a more stable semantic representation across syntactically different sentences, thereby enabling a range of NLP tasks such as information extraction and question answering).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.7735787928104401}, {"text": "question answering", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.8148951828479767}]}, {"text": "Projects such as the Proposition Bank (PropBank) () spent considerable effort to annotate corpora with semantic labels, in turn enabling supervised learning of statistical SRL parsers for English.", "labels": [], "entities": [{"text": "Proposition Bank (PropBank)", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.775847190618515}, {"text": "SRL parsers", "start_pos": 172, "end_pos": 183, "type": "TASK", "confidence": 0.8398337364196777}]}, {"text": "Unfor- * This work was conducted at IBM.", "labels": [], "entities": []}, {"text": "tunately, due to the high costs of manual annotation, comparable SRL resources do not exist for most other languages, with few exceptions.", "labels": [], "entities": [{"text": "SRL", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9443082809448242}]}, {"text": "As a cost-effective alternative to manual annotation, previous work has investigated the direct projection of semantic labels from a resource rich language (English) to a resource poor target language (TL) in parallel corpora).", "labels": [], "entities": [{"text": "projection of semantic labels from a resource rich language (English)", "start_pos": 96, "end_pos": 165, "type": "TASK", "confidence": 0.7335992604494095}]}, {"text": "The underlying assumption is that original and translated sentences in parallel corpora are semantically broadly equivalent.", "labels": [], "entities": []}, {"text": "Hence, if English sentences of a parallel corpus are automatically labeled using an SRL system, these labels can be projected onto aligned words in the TL corpus, thereby automatically labeling the TL corpus with semantic labels.", "labels": [], "entities": []}, {"text": "This way, PropBank-like resources can automatically be created that enable the training of statistical SRL systems for new TLs.", "labels": [], "entities": [{"text": "SRL", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.7699727416038513}]}, {"text": "However, as noted in previous work: Pair of parallel sentences from Frenchgoldwith word alignments (dotted lines), SRL labels for the English sentence, and gold SRL labels for the French sentence.", "labels": [], "entities": [{"text": "SRL", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9064703583717346}]}, {"text": "Only two of the seven English SRL labels should be projected here.", "labels": [], "entities": [{"text": "English SRL labels", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.7771127621332804}]}, {"text": "shifts that go against this assumption.", "labels": [], "entities": []}, {"text": "For example, in, the English sentence \"We need to hold people responsible\" is translated into a French sentence that literally reads as \"There need to exist those responsible\".", "labels": [], "entities": []}, {"text": "Hence, the predicate label of the English word \"hold\" should not be projected onto the French verb, which has a different meaning.", "labels": [], "entities": []}, {"text": "As the example in shows, this means that only a subset of all SL labels can be directly projected.", "labels": [], "entities": []}, {"text": "In this paper, we aim to create PropBank-like resources fora range of languages from different language groups.", "labels": [], "entities": []}, {"text": "To this end, we propose a two-stage approach to cross-lingual semantic labeling that addresses such errors, shown in: Given a parallel corpus in which the source language (SL) side is automatically labeled with PropBank labels and the TL side is syntactically parsed, we use a filtered projection approach that allows the projection only of high-confidence SL labels.", "labels": [], "entities": [{"text": "cross-lingual semantic labeling", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6502187550067902}]}, {"text": "This results in a TL corpus with low recall but high precision.", "labels": [], "entities": [{"text": "TL corpus", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.7735523581504822}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9991138577461243}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9980078339576721}]}, {"text": "In the second stage, we repeatedly sample a subset of complete TL sentences and train a classifier to iteratively add new labels, significantly increasing the recall in the TL corpus while retaining the improvement in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9994118213653564}, {"text": "precision", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9981086254119873}]}, {"text": "Our contributions are: (1) We propose filtered projection focused specifically on raising the precision of projected labels, based on a detailed analysis of direct projection errors.", "labels": [], "entities": [{"text": "filtered projection", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7150730639696121}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9980137348175049}]}, {"text": "(2) We propose a bootstrap learning approach to retrain the SRL to iteratively improve recall without a significant reduction of precision, especially for arguments; We demonstrate the effectiveness and generalizability of our approach via an extensive set of experiments over 7 different language pairs.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9906056523323059}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9963996410369873}]}, {"text": "(4) We generate PropBanks for each of these languages and release them to the research community.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use our method to generate Proposition Banks for 7 languages and evaluate the generated resources.", "labels": [], "entities": []}, {"text": "We seek to answer the following questions: (1) What is the estimated quality for the generated PropBanks?", "labels": [], "entities": [{"text": "PropBanks", "start_pos": 95, "end_pos": 104, "type": "DATASET", "confidence": 0.903071939945221}]}, {"text": "How well does the approach work without language-specific adaptation?", "labels": [], "entities": []}, {"text": "(2) Are there notable differences in quality from language to language; if so, why?", "labels": [], "entities": []}, {"text": "We also present initial investigations on how different factors affect the performance of our method.", "labels": [], "entities": []}, {"text": "5 lists the 7 different TLs and resources used in our experiments.", "labels": [], "entities": []}, {"text": "We chose these TLs because (1) they are among top 10 most influential languages in the world (Weber, 1997); and (2) we could find language experts to evaluate the results.", "labels": [], "entities": []}, {"text": "English is used as SL in all our experiments.", "labels": [], "entities": []}, {"text": "Approach Tested For each TL, we used FB best (Sec. 3.3) to generate a corpus with semantic labels.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9776440262794495}, {"text": "FB", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.7683432698249817}]}, {"text": "From each TL corpus, we extracted all complete sentences to form the generated PropBanks.", "labels": [], "entities": [{"text": "PropBanks", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9569942951202393}]}, {"text": "8 From each parallel corpus, we only keep sentences that are considered well-formed based on a set of standard heuristics.", "labels": [], "entities": []}, {"text": "For example, we require a well-formed sentence to end in punctuation and not to contain certain special characters.", "labels": [], "entities": []}, {"text": "For Arabic, as the dependency parser we use has relatively poor parsing accuracy, we additionally require sentences to be shorter than 100 characters.: Estimated precision and recall over seven languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.8652461171150208}, {"text": "Estimated", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9722991585731506}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9240129590034485}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9992517828941345}]}, {"text": "Manual Evaluation While a gold annotated corpus for French (French gold ) was available for our experiments in the previous Sections, no such resources existed for the other TLs we wished to evaluate.", "labels": [], "entities": [{"text": "French (French gold )", "start_pos": 52, "end_pos": 73, "type": "DATASET", "confidence": 0.6748711824417114}]}, {"text": "We therefore chose to conduct a manual evaluation for each TL, each executed identically: For each TL we randomly selected 100 complete sentences with their generated semantic labels and assigned them to two language experts who were instructed to evaluate the semantic labels (based on their English descriptions) for the predicates and their core arguments.", "labels": [], "entities": []}, {"text": "For each label, they were asked to determine (1) whether the label is correct; (2) if yes, then whether the boundary of the labeled constituent is correct: If also yes, mark the label as fully correct, otherwise as partially correct.", "labels": [], "entities": []}, {"text": "Metrics We used the standard measures of precision, recall, and F1 to measure the performance of the SRLs, with the following two schemes: (1) Exact: Only fully correct labels are considered as true positives; (2) Partial: Both fully and partially correct matches are considered as true positives.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9996668100357056}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.999489426612854}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9998418092727661}, {"text": "Exact", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.9750941395759583}]}, {"text": "6 summarizes the estimated quality of semantic labels generated by our method for all seven TL.", "labels": [], "entities": []}, {"text": "As can be seen, our method performed well for all  seven languages and generated high quality semantics labels across the board.", "labels": [], "entities": []}, {"text": "For predicate labels, the precision is over 95% and the recall is over 85% for all languages except for Hindi.", "labels": [], "entities": [{"text": "predicate labels", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8711391091346741}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9997437596321106}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9997971653938293}]}, {"text": "For argument labels, when considering partially correct matches, the precision is at least 85% (above 90% for most languages) and the recall is between 66% to 83% for all the languages.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9995061159133911}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9997194409370422}]}, {"text": "These encouraging results obtained from a diverse set of languages implies the generalizability of our method.", "labels": [], "entities": []}, {"text": "In addition, the inter-annotator agreement is very high for all the languages, indicating that the results obtained based on manual evaluation are very reliable.", "labels": [], "entities": []}, {"text": "In addition, we make a number of interesting observations: Dependency Parsing Accuracy The precision for exact argument labels is significantly below partial matches, particularly for Hindi (\u219335 pp) and Arabic (\u219319 pp).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.7972710132598877}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9992095232009888}]}, {"text": "Since argument boundaries are determined syntactically, such errors are caused by dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7388797402381897}]}, {"text": "The fact that Hindi and Arbic suffer the most from this issue is consistent with the poorer performance of their dependency parsers compared to other languages (.", "labels": [], "entities": []}, {"text": "Hindi as the Main Outlier The results for Hindi are much worse than the results for other languages.", "labels": [], "entities": []}, {"text": "Besides the poorer dependency parser performance, the size of the parallel corpus used could be a factor: Hindencorp is one to two orders of magnitude smaller than the other corpora.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6707625240087509}]}, {"text": "The quality of the parallel corpus could be a reason as well: Hindencorp was collected from various sources, while both UN and Europarl were extracted from governmental proceedings.", "labels": [], "entities": [{"text": "Hindencorp", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.8181914687156677}, {"text": "UN", "start_pos": 120, "end_pos": 122, "type": "DATASET", "confidence": 0.8923602104187012}, {"text": "Europarl", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.6176833510398865}]}, {"text": "Language-specific Errors Certain errors occur more frequently in some languages than others.", "labels": [], "entities": []}, {"text": "An example are deverbal nouns in Chinese) informal passive constructions with support verb \u53d7.", "labels": [], "entities": []}, {"text": "Since we currently only consider verbs for pred-   icate labels, predicate labels are projected onto the support verbs instead of the deverbal nouns.", "labels": [], "entities": []}, {"text": "Such errors appear for light verb constructions in all languages, but particularly affect Chinese due to the high frequency of this passive construction in the UN corpus.", "labels": [], "entities": []}, {"text": "Low Fraction of Complete Sentences As Tab.", "labels": [], "entities": [{"text": "Fraction", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.950419545173645}]}, {"text": "7 shows, the fraction of complete sentences in the generated PropBanks is rather low, indicating the impact of moderate recall on the size of generated PropBanks.", "labels": [], "entities": [{"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9969655871391296}]}, {"text": "Especially for languages for which only small parallel corpora are available, such as Hindi, this points to the need to address recall issues in future work.", "labels": [], "entities": [{"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9943053126335144}]}, {"text": "The observations made in Sec.", "labels": [], "entities": []}, {"text": "4.2 suggests a few factors that may potentially affect the performance of our method.", "labels": [], "entities": []}, {"text": "To better understand their impact, we conducted the following initial investigation.", "labels": [], "entities": []}, {"text": "SRL models produced in this set of experiments were evaluated using French gold , sampled and evaluated in the same way as other experiments in this section for comparability.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7147167921066284}, {"text": "French gold", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.8482147753238678}]}, {"text": "Data Size We varied the data size for French by downsampling the UN corpus.", "labels": [], "entities": [{"text": "Data Size", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.6296228468418121}, {"text": "UN corpus", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9356947839260101}]}, {"text": "As one can see from Tab.", "labels": [], "entities": []}, {"text": "8, downsampling the dataset by one order of magnitude (to 250k sentences) only slightly affects precision, while downsampling to 25k sentences has a more pronounced but still small impact on recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9995207786560059}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9974728226661682}]}, {"text": "It appears that data size does not have significant impact on the performance of our method.", "labels": [], "entities": []}, {"text": "Language-specific Customizations While our method is language-agnostic, intuitively languagespecific customization can be helpful in address-ing language-specific errors.", "labels": [], "entities": []}, {"text": "As an initial experiment, we added a simple heuristic to filter out French verbs that are commonly used for \"existential there\" constructions, as one type of common errors for French involves the syntactic expletive il () in \"existential there\" constructions such as il faut (see (TL sentence) for an example) wrongly labeled with with role information.", "labels": [], "entities": []}, {"text": "9, this simple customization results in a small increase in precision, suggesting that language-specific customization can be helpful.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9993995428085327}]}, {"text": "Quality of English SRL As noted in Sec.", "labels": [], "entities": [{"text": "SRL", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.5243737101554871}]}, {"text": "2.5, errors made by English SRL are often prorogated to the TL via projection.", "labels": [], "entities": []}, {"text": "To assess the impact of English SRL quality, we used two different English SRL systems: CLEARNLP and MATE-SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8180298209190369}, {"text": "CLEARNLP", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.7938980460166931}, {"text": "MATE-SRL", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.833844780921936}]}, {"text": "As can be seen from Tab.", "labels": [], "entities": [{"text": "Tab", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.9699035882949829}]}, {"text": "9, the impact of English SRL quality is substantial on argument labeling.", "labels": [], "entities": [{"text": "SRL", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.6873300671577454}, {"text": "argument labeling", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7597277462482452}]}], "tableCaptions": [{"text": " Table 1: Breakdown of error classes in predicate projection.", "labels": [], "entities": [{"text": "predicate projection", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7547024786472321}]}, {"text": " Table 3: Quality of predicate and argument labels for different  projection methods on Frenchgold, including upper bound.", "labels": [], "entities": [{"text": "Frenchgold", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.9888956546783447}]}, {"text": " Table 4: Experiments on Frenchgold, with different projection  and SRL training methods. SP=Supplement; OW=Overwrite.", "labels": [], "entities": [{"text": "Frenchgold", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.9839387536048889}, {"text": "SP", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9474910497665405}, {"text": "Supplement", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.806447446346283}, {"text": "OW", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9892382621765137}, {"text": "Overwrite", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.851682722568512}]}, {"text": " Table 6: Estimated precision and recall over seven languages.", "labels": [], "entities": [{"text": "Estimated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9917068481445312}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9515028595924377}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9989013671875}]}, {"text": " Table 7: Characteristics of the generated PropBanks.", "labels": [], "entities": []}, {"text": " Table 8: Estimated impact of downsampling parallel corpus.", "labels": [], "entities": [{"text": "Estimated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9785411953926086}]}, {"text": " Table 9: Impact of English SRLs (  *  =CLEARNLP,  *  *  =MATE- SRL) and language-spec. customization (filter synt. expletive).", "labels": [], "entities": []}]}