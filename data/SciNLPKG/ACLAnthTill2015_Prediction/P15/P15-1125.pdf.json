{"title": [], "abstractContent": [{"text": "Existing distributed representations are limited in utilizing structured knowledge to improve semantic relatedness modeling.", "labels": [], "entities": [{"text": "semantic relatedness modeling", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.7417550881703695}]}, {"text": "We propose a principled framework of embedding entities that integrates hierarchical information from large-scale knowledge bases.", "labels": [], "entities": []}, {"text": "The novel embedding model associates each category node of the hierarchy with a distance metric.", "labels": [], "entities": []}, {"text": "To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths.", "labels": [], "entities": [{"text": "context prediction", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7377623021602631}]}, {"text": "We show that both the entity vectors and category distance metrics encode meaningful semantics.", "labels": [], "entities": []}, {"text": "Experiments in entity linking and entity search show superiority of the proposed method.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.8702076971530914}, {"text": "entity search", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.8566412627696991}]}], "introductionContent": [{"text": "There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (), phrases (, and concepts), etc.", "labels": [], "entities": []}, {"text": "The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis), machine translation ( , and information retrieval, to name a few.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.9570841491222382}, {"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.8249345421791077}, {"text": "information retrieval", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.7782848179340363}]}, {"text": "Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation.", "labels": [], "entities": []}, {"text": "For instance, word and phrase embeddings are largely induced from plain text.", "labels": [], "entities": []}, {"text": "Though recent knowledge graph embeddings () integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7440477013587952}]}, {"text": "In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs).", "labels": [], "entities": []}, {"text": "An entity hierarchy groups entities into categories which are further organized to form a taxonomy.", "labels": [], "entities": []}, {"text": "It provides rich structured knowledge on entity relatedness.", "labels": [], "entities": []}, {"text": "Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features, and develops a principled optimization-based framework.", "labels": [], "entities": []}, {"text": "We learn a distance metric for each category node, and measure entity-context similarity under the aggregated metrics of all relevant categories.", "labels": [], "entities": []}, {"text": "The metric aggregation encodes the hierarchical property that nearby entities tend to share common semantic features.", "labels": [], "entities": []}, {"text": "We further provide a highly-efficient implementation in order to handle large complex hierarchies.", "labels": [], "entities": []}, {"text": "We train a distributed representation for the whole entity hierarchy of Wikipedia.", "labels": [], "entities": []}, {"text": "Both the entity vectors and the category distance metrics capture meaningful semantics.", "labels": [], "entities": []}, {"text": "We deploy the embedding in both entity linking) and entity search tasks.", "labels": [], "entities": []}, {"text": "Hierarchy embedding significantly outperforms that without structural knowledge.", "labels": [], "entities": []}, {"text": "Our methods also show superiority over existing competitors.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to learn distributed representations that incorporates hierarchical knowledge in a principled framework.", "labels": [], "entities": []}, {"text": "Our model that encodes hierarchy by distance metric learning and aggregation provides a potentially important and general scheme for utilizing hierarchical knowledge.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: \u00a72 describes the proposed embedding model; \u00a73 presents the application of the learned embedding; \u00a74 evaluates the approach; \u00a75 reviews related work; and finally, \u00a76 concludes the paper.", "labels": [], "entities": []}, {"text": "Figure 1: The model architecture.", "labels": [], "entities": []}, {"text": "The text context of an entity is based on its KB encyclopedia article.", "labels": [], "entities": [{"text": "KB encyclopedia article", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.9633782704671224}]}, {"text": "The entity hierarchical structure is incorporated through distance metric learning and aggregation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We validate the quality of our entity representation by evaluating its applications of entity linking and entity search on public benchmarks.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7083092480897903}, {"text": "entity search", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.7214523255825043}]}, {"text": "In the entity linking task, our approach improves the F1 score by 10% over state-of-the-art results.", "labels": [], "entities": [{"text": "entity linking task", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8229752779006958}, {"text": "F1 score", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.989408403635025}]}, {"text": "We also validate the advantage of incorporating hierarchical structure.", "labels": [], "entities": []}, {"text": "In the entity search task, our simple algorithm shows competitive performance.", "labels": [], "entities": [{"text": "entity search task", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8269081314404806}]}, {"text": "We further qualitatively analyze the entity vectors and category metrics, both of which capture meaningful semantics, and can potentially open up a wide rage of other applications.", "labels": [], "entities": []}, {"text": "Knowledge base We use the Wikipedia snapshot from Jan 12nd, 2015 as our training data and KB.", "labels": [], "entities": [{"text": "Wikipedia snapshot from Jan 12nd", "start_pos": 26, "end_pos": 58, "type": "DATASET", "confidence": 0.9681346416473389}, {"text": "KB", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9619022607803345}]}, {"text": "After pruning administrative information we obtain an entity hierarchy including about 4.1M entities and 0.8M categories organized into 12 layers.", "labels": [], "entities": []}, {"text": "Loops in the original hierarchy are removed by deleting bottom-up edges, yielding a DAG structure.", "labels": [], "entities": []}, {"text": "We extract a set of 87.6M entity pairs from the wiki links on Wikipedia articles.", "labels": [], "entities": []}, {"text": "We train 100-dimensional vector representations for the entities and distance metrics (100\u00d7100 diagonal matrixes) for the categories (we would study the impact of dimensionality in the future).", "labels": [], "entities": []}, {"text": "We set the batch size B = 500, the initial learning rate \u03b7 = 0.1 and decrease it by a factor of 5 whenever the objective value does not increase, and the negative sample size k = 5.", "labels": [], "entities": [{"text": "initial learning rate \u03b7", "start_pos": 35, "end_pos": 58, "type": "METRIC", "confidence": 0.7765127718448639}]}, {"text": "The model is trained on a Linux machine with 128G RAM and 16 cores.", "labels": [], "entities": []}, {"text": "It takes 5 days to converge.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Entity linking performance", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7604033350944519}]}, {"text": " Table 2: Entity search performance.", "labels": [], "entities": []}]}