{"title": [], "abstractContent": [{"text": "In this paper we present the task of un-supervised prediction of speakers' acceptability judgements.", "labels": [], "entities": [{"text": "un-supervised prediction of speakers' acceptability judgements", "start_pos": 37, "end_pos": 99, "type": "TASK", "confidence": 0.7346297105153402}]}, {"text": "We use a test set generated from the British National Corpus (BNC) containing both grammatical sentences and sentences containing a variety of syntactic infelicities introduced by round trip machine translation.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.971643457810084}]}, {"text": "This set was annotated for acceptability judgements through crowd sourcing.", "labels": [], "entities": [{"text": "acceptability judgements", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7872039973735809}]}, {"text": "We trained a variety of unsupervised language models on the original BNC, and tested them to seethe extent to which they could predict mean speakers' judgements on the test set.", "labels": [], "entities": [{"text": "BNC", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9383741617202759}]}, {"text": "To map probability to acceptability, we experimented with several normalisa-tion functions to neutralise the effects of sentence length and word frequencies.", "labels": [], "entities": []}, {"text": "We found encouraging results with the unsu-pervised models predicting acceptability across two different datasets.", "labels": [], "entities": []}, {"text": "Our methodology is highly portable to other domains and languages, and the approach has potential implications for the representation and the acquisition of linguistic knowledge .", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modelling involves predicting the probability of a sentence.", "labels": [], "entities": [{"text": "Language modelling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6792807579040527}, {"text": "predicting the probability of a sentence", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.7415936390558878}]}, {"text": "Given a trained model, we can infer the quantitative likelihood that a sentence occurs.", "labels": [], "entities": []}, {"text": "Acceptability, on the other hand, indicates the extent to which a sentence is permissible or acceptable to native speakers of the language.", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9875695109367371}]}, {"text": "While acceptability is affected by frequency and exhibits gradience), there is limited research on the relationship between acceptability and probability.", "labels": [], "entities": []}, {"text": "In this paper, we consider the the task of unsupervised prediction of acceptability.", "labels": [], "entities": []}, {"text": "Speakers have robust intuitions about acceptability, and acceptability has been consistently rated on various scales.", "labels": [], "entities": []}, {"text": "The acceptability of a sentence appears to be relatively unaffected by its length (within certain bounds), or the frequency of its words, properties that we have confirmed experimentally.", "labels": [], "entities": []}, {"text": "By contrast sentence probability does depend on these factors.", "labels": [], "entities": []}, {"text": "To filter the effects of sentence length and word frequency, we devise normalising functions to map the probability of a sentence (inferred by our unsupervised language models) to an acceptability score. and present evidence that acceptability exhibits gradience.", "labels": [], "entities": []}, {"text": "Accordingly, we treat acceptability as a continuous variable here.", "labels": [], "entities": []}, {"text": "We train a variety of unsupervised models for the acceptability prediction task, and we assess the performance of these models by measuring the correlation between their normalised acceptability scores and the mean crowdsourced acceptability judgements on a set of test sentences.", "labels": [], "entities": [{"text": "acceptability prediction task", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.9176835815111796}]}, {"text": "There area number of NLP tasks to which our work can be fruitfully applied.", "labels": [], "entities": []}, {"text": "It can be used to evaluate the fluency of the output for machine translation and other language generation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7918235957622528}]}, {"text": "It could also contribute to automatic essay scoring, and to second language tutorial systems.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7350000590085983}]}, {"text": "There are several reasons to favour unsupervised models.", "labels": [], "entities": []}, {"text": "From an engineering perspective, unsupervised models offer greater portability to other domains and languages.", "labels": [], "entities": []}, {"text": "Our methodology takes only unannotated text as input.", "labels": [], "entities": []}, {"text": "Extending 1618 our methodology to other domains/languages is therefore straightforward, as it requires only a raw training corpus in that domain/language.", "labels": [], "entities": []}, {"text": "Our work may also have significant implications for the cognitive foundations of the representation and acquisition of linguistic knowledge.", "labels": [], "entities": [{"text": "representation and acquisition of linguistic knowledge", "start_pos": 85, "end_pos": 139, "type": "TASK", "confidence": 0.8381692667802175}]}, {"text": "The unannotated training corpora of our language models are impoverished input in comparison to the data available to humans language learners, who learn from a variety of data sources (visual and auditory cues, interaction with adults and peers in a non-linguistic environment, etc).", "labels": [], "entities": []}, {"text": "If an unsupervised language model can reliably predict human acceptability judgements, then it provides a benchmark of what humans could, in principle, achieve with the same learning algorithm.", "labels": [], "entities": []}, {"text": "Success in this task raises interesting questions about the nature of grammatical knowledge.", "labels": [], "entities": []}, {"text": "If acceptability judgments can be accurately modeled through these techniques, then it seems unnecessary to posit an underlying binary model of syntax which enumerates all and only the set of wellformed sentences.", "labels": [], "entities": []}, {"text": "Instead it is reasonable to suggest that humans represent linguistic knowledge as a probabilistic, rather than as a binary system.", "labels": [], "entities": []}, {"text": "Probability distributions provide a natural explanation of the gradience that characterises acceptability judgements.", "labels": [], "entities": []}, {"text": "Gradience is intrinsic to probability distributions, and to the acceptability scores that we derive from these distributions.", "labels": [], "entities": [{"text": "Gradience", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.985106348991394}]}, {"text": "While our results raise important questions concerning the nature of syntactic representation and of language acquisition, we leave them open for further research.", "labels": [], "entities": [{"text": "syntactic representation", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.7846037745475769}, {"text": "language acquisition", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7213717401027679}]}, {"text": "We refrain from making strong claims on cognitive issues here.", "labels": [], "entities": []}, {"text": "Clearly additional psychological evidence is required to motivate substantive conclusions on these issues, even if our results suggest them.", "labels": [], "entities": []}, {"text": "Our focus in this paper is on the task of predicting speakers' acceptability judgements through unsupervised language models.", "labels": [], "entities": [{"text": "predicting speakers' acceptability judgements", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.8397776633501053}]}, {"text": "We take this to be a problem in natural language processing, whose solution has useful applications in language technology.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6660474936167399}]}, {"text": "All models described in this paper are implemented in an open source toolkit.", "labels": [], "entities": []}, {"text": "We describe our dataset in Section 2, which consists of crowd sourced acceptability judgments applied to sentences with errors introduced through round trip machine translation.", "labels": [], "entities": []}, {"text": "We de-scribe the models and their results in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we present results with a different corpus based on English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9350968897342682}]}, {"text": "The new dataset shows that our observations generalise to another domain.", "labels": [], "entities": []}, {"text": "We compare our methodology to a supervised system in the acceptability prediction task in Section 5.", "labels": [], "entities": [{"text": "acceptability prediction task", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.9348523219426473}]}, {"text": "We look more closely at the influence of sentence length and lexical frequency in Section 6, and we show that the normalising functions succeed in neutralising these effects.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.8761942684650421}]}, {"text": "Finally, we discuss the implications of our results, and draw conclusions from them in Section 7 and Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we require a collection of sentences that exhibit varying degrees of grammatical well-formedness.", "labels": [], "entities": []}, {"text": "We use the dataset that we discuss in.", "labels": [], "entities": []}, {"text": "We translated British National Corpus) English sentences to four other languages -Norwegian, Spanish, Japanese and Chinese -and then back to English using Google Translate.", "labels": [], "entities": [{"text": "British National Corpus) English sentences", "start_pos": 14, "end_pos": 56, "type": "DATASET", "confidence": 0.9529287815093994}]}, {"text": "To collect human judgements of acceptability for the sentences, we used Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9597678383191427}]}, {"text": "A total of 2,500 sentences were annotated.", "labels": [], "entities": []}, {"text": "Three modes of presentation were used for rating a sentence: (1) binary with two options (unnatural vs. natural); (2) four options (extremely unnatural, somewhat unnatural, somewhat natural and extremely natural); and (3) a sliding scale with two extremes (extremely unnatural and extremely natural).", "labels": [], "entities": []}, {"text": "To aggregate the ratings over multiple speakers for each sentence, we computed the arithmetic mean.", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.9500040709972382}]}, {"text": "As there is a high correlation of mean ratings among different modes of presentation, we take the judgements for the four-option mode of presentation as the gold-standard for our experiments.", "labels": [], "entities": []}, {"text": "To predict the ratings of the 2,500 test sentences, we trained several probabilistic models on the BNC, and then used the trained models to infer the probabilities of the test sentences.", "labels": [], "entities": [{"text": "BNC", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.9697157144546509}]}, {"text": "Models are trained on the written portion of the BNC, which has approximately 100 million words (henceforth referred to as BNC-100M).", "labels": [], "entities": [{"text": "BNC", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8597654700279236}]}, {"text": "We used only the words, and no forms of annotation information in the BNC, as input to training.", "labels": [], "entities": [{"text": "BNC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9094987511634827}]}, {"text": "We first experiment with simple lexical N -gram models, and then move to Bayesian and neural network models, increasing the complexity of the models to better capture word dependencies.", "labels": [], "entities": []}, {"text": "To translate probability into acceptability scores, we compute several acceptability measures extracted from the model parameters.", "labels": [], "entities": []}, {"text": "The acceptability measures are variants of the sentence's log probability, devised to normalise sentence length and low frequency words.", "labels": [], "entities": []}, {"text": "These measures are summarised in.", "labels": [], "entities": []}, {"text": "For each measure (including LogProb as a baseline) we compute its Pearson correlation coefficient with the gold standard sentence mean rating to evaluate its effectiveness in predicting acceptability.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 66, "end_pos": 97, "type": "METRIC", "confidence": 0.966716448465983}]}, {"text": "We tokenised the training data (BNC-100M) and the test sentences using OpenNLP, and we converted all words to lowercase.", "labels": [], "entities": [{"text": "BNC-100M", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.7674195170402527}, {"text": "OpenNLP", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9326724410057068}]}, {"text": "To address out of vocabulary (OOV) words that are seen in the test sentences but not in the training data, we adopt the Berkeley Parser approach, where we replace low frequency or OOV words using the UNK signature.", "labels": [], "entities": [{"text": "UNK signature", "start_pos": 200, "end_pos": 213, "type": "DATASET", "confidence": 0.8739064931869507}]}, {"text": "We capture additional surface characteristics of the original word by attaching features at the end of the signature (e.g. the OOV word 1949 would be replaced by the signature UNK-NUM).", "labels": [], "entities": [{"text": "OOV word 1949", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.6314612130324045}, {"text": "UNK-NUM", "start_pos": 176, "end_pos": 183, "type": "DATASET", "confidence": 0.8426563739776611}]}], "tableCaptions": [{"text": " Table 2: Pearson's r of acceptability measure and mean sentence rating for all experimented models in BNC. Boldface", "labels": [], "entities": [{"text": "Pearson's r of acceptability measure", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.7546435395876566}, {"text": "mean sentence rating", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.8301376899083456}, {"text": "BNC", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8855558037757874}]}, {"text": " Table 3: Pearson's r of acceptability measure and mean sentence rating for all experimented models in ENWIKI. Boldface", "labels": [], "entities": [{"text": "Pearson's r of acceptability measure", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.7372405628363291}, {"text": "mean sentence rating", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.8046505649884542}, {"text": "ENWIKI", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.9229611158370972}]}, {"text": " Table 4: A comparison of results of our system and Heil-", "labels": [], "entities": []}]}