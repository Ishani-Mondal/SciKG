{"title": [{"text": "Model-based Word Embeddings from Decompositions of Count Matrices", "labels": [], "entities": []}], "abstractContent": [{"text": "This work develops anew statistical understanding of word embeddings induced from transformed count data.", "labels": [], "entities": []}, {"text": "Using the class of hidden Markov models (HMMs) underlying Brown clustering as a genera-tive model, we demonstrate how canoni-cal correlation analysis (CCA) and certain count transformations permit efficient and effective recovery of model parameters with lexical semantics.", "labels": [], "entities": []}, {"text": "We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE.", "labels": [], "entities": [{"text": "word similarity and analogy tasks", "start_pos": 105, "end_pos": 138, "type": "TASK", "confidence": 0.8105927467346191}]}], "introductionContent": [{"text": "The recent spike of interest in dense, lowdimensional lexical representations-i.e., word embeddings-is largely due to their ability to capture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks.", "labels": [], "entities": []}, {"text": "A successful method for deriving such embeddings is the negative sampling training of the skip-gram model suggested by and implemented in the popular software WORD2VEC.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 159, "end_pos": 167, "type": "DATASET", "confidence": 0.9088332056999207}]}, {"text": "The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way.", "labels": [], "entities": []}, {"text": "This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8321148157119751}]}, {"text": "What is the right matrix to decompose?", "labels": [], "entities": []}, {"text": "Are there rigorous justifications for the choice of matrix and count transformations?", "labels": [], "entities": []}, {"text": "In this paper, we answer some of these questions by investigating the decomposition specified by CCA, a powerful technique for inducing generic representations whose computation is efficiently and exactly reduced to that of a matrix singular value decomposition (SVD).", "labels": [], "entities": []}, {"text": "We build on and strengthen the work of which uses CCA for learning the class of HMMs underlying Brown clustering.", "labels": [], "entities": []}, {"text": "We show that certain count transformations enhance the accuracy of the estimation method and significantly improve the empirical performance of word representations derived from these model parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9989801049232483}]}, {"text": "In addition to providing a rigorous justification for CCA-based word embeddings, we also supply a general template that encompasses a range of spectral methods (algorithms employing SVD) for inducing word embeddings in the literature, including the method of.", "labels": [], "entities": []}, {"text": "In experiments, we demonstrate that CCA combined with the square-root transformation achieves the best result among spectral methods and performs competitively with other popular methods such as WORD2VEC and GLOVE on word similarity and analogy tasks.", "labels": [], "entities": [{"text": "word similarity and analogy tasks", "start_pos": 217, "end_pos": 250, "type": "TASK", "confidence": 0.7435921609401703}]}, {"text": "We additionally demonstrate that CCA embeddings provide the most competitive improvement when used as features in named-entity recognition (NER).", "labels": [], "entities": [{"text": "named-entity recognition (NER)", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.8509191036224365}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of various spectral methods on the development portion of data.", "labels": [], "entities": []}, {"text": " Table 1: Performance of CCA", "labels": [], "entities": [{"text": "CCA", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.4281752407550812}]}, {"text": " Table 3: Performance of different word embedding methods on the test portion of data. See the main text  for the configuration details of spectral methods.", "labels": [], "entities": []}]}