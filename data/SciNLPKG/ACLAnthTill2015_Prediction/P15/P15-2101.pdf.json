{"title": [{"text": "Reducing infrequent-token perplexity via variational corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Recurrent neural network (RNN) is recognized as a powerful language model (LM).", "labels": [], "entities": [{"text": "Recurrent neural network (RNN)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6930048863093058}]}, {"text": "We investigate deeper into its performance portfolio, which performs well on frequent grammatical patterns but much less soon less frequent terms.", "labels": [], "entities": []}, {"text": "Such portfolio is expected and desirable in applications like autocomplete, but is less useful in social content analysis where many creative, unexpected usages occur (e.g., URL insertion).", "labels": [], "entities": [{"text": "social content analysis", "start_pos": 98, "end_pos": 121, "type": "TASK", "confidence": 0.7241420348485311}, {"text": "URL insertion", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.7058335542678833}]}, {"text": "We adapt a generic RNN model and show that, with variational training corpora and epoch unfolding, the model improves its performance for the task of URL insertion suggestions.", "labels": [], "entities": [{"text": "URL insertion suggestions", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.8910297354062399}]}], "introductionContent": [{"text": "Just 135 most frequent words account for 50% text of the entire Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9625357985496521}]}, {"text": "But over 44% (22,010 out of 49,815) of Brown's vocabulary are hapax legomena . The intricate relationship between vocabulary words and their utterance frequency results in some important advancements in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 203, "end_pos": 236, "type": "TASK", "confidence": 0.7383740345637003}]}, {"text": "For example, tf-idf results from rules applied to word frequencies in global and local context.", "labels": [], "entities": []}, {"text": "A common preprocessing step for tf-idf is filtering rare words, which is usually justified for two reasons.", "labels": [], "entities": []}, {"text": "First, low frequency cutoff promises computational speedup due to Zipf's law.", "labels": [], "entities": []}, {"text": "Second, many believe that most NLP and machine learning algorithms demand repetitive patterns and reoccurrences, which are by definition missing in low frequency words.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1.1, \"Filtered cor- pus\" filters out all the hapax legomena in \"Full cor- pus\".", "labels": [], "entities": []}, {"text": " Table 1: Filtered corpus gains little in running time  or memory usage when using a RNN LM.", "labels": [], "entities": []}, {"text": " Table  1.2. The LM achieves exceptionally low perplex- ity on words such as <apostr.>s ('s, the posses- sive case), <comma> (, the comma). And these  tokens' high frequencies in corpus have promised  the model's average performance. Meanwhile, the  LM has bafflingly high perplexity on common- place words such as read and considering.", "labels": [], "entities": []}, {"text": " Table 2: A close look at RNN-LSTM's perplexity  at word level. \"Perplexity 1\" is model perplexity  based on filtered corpus (c.f.,", "labels": [], "entities": []}, {"text": " Table 1.1) and \"Per- plexity 2\" is based on full corpus.", "labels": [], "entities": []}, {"text": " Table 3: Experiments compare average perplexity produced by the proposed variational corpora approach  and other methods on a same test corpus. Bold fonts indicate best. \"Freq.\" indicates the average corpus- frequency (e.g., Freq.=1K means that words in this group, on average, appear 1,000 times in corpus).  Perplexity numbers are averaged over 5 runs with standard deviation reported in parentheses. GPU  memory usage and running time are also reported for each method.", "labels": [], "entities": [{"text": "Freq.", "start_pos": 172, "end_pos": 177, "type": "METRIC", "confidence": 0.9950802326202393}, {"text": "Freq.", "start_pos": 226, "end_pos": 231, "type": "METRIC", "confidence": 0.9679649472236633}]}, {"text": " Table 3. We apply our  method to locate URLs in over 400,000 Pinterest  captions. Unlike Facebook, Twitter, Pinterest is  not a \"social hub\" but rather an interest-discovery", "labels": [], "entities": []}]}