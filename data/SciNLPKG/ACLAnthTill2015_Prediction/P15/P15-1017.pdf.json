{"title": [{"text": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6581837832927704}]}], "abstractContent": [{"text": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools.", "labels": [], "entities": [{"text": "ACE event extraction", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.9154044191042582}]}, {"text": "These traditional approaches lack generalization , take a large amount of human effort and are prone to error propagation and data sparsity problems.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7025769650936127}]}, {"text": "This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools.", "labels": [], "entities": []}, {"text": "We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues.", "labels": [], "entities": []}, {"text": "However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences.", "labels": [], "entities": []}, {"text": "We propose a dynamic multi-pooling convolutional neu-ral network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information.", "labels": [], "entities": []}, {"text": "The experimental results show that our approach significantly outperforms other state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7886545658111572}, {"text": "Information Extraction (IE)", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.865569531917572}]}, {"text": "Current state-of-the-art methods ( often use a set of elaborately designed features that are extracted by textual analysis and linguistic knowledge.", "labels": [], "entities": []}, {"text": "In general, we can divide the features into two categories: lexical features and contextual features.", "labels": [], "entities": []}, {"text": "Lexical features contain part-of-speech tags (POS), entity information, and morphology features (e.g., token, lemma, etc.), which aim to capture semantics or the background knowledge of words.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence with an ambiguous word beats: S1: Obama beats McCain.", "labels": [], "entities": []}, {"text": "S2: Tyson beats his opponent . In S1, beats is a trigger of type Elect.", "labels": [], "entities": []}, {"text": "However, in S2, beats is a trigger of type Attack, which is more common than type Elect.", "labels": [], "entities": []}, {"text": "Because of the ambiguity, a traditional approach may mislabel beats in S1 as a trigger of Attack.", "labels": [], "entities": []}, {"text": "However, if we have the priori knowledge that Obama and McCain are presidential contenders, we have ample evidence to predict that beats is a trigger of type Elect.", "labels": [], "entities": []}, {"text": "We call these knowledge lexical-level clues.", "labels": [], "entities": []}, {"text": "To represent such features, the existing methods) often rely on human ingenuity, which is a time-consuming process and lacks generalization.", "labels": [], "entities": []}, {"text": "Furthermore, traditional lexical features in previous methods area one-hot representation, which may suffer from the data sparsity problem and may not be able to adequately capture the semantics of the words (.", "labels": [], "entities": []}, {"text": "To identify events and arguments more precisely, previous methods often captured contextual features, such as syntactic features, which aim to understand how facts are tied together from a larger field of view.", "labels": [], "entities": []}, {"text": "For example, in S3, there are two events that share three arguments as shown in.", "labels": [], "entities": []}, {"text": "From the dependency relation of nsubj between the argument cameraman and trigger died, we can induce a Victim role to cameraman in the Die event.", "labels": [], "entities": []}, {"text": "We call such information sentence-level clues.", "labels": [], "entities": []}, {"text": "However, the argument word cameraman and its trigger word fired are in different clauses, and there is no direct de- pendency path between them.", "labels": [], "entities": []}, {"text": "Thus it is difficult to find the Target role between them using traditional dependency features.", "labels": [], "entities": []}, {"text": "In addition, extracting such features depends heavily on the performance of pre-existing NLP systems, which could suffer from error propagation.", "labels": [], "entities": []}, {"text": "S3: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.", "labels": [], "entities": [{"text": "Palestine Hotel", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9680379331111908}]}, {"text": "To correctly attach cameraman to fired as a Target argument, we must exploit internal semantics over the entire sentence such that the Attack event results in Die event.", "labels": [], "entities": []}, {"text": "Recent improvements of convolutional neural networks (CNNs) have been proven to be efficient for capturing syntactic and semantics between words within a sentence) for NLP tasks.", "labels": [], "entities": []}, {"text": "CNNs typically use a max-pooling layer, which applies a max operation over the representation of an entire sentence to capture the most useful information.", "labels": [], "entities": []}, {"text": "However, in event extraction, one sentence may contain two or more events, and these events may share the argument with different roles.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.731368213891983}]}, {"text": "For example, there are two events in S3, namely, the Die event and Attack event.", "labels": [], "entities": []}, {"text": "If we use a traditional max-pooling layer and only keep the most important information to represent the sentence, we may obtain the information that depicts \"a cameraman died\" but miss the information about \"American tank fired on the Palestine Hotel\", which is important for predicting the Attack event and valuable for attaching cameraman to fired as an Target argument.", "labels": [], "entities": [{"text": "Palestine Hotel", "start_pos": 235, "end_pos": 250, "type": "DATASET", "confidence": 0.8611455857753754}, {"text": "predicting the Attack event", "start_pos": 276, "end_pos": 303, "type": "TASK", "confidence": 0.749619647860527}]}, {"text": "In our experiments, we found that such multiple-event sentences comprise 27.3% of our dataset, which is a phenomenon we cannot ignore.", "labels": [], "entities": []}, {"text": "In this paper, we propose a dynamic multipooling convolutional neural network (DMCNN) to address the problems stated above.", "labels": [], "entities": []}, {"text": "To capture lexical-level clues and reduce human effort, we introduce a word-representation model, which has been shown to be able to capture the meaningful semantic regularities of words ().", "labels": [], "entities": []}, {"text": "To capture sentence-level clues without using complicated NLP tools, and to reserve information more comprehensively, we devise a dynamic multi-pooling layer for CNN, which returns the maximum value in each part of the sentence according to event triggers and arguments.", "labels": [], "entities": []}, {"text": "In summary, the contributions of this paper are as follows: \u2022 We present a novel framework for event extraction, which can automatically induce lexical-level and sentence-level features from plain texts without complicated NLP preprocessing.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7358563989400864}]}, {"text": "\u2022 We devise a dynamic multi-pooling convolutional neural network (DMCNN), which aims to capture more valuable information within a sentence for event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7334440797567368}]}, {"text": "\u2022 We conduct experiments on a widely used ACE2005 event extraction dataset, and the experimental results show that our approach outperforms other state-of-the-art methods.", "labels": [], "entities": [{"text": "ACE2005 event extraction dataset", "start_pos": 42, "end_pos": 74, "type": "DATASET", "confidence": 0.8732756227254868}]}], "datasetContent": [{"text": "We utilized the ACE 2005 corpus as our dataset.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9807133277257284}]}, {"text": "For comparison, as the same as, and, we used the same test set with 40 newswire articles and the same development set with 30 other documents randomly selected from different genres and the rest 529 documents are used for training.", "labels": [], "entities": []}, {"text": "Similar to previous work (, we use the following criteria to judge the correctness of each predicted event mention: \u2022 A trigger is correct if its event subtype and offsets match those of a reference trigger.", "labels": [], "entities": []}, {"text": "\u2022 An argument is correctly identified if its event subtype and offsets match those of any of the reference argument mentions.", "labels": [], "entities": []}, {"text": "\u2022 An argument is correctly classified if its event subtype, offsets and argument role match those of any of the reference argument mentions.", "labels": [], "entities": []}, {"text": "Finally we use Precision (P ), Recall (R) and F measure (F 1 ) as the evaluation metrics.", "labels": [], "entities": [{"text": "Precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9956692457199097}, {"text": "Recall (R)", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9468374252319336}, {"text": "F measure (F 1 )", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.9699520866076151}]}], "tableCaptions": [{"text": " Table 1: Overall performance on blind test data", "labels": [], "entities": []}, {"text": " Table 2: The proportion of multiple events within  one sentence. 1/1 means that one sentence only  has one trigger or one argument plays a role in one  sentence; otherwise, 1/N is used.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the event extraction scores  obtained for the Traditional, CNN and DMCNN  models", "labels": [], "entities": [{"text": "event extraction", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7302744388580322}, {"text": "Traditional", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9513762593269348}, {"text": "DMCNN", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.569290816783905}]}, {"text": " Table 4: Comparison of the results for the tradi- tional lexical feature and our lexical feature. A de- notes the triggers or arguments appearing in both  training and test datasets, and B indicates all other  cases.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of the trigger-classification  score and argument-classification score obtained  by lexical-level features, sentence-level features  and a combination of both", "labels": [], "entities": []}]}