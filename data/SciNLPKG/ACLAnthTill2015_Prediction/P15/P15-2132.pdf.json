{"title": [{"text": "Compact Lexicon Selection with Spectral Methods", "labels": [], "entities": [{"text": "Compact Lexicon Selection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.720432182153066}]}], "abstractContent": [{"text": "In this paper, we introduce the task of selecting compact lexicon from large, noisy gazetteers.", "labels": [], "entities": []}, {"text": "This scenario arises often in practice, in particular spoken language understanding (SLU).", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.7955194363991419}]}, {"text": "We propose a simple and effective solution based on matrix decomposition techniques: canonical correlation analysis (CCA) and rank-revealing QR (RRQR) factorization.", "labels": [], "entities": []}, {"text": "CCA is first used to derive low-dimensional gazetteer embeddings from domain-specific search logs.", "labels": [], "entities": []}, {"text": "Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space.", "labels": [], "entities": [{"text": "RRQR", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.8807889223098755}]}, {"text": "Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of > 50% over randomly selected lexicon.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.8756427764892578}, {"text": "relative error reduction", "start_pos": 101, "end_pos": 125, "type": "METRIC", "confidence": 0.7284882565339407}]}], "introductionContent": [{"text": "Discriminative models trained with large quantities of arbitrary features area dominant paradigm in spoken language understanding (SLU) ().", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 100, "end_pos": 135, "type": "TASK", "confidence": 0.7910150488217672}]}, {"text": "An important category of these features comes from entity dictionaries or gazetteers-lists of phrases whose labels are given.", "labels": [], "entities": []}, {"text": "For instance, they can be lists of movies, music titles, actors, restaurants, and cities.", "labels": [], "entities": []}, {"text": "These features enable SLU models to robustly handle unseen entities attest time.", "labels": [], "entities": []}, {"text": "However, these lists are often massive and very noisy.", "labels": [], "entities": []}, {"text": "This is because they are typically obtained automatically by mining the web for recent entries (such as newly launched movie names).", "labels": [], "entities": []}, {"text": "Ideally, we would like an SLU model to have access to this vast source of information at deployment.", "labels": [], "entities": []}, {"text": "But this is difficult in practice because an SLU model needs to be light-weight to support fast user interaction.", "labels": [], "entities": []}, {"text": "It becomes more challenging when we consider multiple domains, languages, and locales.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the task of selecting a small, representative subset of noisy gazetteers that will nevertheless improve model performance nearly as much as the original lexicon.", "labels": [], "entities": []}, {"text": "This will allow an SLU model to take full advantage of gazetteer resources attest time without being overwhelmed by their scale.", "labels": [], "entities": []}, {"text": "Our selection method is two steps.", "labels": [], "entities": []}, {"text": "First, we gather relevant information for each gazetteer element using domain-specific search logs.", "labels": [], "entities": []}, {"text": "Then we perform CCA using this information to derive lowdimensional gazetteer embeddings.", "labels": [], "entities": []}, {"text": "Second, we use a subset selection method based on RRQR to locate gazetteer embeddings whose span approximates the the entire lexicon space (.", "labels": [], "entities": []}, {"text": "We show in slot tagging experiments that the gazetteer elements selected by our method not only preserve the performance of using full lexicon but even improve it in some cases.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.855657547712326}]}, {"text": "Compared to random selection, our method achieves average relative error reduction of > 50%.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 58, "end_pos": 82, "type": "METRIC", "confidence": 0.7961207628250122}]}], "datasetContent": [{"text": "To test the effectiveness of the proposed gazetteer selection method, we conduct slot tagging experiments across a test suite of three domains: Movies, Music and Places, which are very sensitive domains to gazetteer features.", "labels": [], "entities": [{"text": "gazetteer selection", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8428597450256348}, {"text": "slot tagging", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.7975743114948273}]}, {"text": "The task of slot tagging is to find the correct sequence of tags of words given a user utterance.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.8245777487754822}]}, {"text": "For example, in Places domain, a user could say \"search for home depot in kingsport\" and the phrase \"home depot\" and \"kingsport\" are tagged with Place Name and Location respectively.", "labels": [], "entities": []}, {"text": "The data statistics are shown in.", "labels": [], "entities": []}, {"text": "One domain can have various kinds of gazetteers.", "labels": [], "entities": []}, {"text": "For example, Places domain has business name, restaurant name, school name and etc.", "labels": [], "entities": []}, {"text": "Candidate dictionaries are mined from the web and search logs automatically using basic pattern matching approaches (e.g. entities sharing the same or similar context in queries or documents) and consequently contain significant amount of noise.", "labels": [], "entities": []}, {"text": "As the table indicates, the number of elements in total across all the gazetteers (#total gazet elements) in each domain are too large for models to consume.", "labels": [], "entities": []}, {"text": "In all our experiments, we trained conditional random fields (CRFs) () with the following features: (1) n-gram features up ton = 3, (2) regular expression features, and (3) Brown clusters () induced from search logs.", "labels": [], "entities": []}, {"text": "With these features, we compare the following methods to demonstrate the importance of adding appropriate gazetteers: \u2022 NoG: train without gazetteer features.", "labels": [], "entities": []}, {"text": "\u2022 AllG: train with all gazetteers.", "labels": [], "entities": [{"text": "AllG", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.5798186659812927}]}, {"text": "\u2022 RandG: train with randomly selected gazetteers.", "labels": [], "entities": [{"text": "RandG", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.5853860378265381}]}, {"text": "\u2022 RRQRG: train with gazetteers selected from RRQR.", "labels": [], "entities": [{"text": "RRQRG", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.5473197102546692}, {"text": "RRQR", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.9308727383613586}]}, {"text": "\u2022 RankAllG: train with all ranked gazetteers.", "labels": [], "entities": [{"text": "RankAllG", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.7418986558914185}]}, {"text": "Here gazetteer features are activated when a phrase contains an entity in a dictionary.", "labels": [], "entities": []}, {"text": "For RandG, we first sample a category of gazetteers uniformly and then choose a lexicon from gazetteers in that category.", "labels": [], "entities": []}, {"text": "The results when we use selected gazetteer randomly in whole categories are very low and did not include them here.", "labels": [], "entities": []}, {"text": "For selecting gazetteer methods (NoG, RnadG and RRQRG), we select 500,000 elements in total.", "labels": [], "entities": [{"text": "RRQRG", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.7632771134376526}]}], "tableCaptions": [{"text": " Table 4: Comparison of models evaluated on three do-", "labels": [], "entities": []}, {"text": " Table 6. We see that the  Ranked gazetteers approach (RankAllG) has con- sistent gains across domains over AllG.", "labels": [], "entities": [{"text": "AllG", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.955447256565094}]}, {"text": " Table 6: Comparison of models with or without ranked", "labels": [], "entities": []}]}