{"title": [{"text": "SOLAR: Scalable Online Learning Algorithms for Ranking", "labels": [], "entities": []}], "abstractContent": [{"text": "Traditional learning to rank methods learn ranking models from training data in a batch and offline learning mode, which suffers from some critical limitations, e.g., poor scalability as the model has to be retrained from scratch whenever new training data arrives.", "labels": [], "entities": []}, {"text": "This is clearly non-scalable for many real applications in practice where training data often arrives sequentially and frequently.", "labels": [], "entities": []}, {"text": "To overcome the limitations, this paper presents SOLAR anew framework of Scalable On-line Learning Algorithms for Ranking, to tackle the challenge of scalable learning to rank.", "labels": [], "entities": []}, {"text": "Specifically, we propose two novel SOLAR algorithms and analyze their IR measure bounds theoretically.", "labels": [], "entities": []}, {"text": "We conduct extensive empirical studies by comparing our SOLAR algorithms with conventional learning to rank algorithms on benchmark testbeds, in which promising results validate the efficacy and scalability of the proposed novel SOLAR algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to rank aims to learn some ranking model from training data using machine learning methods, which has been actively studied in information retrieval (IR).", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 136, "end_pos": 162, "type": "TASK", "confidence": 0.8454334378242493}]}, {"text": "Specifically, consider a document retrieval task, given a query, a ranking model assigns a relevance score to each document in a collection of documents, and then ranks the documents in decreasing order of relevance scores.", "labels": [], "entities": [{"text": "document retrieval task", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7739047805468241}]}, {"text": "The goal of learning to rank is to build a ranking model from training data of a set of queries by optimizing some IR performance measures using machine learning techniques.", "labels": [], "entities": []}, {"text": "In literature, various learning to rank techniques have been proposed, ranging from early pointwise approaches, to popular pairwise, and recent listwise approaches.", "labels": [], "entities": []}, {"text": "Learning to rank has many applications, including document retrieval, collaborative filtering, online ad, answer ranking for online QA in NLP, etc.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7974291145801544}, {"text": "answer ranking", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7696095407009125}]}, {"text": "Most existing learning to rank techniques follow batch and offline machine learning methodology, which typically assumes all training data are available prior to the learning task and the ranking model is trained by applying some batch learning method, e.g., neural networks or SVM.", "labels": [], "entities": []}, {"text": "Despite being studied extensively, the batch learning to rank methodology has some critical limitations.", "labels": [], "entities": []}, {"text": "One of serious limitations perhaps is its poor scalability for real-world web applications, where the ranking model has to be re-trained from scratch whenever new training data arrives.", "labels": [], "entities": []}, {"text": "This is apparently inefficient and non-scalable since training data often arrives sequentially and frequently in many real applications.", "labels": [], "entities": []}, {"text": "Besides, batch learning to rank methodology also suffers from slow adaption to fast-changing environment of web applications due to the static ranking models pre-trained from historical batch training data.", "labels": [], "entities": []}, {"text": "To overcome the above limitations, this paper investigates SOLAR -a new framework of Scalable Online Learning Algorithms for Ranking, which aims to learn a ranking model from a sequence of training data in an online learning fashion.", "labels": [], "entities": []}, {"text": "Specifically, by following the pairwise learning to rank framework, we formally formulate the learning problem, and then present two different SOLAR algorithms to solve the challenging task together with the analysis of their theoretical properties.", "labels": [], "entities": []}, {"text": "We conduct an extensive set of experiments by evaluating the performance of the proposed algorithms under different settings by comparing them with both online and batch algorithms on benchmark testbeds in literature.", "labels": [], "entities": []}, {"text": "As a summary, the key contributions of this pa-per include: (i) we present anew framework of Scalable Online Learning Algorithms for Ranking, which tackles the pairwise learning to ranking problem via a scalable online learning approach; (ii) we present two SOLAR algorithms: a first-order learning algorithm (SOLAR-I) and a second-order learning algorithm (SOLAR-II); (iii) we analyze the theoretical bounds of the proposed algorithms in terms of standard IR performance measures; and (iv) finally we examine the efficacy of the proposed algorithms by an extensive set of empirical studies on benchmark datasets.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 gives problem formulations of the proposed framework and presents our algorithms, followed by theoretical analysis in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents our experimental results, and Section 6 concludes this work and indicates future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct extensive experiments to evaluate the efficacy of our algorithms in two major aspects: (i) to examine the learning efficacy of the proposed SOLAR algorithms for online learning to rank tasks; (ii) to directly compare the proposed SOLAR algorithms with the state-of-the-art batch learning to rank algorithms.", "labels": [], "entities": []}, {"text": "Besides, we also show an application of our algorithms for transfer learning to rank tasks to demonstrate the importance of capturing changing search intention timely in real web applications.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8759737908840179}]}, {"text": "The results are in the supplemental file due to space limitation.", "labels": [], "entities": []}, {"text": "We adopt the popular benchmark testbed for learning to rank: LETOR 1.", "labels": [], "entities": [{"text": "LETOR 1", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9796268939971924}]}, {"text": "To make a comprehensive comparison, we perform experiments on all the available datasets in LETOR3.0 and LETOR4.0.", "labels": [], "entities": [{"text": "LETOR3.0", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.8560152649879456}, {"text": "LETOR4.0", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.8051038384437561}]}, {"text": "The statistics are shown in.", "labels": [], "entities": []}, {"text": "For performance evaluation metrics, we adopt the standard IR measures, including \"MAP\", \"NDCG@1\", \"NDCG@5\", and \"NDCG@10\".", "labels": [], "entities": [{"text": "MAP", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9689035415649414}]}, {"text": "This experiment evaluates the online learning performance of the proposed algorithms for online learning to rank tasks by comparing them with the existing \"Prank\" algorithm, a Perceptronbased pointwise online learning to rank algorithm, and a recently proposed \"Committee Perceptron (Com-P)\" algorithm, which explores the ensemble learning for Perceptron.", "labels": [], "entities": []}, {"text": "We evaluate the performance in terms of both online cumulative NDCG and MAP measures.", "labels": [], "entities": [{"text": "MAP", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.8661395311355591}]}, {"text": "As it is an online learning task, the parameter C of SOLAR-I is fixed to 10 \u22125 and the parameter \u03b3 of SOLAR-II is fixed to 10 4 for all the datasets, as suggested by, we set the number of experts in \"Com-P\" to 20.", "labels": [], "entities": []}, {"text": "All experiments were conducted over 10 random permutations of each dataset, and all results were averaged over the 10 runs.", "labels": [], "entities": []}, {"text": "give the results of NDCG on all the datasets, where the best results were bolded.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.7900875210762024}]}, {"text": "Several observations can be drawn as follows.", "labels": [], "entities": []}, {"text": "First of all, among all the algorithms, we found that both SOLAR-I and SOLAR-II achieve significantly better performance than Prank, which proves the efficacy of the proposed pairwise algorithms.", "labels": [], "entities": []}, {"text": "Second, we found that Prank (pointwise) performs extremely poor on several datasets (HP2003, HP2004, NP2003, NP2004, TD2003, TD2004).", "labels": [], "entities": [{"text": "HP2003", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9634429216384888}, {"text": "HP2004", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.8019804358482361}, {"text": "TD2003", "start_pos": 117, "end_pos": 123, "type": "DATASET", "confidence": 0.8833994269371033}, {"text": "TD2004", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.9206368327140808}]}, {"text": "By looking into the details, we found that it is likely because Prank (pointwise), as a pointwise algorithm, is highly sensitive to the imbalance of training data, and the above datasets are indeed highly imbalanced in which very few documents are labeled as relevant among about 1000 documents per query.", "labels": [], "entities": []}, {"text": "By contrast, the pairwise algorithm performs much better.", "labels": [], "entities": []}, {"text": "This observation further validates the importance of the proposed pairwise SOLAR algorithms that are insensitive to imbalance issue.", "labels": [], "entities": []}, {"text": "Last, by comparing the two SOLAR algorithms, we found SOLAR-II outperforms SOLAR-I inmost cases, validating the efficacy of exploiting second-order information.", "labels": [], "entities": []}, {"text": "This experiment aims to examine the scalability of the proposed SOLAR algorithms.", "labels": [], "entities": [{"text": "SOLAR", "start_pos": 64, "end_pos": 69, "type": "TASK", "confidence": 0.8963842988014221}]}, {"text": "We com-  pare it with RankSVM, a widely used and efficient batch algorithm.", "labels": [], "entities": [{"text": "RankSVM", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8618212938308716}]}, {"text": "For implementation, we adopt the code from 3 , which is known to be the fastest implementation.", "labels": [], "entities": []}, {"text": "illustrates the scalability evaluation on \"2008MQ\" dataset.", "labels": [], "entities": [{"text": "2008MQ\" dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9699389537175497}]}, {"text": "From the results, we observe that SOLAR is much faster (e.g., 100+ times faster on this dataset)and significantly more scalable than RankSVM.", "labels": [], "entities": [{"text": "SOLAR", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.5600983500480652}, {"text": "RankSVM", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.960206925868988}]}], "tableCaptions": [{"text": " Table 1.  For performance evaluation metrics, we adopt  the standard IR measures, including \"MAP\",  \"NDCG@1\", \"NDCG@5\", and \"NDCG@10\".", "labels": [], "entities": [{"text": "MAP", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9836070537567139}]}, {"text": " Table 1: LETOR datasets used in the experiments.", "labels": [], "entities": [{"text": "LETOR datasets", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9251478910446167}]}, {"text": " Table 2: Evaluation of NDCG performance of online learning to rank algorithms.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of NDCG of Online vs Batch Learning to Rank algorithms.", "labels": [], "entities": []}]}