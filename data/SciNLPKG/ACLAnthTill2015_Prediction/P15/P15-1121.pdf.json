{"title": [], "abstractContent": [{"text": "This paper proposes a novel approach for incorporating discourse information into machine comprehension applications.", "labels": [], "entities": []}, {"text": "Traditionally, such information is computed using off-the-shelf discourse analyz-ers.", "labels": [], "entities": []}, {"text": "This design provides limited opportunities for guiding the discourse parser based on the requirements of the target task.", "labels": [], "entities": [{"text": "discourse parser", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7099129557609558}]}, {"text": "In contrast, our model induces relations between sentences while optimizing a task-specific objective.", "labels": [], "entities": []}, {"text": "This approach enables the model to benefit from discourse information without relying on explicit annotations of discourse structure during training.", "labels": [], "entities": []}, {"text": "The model jointly identifies relevant sentences, establishes relations between them and predicts an answer.", "labels": [], "entities": []}, {"text": "We implement this idea in a discrim-inative framework with hidden variables that capture relevant sentences and relations unobserved during training.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that the discourse aware model outperforms state-of-the-art machine comprehension systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of machine comprehension concerns the automatic extraction of answers from a given passage.", "labels": [], "entities": [{"text": "automatic extraction of answers from a given passage", "start_pos": 47, "end_pos": 99, "type": "TASK", "confidence": 0.8256302624940872}]}, {"text": "Often, the relevant information required to answer a question is distributed across multiple sentences.", "labels": [], "entities": []}, {"text": "Understanding the relation(s) between these sentences is key to finding the correct answer.", "labels": [], "entities": []}, {"text": "To answer the question about why Sally put on her shoes , we need to infer that She put on her shoes and She went outside to walk are connected by a causality relation.", "labels": [], "entities": []}, {"text": "She put on her shoes.", "labels": [], "entities": []}, {"text": "She went outside to walk.", "labels": [], "entities": []}, {"text": "Missy the cat meowed to Sally.", "labels": [], "entities": []}, {"text": "Sally waved to Missy the cat.", "labels": [], "entities": []}, {"text": "\"Sally, Sally, come home\", Sally's mom calls out.", "labels": [], "entities": []}, {"text": "Sally runs home to her Mom.", "labels": [], "entities": []}, {"text": "Why did Sally put on her shoes?", "labels": [], "entities": []}, {"text": "A) To wave to Missy the cat B) To hear her name C) Because she wanted to go outside D) To come home Prior work has demonstrated the value of discourse relations in related applications such as question answering).", "labels": [], "entities": [{"text": "question answering", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.9217212796211243}]}, {"text": "Traditionally, however, these approaches rely on outputs from off-the-shelf discourse analyzers, using them as features for target applications.", "labels": [], "entities": []}, {"text": "Such pipeline designs provide limited opportunities for guiding the discourse parser based on the requirements of the end task.", "labels": [], "entities": []}, {"text": "Given a wide spectrum of discourse frameworks), it is not clear a priori what the optimal set of discourse annotations is for the task.", "labels": [], "entities": []}, {"text": "Moreover, a generic discourse parser may introduce additional errors due to the mismatch between its training corpus and a dataset used in an application.", "labels": [], "entities": []}, {"text": "In fact, the largest discourse treebanks are based on newspaper corpora), which differ significantly in style from text used in machine comprehension corpora.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach for incorporating discourse structure into machine comprehension applications.", "labels": [], "entities": []}, {"text": "Rather than using a standalone parser that is trained on external supervised data to annotate discourse relations, the model induces relations between sentences while optimizing a task-specific objective.", "labels": [], "entities": []}, {"text": "This design biases the model to learn relations at a granularity optimized for the machine comprehension task.", "labels": [], "entities": []}, {"text": "In contrast to a generic discourse analyzer, our method can also utilize additional information available in the machine comprehension context.", "labels": [], "entities": []}, {"text": "For instance, question types provide valuable cues for determining discourse relations, and thus can facilitate learning.", "labels": [], "entities": []}, {"text": "We implement these ideas in a discriminative log-linear model with hidden variables.", "labels": [], "entities": []}, {"text": "The model jointly identifies relevant sentences, establishes relations between them and predicts an answer.", "labels": [], "entities": []}, {"text": "Since the same set of sentences can give rise to multiple questions, we do not limit the model to a single discourse relation, but rather model a distribution over possible relations.", "labels": [], "entities": []}, {"text": "During training, we only have access to questions and gold answers.", "labels": [], "entities": []}, {"text": "Since relevant sentences and their relations are not known, we model them as hidden variables.", "labels": [], "entities": []}, {"text": "To guide the model towards linguistically plausible discourse relations, we add a few seed markers that are typical of each relation.", "labels": [], "entities": []}, {"text": "The model predicts relations not only based on the sentences, but also incorporates information about the question.", "labels": [], "entities": []}, {"text": "By decomposing the dependencies between model components, we can effectively train the model using a standard gradient descent approach.", "labels": [], "entities": []}, {"text": "We evaluate our model using a recently released machine comprehension dataset (.", "labels": [], "entities": []}, {"text": "In this corpus, roughly half of the questions rely on multiple sentences in the passage to generate the correct answer.", "labels": [], "entities": []}, {"text": "For baselines, we use the best published results on this dataset.", "labels": [], "entities": []}, {"text": "Our results demonstrate that our relation-aware model outperforms the individual baselines by up to 5.7% and rivals the performance of a state-ofthe-art combination system.", "labels": [], "entities": []}, {"text": "Moreover, we show that the discourse relations it predicts for sentence pairs exhibit considerable overlap with relations identified by human annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data and Setup We run our experiments on a recently compiled dataset for machine comprehension: MCTest ().", "labels": [], "entities": [{"text": "MCTest", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.8665288090705872}]}, {"text": "The data consists of two distinct sets: MC160 and MC500, which are of different sizes.", "labels": [], "entities": [{"text": "MC160", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9263516664505005}]}, {"text": "gives details on the data splits for each dataset.", "labels": [], "entities": []}, {"text": "Each passage has 4 questions, with 4 answer choices each.", "labels": [], "entities": []}, {"text": "The questions are also annotated into 2 types: single, if the question can be answered using a single sentence in the passage, or multi otherwise.", "labels": [], "entities": []}, {"text": "We do not use the type information in our learning; we only use it for categorizing accuracy during evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9965656399726868}]}, {"text": "We report final results on all our models trained with \u03bb = 0.1, tuned using the Dev sets.", "labels": [], "entities": [{"text": "Dev sets", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.8091313540935516}]}, {"text": "Evaluation We report accuracy scores for each model averaged over the questions in the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9994105100631714}]}, {"text": "For each question, the system gains 1 point if it scores the correct answer highest and 0 otherwise.", "labels": [], "entities": []}, {"text": "In case of ties, we use an inverse weighting  scheme to assign partial credit.", "labels": [], "entities": []}, {"text": "So, if three answers (including the correct one) tie for the highest score, the system gains 1/3 points.", "labels": [], "entities": []}, {"text": "Baselines We use the systems proposed by as our baselines.", "labels": [], "entities": []}, {"text": "These systems have the best reported scores on this dataset.", "labels": [], "entities": []}, {"text": "The first baseline, SWD, uses a sliding window to count matches between the passage words and the words in the answer.", "labels": [], "entities": []}, {"text": "This is then combined with a score representing the average distance between answer and question words in the passage.", "labels": [], "entities": []}, {"text": "The second baseline, RTE, uses a textual entailment recognizer () to determine if the answer (turned into a statement along with the question) is entailed by the passage.", "labels": [], "entities": [{"text": "RTE", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.49267223477363586}]}, {"text": "The third system, RTE+SWD, is a weighted combination of the first two baselines and achieves the highest accuracy on the dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9992130994796753}]}], "tableCaptions": [{"text": " Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  \u2020 vs RTE.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9965319633483887}, {"text": "Sentence window  (k)", "start_pos": 192, "end_pos": 212, "type": "METRIC", "confidence": 0.8930160164833069}, {"text": "Statistical significance", "start_pos": 268, "end_pos": 292, "type": "METRIC", "confidence": 0.9154776036739349}, {"text": "RTE", "start_pos": 385, "end_pos": 388, "type": "METRIC", "confidence": 0.6593965291976929}]}, {"text": " Table 5: Recall (%) of relevant sentence(s) in the ranking by models 1, 2 and 3 compared with a match- frequency baseline (Freq) at various thresholds, for different question types in MC160. Question fre- quencies are in parentheses. Bold numbers represent best scores.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9866803288459778}, {"text": "match- frequency baseline (Freq)", "start_pos": 97, "end_pos": 129, "type": "METRIC", "confidence": 0.8973890713282994}, {"text": "MC160", "start_pos": 185, "end_pos": 190, "type": "DATASET", "confidence": 0.9540048837661743}]}, {"text": " Table 6: Recall of annotated relations at various  thresholds in the ordered relation distribution pre- dicted by model 3. Relation frequencies are in  parentheses.", "labels": [], "entities": [{"text": "Relation", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9743250012397766}]}]}