{"title": [], "abstractContent": [{"text": "We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8681628306706747}]}, {"text": "In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses.", "labels": [], "entities": []}, {"text": "These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks.", "labels": [], "entities": []}, {"text": "The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic machine translation (MT) evaluation is a necessary step when developing or comparing MT systems.", "labels": [], "entities": [{"text": "Automatic machine translation (MT) evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7979533459459033}, {"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9331144094467163}]}, {"text": "Reference-based MT evaluation, i.e., comparing the system output to one or more human reference translations, is the most common approach.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8933647274971008}]}, {"text": "Existing MT evaluation measures typically output an absolute quality score by computing the similarity between the machine and the human translations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9655586183071136}]}, {"text": "In the simplest case, the similarity is computed by counting word n-gram matches between the translation and the reference.", "labels": [], "entities": [{"text": "similarity", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9868113398551941}]}, {"text": "This is the case of BLEU (), which has been the standard for MT evaluation for years.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9986975193023682}, {"text": "MT evaluation", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9304387271404266}]}, {"text": "Nonetheless, more recent evaluation measures take into account various aspects of linguistic similarity, and achieve better correlation with human judgments.", "labels": [], "entities": []}, {"text": "Having absolute quality scores at the sentence level allows to rank alternative translations fora given source sentence.", "labels": [], "entities": []}, {"text": "This is useful, for instance, for statistical machine translation (SMT) parameter tuning, for system comparison, and for assessing the progress during MT system development.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) parameter tuning", "start_pos": 34, "end_pos": 88, "type": "TASK", "confidence": 0.8432420417666435}, {"text": "system comparison", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7762194871902466}, {"text": "MT system development", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.8967709342638651}]}, {"text": "The quality of automatic MT evaluation metrics is usually assessed by computing their correlation with human judgments.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9497767984867096}]}, {"text": "To that end, quality rankings of alternative translations have been created by human judges.", "labels": [], "entities": []}, {"text": "It is known that assigning an absolute score to a translation is a difficult task for humans.", "labels": [], "entities": []}, {"text": "Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement.", "labels": [], "entities": []}, {"text": "These human quality judgments can be used to train automatic metrics.", "labels": [], "entities": []}, {"text": "This supervised learning can be oriented to predict absolute scores, e.g., using regression, or rankings.", "labels": [], "entities": []}, {"text": "A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better.", "labels": [], "entities": []}, {"text": "This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses.", "labels": [], "entities": []}, {"text": "In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation.", "labels": [], "entities": []}, {"text": "Although the pairwise setting does not produce absolute quality scores (i.e., it is not an evaluation metric applicable to a single translation), it is useful and arguably sufficient for most evaluation and MT development scenarios.", "labels": [], "entities": [{"text": "MT development", "start_pos": 207, "end_pos": 221, "type": "TASK", "confidence": 0.9172963201999664}]}, {"text": "Recently, presented a learning framework for this pairwise setting, based on preference kernels and support vector machines (SVM).", "labels": [], "entities": []}, {"text": "They obtained promising results using syntactic and discourse-based structures.", "labels": [], "entities": []}, {"text": "However, using convolution kernels over complex structures comes at a high computational cost both at training and at testing time because the use of kernels requires that the SVM operate in the much slower dual space.", "labels": [], "entities": []}, {"text": "Thus, some simplification is needed to make it practical.", "labels": [], "entities": []}, {"text": "While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction.", "labels": [], "entities": []}, {"text": "We present a novel neural-based architecture for learning in the pairwise setting for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.969007134437561}]}, {"text": "Lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations and fed into the input layer, together with a set of individual real-valued features coming from simple pre-existing MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 273, "end_pos": 286, "type": "TASK", "confidence": 0.9044781923294067}]}, {"text": "A hidden layer, motivated by our intuitions on the pairwise ranking problem, is used to capture interactions between the relevant input components.", "labels": [], "entities": []}, {"text": "Finally, we present a task-oriented cost function, specifically tailored for this problem.", "labels": [], "entities": []}, {"text": "Our evaluation results on the WMT12 metrics task benchmark datasets show very high correlation with human judgments.", "labels": [], "entities": [{"text": "WMT12 metrics task benchmark datasets", "start_pos": 30, "end_pos": 67, "type": "DATASET", "confidence": 0.8090596795082092}]}, {"text": "These results clearly surpass) and are comparable to the best previously reported results for this dataset, achieved by), which is a much heavier combination-based metric.", "labels": [], "entities": []}, {"text": "Another advantage of the proposed architecture is efficiency.", "labels": [], "entities": []}, {"text": "Due to the vector-based compression of the linguistic structure and the relatively reduced size of the network, testing is fast, which would greatly facilitate the practical use of this approach in real MT evaluation and development.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 203, "end_pos": 216, "type": "TASK", "confidence": 0.9659237265586853}]}, {"text": "Finally, we empirically show that syntacticallyand semantically-oriented embeddings can be incorporated to produce sizeable and cumulative gains in performance over a strong combination of pre-existing MT evaluation measures (BLEU, NIST, METEOR, and TER).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 202, "end_pos": 215, "type": "TASK", "confidence": 0.8789511322975159}, {"text": "BLEU", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.9964970946311951}, {"text": "NIST", "start_pos": 232, "end_pos": 236, "type": "DATASET", "confidence": 0.8202531337738037}, {"text": "METEOR", "start_pos": 238, "end_pos": 244, "type": "METRIC", "confidence": 0.9124280214309692}, {"text": "TER", "start_pos": 250, "end_pos": 253, "type": "METRIC", "confidence": 0.9928561449050903}]}, {"text": "This is promising evidence towards our longer-term goal of defining a general platform for integrating varied linguistic information and for producing more informed MT evaluation measures.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.9628230631351471}]}], "datasetContent": [{"text": "In this section, we describe the different aspects of our general experimental setup (we will discuss some extensions thereof in Section 6), starting with a description of the input representations we use to capture the syntactic and semantic characteristics of the two hypothesis translations and the corresponding reference, as well as the datasets used to evaluate the performance of our model.", "labels": [], "entities": []}, {"text": "We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.9485524296760559}, {"text": "WMT12", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.720143735408783}, {"text": "WMT13 Metrics shared tasks", "start_pos": 104, "end_pos": 130, "type": "DATASET", "confidence": 0.8432354778051376}]}, {"text": "We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru).", "labels": [], "entities": [{"text": "WMT11", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9708388447761536}, {"text": "WMT12 datasets", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.925727367401123}, {"text": "WMT13", "start_pos": 166, "end_pos": 171, "type": "DATASET", "confidence": 0.971207857131958}]}, {"text": "We evaluate our metrics in terms of correlation with human judgments measured using Kendall's \u03c4 . We report \u03c4 for the individual languages as well as macro-averaged across all languages.", "labels": [], "entities": []}, {"text": "Note that there were different versions of \u03c4 at WMT over the years.", "labels": [], "entities": [{"text": "WMT", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.8909034729003906}]}, {"text": "Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14.", "labels": [], "entities": [{"text": "WMT", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9098623394966125}, {"text": "WMT13", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9783897399902344}, {"text": "WMT14", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.9911770820617676}]}, {"text": "Here we use the strict version used at WMT11 and WMT12.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9773371815681458}, {"text": "WMT12", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9680328965187073}]}, {"text": "Datasets: We train our neural models on WMT11 and we evaluate them on WMT12.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9774871468544006}, {"text": "WMT12", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.990208625793457}]}, {"text": "We further use a random subset of 5,000 examples from WMT13 as a validation set to implement early stopping.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.9576420187950134}]}, {"text": "Early stopping: We train on WMT11 for up to 10,000 epochs, and we calculate Kendall's \u03c4 on the development set after each epoch.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.9829298257827759}, {"text": "Kendall's \u03c4", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.7548354466756185}]}, {"text": "We then select the model that achieves the highest \u03c4 on the validation set; in case of ties for the best \u03c4 , we select the latest epoch that achieved the highest \u03c4 . Network parameters: We train our neural network using SGD with adagrad, an initial learning rate of \u03b7 = 0.01, mini-batches of size 30, and L 2 regularization with a decay parameter \u03bb = 1e \u22124 . We initialize the weights for our matrices by sampling from a uniform distribution following.", "labels": [], "entities": []}, {"text": "We further set the size of each of our pairwise hidden layers H to four nodes, and we normalize the input data using minmax to map the feature values to the range [\u22121, 1].", "labels": [], "entities": []}, {"text": "The main findings of our experiments are shown in.", "labels": [], "entities": []}, {"text": "Section I of shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9533723294734955}, {"text": "BLEU", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.9987406134605408}, {"text": "TER", "start_pos": 253, "end_pos": 256, "type": "METRIC", "confidence": 0.9942995309829712}, {"text": "METEOR", "start_pos": 262, "end_pos": 268, "type": "METRIC", "confidence": 0.9924238920211792}]}, {"text": "We will refer to the set of these four metrics as 4METRICS.", "labels": [], "entities": []}, {"text": "These metrics are not tuned and achieve Kendall's \u03c4 between 18.5 and 23.5.", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.8678144613901774}]}, {"text": "Section II of shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX25 and WIKI-GW25.", "labels": [], "entities": [{"text": "WIKI-GW25", "start_pos": 122, "end_pos": 131, "type": "DATASET", "confidence": 0.9192333221435547}]}, {"text": "These networks achieve modest \u03c4 values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference.", "labels": [], "entities": []}, {"text": "However, as will be discussed below, their contribution is complementary to the four previous evaluation metrics and will lead to significant improvements in combination with them.", "labels": [], "entities": []}, {"text": "Section III of shows the results for neural networks that combine the four metrics from 4METRICS with SYNTAX25 and WIKI-GW25.", "labels": [], "entities": [{"text": "WIKI-GW25", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.9098567366600037}]}, {"text": "We can see that just combining the four metrics in a flat neural net (i.e., no hidden layer), which is equivalent to a logistic regression, yields a \u03c4 of 27.06, which is better than the best of the four metrics by 3.5 points absolute, and also better by over 1.5 points absolute than the best metric that participated at the WMT12 metrics task competition (SPEDE07PP with \u03c4 = 25.4).", "labels": [], "entities": [{"text": "WMT12 metrics task competition", "start_pos": 325, "end_pos": 355, "type": "TASK", "confidence": 0.5945192128419876}]}, {"text": "Indeed, 4METRICS is a strong mix that involves not only simple lexical overlap but also approximate matching, paraphrases, edit distance, lengths, etc.", "labels": [], "entities": []}, {"text": "Yet, adding to 4METRICS the embedding vectors yields sizeable further improvements: +1.5 and +2.0 points absolute when adding SYNTAX25 and WIKI-GW25, respectively.", "labels": [], "entities": [{"text": "WIKI-GW25", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.8712214231491089}]}, {"text": "Finally, adding both yields even further improvements close to \u03c4 of 30 (+2.64 \u03c4 points), showing that lexical semantics and syntactic representations are complementary.", "labels": [], "entities": []}, {"text": "Section IV of puts these numbers in perspective: it lists the \u03c4 for the top three systems that participated at WMT12, whose scores ranged between We can see that 4METRICS is much stronger than the winner at WMT12, and thus arguably a baseline hard to improve upon.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.9433860778808594}, {"text": "4METRICS", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.8695588111877441}, {"text": "WMT12", "start_pos": 207, "end_pos": 212, "type": "DATASET", "confidence": 0.9703998565673828}]}, {"text": "While our results are slightly behind those of DiscoTK (), we should note that we only combine four metrics, plus the vectors, while DiscoTK combines over 20 metrics, many of which are costly to compute.", "labels": [], "entities": [{"text": "DiscoTK", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.8989588022232056}, {"text": "DiscoTK", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9024759531021118}]}, {"text": "On the other hand, we work in a ranking framework, i.e., we are not interested in producing an absolute score, but in making pairwise decisions only.", "labels": [], "entities": []}, {"text": "Mapping these pairwise decisions into an absolute score is challenging and in our experiments it leads to a slight drop in \u03c4 (results omitted hereto save space).", "labels": [], "entities": []}, {"text": "The only other result on WMT12 by authors working with our pairwise framework is our own previous work, where we used a preference kernel approach to combine syntactic and discourse trees with lexical information; as we can see, our earlier results are 6 absolute points lower than those we achieve here.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.6293709874153137}]}, {"text": "Moreover, our NN approach offers advantages over SVMs in terms of computational cost.", "labels": [], "entities": []}, {"text": "Based on these results, we can conclude that word embeddings, whether syntactic or semantic, offer generalizations that efficiently complement very strong metric combinations, and thus should be considered when designing future MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation metrics", "start_pos": 228, "end_pos": 249, "type": "TASK", "confidence": 0.9054178794225057}]}], "tableCaptions": [{"text": " Table 1: Kendall's tau (\u03c4 ) on the WMT12 dataset for various metrics. Notes: (i) the version of METEOR that took part in the", "labels": [], "entities": [{"text": "Kendall's tau (\u03c4 )", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7024639000495275}, {"text": "WMT12 dataset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9913392066955566}, {"text": "METEOR", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.4667196571826935}]}, {"text": " Table 2: Kendall's \u03c4 on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,", "labels": [], "entities": [{"text": "WMT12", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.8747817873954773}, {"text": "BLEUCOMP", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9775355458259583}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.7308384776115417}]}, {"text": " Table 3: Average Kendall's \u03c4 on WMT12 for semantic vec-", "labels": [], "entities": [{"text": "Average Kendall's \u03c4", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.6383343189954758}, {"text": "WMT12", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.7554971575737}]}, {"text": " Table 4: Kendall's tau (\u03c4 ) on the WMT12 dataset for al-", "labels": [], "entities": [{"text": "Kendall's tau (\u03c4 )", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7409347792466482}, {"text": "WMT12 dataset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9781765341758728}]}, {"text": " Table 5: Kendall's tau (\u03c4 ) on WMT12 for alternative cost", "labels": [], "entities": [{"text": "WMT12", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9727875590324402}]}]}