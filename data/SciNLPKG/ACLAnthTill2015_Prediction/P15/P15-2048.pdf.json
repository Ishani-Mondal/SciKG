{"title": [{"text": "Embedding Methods for Fine Grained Entity Type Classification", "labels": [], "entities": [{"text": "Fine Grained Entity Type Classification", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.6867452919483185}]}], "abstractContent": [{"text": "We propose anew approach to the task of fine grained entity type classifications based on label embeddings that allows for information sharing among related labels.", "labels": [], "entities": [{"text": "fine grained entity type classifications", "start_pos": 40, "end_pos": 80, "type": "TASK", "confidence": 0.6250236213207245}]}, {"text": "Specifically, we learn an embedding for each label and each feature such that labels which frequently co-occur are close in the embedded space.", "labels": [], "entities": []}, {"text": "We show that it out-performs state-of-the-art methods on two fine grained entity-classification benchmarks and that the model can exploit the finer-grained labels to improve classification of standard coarse types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity type classification is the task of assigning type labels (e.g., person, location, organization) to mentions of entities in documents.", "labels": [], "entities": [{"text": "Entity type classification is the task of assigning type labels (e.g., person, location, organization) to mentions of entities in documents", "start_pos": 0, "end_pos": 139, "type": "Description", "confidence": 0.7343561005592346}]}, {"text": "These types are useful for deeper natural language analysis such as coreference resolution, relation extraction (, and downstream applications such as knowledge base construction and question answering (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.9638656973838806}, {"text": "relation extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8580167889595032}, {"text": "knowledge base construction", "start_pos": 151, "end_pos": 178, "type": "TASK", "confidence": 0.6316906909147898}, {"text": "question answering", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.8900345265865326}]}, {"text": "Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20).", "labels": [], "entities": [{"text": "entity type classification tasks", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.7206375747919083}]}, {"text": "Recent work has focused on a much larger set of fine grained labels ().", "labels": [], "entities": []}, {"text": "Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy.", "labels": [], "entities": []}, {"text": "See for the label sets used in our experiments.", "labels": [], "entities": []}, {"text": "A mention labeled with type artist should also be labeled with all ancestors of artist.", "labels": [], "entities": []}, {"text": "Since we allow mentions to have multiple labels, this is a multilabel classification task.", "labels": [], "entities": [{"text": "multilabel classification task", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.7835643291473389}]}, {"text": "Multiple labels typically correspond to a single path in the tree (from root to a leaf or internal node).", "labels": [], "entities": []}, {"text": "An important aspect of context-dependent fine grained entity type classification is that mentions of an entity can have different types depending on the context.", "labels": [], "entities": [{"text": "context-dependent fine grained entity type classification", "start_pos": 23, "end_pos": 80, "type": "TASK", "confidence": 0.6487943232059479}]}, {"text": "Consider the following example: Madonna starred as Breathless Mahoney in the film Dick Tracy.", "labels": [], "entities": []}, {"text": "In this context, the most appropriate label for the mention Madonna is actress, since the sentence talks about her role in a film.", "labels": [], "entities": []}, {"text": "In the majority of other cases, Madonna is likely to be labeled as a musician.", "labels": [], "entities": []}, {"text": "The main difficulty in fine grained entity type classification is the absence of labeled training examples.", "labels": [], "entities": [{"text": "fine grained entity type classification", "start_pos": 23, "end_pos": 62, "type": "TASK", "confidence": 0.6203300058841705}]}, {"text": "Training data is typically generated automatically (e.g. by mapping Freebase labels of resolved entities), without taking context into account, so it is common for mentions to have noisy labels.", "labels": [], "entities": []}, {"text": "In our example, the labels for the mention Madonna would include musician, actress, author, and potentially others, even though not all of these labels apply here.", "labels": [], "entities": []}, {"text": "Ideally, a fine grained type classification system should be robust to such noisy training data, as well as capable of exploiting relationships between labels during learning.", "labels": [], "entities": [{"text": "type classification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7017469108104706}]}, {"text": "We describe a model that uses a ranking loss-which tends to be more robust to label noise-and that learns a joint representation of features and labels, which allows for information sharing among related labels.", "labels": [], "entities": []}, {"text": "1 A related idea to learn output representations for multiclass document classification and part-of-speech tagging was considered in.", "labels": [], "entities": [{"text": "multiclass document classification", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.6582319041093191}, {"text": "part-of-speech tagging", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7178482115268707}]}, {"text": "We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks.", "labels": [], "entities": []}, {"text": "We also evaluate our model on standard coarse type classification and find that training embedding models on all fine grained labels gives better results than training it on just the coarse types of interest.", "labels": [], "entities": [{"text": "coarse type classification", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6550484697024027}]}], "datasetContent": [{"text": "Setup and Baselines We evaluate our methods on two publicly available datasets that are manually annotated with gold labels for fine grained entity type classification: GFT (Google Fine Types;) and FIGER ().", "labels": [], "entities": [{"text": "fine grained entity type classification", "start_pos": 128, "end_pos": 167, "type": "TASK", "confidence": 0.6948574781417847}, {"text": "GFT", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.8318607211112976}, {"text": "FIGER", "start_pos": 198, "end_pos": 203, "type": "METRIC", "confidence": 0.9734550714492798}]}, {"text": "On the GFT dataset, we compare with state-of-the-art baselines from: flat logistic regression (FLAT), an extension of multiclass logistic regression for multilabel classification problems; and multiple independent binary logistic regression (BINARY), one per label t \u2208 {1, 2, . .", "labels": [], "entities": [{"text": "GFT dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.8745549917221069}, {"text": "FLAT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.8353680372238159}, {"text": "BINARY", "start_pos": 242, "end_pos": 248, "type": "METRIC", "confidence": 0.9075124859809875}]}, {"text": "On the FIGER dataset, we compare with a state-of-the-art baseline from.", "labels": [], "entities": [{"text": "FIGER dataset", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8580983281135559}]}, {"text": "We denote the standard embedding method by WSABIE and its extension by K-WSABIE.", "labels": [], "entities": [{"text": "WSABIE", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8604444265365601}]}, {"text": "We fix our embedding size to H = 50.", "labels": [], "entities": []}, {"text": "We report microaveraged precision, recall, and F1-score for each of the competing methods (this is called Loose Micro by Ling and Weld).", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9605866074562073}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9994156360626221}, {"text": "F1-score", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9997296929359436}]}, {"text": "When development data is available, we use it to tune \u03b4 by optimizing F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9961112141609192}]}, {"text": "Training data Because we have no manually annotated data, we create training data using the technique described in.", "labels": [], "entities": []}, {"text": "A set of 133,000 news documents are automatically annotated by a parser, a mention chunker, and an entity resolver that assigns Freebase types to entites, which we map to fine grained labels.", "labels": [], "entities": []}, {"text": "This approach results in approximately 3 million training examples which we use to train all the models evaluated below.", "labels": [], "entities": []}, {"text": "The only difference between models trained for different tasks is the mapping from Freebase types.", "labels": [], "entities": []}, {"text": "lists the features we use-the same set as used by, and very similar to those used by Ling and Weld.", "labels": [], "entities": []}, {"text": "String features are randomly hashed to a value in 0 to 999,999, which simplifies feature extraction and adds some additional regularization (.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7457315027713776}]}], "tableCaptions": [{"text": " Table 1: List of features used in our experiments, similar to features in Gillick et al. (2014). Features are extracted from each", "labels": [], "entities": []}, {"text": " Table 2: Mention counts in our datasets.", "labels": [], "entities": [{"text": "Mention counts", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.953379213809967}]}, {"text": " Table 3: Precision (P), Recall (R), and F1-score on the GFT", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9513144493103027}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9648747891187668}, {"text": "F1-score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.999678373336792}, {"text": "GFT", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.7971832752227783}]}, {"text": " Table 4: Precision (P), Recall (R), and F1-score on the", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9526529014110565}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.965378537774086}, {"text": "F1-score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9997536540031433}]}, {"text": " Table 5: Comparison of two WSABIE models on coarse", "labels": [], "entities": [{"text": "WSABIE", "start_pos": 28, "end_pos": 34, "type": "TASK", "confidence": 0.43598473072052}]}, {"text": " Table 6: WSABIE model's Precision (P), Recall (R), and", "labels": [], "entities": [{"text": "WSABIE", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.4657510817050934}, {"text": "Precision (P)", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9601220935583115}, {"text": "Recall (R)", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9637341201305389}]}]}