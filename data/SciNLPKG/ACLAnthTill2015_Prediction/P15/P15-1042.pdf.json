{"title": [{"text": "Learning Bilingual Sentiment Word Embeddings for Cross-language Sentiment Classification", "labels": [], "entities": [{"text": "Cross-language Sentiment Classification", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.8118508060773214}]}], "abstractContent": [{"text": "The sentiment classification performance relies on high-quality sentiment resources.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9682663083076477}]}, {"text": "However, these resources are imbalanced in different languages.", "labels": [], "entities": []}, {"text": "Cross-language sentiment classification (CLSC) can leverage the rich resources in one language (source language) for sentiment classification in a resource-scarce language (target language).", "labels": [], "entities": [{"text": "Cross-language sentiment classification (CLSC)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8737445374329885}, {"text": "sentiment classification", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.9503977298736572}]}, {"text": "Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the sentiment information of text.", "labels": [], "entities": []}, {"text": "This paper proposes an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC.", "labels": [], "entities": [{"text": "learning bilingual sentiment word embeddings (BSWE)", "start_pos": 35, "end_pos": 86, "type": "TASK", "confidence": 0.7156461328268051}]}, {"text": "The proposed B-SWE incorporate sentiment information of text into bilingual embeddings.", "labels": [], "entities": []}, {"text": "Furthermore , we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on large-scale parallel corpora.", "labels": [], "entities": [{"text": "BSWE", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.5537124872207642}]}, {"text": "Experiments on NLP&CC 2013 CLSC dataset show that our approach outperforms the state-of-the-art systems.", "labels": [], "entities": [{"text": "NLP&CC 2013 CLSC dataset", "start_pos": 15, "end_pos": 39, "type": "DATASET", "confidence": 0.9485630591710409}]}], "introductionContent": [{"text": "Sentiment classification is a task of predicting sentiment polarity of text, which has attracted considerable interest in the NLP field.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9598079323768616}, {"text": "predicting sentiment polarity of text", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.8835310816764832}]}, {"text": "To date, a number of corpus-based approaches () have been developed for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.9529230892658234}]}, {"text": "The approaches heavily rely on quality and quantity of the labeled corpora, which are considered as the most valuable resources in sentiment classification task.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.9519475996494293}]}, {"text": "However, such sentiment resources are imbalanced in different languages.", "labels": [], "entities": []}, {"text": "To leverage resources in the source language to improve the sentiment classification performance in the target language, cross-language sentiment classification (CLSC) approaches have been investigated.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.9095032513141632}, {"text": "cross-language sentiment classification (CLSC)", "start_pos": 121, "end_pos": 167, "type": "TASK", "confidence": 0.8257275521755219}]}, {"text": "The traditional CLSC approaches employ machine translation (MT) systems to translate corpora in the source language into the target language, and train the sentiment classifiers in the target language (.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8506803393363953}]}, {"text": "Directly employing the translated resources for sentiment classification in the target language is simple and could get acceptable results.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.9601701498031616}]}, {"text": "However, the gap between the source language and target language inevitably impacts the performance of sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.9449186027050018}]}, {"text": "To improve the classification accuracy, multiview approaches have been proposed.", "labels": [], "entities": [{"text": "classification", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.9703477621078491}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9624586701393127}]}, {"text": "In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views).", "labels": [], "entities": []}, {"text": "The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers.", "labels": [], "entities": []}, {"text": "However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process.", "labels": [], "entities": []}, {"text": "With the revival of interest in deep learning (), shared deep representations (or embeddings) () are employed for CLSC.", "labels": [], "entities": []}, {"text": "Usually, paired sentences from parallel corpora are used to learn word embeddings across languages), eliminating the need of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9748392701148987}]}, {"text": "The learned bilingual embeddings could easily project the training data and test data into a common space, where training and testing are performed.", "labels": [], "entities": []}, {"text": "However, high-quality bilingual embeddings rely on the large-scale task-related parallel corpora, which are not always readily available.", "labels": [], "entities": []}, {"text": "Meanwhile, though semantic similarities across languages are captured during bilingual embedding learning process, sentiment information of text is ignored.", "labels": [], "entities": []}, {"text": "That is, bilingual embeddings learned from unlabeled parallel corpora are not effective enough for CLSC because of alack of explicit sentiment information.", "labels": [], "entities": []}, {"text": "first proposed a bilingual sentiment embedding model using the original training data and the corresponding translations through a linear mapping rather than deep learning technique.", "labels": [], "entities": []}, {"text": "This paper proposes a denoising autoencoder based approach to learning bilingual sentiment word embeddings (BSWE) for CLSC, which incorporates sentiment polarities of text into the bilingual embeddings.", "labels": [], "entities": [{"text": "learning bilingual sentiment word embeddings (BSWE)", "start_pos": 62, "end_pos": 113, "type": "TASK", "confidence": 0.628135297447443}]}, {"text": "The proposed approach learns BSWE with the original labeled documents and their translations instead of parallel corpora.", "labels": [], "entities": [{"text": "BSWE", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.602302610874176}]}, {"text": "The BSWE learning process consists of two phases: the unsupervised phase of semantic learning and the supervised phase of sentiment learning.", "labels": [], "entities": [{"text": "BSWE learning process", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.5341458519299825}, {"text": "semantic learning", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8463931679725647}, {"text": "sentiment learning", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.9216435253620148}]}, {"text": "In the unsupervised phase, sentiment words and their negation features are extracted from the source training data and their translations to represent paired documents.", "labels": [], "entities": []}, {"text": "These features are used as inputs fora denoising autoencoder to learn the bilingual embeddings.", "labels": [], "entities": []}, {"text": "In the supervised phase, sentiment polarity labels of documents are used to guide BSWE learning for incorporating sentiment information into the bilingual embeddings.", "labels": [], "entities": [{"text": "BSWE learning", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.7456348538398743}]}, {"text": "The learned BSWE are applied to project English training data and Chinese test data into a common space.", "labels": [], "entities": [{"text": "BSWE", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.7978847622871399}]}, {"text": "In this space, a linear support vector machine (SVM) is used to perform training and testing.", "labels": [], "entities": []}, {"text": "The experiments are carried on NLP&CC 2013 CLSC dataset, including book, DVD and music categories.", "labels": [], "entities": [{"text": "NLP&CC 2013 CLSC dataset", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.9452796677748362}]}, {"text": "Experimental results show that our approach achieves 80.68% average accuracy, which outperforms the state-of-the-art systems on this dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9577187895774841}]}, {"text": "Although the BSWE are only evaluated on English-Chinese CLSC here, it can be popularized to many other languages.", "labels": [], "entities": [{"text": "BSWE", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8627423048019409}]}, {"text": "The major contributions of this work can be summarized as follows: \u2022 We propose bilingual sentiment word embeddings (BSWE) for CLSC based on deep learning technique.", "labels": [], "entities": [{"text": "bilingual sentiment word embeddings (BSWE)", "start_pos": 80, "end_pos": 122, "type": "TASK", "confidence": 0.689565143414906}]}, {"text": "Experimental results show that the proposed BSWE significantly outperform the bilingual embeddings by incorporating sentiment information.", "labels": [], "entities": [{"text": "BSWE", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.6686440110206604}]}, {"text": "\u2022 Instead of large-scale parallel corpora, only the labeled English corpora and Englishto-Chinese translations are required for B-SWE learning.", "labels": [], "entities": []}, {"text": "It is proved that in spite of the small-scale of training set, our approach outperforms the state-of-the-art systems in NLP&CC 2013 CLSC share task.", "labels": [], "entities": [{"text": "NLP&CC 2013 CLSC share task", "start_pos": 120, "end_pos": 147, "type": "DATASET", "confidence": 0.8989837339946202}]}, {"text": "\u2022 We employ sentiment words and their negation features rather than all words in documents to learn sentiment-specific embeddings, which significantly reduces the dimension of input vectors as well as improves sentiment classification performance.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 210, "end_pos": 234, "type": "TASK", "confidence": 0.8516134023666382}]}], "datasetContent": [{"text": "The proposed approach is evaluated on NLP&CC 2013 CLSC dataset 2 3 . The dataset con-sists of product reviews on three categories: book, DVD, and music.", "labels": [], "entities": [{"text": "NLP&CC 2013 CLSC dataset 2", "start_pos": 38, "end_pos": 64, "type": "DATASET", "confidence": 0.9514760545321873}]}, {"text": "Each category contains 4,000 English labeled data as training data (the ratio of the number of positive and negative samples is 1:1) and 4,000 Chinese unlabeled data as test data.", "labels": [], "entities": []}, {"text": "In our experiments, Google Translate 4 is adopted for both English-to-Chinese and Chineseto-English translation.", "labels": [], "entities": [{"text": "Chineseto-English translation", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.6563586741685867}]}, {"text": "ICTCLAS () is used as Chinese word segmentation tool.", "labels": [], "entities": [{"text": "ICTCLAS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7405957579612732}, {"text": "Chinese word segmentation", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.5692317386468252}]}, {"text": "A denoising autoencoder is developed based on Theano system (.", "labels": [], "entities": [{"text": "Theano system", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9784560799598694}]}, {"text": "BSWE are trained for 50 and 30 epochs in unsupervised phase and supervised phases respectively.", "labels": [], "entities": [{"text": "BSWE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9541800618171692}]}, {"text": "SV M light) is used to train linear SVM sentiment classifiers Evaluation Metric.", "labels": [], "entities": [{"text": "SVM sentiment classifiers Evaluation", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.8073492348194122}]}, {"text": "The performance is evaluated by the classification accuracy for each category, and the average accuracy of three categories, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8896557688713074}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9644796252250671}]}, {"text": "The category accuracy is defined as: where c is one of the three categories, and #system correct c and #system total c stand for the number of being correctly classified reviews and the number of total reviews in the category c, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9773284196853638}, {"text": "#system correct c and #system total c", "start_pos": 81, "end_pos": 118, "type": "METRIC", "confidence": 0.7151168419255151}]}, {"text": "The average accuracy is shown as:  In this section, we evaluate the quality of BSWE for CLSC.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9994916915893555}, {"text": "BSWE", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.897914469242096}]}, {"text": "The dimension of bilingual embeddings dis set to 50, and destruction fraction \u03bd is set to 0.2.", "labels": [], "entities": [{"text": "destruction fraction \u03bd", "start_pos": 57, "end_pos": 79, "type": "METRIC", "confidence": 0.8798735539118449}]}], "tableCaptions": [{"text": " Table 1: The classification accuracy with the  Boolean and TF-IDF methods.", "labels": [], "entities": [{"text": "classification", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9622278213500977}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9870263934135437}, {"text": "Boolean", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9506627321243286}]}, {"text": " Table 2: Performance comparisons on the  NLP&CC 2013 CLSC dataset.", "labels": [], "entities": [{"text": "NLP&CC 2013 CLSC dataset", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.9364613691965739}]}]}