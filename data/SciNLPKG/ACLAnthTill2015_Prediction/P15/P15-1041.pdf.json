{"title": [{"text": "Learning to Adapt Credible Knowledge in Cross-lingual Sentiment Analysis", "labels": [], "entities": [{"text": "Cross-lingual Sentiment Analysis", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.7543713450431824}]}], "abstractContent": [{"text": "Cross-lingual sentiment analysis is a task of identifying sentiment polarities of texts in a low-resource language by using sentiment knowledge in a resource-abundant language.", "labels": [], "entities": [{"text": "Cross-lingual sentiment analysis", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.766182561715444}]}, {"text": "While most existing approaches are driven by transfer learning, their performance does not reach to a promising level due to the transferred errors.", "labels": [], "entities": []}, {"text": "In this paper, we propose to integrate into knowledge transfer a knowledge validation model , which aims to prevent the negative influence from the wrong knowledge by distinguishing highly credible knowledge.", "labels": [], "entities": []}, {"text": "Experiment results demonstrate the necessity and effectiveness of the model.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the wide range of business value, sentiment analysis has drawn increasing attention in the past years.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9732908308506012}]}, {"text": "The extensive research and development efforts produce a variety of reliable sentiment resources for English, one of the most popular language in the world.", "labels": [], "entities": []}, {"text": "These available rich resources become the treasure of knowledge to help conductor enhance sentiment analysis in the other languages, which is a task known as cross-lingual sentiment analysis (CLSA).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.9314074814319611}, {"text": "cross-lingual sentiment analysis (CLSA)", "start_pos": 158, "end_pos": 197, "type": "TASK", "confidence": 0.8203189671039581}]}, {"text": "In the literature of CSLA, the language with abundant reliable resources is called the source language (e.g., English), while the low-resource language is referred to as the target language (e.g., Chinese).", "labels": [], "entities": []}, {"text": "However, in this paper, the situation is a low resource language scenario, where the source language is English, and the target language is Chinese.", "labels": [], "entities": []}, {"text": "The main idea of existing CLSA researches is to first buildup the connection between the source and target languages to overcome the language barrier, and then develop an appropriate knowledge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 278, "end_pos": 302, "type": "TASK", "confidence": 0.8592590093612671}]}, {"text": "In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language.", "labels": [], "entities": []}, {"text": "The machine translation (MT) service is one of the most common ways used to build the language connection).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8277930796146393}]}, {"text": "Although it is claimed in that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.960727870464325}, {"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9588143229484558}]}, {"text": "In our preliminary study, we find that even the Google translator 1 (i.e., one of the most widely used online MT service) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%.", "labels": [], "entities": []}, {"text": "[Original English Text]: I am at home on bed rest and desperate for something good to read.", "labels": [], "entities": []}, {"text": "[Translated Chinese Text]: \u00b73[\u00b9K>E \u00da\u00fd\"\u00c0\u00dc\u00e9\u00d0w\" {Meaning: I am in bed to rest at home and feel that desperate things are also good to read.} The noisy data generated by MT errors for sure will weaken the contribution of the transferred knowledge and even worse may create conflicting knowledge.", "labels": [], "entities": [{"text": "MT", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.945328950881958}]}, {"text": "While it is a critical step in CLSA to localize the sentiment knowledge learned from the source language in the target language, to the best of our knowledge, hardly any previous research has focused on knowledge validation to filter out the noisy knowledge having sentiment changes caused by wrong translations during knowledge transfer.", "labels": [], "entities": []}, {"text": "To reduce the noisy sentiment knowledge introduced into the target language, we are motivated to validate the knowledge transferred from the source language by checking its linguistic distributions and sentiment polarity consistency with the known knowledge in the target language.", "labels": [], "entities": []}, {"text": "Different from previous co-training based approaches where two language views recommend knowledge to each other in the same manner, we consider the source language as the \"supervisor\" and the target language as the \"learner\".", "labels": [], "entities": []}, {"text": "The \"supervisor\" boosts itself with its own accumulated labeled data (called knowledge) and meanwhile recommends its confident knowledge to the \"learner\".", "labels": [], "entities": []}, {"text": "The \"learner\" tries to select trustworthy knowledge based on the recommendation to update and expand its training data.", "labels": [], "entities": []}, {"text": "Adding a process to efficiently filter out noisy knowledge and retain the self-adaptive and interested new knowledge makes the subsequent boosting process more credible.", "labels": [], "entities": []}, {"text": "This is why our approach can outperform state-ofthe-art CLSA approaches.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes the related work.", "labels": [], "entities": []}, {"text": "Section 3 explains the proposed model.", "labels": [], "entities": []}, {"text": "Section 4 presents experimental results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper and suggests future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, there are two main parameters that may significantly influence the performance of our proposed model.", "labels": [], "entities": []}, {"text": "They are the new knowledge validation boundary \u03c8 and the validation scale \u03b6 + in the training data.", "labels": [], "entities": []}, {"text": "We set the values of parameters with the grid search strategy.", "labels": [], "entities": []}, {"text": "We first fix initial \u03b6 + = 14 to search the best new knowledge validation boundary \u03c8 from an empirical value set {0.30, 0.35, 0.40, 0.45, 0.50}.", "labels": [], "entities": []}, {"text": "We then fix the best \u03c8 = 0.40 to check the suitable validation scale \u03b6 + from the initial value set {6, 8, 9, 10, 11, 12, 14, 16} in which values are comparable with the knowledge transfer scale of CoTr in the training data.", "labels": [], "entities": []}, {"text": "Besides, the recommendation size m for English is set to 20 and the recommendation size n for Chinese is set to 40.", "labels": [], "entities": []}, {"text": "The final settings are listed in.", "labels": [], "entities": []}, {"text": "The performance is evaluated in terms of accuracy (Ac) defined by Formula (7).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9995915293693542}, {"text": "Ac)", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9113712012767792}]}, {"text": "where pf is the number of correct predictions and P f is the total number of the test data; F \u2208 {Books, DVD, M usic} is the domain set.", "labels": [], "entities": [{"text": "Books, DVD, M usic}", "start_pos": 97, "end_pos": 116, "type": "DATASET", "confidence": 0.8907316156796047}]}, {"text": "The parameter setting used in this paper is '-s 7'.", "labels": [], "entities": []}, {"text": "The performances are reported in.", "labels": [], "entities": []}, {"text": "As shown, CredBoost outperforms all the other comparison methods.", "labels": [], "entities": []}, {"text": "The first four baselines have poor performances compared to others.", "labels": [], "entities": []}, {"text": "This suggests that the CLSA problem cannot be well solved by directly learning from the labeled translated data without any knowledge adaption or knowledge validation.", "labels": [], "entities": []}, {"text": "SB-CN, BTL-2 and CoTr employ iterative boosting to adapt knowledge from the source English to the target Chinese without validating the transferred knowledge.", "labels": [], "entities": [{"text": "BTL-2", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8423982262611389}]}, {"text": "They inevitably mis-recommend the massive noisy data into Chinese.", "labels": [], "entities": []}, {"text": "CredBoost, in contrast, introduces knowledge validation into transfer learning with iterative boosting.", "labels": [], "entities": []}, {"text": "It better adapts knowledge from English to Chinese and thus ensures the credibility of the accepted knowledge.", "labels": [], "entities": []}, {"text": "Its best result justifies our assumption.", "labels": [], "entities": []}, {"text": "Specifically, SB-CN leverages both the Chinese training data translated from the labeled English data and the unlabeled Chinese data used for boosting.", "labels": [], "entities": []}, {"text": "The boosting in Chinese iteratively selects the trustworthy data with the labels assigned by the Chinese classifier.", "labels": [], "entities": []}, {"text": "Our proposed method, however, exploits two different languages simultaneously with an additional boosting step, i.e., it transfers knowledge from English to Chinese during boosting.", "labels": [], "entities": []}, {"text": "We then use knowledge validation model to validate the unlabeled Chinese data whose labels are assigned by the English, the better performance of our proposed method compared with that of the self-boosting method further suggests the effectiveness of our proposed knowledge validation model.", "labels": [], "entities": []}, {"text": "illustrates the continuous changes of performances vs. the corresponding growth sizes of the training data sets for SB-CN, BTL-2, CoTr, and CredBoost.", "labels": [], "entities": [{"text": "BTL-2", "start_pos": 123, "end_pos": 128, "type": "DATASET", "confidence": 0.8337231278419495}]}, {"text": "According to our commonsense, noisy data have negative influence on performance improvement.", "labels": [], "entities": []}, {"text": "Compared to the other three methods, CredBoost accepts less number of training instances during learning while it achieves more improvement.", "labels": [], "entities": []}, {"text": "This verifies the ability of CredBoost that can filter out the noisy data recommended by the English sentiment classifier.", "labels": [], "entities": [{"text": "English sentiment classifier", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.5096057256062826}]}, {"text": "In(a), the curves of BTL-2 and CoTr suggest that directly transferring the knowledge recommended from English imports many noisy data into Chinese.", "labels": [], "entities": [{"text": "BTL-2", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.9366540312767029}]}, {"text": "It is also obvious that the performance curve of CredBoost implies a stable improvement trend while the other three decrease after certain iterations because of the accumulated negative influence from the noisy data.(b) shows CredBoost accepts decreased training instances after certain iterations because the number of \"high-quality\" instances decrease when learning proceeds.", "labels": [], "entities": []}, {"text": "This finding suggests that knowledge validation would rather abandon \"lesscredible\" knowledge with higher probability than easily accept it.", "labels": [], "entities": [{"text": "knowledge validation", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7920346856117249}]}, {"text": "Knowledge validation in the proposed model guarantees highly-credible learning when transferring knowledge from English to Chinese.", "labels": [], "entities": [{"text": "Knowledge validation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6704141795635223}]}, {"text": "The results also show that CredBoost has great potential to achieve better performance approaching to supervised approaches if more unlabeled Chinese data are available.", "labels": [], "entities": []}, {"text": "Another interesting finding is also observed.", "labels": [], "entities": []}, {"text": "The similar performance curves of CoTr is also reported in ().", "labels": [], "entities": []}, {"text": "Although document-to-vector represents content semantic well, it cannot determine the sentiment polarity of text well, even when the documentto-vectors that are used to train basic classifiers are learned on the mixture of the translated and original reviews.", "labels": [], "entities": []}, {"text": "The superior performance of CredBoost to dCredB suggests that the semantic representation is effective to identify highlycredible acquired knowledge and new knowledge but it alone may not be sufficient enough to model the sentiment information.", "labels": [], "entities": []}, {"text": "We also conduct some other experiments to study the sensitivity of the new knowledge validation boundary \u03c8 and the validation scale \u03b6 + in the training data.", "labels": [], "entities": []}, {"text": "The experimental results show that the performances with different parameter settings fluctuate around the best result reported in in a small range.", "labels": [], "entities": []}, {"text": "Our model is basically quite stable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parameter settings of three domains in  this paper.", "labels": [], "entities": []}, {"text": " Table 3: Macro performance of all approaches  in three domains. All values are accuracies and  Avg-Ac represents the average accuracy in three  domains.", "labels": [], "entities": [{"text": "Avg-Ac", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9941084384918213}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9963065385818481}]}, {"text": " Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.", "labels": [], "entities": [{"text": "Precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9811446070671082}, {"text": "Recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.934162437915802}, {"text": "F1: micro-F  measure", "start_pos": 89, "end_pos": 109, "type": "METRIC", "confidence": 0.7803472429513931}, {"text": "Ac", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9551560282707214}, {"text": "Accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.746749997138977}, {"text": "BR2013", "start_pos": 163, "end_pos": 169, "type": "DATASET", "confidence": 0.9420594573020935}]}]}