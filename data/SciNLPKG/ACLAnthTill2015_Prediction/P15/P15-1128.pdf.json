{"title": [{"text": "Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8656100332736969}, {"text": "Question Answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.841124027967453}]}], "abstractContent": [{"text": "We propose a novel semantic parsing framework for question answering using a knowledge base.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7581973075866699}, {"text": "question answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8313769400119781}]}, {"text": "We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form.", "labels": [], "entities": []}, {"text": "Semantic parsing is reduced to query graph generation, formulated as a staged search problem.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7739556133747101}, {"text": "query graph generation", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7184762756029764}]}, {"text": "Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simplifies the semantic matching problem.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.748600423336029}]}, {"text": "By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outper-forms previous methods substantially, and achieves an F 1 measure of 52.5% on the WEBQUESTIONS dataset.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 212, "end_pos": 223, "type": "METRIC", "confidence": 0.9907617568969727}, {"text": "WEBQUESTIONS dataset", "start_pos": 240, "end_pos": 260, "type": "DATASET", "confidence": 0.9705420434474945}]}], "introductionContent": [{"text": "Organizing the world's facts and storing them in a structured database, large-scale knowledge bases (KB) like and have become important resources for supporting open-domain question answering (QA).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 173, "end_pos": 196, "type": "TASK", "confidence": 0.8410802721977234}]}, {"text": "Most state-of-the-art approaches to KB-QA are based on semantic parsing, where a question (utterance) is mapped to its formal meaning representation (e.g., logical form) and then translated to a KB query.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7751309871673584}]}, {"text": "The answers to the question can then be retrieved simply by executing the query.", "labels": [], "entities": []}, {"text": "The semantic parse also provides a deeper understanding of the question, which can be used to justify the answer to users, as well as to provide easily interpretable information to developers for error analysis.", "labels": [], "entities": [{"text": "semantic parse", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7448736429214478}]}, {"text": "However, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.8436069488525391}]}, {"text": "For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7830736935138702}]}, {"text": "Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem.", "labels": [], "entities": []}, {"text": "Inspired by;), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7481917142868042}]}, {"text": "We first define a query graph that can be straightforwardly mapped to a logical form in \u03bb-calculus and is semantically closely related to \u03bb-DCS (.", "labels": [], "entities": []}, {"text": "Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6940056085586548}, {"text": "query graph generation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7114538351694742}]}, {"text": "Each state is a candidate parse in the query graph representation and each action defines away to grow the graph.", "labels": [], "entities": []}, {"text": "The representation power of the semantic parse is thus controlled by the set of legitimate actions applicable to each state.", "labels": [], "entities": []}, {"text": "In particular, we stage the actions into three main steps: locating the topic entity in the question, finding the main relationship between the answer and the topic entity, and expanding the query graph with additional constraints that describe properties the answer needs to have, or relationships between the answer and other entities in the question.", "labels": [], "entities": []}, {"text": "One key advantage of this staged design is that through grounding partially the utterance to some entities and predicates in the KB, we make the search far more efficient by focusing on the promising areas in the space that most likely lead to the correct query graph, before the full parse is determined.", "labels": [], "entities": []}, {"text": "For example, after linking \"Fam-ily Guy\" in the question \"Who first voiced Meg on Family Guy?\" to FamilyGuy (the TV show) in the knowledge base, the procedure needs only to examine the predicates that can be applied to FamilyGuy instead of all the predicates in the KB.", "labels": [], "entities": [{"text": "Fam-ily Guy\" in the question \"Who first voiced Meg on Family Guy?\"", "start_pos": 28, "end_pos": 94, "type": "TASK", "confidence": 0.5134420653184255}, {"text": "FamilyGuy", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.7303958535194397}, {"text": "FamilyGuy", "start_pos": 219, "end_pos": 228, "type": "DATASET", "confidence": 0.9701539874076843}]}, {"text": "Resolving other entities also becomes easy, as given the context, it is clear that Meg refers to MegGriffin (the character in Family Guy).", "labels": [], "entities": [{"text": "MegGriffin", "start_pos": 97, "end_pos": 107, "type": "DATASET", "confidence": 0.9203290939331055}]}, {"text": "Our design divides this particular semantic parsing problem into several sub-problems, such as entity linking and relation matching.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7534796893596649}, {"text": "entity linking", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.7617999017238617}, {"text": "relation matching", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.8188113570213318}]}, {"text": "With this integrated framework, best solutions to each subproblem can be easily combined and help produce the correct semantic parse.", "labels": [], "entities": []}, {"text": "For instance, an advanced entity linking system that we employ outputs candidate entities for each question with both high precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9988396763801575}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9950482249259949}]}, {"text": "In addition, by leveraging a recently developed semantic matching framework based on convolutional networks, we present better relation matching models using continuous-space representations instead of pure lexical matching.", "labels": [], "entities": [{"text": "relation matching", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7786440551280975}]}, {"text": "Our semantic parsing approach improves the state-of-the-art result on the WEBQUESTIONS dataset) to 52.5% in F 1 , a 7.2% absolute gain compared to the best existing method.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7102038562297821}, {"text": "WEBQUESTIONS dataset", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.9417265951633453}, {"text": "F 1", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9231486320495605}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "2 introduces the basic notion of the graph knowledge base and the design of our query graph.", "labels": [], "entities": []}, {"text": "3 presents our search-based approach for generating the query graph.", "labels": [], "entities": []}, {"text": "The experimental results are shown in Sec.", "labels": [], "entities": []}, {"text": "4, and the discussion of our approach and the comparisons to related work are in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first introduce the dataset and evaluation metric, followed by the main experimental results and some analysis.", "labels": [], "entities": []}, {"text": "We use the WEBQUESTIONS dataset, which consists of 5,810 question/answer pairs.", "labels": [], "entities": [{"text": "WEBQUESTIONS dataset", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.9086365401744843}]}, {"text": "These questions were collected using Google Suggest API and the answers were obtained from Freebase with the help of Amazon MTurk.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9740890264511108}, {"text": "Amazon MTurk", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.839881956577301}]}, {"text": "The questions are split into training and testing sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively.", "labels": [], "entities": []}, {"text": "This dataset has several unique properties that make it appealing and was used in several recent papers on semantic parsing and question answering.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.8806741237640381}, {"text": "question answering", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.8899127542972565}]}, {"text": "For instance, although the questions are not directly sampled from search query logs, the selection process was still biased to commonly asked questions on a search engine.", "labels": [], "entities": []}, {"text": "The distribution of this question set is thus closer to the \"real\" information need of search users than that of a small number of human editors.", "labels": [], "entities": []}, {"text": "The system performance is basically measured by the ratio of questions that are answered correctly.", "labels": [], "entities": []}, {"text": "Because there can be more than one answer to a question, precision, recall and F 1 are computed based on the system output for each individual question.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9996936321258545}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9996918439865112}, {"text": "F 1", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9896927773952484}]}, {"text": "The average F 1 score is reported as the main evaluation metric . Because this dataset contains only question and answer pairs, we use essentially the same search procedure to simulate the semantic parses for training the CNN models and the overall reward function.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9746604363123575}]}, {"text": "Candidate topic entities are first generated using the same entity linking system for each question in the training data.", "labels": [], "entities": []}, {"text": "Paths on the Freebase knowledge graph that connect a candidate entity to at least one answer entity are identified as the core inferential chains . If an inferentialchain query returns more entities than the correct answers, we explore adding constraint and aggregation nodes, until the entities retrieved by the query graph are identical to the labeled answers, or the F 1 score cannot be increased further.", "labels": [], "entities": [{"text": "Freebase knowledge graph", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.9548460046450297}, {"text": "F 1 score", "start_pos": 370, "end_pos": 379, "type": "METRIC", "confidence": 0.9871376554171244}]}, {"text": "Negative examples are sampled from of the incorrect candidate graphs generated during the search process.: The results of our approach compared to existing work.", "labels": [], "entities": []}, {"text": "The numbers of other systems are either from the original papers or derived from the evaluation script, when the output is available.", "labels": [], "entities": []}, {"text": "In the end, we produce 17,277 query graphs with none-zero F 1 scores from the training set questions and about 1.7M completely incorrect ones.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9436961611111959}]}, {"text": "For training the CNN models to identify the core inferential chain (Sec.", "labels": [], "entities": []}, {"text": "3.2.1), we only use 4,058 chain-only query graphs that achieve F 1 = 0.5 to form the parallel question and predicate sequence pairs.", "labels": [], "entities": [{"text": "F 1", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9736556112766266}]}, {"text": "The hyper-parameters in CNN, such as the learning rate and the numbers of hidden nodes at the convolutional and semantic layers were chosen via cross-validation.", "labels": [], "entities": []}, {"text": "We reserved 684 pairs of patterns and inference-chains from the whole training examples as the held-out set, and the rest as the initial training set.", "labels": [], "entities": []}, {"text": "The optimal hyper-parameters were determined by the performance of models trained on the initial training set when applied to the held-out data.", "labels": [], "entities": []}, {"text": "We then fixed the hyper-parameters and retrained the CNN models using the whole training set.", "labels": [], "entities": []}, {"text": "The performance of CNN is insensitive to the hyperparameters as long as they are in a reasonable range (e.g., 1000 \u00b1 200 nodes in the convolutional layer, 300 \u00b1 100 nodes in the semantic layer, and learning rate 0.05 \u223c 0.005) and the training process often converges after \u223c 800 epochs.", "labels": [], "entities": []}, {"text": "When training the reward function, we created up to 4,000 examples for each question that contain all the positive query graphs and randomly selected negative examples.", "labels": [], "entities": []}, {"text": "The model is trained as a ranker, where example query graphs are ranked by their F 1 scores.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9244675040245056}]}], "tableCaptions": [{"text": " Table 1: The results of our approach compared to  existing work. The numbers of other systems are  either from the original papers or derived from the  evaluation script, when the output is available.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of entity linking results on train- ing set questions. Both methods cover roughly the  same number of questions, but Freebase API sug- gests twice the number of entities output by our  entity linking system and covers fewer topic enti- ties labeled in the original data.", "labels": [], "entities": []}, {"text": " Table 3: The system results when only the  inferential-chain query graphs are generated. We  started with the PatChain CNN model and then  added QuesEP and ClueWeb sequentially. See  Sec. 3.4 for the description of these models.", "labels": [], "entities": [{"text": "PatChain CNN model", "start_pos": 111, "end_pos": 129, "type": "DATASET", "confidence": 0.9586456815401713}]}]}