{"title": [{"text": "On metric embedding for boosting semantic similarity computations", "labels": [], "entities": [{"text": "semantic similarity computations", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6815432409445444}]}], "abstractContent": [{"text": "Computing pairwise word semantic similarity is widely used and serves as a building block in many tasks in NLP.", "labels": [], "entities": [{"text": "pairwise word semantic similarity", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6709930524230003}]}, {"text": "In this paper, we explore the embedding of the shortest-path metrics from a knowledge base (Wordnet) into the Hamming hyper-cube, in order to enhance the computation performance.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9139243364334106}]}, {"text": "We show that, although an isometric embedding is untractable, it is possible to achieve good non-isometric embeddings.", "labels": [], "entities": []}, {"text": "We report a speedup of three orders of magnitude for the task of computing Leacock and Chodorow (LCH) similarity while keeping strong correlations (r = .819, \u03c1 = .826).", "labels": [], "entities": [{"text": "computing Leacock and Chodorow (LCH) similarity", "start_pos": 65, "end_pos": 112, "type": "TASK", "confidence": 0.5616876445710659}]}], "introductionContent": [{"text": "Among semantic relatedness measures, semantic similarity encodes the conceptual distance between two units of language -this goes beyond lexical ressemblance.", "labels": [], "entities": []}, {"text": "When words are the speech units, semantic similarity is at the very core of many NLP problems.", "labels": [], "entities": []}, {"text": "It has proven to be essential for word sense disambiguation), open domain question answering, and information retrieval on the Web (), to name a few.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.7289091348648071}, {"text": "open domain question answering", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.5796177685260773}, {"text": "information retrieval", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7617757618427277}]}, {"text": "Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics.", "labels": [], "entities": [{"text": "pairwise word semantic similarity", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.639211893081665}]}, {"text": "Knowledge-based approaches exploit the structure of the taxonomy ((), its content ((), or both).", "labels": [], "entities": []}, {"text": "In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy.", "labels": [], "entities": [{"text": "Wordnet-based semantic similarity", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.7242856621742249}]}, {"text": "It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies -a tedious task in practice.", "labels": [], "entities": []}, {"text": "In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora.", "labels": [], "entities": []}, {"text": "While providing a significant impact on abroad range of applications,, distributional semantics -similarly to knowledge-based strategies -struggle to process the ever-increasing size of textual corpora in a reasonable amount of time.", "labels": [], "entities": []}, {"text": "As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding) has emerged as a popular method.", "labels": [], "entities": []}, {"text": "Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance -usually the cosine similarity -encodes the word-to-word semantic similarity.", "labels": [], "entities": [{"text": "Word embedding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7360225319862366}]}, {"text": "Although word embedding was successfully applied for several NLP tasks, it implies a slow training phase -measured in days, though re-embedding words seems promising.", "labels": [], "entities": []}, {"text": "There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data.", "labels": [], "entities": []}, {"text": "Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation.", "labels": [], "entities": [{"text": "word-to-word semantic similarity computation", "start_pos": 114, "end_pos": 158, "type": "TASK", "confidence": 0.6722436025738716}]}, {"text": "In this context, it is surprising that embedding semantic similarity of words in low dimensional spaces for knowledge-based approaches is understudied.", "labels": [], "entities": []}, {"text": "This oversight may well condemn the word-to-word semantic similarity task to remain corpus-dependant -i.e. ignoring the background knowledge provided by a lexical hierarchy.", "labels": [], "entities": [{"text": "word-to-word semantic similarity task", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.6948896422982216}]}, {"text": "In this paper, we propose an embedding of knowledge base semantic similarity based on the shortest path metric (, into the Hamming hypercube of size n (the size of targeted binary codes).", "labels": [], "entities": [{"text": "knowledge base semantic similarity", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.6242872998118401}]}, {"text": "The Leacock and Chodorow semantic similarity is one of the most meaningful measure.", "labels": [], "entities": []}, {"text": "It yields the second rank for highest correlation with the data collected by, and the first one within edge centric approaches, as shown by).", "labels": [], "entities": []}, {"text": "This method is only surpassed by the information theoretic based similarity from.", "labels": [], "entities": []}, {"text": "A second study present similar result (), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification).", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9984630346298218}, {"text": "paraphrase identification", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.9274144470691681}]}, {"text": "The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs.", "labels": [], "entities": []}, {"text": "This allows the computation of several millions distances per second.", "labels": [], "entities": []}, {"text": "Multi-index techniques allows the very fast computation of top-k queries () on the Hamming space.", "labels": [], "entities": []}, {"text": "However, the dimension of the hypercube (i.e. the number of bits used to represent an element) should obey the threshold of few CPU words (64, 128 . .", "labels": [], "entities": []}, {"text": ", bits) to maintain such efficiency ().", "labels": [], "entities": []}, {"text": "An isometric embedding requires a excessively high number of dimensions to be feasible.", "labels": [], "entities": []}, {"text": "However, in this paper we show that practical embeddings exist and present a method to construct them.", "labels": [], "entities": []}, {"text": "The best embedding presents very strong correlations (r = .819, \u03c1 = .829) with the Leacock & Chodorow similarity measure (LCH in the rest of this paper).", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 102, "end_pos": 120, "type": "METRIC", "confidence": 0.8040454387664795}, {"text": "LCH", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.4962064325809479}]}, {"text": "Our experiments against the state-of-the art implementation including caching techniques show that performance is increased by up to three orders of magnitude.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we run two experiments to evaluate both the soundness and the performance of our approach.", "labels": [], "entities": []}, {"text": "In the first experiment, we test the quality of our embedding against the tree distance and the LCH similarity.", "labels": [], "entities": [{"text": "LCH similarity", "start_pos": 96, "end_pos": 110, "type": "METRIC", "confidence": 0.8569434285163879}]}, {"text": "The goal is to assess the soundness of our approach and to measure the correlation between the approximate embedding and the original LCH similarity.", "labels": [], "entities": []}, {"text": "In the second experiment we compare the computational performance of our approach against an optimized in-memory library that implements the LCH similarity.", "labels": [], "entities": [{"text": "LCH similarity", "start_pos": 141, "end_pos": 155, "type": "METRIC", "confidence": 0.6817699074745178}]}, {"text": "Our algorithm called FSE for Fast Similarity Embedding, is implemented in Java and available publicly . Our testbed is an Intel Xeon E3 1 Source code, binaries and instructions to reproduce 1246v3 with 16GB of memory, a 256Go PCI Express SSD.", "labels": [], "entities": [{"text": "FSE", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9836534857749939}, {"text": "Intel Xeon E3 1 Source code", "start_pos": 122, "end_pos": 149, "type": "DATASET", "confidence": 0.7072078138589859}]}, {"text": "The system runs a 64-bit Linux 3.13.0 kernel with Oracle's JDK 7u67.", "labels": [], "entities": [{"text": "Oracle's JDK 7u67", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.6134847849607468}]}, {"text": "The FSE algorithm is implemented in various flavours.", "labels": [], "entities": [{"text": "FSE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.45882362127304077}]}, {"text": "FSE-Base denotes the basic algorithm, containing none of the optimizations detailed in the previous section.", "labels": [], "entities": [{"text": "FSE-Base", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.5372778177261353}]}, {"text": "FSE-Base can be augmented with either or both of the optimizations.", "labels": [], "entities": [{"text": "FSE-Base", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.745373547077179}]}, {"text": "This latter version is denoted FSE-Best.", "labels": [], "entities": [{"text": "FSE-Best", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.6384695768356323}]}], "tableCaptions": [{"text": " Table 1: Correlations between LCH, isometric em- bedding, and FSE for all distances on all Wordnet- Core noun pairs (p-values \u2264 10 \u221214 ).", "labels": [], "entities": [{"text": "FSE", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9988232254981995}, {"text": "Wordnet- Core noun pairs", "start_pos": 92, "end_pos": 116, "type": "DATASET", "confidence": 0.9109594225883484}]}, {"text": " Table 2: Running time in milliseconds for pairwise  similarity computations.", "labels": [], "entities": [{"text": "Running time", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9452159702777863}]}]}