{"title": [], "abstractContent": [{"text": "We add an interpretable semantics to the paraphrase database (PPDB).", "labels": [], "entities": []}, {"text": "To date, the relationship between phrase pairs in the database has been weakly defined as approximately equivalent.", "labels": [], "entities": []}, {"text": "We show that these pairs represent a variety of relations, including directed entail-ment (little girl/girl) and exclusion (no-body/someone).", "labels": [], "entities": []}, {"text": "We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction.", "labels": [], "entities": []}, {"text": "We demonstrate that our model assigns these relations with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9986140727996826}]}, {"text": "Ina downstream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9633097052574158}]}], "introductionContent": [], "datasetContent": [{"text": "We test the performance of our classifier intrinsically, through its ability to reproduce the human labels for the phrase pairs from the SICK test sentences.", "labels": [], "entities": []}, {"text": "shows the precision and recall achieved by the classifier for each of our 5 entailment classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999618411064148}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9995755553245544}]}, {"text": "The classifier is able to achieve an overall 79% accuracy, reaching >70% precision while maintaining good levels of recall on all classes.: Example misclassifications from some of the most frequent and most interesting error categories.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9983939528465271}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9991920590400696}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9991146922111511}]}, {"text": "shows the classifier's confusion matrix and shows some examples of common and interesting error cases.", "labels": [], "entities": []}, {"text": "The majority of errors (26%) come from confusing the \u21e0 class with the # class.", "labels": [], "entities": [{"text": "errors", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9820225834846497}]}, {"text": "This mistake is not too concerning from an RTE perspective since \u21e0 can be treated as a special case of # (Section 5).", "labels": [], "entities": []}, {"text": "There are very few cases in which the classifier makes extreme errors, e.g. confusing \u2318 with \u00ac or with #; some interesting examples of such errors arise when the phrases contain pronouns (e.g. girl \u2318 she) or when the relation uses a highly infrequent word sense (e.g. photo \u2318 still).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: F1 measure (\u21e5100) achieved by entailment classifier  using 10-fold cross validation on the training data.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9632800817489624}]}, {"text": " Table 6: Example misclassifications from some of the most  frequent and most interesting error categories.", "labels": [], "entities": []}, {"text": " Table 7: F1 measure (\u21e5100) achieved by entailment classifier  on the held out phrase pairs from the sentences in SICK test.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9680319726467133}, {"text": "SICK test", "start_pos": 114, "end_pos": 123, "type": "DATASET", "confidence": 0.6652152240276337}]}, {"text": " Table 8: Nutcracker's overall system accuracy and proof cov- erage when using different sources of axioms. Coverage is  measured as the percent of sentence pairs for which NC's  theorem prover or model builder is able to find a complete  logical proof of either entailment or contradiction. When NC  fails to find either type of proof, it guesses the most frequent  class, NEUTRAL. NC alone uses no axioms. PPDB+ refers  to the axioms generated automatically using the classifier de- scribed in this paper. PPDB-H refers axioms generated using  the human labels on which the classifier was trained.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9985257983207703}]}]}