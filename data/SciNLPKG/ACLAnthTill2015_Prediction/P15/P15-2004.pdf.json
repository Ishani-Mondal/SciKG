{"title": [{"text": "A Multitask Objective to Inject Lexical Contrast into Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional semantic models have trouble distinguishing strongly contrasting words (such as antonyms) from highly compatible ones (such as synonyms), because both kinds tend to occur in similar contexts in corpora.", "labels": [], "entities": []}, {"text": "We introduce the mul-titask Lexical Contrast Model (mLCM), an extension of the effective Skip-gram method that optimizes semantic vectors on the joint tasks of predicting corpus contexts and making the representations of WordNet synonyms closer than that of matching WordNet antonyms.", "labels": [], "entities": []}, {"text": "mLCM outperforms Skip-gram both on general semantic tasks and on synonym/antonym discrimination, even when no direct lexical contrast information about the test words is provided during training.", "labels": [], "entities": [{"text": "synonym/antonym discrimination", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6642878204584122}]}, {"text": "mLCM also shows promising results on the task of learning a compositional negation operator mapping adjectives to their antonyms.", "labels": [], "entities": [{"text": "mLCM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9534524083137512}]}], "introductionContent": [{"text": "Distributional semantic models (DSMs) extract vectors representing word meaning by relying on the distributional hypothesis, that is, the idea that words that are related in meaning will tend to occur in similar contexts.", "labels": [], "entities": []}, {"text": "While extensive work has shown that contextual similarity is an excellent proxy to semantic similarity, a big problem for DSMs is that both words with very compatible meanings (e.g., near synonyms) and words with strongly contrasting meanings (e.g., antonyms) tend to occur in the same contexts.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 122, "end_pos": 126, "type": "TASK", "confidence": 0.9337319731712341}]}, {"text": "Indeed, have shown that synonyms and antonyms are indistinguishable in terms of their average degree of distributional similarity.", "labels": [], "entities": []}, {"text": "This is problematic for the application of DSMs to reasoning tasks such as entailment detection (black is very close to both dark and white in distributional semantic space, but it implies the former while contradicting the latter).", "labels": [], "entities": [{"text": "entailment detection", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.8578760921955109}]}, {"text": "Beyond wordlevel relations, the same difficulties make it challenging for compositional extensions of DSMs to capture the fundamental phenomenon of negation at the phrasal and sentential levels (the distributional vectors for good and not good are nearly identical) (.", "labels": [], "entities": []}, {"text": "Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources.", "labels": [], "entities": []}, {"text": "Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors.", "labels": [], "entities": []}, {"text": "We propose instead to induce word vectors using a multitask cost function combining a traditional DSM context-prediction objective with a term forcing words to be closer to their WordNet synonyms than to their antonyms.", "labels": [], "entities": []}, {"text": "In this way, we make the model aware that contrasting words such as hot and cold, while still semantically related, should not be nearest neighbours in the space.", "labels": [], "entities": []}, {"text": "Ina similar spirit, devise a DSM in which the embeddings of the antonyms of a word are pushed to be the vectors that are farthest away from its representation.", "labels": [], "entities": []}, {"text": "While their model is able to correctly pick the antonym of a target item from a list of candidates (since it is the most dissimilar element in the list), we conjecture that their radical strategy produces embeddings with poor performance on general semantic tasks.", "labels": [], "entities": []}, {"text": "Our method has instead a beneficial global effect on semantic vectors, leading to state-of-theart results in a challenging similarity task, and enabling better learning of a compositional negation function.", "labels": [], "entities": []}, {"text": "Our work is also closely related to, who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9600585699081421}]}, {"text": "This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the whole lexicon.", "labels": [], "entities": []}, {"text": "Other recent work has instead adopted multitask objectives similar to ours in order to directly plugin knowledge from structured resources at DSM induction time ().", "labels": [], "entities": []}, {"text": "Our main novelties with respect to these proposals are the focus on capturing semantic contrast, and explicitly testing the hypothesis that the multitask objective is also beneficial to words that are not directly exposed to WordNet evidence during training.", "labels": [], "entities": [{"text": "capturing semantic contrast", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7200455069541931}]}], "datasetContent": [{"text": "We Both models are evaluated in four tasks: two lexical tasks testing the general quality of the learned embeddings and one focusing on antonymy, and a negation task which verifies the positive influence of lexical contrast in a compositional setting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Average accuracy in retrieving antonym  as nearest neighbour when applying the not com- position function to 4,000 adjectives.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9948318004608154}]}]}