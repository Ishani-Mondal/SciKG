{"title": [{"text": "Text Categorization as a Graph Classification Problem", "labels": [], "entities": [{"text": "Graph Classification", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7223894894123077}]}], "abstractContent": [{"text": "In this paper, we consider the task of text categorization as a graph classification problem.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7937931716442108}, {"text": "graph classification", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.793356716632843}]}, {"text": "By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining.", "labels": [], "entities": []}, {"text": "Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part-its main core-speeding up the feature extraction step for little to no cost in prediction performances.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7359215319156647}]}, {"text": "Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-averaged F1-score compared to baseline approaches.", "labels": [], "entities": [{"text": "text classification", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7186968326568604}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9994295239448547}, {"text": "F1-score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9657503366470337}]}], "introductionContent": [{"text": "The task of text categorization finds applications in a wide variety of domains, from news filtering and document organization to opinion mining and spam detection.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7095902115106583}, {"text": "news filtering", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.7211057841777802}, {"text": "document organization", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.7304630279541016}, {"text": "opinion mining", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.8376581072807312}, {"text": "spam detection", "start_pos": 149, "end_pos": 163, "type": "TASK", "confidence": 0.8494181334972382}]}, {"text": "With the ever-growing quantity of information available online nowadays, it is crucial to provide effective systems capable of classifying text in a timely fashion.", "labels": [], "entities": []}, {"text": "Compared to other application domains of classification, its specificity lies in its high number of features, its sparse feature vectors and its skewed multiclass scenario.", "labels": [], "entities": []}, {"text": "For instance, when dealing with thousands of news articles, it is not uncommon to have millions of n-gram features, only a few hundreds actually present in each document and tens of class labels -some of them with thousands of articles and some others will only a few hundreds.", "labels": [], "entities": []}, {"text": "These particularities have to betaken into account when envisaging a different representation fora document and in our case when considering the task as a graph classification problem.", "labels": [], "entities": [{"text": "graph classification", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.7601580321788788}]}, {"text": "Graphs are powerful data structures that are used to represent complex information about entities and interaction between them and we think text makes no exception.", "labels": [], "entities": []}, {"text": "Historically, following the traditional bag-of-words representation, unigrams have been considered as the natural features and later extended to n-grams to capture some word dependency and word order.", "labels": [], "entities": []}, {"text": "However, ngrams correspond to sequences of words and thus fail to capture word inversion and subset matching (e. g., \"article about news\" vs. \"news article\").", "labels": [], "entities": []}, {"text": "We believe graphs can help solve these issues like they did for instance with chemical compounds where repeating substructure patterns are good indicators of belonging to one particular class, e. g., predicting carcinogenicity in molecules ().", "labels": [], "entities": [{"text": "predicting carcinogenicity in molecules", "start_pos": 200, "end_pos": 239, "type": "TASK", "confidence": 0.8553541302680969}]}, {"text": "Graph classification has received a lot of attention this past decade and various techniques have been developed to deal with the task but rarely applied on textual data and at its scale.", "labels": [], "entities": [{"text": "Graph classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9512250423431396}]}, {"text": "In our work, we explored a graph representation of text, namely graph-of-words, to challenge the traditional bag-of-words representation and help better classify textual documents into categories.", "labels": [], "entities": []}, {"text": "We first trained a classifier using frequent subgraphs as features for increased effectiveness.", "labels": [], "entities": []}, {"text": "We then reduced each graph-of-words to its main core before mining the features for increased efficiency.", "labels": [], "entities": []}, {"text": "Finally, we also used this technique to reduce the total number of n-gram features considered in the baselines for little to no loss in prediction performances.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides a review of the related work.", "labels": [], "entities": []}, {"text": "Section 3 defines the preliminary concepts upon which our work is built.", "labels": [], "entities": []}, {"text": "Section 4 introduces the proposed approaches.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental settings and presents the results we obtained on four standard datasets.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes our paper and mentions future work directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the experiments we conducted to validate our approaches.", "labels": [], "entities": []}, {"text": "We used four standard text datasets: two for multiclass document categorization (WebKB and R8), one for spam detection (LingSpam) and one for opinion mining (Amazon) so as to coverall the main subtasks of text categorization: \u2022 WebKB: 4 most frequent categories among labeled webpages from various CS departments -split into 2,803 for training and 1,396 for test).", "labels": [], "entities": [{"text": "spam detection", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7294867187738419}, {"text": "opinion mining", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7335773408412933}]}, {"text": "\u2022 R8: 8 most frequent categories of Reuters-21578, a set of labeled news articles from the 1987 Reuters newswire -split into 5,485 for training and 2,189 for test).", "labels": [], "entities": [{"text": "R8", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.8946607112884521}, {"text": "Reuters-21578, a set of labeled news articles from the 1987 Reuters newswire", "start_pos": 36, "end_pos": 112, "type": "DATASET", "confidence": 0.6586456826099982}]}, {"text": "\u2022 LingSpam: 2,893 emails classified as spam or legitimate messages -split into 10 sets for 10-fold cross validation ().", "labels": [], "entities": []}, {"text": "\u2022 Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative -split into 1,600 for training and 400 for test each).", "labels": [], "entities": [{"text": "Amazon", "start_pos": 2, "end_pos": 8, "type": "DATASET", "confidence": 0.794072687625885}]}, {"text": "To evaluate the performance of our proposed approaches over standard baselines, we computed on the test set both the micro-and macro-average F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.8194148540496826}]}, {"text": "Because we are dealing with single-label classification, the micro-average F1-score corresponds to the accuracy and is a measure of the overall prediction effectiveness (Manning et al.,: Total number of features (n-grams or subgraphs) vs. number of features present only in main cores along with the reduction of the dimension of the feature space on all four datasets.", "labels": [], "entities": [{"text": "single-label classification", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6892634630203247}, {"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8944640755653381}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9996391534805298}]}, {"text": "Conversely, the macro-average F1-score takes into account the skewed class label distributions by weighting each class uniformly.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9547219276428223}]}, {"text": "The statistical significance of improvement inaccuracy over the n-gram SVM baseline was assessed using the micro sign test (p < 0.05)).", "labels": [], "entities": []}, {"text": "For the Amazon dataset, we report the average of each metric over the four sub-collections.", "labels": [], "entities": [{"text": "Amazon dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9787900149822235}]}, {"text": "shows the results on the four considered datasets.", "labels": [], "entities": []}, {"text": "The first three rows correspond to the baselines: unsupervised n-gram feature extraction and then supervised learning using kNN, NB (Multinomial but Bernoulli yields similar results) and linear SVM.", "labels": [], "entities": [{"text": "unsupervised n-gram feature extraction", "start_pos": 50, "end_pos": 88, "type": "TASK", "confidence": 0.6663848757743835}]}, {"text": "The last three rows correspond to our approaches.", "labels": [], "entities": []}, {"text": "In our first approach, denoted as \"gSpan + SVM\", we mine frequent subgraphs (gSpan) as features and then train a linear SVM.", "labels": [], "entities": []}, {"text": "These features correspond to long-distance n-grams.", "labels": [], "entities": []}, {"text": "This leads to the best results in text categorization on almost all datasets (all if we compare to baseline methods), in particular on multiclass document categorization (R8 and WebKB).", "labels": [], "entities": [{"text": "text categorization", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7122979611158371}]}], "tableCaptions": [{"text": " Table 1: Total number of features (n-grams or sub- graphs) vs. number of features present only in  main cores along with the reduction of the dimen- sion of the feature space on all four datasets.", "labels": [], "entities": []}, {"text": " Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9970408082008362}, {"text": "F1-score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.925205409526825}, {"text": "LingSpam", "start_pos": 448, "end_pos": 456, "type": "DATASET", "confidence": 0.924827516078949}]}]}