{"title": [{"text": "Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences.", "labels": [], "entities": []}, {"text": "C-PHRASE outperforms the state-of-the-art C-BOW model on a variety of lexical tasks.", "labels": [], "entities": []}, {"text": "Moreover, since C-PHRASE word vectors are induced through a composi-tional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy).", "labels": [], "entities": []}, {"text": "The recent evaluation of suggests that the C-BOW model introduced by is, consistently, the best across many tasks.", "labels": [], "entities": []}, {"text": "Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts.", "labels": [], "entities": []}, {"text": "This is reminiscent of how the parameters of some compositional distributional seman- We refer here not only to the results reported in, but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unitn. it/composes/semantic-vectors.html).", "labels": [], "entities": []}, {"text": "The experiments there suggest that only the Glove vectors of are competitive with C-BOW, and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW.", "labels": [], "entities": []}, {"text": "tic models are estimated by optimizing the prediction of the contexts in which phrases occur in corpora (.", "labels": [], "entities": []}, {"text": "However, these compositional approaches assume that word vectors have already been constructed, and contextual evidence is only used to induce optimal combination rules to derive representations of phrases and sentences.", "labels": [], "entities": []}, {"text": "In this paper, we follow through on this observation to propose the new C-PHRASE model.", "labels": [], "entities": []}, {"text": "Similarly to C-BOW, C-PHRASE learns word representations by optimizing their joint context prediction.", "labels": [], "entities": []}, {"text": "However, unlike in flat, window-based C-BOW, C-PHRASE groups words according to their syntactic structure, and it simultaneously optimizes context-predictions at different levels of the syntactic hierarchy.", "labels": [], "entities": []}, {"text": "For example, given training sentence \"A sad dog is howling in the park\", C-PHRASE will optimize context prediction for dog, sad dog, a sad dog, a sad dog is howling, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves.", "labels": [], "entities": []}, {"text": "C-PHRASE word representations outperform C-BOW on several word-level benchmarks.", "labels": [], "entities": []}, {"text": "In addition, because they are estimated in a compositional way, C-PHRASE word vectors, when combined through simple addition, produce sentence representations that are better than those obtained when adding other kinds of vectors, and competitive against ad-hoc compositional methods on various sentence meaning benchmarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Lexical task performance. See Section 3.1 for figures of merit (all in percentage form) and  state-of-the-art references. C-BOW results (tuned on rg) are taken from Baroni et al. 2014b.", "labels": [], "entities": []}, {"text": " Table 2: Sentential task performance. See Section 3.1 for figures of merit (all in percentage form) and  state-of-the-art references. The PLF results on msrvid and onwn2 are taken from Paperno et al. 2014.", "labels": [], "entities": [{"text": "PLF", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.6621981263160706}]}]}