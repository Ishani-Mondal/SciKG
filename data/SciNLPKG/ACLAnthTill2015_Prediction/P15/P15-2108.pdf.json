{"title": [{"text": "Lexical Comparison Between Wikipedia and Twitter Corpora by Using Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Compared with carefully edited prose, the language of social media is informal in the extreme.", "labels": [], "entities": []}, {"text": "The application of NLP techniques in this context may require a better understanding of word usage within social media.", "labels": [], "entities": []}, {"text": "In this paper, we compute a word embedding fora corpus of tweets, comparing it to a word embedding for Wikipedia.", "labels": [], "entities": []}, {"text": "After learning a transformation of one vector space to the other, and adjusting similarity values according to term frequency, we identify words whose usage differs greatly between the two corpora.", "labels": [], "entities": []}, {"text": "For any given word, the set of words closest to it in a particular embedding provides a characterization for that word's usage within the corresponding corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Users of social media typically employ highly informal language, including slang, acronyms, typos, deliberate misspellings, and interjections ().", "labels": [], "entities": []}, {"text": "This heavy use of nonstandard language, as well as the overall level of noise on social media, creates substantial problems when applying standard NLP tools and techniques.", "labels": [], "entities": []}, {"text": "For example, apply machine translation methods to convert tweets to standard English in an attempt to ameliorate this problem.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7055120766162872}]}, {"text": "Similarly, and address this problem by generating corrections for irregularly spelled words in social media.", "labels": [], "entities": []}, {"text": "In this short paper, we continue this line of research, applying word embedding to the problem of translating between the informal English of social media, specifically Twitter, and the formal English of carefully edited texts, such as those found in Wikipedia.", "labels": [], "entities": [{"text": "translating", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.9645310044288635}]}, {"text": "Starting with a large collection of tweets and a copy of Wikipedia, we construct word embeddings for both corpora.", "labels": [], "entities": []}, {"text": "We then generate a transformation matrix, mapping one vector space into another.", "labels": [], "entities": []}, {"text": "After applying a normalization based on term frequency, we use distances in the transformed space as an indicator of differences in word usage between the two corpora.", "labels": [], "entities": []}, {"text": "The method identifies differences in usage due to jargon, contractions, abbreviations, hashtags, and the influence of popular culture, as well as other factors.", "labels": [], "entities": []}, {"text": "As a method of validation, we examine the overlap in closely related words, showing that distance after transformation and normalization correlates with the degree of overlap.", "labels": [], "entities": [{"text": "validation", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.969255268573761}]}], "datasetContent": [{"text": "In this section, we describe the results of applying our method to Twitter and Wikipedia.", "labels": [], "entities": []}, {"text": "The Wikipedia dataset for our experiments consists of all English Wikipedia articles downloaded from MediaWiki data dumps 1 . The Twitter dataset was collected through the Twitter Streaming API from November 2013 to March 2015.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8572793304920197}, {"text": "MediaWiki data dumps", "start_pos": 101, "end_pos": 121, "type": "DATASET", "confidence": 0.9344416658083597}, {"text": "Twitter dataset", "start_pos": 130, "end_pos": 145, "type": "DATASET", "confidence": 0.7708648443222046}]}, {"text": "We restricted the dataset to English-language tweets on the basis of the language field contained in each tweet.", "labels": [], "entities": []}, {"text": "To obtain distributed word representation for both corpora, we trained word vectors separately by applying the word2vec 2 tool, a wellknown implementation of word embedding.", "labels": [], "entities": []}, {"text": "Before applying the tool, we cleaned Wikipedia and Twitter corpora.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9360478520393372}]}, {"text": "The clean version of Wikipedia retains only normally visible article text on Wikipedia web pages.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.93585205078125}]}, {"text": "The Twitter clean version removes HTML code, URLs, user mentions(@), the # symbol of hashtags, and all the retweeted tweets.", "labels": [], "entities": []}, {"text": "The sizes of document and vocabulary in both corpora are listed in.", "labels": [], "entities": []}, {"text": "There are two major parameters that affect word2vec training quality: the dimensionality of word vectors, and the size of the surrounding words window.", "labels": [], "entities": []}, {"text": "We choose 300 for our word vector dimensionality, which is typical for training large dataset with word2vec.", "labels": [], "entities": []}, {"text": "We choose 10 words for the window, since tweet sentence length is 9.2 \u00b1 6.4).", "labels": [], "entities": []}], "tableCaptions": []}