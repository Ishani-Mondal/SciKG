{"title": [{"text": "Abstractive Multi-Document Summarization via Phrase Selection and Merging *", "labels": [], "entities": [{"text": "Abstractive Multi-Document Summarization", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.47022010882695514}, {"text": "Phrase Selection", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7476642727851868}]}], "abstractContent": [{"text": "We propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences , namely, noun/verb phrases.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.5816207230091095}]}, {"text": "Different from existing abstraction-based approaches , our method first constructs a pool of concepts and facts represented by phrases from the input documents.", "labels": [], "entities": []}, {"text": "Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints.", "labels": [], "entities": []}, {"text": "We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution fora summary.", "labels": [], "entities": [{"text": "phrase selection and merging", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.7485737800598145}]}, {"text": "Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation.", "labels": [], "entities": [{"text": "benchmark data set TAC 2011", "start_pos": 28, "end_pos": 55, "type": "DATASET", "confidence": 0.8147754311561585}]}], "introductionContent": [{"text": "Existing multi-document summarization (MDS) methods fall in three categories: extraction-based, compression-based and abstraction-based.", "labels": [], "entities": [{"text": "multi-document summarization (MDS)", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.857833993434906}]}, {"text": "Most * The work described in this paper is substantially supported by grants from the Research and Development Grant of Huawei Technologies Co.", "labels": [], "entities": [{"text": "Huawei Technologies Co", "start_pos": 120, "end_pos": 142, "type": "DATASET", "confidence": 0.9469918012619019}]}, {"text": "Ltd (YB2013090068/TH138232) and the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414).", "labels": [], "entities": [{"text": "YB2013090068/TH138232)", "start_pos": 5, "end_pos": 27, "type": "DATASET", "confidence": 0.7233629822731018}]}, {"text": "The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9747535586357117}]}, {"text": "However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary.", "labels": [], "entities": []}, {"text": "To this end, some researchers apply compression on the selected sentences by deleting words or phrases;, which is the compression-based method.", "labels": [], "entities": []}, {"text": "Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence.", "labels": [], "entities": []}, {"text": "In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion).", "labels": [], "entities": []}, {"text": "Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences.", "labels": [], "entities": []}, {"text": "One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7894460558891296}]}, {"text": "These works first conduct clustering on sentences to compute the salience of topical themes.", "labels": [], "entities": []}, {"text": "Then, sentence fusion is applied within each cluster of related sentences to generate anew sentence containing common information units of the sentences.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7123964428901672}]}, {"text": "The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner.", "labels": [], "entities": []}, {"text": "In this paper, we propose an abstractive MDS framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases (NPs/VPs).", "labels": [], "entities": []}, {"text": "This idea is based on two observations.", "labels": [], "entities": []}, {"text": "First, the major constituent phrases loosely correspond to the concepts and facts.", "labels": [], "entities": []}, {"text": "After reading a set of documents describing the same topic or event, a person digests these documents as key concepts and facts in his/her mind, such as \"an armed man\" and \"walked into an Amish school\" from.", "labels": [], "entities": []}, {"text": "Second, a summary writer re-organizes the key concepts and facts to form new sentences for the summary.", "labels": [], "entities": []}, {"text": "Accordingly, our proposed framework has two major components corresponding to the above observations.", "labels": [], "entities": []}, {"text": "The first component creates a pool of concepts and facts represented by NPs and VPs from the input documents.", "labels": [], "entities": []}, {"text": "A salience score is computed for each phrase by exploiting redundancy of the document content in a global manner.", "labels": [], "entities": []}, {"text": "The second component constructs new sentences by selecting and merging phrases based on their salience scores, and ensures the validity of new sentences using a integer linear optimization model.", "labels": [], "entities": [{"text": "validity", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.94925457239151}]}, {"text": "The contribution of this paper is two folds.", "labels": [], "entities": []}, {"text": "(1) We extract NPs/VPs from constituency trees to represent key concepts/facts, and merge them to construct new sentences, which allows more summary content units (SCUs)) to be included in a sentence by breaking the original sentence boundaries.", "labels": [], "entities": []}, {"text": "(2) The designed optimization framework for addressing the problem is unique and effective.", "labels": [], "entities": []}, {"text": "Our optimization algorithm simultaneously selects and merges a set of phrases that maximize the number of covered SCUs in a summary.", "labels": [], "entities": []}, {"text": "Meanwhile, since the basic unit is phrases, we design compatibility relations among NPs and VPs, as well as other optimization constraints, to ensure that the generated sentences contain correct facts.", "labels": [], "entities": []}, {"text": "Compared with the sentence fusion approaches that compute salience scores of sentence clusters, our proposed framework explores a more fine-grained textual unit (i.e., phrases), and maximizes the salience of selected phrases in a global manner.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7202727198600769}]}], "datasetContent": [{"text": "The data set of traditional summarization task in Text Analysis Conference (TAC) 2011 is used to evaluate the performance of our approach.", "labels": [], "entities": [{"text": "summarization task in Text Analysis Conference (TAC) 2011", "start_pos": 28, "end_pos": 85, "type": "TASK", "confidence": 0.8362027704715729}]}, {"text": "This data set is the latest one and it contains 44 topics.", "labels": [], "entities": []}, {"text": "Each topic falls into one of 5 predefined event categories and contains 10 related news documents.", "labels": [], "entities": []}, {"text": "There are four writers to write model summaries for each topic.", "labels": [], "entities": []}, {"text": "The data set of traditional summarization task in TAC 2010 is employed as the development/tuning data set.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9063373506069183}, {"text": "TAC 2010", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7877247035503387}]}, {"text": "This data set contains 46 topics from the same predefined categories.", "labels": [], "entities": []}, {"text": "Each topic also has 10 documents and 4 model summaries.", "labels": [], "entities": []}, {"text": "Based on the tuning set, the key parameters of our model are set as follows.", "labels": [], "entities": []}, {"text": "The constants B and \u03c1 in the weighting function are set to 6 and 0.5 repectively.", "labels": [], "entities": [{"text": "B", "start_pos": 14, "end_pos": 15, "type": "METRIC", "confidence": 0.9622184634208679}]}, {"text": "The similarity threshold in obtaining the alternative VPs is 0.75.", "labels": [], "entities": [{"text": "similarity threshold", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.9787299036979675}]}, {"text": "We did not observe significant difference between cosine similarity and Jaccard Index.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 50, "end_pos": 67, "type": "METRIC", "confidence": 0.7799932062625885}, {"text": "Jaccard Index", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.9261565506458282}]}, {"text": "We mainly evaluate the system by pyramid evaluation.", "labels": [], "entities": []}, {"text": "To gain a comprehensive understanding, we also evaluate by ROUGE evaluation and manual linguistic quality evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9871295094490051}]}, {"text": "The pyramid evaluation metric (Nenkova and Passonneau, 2004) involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning.", "labels": [], "entities": []}, {"text": "Different weights are assigned to SCUs based on their frequency in model summaries.", "labels": [], "entities": []}, {"text": "A weighted inventory of SCUs named a pyramid is created, which constitutes a resource for investigating alternate realizations of the same meaning.", "labels": [], "entities": []}, {"text": "Such property makes pyramid method more suitable to evalu-).", "labels": [], "entities": []}, {"text": "Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.9000591039657593}, {"text": "TAC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.7028266191482544}]}, {"text": "Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in ().", "labels": [], "entities": [{"text": "TAC", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.6267767548561096}]}, {"text": "The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary.", "labels": [], "entities": []}, {"text": "Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model.", "labels": [], "entities": []}, {"text": "Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations.", "labels": [], "entities": []}, {"text": "An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 72, "end_pos": 88, "type": "METRIC", "confidence": 0.9687694013118744}]}, {"text": "showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods shows the comparison with them under the automated pyramid evaluation.", "labels": [], "entities": []}, {"text": "Our method achieves the best results in both thresholds, which means that our method is able to find more semantic content units (SCUs) than the state-of-the-art system in TAC 2011.", "labels": [], "entities": [{"text": "TAC 2011", "start_pos": 172, "end_pos": 180, "type": "DATASET", "confidence": 0.9058441519737244}]}, {"text": "In addition, paired t-test (with p < 0.01) comparing our model with the best system in TAC 2011, i.e., System 22, shows that the performance of our model is significantly better.", "labels": [], "entities": [{"text": "TAC 2011", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.9002801477909088}]}, {"text": "It is worth noting that the three systems used additional external linguistic resources: System 22 used a Wikipedia corpus for providing domain knowledge, System 17 and 43 defined some categoryspecific features.", "labels": [], "entities": []}, {"text": "Without any domain adaption, our framework can still achieve encouraging performance.", "labels": [], "entities": []}, {"text": "We calculate Pearson's correlation to measure how well the automatic pyramid approximates the manual pyramid scores for 50 system submissions in TAC 2011.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 13, "end_pos": 34, "type": "METRIC", "confidence": 0.9624116023381551}, {"text": "TAC 2011", "start_pos": 145, "end_pos": 153, "type": "DATASET", "confidence": 0.9127647876739502}]}, {"text": "The values are 0.91 and 0.93 for thresholds 0.6 and 0.65 respectively.", "labels": [], "entities": []}, {"text": "It demonstrates that the automated pyramid is reliable to differentiate the performance of different methods.", "labels": [], "entities": []}, {"text": "As mentioned above, we favor the pyramid evaluation over the ROUGE score because it can measure the summary quality beyond simply string matching.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.974553257226944}]}, {"text": "Here, we also provide ROUGE score for our reference.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9772953689098358}]}, {"text": "ROUGE-1.5.5 package 3 is employed with the same parameters as in TAC.", "labels": [], "entities": [{"text": "ROUGE-1.5.5", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.5307828783988953}, {"text": "TAC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.6505875587463379}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "Our performance is slightly better than System 22, and it is not as good as System 43 and 17.", "labels": [], "entities": []}, {"text": "The reason is that System 43 and 17 used category-specific features and trained the feature weights with the category information in TAC 2010 data.", "labels": [], "entities": [{"text": "TAC 2010 data", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.9556051095326742}]}, {"text": "These features help them select better category-specific content for the summary.", "labels": [], "entities": []}, {"text": "However, the usability of such features depends on the availability of predefined categories in the summarization task, as well as the availability of training data with the same predefined categories for estimating feature weights.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.9072615504264832}]}, {"text": "Therefore, the adaptability of these methods is limited to some extent.", "labels": [], "entities": []}, {"text": "In contrast, our framework does not define any category-specific feature and only uses TAC 2010 data to tune the parameters for general summarization purpose.", "labels": [], "entities": [{"text": "TAC 2010 data", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9119725227355957}]}, {"text": "The linguistic quality of summaries is evaluated using the five linguistic quality questions on grammaticality (Q1), non-redundancy (Q2), referential clarity (Q3), focus (Q4), and coherence (Q5) in Document Understanding Conferences (DUC).", "labels": [], "entities": []}, {"text": "A Likert scale with five levels is employed with 5 being very good with 1 being very poor.", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.8509779274463654}]}, {"text": "A summary was blindly evaluated by three assessors on each question.", "labels": [], "entities": []}, {"text": "System 22 performed better than System 43 and 17 in TAC 2011 on the evaluation of readability, which is an aggregation of the above questions.", "labels": [], "entities": [{"text": "TAC 2011", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8520849645137787}]}, {"text": "Considering the intensive labor force of manual assessment, we only conduct comparison with System 22.", "labels": [], "entities": [{"text": "manual assessment", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.6592512875795364}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "On average, the two systems perform very closely.", "labels": [], "entities": []}, {"text": "System 22 is an extraction-based method that picks the original sentences, hence it achieves higher score in Q1 grammaticality, while our approach has some new sentences with grammar mistakes, which is a common problem for abstractive methods and deserves more future research effort.", "labels": [], "entities": []}, {"text": "For Q4 focus, our score is higher than System 22, which reveals that our summary sentences are relatively more cohesive.", "labels": [], "entities": [{"text": "Q4 focus", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.6588050127029419}]}, {"text": "The score of Q3 referential clarity shows that the referential relation is basically clear in our summaries, even when new sentences are automatically generated.", "labels": [], "entities": [{"text": "referential clarity", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.562744140625}]}, {"text": "In general, ignoring the grammaticality scores, our system still performs better than System 22.", "labels": [], "entities": []}, {"text": "Specifically, the average scores of our system and System 22 on the last four questions are 3.37 and 3.33 respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison with the top 3 systems in  TAC 2011.", "labels": [], "entities": [{"text": "TAC 2011", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.8912968039512634}]}, {"text": " Table 3: Performance under ROUGE metric.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.8475815057754517}]}, {"text": " Table 3. Our performance is  slightly better than System 22, and it is not as good  as System 43 and 17. The reason is that System 43  and 17 used category-specific features and trained  the feature weights with the category information in TAC 2010 data. These features help them se- lect better category-specific content for the sum- mary. However, the usability of such features de- pends on the availability of predefined categories  in the summarization task, as well as the avail- ability of training data with the same predefined  categories for estimating feature weights. There- fore, the adaptability of these methods is limited to  some extent. In contrast, our framework does not  define any category-specific feature and only uses  TAC 2010 data to tune the parameters for general  summarization purpose.", "labels": [], "entities": [{"text": "TAC 2010 data", "start_pos": 241, "end_pos": 254, "type": "DATASET", "confidence": 0.9261123736699423}, {"text": "summarization task", "start_pos": 445, "end_pos": 463, "type": "TASK", "confidence": 0.8976751565933228}, {"text": "TAC 2010 data", "start_pos": 745, "end_pos": 758, "type": "DATASET", "confidence": 0.8965265154838562}]}, {"text": " Table 4: Evaluation of linguistic quality.", "labels": [], "entities": []}]}