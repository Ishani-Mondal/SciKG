{"title": [{"text": "End-to-end Learning of Semantic Role Labeling Using Recurrent Neural Networks", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.5898521443208059}]}], "abstractContent": [{"text": "Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8551733195781708}, {"text": "natural language processing (NLP)", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7381312946478525}]}, {"text": "To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used.", "labels": [], "entities": [{"text": "SRL", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9845514893531799}]}, {"text": "The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011).", "labels": [], "entities": [{"text": "SRL learning", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9172153174877167}]}, {"text": "In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9874926805496216}]}, {"text": "We take only original text information as input feature , without using any syntactic knowledge.", "labels": [], "entities": []}, {"text": "The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F 1 score of 81.07.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7666860222816467}, {"text": "CoNLL-2005 shared task", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.899296224117279}, {"text": "F 1 score", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9885508418083191}]}, {"text": "This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models.", "labels": [], "entities": []}, {"text": "We also obtained the same conclusion with F 1 = 81.27 on CoNLL-2012 shared task.", "labels": [], "entities": [{"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.989904373884201}, {"text": "CoNLL-2012 shared task", "start_pos": 57, "end_pos": 79, "type": "DATASET", "confidence": 0.8590484062830607}]}, {"text": "As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second.", "labels": [], "entities": [{"text": "parsing", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9603580236434937}]}, {"text": "Our analysis shows that our model is better at handling longer sentences than traditional models.", "labels": [], "entities": []}, {"text": "And the latent variables of our model implicitly capture the syntactic structure of a sentence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (SRL) is a form of shallow semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8408456742763519}, {"text": "shallow semantic parsing", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.740710973739624}]}, {"text": "Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized.", "labels": [], "entities": []}, {"text": "Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc..", "labels": [], "entities": [{"text": "Manner", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9256366491317749}]}, {"text": "SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (, automatic document categorization () and questionanswering (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.840863823890686}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.8360297977924347}]}, {"text": "SRL is considered as a supervised machine learning problem.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8453810811042786}]}, {"text": "In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus.", "labels": [], "entities": []}, {"text": "Actually, people often treat this problem as a multi-step classification task.", "labels": [], "entities": []}, {"text": "First, whether an argument is related to the predicate is determined; next the detail relation type was decided(.", "labels": [], "entities": []}, {"text": "Syntactic information is considered to play an essential role in solving this problem).", "labels": [], "entities": []}, {"text": "The location of an argument on syntactic tree provides an intermediate tag for improving the performance.", "labels": [], "entities": []}, {"text": "However, building this syntactic tree also introduces the prediction risk inevitably.", "labels": [], "entities": []}, {"text": "The analysis in found that the major source of the incorrect predictions was the syntactic parser.", "labels": [], "entities": []}, {"text": "Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level ().", "labels": [], "entities": []}, {"text": "Besides, feature templates in this classification task strongly rely on the expert experience.", "labels": [], "entities": []}, {"text": "They need iterative modification after analyzing how the system performs on development data.", "labels": [], "entities": []}, {"text": "When the corpus and data distribution are changed, or when people move to another language, the feature templates have to be re-designed.", "labels": [], "entities": []}, {"text": "To address the above issues,) proposed a unified neural network architecture using word embedding and convolution.", "labels": [], "entities": []}, {"text": "They applied their architecture on four standard NLP tasks: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "Part-Of-Speech tagging (POS)", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.8271346688270569}, {"text": "Named Entity Recognition (NER)", "start_pos": 108, "end_pos": 138, "type": "TASK", "confidence": 0.7510257015625635}, {"text": "Semantic Role Labeling (SRL)", "start_pos": 143, "end_pos": 171, "type": "TASK", "confidence": 0.75931017100811}]}, {"text": "They were able to reach the previous state-of-the-art performance on all these tasks except for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.7835955023765564}]}, {"text": "They had to resort to parsing features in order to make the system competitive with state-of-the-art performance.", "labels": [], "entities": []}, {"text": "In this work, we propose an end-to-end system using deep bi-directional long short-term memory (DB-LSTM) model to address the above difficulties.", "labels": [], "entities": []}, {"text": "We take only original text as the input features, without any intermediate tag such as syntactic information.", "labels": [], "entities": []}, {"text": "The input features are processed by the following 8 layers of LSTM bidirectionally.", "labels": [], "entities": []}, {"text": "At the top locates the conditional random field (CRF) model for tag sequence prediction.", "labels": [], "entities": [{"text": "tag sequence prediction", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.7011607885360718}]}, {"text": "We achieve the state-of-the-art performance of f-score F 1 = 81.07 on CoNLL-2005 shared task and F 1 = 81.27 on CoNLL-2012 shared task.", "labels": [], "entities": [{"text": "f-score F 1", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.896429717540741}, {"text": "CoNLL-2005 shared task", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.8287927707036337}, {"text": "F 1 = 81.27", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9560664296150208}, {"text": "CoNLL-2012 shared task", "start_pos": 112, "end_pos": 134, "type": "DATASET", "confidence": 0.8825335502624512}]}, {"text": "At last, we find the traditional syntactic information can also be inferred from the learned representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We mainly evaluated and analyzed our system on the commonly used CoNLL-2005 shared task data set and the conclusions are also validated on CoNLL-2012 shared task.", "labels": [], "entities": [{"text": "CoNLL-2005 shared task data set", "start_pos": 65, "end_pos": 96, "type": "DATASET", "confidence": 0.9245072364807129}, {"text": "CoNLL-2012 shared task", "start_pos": 139, "end_pos": 161, "type": "DATASET", "confidence": 0.892673909664154}]}], "tableCaptions": [{"text": " Table 1: An example sequence with 4 input fea- tures: argument, predicate, predicate context (con- text length is 3) , region mark. \"IOB\" tagging  scheme is used (Collobert et al., 2011).", "labels": [], "entities": [{"text": "IOB\" tagging", "start_pos": 134, "end_pos": 146, "type": "TASK", "confidence": 0.5134629805882772}]}, {"text": " Table 2: F 1 of CNN method on development set  and test set of CoNLL-2005 data set.", "labels": [], "entities": [{"text": "CoNLL-2005 data set", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.9837470849355062}]}, {"text": " Table 3: F 1 with LSTM method on development  set and test set of CoNLL-2005 data set and  CoNLL-2012 data set. Emb: the type of embed- ding. d: the number of LSTM layers. ctx-p: pred- icate context length. m r : region mark feature. h:  hidden layer size.", "labels": [], "entities": [{"text": "CoNLL-2005 data set", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.9836239417394003}, {"text": "CoNLL-2012 data set", "start_pos": 92, "end_pos": 111, "type": "DATASET", "confidence": 0.9711400866508484}]}, {"text": " Table 4: Comparison with previous methods.", "labels": [], "entities": []}, {"text": " Table 5: F 1 on each sub sets and classes", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9983657002449036}]}]}