{"title": [{"text": "Learning Answer-Entailing Structures for Machine Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Understanding open-domain text is one of the primary challenges in NLP.", "labels": [], "entities": []}, {"text": "Machine comprehension evaluates the sys-tem's ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text.", "labels": [], "entities": []}, {"text": "For this task, we posit that there is a hidden (latent) structure that explains the relation between the question, correct answer, and text.", "labels": [], "entities": []}, {"text": "We call this the answer-entailing structure; given the structure, the correct-ness of the answer is evident.", "labels": [], "entities": []}, {"text": "Since the structure is latent, it must be inferred.", "labels": [], "entities": []}, {"text": "We present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts.", "labels": [], "entities": []}, {"text": "We extend this framework to incorporate multi-task learning on the different sub-tasks that are required to perform machine comprehension.", "labels": [], "entities": []}, {"text": "Evaluation on a publicly available dataset shows that our framework outperforms various IR and neural-network baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.999005138874054}]}], "introductionContent": [{"text": "Developing an ability to understand natural language is a long-standing goal in NLP and holds the promise of revolutionizing the way in which people interact with machines and retrieve information (e.g., for scientific endeavor).", "labels": [], "entities": []}, {"text": "To evaluate this ability, proposed the task of machine comprehension (MCTest), along with a dataset for evaluation.", "labels": [], "entities": []}, {"text": "Machine comprehension evaluates a machine's understanding by posing a series of reading comprehension questions and associated texts, where the answer to each question can be found only in its associated text.", "labels": [], "entities": []}, {"text": "Solutions typically focus on some semantic interpretation of the text, possibly with some form of probabilistic or logical inference, in order to answer the questions.", "labels": [], "entities": []}, {"text": "Despite significant recent interest, the problem remains unsolved.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach for machine comprehension.", "labels": [], "entities": [{"text": "machine comprehension", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7823142409324646}]}, {"text": "Our approach learns latent answer-entailing structures that can help us answer questions about a text.", "labels": [], "entities": []}, {"text": "The answer-entailing structures in our model are closely related to the inference procedure often used in various models for MT), RTE (), paraphrase (), QA (, etc. and correspond to the best (latent) alignment between a hypothesis (formed from the question and a candidate answer) with appropriate snippets in the text that are required to answer the question.", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9555234313011169}, {"text": "RTE", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.8484618067741394}, {"text": "paraphrase", "start_pos": 138, "end_pos": 148, "type": "METRIC", "confidence": 0.9232096672058105}, {"text": "QA", "start_pos": 153, "end_pos": 155, "type": "METRIC", "confidence": 0.9547780156135559}]}, {"text": "An example of such an answer-entailing structure is given in.", "labels": [], "entities": []}, {"text": "The key difference between the answerentailing structures considered here and the alignment structures considered in previous works is that we can align multiple sentences in the text to the hypothesis.", "labels": [], "entities": []}, {"text": "The sentences in the text considered for alignment are not restricted to occur contiguously in the text.", "labels": [], "entities": [{"text": "alignment", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.9708706736564636}]}, {"text": "To allow such a discontiguous alignment, we make use of the document structure; in particular, we take help from rhetorical structure theory) and event and entity coreference links across sentences.", "labels": [], "entities": []}, {"text": "Modelling the inference procedure via answer-entailing structures is a crude yet effective and computationally inexpensive proxy to model the semantics needed for the problem.", "labels": [], "entities": []}, {"text": "Learning these latent structures can also be bene-: The answer-entailing structure for an example from MCTest500 dataset.", "labels": [], "entities": [{"text": "MCTest500 dataset", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.9764606654644012}]}, {"text": "The question and answer candidate are combined to generate a hypothesis sentence.", "labels": [], "entities": []}, {"text": "Then latent alignments are found between the hypothesis and the appropriate snippets in the text.", "labels": [], "entities": []}, {"text": "The solid red lines show the word alignments from the hypothesis words to the passage words, the dashed black lines show auxiliary co-reference links in the text and the labelled dotted black arrows show the RST relation (elaboration) between the two sentences.", "labels": [], "entities": [{"text": "RST", "start_pos": 208, "end_pos": 211, "type": "TASK", "confidence": 0.8502689599990845}]}, {"text": "Note that the two sentences do not have to be contiguous sentences in the text.", "labels": [], "entities": []}, {"text": "We provide some more examples of answer-entailing structures in the supplementary.", "labels": [], "entities": []}, {"text": "ficial as they can assist a human in verifying the correctness of the answer, eliminating the need to read a lengthy document.", "labels": [], "entities": []}, {"text": "The overall model is trained in a max-margin fashion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent.", "labels": [], "entities": []}, {"text": "We also extend our LSSVM to multi-task settings using a top-level question-type classification.", "labels": [], "entities": []}, {"text": "Many QA systems include a question classification component (, which typically divides the questions into semantic categories based on the type of the question or answers expected.", "labels": [], "entities": [{"text": "question classification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7542313933372498}]}, {"text": "This helps the system impose some constraints on the plausible answers.", "labels": [], "entities": []}, {"text": "Machine comprehension can benefit from such a pre-classification step, not only to constrain plausible answers, but also to allow the system to use different processing strategies for each category.", "labels": [], "entities": []}, {"text": "Recently, defined a set of 20 sub-tasks in the machine comprehension setting, each referring to a specific aspect of language understanding and reasoning required to build a machine comprehension system.", "labels": [], "entities": []}, {"text": "They include fact chaining, negation, temporal and spatial reasoning, simple induction, deduction and many more.", "labels": [], "entities": [{"text": "fact chaining", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7798112630844116}, {"text": "negation", "start_pos": 28, "end_pos": 36, "type": "TASK", "confidence": 0.9697372317314148}, {"text": "simple induction", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6882486343383789}]}, {"text": "We use this set to learn to classify questions into the various machine comprehension subtasks, and show that this task classification further improves our performance on MCTest.", "labels": [], "entities": []}, {"text": "By using the multi-task setting, our learner is able to exploit the commonality among tasks where possible, while having the flexibility to learn taskspecific parameters where needed.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first use of multi-task learning in a structured prediction model for QA.", "labels": [], "entities": [{"text": "QA", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9041305184364319}]}, {"text": "We provide experimental validation for our model on a real-world dataset () and achieve superior performance vs. a number of IR and neural network baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: We use two datasets for our evaluation.", "labels": [], "entities": []}, {"text": "(1) First is the MCTest-500 dataset 1 , a freely available set of 500 stories (split into 300 train, 50 dev and 150 test) and associated questions ().", "labels": [], "entities": [{"text": "MCTest-500 dataset 1", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.9492970506350199}]}, {"text": "The stories are fictional so the answers can be found only in the story itself.", "labels": [], "entities": []}, {"text": "The stories and questions are carefully limited, thereby minimizing the world knowledge required for this task.", "labels": [], "entities": []}, {"text": "Yet, the task is challenging for most modern NLP systems.", "labels": [], "entities": []}, {"text": "Each story in MCTest has four multiple choice questions, each with four answer choices.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.8544719219207764}]}, {"text": "Each question has only one correct answer.", "labels": [], "entities": []}, {"text": "Furthermore, questions are also annotated with 'single' and 'multiple' labels.", "labels": [], "entities": []}, {"text": "The questions annotated 'single' only require one sentence in the story to answer them.", "labels": [], "entities": []}, {"text": "For 'multiple' questions it should not be possible to find the answer to the question in any individual sentence of the passage.", "labels": [], "entities": []}, {"text": "Ina sense, the 'multiple' questions are harder than the 'single' questions as they typically require complex lexical analysis, some inference and some form of limited reasoning.", "labels": [], "entities": []}, {"text": "Cucerzanconverted questions can also be downloaded from the MCTest website.", "labels": [], "entities": [{"text": "MCTest website", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9460079371929169}]}, {"text": "(2) The second dataset is a synthetic dataset released under the bAbI project 2 (.", "labels": [], "entities": [{"text": "bAbI project 2", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.9218866229057312}]}, {"text": "The dataset presents a set of 20 'tasks', each testing a different aspect of text understanding and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner.", "labels": [], "entities": [{"text": "text understanding and reasoning", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.7739347591996193}]}, {"text": "For each 'task', 1000 questions are used for training and 1000 for testing.", "labels": [], "entities": []}, {"text": "The 'tasks' refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg.", "labels": [], "entities": []}, {"text": "relations, yes/no questions, counting questions, etc.", "labels": [], "entities": []}, {"text": "Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appearing in the text, etc.", "labels": [], "entities": []}, {"text": "We write simple rules to convert the question and answer candidate pairs to hypotheses.", "labels": [], "entities": []}, {"text": "Baselines: We have five baselines.", "labels": [], "entities": []}, {"text": "(1) The first three baselines are inspired from.", "labels": [], "entities": []}, {"text": "The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text.", "labels": [], "entities": []}, {"text": "(2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well.", "labels": [], "entities": []}, {"text": "As far as we know, SW+D is the best previously published result on this task.", "labels": [], "entities": []}, {"text": "4 (3) The third baseline (called RTE) uses textual entailment to answer MCTest questions.", "labels": [], "entities": []}, {"text": "For this baseline, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using) and then selecting the answer whose statement has the highest likelihood of being entailed by the 2 https://research.facebook.com/researchers/1543934539189348 3 Note that the bAbI dataset is artificial and not meant for open-domain machine comprehension.", "labels": [], "entities": [{"text": "bAbI dataset", "start_pos": 300, "end_pos": 312, "type": "DATASET", "confidence": 0.8351309597492218}]}, {"text": "It is a toy dataset generated from a simulated world.", "labels": [], "entities": []}, {"text": "Due to its restrictive nature, we do not use it directly in evaluating our method vs. other open-domain machine comprehension methods.", "labels": [], "entities": []}, {"text": "However, it provides benefit in identifying interesting subtasks of machine comprehension.", "labels": [], "entities": []}, {"text": "As will be seen, we are able to leverage the dataset both to improve our multi-task learning algorithm, as well as to analyze the strengths and weaknesses of our model.", "labels": [], "entities": []}, {"text": "We also construct two additional baselines (LSTM and QUANTA) for comparison in this paper both of which achieve superior performance to SW+D. story.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.8931917548179626}, {"text": "QUANTA", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9783210754394531}, {"text": "SW+D. story", "start_pos": 136, "end_pos": 147, "type": "DATASET", "confidence": 0.5166101232171059}]}, {"text": "5 (4) The fourth baseline (called LSTM) is taken from.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8353404998779297}]}, {"text": "The baseline uses LSTMs to accomplish the task.", "labels": [], "entities": []}, {"text": "LSTMs have recently achieved state-of-the-art results in a variety of tasks due to their ability to model longterm context information as opposed to other neural networks based techniques.", "labels": [], "entities": []}, {"text": "The fifth baseline (called QANTA) 6 is taken from.", "labels": [], "entities": [{"text": "QANTA) 6", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9146681229273478}]}, {"text": "QANTA too uses a recursive neural network for question answering.", "labels": [], "entities": [{"text": "QANTA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8568707704544067}, {"text": "question answering", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8998289108276367}]}, {"text": "Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments.", "labels": [], "entities": []}, {"text": "First, we look at question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8720579743385315}]}, {"text": "We use a simple question classification based on the question word (what, why, what, etc.).", "labels": [], "entities": []}, {"text": "Next, we also use a question/answer classification 7 from.", "labels": [], "entities": []}, {"text": "This classifies questions into different semantic classes based on the possible semantic types of the answers sought.", "labels": [], "entities": []}, {"text": "Finally, we also learn a classifier for the 20 tasks in the Machine Comprehension gamut described in.", "labels": [], "entities": []}, {"text": "The classification algorithm (called TaskClassification) was built on the bAbI training set.", "labels": [], "entities": [{"text": "bAbI training set", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9208763241767883}]}, {"text": "It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the question and answer.", "labels": [], "entities": []}, {"text": "The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting.", "labels": [], "entities": []}, {"text": "In our experiments we will see that learning these strategies is better than learning the question answer classification which is in turn better than learning the question classification.", "labels": [], "entities": []}, {"text": "Results: We compare multiple variants of our LSSVM 8 where we consider a variety of answerentailing structures and our modification for negation and multi-task LSSVM, where we consider three kinds of task classification strategies against the baselines on the MCTest dataset.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 260, "end_pos": 274, "type": "DATASET", "confidence": 0.96249720454216}]}, {"text": "We consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG 4: Comparison of variations of our method against several baselines on the MCTest-500 dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9994409680366516}, {"text": "NDCG 4", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.8543405532836914}, {"text": "MCTest-500 dataset", "start_pos": 174, "end_pos": 192, "type": "DATASET", "confidence": 0.9826986491680145}]}, {"text": "The figure shows two statistics, accuracy (on the left) and NDCG4 (on the right) on the test set of MCTest-500.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9997403025627136}, {"text": "NDCG4", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9664254784584045}, {"text": "MCTest-500", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.7953721284866333}]}, {"text": "All differences between the baselines and LSSVMs, the improvement due to negation and the improvements due to multi-task learning are significant (p < 0.01) using the two-tailed paired T-test.", "labels": [], "entities": []}, {"text": "The exact numbers are available in the supplementary.", "labels": [], "entities": []}, {"text": "Unlike classification accuracy which evaluates if the prediction is corrector not, NDCG 4 , being a measure of ranking quality, evaluates the position of the correct answer in our predicted ranking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8496730327606201}]}, {"text": "describes the comparison on MCTest.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9584376811981201}]}, {"text": "We can observe that all the LSSVM models have a better performance than all the five baselines (including LSTMs and RNNs which are state-ofthe-art for many other NLP tasks) on both metrics.", "labels": [], "entities": []}, {"text": "Very interestingly, LSSVMs have a considerable improvement over the baselines for \"multiple\" questions.", "labels": [], "entities": []}, {"text": "We posit that this is because of our answer-entailing structure alignment strategy which is a weak proxy to the deep semantic inference procedure required for machine comprehension.", "labels": [], "entities": [{"text": "answer-entailing structure alignment", "start_pos": 37, "end_pos": 73, "type": "TASK", "confidence": 0.6721127728621165}]}, {"text": "The RTE baseline achieves the best performance on the \"single\" questions.", "labels": [], "entities": [{"text": "RTE baseline", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.6929759383201599}]}, {"text": "This is perhaps because the RTE community has almost entirely focused on single sentence text hypothesis pairs fora longtime.", "labels": [], "entities": []}, {"text": "However, RTE fares pretty poorly on the \"multiple\" questions indicating that of-the-shelf RTE systems cannot perform inference across large texts.", "labels": [], "entities": []}, {"text": "Here we observe a clear benefit of using the alignment to the best subset structure over alignment to best sentence structure.", "labels": [], "entities": []}, {"text": "We furthermore see improvements when the best subset alignment structure is augmented with the subset+ features.", "labels": [], "entities": []}, {"text": "We can observe that the negation heuristic also helps, especially for \"single\" questions (majority of negation cases in the MCTest dataset are for the \"single\" questions).", "labels": [], "entities": [{"text": "negation heuristic", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.921209067106247}, {"text": "MCTest dataset", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.9740476906299591}]}, {"text": "It is also interesting to see that the multi-task learners show a substantial boost over the single task SSVM.", "labels": [], "entities": []}, {"text": "Also, it can be observed that the multi-task learner greatly benefits if we can learn a separation between the various strategies needed to learn an overarching list of subtasks required to solve the machine comprehension task.", "labels": [], "entities": []}, {"text": "The multi-task method (TaskClassification) which uses the Weston style categorization does better than the multi-task method (QAClassification) that learns the question answer classification.", "labels": [], "entities": [{"text": "question answer classification", "start_pos": 160, "end_pos": 190, "type": "TASK", "confidence": 0.6252059439818064}]}, {"text": "QAClassification in turn performs better than multi-task method (QClassification) that learns the question classification only.", "labels": [], "entities": [{"text": "QAClassification", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.7621992826461792}]}], "tableCaptions": []}