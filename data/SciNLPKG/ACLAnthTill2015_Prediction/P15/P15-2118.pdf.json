{"title": [{"text": "Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a simple yet effective approach to learning bilingual word embeddings (BWEs) from non-parallel document-aligned data (based on the omnipresent skip-gram model), and its application to bilingual lexicon induction (BLI).", "labels": [], "entities": [{"text": "learning bilingual word embeddings (BWEs)", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.6730409775461469}, {"text": "bilingual lexicon induction (BLI)", "start_pos": 195, "end_pos": 228, "type": "TASK", "confidence": 0.7909929553667704}]}, {"text": "We demonstrate the utility of the induced BWEs in the BLI task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) () have been introduced recently as part of neural network architectures for statistical language modeling.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 174, "end_pos": 203, "type": "TASK", "confidence": 0.7990086476008097}]}, {"text": "Recent studies ( have showcased a direct link and comparable performance to \"more traditional\" distributional models, but the skip-gram model with negative sampling (SGNS) () is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (.", "labels": [], "entities": [{"text": "word representation", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.714665487408638}]}, {"text": "A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (.", "labels": [], "entities": []}, {"text": "When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar).", "labels": [], "entities": []}, {"text": "These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc.", "labels": [], "entities": [{"text": "computing cross-lingual and multilingual semantic word similarity", "start_pos": 152, "end_pos": 217, "type": "TASK", "confidence": 0.5722322719437736}, {"text": "bilingual lexicon induction (BLI)", "start_pos": 222, "end_pos": 255, "type": "TASK", "confidence": 0.7888713280359904}]}, {"text": "However, all these models critically require at least sentence-aligned parallel data and/or readilyavailable translation dictionaries to induce bilingual word embeddings (BWEs) that are consistent and closely aligned over languages in the same semantic space.", "labels": [], "entities": []}, {"text": "Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs).", "labels": [], "entities": []}, {"text": "Our BLI model based on our novel BWEs significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other BLI models based on recently proposed BWE induction models ().", "labels": [], "entities": []}, {"text": "The focus of the work is on learning lexicons from documentaligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links).", "labels": [], "entities": []}, {"text": "The architecture of our BWE Skip-Gram model for learning bilingual word embeddings from document-aligned comparable data.", "labels": [], "entities": [{"text": "BWE Skip-Gram", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8075614273548126}]}, {"text": "Source language words and documents are drawn as gray boxes, while target language words and documents are drawn as blue boxes.", "labels": [], "entities": []}, {"text": "The right side of the figure (separated by a vertical dashed line) illustrates how a pseudo-bilingual document is constructed from a pair of two aligned documents; two documents are first merged, and then words in the pseudo-bilingual document are randomly shuffled to ensure that both source and target language words occur as context words.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training Data We use comparable Wikipedia data introduced in) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN).", "labels": [], "entities": []}, {"text": "All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations.", "labels": [], "entities": []}, {"text": "Following prior work (, we retain only nouns that occur at least 5 times in the corpus.", "labels": [], "entities": []}, {"text": "Lemmatized word forms are recorded when available, and original forms otherwise. is used for POS tagging and lemmatization.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.8500099778175354}]}, {"text": "After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair.", "labels": [], "entities": []}, {"text": "Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison.", "labels": [], "entities": []}, {"text": "BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for all three training corpora with the following parameters from the word2vec package (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with 25 samples, and a subsampling rate of value 1e\u22124.", "labels": [], "entities": [{"text": "BWESG Training Setup", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9517471591631571}, {"text": "BWESG", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8848921656608582}]}, {"text": "All models are trained for 15 epochs.", "labels": [], "entities": []}, {"text": "We have varied the number of embedding dimensions: d = 100, 200, 300, and have also trained the model with d = 40 to be directly comparable to pre-trained state-of-theart BWEs from ().", "labels": [], "entities": []}, {"text": "Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.", "labels": [], "entities": []}, {"text": "Since cosine is used for all similarity computations in the BLI task, we call our new BLI model BWESG+cos.", "labels": [], "entities": []}, {"text": "Baseline BLI Models We compare BWESG+cos to a series of state-of-the-art BLI models from document-aligned comparable data: (1) BiLDA-BLI -A BLI model that relies on the induction of latent cross-lingual topics ( by the bilingual LDA model and represents words as probability distributions over these topics.", "labels": [], "entities": []}, {"text": "The seed lexicon is bootstrapped using the method from (.", "labels": [], "entities": []}, {"text": "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work ().", "labels": [], "entities": []}, {"text": "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (.", "labels": [], "entities": []}, {"text": "Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN)).", "labels": [], "entities": []}, {"text": "Translation direction is ES/IT/NL \u2192 EN.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9355024099349976}]}, {"text": "Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc 1 score, that is, the number of source language (ES/IT/NL) words w Si from ground truth translation pairs for which the top ranked word cross-lingually is the correct trans-", "labels": [], "entities": [{"text": "Acc 1 score", "start_pos": 156, "end_pos": 167, "type": "METRIC", "confidence": 0.9832232594490051}]}], "tableCaptions": [{"text": " Table 2: BLI performance for all tested BLI  models for ES/IT/NL-EN, with all bilingual word  representations except CHANDAR and GOUWS  learned from comparable Wikipedia data. The  scores for BWESG+cos are computed as post-hoc  averages over 10 random shuffles.", "labels": [], "entities": [{"text": "CHANDAR", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.956571638584137}, {"text": "GOUWS", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.9031505584716797}]}]}