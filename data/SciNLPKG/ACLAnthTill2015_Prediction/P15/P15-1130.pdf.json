{"title": [{"text": "Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory", "labels": [], "entities": [{"text": "Predicting Polarities of Tweets", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8918674886226654}]}], "abstractContent": [{"text": "In this paper, we introduce Long Short-Term Memory (LSTM) recurrent network for twitter sentiment prediction.", "labels": [], "entities": [{"text": "twitter sentiment prediction", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.7337216933568319}]}, {"text": "With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function.", "labels": [], "entities": []}, {"text": "Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique.", "labels": [], "entities": []}, {"text": "According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features.", "labels": [], "entities": []}, {"text": "Furthermore , words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified.", "labels": [], "entities": []}, {"text": "An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases.", "labels": [], "entities": [{"text": "negation expression processing", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.9642655253410339}]}], "introductionContent": [{"text": "Twitter and other similar microblogs are rich resources for opinions on various kinds of products and events.", "labels": [], "entities": []}, {"text": "Detecting sentiment in microblogs is a challenging task that has attracted increasing research interest in recent years (.", "labels": [], "entities": [{"text": "Detecting sentiment in microblogs", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9082352072000504}]}, {"text": "carried out the pioneer work of predicting sentiment in tweets using machine learning technology.", "labels": [], "entities": [{"text": "predicting sentiment in tweets", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.9130409061908722}]}, {"text": "They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature.", "labels": [], "entities": []}, {"text": "Such feature is widely used because it's simple and surprisingly efficient in many tasks.", "labels": [], "entities": []}, {"text": "However, there are also disadvantages of bag-of-words features represented by onehot vectors.", "labels": [], "entities": []}, {"text": "Firstly, it bears a data sparsity issue ().", "labels": [], "entities": []}, {"text": "In tweets, irregularities and 140-character limitation exacerbate the sparseness.", "labels": [], "entities": []}, {"text": "Secondly, losing sequence information makes it difficult to figure out the polarity properly (.", "labels": [], "entities": []}, {"text": "A typical case is that the sentiment word in a negation phrase tends to express opposite sentiment to that of the context.", "labels": [], "entities": []}, {"text": "Distributed representations of words can ease the sparseness, but there are limitations to the unsupervised-learned ones.", "labels": [], "entities": []}, {"text": "Words with special functions in specific tasks are not distinguished.", "labels": [], "entities": []}, {"text": "Such as negation words, which play a special role in polarity classification, are represented similarly with other adverbs.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8102465867996216}]}, {"text": "Such similarities will limit the compositional models' abilities of describing a sentiment-specific interaction between words.", "labels": [], "entities": []}, {"text": "Moreover, word vectors trained by cooccurrence statistics in a small window of context effectively represent the syntactic and semantic similarity.", "labels": [], "entities": []}, {"text": "Thus, words like good and bad have very similar representations).", "labels": [], "entities": []}, {"text": "It's problematic for sentiment classifiers.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.9085817635059357}]}, {"text": "Sentiment is expressed by phrases rather than by.", "labels": [], "entities": [{"text": "Sentiment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9591151475906372}]}, {"text": "Seizing such sequence information would help to analyze complex sentiment expressions.", "labels": [], "entities": []}, {"text": "One possible method to leverage context is connecting embeddings of words in a window and compose them to a fixlength vector).", "labels": [], "entities": []}, {"text": "However, window-based methods may have difficulty reaching long-distance words and simply connected vectors do not always represent the interactions of context properly.", "labels": [], "entities": []}, {"text": "Theoretically, a recurrent neural network could process the whole sentence of arbitrary length by encoding the context cyclically.", "labels": [], "entities": []}, {"text": "However, the length of reachable context is often limited when using stochastic gradient descent (.", "labels": [], "entities": []}, {"text": "Besides that, a traditional recurrent architecture is not powerful enough to deal with the complex sentiment expressions.", "labels": [], "entities": []}, {"text": "Fixed input limits the network's ability of learning task-specific representations and simple additive combination of hidden activations and input activations has difficulty capturing more complex linguistic phenomena.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the Long ShortTerm Memory (LSTM) recurrent neural network for twitter sentiment classification by means of simulating the interactions of words during the compositional process.", "labels": [], "entities": [{"text": "twitter sentiment classification", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.7830551862716675}]}, {"text": "Multiplicative operations between word embeddings through gate structures provide more flexibility and lead to better compositional results compare to the additive ones in simple recurrent neural network.", "labels": [], "entities": []}, {"text": "Experimentally, the proposed architecture outperforms various classifiers and feature engineering approaches, matching the performance of the current best datadriven approach.", "labels": [], "entities": []}, {"text": "Vectors of task-distinctive words (such as not) are distinguished after tuning and representations of opposite-polarity words are separated.", "labels": [], "entities": []}, {"text": "Moreover, predicting result on negation test set shows our model is effective in dealing with negation phrases (a typical case of sentiment expressed by sequence).", "labels": [], "entities": []}, {"text": "We study the process of the network handling the negation expressions and show the promising potential of our model simulating complex linguistic phenomena with gates and constant error carousels in the LSTM blocks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recurrent Neural Network: We implement the recurrent architecture with trainable lookuptable layer by modifying RNNLIB (Graves, 2010) toolkit.", "labels": [], "entities": [{"text": "RNNLIB (Graves, 2010) toolkit", "start_pos": 112, "end_pos": 141, "type": "DATASET", "confidence": 0.9095809800284249}]}, {"text": "Early Stopping: From the noisy labelled data, we randomly selected 20,000 negative and 20,000 1 http://twittersentiment.appspot.com/ positive tweets as validation set for early stopping.", "labels": [], "entities": [{"text": "Early Stopping", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6659737825393677}, {"text": "early stopping", "start_pos": 171, "end_pos": 185, "type": "TASK", "confidence": 0.8054259717464447}]}, {"text": "The rest 1,560,000 tweets are used as training set.", "labels": [], "entities": []}, {"text": "Parameter Setting: Tuned on the validation set, the size of the hidden layer is set to 60.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9272909760475159}]}, {"text": "Word Embeddings: We run word2vec on the training set of 1.56M tweets (without labels) to get domain-specific representations and use them as initial input of the model.", "labels": [], "entities": []}, {"text": "Limited to the input format of the toolkit, we learned 25-dimensional (relatively small) vectors.", "labels": [], "entities": []}, {"text": "Skip-gram architecture and hierarchical softmax algorithm are chosen during training.", "labels": [], "entities": []}, {"text": "Recursive Autoencoder (RAE) has proven to bean effective model to compose words vectors in sentiment classification tasks).", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.8942372798919678}]}, {"text": "We run RAE with randomly initialized word embeddings.", "labels": [], "entities": [{"text": "RAE", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.6854602098464966}]}, {"text": "We do not compare with) for lack of phrase-level sentiment labels and accurate parsing results.", "labels": [], "entities": []}, {"text": "shows the accuracies of different classifiers.", "labels": [], "entities": []}, {"text": "Notably, RNN-TLT and LSTM-TLT out-perform the three non-neural classifiers.", "labels": [], "entities": []}, {"text": "Trained on the considerable data, these classifiers provide strong baselines.", "labels": [], "entities": []}, {"text": "However, bag-of-words representations are not powerful enough.", "labels": [], "entities": []}, {"text": "Sparsity and losing sequence information hurt the performance of classifiers.", "labels": [], "entities": []}, {"text": "Neural models overcome these problems by using distributed representations and temporally encoding the contextual interaction.", "labels": [], "entities": []}, {"text": "Different from STS dataset deciding the polarity based on emoticons, the benchmark dataset in) is labelled by human annotators.", "labels": [], "entities": [{"text": "STS dataset", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.7321311682462692}]}, {"text": "In this work we focus on the binary polarity classification and abandon the neutral tweets.", "labels": [], "entities": [{"text": "binary polarity classification", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.693930983543396}]}, {"text": "There are 4099/735/1742 available tweets in the training/dev/test set respectively.", "labels": [], "entities": [{"text": "training/dev/test set", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.7063678999741873}]}, {"text": "Since the training set is relatively small, we don't apply fine tuning on word vectors.", "labels": [], "entities": []}, {"text": "Namely we use fixed lookup-table for both RNN and LSTM.", "labels": [], "entities": []}, {"text": "300-dimensional vectors are learned on the 1.56M tweets of STS dataset using word2vec.", "labels": [], "entities": [{"text": "STS dataset", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9533578455448151}]}, {"text": "Other settings stay the same as previous experiments.", "labels": [], "entities": []}, {"text": "shows our work compared to SVM and Recursive Autoencoder.", "labels": [], "entities": []}, {"text": "From the result, we can see that the recurrent models outperforms the baselines by exploiting more context information of word interactions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies of different classifiers.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9797931909561157}]}, {"text": " Table 2: Comparison with different feature engi- neering methods.", "labels": [], "entities": []}, {"text": " Table 3: Accuracies of different methods on Se- mEval 2013", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9844516515731812}, {"text": "Se- mEval 2013", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.7342224717140198}]}, {"text": " Table 4: Accuracy on generated negation phrases  test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9950966238975525}]}]}