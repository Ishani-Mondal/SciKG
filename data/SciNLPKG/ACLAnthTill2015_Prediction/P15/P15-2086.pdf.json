{"title": [{"text": "Learning Hidden Markov Models with Distributed State Representations for Domain Adaptation", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.73285773396492}]}], "abstractContent": [{"text": "Recently, a variety of representation learning approaches have been developed in the literature to induce latent generalizable features across two domains.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.9146623313426971}]}, {"text": "In this paper, we extend the standard hidden Markov models (HMMs) to learn distributed state representations to improve cross-domain prediction performance.", "labels": [], "entities": [{"text": "cross-domain prediction", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.7056995928287506}]}, {"text": "We reformu-late the HMMs by mapping each discrete hidden state to a distributed representation vector and employ an expectation-maximization algorithm to jointly learn distributed state representations and model parameters.", "labels": [], "entities": []}, {"text": "We empirically investigate the proposed model on cross-domain part-of-speech tagging and noun-phrase chunking tasks.", "labels": [], "entities": [{"text": "cross-domain part-of-speech tagging", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.6440203686555227}, {"text": "noun-phrase chunking tasks", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.8336593707402548}]}, {"text": "The experimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7298991829156876}]}], "introductionContent": [{"text": "Domain adaptation aims to obtain an effective prediction model fora particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.79964879155159}]}, {"text": "Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.79073566198349}, {"text": "natural language processing (NLP)", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.8215551773707072}]}, {"text": "Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging), chunking, named entity recognition (, dependency parsing ( and semantic role labeling.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.7034910023212433}, {"text": "chunking", "start_pos": 102, "end_pos": 110, "type": "TASK", "confidence": 0.9594150185585022}, {"text": "named entity recognition", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.6087029178937277}, {"text": "dependency parsing", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7516813576221466}, {"text": "semantic role labeling", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.6463053226470947}]}, {"text": "Ina typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical ().", "labels": [], "entities": []}, {"text": "Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue.", "labels": [], "entities": [{"text": "cross-domain feature distribution divergence", "start_pos": 181, "end_pos": 225, "type": "TASK", "confidence": 0.6007973030209541}]}, {"text": "A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods), word embedding based representation learning methods ( and some other representation learning methods).", "labels": [], "entities": [{"text": "cross-domain feature divergence", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6034342149893442}]}, {"text": "In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation.", "labels": [], "entities": [{"text": "distributed state representation learning", "start_pos": 77, "end_pos": 118, "type": "TASK", "confidence": 0.6803739294409752}, {"text": "domain adaptation", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7141447216272354}]}, {"text": "Instead of learning a single discrete latent state for each observation in a given sentence, we learn a distributed representation vector.", "labels": [], "entities": []}, {"text": "We define a state embedding matrix to map each latent state value to a low-dimensional distributed vector and reformulate the three local distributions of HMMs based on the distributed state representations.", "labels": [], "entities": []}, {"text": "We then simultaneously learn the state embedding matrix and the model parameters using an expectation-maximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "The hidden states of each word in a sentence can be decoded using the standard Viterbi decoding procedure of HMMs, and its distributed representation can be obtained by a simple mapping with the state embedding matrix.", "labels": [], "entities": []}, {"text": "We then use the context-aware distributed representations of the words as their augmenting features to perform cross-domain part-of-speech (POS) tagging and noun-phrase (NP) chunking.", "labels": [], "entities": [{"text": "cross-domain part-of-speech (POS) tagging", "start_pos": 111, "end_pos": 152, "type": "TASK", "confidence": 0.5680061429738998}, {"text": "noun-phrase (NP) chunking", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.6585930109024047}]}, {"text": "The proposed approach is closely related to the clustering based method) as we both use latent state representations as generalizable features.", "labels": [], "entities": []}, {"text": "However, they use standard HMMs to produce discrete hidden state features for each observation word, while we induce distributed state representation vectors.", "labels": [], "entities": []}, {"text": "Our distributed HMMs share similarities with the word embedding based method (, and can be more space-efficient than the standard HMMs.", "labels": [], "entities": []}, {"text": "Moreover, our model can incorporate context information into observation feature vectors to perform representation learning in a context-aware manner.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.8928396999835968}]}, {"text": "The distributed state representations induced by our model hence have larger representing capacities and generalizing capabilities for cross-domain learning than standard HMMs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks.", "labels": [], "entities": [{"text": "cross-domain partof-speech (POS) tagging", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.5736572742462158}, {"text": "noun-phrase (NP) chunking tasks", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7198986709117889}]}, {"text": "We used the same experimental datasets as in) for cross-domain POS tagging from Wall Street Journal (WSJ) domain to MED-LINE domain) and for crossdomain NP chunking from CoNLL shared task dataset () to Open American National Corpus (OANC) ().", "labels": [], "entities": [{"text": "cross-domain POS tagging", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6444279650847117}, {"text": "Wall Street Journal (WSJ) domain", "start_pos": 80, "end_pos": 112, "type": "DATASET", "confidence": 0.8986256888934544}, {"text": "crossdomain NP chunking", "start_pos": 141, "end_pos": 164, "type": "TASK", "confidence": 0.6169098714987437}, {"text": "CoNLL shared task dataset", "start_pos": 170, "end_pos": 195, "type": "DATASET", "confidence": 0.6473639160394669}, {"text": "Open American National Corpus (OANC)", "start_pos": 202, "end_pos": 238, "type": "DATASET", "confidence": 0.8030613192490169}]}], "tableCaptions": [{"text": " Table 1: Test performance for cross-domain POS tagging and NP chunking.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.7691360414028168}, {"text": "NP chunking", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.7788921296596527}]}]}