{"title": [{"text": "The NL2KR Platform for building Natural Language Translation Systems", "labels": [], "entities": [{"text": "Natural Language Translation", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6480502088864645}]}], "abstractContent": [{"text": "This paper presents the NL2KR platform to build systems that can translate text to different formal languages.", "labels": [], "entities": []}, {"text": "It is freely-available 1 , customizable, and comes with an Interactive GUI support that is useful in the development of a translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.9647133350372314}]}, {"text": "Our key contribution is a user-friendly system based on an interactive multistage learning algorithm.", "labels": [], "entities": []}, {"text": "This effective algorithm employs Inverse-\u03bb, Generalization and user provided dictionary to learn new meanings of words from sentences and their representations.", "labels": [], "entities": []}, {"text": "Using the learned meanings, and the Generalization approach, it is able to translate new sentences.", "labels": [], "entities": [{"text": "Generalization", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.9636742472648621}, {"text": "translate new sentences", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.890893280506134}]}, {"text": "NL2KR is evaluated on two standard corpora, Jobs and GeoQuery and it exhibits state-of-the-art performance on both of them.", "labels": [], "entities": [{"text": "NL2KR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8733246326446533}]}], "introductionContent": [], "datasetContent": [{"text": "We have evaluated NL2KR on two standard corpora: GeoQuery and Jobs.", "labels": [], "entities": []}, {"text": "For both the corpus, the output generated by the learned system has been considered correct if it is an exact replica of the logical formula described in the corpus.", "labels": [], "entities": []}, {"text": "We report the performance in terms of precision (percentage of returned logical-forms that are correct), recall (percentage of sentences for which the correct logical-form was returned), F1-measure (harmonic mean of precision and recall) and the size of the initial dictionary.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9992795586585999}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9995225667953491}, {"text": "F1-measure", "start_pos": 187, "end_pos": 197, "type": "METRIC", "confidence": 0.9992210865020752}, {"text": "precision", "start_pos": 216, "end_pos": 225, "type": "METRIC", "confidence": 0.8428589105606079}, {"text": "recall", "start_pos": 230, "end_pos": 236, "type": "METRIC", "confidence": 0.9746730327606201}]}, {"text": "We compare the performance of our system with recently published, directly-comparable works, namely, FUBL (), UBL (, \u03bb-WASP (), ZC07 (Zettlemoyer and Collins, 2007) and ZC05) systems.", "labels": [], "entities": [{"text": "FUBL", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9469265341758728}, {"text": "ZC07 (Zettlemoyer and Collins, 2007)", "start_pos": 128, "end_pos": 164, "type": "DATASET", "confidence": 0.8007253557443619}]}], "tableCaptions": [{"text": " Table 1: Comparison of Initial and Learned dictionary for GeoQuery corpus on the basis of the number of entries in the  dictionary, number of unique <word, CCG category> pairs and the number of unique meanings across all the entries. \"GUI  Driven\" denotes the amount of the total meanings given through interactive GUI and is a subset of the Initial dictionary.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of Initial and Learned dictionary for Jobs corpus.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of Precision, Recall and F1-measure on  GeoQuery dataset.", "labels": [], "entities": [{"text": "Precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9833147525787354}, {"text": "Recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9628289937973022}, {"text": "F1-measure", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9928245544433594}, {"text": "GeoQuery dataset", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9506487846374512}]}, {"text": " Table 4: Comparison of Precision, Recall and F1-measure on  Jobs dataset.", "labels": [], "entities": [{"text": "Precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9624807834625244}, {"text": "Recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9607127904891968}, {"text": "F1-measure", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9924499988555908}, {"text": "Jobs dataset", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.6920385956764221}]}]}