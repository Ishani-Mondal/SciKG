{"title": [{"text": "Learning Topic Hierarchies for Wikipedia Categories", "labels": [], "entities": []}], "abstractContent": [{"text": "Existing studies have utilized Wikipedia for various knowledge acquisition tasks.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7829593420028687}]}, {"text": "However, no attempts have been made to explore multi-level topic knowledge contained in Wikipedia articles' Contents tables.", "labels": [], "entities": []}, {"text": "The articles with similar subjects are grouped together into Wikipedia categories.", "labels": [], "entities": []}, {"text": "In this work, we propose novel methods to automatically construct comprehensive topic hierarchies forgiven categories based on the structured Contents tables as well as corresponding unstruc-tured text descriptions.", "labels": [], "entities": []}, {"text": "Such a hierarchy is important for information browsing, document organization and topic prediction.", "labels": [], "entities": [{"text": "document organization", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7475716173648834}, {"text": "topic prediction", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7952905595302582}]}, {"text": "Experimental results show our proposed approach, incorporating both the structural and textual information, achieves high quality category topic hierarchies.", "labels": [], "entities": []}], "introductionContent": [{"text": "As a free-access online encyclopedia, written collaboratively by people allover the world, Wikipedia (abbr. to Wiki) offers a surplus of rich information.", "labels": [], "entities": []}, {"text": "Millions of articles cover various concepts and instances . Wiki has been widely used for various knowledge discovery tasks.", "labels": [], "entities": [{"text": "knowledge discovery tasks", "start_pos": 98, "end_pos": 123, "type": "TASK", "confidence": 0.7957094609737396}]}, {"text": "Some good examples include knowledge mining from Wiki infoboxes (, and taxonomy deriving from Wiki category system.", "labels": [], "entities": [{"text": "knowledge mining", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7712534666061401}]}, {"text": "We observe that, in addition to Wiki's infoboxes and category system, Wiki articles' Contents tables (CT for short) also provide valuable structured topic knowledge with different levels of granularity.", "labels": [], "entities": []}, {"text": "For example, in the article \"2010 Haiti Earthquake\", shown in, the left Contents zone is a CT formed in a topic hierarchy for- mat.", "labels": [], "entities": []}, {"text": "If we view \"2010 Haiti earthquake\" as the root topic, the first-level \"Geology\" and \"Damage to infrastructure\" tags can be viewed as its subtopics, and the second-level \"Tsunami\" and \"Aftershocks\" tags underneath \"Geology\" are the subtopics of \"Geology\".", "labels": [], "entities": []}, {"text": "Clicking any of the tags in Contents, we can jump to the corresponding text description.", "labels": [], "entities": []}, {"text": "Wiki articles contain a wealth of this kind of structured and unstructured information.", "labels": [], "entities": []}, {"text": "However, to our best knowledge, little work has been done to leverage the knowledge in CT.", "labels": [], "entities": []}, {"text": "In Wiki, similar articles (each with their own CT) belonging to the same subject are grouped together into a Wiki category.", "labels": [], "entities": []}, {"text": "We aim to integrate multiple topic hierarchies represented by C-T (from the articles under the same Wiki category) into a comprehensive category topic hierarchy (CTH).", "labels": [], "entities": []}, {"text": "While there also exist manually built CTH represented by CT in corresponding Wiki articles, they are still too high-level and incomplete.", "labels": [], "entities": []}, {"text": "Take the \"Earthquake\" category as an example, its corresponding Wiki article 2 only contains some major and common topics.", "labels": [], "entities": []}, {"text": "It does not include the subtopic \"nuclear power plant\", which is an important subtopic of the \"2011 Japan earthquake\".", "labels": [], "entities": [{"text": "Japan earthquake\"", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.8014768560727438}]}, {"text": "A comprehensive CTH is believed to be more useful for information browsing, document organization and topic extraction in new text corpus ().", "labels": [], "entities": [{"text": "document organization", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7397263050079346}, {"text": "topic extraction", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.778961181640625}]}, {"text": "Thus, we propose to investigate the Wiki articles of the same category to automatically build a comprehensive CTH to enhance the manually built CTH.", "labels": [], "entities": []}, {"text": "Clearly, it is very challenging to learn a CTH from multiple topic hierarchies in different articles due to the following 3 reasons: 1) A topic can be denoted by a variety of tags in different articles (e.g., \"foreign aids\" and \"aids from other countries\"); 2) Structural/hierarchical information can be inconsistent (or even opposite) across different articles (e.g., \"response subtopicOf aftermath\" and \"aftermath subtopicOf response\" in different earthquake event articles); 3) Intuitively, text descriptions of the topics in Wiki articles are supposed to be able to help determine subtopic relations between topics.", "labels": [], "entities": []}, {"text": "However, how can we model the textual correlation?", "labels": [], "entities": []}, {"text": "In this study, we propose a novel approach to build a high-quality CTH for any given Wiki category.", "labels": [], "entities": []}, {"text": "We use a Bayesian network to model a CTH, and map the CTH learning problem as a structure learning problem.", "labels": [], "entities": []}, {"text": "We leverage both structural and textual information of topics in the articles to induce the optimal tree structure.", "labels": [], "entities": []}, {"text": "Experiments on 3 category data demonstrate the effectiveness of our approach for CTH learning.", "labels": [], "entities": [{"text": "CTH learning", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.9095693528652191}]}], "datasetContent": [{"text": "We evaluate the CTH automatically generated by our proposed methods via comparing it with a manually constructed ground-truth CTH.", "labels": [], "entities": []}, {"text": "We evaluate our methods on 3 categories, i.e., English \"earthquake\" and \"election\" categories containing 293 and 60 articles, and Chinese \"earthquake\" category containing 48 articles . After removing noisy tags such as \"references\" and \"see also\", they contain 463, 79 and 426 unique tags respectively.", "labels": [], "entities": []}, {"text": "After tag clustering 6 , we can get 176, 57 and 112 topics for each category.", "labels": [], "entities": []}, {"text": "We employ the precision measure to evaluate the performance of our methods.", "labels": [], "entities": [{"text": "precision measure", "start_pos": 14, "end_pos": 31, "type": "METRIC", "confidence": 0.9774227142333984}]}, {"text": "Let Rand R s be subtopic relation sets of our generated result and ground-truth result respectively, then precison=|R \u2229 R s |/|R|.", "labels": [], "entities": [{"text": "precison", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9464402198791504}]}, {"text": "Due to the number of relations |R|=|R s | = |T c \u2212 1|, we have precison=recall=F1=|R \u2229 R s |/|R|.", "labels": [], "entities": [{"text": "precison", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.983302652835846}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.995079517364502}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9897907376289368}]}, {"text": "We compare three methods, including our basic method (Basic) which uses only non-normalized structural information, our proposed probabilistic method considering only structural information We use min-max normalization x * = x\u2212min max\u2212min We filter articles with little information in Contents.", "labels": [], "entities": []}, {"text": "We use an incremental clustering algorithm (\u03bb = 0) (Pro+S), and considering both structural and textual information (0 < \u03bb < 1) (Pro+ST).", "labels": [], "entities": []}], "tableCaptions": []}