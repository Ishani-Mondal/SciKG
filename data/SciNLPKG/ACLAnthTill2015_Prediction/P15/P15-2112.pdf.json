{"title": [{"text": "Rhetoric Map of an Answer to Compound Queries", "labels": [], "entities": []}], "abstractContent": [{"text": "Given a discourse tree fora text as a candidate answer to a compound query, we propose a rule system for valid and invalid occurrence of the query keywords in this tree.", "labels": [], "entities": []}, {"text": "To be a valid answer to a query, its keywords need to occur in a chain of elementary discourse unit of this answer so that these units are fully ordered and connected by nucleus-satellite relations.", "labels": [], "entities": []}, {"text": "An answer might be invalid if the que-ries' keywords occur in the answer's satellite discourse units only.", "labels": [], "entities": []}, {"text": "We build the rhetoric map of an answer to prevent it from firing by queries whose keywords occur in non-adjacent areas of the Answer Map.", "labels": [], "entities": []}, {"text": "We evaluate the improvement of search relevance by filtering out search results not satisfying the proposed rule system, demonstrating a 4% increase of accuracy with respect to the nearest neighbor learning approach which does not use the discourse tree structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9992626309394836}]}], "introductionContent": [{"text": "Answering compound queries, where its keywords are distributed through text of a candidate answer, is a sophisticated problem requiring deep linguistic analysis.", "labels": [], "entities": [{"text": "Answering compound queries", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9133286277453104}]}, {"text": "If the query keywords occur in an answer text in a linguistically connected manner, this answer is most likely relevant.", "labels": [], "entities": []}, {"text": "This is usually true when all these keywords occur in the same sentence: they should be connected syntactically.", "labels": [], "entities": []}, {"text": "For the inter-sentence connections, these keywords need to be connected via anaphora, refer to the same entity or sub-entity, or be linked via rhetoric discourse.", "labels": [], "entities": []}, {"text": "If the query keywords occur in different sentences, there should be linguistic cues for some sort of connections between these occurrences.", "labels": [], "entities": []}, {"text": "If there is no connection, then different constraints for an object expressed by a query might be applied to different objects in the answer text, therefore, this answer is perhaps irrelevant.", "labels": [], "entities": []}, {"text": "There are following possibilities of such connections.", "labels": [], "entities": []}, {"text": "If two areas of keyword occurrences are connected with anaphoric relation, the answer is most likely relevant.", "labels": [], "entities": []}, {"text": "If the text contains a dialogue, and some question keywords are in a request and other are in the reply to this request, then these keywords are connected and the answer is relevant.", "labels": [], "entities": []}, {"text": "To identify such situation, one needs to find a pair of communicative actions and to confirm that this pair is of request-reply kind.", "labels": [], "entities": []}, {"text": "They indicate the coherence structure of a text.", "labels": [], "entities": []}, {"text": "Rhetoric relations for text can be represented by a Discourse tree (DT) which is a labeled tree.", "labels": [], "entities": []}, {"text": "The leaves of this tree correspond to contiguous units for clauses (elementary discourse units, EDU).", "labels": [], "entities": []}, {"text": "Adjacent EDUs as well as higher-level (larger) discourse units are organized in a hierarchy by rhetoric relation (e.g., background, attribution).", "labels": [], "entities": []}, {"text": "Anti-symmetric relation takes a pair of EDUs: nuclei, which are core parts of the relation, and satellites, the supportive parts of the rhetoric relation.", "labels": [], "entities": []}, {"text": "The most important class of connections we focus in this study is rhetoric.", "labels": [], "entities": []}, {"text": "Once an answer text is split into EDUs, and rhetoric relations are established between them, it is possible to establish rules for whether query keywords occurring in text are connected by rhetoric relations (and therefore, this answer is likely relevant) or not connected (and this answer is most likely irrelevant).", "labels": [], "entities": []}, {"text": "Hence we use the DT as abase for an Answer Map of a text: certain sets of nodes in DT correspond to queries so that this text is a valid answer, and certain sets of nodes correspond to an invalid answer.", "labels": [], "entities": []}, {"text": "Our definition of the Answer Map follows the methodology of inverse index for search: instead of taking queries and considering all valid answers for it from a set of text, we take a text (answer) and consider the totality of valid and invalid queries consisting of the keywords from this text.", "labels": [], "entities": []}, {"text": "Usually, the main clause of a compound query includes the main entity and some of its constraints, and the supplementary clause includes the other constraint.", "labels": [], "entities": []}, {"text": "In the most straightforward way, the main clause of a query is mapped into a nucleus and the supplementary clause is mapped into a satellite of RST relation such as elaboration.", "labels": [], "entities": []}, {"text": "Connection by other RST relation, where a satellite introduces additional constraints fora nucleus, has the same meaning for answer validity.", "labels": [], "entities": [{"text": "answer validity", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.8124997615814209}]}, {"text": "This validity still holds when two EDUs are connected with asymmetric relation such as joint.", "labels": [], "entities": [{"text": "validity", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9618610739707947}]}, {"text": "However, when the images of the main and supplementary clause of the query are satellites of different nucleus, it most likely means that they express constraints for different entities, and therefore constitute an irrelevant answer for this query.", "labels": [], "entities": []}, {"text": "There is a number of recent studies employing RST features for passage re-ranking under question answering).", "labels": [], "entities": [{"text": "RST", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.954176127910614}, {"text": "passage re-ranking", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.9313874542713165}, {"text": "question answering", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7712038457393646}]}, {"text": "In the former study, the feature space of subtrees of parse trees includes the RST relations to improve question answer accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9285403490066528}]}, {"text": "In the latter project, RST features contributed to the totality of features learned to rerank the answers.", "labels": [], "entities": [{"text": "RST", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9644086956977844}]}, {"text": "In () rhetoric structure, in particular, was used to broaden the set of parse trees to enrich the feature space by taking into account overall discourse structure of candidate answers.", "labels": [], "entities": []}, {"text": "Statistical learning in these studies demonstrated that rhetoric relation can be leveraged for better search relevance.", "labels": [], "entities": [{"text": "rhetoric relation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9408327341079712}]}, {"text": "In the current study, we formulate the explicit rules for how a query can be mapped into the answer DT and the relevance of this map can be verified.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the TREC evaluation dataset as a list of topics: http://trec.nist.gov/data/qa/.", "labels": [], "entities": [{"text": "TREC evaluation dataset", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.7959617177645365}]}, {"text": "Given a short factoid question for entity, person, organization, event, etc.", "labels": [], "entities": []}, {"text": "such as #EVENT Pakistan earthquakes of October 2005# we ran a web search and automatically (using shallow parsing provided by Stanford NLP) extracted compound sentences from search expressions, such as A massive earthquake struck Pakistan and parts of India and Afghanistan on Saturday morning.", "labels": [], "entities": [{"text": "EVENT", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.8851456046104431}]}, {"text": "This was the strongest earthquake in the area during the last hundred years.", "labels": [], "entities": []}, {"text": "Ten to twenty such queries were derived fora topic.", "labels": [], "entities": []}, {"text": "Those portions of text were selected with obvious rhetoric relation between the clauses.", "labels": [], "entities": []}, {"text": "We then fed Bing Search Engine API such queries and built the Answer Map for each candidate answer.", "labels": [], "entities": [{"text": "Bing Search Engine", "start_pos": 12, "end_pos": 30, "type": "DATASET", "confidence": 0.8811405897140503}]}, {"text": "We then ran the Answer Map -based filter.", "labels": [], "entities": []}, {"text": "Finally, we manually verify that these filtered answers are relevant to the initial questions and to the queries.", "labels": [], "entities": []}, {"text": "We evaluated improvement of search relevance for compound queries by applying the DT rules.", "labels": [], "entities": []}, {"text": "These rules provide Boolean decisions for candidate answers, but we compare them with score-based answer re-ranking based on ML of baseline SVM tree kernel), discourse-based SVM) and nearestneighbor Parse Thicket-based approach ( . The approach based on SVM tree kernel takes question-answer pairs (also from TREC dataset) and forms the positive set from the correct pairs and negative set from the incorrect pairs.", "labels": [], "entities": [{"text": "ML", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.822176456451416}, {"text": "SVM tree kernel", "start_pos": 140, "end_pos": 155, "type": "DATASET", "confidence": 0.7623511652151743}, {"text": "nearestneighbor Parse Thicket-based", "start_pos": 183, "end_pos": 218, "type": "METRIC", "confidence": 0.8110445539156595}, {"text": "TREC dataset", "start_pos": 309, "end_pos": 321, "type": "DATASET", "confidence": 0.8440491557121277}]}, {"text": "The tree kernel learning () for the pairs of extended parse trees produces multiple parse trees for each sentence, linking them by discourse relations of anaphora, communicative actions, \"same entity\" relation and rhetoric relations ().", "labels": [], "entities": []}, {"text": "In the Nearest Neighbor approach to question -answer classification one takes the same data of parse trees connected by discourse relations and instead of applying SVM learning to pairs, compare these data for question and answer directly, finding the highest similarity.", "labels": [], "entities": [{"text": "question -answer classification", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7795666605234146}]}, {"text": "To compare the score-based answer re-ranking approaches with the rule-based answer filtering one, we took first 20 Bing answers and classified them as valid (top 10) and invalid (bottom 10) under the former set of approaches and selected up to 10 acceptable (using the original ranking) under the latter approach.", "labels": [], "entities": []}, {"text": "Hence the order of these selected set of 10 answers is irrelevant for our evaluation and we measured the percentage of valid answers among them (the focus of evaluation is search precision, not recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.8581705093383789}, {"text": "recall", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.9984416365623474}]}, {"text": "Answer validity was assessed by Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "validity", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.6600226759910583}, {"text": "Amazon Mechanical Turk", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.9520966211954752}]}, {"text": "The assessors were asked to choose relevant answers from the randomly sorted list of candidate answers.", "labels": [], "entities": []}, {"text": "The top two rows show the answer filtering methods and sources of discourse information.", "labels": [], "entities": [{"text": "answer filtering", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7736013233661652}]}, {"text": "Bottom rows show evaluation results for queries with various rhetoric relations between clauses.", "labels": [], "entities": []}, {"text": "One can observe just a 1.5% improvement by using SVM tree kernel without discourse, further 3.5% improvement by using discourse-enabled SVM tree kernel, and further improvement of 2.8% by using nearest neighbor learning.", "labels": [], "entities": []}, {"text": "The latter is still 4% lower than the Answer Map approach, which is the focus of this study.", "labels": [], "entities": [{"text": "Answer Map", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.6311212480068207}]}, {"text": "We observe that the baseline search improvement, SVM tree kernel approach has a limited capability of filtering out irrelevant search results in our evaluation settings.", "labels": [], "entities": []}, {"text": "Also, the role of discourse information in improving search results for queries with symmetric rhetoric relation between clauses is lower than that of the anti-symmetric relations.", "labels": [], "entities": []}, {"text": "Code and examples are available at code.google.com/p/relevance-based-on-parsetrees/ (package opennlp.tools.parse_thicket.external_rst).", "labels": [], "entities": []}], "tableCaptions": []}