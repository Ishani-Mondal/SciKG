{"title": [{"text": "Coupled Sequence Labeling on Heterogeneous Annotations: POS Tagging as a Case Study", "labels": [], "entities": [{"text": "Coupled Sequence Labeling", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.67828369140625}, {"text": "POS Tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8240711987018585}]}], "abstractContent": [{"text": "In order to effectively utilize multiple datasets with heterogeneous annotations, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, and to facilitate discussion we use Chinese part-of-speech (POS) tagging as our case study.", "labels": [], "entities": [{"text": "Chinese part-of-speech (POS) tagging", "start_pos": 252, "end_pos": 288, "type": "TASK", "confidence": 0.58507239818573}]}, {"text": "The key idea is to bundle two sets of POS tags together (e.g. \"[NN, n]\"), and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags with the help of ambiguous labelings.", "labels": [], "entities": []}, {"text": "To train our model on two non-overlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by considering all possible mappings at the missing side and derive an objective function based on ambiguous labelings.", "labels": [], "entities": []}, {"text": "The key advantage of our coupled model is to provide us with the flexibility of 1) incorporating joint features on the bundled tags to implicitly learn the loose mapping between heterogeneous annotations, and 2) exploring separate features on one-side tags to overcome the data sparseness problem of using only bundled tags.", "labels": [], "entities": []}, {"text": "Experiments on benchmark datasets show that our coupled model significantly outperforms the state-of-the-art baselines on both one-side POS tagging and annotation conversion tasks.", "labels": [], "entities": [{"text": "POS tagging and annotation conversion", "start_pos": 136, "end_pos": 173, "type": "TASK", "confidence": 0.7211234927177429}]}, {"text": "The codes and newly annotated data are released for non-commercial usage.", "labels": [], "entities": []}], "introductionContent": [{"text": "The scale of available labeled data significantly affects the performance of statistical data-driven models.", "labels": [], "entities": []}, {"text": "As a widely-used structural classification problem, sequence labeling is prone to suffer from the data sparseness issue.", "labels": [], "entities": [{"text": "structural classification", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.7103241831064224}, {"text": "sequence labeling", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7900945544242859}]}, {"text": "However, the heavy cost of manual annotation typically limits one labeled resource in both scale and genre.", "labels": [], "entities": []}, {"text": "As a promising research line, semi-supervised learning for sequence labeling has been extensively studied.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.5809665620326996}]}, {"text": "show that standard self-training can boost the performance of a simple hidden Markov model (HMM) based part-of-speech (POS) tagger.", "labels": [], "entities": []}, {"text": "S\u00f8gaard (2011) apply tri-training to English POS tagging, boosting accuracy from 97.27% to 97.50%.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.7822172939777374}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.999484658241272}]}, {"text": "Sun and Uszkoreit (2012) derive word clusters from largescale unlabeled data as extra features for Chinese POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.6477447897195816}]}, {"text": "Recently, the use of natural annotation has becomes a hot topic in Chinese word segmentation).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6485464175542196}]}, {"text": "The idea is to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models.", "labels": [], "entities": [{"text": "segmentation boundaries from implicit information encoded in web texts", "start_pos": 22, "end_pos": 92, "type": "TASK", "confidence": 0.7724940843052335}]}, {"text": "The existence of multiple annotated resources opens another door for alleviating data sparseness.", "labels": [], "entities": []}, {"text": "For example, Penn Chinese Treebank (CTB) contains about 20 thousand sentences annotated with word boundaries, POS tags, and syntactic structures (), which is widely used for research on Chinese word segmentation and POS tagging.", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 13, "end_pos": 40, "type": "DATASET", "confidence": 0.9719816644986471}, {"text": "Chinese word segmentation", "start_pos": 186, "end_pos": 211, "type": "TASK", "confidence": 0.6471844613552094}, {"text": "POS tagging", "start_pos": 216, "end_pos": 227, "type": "TASK", "confidence": 0.8491712808609009}]}, {"text": "People's Daily corpus (PD): An example to illustrate the annotation differences between CTB (above) and PD (below), and how to transform a one-side tag into a set of bundled tags.", "labels": [], "entities": [{"text": "People's Daily corpus (PD)", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.9374184693608966}]}, {"text": "\"NN\" and \"n\" represent nouns; \"VV\"and \"v\" represent verbs.", "labels": [], "entities": []}, {"text": "Daily newspaper (see).", "labels": [], "entities": [{"text": "Daily newspaper", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9736077785491943}]}, {"text": "The two resources were independently built for different purposes.", "labels": [], "entities": []}, {"text": "CTB was designed to serve syntactic analysis, whereas PD was developed to support information extraction systems.", "labels": [], "entities": [{"text": "CTB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9624682068824768}, {"text": "syntactic analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7374393343925476}, {"text": "information extraction", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.79213547706604}]}, {"text": "However, the key challenge of exploiting the two resources is that they adopt different sets of POS tags which are impossible to be precisely converted from one to another based on heuristic rules.", "labels": [], "entities": []}, {"text": "shows two example sentences from CTB and PD.", "labels": [], "entities": []}, {"text": "Please refer to.3 in Xia (2000) for detailed comparison of the two guidelines.", "labels": [], "entities": [{"text": "Xia (2000)", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.799536406993866}]}, {"text": "Previous work on exploiting heterogeneous data (CTB and PD) mainly focuses on indirect guidefeature based methods.", "labels": [], "entities": []}, {"text": "The basic idea is to use one resource to generate extra guide features on another resource (, which is similar to stacked learning.", "labels": [], "entities": []}, {"text": "First, PD is used as source data to train a source model Tagger PD . Then, Tagger PD generates automatic POS tags on the target data CTB, called source annotations.", "labels": [], "entities": []}, {"text": "Finally, a target model Tagger CTB-guided is trained on CTB, using source annotations as extra guide features.", "labels": [], "entities": [{"text": "CTB", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8941996693611145}]}, {"text": "Although the guide-feature based method is effective in boosting performance of the target model, we argue that it may have two potential drawbacks.", "labels": [], "entities": []}, {"text": "First, the target model Tagger CTB-guided does not directly use PD as training data, and therefore fails to make full use of rich language phenomena in PD.", "labels": [], "entities": []}, {"text": "Second, the method is more complicated in real applications since it needs to parse a test sentence twice to get the final results.", "labels": [], "entities": []}, {"text": "This paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously.", "labels": [], "entities": []}, {"text": "We use Chinese part-of-speech (POS) tagging as our case study.", "labels": [], "entities": [{"text": "Chinese part-of-speech (POS) tagging", "start_pos": 7, "end_pos": 43, "type": "TASK", "confidence": 0.5619112700223923}]}, {"text": "The key idea is to bundle two sets of POS tags together (e.g. \" n]\"), and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags.", "labels": [], "entities": []}, {"text": "To make use of two non-overlapping datasets that each has only one-side tags, we transform a oneside tag into a set of bundled tags by considering all possible mappings at the missing side and derive an objective function based on ambiguous labelings.", "labels": [], "entities": []}, {"text": "During training, the CRF-based coupled model is supervised by such ambiguous labelings.", "labels": [], "entities": []}, {"text": "The advantages of our coupled model are to provide us the flexibility of 1) incorporating joint features on the bundled tags to implicitly learn the loose mapping between two sets of annotations, and 2) exploring separate features on one-side tags to overcome the data sparseness problem of using bundled tags.", "labels": [], "entities": []}, {"text": "In summary, this work makes two major contributions: 1.", "labels": [], "entities": []}, {"text": "We propose a coupled model which can more effectively make use of multiple resources with heterogeneous annotations, compared with both the baseline and guide-feature based method.", "labels": [], "entities": []}, {"text": "Experiments show our approach can significantly improve POS tagging accuracy from 94.10% to 95.00% on CTB.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8711100518703461}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9722460508346558}, {"text": "CTB", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9732313752174377}]}, {"text": "2. We have manually annotated CTB tags for 1, 000 PD sentences, which is the first dataset with two-side annotations and can be used for annotation-conversion evaluation.", "labels": [], "entities": []}, {"text": "Experiments on the newly annotated data show that our coupled model also works effectively on the annotation conversion task, improving conversion accuracy from 90.59% to 93.90% (+3.31%).", "labels": [], "entities": [{"text": "annotation conversion task", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.773745059967041}, {"text": "conversion", "start_pos": 136, "end_pos": 146, "type": "TASK", "confidence": 0.9244864583015442}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9669008851051331}]}], "datasetContent": [{"text": "We adopt stochastic gradient descent (SGD) to iteratively learn \u03b8 for our baseline and coupled models.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.7789392669995626}]}, {"text": "However, we have two separate training data, and CTB maybe overwhelmed by PD if directly merging the two datasets into one, since PD is 15 times larger than CTB (see), Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where D bk is a subset of training data used ink th step update; b is the batch size; \u03b7 k is a update step.", "labels": [], "entities": []}, {"text": "The idea is to randomly sample instances from each training data in a certain proportion before each iteration.", "labels": [], "entities": []}, {"text": "The sampled data is then used for one-iteration training.", "labels": [], "entities": []}, {"text": "Later experiments will investigate the effect of the weighting proportion.", "labels": [], "entities": []}, {"text": "In this work, we use b = 30, and follow the implementation in CRFsuite to decide \u03b7 k .  In this section, we conduct experiments to verify the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "We adopt CTB (version 5.1) with the standard data split, and randomly split PD into four sets, among which one set is 20% partially annotated with CTB tags.", "labels": [], "entities": []}, {"text": "The data statistics is shown in.", "labels": [], "entities": []}, {"text": "The main concern of this work is to improve accuracy on CTB by exploring large-scale PD, since CTB is relatively small, but is widely-used benchmark data in the research community.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9982971549034119}]}, {"text": "We use the standard token-wise tagging accuracy as the evaluation metric.", "labels": [], "entities": [{"text": "token-wise tagging", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.5347943902015686}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.6556149125099182}]}, {"text": "For significance test, we adopt Dan Bikel's randomized parsing evaluation comparator.", "labels": [], "entities": []}, {"text": "The baseline CRF is trained on either CTB training data with 33 tags, or PD training data with 38 tags.", "labels": [], "entities": [{"text": "CTB training data", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.8652029633522034}, {"text": "PD training data", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.644461194674174}]}, {"text": "The coupled CRF is trained on both two separate training datasets with bundled tags (179 tags for the relaxed mapping function).", "labels": [], "entities": []}, {"text": "During evaluation, the coupled CRF is not directly evaluated on bundled tags, since bundled tags are unavailable in either CTB or PD test data.", "labels": [], "entities": [{"text": "CTB or PD test data", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.7918477654457092}]}, {"text": "Instead, the coupled and baseline CRFs are both evaluated on one-side tags.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data statistics. Please kindly note that the 1, 000 sentences originally from PD are only partially  annotated with CTB tags (about 20% most ambiguous tokens).", "labels": [], "entities": []}, {"text": " Table 3: Final results on CTB test data.  \u2020  means the corresponding approach significantly  outperforms the baseline at confidence level of  p < 10 \u22125 ; whereas  \u2021 means the accuracy", "labels": [], "entities": [{"text": "CTB test data", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.91115270058314}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.998976469039917}]}, {"text": " Table 4: Accuracy on CTB: feature study.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9965984225273132}, {"text": "CTB", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.6525697112083435}]}, {"text": " Table 6: Accuracy on CTB: using converted PD.   \u2020 means the corresponding approach significantly  outperforms the baseline at confidence level of  p < 10 \u22125 ; whereas  \u2021 means the accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989619255065918}, {"text": "CTB", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.6648780703544617}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9990338087081909}]}]}