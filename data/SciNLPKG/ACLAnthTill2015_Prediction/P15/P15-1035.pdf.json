{"title": [{"text": "Joint Information Extraction and Reasoning: A Scalable Statistical Relational Learning Approach", "labels": [], "entities": [{"text": "Information Extraction and Reasoning", "start_pos": 6, "end_pos": 42, "type": "TASK", "confidence": 0.7674481719732285}, {"text": "Statistical Relational Learning Approach", "start_pos": 55, "end_pos": 95, "type": "TASK", "confidence": 0.6835752502083778}]}], "abstractContent": [{"text": "A standard pipeline for statistical rela-tional learning involves two steps: one first constructs the knowledge base (KB) from text, and then performs the learning and reasoning tasks using probabilis-tic first-order logics.", "labels": [], "entities": [{"text": "statistical rela-tional learning", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6893280545870463}]}, {"text": "However, a key issue is that information extraction (IE) errors from text affect the quality of the KB, and propagate to the reasoning task.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.829179972410202}]}, {"text": "In this paper, we propose a statistical rela-tional learning model for joint information extraction and reasoning.", "labels": [], "entities": [{"text": "joint information extraction", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6253684461116791}]}, {"text": "More specifically , we incorporate context-based entity extraction with structure learning (SL) in a scalable probabilistic logic framework.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.695235475897789}]}, {"text": "We then propose a latent context invention (LCI) approach to improve the performance.", "labels": [], "entities": []}, {"text": "In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL significantly improve both tasks; that latent context invention further improves the results .", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering), machine translation (, or other applications ().", "labels": [], "entities": [{"text": "Information extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8737254500389099}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8290697634220123}, {"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7862516343593597}]}, {"text": "Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (), a task sometimes called KB completion).", "labels": [], "entities": []}, {"text": "Pipelines of this sort frequently suffer from error cascades, which reduces performance of the full system . In this paper, we address this issue, and propose a joint model system for IE and KB completion in a statistical relational learning (SRL) setting).", "labels": [], "entities": [{"text": "IE and KB completion", "start_pos": 184, "end_pos": 204, "type": "TASK", "confidence": 0.6779702380299568}, {"text": "statistical relational learning (SRL)", "start_pos": 210, "end_pos": 247, "type": "TASK", "confidence": 0.6256610502799352}]}, {"text": "In particular, we outline a system which takes as input a partially-populated KB and a set of relation mentions in context, and jointly learns: 1) how to extract new KB facts from the relation mentions, and; 2) a set of logical rules that allow one to infer new KB facts.", "labels": [], "entities": []}, {"text": "Evaluation of the KB facts inferred by the joint system shows that the joint model outperforms its individual components.", "labels": [], "entities": []}, {"text": "We also introduce a novel extension of this model called Latent Context Invention (LCI), which associates latent states with context features for the IE component of the model.", "labels": [], "entities": [{"text": "Latent Context Invention (LCI)", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.7278614391883215}]}, {"text": "We show that LCI further improves performance, leading to a substantial improvement over prior state-of-the-art methods for joint relation-learning and IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 152, "end_pos": 154, "type": "TASK", "confidence": 0.9783821105957031}]}, {"text": "To summarize our contributions: \u2022 We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (), a state-of-theart joint method; \u2022 We incorporate latent context into the joint SRL model, bringing additional improvements.", "labels": [], "entities": [{"text": "IE", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9727395176887512}]}, {"text": "In next section, we discuss related work.", "labels": [], "entities": []}, {"text": "We describe our approach in Section 3.", "labels": [], "entities": []}, {"text": "The details of the datasets are introduced in Section 4.", "labels": [], "entities": []}, {"text": "We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the data generation process that we described in subsection 3.2, we extract two datasets from the supercategories of \"European royal families\" and \"American people of English descent, and third geographic dataset using three lists: \"List of countries by population\", \"List of largest cities and second largest cities by country\" and \"List of national capitals by population\".", "labels": [], "entities": []}, {"text": "For the royal dataset, we have 2,258 pages with 67,483 source-context-target mentions, and we use 40,000 for training, and 27,483 for testing.", "labels": [], "entities": [{"text": "royal dataset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9252835810184479}]}, {"text": "There are 15 relations 7 . In the American dataset, we have 679 pages with 11,726 mentions, and we use 7,000 for training, and 4,726 for testing.", "labels": [], "entities": [{"text": "American dataset", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9855253100395203}]}, {"text": "This dataset includes 30 relations 8 . As for the Geo dataset, there are 497 To give some background on this nomenclature, we note that the SL method is inspired by Cropper and Muggleton's Metagol system, which includes predicate invention.", "labels": [], "entities": [{"text": "Geo dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9357806444168091}]}, {"text": "In principle predicates could be invented by SL, by extending the interpreter to consider \"invented\" predicate symbols as binding to its template variables (e.g., P and R); however, in practice invented predicates leads to close dependencies between learned rules, and are highly sensitive to the level of noise in the data.", "labels": [], "entities": []}, {"text": "birthPlace, child, commander, deathPlace, keyPerson, knownFor, monarch, parent, partner, predecessor, relation, restingPlace, spouse, successor, territory 8 architect, associatedBand, associatedMusicalArtist, au-pages with 43,475 mentions, and we use 30,000 for training, and 13,375 for testing.", "labels": [], "entities": []}, {"text": "There are 10 relations . The datasets are freely available for download at http://www.cs.cmu.edu/ \u02dc yww/data/jointIE+Reason.zip.", "labels": [], "entities": []}, {"text": "To evaluate these methods, we use the setting of Knowledge Base completion ().", "labels": [], "entities": []}, {"text": "We randomly remove a fixed percentage of facts in a training knowledge base, train the learner from the partial KB, and use the learned model to predict facts in the test KB.", "labels": [], "entities": []}, {"text": "KB completion is a wellstudied task in SRL, where multiple relations are often needed to fill in missing facts, and thus reconstruct the incomplete KB.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8486785888671875}, {"text": "SRL", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.964489758014679}]}, {"text": "Following prior work (), we use mean average precision (MAP) as the evaluation metric.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 32, "end_pos": 60, "type": "METRIC", "confidence": 0.9504776895046234}]}], "tableCaptions": [{"text": " Table 3: The MAP results for KB completion on three datasets. U: unigram. B: bigram. Best result in  each column is highlighted in bold.", "labels": [], "entities": [{"text": "MAP", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.5640731453895569}, {"text": "KB completion", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.7417972683906555}]}]}