{"title": [{"text": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "labels": [], "entities": [{"text": "Transition-Based Dependency Parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5287471910317739}]}], "abstractContent": [{"text": "We propose a technique for learning representations of parser states in transition-based dependency parsers.", "labels": [], "entities": []}, {"text": "Our primary innovation is anew control structure for sequence-to-sequence neural networks-the stack LSTM.", "labels": [], "entities": []}, {"text": "Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack inconstant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents.", "labels": [], "entities": []}, {"text": "This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures.", "labels": [], "entities": []}, {"text": "Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures).", "labels": [], "entities": [{"text": "Transition-based dependency parsing formalizes the parsing problem", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.7022797593048641}]}, {"text": "This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph-and grammarbased formalisms.", "labels": [], "entities": []}, {"text": "The challenge in transitionbased parsing is modeling which action should betaken in each of the unboundedly many states encountered as the parser progresses.", "labels": [], "entities": []}, {"text": "This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment decisions, through feature engineering () and more recently using neural networks).", "labels": [], "entities": []}, {"text": "We extend this last line of work by learning representations of the parser state that are sensitive to the complete contents of the parser's state: that is, the complete input buffer, the complete history of parser actions, and the complete contents of the stack of partially constructed syntactic structures.", "labels": [], "entities": []}, {"text": "This \"global\" sensitivity to the state contrasts with previous work in transitionbased dependency parsing that uses only a narrow view of the parsing state when constructing representations (e.g., just the next few incoming words, the head words of the top few positions in the stack, etc.).", "labels": [], "entities": [{"text": "transitionbased dependency parsing", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.6558626393477122}]}, {"text": "Although our parser integrates large amounts of information, the representation used for prediction at each time step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence.", "labels": [], "entities": []}, {"text": "The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs ( \u00a72), and which support both reading (pushing) and \"forgetting\" (popping) inputs.", "labels": [], "entities": []}, {"text": "Our parsing model uses three stack LSTMs: one representing the input, one representing the stack of partial syntactic trees, and one representing the history of parse actions to encode parser states ( \u00a73).", "labels": [], "entities": []}, {"text": "Since the stack of partial syntactic trees may contain both individual tokens and partial syntactic structures, representations of individual tree fragments are computed compositionally with recursive (i.e., similar to) neural networks.", "labels": [], "entities": []}, {"text": "The parameters are learned with backpropagation ( \u00a74), and we obtain state-of-the-art results on Chinese and English dependency parsing tasks ( \u00a75).", "labels": [], "entities": [{"text": "Chinese and English dependency parsing tasks", "start_pos": 97, "end_pos": 141, "type": "TASK", "confidence": 0.5989256252845129}]}], "datasetContent": [{"text": "We applied our parsing model and several variations of it to two parsing tasks and report results below.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9747869372367859}]}, {"text": "We report results on five experimental configurations per language, as well as the Chen and Manning (2014) baseline.", "labels": [], "entities": []}, {"text": "These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (\u2212POS), the stack LSTM parsing model without pretrained language model embeddings (\u2212pretraining), the stack LSTM parsing model that uses just head words on the stack instead of composed representations (\u2212composition), and the full parsing model where rather than an LSTM, a classical recurrent neural network is used (S-RNN).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English parsing results (SD)", "labels": [], "entities": [{"text": "English parsing", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6590251326560974}]}, {"text": " Table 2: Chinese parsing results (CTB5)", "labels": [], "entities": [{"text": "Chinese parsing", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.645369365811348}]}]}