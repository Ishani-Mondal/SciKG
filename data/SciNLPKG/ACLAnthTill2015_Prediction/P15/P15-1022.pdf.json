{"title": [{"text": "Online Multitask Learning for Machine Translation Quality Estimation", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.8464848846197128}]}], "abstractContent": [{"text": "We present a method for predicting machine translation output quality geared to the needs of computer-assisted translation.", "labels": [], "entities": [{"text": "predicting machine translation output", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8399999141693115}, {"text": "computer-assisted translation", "start_pos": 93, "end_pos": 122, "type": "TASK", "confidence": 0.7231225967407227}]}, {"text": "These include the capability to: i) continuously learn and self-adapt to a stream of data coming from multiple translation jobs, ii) react to data diversity by exploiting human feedback, and iii) leverage data similarity by learning and transferring knowledge across domains.", "labels": [], "entities": []}, {"text": "To achieve these goals, we combine two supervised machine learning paradigms, online and multitask learning, adapting and unifying them in a single framework.", "labels": [], "entities": []}, {"text": "We show the effectiveness of our approach in a regression task (HTER prediction), in which online multitask learning outperforms the competitive online single-task and pooling methods used for comparison.", "labels": [], "entities": [{"text": "HTER prediction)", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7371102273464203}]}, {"text": "This indicates the feasibility of integrating in a CAT tool a single QE component capable to simultaneously serve (and continuously learn from) multiple translation jobs involving different domains and users.", "labels": [], "entities": []}], "introductionContent": [{"text": "Even if not perfect, machine translation (MT) is now getting reliable enough to support and speedup human translation.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8496447205543518}, {"text": "human translation", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.6984779834747314}]}, {"text": "Thanks to this progress, the work of professional translators is gradually shifting from full translation from scratch to MT post-editing.", "labels": [], "entities": [{"text": "MT post-editing", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.8921488523483276}]}, {"text": "Advanced computer-assisted translation (CAT) tools 1 provide a natural framework for this activity by proposing, for each segment in a source document, one or more suggestions obtained either from a translation memory (TM) or from an MT engine.", "labels": [], "entities": [{"text": "computer-assisted translation (CAT)", "start_pos": 9, "end_pos": 44, "type": "TASK", "confidence": 0.8699893116950989}]}, {"text": "In both cases, accurate mechanisms to indicate the reliability of a suggestion are extremely useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch.", "labels": [], "entities": []}, {"text": "However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue.", "labels": [], "entities": [{"text": "TM matches", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.8362325727939606}, {"text": "MT suggestions", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.8556622266769409}]}, {"text": "This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (.", "labels": [], "entities": [{"text": "MT quality estimation (QE)", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8699520428975424}]}, {"text": "So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors.", "labels": [], "entities": [{"text": "QE", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9549655318260193}]}, {"text": "Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: 1.", "labels": [], "entities": []}, {"text": "The QE module is exposed to a continuous stream of data.", "labels": [], "entities": []}, {"text": "The amount of such data and the tight schedule of multiple, simultaneous translation jobs prevents from (theoretically feasible but impractical) complete re-training procedures in a batch fashion and advocate for continuous learning methods.", "labels": [], "entities": []}, {"text": "2. The input data can be diverse in nature.", "labels": [], "entities": []}, {"text": "Continuous learning should be sensitive to such differences, in away that each translation job and user is supported by a reactive model that is robust to variable working conditions.", "labels": [], "entities": []}, {"text": "3. The input data can show similarities with previous observations.", "labels": [], "entities": []}, {"text": "Continuous learning should leverage such similarities, so that QE can capitalize from all the previously processed segments even if they come from different domains, genres or users.", "labels": [], "entities": []}, {"text": "While previous QE research disregarded these challenges or addressed them in isolation, our work tackles them in a single unifying framework based on the combination of two paradigms: online and multitask learning.", "labels": [], "entities": []}, {"text": "The former provides continuous learning capabilities that allow the QE model to be robust and self-adapt to a stream of potentially diverse data.", "labels": [], "entities": []}, {"text": "The latter provides the model with the capability to exploit the similarities between data coming from different sources.", "labels": [], "entities": []}, {"text": "Along this direction our contributions are: \u2022 The first application of online multitask learning to QE, geared to the challenges posed by CAT technology.", "labels": [], "entities": []}, {"text": "In this framework, our models are trained to predict MT quality in terms of HTER ().", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9911212921142578}, {"text": "HTER", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.6612050533294678}]}, {"text": "2 \u2022 The extension of current online multitask learning methods to regression.", "labels": [], "entities": []}, {"text": "Prior works in the machine learning field applied this paradigm to classification problems, but its use for HTER estimation requires real-valued predictions.", "labels": [], "entities": [{"text": "HTER estimation", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.9009660482406616}]}, {"text": "To this aim, we propose anew regression algorithm that, at the same time, handles positive and negative transfer and performs online weight updates.", "labels": [], "entities": []}, {"text": "\u2022 A comparison between online multitask and alternative, state-of-the-art online learning strategies.", "labels": [], "entities": []}, {"text": "Our experiments, carried out in a realistic scenario involving a stream of data from four domains, lead to consistent results that prove the effectiveness of our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the data used in our experiments, the features extracted from the source and target sentences, the evaluation metric and the baselines used for comparison.", "labels": [], "entities": []}, {"text": "We experiment with English-French datasets coming from Technology Entertainment Design talks (TED), Information Technology manuals (IT) and Education Material (EM).", "labels": [], "entities": [{"text": "Technology Entertainment Design talks (TED)", "start_pos": 55, "end_pos": 98, "type": "TASK", "confidence": 0.6251321051801954}]}, {"text": "All datasets provide a set of tuples composed by (source, translation and post-edited translation).", "labels": [], "entities": []}, {"text": "The TED dataset is distributed in the Trace corpus and includes, as source sentences, the subtitles of several talks spanning a range of topics presented in the TED conferences.", "labels": [], "entities": [{"text": "TED dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9053687751293182}, {"text": "Trace corpus", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.8794773519039154}, {"text": "TED conferences", "start_pos": 161, "end_pos": 176, "type": "DATASET", "confidence": 0.8456150889396667}]}, {"text": "Translations were generated by two different MT systems: a phrase-based statistical MT system and a commercial rule-based system.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9806308150291443}, {"text": "phrase-based statistical MT", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.4801315764586131}]}, {"text": "Post-editions were collected from four different translators, as described by.", "labels": [], "entities": []}, {"text": "The IT manuals data come from two language service providers, henceforth LSP 1 and LSP 2.", "labels": [], "entities": [{"text": "IT manuals data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.735226035118103}]}, {"text": "The IT LSP 1 tuples belong to a software manual translated by an SMT system trained using the Moses toolkit (.", "labels": [], "entities": [{"text": "IT LSP 1 tuples", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.875017985701561}, {"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.920896589756012}]}, {"text": "The posteditions were produced by one professional trans- In total, we end up with four domains (TED, IT LSP 1 , EM and IT LSP 2 ), which allows us to evaluate the PAMTL algorithm in realistic conditions where the QE component is exposed to a continuous stream of heterogeneous data.", "labels": [], "entities": [{"text": "posteditions", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9670193791389465}, {"text": "TED", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.7894510626792908}]}, {"text": "Each domain is composed by 1,000 tuples formed by: i) the English source sentence, ii) its automatic translation in French, and iii) a real-valued quality label obtained by computing the HTER between the translation and the post-edition with the TERCpp open source tool.", "labels": [], "entities": [{"text": "HTER", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.959450900554657}]}, {"text": "Table 1 reports some macro-indicators (number of tokens, vocabulary size, average sentence length) that give an idea about the similarities and differences between domains.", "labels": [], "entities": []}, {"text": "Although they contain data from different software manuals, similar vocabulary size and sentence lengths for the two IT domains seem to reflect some commonalities in their technical style and jargon.", "labels": [], "entities": []}, {"text": "Larger values for TED and EM evidence a higher lexical variability in the topics that compose these domains and the expected stylistic differences featured by speech transcriptions and non-technical writing.", "labels": [], "entities": []}, {"text": "Overall, these numbers suggest a possible dissimilar- ity between IT LSP 1 and IT LSP 2 and the other two domains, which might make knowledge transfer across them more difficult and QE model reactivity to domain changes particularly important.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7712607085704803}]}, {"text": "Our models are trained using the 17 baseline features proposed in (, extracted with the online version of the QuEst feature extractor).", "labels": [], "entities": []}, {"text": "These features take into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the translation (e.g. language model probabilities).", "labels": [], "entities": []}, {"text": "Their description is available in.", "labels": [], "entities": []}, {"text": "The results of previous WMT QE shared tasks have shown that these features are particularly competitive in the HTER prediction task.", "labels": [], "entities": [{"text": "WMT QE shared tasks", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8567446172237396}, {"text": "HTER prediction task", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.9295352101325989}]}, {"text": "We compare the performance of PAMTL against three baselines: i) pooling mean, ii) pooling online single task learning (STL pool ) and iii) in-domain online single task learning (STL in ).", "labels": [], "entities": []}, {"text": "The pooling mean is obtained by assigning a fixed prediction value to each test point.", "labels": [], "entities": []}, {"text": "This value is the average HTER computed on the entire pool of training data.", "labels": [], "entities": [{"text": "HTER", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.99549400806427}]}, {"text": "Although assigning the same prediction to each test instance would be useless in real applications, we compare against the mean baseline since it is often hard to beat in regression tasks, especially when dealing with heterogeneous data distributions (.", "labels": [], "entities": []}, {"text": "The two online single task baselines implement the PA algorithm described in Section 3.1.", "labels": [], "entities": [{"text": "PA", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.8344677686691284}]}, {"text": "The choice of PA is to make them comparable to our method, so that we can isolate more precisely the contribution of multitask learning.", "labels": [], "entities": []}, {"text": "STL pool results are obtained by a single model trained on the entire pool of available training data presented in random order.", "labels": [], "entities": []}, {"text": "STL in results are obtained by separately training one model for each domain.", "labels": [], "entities": [{"text": "STL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7019124627113342}]}, {"text": "These represent two alternative strategies for the integration of QE in the CAT framework.", "labels": [], "entities": []}, {"text": "The former would allow a single model to simultaneously support multiple translation jobs in different domains, without any notion about their relations.", "labels": [], "entities": []}, {"text": "The latter would lead to a more complex architecture, organized as a pool of independent, specialized QE modules.", "labels": [], "entities": []}, {"text": "The performance of our regression models is evaluated in terms of mean absolute error (MAE), a standard error measure for regression problems commonly used also for QE).", "labels": [], "entities": [{"text": "mean absolute error (MAE)", "start_pos": 66, "end_pos": 91, "type": "METRIC", "confidence": 0.9019668698310852}]}, {"text": "The MAE is the average of the absolute errors e i = | \u02c6 y i \u2212 y i |, wher\u00eawher\u00ea y i is the prediction of the model and y i is the true value for the i th instance.", "labels": [], "entities": [{"text": "MAE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9902352690696716}]}, {"text": "As it is an error measure, lower values indicate better performance (\u2193).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics for each domain.", "labels": [], "entities": []}]}