{"title": [{"text": "Revisiting Word Embedding for Contrasting Meaning", "labels": [], "entities": [{"text": "Revisiting Word Embedding", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7866990764935812}, {"text": "Contrasting Meaning", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7707378566265106}]}], "abstractContent": [{"text": "Contrasting meaning is a basic aspect of semantics.", "labels": [], "entities": []}, {"text": "Recent word-embedding models based on distributional semantics hypothesis are known to be weak for mod-eling lexical contrast.", "labels": [], "entities": []}, {"text": "We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE \"most contrasting word\" questions (Mohammad et al., 2008).", "labels": [], "entities": [{"text": "F-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9993458390235901}, {"text": "GRE", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.7360188961029053}]}, {"text": "This is the highest performance seen so far on this dataset.", "labels": [], "entities": []}, {"text": "Surprisingly at the first glance, unlike what was suggested inmost previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget's and WordNet)-corpora statistics did not lead to further improvement.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 268, "end_pos": 275, "type": "DATASET", "confidence": 0.9235490560531616}]}, {"text": "However, this should not be simply taken as that distributional statistics is not useful.", "labels": [], "entities": []}, {"text": "We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning good representations of meaning for different granularities of texts is core to human language understanding, where a basic problem is representing the meanings of words.", "labels": [], "entities": [{"text": "human language understanding", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.7162043253580729}]}, {"text": "Distributed representations learned with neural networks have recently showed to result in significant improvement of performance on a number of language understanding problems (e.g., speech recognition and automatic machine translation) and on many non-language problems (e.g., image recognition).", "labels": [], "entities": [{"text": "language understanding", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.7302129864692688}, {"text": "speech recognition", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7350685745477676}, {"text": "automatic machine translation)", "start_pos": 207, "end_pos": 237, "type": "TASK", "confidence": 0.7455607205629349}, {"text": "image recognition)", "start_pos": 279, "end_pos": 297, "type": "TASK", "confidence": 0.8824796875317892}]}, {"text": "Distributed representations have been leveraged to represent words as in).", "labels": [], "entities": []}, {"text": "Contrasting meaning is a basic aspect of semantics, but it is widely known that word embedding models based on distributional semantics hypothesis are weak in modeling this-contrasting meaning is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable.", "labels": [], "entities": []}, {"text": "Lexical contrast has been modeled in).", "labels": [], "entities": [{"text": "Lexical contrast", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8001140356063843}]}, {"text": "The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances.", "labels": [], "entities": []}, {"text": "For example, proposed to use polarity-primed latent semantic analysis (LSA), called PILSA, to capture contrast, which was further used to initialize a neural network and achieved an F-score of 81% on the same GRE \"most contrasting word\" questions ().", "labels": [], "entities": [{"text": "PILSA", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.8655392527580261}, {"text": "F-score", "start_pos": 182, "end_pos": 189, "type": "METRIC", "confidence": 0.9988512992858887}]}, {"text": "More recently, proposed a tensor factorization approach to solving the problem, resulting in a 82% F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9949524402618408}]}, {"text": "In this paper, we present embedding models that achieve an F-score of 92% on the GRE dataset, which outperforms the previous best result (82%) by a large margin.", "labels": [], "entities": [{"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9994043111801147}, {"text": "GRE dataset", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.9255355894565582}]}, {"text": "Unlike what was suggested in previous work, where relatedness statistics learned from corpora is often claimed to yield extra gains over lexicon-based models, we obtained this new state-of-the-art result relying solely on lexical resources (Roget's and WordNet), and corpus statistics does not seem to bring further improvement.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 253, "end_pos": 260, "type": "DATASET", "confidence": 0.9553197026252747}]}, {"text": "To provide a comprehensive understanding, we constructed our study in a framework that examines a number of basic concerns in modeling contrasting meaning.", "labels": [], "entities": []}, {"text": "We hope our efforts would help shed some light on future directions for this basic semantic modeling problem.", "labels": [], "entities": [{"text": "semantic modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.8325312733650208}]}], "datasetContent": [{"text": "Data Our experiment uses the \"most contrasting word\" questions collected by from Graduate Record Examination (GRE), which was originally created by Educational Testing Service (ETS).", "labels": [], "entities": [{"text": "from Graduate Record Examination (GRE)", "start_pos": 76, "end_pos": 114, "type": "DATASET", "confidence": 0.9314051611082894}, {"text": "Educational Testing Service (ETS)", "start_pos": 148, "end_pos": 181, "type": "DATASET", "confidence": 0.7694497058788935}]}, {"text": "Each GRE question has a target word and five candidate choices; the task is to identify among the choices the most contrasting word with regard to the given target word.", "labels": [], "entities": []}, {"text": "The dataset consists of a development set and a test set, with 162 and 950 questions, respectively.", "labels": [], "entities": []}, {"text": "As an example from, one of the questions has the target word adulterate and the five candidate choices: (A) renounce, (B) forbid, (C) purify, (D) criticize, and (E) correct.", "labels": [], "entities": []}, {"text": "While in this example the choice correct has a meaning that is contrasting with that of adulterate, the word purify is the gold answer as it has the greatest degree of contrast with adulterate.", "labels": [], "entities": [{"text": "purify", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9445657134056091}]}, {"text": "Lexical Resources In our work, we use two publicly available lexical resources, WordNet (Miller, 1995) (version 3.0) and the Roget's Thesaurus.", "labels": [], "entities": [{"text": "WordNet (Miller, 1995)", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.8868654867013296}]}, {"text": "We utilized the labeled antonym relations to obtain more contrasting pairs under the contrast hypothesis (, by assuming a contrasting pair is related to a pair of opposites (antonyms here).", "labels": [], "entities": []}, {"text": "Specifically in WordNet, we consider the word pairs with relations other than antonym as semantically close.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.940950334072113}]}, {"text": "In this way, we obtained a thesaurus containing 83,118 words, 494,579 contrasting pairs, and 368,209 close pairs.", "labels": [], "entities": []}, {"text": "Note that we did not only use synonyms to expand the contrasting pairs.", "labels": [], "entities": []}, {"text": "We will discuss how this affects the performance in the experiment section.", "labels": [], "entities": []}, {"text": "In the Roget's Thesaurus, every word or entry has its synonyms and/or antonyms.", "labels": [], "entities": [{"text": "Roget's Thesaurus", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.722978929678599}]}, {"text": "We obtained 35,717 antonym pairs and 346,619 synonym pairs, which consist of 43,409 word types.", "labels": [], "entities": []}, {"text": "The antonym and synonym pairs in Roget's were combined with contrasting pairs and semantically close pairs in WordNet, respectively.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9592555165290833}]}, {"text": "And in total, we have 92,339 word types, 520,734 antonym pairs, and 646,433 close pairs.", "labels": [], "entities": []}, {"text": "Google Billion-Word Corpus The corpus used in our experiment for modeling lexical relatedness in the CRM component was Google one billion word corpus ().", "labels": [], "entities": [{"text": "Google Billion-Word Corpus", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.8617724776268005}]}, {"text": "Normalization and tokenization were performed using the scripts distributed from https://code.google.com/p/1-billionword-language-modeling-benchmark/, and sentences were shuffled randomly.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9669724702835083}]}, {"text": "We computed embedding fora word if its count in the corpus is equal to or larger than five, with the method described in Section 3.4.", "labels": [], "entities": []}, {"text": "Words with counts lower than five were discarded.", "labels": [], "entities": []}, {"text": "Evaluation Metric Same as in previous work, the evaluation metric is F-score, where precision is the percentage of the questions answered correctly over the questions the models attempt to answer, and recall is the percentage of the questions that are answered correctly among all questions.", "labels": [], "entities": [{"text": "F-score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9988446235656738}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9991797804832458}, {"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9994020462036133}]}, {"text": "In training, we used stochastic gradient descent (SGD) to optimize the objective function, and the dimension of embedding was set to be 200.", "labels": [], "entities": []}, {"text": "In MCE (Equation 2 and 3) the margins \u03b1 and \u03b2 are both set to be 0.4.", "labels": [], "entities": [{"text": "MCE", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.7452035546302795}]}, {"text": "During testing, when using SCE or MCE embedding to answer the GRE questions, we directly calculated distances fora pair between a question word and a candidate choice in these two corresponding embedding spaces to report their performances.", "labels": [], "entities": [{"text": "GRE questions", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.6720420718193054}]}, {"text": "We also combined SCE/MCE with other components in the contrast inference layer, for which we used ten-fold cross validation to tune the weights of the top hidden layers on nine fold and test on the rest and repeated this for ten times to report the results.", "labels": [], "entities": []}, {"text": "As discussed above, errors were not backpropagated to modify word embedding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the GRE \"most contrasting words\" questions.", "labels": [], "entities": [{"text": "GRE \"most contrasting words\"", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6073660751183828}]}]}