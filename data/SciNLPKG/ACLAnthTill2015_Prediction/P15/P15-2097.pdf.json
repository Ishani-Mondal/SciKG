{"title": [{"text": "Ground Truth for Grammatical Error Correction Metrics", "labels": [], "entities": []}], "abstractContent": [{"text": "How do we know which grammatical error correction (GEC) system is best?", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.7325948278109232}]}, {"text": "A number of metrics have been proposed over the years, each motivated by weaknesses of previous metrics; however, the metrics themselves have not been compared to an empirical gold standard grounded inhuman judgments.", "labels": [], "entities": []}, {"text": "We conducted the first human evaluation of GEC system outputs, and show that the rankings produced by metrics such as MaxMatch and I-measure do not correlate well with this ground truth.", "labels": [], "entities": [{"text": "GEC", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.733301043510437}]}, {"text": "As a step towards better metrics, we also propose GLEU, a simple variant of BLEU, modified to account for both the source and the reference, and show that it hews much more closely to human judgments .", "labels": [], "entities": [{"text": "GLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9940369129180908}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9973551034927368}]}], "introductionContent": [{"text": "Automatic metrics area critical component for all tasks in natural language processing.", "labels": [], "entities": []}, {"text": "For many tasks, such as parsing and part-of-speech tagging, there is a single correct answer, and thus a single metric to compute it.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9809547662734985}, {"text": "part-of-speech tagging", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6943704932928085}]}, {"text": "For other tasks, such as machine translation or summarization, there is no effective limit to the size of the set of correct answers.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8489932715892792}, {"text": "summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9504226446151733}]}, {"text": "For such tasks, metrics proliferate and compete with each other for the role of the dominant metric.", "labels": [], "entities": []}, {"text": "In such cases, an important question to answer is by what means such metrics should be compared.", "labels": [], "entities": []}, {"text": "That is, what is the metric metric?", "labels": [], "entities": []}, {"text": "The answer is that it should be rooted in the end-use case for the task under consideration.", "labels": [], "entities": []}, {"text": "This could be some other metric further downstream of the task, or something simpler like direct human evaluation.", "labels": [], "entities": []}, {"text": "This latter approach is the one often taken in machine translation; for example, the organizers of the Workshop on Statistical Machine Translation have long argued that human evaluation is the ultimate ground truth, and have therefore conducted an extensive human evaluation to produce a system ranking, which is then used to compare metrics ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7597126662731171}, {"text": "Statistical Machine Translation", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.6143679916858673}]}, {"text": "Unfortunately, for the subjective task of grammatical error correction (GEC), no such ground truth has ever been established.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.7426852285861969}]}, {"text": "Instead, the rankings produced by new metrics are justified by their correlation with explicitly-corrected errors in one or more references, and by appeals to intuition for the resulting rankings.", "labels": [], "entities": []}, {"text": "However, arguably even more so than for machine translation, the use case for grammatical error correction is human consumption, and therefore, the ground truth ranking should be rooted inhuman judgments.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7324942052364349}, {"text": "grammatical error correction", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.6547288795312246}]}, {"text": "We establish aground truth for GEC by conducting a human evaluation and producing a human ranking of the systems entered into the CoNLL-2014 Shared Task on GEC.", "labels": [], "entities": [{"text": "CoNLL-2014 Shared Task on GEC", "start_pos": 130, "end_pos": 159, "type": "DATASET", "confidence": 0.7890665411949158}]}, {"text": "We find that existing GEC metrics correlate very poorly with the ranking produced by this human evaluation.", "labels": [], "entities": []}, {"text": "As a step in the direction of better metrics, we develop the Generalized Language Evaluation Understanding metric (GLEU) inspired by BLEU, which correlates much better with the human ranking than current GEC metrics.", "labels": [], "entities": [{"text": "Generalized Language Evaluation Understanding metric (GLEU)", "start_pos": 61, "end_pos": 120, "type": "METRIC", "confidence": 0.5821010172367096}, {"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9902880191802979}]}], "datasetContent": [], "tableCaptions": []}