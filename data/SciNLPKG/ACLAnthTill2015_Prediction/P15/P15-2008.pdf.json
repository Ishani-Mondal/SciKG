{"title": [{"text": "Document Classification by Inversion of Distributed Language Representations", "labels": [], "entities": [{"text": "Document Classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8918316662311554}]}], "abstractContent": [{"text": "There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition.", "labels": [], "entities": []}, {"text": "This vector-space is the distributed language representation.", "labels": [], "entities": []}, {"text": "The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule.", "labels": [], "entities": []}, {"text": "The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model.", "labels": [], "entities": []}, {"text": "In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.", "labels": [], "entities": [{"text": "Yelp reviews", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.925137996673584}]}], "introductionContent": [{"text": "Distributed, or vector-space, language representations V consist of a location, or embedding, for every vocabulary word in R K , where K is the dimension of the latent representation space.", "labels": [], "entities": []}, {"text": "These locations are learned to optimize, perhaps approximately, an objective function defined on the original text such as a likelihood for word occurrences.", "labels": [], "entities": []}, {"text": "A popular example is the Word2Vec machinery of.", "labels": [], "entities": [{"text": "Word2Vec machinery", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.9294437170028687}]}, {"text": "This trains the distributed representation to be useful as an input layer for prediction of words from their neighbors in a Skipgram likelihood.", "labels": [], "entities": []}, {"text": "That is, to maximize t+b j =t, j=t\u2212b log p V (w sj | w st ) summed across all words w st in all sentences w s , where b is the skip-gram window (truncated by the ends of the sentence) and p V (w sj |w st ) is a neural network classifier that takes vector representations for w stand w sj as input (see Section 2).", "labels": [], "entities": []}, {"text": "Distributed language representations have been studied since the early work on neural networks and have long been applied in natural language processing).", "labels": [], "entities": [{"text": "Distributed language representations", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8259002566337585}]}, {"text": "The models are generating much recent interest due to the large performance gains from the newer systems, including Word2Vec and the Glove model of, observed in, e.g., word prediction, word analogy identification, and named entity recognition.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.96458500623703}, {"text": "word prediction", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.8565740287303925}, {"text": "word analogy identification", "start_pos": 185, "end_pos": 212, "type": "TASK", "confidence": 0.874096155166626}, {"text": "named entity recognition", "start_pos": 218, "end_pos": 242, "type": "TASK", "confidence": 0.6782304445902506}]}, {"text": "Given the success of these new models, researchers have begun searching for ways to adapt the representations for use in document classification tasks such as sentiment prediction or author identification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 121, "end_pos": 144, "type": "TASK", "confidence": 0.7227454334497452}, {"text": "sentiment prediction", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.9509245753288269}, {"text": "author identification", "start_pos": 183, "end_pos": 204, "type": "TASK", "confidence": 0.8332873582839966}]}, {"text": "One naive approach is to use aggregated word vectors across a document (e.g., a document's average word-vector location) as input to a standard classifier (e.g., logistic regression).", "labels": [], "entities": []}, {"text": "However, a document is actually an ordered path of locations through R K , and simple averaging destroys much of the available information.", "labels": [], "entities": []}, {"text": "More sophisticated aggregation is proposed in, where recursive neural networks are used to combine the word vectors through the estimated parse tree for each sentence.", "labels": [], "entities": []}, {"text": "Alternatively, adds document labels to the conditioning set in (1) and has them influence the skip-gram likelihood through a latent input vector location in V.", "labels": [], "entities": [{"text": "skip-gram likelihood", "start_pos": 94, "end_pos": 114, "type": "METRIC", "confidence": 0.8085768520832062}]}, {"text": "In each case, the end product is a distributed representation for every sentence (or document for Doc2Vec) that can be used as input to a generic classifier.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Simple phrase- count regression is consistently the strongest per- former, bested only by Word2Vec inversion on  task b. This is partially due to the relative strengths  of discriminative (e.g., logistic regression) vs gen-", "labels": [], "entities": []}, {"text": " Table 1: Out-of-sample misclassification rates.", "labels": [], "entities": []}]}