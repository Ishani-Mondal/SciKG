{"title": [{"text": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "labels": [], "entities": [{"text": "Improved Semantic Representations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8515205581982931}]}], "abstractContent": [{"text": "Because of their superior ability to preserve sequence information overtime, Long Short-Term Memory (LSTM) networks , a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 238, "end_pos": 255, "type": "TASK", "confidence": 0.7328476309776306}]}, {"text": "The only underlying LSTM structure that has been explored so far is a linear chain.", "labels": [], "entities": []}, {"text": "However, natural language exhibits syntactic properties that would naturally combine words to phrases.", "labels": [], "entities": []}, {"text": "We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies.", "labels": [], "entities": []}, {"text": "Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "labels": [], "entities": [{"text": "predicting the semantic relatedness of two sentences", "start_pos": 83, "end_pos": 135, "type": "TASK", "confidence": 0.8973784787314278}, {"text": "sentiment classification", "start_pos": 163, "end_pos": 187, "type": "TASK", "confidence": 0.915267139673233}, {"text": "Stanford Sentiment Treebank)", "start_pos": 189, "end_pos": 217, "type": "DATASET", "confidence": 0.8494175523519516}]}], "introductionContent": [{"text": "Most models for distributed representations of phrases and sentences-that is, models where realvalued vectors are used to represent meaning-fall into one of three classes: bag-of-words models, sequence models, and tree-structured models.", "labels": [], "entities": []}, {"text": "In bag-of-words models, phrase and sentence representations are independent of word order; for example, they can be generated by averaging constituent word representations (Landauer and Dumais, 1997;).", "labels": [], "entities": []}, {"text": "In contrast, sequence models construct sentence representations as an order-sensitive function of the sequence of tokens.", "labels": [], "entities": []}, {"text": "Lastly, tree-structured models compose each phrase and sentence representation from its constituent subphrases according to a given syntactic structure over the sentence Figure 1: Top: A chain-structured LSTM network.", "labels": [], "entities": []}, {"text": "Bottom: A tree-structured LSTM network with arbitrary branching factor.", "labels": [], "entities": []}, {"text": "Order-insensitive models are insufficient to fully capture the semantics of natural language due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure (e.g., \"cats climb trees\" vs. \"trees climb cats\").", "labels": [], "entities": []}, {"text": "We therefore turn to ordersensitive sequential or tree-structured models.", "labels": [], "entities": []}, {"text": "In particular, tree-structured models area linguistically attractive option due to their relation to syntactic interpretations of sentence structure.", "labels": [], "entities": []}, {"text": "A natural question, then, is the following: to what extent (if at all) can we do better with tree-structured models as opposed to sequential models for sentence representation?", "labels": [], "entities": []}, {"text": "In this paper, we work towards addressing this question by directly comparing a type of sequential model that has recently been used to achieve state-of-the-art results in several NLP tasks against its tree-structured generalization.", "labels": [], "entities": []}, {"text": "Due to their capability for processing arbitrarylength sequences, recurrent neural networks (RNNs) area natural choice for sequence modeling tasks.", "labels": [], "entities": []}, {"text": "Recently, RNNs with Long Short-Term Memory (LSTM) units) have re-emerged as a popular architecture due to their representational power and effectiveness at capturing long-term dependencies.", "labels": [], "entities": []}, {"text": "LSTM networks, which we review in Sec.", "labels": [], "entities": []}, {"text": "2, have been successfully applied to a variety of sequence modeling and prediction tasks, notably machine translation (), speech recognition (, image caption generation (, and program execution (.", "labels": [], "entities": [{"text": "sequence modeling and prediction", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.787185937166214}, {"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.8441325724124908}, {"text": "speech recognition", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.8220486342906952}, {"text": "image caption generation", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.8338278333346049}]}, {"text": "In this paper, we introduce a generalization of the standard LSTM architecture to tree-structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM.", "labels": [], "entities": []}, {"text": "While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step, the tree-structured LSTM, or Tree-LSTM, composes its state from an input vector and the hidden states of arbitrarily many child units.", "labels": [], "entities": []}, {"text": "The standard LSTM can then be considered a special case of the Tree-LSTM where each internal node has exactly one child.", "labels": [], "entities": []}, {"text": "In our evaluations, we demonstrate the empirical strength of Tree-LSTMs as models for representing sentences.", "labels": [], "entities": []}, {"text": "We evaluate the Tree-LSTM architecture on two tasks: semantic relatedness prediction on sentence pairs and sentiment classification of sentences drawn from movie reviews.", "labels": [], "entities": [{"text": "semantic relatedness prediction", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.755974551041921}, {"text": "sentiment classification of sentences drawn from movie reviews", "start_pos": 107, "end_pos": 169, "type": "TASK", "confidence": 0.9034005925059319}]}, {"text": "Our experiments show that Tree-LSTMs outperform existing systems and sequential LSTM baselines on both tasks.", "labels": [], "entities": []}, {"text": "Implementations of our models and experiments are available at https:// github.com/stanfordnlp/treelstm.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our Tree-LSTM architectures on two tasks: (1) sentiment classification of sentences sampled from movie reviews and (2) predicting the semantic relatedness of sentence pairs.", "labels": [], "entities": [{"text": "sentiment classification of sentences sampled from movie reviews", "start_pos": 58, "end_pos": 122, "type": "TASK", "confidence": 0.9277273863554001}, {"text": "predicting the semantic relatedness of sentence pairs", "start_pos": 131, "end_pos": 184, "type": "TASK", "confidence": 0.7498723609106881}]}, {"text": "In comparing our Tree-LSTMs against sequential LSTMs, we control for the number of LSTM parameters by varying the dimensionality of the hidden states 2 . Details for each model variant are summarized in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Memory dimensions d and composition  function parameter counts |\u03b8| for each LSTM vari- ant that we evaluate.", "labels": [], "entities": []}, {"text": " Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean  scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval  2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs.", "labels": [], "entities": [{"text": "SICK semantic relatedness", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.803841749827067}]}]}