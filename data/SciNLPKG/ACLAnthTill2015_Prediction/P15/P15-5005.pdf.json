{"title": [{"text": "Matrix and Tensor Factorization Methods for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "1 Tutorial Objectives Tensor and matrix factorization methods have attracted a lot of attention recently thanks to their successful applications to information extraction, knowledge base population, lexical semantics and dependency parsing.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.8147875666618347}, {"text": "dependency parsing", "start_pos": 221, "end_pos": 239, "type": "TASK", "confidence": 0.8456386029720306}]}, {"text": "In the first part, we will first cover the basics of matrix and tensor factorization theory and optimization, and then proceed to more advanced topics involving convex surrogates and alternative losses.", "labels": [], "entities": [{"text": "matrix and tensor factorization theory", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.5937003076076508}]}, {"text": "In the second part we will discuss recent NLP applications of these methods and show the connections with other popular methods such as transductive learning, topic models and neural networks.", "labels": [], "entities": []}, {"text": "The aim of this tutorial is to present in detail applied factorization methods, as well as to introduce more recently proposed methods that are likely to be useful to NLP applications.", "labels": [], "entities": []}, {"text": "2 Tutorial Overview 2.1 Matrix/Tensor Factorization Basics In this part, we first remind essential results on bilinear forms, spectral representations of matrices and low-rank approximation theorems, which are often omitted in undergraduate linear algebra courses.", "labels": [], "entities": []}, {"text": "This includes the link between eigen-value decomposition and singular value decomposition and the trace-norm (a.k.a. nuclear norm) as a convex surrogate of the low-rank constraint on optimization problems.", "labels": [], "entities": [{"text": "singular value decomposition", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6666430830955505}]}, {"text": "Then, an overview of the most efficient algorithms to solve low-rank constrained problems is made, from the power iteration method, the Lanczos algorithm and the implicitly restarted Arnoldi method that is implemented in the LAPACK library (Anderson et al., 1999).", "labels": [], "entities": [{"text": "LAPACK library", "start_pos": 225, "end_pos": 239, "type": "DATASET", "confidence": 0.9014447927474976}]}, {"text": "We show how to interpret low-rank models as probabilistic models (Bishop, 1999) and how we can extend SVD algorithms that can factor-ize non-standard matrices (i.e. with non-Gaussian noise and missing data) using gradient descent, re-weighted SVD or Frank-Wolfe algorithms.", "labels": [], "entities": []}, {"text": "We then show that combining different convex objectives can be a powerful tool, and we illustrate it by deriving the robust PCA algorithm by adding an L 1 penalty term in the objective function (Cand\u00e8s and Recht, 2009).", "labels": [], "entities": []}, {"text": "Furthermore, we introduce Bayesian Personalized Ranking (BPR) for matrix and tensor factorization which deals with implicit feedback in ranking tasks (Rendle et al., 2009).", "labels": [], "entities": [{"text": "Bayesian Personalized Ranking (BPR)", "start_pos": 26, "end_pos": 61, "type": "METRIC", "confidence": 0.7157941907644272}]}, {"text": "Finally , will introduce the collective matrix factor-ization model (Singh and Gordon, 2008) and ten-sor extensions (Nickel et al., 2011) for relational learning.", "labels": [], "entities": [{"text": "relational learning", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.8636253476142883}]}, {"text": "2.2 Applications in NLP In this part we will discuss recent work applying matrix/tensor factorization methods in the context of NLP.", "labels": [], "entities": []}, {"text": "We will review the Universal Schema paradigm for knowledge base construction (Riedel et al., 2013) which relies on matrix factoriza-tion and BPR, as well as recent extensions of the RESCAL tensor factorization (Nickel et al., 2011) approach and methods of injecting logic into the embeddings learned (Rockt\u00e4schel et al., 2015).", "labels": [], "entities": [{"text": "knowledge base construction", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6549346546332041}, {"text": "BPR", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9644941687583923}]}, {"text": "These applications will motivate the connections between matrix factorization and trans-ductive learning (Goldberg et al., 2010), as well as tensor factorization and multi-task learning (Romera-Paredes et al., 2013).", "labels": [], "entities": []}, {"text": "Furthermore, we will review work on applying matrix and tensor factorization to sparsity reduction in syntactic dependency parsing (Lei et al., 2014) and word representation learning (Pennington et al., 2014).", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.6681901017824808}, {"text": "word representation learning", "start_pos": 154, "end_pos": 182, "type": "TASK", "confidence": 0.8574080467224121}]}, {"text": "In addition, we will discuss the connections between matrix factorization, latent semantic analysis and topic modeling (Stevens et al., 2012).", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.7125742038091024}, {"text": "topic modeling", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7369591593742371}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}