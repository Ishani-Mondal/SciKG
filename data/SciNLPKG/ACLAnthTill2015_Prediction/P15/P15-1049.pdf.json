{"title": [{"text": "S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking", "labels": [], "entities": [{"text": "Tweet Entity Linking", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.8220196763674418}]}], "abstractContent": [{"text": "Non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features.", "labels": [], "entities": []}, {"text": "However, tree-based models are seldom studied in the context of structured learning despite their recent success on various classification and ranking tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose SMART , a tree-based structured learning framework based on multiple additive regression trees.", "labels": [], "entities": []}, {"text": "SMART is especially suitable for handling tasks with dense features , and can be used to learn many different structures under various loss functions.", "labels": [], "entities": [{"text": "SMART", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.810818612575531}]}, {"text": "We apply SMART to the task of tweet entity linking-a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base.", "labels": [], "entities": [{"text": "SMART", "start_pos": 9, "end_pos": 14, "type": "TASK", "confidence": 0.9605398774147034}, {"text": "tweet entity linking-a", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.6349093616008759}, {"text": "tweet information extraction", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6875352462132772}]}, {"text": "A novel inference algorithm is proposed to handle the special structure of the task.", "labels": [], "entities": []}, {"text": "The experimental results show that SMART significantly outperforms state-of-the-art tweet entity linking systems.", "labels": [], "entities": [{"text": "SMART", "start_pos": 35, "end_pos": 40, "type": "TASK", "confidence": 0.9517492055892944}, {"text": "tweet entity linking", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.6710211038589478}]}], "introductionContent": [{"text": "Many natural language processing (NLP) problems can be formalized as structured prediction tasks.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.7551642457644144}]}, {"text": "Standard algorithms for structured learning include Conditional Random Field (CRF)) and Structured Supported Vector Machine (SSVM) ().", "labels": [], "entities": []}, {"text": "These algorithms, usually equipped with a linear model and sparse lexical features, achieve stateof-the-art performances in many NLP applications such as part-of-speech tagging, named entity recognition and dependency parsing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.7313855588436127}, {"text": "named entity recognition", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.6684191624323527}, {"text": "dependency parsing", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.8528681993484497}]}, {"text": "This classical combination of linear models and sparse features is challenged by the recent emerging usage of dense features such as statistical and embedding features.", "labels": [], "entities": []}, {"text": "Tasks with these low dimensional dense features require models to be more sophisticated to capture the relationships between features.", "labels": [], "entities": []}, {"text": "Therefore, non-linear models start to receive more attention as they are often more expressive than linear models.", "labels": [], "entities": []}, {"text": "Tree-based models such as boosted trees) are flexible non-linear models.", "labels": [], "entities": []}, {"text": "They can handle categorical features and count data better than other non-linear models like Neural Networks.", "labels": [], "entities": []}, {"text": "Unfortunately, to the best of our knowledge, little work has utilized tree-based methods for structured prediction, with the exception of TreeCRF (.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.6978167295455933}]}, {"text": "In this paper, we propose a novel structured learning framework called S-MART (Structured Multiple Additive Regression Trees).", "labels": [], "entities": []}, {"text": "Unlike TreeCRF, S-MART is very versatile, as it can be applied to tasks beyond sequence tagging and can be trained under various objective functions.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.719097375869751}]}, {"text": "S-MART is also powerful, as the high order relationships between features can be captured by nonlinear regression trees.", "labels": [], "entities": []}, {"text": "We further demonstrate how S-MART can be applied to tweet entity linking, an important and challenging task underlying many applications including product feedback and topic detection and tracking.", "labels": [], "entities": [{"text": "tweet entity linking", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7995917598406473}, {"text": "topic detection and tracking", "start_pos": 168, "end_pos": 196, "type": "TASK", "confidence": 0.8032284528017044}]}, {"text": "We apply S-MART to entity linking using a simple logistic function as the loss function and propose a novel inference algorithm to prevent overlaps between entities.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.7430653274059296}]}, {"text": "Our contributions are summarized as follows: \u2022 We propose a novel structured learning framework called S-MART.", "labels": [], "entities": []}, {"text": "S-MART combines non-linearity and efficiency of treebased models with structured prediction, leading to a family of new algorithms.", "labels": [], "entities": []}, {"text": "(Section 2) \u2022 We apply S-MART to tweet entity linking.", "labels": [], "entities": [{"text": "tweet entity linking", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.6757818460464478}]}, {"text": "Building on top of S-MART, we propose a novel inference algorithm for nonoverlapping structure with the goal of preventing conflicting entity assignments.", "labels": [], "entities": []}, {"text": "(Section 3) \u2022 We provide a systematic study of evaluation criteria in tweet entity linking by conducting extensive experiments over major data sets.", "labels": [], "entities": [{"text": "tweet entity linking", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.6759705940882365}]}, {"text": "The results show that S-MART significantly outperforms state-of-the-art entity linking systems, including the system that is used to win the NEEL 2014 challenge.", "labels": [], "entities": [{"text": "NEEL 2014 challenge", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.56744384765625}]}], "datasetContent": [{"text": "Our experiments are designed to answer the following three research questions in the context of tweet entity linking: \u2022 Do non-linear learning algorithms perform better than linear learning algorithms?", "labels": [], "entities": [{"text": "tweet entity linking", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.69476185242335}]}, {"text": "\u2022 Do structured entity linking models perform better than non-structured ones?", "labels": [], "entities": []}, {"text": "\u2022 How can we best capture the relationships between entities?", "labels": [], "entities": []}, {"text": "We evaluate each entity linking system using two evaluation policies: Information Extraction (IE) driven evaluation and Information Retrieval (IR) driven evaluation.", "labels": [], "entities": []}, {"text": "For both evaluation settings, precision, recall and F1 scores are reported.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999832034111023}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9995214939117432}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9997003078460693}]}, {"text": "Our data is constructed from two publicly available sources: Named Entity Extraction & Linking (NEEL) Challenge ( ) datasets, and the datasets released by.", "labels": [], "entities": [{"text": "Named Entity Extraction & Linking (NEEL) Challenge", "start_pos": 61, "end_pos": 111, "type": "TASK", "confidence": 0.7453574472003512}]}, {"text": "Note that we gather two datasets from Fang and Chang (2014) and they are used in two different evaluation settings.", "labels": [], "entities": []}, {"text": "We refer to these two datasets as TACL-IE and TACL-IR, respectively.", "labels": [], "entities": [{"text": "TACL-IE", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.7373188734054565}, {"text": "TACL-IR", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.7485947012901306}]}, {"text": "We perform some data cleaning and unification on these sets.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7052396237850189}]}, {"text": "The statistics of the datasets are presented in.", "labels": [], "entities": []}, {"text": "IE-driven evaluation The IE-driven evaluation is the standard evaluation for an end-to-end entity linking system.", "labels": [], "entities": []}, {"text": "We follow and relax the definition of the correct mention boundaries, as they are often ambiguous.", "labels": [], "entities": []}, {"text": "A mention boundary is considered to be correct if it overlaps (instead of being the same) with the gold mention boundary.", "labels": [], "entities": []}, {"text": "Please see () for more details on the procedure of calculating the precision, recall and F1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9997366070747375}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9982692003250122}, {"text": "F1 score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9867379665374756}]}, {"text": "The NEEL and TACL-IE datasets have different annotation guidelines and different choices of knowledge bases, so we perform the following procedure to clean the data and unify the annotations.", "labels": [], "entities": [{"text": "NEEL", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8981407880783081}, {"text": "TACL-IE datasets", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.7180260419845581}]}, {"text": "We first filter out the annotations that link to entities excluded by our knowledge base.", "labels": [], "entities": []}, {"text": "We use the same knowledge base as the ERD 2014 competition (), which includes the union of entities in Wikipedia and Freebase.", "labels": [], "entities": [{"text": "ERD 2014 competition", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.570920447508494}]}, {"text": "Second, we follow NEEL annotation guideline and re-annotate TACL-IE dataset.", "labels": [], "entities": [{"text": "NEEL annotation guideline", "start_pos": 18, "end_pos": 43, "type": "DATASET", "confidence": 0.624788761138916}, {"text": "TACL-IE dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.795737236738205}]}, {"text": "For instance, in order to be consistent with NEEL, all the user tags (e.g. @BarackObama) are re-labeled as entities in TACL-IE.", "labels": [], "entities": [{"text": "NEEL", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.6843843460083008}]}, {"text": "We train all the models with NEEL Train dataset and evaluate different systems on NEEL Test and TACL-IE datasets.", "labels": [], "entities": [{"text": "NEEL Train dataset", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.9089029431343079}, {"text": "NEEL Test", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.88604936003685}, {"text": "TACL-IE datasets", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.7913507223129272}]}, {"text": "In addition, we sample 800 tweets from NEEL Train dataset as our development set to perform parameter tuning.", "labels": [], "entities": [{"text": "NEEL Train dataset", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9772194027900696}, {"text": "parameter tuning", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.6987431794404984}]}, {"text": "IR-driven evaluation The IR-driven evaluation is proposed by.", "labels": [], "entities": []}, {"text": "It is motivated by a key application of entity linking -retrieval of relevant tweets for target entities, which is crucial for downstream applications such as product research and sentiment analysis.", "labels": [], "entities": [{"text": "entity linking -retrieval of relevant tweets", "start_pos": 40, "end_pos": 84, "type": "TASK", "confidence": 0.8047869460923331}, {"text": "sentiment analysis", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.9247687757015228}]}, {"text": "In particular, given a query entity we can search for tweets based on the match with some potential surface forms of the query entity.", "labels": [], "entities": []}, {"text": "Then, an entity linking system is evaluated by its ability to correctly identify the presence or absence of the query entity in every tweet.", "labels": [], "entities": []}, {"text": "Our IR-driven evaluation is based on the TACL-IR set, which includes 980 tweets sampled for ten query entities of five entity types (roughly 100 tweets per entity).", "labels": [], "entities": [{"text": "TACL-IR set", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.8096409738063812}]}, {"text": "About 37% of the sampled tweets did not mention the query entity due to the anchor ambiguity.", "labels": [], "entities": []}, {"text": "Features We employ a total number of 37 dense features as our basic feature set.", "labels": [], "entities": []}, {"text": "Most of the features are adopted from).", "labels": [], "entities": []}, {"text": "For non-linear models, we consider polynomial SSVM, which employs polynomial kernel inside the structured SVM algorithm.", "labels": [], "entities": []}, {"text": "We also include LambdaRank (, a neuralbased learning to rank algorithm, which is widely used in the information retrieval literature.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7798532247543335}]}, {"text": "We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured information.", "labels": [], "entities": [{"text": "MART", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.5996209383010864}, {"text": "multiclass classification", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.7622163593769073}]}, {"text": "Finally, we have our proposed log-loss S-MART algorithm, as described in Section 3.", "labels": [], "entities": []}, {"text": "Note that our baseline systems are quite strong.", "labels": [], "entities": []}, {"text": "Linear SSVM has been used in one of the stateof-the-art tweet entity linking systems (), and the system based on MART is the winning system of the 2014 NEEL Challenge (Cano and others, 2014) . summarizes several properties of the algorithms.", "labels": [], "entities": [{"text": "tweet entity linking", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6841638882954916}, {"text": "NEEL Challenge", "start_pos": 152, "end_pos": 166, "type": "DATASET", "confidence": 0.6737519800662994}]}, {"text": "For example, most algorithms are struc-: Included algorithms and their properties.", "labels": [], "entities": []}, {"text": "tured (e.g. they perform dynamic programming attest time) except for MART and LambdaRank, which treat mention candidates independently.", "labels": [], "entities": [{"text": "MART", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.7734396457672119}]}, {"text": "Parameter tuning All the hyper-parameters are tuned on the development set.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9327360391616821}]}, {"text": "Then, we re-train our models on full training data (including the dev set) with the best parameters.", "labels": [], "entities": []}, {"text": "We choose the soft margin parameter C from {0.5, 1, 5, 10} for two structured SVM methods.", "labels": [], "entities": []}, {"text": "After a preliminary parameter search, we fixed the number of trees to 300 and the minimum number of documents in a leaf to 30 for all tree-based models.", "labels": [], "entities": []}, {"text": "For LambdaRank, we use a two layer feed forward network.", "labels": [], "entities": []}, {"text": "We select the number of hidden units from {10, 20, 30, 40} and learning rate from {0.1, 0.01, 0.001}.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9623290300369263}]}, {"text": "It is widely known that F1 score can be affected by the trade-off between precision and recall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9670567512512207}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994707703590393}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9945916533470154}]}, {"text": "In order to make the comparisons between all algorithms fairer in terms of F1 score, we include a post-processing step to balance precision and recall for all the systems.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9895510375499725}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9993859529495239}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9990965127944946}]}, {"text": "Note the tuning is only conducted for the purpose of robust evaluation.", "labels": [], "entities": []}, {"text": "In particular, we adopt a simple tuning strategy that works well for all the algorithms, in which we add a bias term b to the scoring function value of Nil: We choose the bias term b from values between \u22123.0 to 3.0 on the dev set and apply the same bias term attest time.", "labels": [], "entities": []}, {"text": "presents the empirical findings for S-MART and competitive methods on tweet entity linking task in both IE and IR settings.", "labels": [], "entities": [{"text": "tweet entity linking task", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7298080623149872}]}, {"text": "In the following, we analyze the empirical results in details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of data sets.", "labels": [], "entities": []}, {"text": " Table 3: IE-driven and IR-driven evaluation results for different models. The best results with basic features are in bold. The  results are underlined if adding entity-entity features gives the overall best results.", "labels": [], "entities": []}]}