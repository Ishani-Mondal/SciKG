{"title": [], "abstractContent": [{"text": "Current distributed representations of words show little resemblance to theories of lexical semantics.", "labels": [], "entities": []}, {"text": "The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy).", "labels": [], "entities": []}, {"text": "We propose methods that transform word vectors into sparse (and optionally binary) vectors.", "labels": [], "entities": []}, {"text": "The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora.", "labels": [], "entities": []}, {"text": "Because the vectors are highly sparse, they are computationally easy to work with.", "labels": [], "entities": []}, {"text": "Most importantly, we find that they out-perform the original vectors on benchmark tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations of words have been shown to benefit NLP tasks like parsing), named entity recognition, and sentiment analysis.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9486457705497742}, {"text": "named entity recognition", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.691284716129303}, {"text": "sentiment analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.957754909992218}]}, {"text": "The attraction of word vectors is that they can be derived directly from raw, unannotated corpora.", "labels": [], "entities": []}, {"text": "Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics).", "labels": [], "entities": []}, {"text": "Yet word vectors do not look anything like the representations described inmost lexical semantic theories, which focus on identifying classes of words) and relationships among word meanings.", "labels": [], "entities": []}, {"text": "Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired.", "labels": [], "entities": []}, {"text": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (.", "labels": [], "entities": []}, {"text": "Our contribution to this discussion is anew, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors ( \u00a72).", "labels": [], "entities": []}, {"text": "Unlike recent approaches of incorporating semantics in distributional word vectors (, the method does not rely on any external information source.", "labels": [], "entities": []}, {"text": "The transformation results in longer, sparser vectors, sometimes called an \"overcomplete\" representation.", "labels": [], "entities": []}, {"text": "Sparse, overcomplete representations have been motivated in other domains as away to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions), and to increase stability in the presence of noise ().", "labels": [], "entities": []}, {"text": "Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly.", "labels": [], "entities": []}, {"text": "Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability () and usability of word vectors as features in downstream tasks ().", "labels": [], "entities": []}, {"text": "The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex-ical semantic theories.", "labels": [], "entities": []}, {"text": "Using a number of stateof-the-art word vectors as input, we find consistent benefits of our method on a suite of standard benchmark evaluation tasks ( \u00a73).", "labels": [], "entities": []}, {"text": "We also evaluate our word vectors in a word intrusion experiment with humans ( and find that our sparse vectors are more interpretable than the original vectors ( \u00a74).", "labels": [], "entities": []}, {"text": "We anticipate that sparse, binary vectors can play an important role as features in statistical NLP models, which still rely predominantly on discrete, sparse features whose interpretability enables error analysis and continued development.", "labels": [], "entities": []}, {"text": "We have made an implementation of our method publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using methods A and B, we constructed sparse overcomplete vector representations A, starting from four initial vector representations X; these are explained in Appendix A. We used one benchmark evaluation (WS-353) to tune hyperparameters, resulting in the settings shown in; seven other tasks were used to evaluate the quality of the sparse overcomplete representations.", "labels": [], "entities": []}, {"text": "The first of these is a word similarity task, where the score is correlation with human judgments, and the others are classification accuracies of an 2 -regularized logistic regression model trained using the word vectors.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.783700962861379}]}, {"text": "These tasks are described in detail in Appendix B.  If a vector dimension is interpretable, the topranking words for that dimension should display semantic or syntactic groupings.", "labels": [], "entities": []}, {"text": "To verify this qualitatively, we select five dimensions with the highest variance of values in initial and sparsified GC vectors.", "labels": [], "entities": []}, {"text": "We compare top-ranked words in the dimensions extracted from the two representations.", "labels": [], "entities": []}, {"text": "The words are listed in, a dimension per row.", "labels": [], "entities": []}, {"text": "Subjectively, we find the semantic groupings better in the sparse vectors than in the initial vectors.", "labels": [], "entities": []}, {"text": "visualizes the sparsified GC vectors for six words.", "labels": [], "entities": []}, {"text": "The dimensions are sorted by the average value across the three \"animal\" vectors.", "labels": [], "entities": []}, {"text": "We now describe the different evaluation benchmarks for word vectors.", "labels": [], "entities": []}, {"text": "We evaluate our word representations on two word similarity tasks.", "labels": [], "entities": []}, {"text": "The first is the WS-353 dataset (), which contains 353 pairs of English words that have been assigned similarity ratings by humans.", "labels": [], "entities": [{"text": "WS-353 dataset", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.9421476125717163}]}, {"text": "This dataset is used to tune sparse vector learning hyperparameters ( \u00a72.5), while the remaining of the tasks discussed in this section are completely held out.", "labels": [], "entities": []}, {"text": "A more recent dataset,, has been constructed to specifically focus on similarity (rather than relatedness).", "labels": [], "entities": []}, {"text": "It contains a balanced set of noun, verb, and adjective pairs.", "labels": [], "entities": []}, {"text": "We calculate cosine similarity between the vectors of two words forming a test item and report Spearman's rank correlation coefficient between the rankings produced by our model against the human rankings.", "labels": [], "entities": [{"text": "rank correlation coefficient", "start_pos": 106, "end_pos": 134, "type": "METRIC", "confidence": 0.8138575951258341}]}, {"text": "Sentiment Analysis (Senti).", "labels": [], "entities": [{"text": "Sentiment Analysis (Senti)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8638172507286072}]}, {"text": "created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "labels": [], "entities": []}, {"text": "The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively.", "labels": [], "entities": []}, {"text": "We use average of the word vectors of a given sentence as feature for classification.", "labels": [], "entities": []}, {"text": "The classifier is tuned on the dev.", "labels": [], "entities": []}, {"text": "set and accuracy is reported on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9997246861457825}]}, {"text": "Question Classification (TREC).", "labels": [], "entities": [{"text": "Question Classification (TREC)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6929476499557495}]}, {"text": "As an aid to question answering, a question maybe classified as belonging to one of many question types.", "labels": [], "entities": [{"text": "question answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9209536015987396}]}, {"text": "The TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information ().", "labels": [], "entities": [{"text": "TREC questions dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.6329562266667684}]}, {"text": "The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions.", "labels": [], "entities": []}, {"text": "An average of the word vectors of the input question is used as features and accuracy is reported on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9995805621147156}]}, {"text": "We consider three binary categorization tasks from the 20 Newsgroups dataset.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.6882572770118713}]}, {"text": "Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with:  length three words, where the first can bean adjective or a noun and the other two are nouns.", "labels": [], "entities": []}, {"text": "The task is to predict the correct bracketing in the parse tree fora given noun phrase.", "labels": [], "entities": []}, {"text": "For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively.", "labels": [], "entities": []}, {"text": "We append the word vectors of the three words in the NP in order and use them as features for binary classification.", "labels": [], "entities": []}, {"text": "The dataset contains 2,227 noun phrases split into 10 folds.", "labels": [], "entities": []}, {"text": "The classifier is tuned on the first fold and cross-validation accuracy is reported on the remaining nine folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9646627306938171}]}], "tableCaptions": [{"text": " Table 1: Hyperparameters for learning sparse  overcomplete vectors tuned on the WS-353 task.  Tasks are explained in  \u00a7B. The four initial vector  representations X are explained in  \u00a7A.", "labels": [], "entities": [{"text": "WS-353 task", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.7609573602676392}]}, {"text": " Table 3: Performance comparison of transformed vectors to initial vectors X. We show sparse over- complete representations A and also binarized representations B. Initial vectors are discussed in  \u00a7A and  tasks in  \u00a7B.", "labels": [], "entities": []}, {"text": " Table 4: Average performance across all tasks and vector models using different transformations.", "labels": [], "entities": []}, {"text": " Table 5: Accuracy of three human annotators on  the word intrusion task, along with the average  inter-annotator agreement (Artstein and Poesio,  2008) and Fleiss' \u03ba (Davies", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9921665191650391}, {"text": "Fleiss' \u03ba", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9203133583068848}]}]}