{"title": [{"text": "Point Process Modelling of Rumour Dynamics in Social Media", "labels": [], "entities": [{"text": "Point Process Modelling of Rumour Dynamics", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6623793443044027}]}], "abstractContent": [{"text": "Rumours on social media exhibit complex temporal patterns.", "labels": [], "entities": []}, {"text": "This paper develops a model of rumour prevalence using a point process, namely a log-Gaussian Cox process , to infer an underlying continuous temporal probabilistic model of post frequencies.", "labels": [], "entities": [{"text": "rumour prevalence", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9247602224349976}]}, {"text": "To generalize over different rumours , we present a multi-task learning method parametrized by the text in posts which allows data statistics to be shared between groups of similar rumours.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that our model outperforms several strong baseline methods for rumour frequency prediction evaluated on tweets from the 2014 Ferguson riots.", "labels": [], "entities": [{"text": "rumour frequency prediction", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.7746110955874125}, {"text": "Ferguson riots", "start_pos": 153, "end_pos": 167, "type": "DATASET", "confidence": 0.7318950593471527}]}], "introductionContent": [{"text": "The ability to model rumour dynamics helps with identifying those, which, if not debunked early, will likely spread very fast.", "labels": [], "entities": [{"text": "rumour dynamics", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8504480421543121}]}, {"text": "One such example is the false rumour of rioters breaking into McDonald's during the 2011 England riots.", "labels": [], "entities": [{"text": "England riots", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.6660240888595581}]}, {"text": "An effective early warning system of this kind is of interest to government bodies and news outlets, who struggle with monitoring and verifying social media posts during emergencies and social unrests.", "labels": [], "entities": []}, {"text": "Another application of modelling rumour dynamics could be to predict the prevalence of a rumour throughout its lifespan, based on occasional spot checks by journalists.", "labels": [], "entities": [{"text": "rumour dynamics", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.9116266369819641}]}, {"text": "The challenge comes from the observation that different rumours exhibit different trajectories.", "labels": [], "entities": []}, {"text": "shows two example rumours from our dataset (see Section 3): online discussion of rumour #10 quickly drops away, whereas rumour #37 takes a lot longer to die out.", "labels": [], "entities": []}, {"text": "Two characteristics can help determine if a rumour will continue to be discussed.", "labels": [], "entities": []}, {"text": "One is the dynamics of post occurrences, e.g. if the frequency profile decays quickly, chances are it would not attract further attention.", "labels": [], "entities": []}, {"text": "A second factor is text from the posts themselves, where phrases such as not true, unconfirmed, or debunk help users judge veracity and thus limit rumour spread (.", "labels": [], "entities": []}, {"text": "This paper considers the problem of modelling temporal frequency profiles of rumours by taking into account both the temporal and textual information.", "labels": [], "entities": []}, {"text": "Since posts occur at continuous timestamps, and their density is typically a smooth function of time, we base our model on point processes, which have been shown to model well such data in epidemiology and conflict mapping.", "labels": [], "entities": [{"text": "conflict mapping", "start_pos": 206, "end_pos": 222, "type": "TASK", "confidence": 0.7877559661865234}]}, {"text": "This framework models count data in a continuous time through the underlying intensity of a Poisson distribution.", "labels": [], "entities": []}, {"text": "The posterior distribution can then be used for several inference problems, e.g. to query the expected count of posts, or to find the probability of a count of posts occurring during an arbitrary time interval.", "labels": [], "entities": []}, {"text": "We model frequency profiles using a log-Gaussian Cox process), a point process where the log-intensity of the Poisson distribution is modelled via a Gaussian Process (GP).", "labels": [], "entities": []}, {"text": "GP is a nonparametric model which allows for powerful modelling of the underlying intensity function.", "labels": [], "entities": []}, {"text": "Modelling the frequency profile of a rumour based on posts is extremely challenging, since many rumours consist of only a small number of posts and exhibit complex patterns.", "labels": [], "entities": []}, {"text": "To overcome this difficulty we propose a multi-task learning approach, where patterns are correlated across multiple rumours.", "labels": [], "entities": []}, {"text": "In this way statistics over a larger training set are shared, enabling more reliable predictions for distant time periods, in which no posts from the target rumour have been observed.", "labels": [], "entities": []}, {"text": "We demonstrate how text from observed posts can be used to weight influence across rumours.", "labels": [], "entities": []}, {"text": "Using a set of Twitter rumours from the 2014 Ferguson unrest, we demonstrate that our models provide good LGCP LGCPICM LGCPTXT (b) rumour #10 Figure 1: Predicted frequency profiles for example rumours.", "labels": [], "entities": [{"text": "Ferguson unrest", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.7141557335853577}, {"text": "LGCP LGCPICM LGCPTXT", "start_pos": 106, "end_pos": 126, "type": "METRIC", "confidence": 0.6037106513977051}]}, {"text": "Black bars denote training intervals, white bars denote test intervals.", "labels": [], "entities": []}, {"text": "Dark-coloured lines correspond to mean predictions by the models, light shaded areas denote the 95% confidence interval, \u00b5 \u00b1 2\u03c3.", "labels": [], "entities": []}, {"text": "This figure is best viewed in colour.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "Introduces the problem of modelling rumour frequency profiles, and presents a method based on a log-Gaussian Cox process; 2.", "labels": [], "entities": []}, {"text": "Incorporates multi-task learning to generalize across disparate rumours; and 3.", "labels": [], "entities": []}, {"text": "Demonstrates how incorporating text into multi-task learning improves results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation metric We use mean squared error (MSE) to measure the difference between true counts and predicted counts in the test intervals.", "labels": [], "entities": [{"text": "mean squared error (MSE)", "start_pos": 25, "end_pos": 49, "type": "METRIC", "confidence": 0.9475833773612976}]}, {"text": "Since probabilistic models (GP, LGCP) return distributions over possible outputs, we also evaluate them via the log-likelihood (LL) of the true counts under the returned distributions (respectively Gaussian and Poisson distribution).", "labels": [], "entities": [{"text": "log-likelihood (LL)", "start_pos": 112, "end_pos": 131, "type": "METRIC", "confidence": 0.7365370094776154}]}, {"text": "Baselines We use the following baselines.", "labels": [], "entities": []}, {"text": "The first is the Homogenous Poisson Process (HPP) trained on the training set of the rumour.", "labels": [], "entities": []}, {"text": "We select its intensity \u03bb using maximum likelihood estimate, which equals to the mean frequency of posts in the training intervals.", "labels": [], "entities": [{"text": "maximum likelihood estimate", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.8550400137901306}]}, {"text": "The second baseline is Gaussian Process (GP) used for predicting hashtag frequencies in Twitter by.", "labels": [], "entities": [{"text": "Gaussian Process (GP)", "start_pos": 23, "end_pos": 44, "type": "METRIC", "confidence": 0.941532552242279}, {"text": "predicting hashtag frequencies", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.8633585373560587}]}, {"text": "Authors considered various kernels in their experiments, most notably periodic kernels.", "labels": [], "entities": []}, {"text": "In our case it is not apparent that rumours exhibit periodic characteristics, as can be seen in.", "labels": [], "entities": []}, {"text": "We restrict our focus to RBF kernel and leave inspection of other kernels such as periodic ones for both GP and LGCP models for future.", "labels": [], "entities": []}, {"text": "The third baseline is to always predict 0 posts in all intervals.", "labels": [], "entities": []}, {"text": "The fourth baseline is tailored for the interpolation setting, and uses simple interpolation by averaging over the frequencies of the closest left and right intervals, or the frequency of the closest interval for test intervals on a boundary.", "labels": [], "entities": []}, {"text": "Data preprocessing In our experiments, we consider the first two hours of each rumour lifespan, which we split into 20 evenly spaced intervals.", "labels": [], "entities": [{"text": "Data preprocessing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.5950140357017517}]}, {"text": "This way, our dataset consists in total of 2280 intervals.", "labels": [], "entities": []}, {"text": "We iterate over rumours using a form of folded cross-validation, wherein each iteration we exclude some (but not all) time intervals fora single target rumour.", "labels": [], "entities": []}, {"text": "The excluded time intervals form the test set: either by selecting half at random (interpolation); or by taking only the second half for testing (extrapolation).", "labels": [], "entities": []}, {"text": "To ameliorate the problems of data sparsity, we replace words with their Brown cluster ids, using 1000 clusters acquired on a large scale Twitter corpus (.", "labels": [], "entities": []}, {"text": "The mean function for the underlying GP in LGCP methods is assumed to be 0, which results in intensity function to be around 1 in the absence of nearby observations.", "labels": [], "entities": [{"text": "intensity function", "start_pos": 93, "end_pos": 111, "type": "METRIC", "confidence": 0.9612695276737213}]}, {"text": "This prevents our method from predicting 0 counts in these regions.", "labels": [], "entities": []}, {"text": "We add 1 to the counts in the intervals to deal with this problem as a preprocessing step.", "labels": [], "entities": []}, {"text": "The original counts can be obtained by decrementing 1 from the predicted counts.", "labels": [], "entities": []}, {"text": "Instead, one could use a GP with a non-zero mean function and learn the mean function, a more elegant way of approaching this problem, which we leave for future work.", "labels": [], "entities": []}, {"text": "The left columns of  proaches.", "labels": [], "entities": []}, {"text": "This is due to GP modelling a distribution with continuous support, which is inappropriate for modelling discrete counts.", "labels": [], "entities": []}, {"text": "Changing the model from a GP to a better fitting to the modelling temporal count data LGCP gives a big improvement, even when a point estimate of the prediction is considered (MSE).", "labels": [], "entities": []}, {"text": "The 0 baseline is very strong, since many rumours have comparatively little discussion in the second hour of their lifespan relative to the first hour.", "labels": [], "entities": [{"text": "0 baseline", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8214084506034851}]}, {"text": "Incorporating information about other rumours helps outperform this method.", "labels": [], "entities": []}, {"text": "ICM, TXT and ICM+TXT multitask learning approaches achieve the best scores and significantly outperform all baselines.", "labels": [], "entities": [{"text": "ICM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8538715839385986}]}, {"text": "TXT turns out to be a good approach to multi-task learning and outperforms ICM.", "labels": [], "entities": [{"text": "TXT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5017896294593811}]}, {"text": "In we show an example rumour frequency profile for the extrapolation setting.", "labels": [], "entities": []}, {"text": "TXT makes a lower error than LGCP and LGCPICM, both of which underestimate the counts in the second hour.", "labels": [], "entities": [{"text": "TXT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9197536706924438}]}, {"text": "Next, we move to the interpolation setting.", "labels": [], "entities": []}, {"text": "Unsurprisingly, Interpolate is the strongest baseline, and outperforms the raw LGCP method.", "labels": [], "entities": []}, {"text": "Again, HPP and GP are outperformed by LGCP in terms of both MSE and LL.", "labels": [], "entities": [{"text": "HPP", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.8094242811203003}, {"text": "GP", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.7423522472381592}, {"text": "MSE", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.686331033706665}]}, {"text": "Considering the output distributions (LL) the difference in performance between the Poisson Process based approaches and GP is especially big, demonstrating how well the principled models handle uncertainty in the predictive distributions.", "labels": [], "entities": []}, {"text": "As for the multi-task methods, we notice that text is particularly useful, with TXT achieving the highest MSE score out of all considered models.", "labels": [], "entities": [{"text": "MSE score", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.825427770614624}]}, {"text": "ICM turns out to be not very helpful in this setting.", "labels": [], "entities": [{"text": "ICM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8400735855102539}]}, {"text": "For example, ICM (just as LGCP) does not learn there should be a peak at the beginning of a rumour frequency profile depicted in.", "labels": [], "entities": [{"text": "ICM", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.5056223273277283}]}, {"text": "TXT manages to make a significantly smaller error by predicting a large posting frequency there.", "labels": [], "entities": [{"text": "TXT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.853396475315094}]}, {"text": "We also found, that fora few rumours ICM made a big error by predicting a high frequency at the start of a rumour lifespan when there was no such peak.", "labels": [], "entities": []}, {"text": "We hypothesize ICM performs poorly because it is hard to learn correct correlations between frequency profiles when training intervals do not form continuous segments of significant sizes.", "labels": [], "entities": [{"text": "ICM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.960637092590332}]}, {"text": "ICM manages to learn correlations more properly in extrapolation setting, where the first hour is fully observed.", "labels": [], "entities": [{"text": "ICM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7344468235969543}]}], "tableCaptions": [{"text": " Table 1: MSE between the true counts and the predicted counts (lower is better) and predictive log likelihood of the true  counts from probabilistic models (higher is better) for test intervals over the 114 Ferguson rumours for extrapolation (left) and  interpolation (right) settings, showing mean \u00b1 std. dev. Baselines are shown above the line, with LGCP models below. Key:   \u2020 denotes significantly better than the best baseline; denotes significantly worse than LGCP TXT, according to one-sided  Wilcoxon signed rank test p < 0.05.", "labels": [], "entities": [{"text": "MSE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9829767942428589}, {"text": "predictive log likelihood", "start_pos": 85, "end_pos": 110, "type": "METRIC", "confidence": 0.8325264652570089}, {"text": "LGCP TXT", "start_pos": 467, "end_pos": 475, "type": "DATASET", "confidence": 0.606500655412674}]}]}