{"title": [{"text": "User Based Aggregation for Biterm Topic Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Biterm Topic Model (BTM) is designed to model the generative process of the word co-occurrence patterns in short texts such as tweets.", "labels": [], "entities": [{"text": "generative process of the word co-occurrence patterns in short texts such as tweets", "start_pos": 50, "end_pos": 133, "type": "TASK", "confidence": 0.7289869418511024}]}, {"text": "However, two aspects of BTM may restrict its performance: 1) user individualities are ignored to obtain the corpus level words co-occurrence patterns; and 2) the strong assumptions that two co-occurring words will be assigned the same topic label could not distinguish background words from topical words.", "labels": [], "entities": [{"text": "BTM", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9231131672859192}]}, {"text": "In this paper, we propose Twitter-BTM model to address those issues by considering user level personalization in BTM.", "labels": [], "entities": []}, {"text": "Firstly, we use user based biterms aggregation to learn user specific topic distribution.", "labels": [], "entities": []}, {"text": "Secondly, each user's preference between background words and topical words is estimated by incorporating a background topic.", "labels": [], "entities": []}, {"text": "Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several state-of-the-art baselines.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.7494886219501495}]}], "introductionContent": [{"text": "In recent years, short texts are increasingly prevalent due to the explosive growth of online social media.", "labels": [], "entities": []}, {"text": "For example, about 500 million tweets are published per day on Twitter 1 , one of the most popular online social networking services.", "labels": [], "entities": []}, {"text": "Probabilistic topic models ( are broadly used to uncover the hidden topics of tweets, since the low-dimensional semantic representation is crucial for many applications, such as product recommendation (), hashtag recommendation (), user interest tracking (), sentiment analysis (.", "labels": [], "entities": [{"text": "user interest tracking", "start_pos": 232, "end_pos": 254, "type": "TASK", "confidence": 0.7132932543754578}, {"text": "sentiment analysis", "start_pos": 259, "end_pos": 277, "type": "TASK", "confidence": 0.9470223486423492}]}, {"text": "However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts.", "labels": [], "entities": []}, {"text": "Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (.", "labels": [], "entities": []}, {"text": "LDA and its variations suffer from context sparsity in each tweet.", "labels": [], "entities": [{"text": "LDA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7234041094779968}]}, {"text": "2) user, user based aggregation is utilized to alleviate the sparsity problem in short texts (.", "labels": [], "entities": []}, {"text": "In these models, all the tweets of the same user are aggregated together as a pseudo document based on the observation that the tweets written by the same user are more similar.", "labels": [], "entities": []}, {"text": "3) corpus, BTM assumes that all the biterms (co-occurring word pairs) are generated by a corpus level topic distribution to benefit from the global rich word co-occurrence patterns.", "labels": [], "entities": []}, {"text": "As far as we know, how to incorporate user factor into BTM has not been studied yet.", "labels": [], "entities": [{"text": "BTM", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8359862565994263}]}, {"text": "User based aggregation has proven effective for LDA.", "labels": [], "entities": [{"text": "LDA", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9501821398735046}]}, {"text": "But unfortunately, our preliminary experiments indicate that simple user-based aggregation for BTM will generate incoherent topics.", "labels": [], "entities": [{"text": "BTM", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.7739124894142151}]}, {"text": "To distinguish between commonly used words (e.g., good, people, etc) and topical words (e.g., food, travel, etc), a background topic is often incorporated into the topic models.", "labels": [], "entities": []}, {"text": "use a background topic in Twitter-LDA to distill discriminative words in tweets.", "labels": [], "entities": []}, {"text": "reduce the perplexity of Twitter-LDA by estimating the ratio between choosing background words and topical words for each user.", "labels": [], "entities": []}, {"text": "They both make a very strong assumption that one tweet only covers one topic.", "labels": [], "entities": []}, {"text": "use a background topic to distinguish between common biterms and bursty biterms, which need external data to evaluate the burstiness of each biterm as prior knowledge.", "labels": [], "entities": []}, {"text": "Unlike those above, we incorporate a background topic to absorb non-discriminative common words in each biterm.", "labels": [], "entities": []}, {"text": "And we also estimate the user's preference between common words and topical words.", "labels": [], "entities": []}, {"text": "Our new model is named as Twitter-BTM, which combines user based aggregation and the background topic in BTM.", "labels": [], "entities": []}, {"text": "Finally, experiments on a Twitter dataset show that Twitter-BTM not only can discover more coherent topics but also can give more accurate topic representation of tweets compared with several state-of-the-art baselines.", "labels": [], "entities": []}, {"text": "We organize the rest of the paper as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief review for BTM.", "labels": [], "entities": [{"text": "BTM", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8629566431045532}]}, {"text": "Section 3 introduces our Twitter-BTM model and its implementation.", "labels": [], "entities": []}, {"text": "Section 4 describes experimental results on a large-scale Twitter dataset.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.8167763352394104}]}, {"text": "Finally, Section 5 contains a conclusion and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this Section, we describe our experiments carried on a Twitter dataset collected form 10th Jun, 2009 to 31st Dec, 2009.", "labels": [], "entities": [{"text": "Twitter dataset collected form 10th Jun, 2009 to 31st Dec, 2009", "start_pos": 58, "end_pos": 121, "type": "DATASET", "confidence": 0.9125784773093003}]}, {"text": "Stop words and words occur less than 5 times are removed.", "labels": [], "entities": []}, {"text": "We also filter tweets which only have one or two words.", "labels": [], "entities": []}, {"text": "All letters are converted into lowercase.", "labels": [], "entities": []}, {"text": "The dataset is divided into two parts.", "labels": [], "entities": []}, {"text": "The first part whose statistics is shown in is used for training.", "labels": [], "entities": []}, {"text": "The second part which consists of 22,496,107 tweets is used as the external dataset in topic coherence evaluation task in Section 4.1.", "labels": [], "entities": [{"text": "topic coherence evaluation", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7332994739214579}]}, {"text": "We compare the performance of Twitter-BTM with five baselines: \u2022 LDA-U, user based aggregation is applied before training LDA.", "labels": [], "entities": []}, {"text": "\u2022 Twitter-LDA (Zhao et al., 2011), which makes a strong assumption that a tweet only covers one topic.", "labels": [], "entities": []}, {"text": "\u2022 TwitterUB-LDA (), an improved version of Twitter-LDA, which models the user level preference between topical words and background words.", "labels": [], "entities": []}, {"text": "\u2022 BTM (, the Biterm Topic Model.", "labels": [], "entities": [{"text": "BTM", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.7975767254829407}]}, {"text": "\u2022 BTM-U, a simplified version of Twitter-BTM without background topic.", "labels": [], "entities": []}, {"text": "For all the above models, we use symmetric Dirichlet priors.", "labels": [], "entities": []}, {"text": "The hyperparameters are set as follows: for all the models, we set \u03b1 = 50/K, \u03b2 = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set \u03b3 = 0.5.", "labels": [], "entities": []}, {"text": "We run Gibbs sampling for 400 iterations.", "labels": [], "entities": []}, {"text": "Perplexity metric is not used in our experiments since it is not a suitable evaluation metric for BTM ().", "labels": [], "entities": [{"text": "Perplexity metric", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.866630345582962}, {"text": "BTM", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.7283279299736023}]}, {"text": "The first reason is that BTM and LDA optimize different likelihood.", "labels": [], "entities": [{"text": "BTM", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.6606706380844116}]}, {"text": "The second reason is that topic models which have better perplexity may infer less semantically topics ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Hashtags selected for evaluation", "labels": [], "entities": []}]}