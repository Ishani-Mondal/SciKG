{"title": [{"text": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7889525890350342}]}], "abstractContent": [{"text": "Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their inputs , which requires many expensive computations.", "labels": [], "entities": []}, {"text": "We present a simple deep neural network that competes with and, in some cases, outperforms such models on sentiment analysis and factoid question answering tasks while taking only a fraction of the training time.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.967725932598114}, {"text": "factoid question answering tasks", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.7538366913795471}]}, {"text": "While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words models by deepening our network and applying a novel variant of dropout.", "labels": [], "entities": []}, {"text": "Moreover , our model performs better than syntactic models on datasets with high syntactic variance.", "labels": [], "entities": []}, {"text": "We show that our model makes similar errors to syntactically-aware models, indicating that for the tasks we consider , nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models for natural language processing (NLP) represent words using low dimensional vectors called embeddings.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.8124398986498514}]}, {"text": "To apply vector space models to sentences or documents, one must first select an appropriate composition function, which is a mathematical process for combining multiple words into a single vector.", "labels": [], "entities": []}, {"text": "Composition functions fall into two classes: unordered and syntactic.", "labels": [], "entities": []}, {"text": "Unordered functions treat input texts as bags of word embeddings, while syntactic functions take word order and sentence structure into account.", "labels": [], "entities": []}, {"text": "Previously published experimental results have shown that syntactic functions outperform unordered functions on many tasks.", "labels": [], "entities": []}, {"text": "However, there is a tradeoff: syntactic functions require more training time than unordered composition functions and are prohibitively expensive in the case of huge datasets or limited computing resources.", "labels": [], "entities": []}, {"text": "For example, the recursive neural network (Section 2) computes costly matrix/tensor products and nonlinearities at every node of a syntactic parse tree, which limits it to smaller datasets that can be reliably parsed.", "labels": [], "entities": []}, {"text": "We introduce a deep unordered model that obtains near state-of-the-art accuracies on a variety of sentence and document-level tasks with just minutes of training time on an average laptop computer.", "labels": [], "entities": []}, {"text": "This model, the deep averaging network (DAN), works in three simple steps: 1.", "labels": [], "entities": []}, {"text": "take the vector average of the embeddings associated with an input sequence of tokens 2.", "labels": [], "entities": []}, {"text": "pass that average through one or more feedforward layers 3.", "labels": [], "entities": []}, {"text": "perform (linear) classification on the final layer's representation The model can be improved by applying a novel dropout-inspired regularizer: for each training instance, randomly drop some of the tokens' embeddings before computing the average.", "labels": [], "entities": []}, {"text": "We evaluate DANs on sentiment analysis and factoid question answering tasks at both the sentence and document level in Section 4.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9713967740535736}, {"text": "factoid question answering", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7054495314757029}]}, {"text": "Our model's successes demonstrate that for these tasks, the choice of composition function is not as important as initializing with pretrained embeddings and using a deep network.", "labels": [], "entities": []}, {"text": "Furthermore, DANs, unlike more complex composition functions, can be effectively trained on data that have high syntactic variance.", "labels": [], "entities": []}, {"text": "A qualitative analysis of the learned layers suggests that the model works by magnifying tiny but meaningful differences in the vector average through multiple hidden layers, and a detailed error analysis shows that syntactically-aware models actually make very similar errors to those of the more na\u00a8\u0131vena\u00a8\u0131ve DAN.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare DANs to both the shallow NBOW model as well as more complicated syntactic models on sentence and document-level sentiment analysis and factoid question answering tasks.", "labels": [], "entities": [{"text": "sentence and document-level sentiment analysis", "start_pos": 95, "end_pos": 141, "type": "TASK", "confidence": 0.6742867350578308}, {"text": "factoid question answering tasks", "start_pos": 146, "end_pos": 178, "type": "TASK", "confidence": 0.7447059154510498}]}, {"text": "The DAN architecture we use for each task is almost identical, differing across tasks only in the type of output layer and the choice of activation function.", "labels": [], "entities": []}, {"text": "Our results show that DANs outperform other bag-ofwords models and many syntactic models with very little training time.", "labels": [], "entities": []}, {"text": "On the question-answering task, DANs effectively train on out-of-domain data, while RecNNs struggle to reconcile the syntactic differences between the training and test data.", "labels": [], "entities": []}, {"text": "We evaluate over both fine-grained and binary sentence-level classification tasks on the SST, and just the binary task on RT and IMDB.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.8651398420333862}]}, {"text": "In the finegrained SST setting, each sentence has a label from zero to five where two is the neutral class.", "labels": [], "entities": [{"text": "SST", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8410993814468384}]}, {"text": "For the binary task, we ignore all neutral sentences.", "labels": [], "entities": []}, {"text": "DANs require less time per epoch and-in generalrequire fewer epochs than their syntactic counterparts.", "labels": [], "entities": []}, {"text": "We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of; the reported times are fora single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors.", "labels": [], "entities": []}, {"text": "Training a DAN on just sentence-level labels on the SST takes under five minutes on a single core of a laptop; when labeled phrases are added as separate training instances, training time jumps to twenty minutes.", "labels": [], "entities": []}, {"text": "All timing experiments were performed on a single core of an Intel I7 processor with 8GB of RAM.", "labels": [], "entities": []}, {"text": "To test this, we train a DAN over the history questions from.", "labels": [], "entities": []}, {"text": "This dataset is aug- The training set contains 14,219 sentences over 3,761 questions.", "labels": [], "entities": []}, {"text": "For more detail about data and baseline systems, mented with 49,581 sentence/page-title pairs from the Wikipedia articles associated with the answers in the dataset.", "labels": [], "entities": []}, {"text": "For fair comparison with QANTA, we use a normalized tanh activation function at the last layer instead of ReLu, and we also change the output layer from a softmax to the margin ranking loss) used in QANTA.", "labels": [], "entities": []}, {"text": "We initialize the DAN with the same pretrained 100-d word embeddings that were used to initialize QANTA.", "labels": [], "entities": [{"text": "initialize QANTA", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.6515889763832092}]}, {"text": "We also evaluate the effectiveness of word dropout on this task in.", "labels": [], "entities": []}, {"text": "Cross-validation indicates that p = 0.3 works best for question answering, although the improvement inaccuracy is negligible for sentiment analysis.", "labels": [], "entities": [{"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8908179104328156}, {"text": "sentiment analysis", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.9550470411777496}]}, {"text": "Finally, continuing the trend observed in the sentiment experiments, DAN converges much faster than QANTA.", "labels": [], "entities": [{"text": "DAN", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8801361322402954}, {"text": "QANTA", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.8762617707252502}]}, {"text": "shows that while DAN is slightly worse than QANTA when trained only on question-answer pairs, it improves when trained on additional outof-domain Wikipedia data (DAN-WIKI), reaching performance comparable to that of a state-of-the-art information retrieval system (IR-WIKI).", "labels": [], "entities": [{"text": "DAN", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.8772048354148865}, {"text": "QANTA", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.7947620749473572}]}, {"text": "QANTA, in contrast, barely improves when Wikipedia data is added (QANTA-WIKI) possibly due to the syntactic differences between Wikipedia text and quiz bowl question text.", "labels": [], "entities": [{"text": "QANTA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7425029277801514}, {"text": "Wikipedia data", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.8688665926456451}]}], "tableCaptions": [{"text": " Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.", "labels": [], "entities": [{"text": "sentiment accu- racies", "start_pos": 34, "end_pos": 56, "type": "METRIC", "confidence": 0.816137507557869}]}, {"text": " Table 2: The DAN achieves slightly lower accu- racies than the more complex QANTA in much  less training time, even at early sentence posi- tions where compositionality plays a bigger role.  When Wikipedia is added to the training set (bot- tom half of table), the DAN outperforms QANTA", "labels": [], "entities": [{"text": "accu- racies", "start_pos": 42, "end_pos": 54, "type": "METRIC", "confidence": 0.9694560567537943}]}]}