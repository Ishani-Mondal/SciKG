{"title": [], "abstractContent": [{"text": "A convex and feature-rich discriminative approach to dependency grammar induction\u00c9douard induction\u00b4induction\u00c9douard Grave Abstract In this paper, we introduce anew method for the problem of unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency grammar induction\u00c9douard induction\u00b4induction\u00c9douard Grave", "start_pos": 53, "end_pos": 121, "type": "TASK", "confidence": 0.7779800295829773}, {"text": "dependency parsing", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.6823906302452087}]}, {"text": "Most current approaches are based on generative models.", "labels": [], "entities": []}, {"text": "Learning the parameters of such models relies on solving a non-convex optimization problem , thus making them sensitive to initial-ization.", "labels": [], "entities": []}, {"text": "We propose anew convex formulation to the task of dependency grammar induction.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8193369309107462}]}, {"text": "Our approach is discriminative, allowing the use of different kinds of features.", "labels": [], "entities": []}, {"text": "We describe an efficient optimization algorithm to learn the parameters of our model, based on the Frank-Wolfe algorithm.", "labels": [], "entities": []}, {"text": "Our method can easily be generalized to other unsupervised learning problems.", "labels": [], "entities": []}, {"text": "We evaluate our approach on ten languages belonging to four different families , showing that our method is competitive with other state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammar induction is an important problem in computational linguistics.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8627186119556427}, {"text": "computational linguistics", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7531985938549042}]}, {"text": "Despite having recently received a lot of attention, it is still considered to bean unsolved problem.", "labels": [], "entities": []}, {"text": "In this work, we are interested in unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.737888365983963}]}, {"text": "More precisely, our goal is to induce directed dependency trees, which capture binary syntactic relations between the words of a sentence.", "labels": [], "entities": []}, {"text": "Since our method is unsupervised, it does not have access to such syntactic structure and only take as input a corpus of words and their associated parts of speech.", "labels": [], "entities": []}, {"text": "Most recent approaches to unsupervised dependency parsing are based on probabilistic generative models, such as the dependency model with valence introduced by.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7241254150867462}]}, {"text": "Learning the parameters of such models is often All languages have their own grammar done by maximizing the log-likelihood of unlabeled data, leading to a non-convex optimization problem.", "labels": [], "entities": []}, {"text": "Thus, the performance of those methods rely heavily on the initialization, and practitioners have to find good heuristics to initialize their models.", "labels": [], "entities": []}, {"text": "In this paper, we describe a different approach to the problem of dependency grammar induction, inspired by discriminative clustering.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.811213473478953}]}, {"text": "We propose to use a feature-rich discriminative parser, and to learn the parameters of this parser using a convex quadratic objective function.", "labels": [], "entities": []}, {"text": "In particular, this approach also allows us to induce non-projective dependency structures.", "labels": [], "entities": []}, {"text": "Following the work of, we use language-independent rules between pairs of parts-of-speech to guide our parser.", "labels": [], "entities": []}, {"text": "More precisely, we make the following contributions: \u2022 Our method is based on a feature-rich discriminative parser (section 3); \u2022 Learning the parameters of our parser is achieved using a convex objective, and is thus not sensitive to initialization (section 4); \u2022 Our method can produce non-projective dependency structures (section 3.2.2); \u2022 We propose an efficient algorithm to optimize the objective, based on the Frank-Wolfe method (section 5); \u2022 We evaluate our approach on the universal treebanks dataset, showing that it is competitive with the state-of-the-art (section 6).", "labels": [], "entities": [{"text": "universal treebanks dataset", "start_pos": 484, "end_pos": 511, "type": "DATASET", "confidence": 0.6497509280840555}]}], "datasetContent": [{"text": "In this section, we report the results of the experiments we have performed to evaluate our approach to grammar induction.: Directed dependency accuracy, on the universal treebanks with universal parts-ofspeech, on sentences of length 10 or less.", "labels": [], "entities": [{"text": "grammar induction.", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7142407894134521}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9593797326087952}]}, {"text": "PR refers to posterior regularization, USR to universal rules.", "labels": [], "entities": []}, {"text": "We use the universal treebanks, version 2.0, introduced by.", "labels": [], "entities": []}, {"text": "This dataset contains dependency trees for ten languages belonging to five different families: Spanish, French, Italian, Portuguese (Romanic family), English, German, Swedish (Germanic family), Korean, Japanese and Indonesian.", "labels": [], "entities": []}, {"text": "The tokens of those treebanks are tagged using the universal part-ofspeech tagset ().", "labels": [], "entities": []}, {"text": "We focus on inducing dependency grammars using universal parts-of-speech, and will thus report results where all methods use (gold) universal POS.", "labels": [], "entities": []}, {"text": "We also evaluate our method on longer sentences (while still training on sentences of length 10 or less).", "labels": [], "entities": []}, {"text": "Directed dependency accuracies are reported in.", "labels": [], "entities": []}, {"text": "On all sentences, our method achieve an overall accuracy of 55.8.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9995647072792053}]}], "tableCaptions": [{"text": " Table 3: Directed dependency accuracy, on  the universal treebanks with universal parts-of- speech, on sentences of length 10 or less. PR refers  to posterior regularization, USR to universal rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9491363167762756}, {"text": "PR", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9064679145812988}, {"text": "USR", "start_pos": 176, "end_pos": 179, "type": "DATASET", "confidence": 0.6422395706176758}]}, {"text": " Table 5: Comparison between projective and non- projective unsupervised dependency parsing using  our method.", "labels": [], "entities": []}, {"text": " Table 6: Feature ablation study.", "labels": [], "entities": []}]}