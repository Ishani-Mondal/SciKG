{"title": [{"text": "A Hierarchical Knowledge Representation for Expert Finding on Social Media", "labels": [], "entities": [{"text": "Hierarchical Knowledge Representation", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.7603481014569601}]}], "abstractContent": [{"text": "Expert finding on social media benefits both individuals and commercial services.", "labels": [], "entities": []}, {"text": "In this paper, we exploit a 5-level tree representation to model the posts on social media and cast the expert finding problem to the matching problem between the learned user tree and domain tree.", "labels": [], "entities": []}, {"text": "We enhance the traditional approximate tree matching algorithm and incorporate word embeddings to improve the matching result.", "labels": [], "entities": [{"text": "approximate tree matching", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6945096850395203}]}, {"text": "The experiments conducted on Sina Microblog demonstrate the effectiveness of our work.", "labels": [], "entities": [{"text": "Sina Microblog", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.8943759202957153}]}], "introductionContent": [{"text": "Expert finding has been arousing great interests among social media researchers after its successful applications on traditional media like academic publications.", "labels": [], "entities": []}, {"text": "As already observed, social media users tend to follow others for professional interests and knowledge).", "labels": [], "entities": []}, {"text": "This builds the basis for mining expertise and finding experts on social media, which facilitates the services of user recommendation and questionanswering, etc.", "labels": [], "entities": []}, {"text": "Despite the demand to access expertise, the challenges of identifying domain experts on social media exist.", "labels": [], "entities": []}, {"text": "Social media often contains plenty of noises such as the tags with which users describe themselves.", "labels": [], "entities": []}, {"text": "Noises impose the inherent drawback on the feature-based learning methods.", "labels": [], "entities": []}, {"text": "Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models ().", "labels": [], "entities": []}, {"text": "When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated ().", "labels": [], "entities": []}, {"text": "Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties.", "labels": [], "entities": []}, {"text": "Experts cannot be simply judged by the number of followers.", "labels": [], "entities": []}, {"text": "The knowledge conveyed in what they say is essential.", "labels": [], "entities": []}, {"text": "This leads to the failures of the network-based methods ().", "labels": [], "entities": []}, {"text": "The challenges mentioned above inherently come from insufficient representations.", "labels": [], "entities": []}, {"text": "They motivate us to propose a more flexible domain expert finding framework to explore effective representations that are able to tackle the complexity lies in the social media data.", "labels": [], "entities": []}, {"text": "The basic idea is as follows.", "labels": [], "entities": []}, {"text": "Experts talk about the professional knowledge in their posts and these posts are supposed to contain more domain knowledge than the posts from the other ordinary users.", "labels": [], "entities": []}, {"text": "We determine whether or not users are experts on specific domains by matching their professional knowledge and domain knowledge.", "labels": [], "entities": []}, {"text": "The key is how to capture such information for both users and domains with the appropriate representation, which is, in our view, the reason why most of previous work fails.", "labels": [], "entities": []}, {"text": "Togo beyond the feature-based classification methods and the vector representation inference inexpert finding, a potential solution is to incorporate the semantic information for knowledge modeling.", "labels": [], "entities": [{"text": "knowledge modeling", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.7778680920600891}]}, {"text": "We achieve this goal by representing user posts using a hierarchical tree structure to capture correlations among words and topics.", "labels": [], "entities": []}, {"text": "To tackle the data sparseness problem, we apply word embeddings to tree-nodes to further enhance semantic representation and to support semantic matching.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7190182507038116}]}, {"text": "Expert finding is then cast to the problem of determining the edit distance between the user tree and the domain tree, which is computed with an approximate tree matching algorithm.", "labels": [], "entities": []}, {"text": "The main contribution of this work is to integrate the hierarchical tree representation and structure matching together to profile users' and do-mains' knowledge.", "labels": [], "entities": []}, {"text": "Using such trees allows us to flexibly incorporate more information into the data representation, such as the relations between latent topics and the semantic similarities between words.", "labels": [], "entities": []}, {"text": "The experiments conducted on Sina Microblog demonstrate the effectiveness of the proposed framework and the corresponding methods.", "labels": [], "entities": [{"text": "Sina Microblog", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9104797542095184}]}], "datasetContent": [{"text": "The experiments are conducted on 5 domains (i.e., Beauty Blogger, Beauty Doctor, Parenting, ECommerce, and Data Science) in Sina Microblog, a Twitter-like microblog in China.", "labels": [], "entities": []}, {"text": "To learn PAM, we manually select 40 users in each domain from the official expert lists released by Sina Microblog 1 , and crawl all of their posts.", "labels": [], "entities": [{"text": "PAM", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9515589475631714}, {"text": "Sina Microblog 1", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.8512807289759318}]}, {"text": "In average, there are 113,924 posts in each domain.", "labels": [], "entities": []}, {"text": "Notice that the expert lists are not of high quality.", "labels": [], "entities": []}, {"text": "We have to do manual verification to filter out noises.", "labels": [], "entities": []}, {"text": "For evaluation, we select another 80 users in each domain from the expert list, with 40 verified as experts and the other 40 as non-experts.", "labels": [], "entities": []}, {"text": "Since there is no state-of-art Chinese word embeddings publicly available, we use another Sina Microblog dataset provided by pennyliang 2 , which contains 25 million posts and nearly 100 million tokens in total, to learn the word embeddings of 50-dimension.", "labels": [], "entities": [{"text": "Sina Microblog dataset", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.864246149857839}]}, {"text": "We pre-process the data with the Rwordseg segmentation package and discard nonsensical words with the pullword package 4 . When learning 5-level PAM, we set fixed parameters \u03b1 = 0.25, \u03b2 = 0.25 and from top to down, I = 10, J = 20, K = 20 for the number of second, third and fourth levels of topics, respectively.", "labels": [], "entities": [{"text": "Rwordseg segmentation", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.6923132240772247}, {"text": "PAM", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9334473013877869}]}, {"text": "And we initialize \u03b3 and \u03b4 with 0.25.", "labels": [], "entities": []}, {"text": "For tree matching, we define the cost of tree-node substitution operation between word a and b as Eq (1).", "labels": [], "entities": [{"text": "tree matching", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.745161771774292}, {"text": "Eq", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.982528030872345}]}, {"text": "The costs of insertion and deletion operations for treestructure matching are MAX VALUE.", "labels": [], "entities": [{"text": "MAX", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9469954371452332}, {"text": "VALUE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.8172110915184021}]}, {"text": "Here we set MAX VALUE as 100 experimentally.", "labels": [], "entities": [{"text": "MAX", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.6819654107093811}, {"text": "VALUE", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.7699001431465149}]}, {"text": "The threshold \u03bb d used to determine the expert is set to be 12 times of MAX VALUE.", "labels": [], "entities": [{"text": "MAX", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9856758117675781}, {"text": "VALUE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.7715290188789368}]}], "tableCaptions": []}