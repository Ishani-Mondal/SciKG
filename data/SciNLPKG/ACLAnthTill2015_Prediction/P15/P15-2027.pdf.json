{"title": [{"text": "Efficient Learning for Undirected Topic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents.", "labels": [], "entities": [{"text": "extracting semantic representations of documents", "start_pos": 78, "end_pos": 126, "type": "TASK", "confidence": 0.8521375298500061}]}, {"text": "Traditional learning strategies such as Contrastive Divergence are very inefficient.", "labels": [], "entities": [{"text": "Contrastive Divergence", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8419418036937714}]}, {"text": "This paper provides a novel esti-mator to speedup the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs.", "labels": [], "entities": []}, {"text": "Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9991106390953064}, {"text": "document retrieval and classification", "start_pos": 114, "end_pos": 151, "type": "TASK", "confidence": 0.6978010684251785}]}], "introductionContent": [{"text": "Topic models are powerful probabilistic graphical approaches to analyze document semantics in different applications such as document categorization and information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.8140811324119568}]}, {"text": "They are mainly constructed by directed structure like pLSA) and LDA ().", "labels": [], "entities": []}, {"text": "Accompanied by the vast developments in deep learning, several undirected topic models, such as, have recently been reported to achieve great improvements in efficiency and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9945846199989319}]}, {"text": "Replicated Softmax model (RSM), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs).", "labels": [], "entities": []}, {"text": "Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD).", "labels": [], "entities": [{"text": "RSM", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9864037036895752}]}, {"text": "However, CD is not really designed for RSM.", "labels": [], "entities": [{"text": "RSM", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.972429096698761}]}, {"text": "Different from RBMs with binary input, RSM adopts softmax units to represent words, resulting in great inefficiency with sampling inside CD, especially fora large vocabulary.", "labels": [], "entities": [{"text": "RSM", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9527782797813416}]}, {"text": "Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application.", "labels": [], "entities": []}, {"text": "Dealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems.", "labels": [], "entities": []}, {"text": "pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and solved it based on a hierarchical binary tree.", "labels": [], "entities": []}, {"text": "A similar architecture was used in word representations like.", "labels": [], "entities": []}, {"text": "Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well.", "labels": [], "entities": [{"text": "RSM", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9404109716415405}]}, {"text": "For instance, found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation.", "labels": [], "entities": []}, {"text": "proposed Ratio Matching (RM) to train unnormalized models, and added stochastic approaches in RM to accommodate high-dimensional inputs.", "labels": [], "entities": [{"text": "Ratio Matching (RM)", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8273294806480408}]}, {"text": "Recently, anew estimator Noise Contrastive Estimate (NCE)) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in.", "labels": [], "entities": [{"text": "estimator Noise Contrastive Estimate (NCE))", "start_pos": 15, "end_pos": 58, "type": "METRIC", "confidence": 0.6126092161451068}]}, {"text": "In this paper, we propose an efficient learning strategy for RSM named \u03b1-NCE, applying NCE as the basic estimator.", "labels": [], "entities": [{"text": "RSM", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9680272936820984}]}, {"text": "Different from most related efforts that use NCE for predicting single word, our method extends NCE to generate noise for documents invariant lengths.", "labels": [], "entities": []}, {"text": "It also enables RSM to use weighted inputs to improve the modelling ability.", "labels": [], "entities": [{"text": "RSM", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9773046970367432}]}, {"text": "As RSM is usually used as the first layer in many deeper undirected models like Deep Boltzmann Machines (, \u03b1-NCE can be readily extended to learn them efficiently.", "labels": [], "entities": [{"text": "RSM", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9568060040473938}]}], "datasetContent": [{"text": "We evaluated the new estimator to train RSMs on two text datasets: 20 Newsgroups and IMDB.", "labels": [], "entities": [{"text": "RSMs", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.9845736622810364}, {"text": "IMDB", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.933506429195404}]}, {"text": "The 20 Newsgroups 2 dataset is a collection of the Usenet posts, which contains 11,345 training and 7,531 testing instances.", "labels": [], "entities": [{"text": "20 Newsgroups 2 dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.6947111487388611}, {"text": "Usenet posts", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.8930467367172241}]}, {"text": "Both the training and testing sets are labeled into 20 classes.", "labels": [], "entities": []}, {"text": "Removing stop words as well as stemming were performed.", "labels": [], "entities": []}, {"text": "The IMDB dataset 3 is a benchmark for sentiment analysis, which consists of 100,000 movie reviews taken from IMDB.", "labels": [], "entities": [{"text": "IMDB dataset 3", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9376203219095866}, {"text": "sentiment analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9756897985935211}, {"text": "IMDB", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.949357807636261}]}, {"text": "The dataset is divided into 75,000 training instances (1/3 labeled and 2/3 unlabeled) and 25,000 testing instances.", "labels": [], "entities": []}, {"text": "Two types of labels, positive and negative, are given to show sentiment.", "labels": [], "entities": []}, {"text": "Following, no stop words are removed from this dataset.", "labels": [], "entities": []}, {"text": "For each dataset, we randomly selected 10% of the training set for validation, and the idf -weight vector is computed in advance.", "labels": [], "entities": [{"text": "validation", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.9788708686828613}]}, {"text": "In addition, replacing the word count\u02c6vcount\u02c6 count\u02c6v by log (1 + \u02c6 v) slightly improved the modelling performance for all models.", "labels": [], "entities": []}, {"text": "We implemented \u03b1-NCE according to the parameter settings in using SGD in minibatches of size 128 and an initialized learning rate of 0.1.", "labels": [], "entities": []}, {"text": "We also implemented CD with the same settings.", "labels": [], "entities": []}, {"text": "All the experiments were run on a single GPU GTX970 using the library.", "labels": [], "entities": []}, {"text": "To make the comparison fair, both \u03b1-NCE and CD share the same implementation.", "labels": [], "entities": []}, {"text": "To evaluate the efficiency in learning, we used the most frequent words as dictionaries with sizes ranging from 100 to 20, 000 for both datasets, and test the computation time both for CD of variant Gibbs steps and \u03b1-NCE of variant noise sample sizes.", "labels": [], "entities": []}, {"text": "The comparison of the mean running, which is averaged on both datasets.", "labels": [], "entities": []}, {"text": "Typically, \u03b1-NCE achieves 10 to 500 times speed-up compared to CD.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9946350455284119}]}, {"text": "Although both CD and \u03b1-NCE run slower when the input dimension increases, CD tends to take much more time due to the multinomial sampling at each iteration, especially when more Gibbs steps are used.", "labels": [], "entities": []}, {"text": "In contrast, running time stays reasonable in \u03b1-NCE even if a larger noise size or a larger dimension is applied.", "labels": [], "entities": []}, {"text": "One direct measure to evaluate the modelling performance is to assess RSM as a generative model to estimate the log-probability per word as perplexity.", "labels": [], "entities": [{"text": "RSM", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.961135745048523}]}, {"text": "However, as \u03b1-NCE learns RSM by distinguishing the data and noise from their respective features, parameters are trained more like a feature extractor than a generative model.", "labels": [], "entities": [{"text": "RSM", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9789532423019409}]}, {"text": "It is not fair to use perplexity to evaluate the performance.", "labels": [], "entities": []}, {"text": "For this reason, we evaluated the modelling performance with some indirect measures.", "labels": [], "entities": []}, {"text": "Figure 2: Precision-Recall curves for the retrieval task on the 20 Newsgroups dataset using RSMs.", "labels": [], "entities": [{"text": "Precision-Recall", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9861606359481812}, {"text": "retrieval task", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.8787571489810944}, {"text": "20 Newsgroups dataset", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.8334171374638876}]}, {"text": "For 20 Newsgroups, we trained RSMs on the training set, and reported the results on document retrieval and document classification.", "labels": [], "entities": [{"text": "RSMs", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9589871764183044}, {"text": "document retrieval", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7258139848709106}, {"text": "document classification", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.7733215689659119}]}, {"text": "For retrieval, we treated the testing set as queries, and retrieved documents with the same labels in the training set by cosine-similarity.", "labels": [], "entities": []}, {"text": "Precision-recall (P-R) curves and mean average precision (MAP) are two metrics we used for evaluation.", "labels": [], "entities": [{"text": "Precision-recall", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9646633267402649}, {"text": "mean average precision (MAP)", "start_pos": 34, "end_pos": 62, "type": "METRIC", "confidence": 0.9509501357873281}]}, {"text": "For classification, we trained a softmax regression on the training set, and checked the accuracy on the testing set.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9838429689407349}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9996963739395142}]}, {"text": "We use this dataset to show the modelling ability of RSM with different estimators.", "labels": [], "entities": [{"text": "RSM", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9611373543739319}]}, {"text": "For IMDB, the whole training set is used for learning RSMs, and an L2-regularized logistic regression is trained on the labeled training set.", "labels": [], "entities": [{"text": "RSMs", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.853451669216156}]}, {"text": "The error rate of sentiment classification on the testing set is reported, compared with several BoW-based baselines.", "labels": [], "entities": [{"text": "error", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9816935658454895}, {"text": "sentiment classification", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.9074802100658417}, {"text": "BoW-based baselines", "start_pos": 97, "end_pos": 116, "type": "DATASET", "confidence": 0.8469020128250122}]}, {"text": "We use this dataset to show the general modelling ability of RSM compared with others.", "labels": [], "entities": [{"text": "RSM", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9701330065727234}]}, {"text": "We trained both \u03b1-NCE and CD, and naturally NCE (without UCE) at a fixed vocabulary size (2000 for 20 Newsgroups, and 5000 for IMDB).", "labels": [], "entities": []}, {"text": "Posteriors of the hidden units were used as topic features.", "labels": [], "entities": []}, {"text": "For \u03b1-NCE , we fixed noise level at 0.5 for 20 Newsgroups and 0.3 for IMDB.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8737805485725403}]}, {"text": "In comparison, we trained CD from 1 up to 5 Gibbs steps. and show that a larger noise size in \u03b1-NCE achieves better modelling perfor- mance, and \u03b1-NCE greatly outperforms CD on retrieval tasks especially around large recall values.", "labels": [], "entities": [{"text": "recall", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.9556127786636353}]}, {"text": "The classification results of \u03b1-NCE is also comparable or slightly better than CD.", "labels": [], "entities": []}, {"text": "Simultaneously, it is gratifying to find that the idf -weighting inputs achieve the best results both in retrieval and classification tasks, as idf -weighting is known to extract information better than word count.", "labels": [], "entities": []}, {"text": "In addition, naturally NCE performs poorly compared to others in    On the other hand, shows the performance of RSM in sentiment classification, where model combinations reported in previous efforts are not considered.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.96279576420784}]}, {"text": "It is clear that \u03b1-NCE learns RSM better than CD, and outperforms BoW and other BoW-based models 4 such as LDA.", "labels": [], "entities": [{"text": "RSM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9681219458580017}, {"text": "BoW", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9659520983695984}]}, {"text": "The idf -4 Accurately, WRRBM uses \"bag of n-grams\" assumption.", "labels": [], "entities": [{"text": "Accurately", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9727736115455627}, {"text": "WRRBM", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8416551947593689}]}, {"text": "weighting inputs also achieve the best performance.", "labels": [], "entities": []}, {"text": "Note that RSM is also based on BoW, indicating \u03b1-NCE has arguably reached the limits of learning BoW-based models.", "labels": [], "entities": [{"text": "RSM", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.959985077381134}, {"text": "BoW", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8600496053695679}]}, {"text": "In future work, RSM can be extended to more powerful undirected topic models, by considering more syntactic information such as word-order or dependency relationship in representation.", "labels": [], "entities": [{"text": "RSM", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9840302467346191}]}, {"text": "\u03b1-NCE can be used to learn them efficiently and achieve better performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of classification accuracy on  the 20 Newsgroups dataset using RSMs.", "labels": [], "entities": [{"text": "classification", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.9081667065620422}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9091270565986633}, {"text": "20 Newsgroups dataset", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.7839580972989401}]}, {"text": " Table 2: The performance of sentiment classifica- tion accuracy on the IMDB dataset using RSMs  compared to other BoW-based approaches.", "labels": [], "entities": [{"text": "sentiment classifica- tion", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7506844103336334}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.7985503673553467}, {"text": "IMDB dataset", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.9717788696289062}]}]}