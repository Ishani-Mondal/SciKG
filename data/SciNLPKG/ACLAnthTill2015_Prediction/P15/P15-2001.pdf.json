{"title": [{"text": "A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.761389285326004}]}, {"text": "Other languages, even those that are widely spoken such as Span-ish, do not have a reliable word similarity evaluation framework.", "labels": [], "entities": []}, {"text": "We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels.", "labels": [], "entities": []}, {"text": "We propose an automatic standardization for the construction of cross-lingual similarity datasets, and provide an evaluation, demonstrating its reliability and robustness.", "labels": [], "entities": []}, {"text": "Based on our procedure and taking the RG-65 word similarity dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and fifteen cross-lingual datasets for six languages: English, Spanish, French, German, Por-tuguese, and Farsi.", "labels": [], "entities": [{"text": "RG-65 word similarity dataset", "start_pos": 38, "end_pos": 67, "type": "DATASET", "confidence": 0.7023671120405197}]}], "introductionContent": [{"text": "Semantic similarity is afield of Natural Language Processing which measures the extent to which two linguistic items are similar.", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8282643854618073}]}, {"text": "In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations.", "labels": [], "entities": []}, {"text": "Applications of word similarity range from Word Sense Disambiguation ( to Machine Translation (, Information Retrieval (), Question Answering), Text Summarization (, Ontology Alignment , and Lexical Substitution (.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7477916479110718}, {"text": "Word Sense Disambiguation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7340745329856873}, {"text": "Machine Translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7731395959854126}, {"text": "Information Retrieval", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7038065046072006}, {"text": "Question Answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7860297560691833}, {"text": "Text Summarization", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.8059222400188446}, {"text": "Ontology Alignment", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7095809578895569}, {"text": "Lexical Substitution", "start_pos": 191, "end_pos": 211, "type": "TASK", "confidence": 0.8211082518100739}]}, {"text": "However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (;), up until the recent creation of datasets built by translating the English RG-65 dataset into French (), German, and Portuguese ().", "labels": [], "entities": [{"text": "RG-65 dataset", "start_pos": 218, "end_pos": 231, "type": "DATASET", "confidence": 0.8592000305652618}]}, {"text": "And what is more, cross-lingual applications have grown in importance over the last few years ().", "labels": [], "entities": []}, {"text": "Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems.", "labels": [], "entities": []}, {"text": "This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word similarity datasets based on RG-65, covering six languages, by proposing an improved version of the approach of for the automatic construction of cross-lingual datasets from aligned monolingual datasets.", "labels": [], "entities": [{"text": "RG-65 dataset", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.8378266096115112}, {"text": "Pearson correlation", "start_pos": 227, "end_pos": 246, "type": "METRIC", "confidence": 0.960529237985611}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2.", "labels": [], "entities": []}, {"text": "We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3.", "labels": [], "entities": [{"text": "Spanish and Farsi word similarity datasets", "start_pos": 73, "end_pos": 115, "type": "DATASET", "confidence": 0.6676618109146754}]}, {"text": "Section 4 provides the details of our algorithm for the automatic construction of the cross-lingual datasets.", "labels": [], "entities": []}, {"text": "We report the results of the evaluation performed on the generated datasets in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we specify the released resources in Section 6, followed by concluding remarks in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we explain our methodology for the construction of the Spanish and Farsi versions of the English RG-65 dataset).", "labels": [], "entities": [{"text": "Spanish and Farsi versions of the English RG-65 dataset", "start_pos": 71, "end_pos": 126, "type": "DATASET", "confidence": 0.7143509321742587}]}, {"text": "The methodology is divided into two main steps: First, the original English dataset is translated into the target language (Section 3.1) and then, the newly translated pairs are scored by human annotators (Section 3.2).", "labels": [], "entities": []}, {"text": "Twelve native Spanish speakers were asked to evaluate the similarity for the Spanish translations.", "labels": [], "entities": [{"text": "similarity", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9691531658172607}]}, {"text": "In order to obtain a more global distribution of judges, we included judges both both Spain and Latin America.", "labels": [], "entities": []}, {"text": "As far as the Farsi dataset was concerned, twelve Farsi native speakers scored the newly translated pairs.", "labels": [], "entities": [{"text": "Farsi dataset", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.7203355878591537}]}, {"text": "The guidelines provided to the annotators were based on the recent SemEval task on Cross-Level Semantic Similarity (, which provides clear indications in order to distinguish similarity and relatedness.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.8680713176727295}]}, {"text": "The annotators were allowed to give scores from 0 to 4, with a step size of 0.5.", "labels": [], "entities": []}, {"text": "shows example pairs with their corresponding scores from the English and the newly created Spanish and Farsi versions of the RG-65 dataset.", "labels": [], "entities": [{"text": "RG-65 dataset", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.955858051776886}]}, {"text": "As we can see from the table, the scores across languages are not necessarily identical, with small, in a few cases significant, differences between the corresponding scores.", "labels": [], "entities": []}, {"text": "This is due to the fact that associated senses with words do not hold one-to-one correspondence across different languages.", "labels": [], "entities": []}, {"text": "This renders the approach of Hassan and Mihalcea (2009) insufficiently accurate for handling these differences.", "labels": [], "entities": []}, {"text": "In this section we present our automatic method for building cross-lingual datasets.", "labels": [], "entities": []}, {"text": "Although being targeted at building semantic similarity datasets, the algorithm is task-independent, so it may also be used for any task which measures any kind of relation between two linguistic items in a numerical way.", "labels": [], "entities": []}, {"text": "proposed a method which exploits two aligned monolingual word similarity datasets for the construction of a French-English cross-lingual dataset.", "labels": [], "entities": []}, {"text": "We followed their initial idea and proposed a generalization of the approach which would be capable of automatically constructing reliable cross-lingual similarity datasets for any pair of languages.", "labels": [], "entities": []}, {"text": "Algorithm 1 shows our procedure for constructing a cross-lingual dataset starting from two monolingual datasets.", "labels": [], "entities": []}, {"text": "Note that the pairs in the two monolingual datasets should be previously aligned.", "labels": [], "entities": []}, {"text": "Specifically, we refer to each dataset D as {P D , SD }, where PD is the set of pairs and SD is a function mapping each pair in PD to a value on a similarity scale (0-4 for RG-65).", "labels": [], "entities": []}, {"text": "For each two aligned pairs a-b and a'-b' across the two datasets, if the difference in the corresponding scores is greater than a quarter of the similarity scale size (1.0 in RG-65), the pairs are not considered (line 7) and therefore discarded.", "labels": [], "entities": [{"text": "similarity scale size", "start_pos": 145, "end_pos": 166, "type": "METRIC", "confidence": 0.9499310255050659}, {"text": "RG-65", "start_pos": 175, "end_pos": 180, "type": "DATASET", "confidence": 0.7436327338218689}]}, {"text": "Otherwise, two new pairs a-b' and a'-b are created with a score equal to the average of the two original pairs' scores (lines 8-11 and 15-18).", "labels": [], "entities": []}, {"text": "In the case of repeated pairs, we merge them into a single pair with a similarity equal to their average score (lines 12-14 and lines 19-21).", "labels": [], "entities": []}, {"text": "By following this procedure we created fifteen cross-lingual datasets based on the RG-65 word similarity datasets for English, French, German, Spanish, Portuguese, and Farsi.", "labels": [], "entities": [{"text": "RG-65 word similarity datasets", "start_pos": 83, "end_pos": 113, "type": "DATASET", "confidence": 0.7640450298786163}]}, {"text": "shows Algorithm 1 Automatic construction of crosslingual similarity datasets where PX is the set of pairs in dataset X and SX is the mapping of these pairs to their corresponding scores.", "labels": [], "entities": []}, {"text": "Output: a cross-lingual semantic similarity dataset C = {PC , SC } 1: PC \u2190 \u2205 2: Define Cnt, which counts how many times an output cross-lingual pair is repeated 3: for each aligned pairs (a, b) \u2208 PD, (a , b ) \u2208 PD 4: avg score = (score + score )/2 7: if |score \u2212 score | \u2264 size(sim scale)/4 then 8: if (a, b ) \u2208 PC then 9: SC (a, b ) = avg score 11: Cnt(a, b ) = 1 12: SC (a , b) = avg score 18: Cnt(a , b) = 1 19: Cnt(a ,b)+1   The inter-annotator agreements according to the average pairwise Pearson correlation among the judges for the newly created Spanish and Farsi datasets are, respectively, 0.83 and 0.88, which maybe used as upper bounds for evaluating automatic systems.", "labels": [], "entities": [{"text": "pairwise Pearson correlation", "start_pos": 485, "end_pos": 513, "type": "METRIC", "confidence": 0.7101073861122131}, {"text": "Farsi datasets", "start_pos": 565, "end_pos": 579, "type": "DATASET", "confidence": 0.7322698533535004}]}, {"text": "Our further analysis revealed that for both datasets no annotator obtained an average Pearson correlation with the rest of the annotators lower than 0.80, which attests to the reliability of our judges and guidelines.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 86, "end_pos": 105, "type": "METRIC", "confidence": 0.9864629507064819}]}, {"text": "The German () and Portuguese () versions of the RG-65 dataset reported a lower inter-annotator agreement of 0.81 and 0.71, respectively, whereas the original English RG-65 reported an inter-annotator agreement of 0.85 fora subset of fifteen judges.", "labels": [], "entities": [{"text": "RG-65 dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9029469192028046}]}, {"text": "As also mentioned earlier, the French version (Joubarne and Inkpen, 2011) did not report any inter-annotator agreement.", "labels": [], "entities": [{"text": "French version (Joubarne and Inkpen, 2011)", "start_pos": 31, "end_pos": 73, "type": "DATASET", "confidence": 0.9116549160745409}]}, {"text": "Along with the monolingual evaluation, we also performed an evaluation on four of the automatically created cross-lingual datasets.", "labels": [], "entities": []}, {"text": "The evaluated language pairs were Spanish-English, SpanishFrench, Spanish-German, and English-Farsi.", "labels": [], "entities": [{"text": "SpanishFrench", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9397594928741455}]}, {"text": "In each case a proficient speaker of both languages was selected to carryout the evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sample word pairs from the English and the newly created Spanish and Farsi RG-65 datasets.", "labels": [], "entities": [{"text": "Sample word pairs", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8055391709009806}, {"text": "Farsi RG-65 datasets", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.8723714550336202}]}, {"text": " Table 2: Number of word pairs for each cross- lingual dataset (EN: English, FR: French, DE:  German, ES: Spanish, PT: Portuguese, FA: Farsi).", "labels": [], "entities": [{"text": "FA: Farsi", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.8701605995496114}]}]}