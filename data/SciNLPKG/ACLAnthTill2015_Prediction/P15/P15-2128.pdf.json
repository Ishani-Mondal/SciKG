{"title": [{"text": "Aspect-Level Cross-lingual Sentiment Classification with Constrained SMT", "labels": [], "entities": [{"text": "Aspect-Level Cross-lingual Sentiment Classification", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6284066960215569}, {"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.8436180353164673}]}], "abstractContent": [{"text": "Most cross-lingual sentiment classification (CLSC) research so far has been performed at sentence or document level.", "labels": [], "entities": [{"text": "cross-lingual sentiment classification (CLSC)", "start_pos": 5, "end_pos": 50, "type": "TASK", "confidence": 0.8816274503866831}]}, {"text": "Aspect-level CLSC, which is more appropriate for many applications, presents the additional difficulty that we consider sub-sentential opinionated units which have to be mapped across languages.", "labels": [], "entities": []}, {"text": "In this paper , we extend the possible cross-lingual sentiment analysis settings to aspect-level specific use cases.", "labels": [], "entities": [{"text": "cross-lingual sentiment analysis", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.7287148733933767}]}, {"text": "We propose a method, based on constrained SMT, to transfer opinionated units across languages by preserving their boundaries.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9787061810493469}]}, {"text": "We show that cross-language sentiment classifiers built with this method achieve comparable results to monolingual ones, and we compare different cross-lingual settings.", "labels": [], "entities": [{"text": "cross-language sentiment classifiers", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.6959216892719269}]}], "introductionContent": [{"text": "Sentiment analysis (SA) is the task of analysing opinions, sentiments or emotions expressed towards entities such as products, services, organisations, issues, and the various attributes of these entities.", "labels": [], "entities": [{"text": "Sentiment analysis (SA) is the task of analysing opinions, sentiments or emotions expressed towards entities such as products, services, organisations, issues, and the various attributes of these entities", "start_pos": 0, "end_pos": 204, "type": "Description", "confidence": 0.7832896777561733}]}, {"text": "The analysis maybe performed at the level of a document (blog post, review) or sentence.", "labels": [], "entities": []}, {"text": "However, this is not appropriate for many applications because the same document or sentence can contain positive opinions towards specific aspects and negative ones towards other aspects.", "labels": [], "entities": []}, {"text": "Thus a finer analysis can be conducted at the level of the aspects of the entities towards which opinions are expressed, identifying for each opinionated unit elements such as its target, polarity and the polar words used to qualify the target.", "labels": [], "entities": []}, {"text": "The two main SA approaches presented in the literature are (i) a machine learning approach, mostly supervised learning with features such as opinion words, dependency information, opinion shifters and quantifiers and (ii) a lexicon-based approach, based on rules involving opinion words and phrases, opinion shifters, contrary clauses (but), etc.", "labels": [], "entities": []}, {"text": "Thus inmost SA systems we may distinguish three types of resources and text: TRAIN Resources (collection of training examples, lexicons) used to train the classifier.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9907181859016418}]}, {"text": "TEST Opinions to be analysed.", "labels": [], "entities": [{"text": "TEST", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.809085488319397}]}, {"text": "OUT Outcome of the analysis.", "labels": [], "entities": [{"text": "OUT", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.920566737651825}, {"text": "Outcome", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.6524502635002136}]}, {"text": "It depends on the level of granularity.", "labels": [], "entities": []}, {"text": "At the document or sentence level, it is the polarity of each document or sentence.", "labels": [], "entities": []}, {"text": "At the aspect level, it may the set of opinion targets with their polarity.", "labels": [], "entities": []}, {"text": "The internet multilingualism and the globalisation of products and services create situations in which these three types of resources are not all in the same language.", "labels": [], "entities": []}, {"text": "In these situations, a language transfer is needed at some point to perform the SA analysis or to understand its results, thus called cross-lingual sentiment analysis (CLSA).", "labels": [], "entities": [{"text": "SA analysis", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.883161336183548}, {"text": "cross-lingual sentiment analysis (CLSA)", "start_pos": 134, "end_pos": 173, "type": "TASK", "confidence": 0.8130587836106619}]}, {"text": "Sentences or documents are handy granularity levels for CLSA because the labels are not related to specific tokens and thus are not affected by a language transfer.", "labels": [], "entities": []}, {"text": "At the aspect level, labels are attached to a specific opinionated unit formed by a sequence of tokens.", "labels": [], "entities": []}, {"text": "When transferring these annotations into another language, the opinionated units in the two languages have thus to be mapped.", "labels": [], "entities": []}, {"text": "This paper is one of the first ones to address CLSA at aspect level (see Section 3).", "labels": [], "entities": []}, {"text": "It makes the following specific contributions: (i) an extended definition of CLSA including use cases and settings specific to aspect-level analyses (Section 2); (ii) a method to perform the language transfer preserving the opinionated unit boundaries.", "labels": [], "entities": [{"text": "language transfer", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.7476850152015686}]}, {"text": "This avoids the need of mapping source and target opinionated units after the language transfer via methods such as word alignment (Section 4); The paper also reports (in Section 5) experiments comparing different settings described in Section 2.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.7436890304088593}]}], "datasetContent": [{"text": "In order to compare CLSA settings a and b (of use case I), we needed data with opinion annotations at the aspect level, in two different languages and in the same domain.", "labels": [], "entities": []}, {"text": "We used the OpeNER 4 opinion corpus, and more specifically the opinion expression and polarity label annotations of the hotel review component, in Spanish and English.", "labels": [], "entities": [{"text": "OpeNER 4 opinion corpus", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.8954325914382935}]}, {"text": "We split the data in training (train) and evaluation (test) sets as indicated in.", "labels": [], "entities": []}, {"text": "The SMT system was trained on freely avail-Source: On the other hand <zone> <x translation=\"ou1-P\">x</x> <wall/> a big advantage <wall/> <x translation=\"/ou1\">x</x> </zone> of the hostel is its placement Translation: por otra parte <ou1-P>una gran ventaja</ou1> del hostal es su colocaci\u00f3n  From these in-domain data we extracted 100k and 50k word corpora, respectively for data selection and language model (LM) interpolation tuning.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.989236056804657}, {"text": "language model (LM) interpolation tuning", "start_pos": 393, "end_pos": 433, "type": "TASK", "confidence": 0.6387663270745959}]}, {"text": "We selected the data closest to the domain in the English-Spanish parallel corpora via a crossentropy-based method, using the open source XenC tool.", "labels": [], "entities": []}, {"text": "The size of available and selected corpora are indicated in the first 4 rows of.", "labels": [], "entities": []}, {"text": "The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2).", "labels": [], "entities": []}, {"text": "We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.", "labels": [], "entities": [{"text": "Moses Experiment Management System (Koehn, 2010)", "start_pos": 8, "end_pos": 56, "type": "DATASET", "confidence": 0.9348681370417277}, {"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9937270879745483}]}, {"text": "Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scored by the Spanish LM than with the English LM, and conversely for the English side.", "labels": [], "entities": []}, {"text": "We conducted supervised sentiment classification experiments for settings a and b of use case I (see Section 2).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.9472598731517792}]}, {"text": "We trained and evaluated classifiers on the annotated data, using as features the tokens (unigrams) within opinion expressions, and SP (Strong Positive), P (Positive), N (Negative) and SN (Strong Negative) as la-6 http://www.statmt.org/wmt13/translation-task.html We kept selected parallel data of the common crawl corpus for tuning and test.", "labels": [], "entities": []}, {"text": "We obtained BLEU scores of 42 and 45 in the English-Spanish and Spanish-English directions.: Experiments corresponding to group of rows 1 of.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9994794726371765}]}, {"text": "\"mono\" refers to monolingual and \"CL a\" and \"CL b\" refer to settings a and b of use case I (Sec. 2). bels.", "labels": [], "entities": []}, {"text": "We performed the experiments with the weka toolkit (, using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm.", "labels": [], "entities": []}, {"text": "represents the experiments conducted with the EN test set.", "labels": [], "entities": [{"text": "EN test set", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9477190971374512}]}, {"text": "A monolingual classifier in English is trained with the EN training set, and evaluated with the EN test set.", "labels": [], "entities": [{"text": "EN training set", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.9095630844434103}, {"text": "EN test set", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.9331550598144531}]}, {"text": "To evaluate cross-lingual setting a, the ES training set is translated into English (see Section 4), and an English classifier is trained on the translated data and evaluated on the EN test set (1 CL a).", "labels": [], "entities": [{"text": "ES training set", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.7812468210856119}, {"text": "EN test set", "start_pos": 182, "end_pos": 193, "type": "DATASET", "confidence": 0.8824406663576762}]}, {"text": "To evaluate setting b, the EN test set is translated into Spanish, and this translated testis used to evaluate a classifier trained on the ES training set (1 CL b).", "labels": [], "entities": [{"text": "EN test set", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.8460859060287476}, {"text": "ES training set", "start_pos": 139, "end_pos": 154, "type": "DATASET", "confidence": 0.7665192286173502}]}, {"text": "With this very simple classifier, we achieve up to 83.4% accuracy in the monolingual case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9986163377761841}]}, {"text": "With cross-lingual settings, we loose from about 4% to 8% accuracy, and with the higher quality SMT system (LM filter), CL-b setting is slightly better than CL-a.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9993459582328796}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.983269453048706}]}, {"text": "The same three experiments were conducted for the ES test set (last three rows of).", "labels": [], "entities": [{"text": "ES test set", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.912123441696167}]}, {"text": "We achieved an accuracy of 81.1% in the monolingual case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996943473815918}]}, {"text": "Here the CL-b setting achieved a clearly better accuracy than the CL-a setting (at least 5% more), and only from 2.3% to 3.5% below the monolingual one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993200302124023}]}, {"text": "Thus with the higher quality SMT system, it is always better to translate the test data (CL-b setting) than the training corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9938082098960876}]}, {"text": "Comparing the SVM classification accuracy in the \"LM Filter\" and \"No Fil\" columns, we can seethe effect of introducing noise in the MT system.", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6319921910762787}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.5752283334732056}, {"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.8661904335021973}]}, {"text": "We observe that the results were more affected by the translation of the test (-2.2% and -0.8% accuracy) than the training set (+0.5% accuracy in both cases).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.998371422290802}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9962024092674255}]}, {"text": "This agrees with the intuition than errors in the test directly affect the results and thus maybe more harmful than in the training set, where they may hardly affect the results if they represent infrequent examples.", "labels": [], "entities": []}, {"text": "Regarding use case II, setting c implies a translation of the analysis outcome.", "labels": [], "entities": []}, {"text": "We can use our method to translate the relevant opinionated units with their predicted label in their test sentence context, and extract the relevant information in the outcome language.", "labels": [], "entities": []}, {"text": "In setting d, the testis translated in the same way as insetting b.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents (Docs), words and  opinionated units (Op. Units) in the OpeNER an- notated data for English (EN) and Spanish (ES).", "labels": [], "entities": [{"text": "opinionated units (Op. Units)", "start_pos": 49, "end_pos": 78, "type": "METRIC", "confidence": 0.6636912567274911}, {"text": "OpeNER an- notated data", "start_pos": 86, "end_pos": 109, "type": "DATASET", "confidence": 0.7681877493858338}]}, {"text": " Table 2: Size of the available and selected corpora  (in million words) in English (EN) and Spanish  (ES) used to train the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9950929880142212}]}, {"text": " Table 3: Accuracy (in %) achieved by the different  systems. LM Filter and No Fil(ter) refer to the  presence or not of the LM filter for the common  crawl parallel corpus. \"Bag.\" refers to bagging.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985285997390747}, {"text": "No Fil(ter)", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.8268756866455078}]}]}