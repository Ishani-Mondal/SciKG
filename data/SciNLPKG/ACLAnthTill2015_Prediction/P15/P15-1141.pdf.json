{"title": [{"text": "A Computationally Efficient Algorithm for Learning Topical Collocation Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Most existing topic models make the bag-of-words assumption that words are generated independently, and so ignore potentially useful information about word order.", "labels": [], "entities": []}, {"text": "Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach, restricted attention to bi-grams, or resulted in models whose inference does not scale to large corpora.", "labels": [], "entities": []}, {"text": "This paper studies how to simultaneously learn both collocations and their topic assignments.", "labels": [], "entities": []}, {"text": "We present an efficient reformula-tion of the Adaptor Grammar-based topical collocation model (AG-colloc) (John-son, 2010), and develop a point-wise sampling algorithm for posterior inference in this new formulation.", "labels": [], "entities": []}, {"text": "We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising inference.", "labels": [], "entities": []}, {"text": "Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model.", "labels": [], "entities": [{"text": "text classification", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.810415118932724}, {"text": "information retrieval", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8022752702236176}]}], "introductionContent": [{"text": "Probabilistic topic models like Latent Dirichlet Allocation (LDA) () are commonly used to study the meaning of text by identifying a set of latent topics from a collection of documents and assigning each word in these documents to one of the latent topics.", "labels": [], "entities": []}, {"text": "A document is modelled as a mixture of latent topics, and each topic is a distribution over a finite vocabulary of words.", "labels": [], "entities": []}, {"text": "It is common for topic models to treat documents as bags-of-words, ignoring any internal structure.", "labels": [], "entities": []}, {"text": "While this simplifies posterior inference, it also ignores the information encoded in, for example, syntactic relationships , word order () and the topic structure of documents ().", "labels": [], "entities": []}, {"text": "Here we are interested in topic models that capture dependencies between adjacent words in a topic dependent way.", "labels": [], "entities": []}, {"text": "For example, the phrase \"white house\" can be interpreted compositionally in a real-estate context, but not in apolitical context.", "labels": [], "entities": []}, {"text": "Several extensions of LDA have been proposed that assign topics not only to individual words but also to multi-word phrases, which we call topical collocations.", "labels": [], "entities": []}, {"text": "However, as we will discuss in section 2, most of those extensions either rely on a pre-processing step to identify potential collocations (e.g., bigrams and trigrams) or limit attention to bigram dependencies.", "labels": [], "entities": []}, {"text": "We want a model that can jointly learn collocations of arbitrary length and their corresponding topic assignments from a large collection of documents.", "labels": [], "entities": []}, {"text": "The AG-colloc model does exactly this.", "labels": [], "entities": []}, {"text": "However, because the model is formulated within the Adaptor Grammar framework, the time complexity of its inference algorithm is cubic in the length of each text fragment, and so it is not feasible to apply the AG-colloc model to large collections of text documents.", "labels": [], "entities": []}, {"text": "In this paper we show how to reformulate the AG-colloc model so it is no longer relies on a general Adaptor Grammar inference procedure.", "labels": [], "entities": []}, {"text": "The new formulation facilitates more efficient inference by extending ideas developed for Bayesian word segmentation ).", "labels": [], "entities": [{"text": "Bayesian word segmentation", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.5758251746495565}]}, {"text": "We adapt a point-wise sampling algorithm from Bayesian word segmentation, which has also been used in, to simultaneously sample collocation boundaries and collocation topic assignments.", "labels": [], "entities": [{"text": "Bayesian word segmentation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6411984662214915}]}, {"text": "This algorithm retains the good performance of the AG-colloc model in document classification and information retrieval tasks.", "labels": [], "entities": [{"text": "document classification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.772539347410202}, {"text": "information retrieval tasks", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.8115759293238322}]}, {"text": "By exploiting the sparse structure of both collocation and topic distributions, using techniques inspired by, our new inference algorithm produces a remarkable speedup in running time and allows our reformulation to scale to a large number of documents.", "labels": [], "entities": []}, {"text": "This algorithm can also be easily parallelised to take advantage of multiple cores by combining the ideas of the distributed LDA model).", "labels": [], "entities": []}, {"text": "Thus, the contribution of this paper is three-fold: 1) a novel reformulation of the AG-colloc model, 2) an easily parallelisable and fast point-wise sampling algorithm exploiting sparsity and 3) systematic experiments with both qualitative and quantitative analysis.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly discuss prior work on learning topical collocations.", "labels": [], "entities": []}, {"text": "We then present our reformulation of the AG-colloc model in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 derives a point-wise Gibbs sampler for the model and shows how this sampler can take advantage of sparsity and be parallelised across multiple cores.", "labels": [], "entities": []}, {"text": "Experimental results are reported in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes this paper and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the effectiveness and efficiency of our Topical Collocation Model (TCM) on different tasks, i.e., a document classification task, an information retrieval task and a topic intrusion detection task.", "labels": [], "entities": [{"text": "document classification task", "start_pos": 128, "end_pos": 156, "type": "TASK", "confidence": 0.79887988169988}, {"text": "information retrieval task", "start_pos": 161, "end_pos": 187, "type": "TASK", "confidence": 0.8080168962478638}, {"text": "topic intrusion detection task", "start_pos": 194, "end_pos": 224, "type": "TASK", "confidence": 0.7946882843971252}]}, {"text": "All the empirical results show that our TCM performs as well as the AG-colloc model and outperforms other collocation models (i.e., LDACOL ( ), TNG (), PA ().", "labels": [], "entities": [{"text": "PA", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.9661919474601746}]}, {"text": "The TCM also runs much faster than the other models.", "labels": [], "entities": [{"text": "TCM", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7164888978004456}]}, {"text": "We also compared the TCM with the Mallet implementation of AD-LDA (, denoted by Mallet-LDA, for completeness.", "labels": [], "entities": []}, {"text": "Following , we used punctuation and Mallet's stop words to split the documents into subsequences of word tokens, then removed those punctuation and stop words from the input.", "labels": [], "entities": [{"text": "Mallet's stop words", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9146189391613007}]}, {"text": "All experiments were run on a cluster with 80 Xeon E7-4850 processors (2.0GHz) and 96 GB memory.", "labels": [], "entities": []}, {"text": "In the classification task, we used three datasets: the movie review dataset (Pang and Lee, 2012) (MReviews), the 20 Newsgroups dataset, and the Reuters-21578 dataset.", "labels": [], "entities": [{"text": "movie review dataset (Pang and Lee, 2012)", "start_pos": 56, "end_pos": 97, "type": "DATASET", "confidence": 0.8706177055835724}, {"text": "MReviews", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.6034849286079407}, {"text": "20 Newsgroups dataset", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.7110613187154134}, {"text": "Reuters-21578 dataset", "start_pos": 145, "end_pos": 166, "type": "DATASET", "confidence": 0.9821435809135437}]}, {"text": "The movie review dataset includes 1,000 positive and 1,000 negative reviews.", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.6531860033671061}]}, {"text": "The 20 Newsgroups dataset is organised: Comparison of all models in the classification task (accuracy in %) and the information retrieval task (MAP scores in %) on small corpora.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7323540647824606}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9974309802055359}, {"text": "information retrieval task", "start_pos": 116, "end_pos": 142, "type": "TASK", "confidence": 0.741294781366984}]}, {"text": "Bold face indicates scores not significantly different from the best score (in italics) according to a Wilcoxon signed rank test (p < 0.05).  into 20 different categories according to different topics.", "labels": [], "entities": [{"text": "Wilcoxon signed rank test", "start_pos": 103, "end_pos": 128, "type": "DATASET", "confidence": 0.746567577123642}]}, {"text": "We further partitioned the 20 newsgroups dataset into four subsets, denoted by Comp, Sci, Sport, and Politics.", "labels": [], "entities": []}, {"text": "They have 4, 891, 3, 952, 1, 993, and 2, 625 documents respectively.", "labels": [], "entities": []}, {"text": "We applied document classification to each subset.", "labels": [], "entities": [{"text": "document classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7409000843763351}]}, {"text": "The Reuters-21578 dataset has 21,578 Reuters news articles which are split into 10 categories.", "labels": [], "entities": [{"text": "Reuters-21578 dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9931623935699463}]}, {"text": "The classification evaluation was carried out as follows.", "labels": [], "entities": []}, {"text": "First, we ran each model on each dataset to derive point estimates of documents' topic distributions (\u03b8), which were used as the only features in classification.", "labels": [], "entities": []}, {"text": "We then randomly selected from each dataset 80% documents for training and 20% for testing.", "labels": [], "entities": []}, {"text": "A Support Vector Machine (SVM) with a linear-kernel was used.", "labels": [], "entities": []}, {"text": "We ran all models for 10,000 iterations with 50 topics on the movie review dataset and 100 on the other two.", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.8052895863850912}]}, {"text": "We set \u03b1 = 1/K and \u03b2 = 0.02 for Mallet-LDA, LDACOL, TNG and PA.", "labels": [], "entities": [{"text": "Mallet-LDA", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.9709116816520691}, {"text": "PA", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.8331101536750793}]}, {"text": "We used the reported settings in Johnson   tion parameter \u03b1 0 was initially set to 100 and resampled using approximated table counts.", "labels": [], "entities": []}, {"text": "Since efficient inference is unavailable for LDACOL, TNG and AG-colloc, making it impractical to evaluate them on the large corpora, we compared our TCM with them only on the MReviews dataset.", "labels": [], "entities": [{"text": "MReviews dataset", "start_pos": 175, "end_pos": 191, "type": "DATASET", "confidence": 0.9737223088741302}]}, {"text": "The first column of shows the classification accuracy of those models.", "labels": [], "entities": [{"text": "classification", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.9299212098121643}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9718421101570129}]}, {"text": "All the collocation models outperform Mallet-LDA.", "labels": [], "entities": [{"text": "Mallet-LDA", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9705116748809814}]}, {"text": "The AG-colloc model yields the highest classification accuracy, and our TCM with/without sparsity performs as well as the AG-colloc model according to the Wilcoxon signed rank test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9675470590591431}]}, {"text": "The Pipeline Approach (PA) is always better than LDACOL and TNG.", "labels": [], "entities": [{"text": "Pipeline Approach (PA)", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.7391853094100952}]}, {"text": "Therefore, in the following experiments we will focus on the comparison among our TCM, Mallet-LDA and PA.", "labels": [], "entities": [{"text": "Mallet-LDA", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.9054508209228516}, {"text": "PA", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9832178354263306}]}, {"text": "shows the classification accuracy of those three models on the larger datasets, i.e., the 20 Newsgroups dataset, and the Reuters-21578 dataset.", "labels": [], "entities": [{"text": "classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9113754034042358}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9599510431289673}, {"text": "20 Newsgroups dataset", "start_pos": 90, "end_pos": 111, "type": "DATASET", "confidence": 0.7665430506070455}, {"text": "Reuters-21578 dataset", "start_pos": 121, "end_pos": 142, "type": "DATASET", "confidence": 0.9923934638500214}]}, {"text": "The TCM outperforms both Mallet-LDA and PA on 3 out of 5 datasets, and performs equally well as PA on the Politics and Reuter-21578 datasets according to a Wilcoxon signed rank test (p < 0.05).", "labels": [], "entities": [{"text": "Mallet-LDA", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.9379007816314697}, {"text": "PA", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9027500748634338}, {"text": "PA", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9609535932540894}, {"text": "Politics and Reuter-21578 datasets", "start_pos": 106, "end_pos": 140, "type": "DATASET", "confidence": 0.7850993573665619}]}, {"text": "For the information retrieval task, we used the method presented by and to calculate the probability of a query given a document.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.8213420212268829}]}, {"text": "We used the San Jose Mercury News (SJMN) dataset and the AP News dataset from TREC.", "labels": [], "entities": [{"text": "San Jose Mercury News (SJMN) dataset", "start_pos": 12, "end_pos": 48, "type": "DATASET", "confidence": 0.6852993853390217}, {"text": "AP News dataset", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9769700169563293}, {"text": "TREC", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.5662568211555481}]}, {"text": "The former has 90,257 documents, the latter has 242,918 documents.", "labels": [], "entities": []}, {"text": "We ran all the models for 10,000 iteration with 100 topics.", "labels": [], "entities": []}, {"text": "The other parameter settings were the same as those used in Section 5.1.", "labels": [], "entities": []}, {"text": "Queries were tokenised using unigrams for Mallet-LDA and collocations for all collocation models.", "labels": [], "entities": [{"text": "Mallet-LDA", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9845744967460632}]}, {"text": "On a small subset of the SJMN data, which contains 2,000 documents (SJMN-2k), we find again that TCM and AG-colloc perform equally well and outperform all other models (LDACOL, TNG, PA), as shown in the second column of Table 1.", "labels": [], "entities": [{"text": "SJMN data", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.7362479418516159}, {"text": "PA", "start_pos": 182, "end_pos": 184, "type": "METRIC", "confidence": 0.9348634481430054}]}, {"text": "We further compare the TCM, Mallet-LDA and PA on the full SJMN dataset and the AP news dataset, as these models can run on large scale.", "labels": [], "entities": [{"text": "TCM", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.5598071813583374}, {"text": "Mallet-LDA", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.8786858916282654}, {"text": "PA", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.996783971786499}, {"text": "SJMN dataset", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9513884782791138}, {"text": "AP news dataset", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.9605945150057474}]}, {"text": "shows the mean average precision (MAP) scores.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9231233894824982}]}, {"text": "The TCM significantly outperforms both Mallet-LDA and the PA approach, and yields the highest MAP score.", "labels": [], "entities": [{"text": "TCM", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8590552806854248}, {"text": "Mallet-LDA", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.949669599533081}, {"text": "MAP score", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9185247123241425}]}, {"text": "We ran a set of topic intrusion detection experiments () that provide a human evaluation of the coherence of the topics learnt by Mallet-LDA, PA and TCM on the SJMN dataset.", "labels": [], "entities": [{"text": "topic intrusion detection", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.6694393853346506}, {"text": "PA", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.908109188079834}, {"text": "SJMN dataset", "start_pos": 160, "end_pos": 172, "type": "DATASET", "confidence": 0.9039193391799927}]}, {"text": "This set of experiments was use to measure how well the inferred topics match human concepts.", "labels": [], "entities": []}, {"text": "Each subject recruited from Amazon Mechanical Turk was presented with a randomly ordered list of 10 tokens (either words or collocations).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.9417254328727722}]}, {"text": "The task of the subject was to identify the token which is semantically different from the others.", "labels": [], "entities": []}, {"text": "To generate the 10-token lists, we experimented with two different methods for selecting tokens (either words or collocations) most strongly associated with a topic t.", "labels": [], "entities": []}, {"text": "The standard method chooses the tokens w that maximise p(w|t).", "labels": [], "entities": []}, {"text": "This method is biased toward high frequency tokens, since low-frequency tokens are unlikely to have a large p(w|t).", "labels": [], "entities": []}, {"text": "We also tried choosing words and collocations w that maximise p(t|w).", "labels": [], "entities": []}, {"text": "This method finds w that are unlikely to appear in any other topic except t, and is biased towards low frequency w.", "labels": [], "entities": []}, {"text": "We reduce this low-frequency bias by using a smoothed estimate for p(t|w) with a Dirichlet pseudo-count \u03b1 = 5.", "labels": [], "entities": []}, {"text": "An intruder token was randomly selected from a set of tokens that had low probability in the current topic but high probability in some other topic.", "labels": [], "entities": []}, {"text": "We then randomly selected one of the 10 tokens   to be replaced by the intruder token.", "labels": [], "entities": []}, {"text": "We expect collocations to be more useful in lists that are constructed using p(t|w) than lists constructed using p(w|t).", "labels": [], "entities": []}, {"text": "This is because p(w|t) can be dominated by the frequency of w, but individual collocations are rare.", "labels": [], "entities": []}, {"text": "The performance was measured by model precision (, which measures the fraction of subjects that agreed with the model.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9237750172615051}]}, {"text": "shows that our TCM outperforms both PA and Mallet-LDA under both ways of constructing the intrusion lists.", "labels": [], "entities": [{"text": "Mallet-LDA", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9503190517425537}]}, {"text": "As expected, the collocation models PA and TCM perform better with lists constructed according to p(t|w) than lists constructed according to p(w|t).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of all models in the classi- fication task (accuracy in %) and the information  retrieval task (MAP scores in %) on small corpora.  Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a  Wilcoxon signed rank test (p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9983482360839844}, {"text": "information  retrieval task", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.7393704752127329}]}, {"text": " Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8672572374343872}]}, {"text": " Table 3: Mean average Precision (MAP in %)  scores in the information retrieval task. Scores in  bold and italics are the significantly best MAP  scores according to a Wilcoxon signed rank test  (p < 0.05).", "labels": [], "entities": [{"text": "Mean average Precision (MAP in %)", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.9170733264514378}, {"text": "information retrieval task", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.8449747562408447}]}, {"text": " Table 4: The model precision (%) derived from the  intrusion detection experiments.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.943047046661377}, {"text": "intrusion detection", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7263813465833664}]}, {"text": " Table 5: The average running time (in seconds)  per iteration.", "labels": [], "entities": []}]}