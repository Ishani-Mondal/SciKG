{"title": [{"text": "Semantic Structure Analysis of Noun Phrases using Abstract Meaning Representation", "labels": [], "entities": [{"text": "Semantic Structure Analysis of Noun Phrases", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8559675415356954}]}], "abstractContent": [{"text": "We propose a method for semantic structure analysis of noun phrases using Abstract Meaning Representation (AMR).", "labels": [], "entities": [{"text": "semantic structure analysis of noun phrases", "start_pos": 24, "end_pos": 67, "type": "TASK", "confidence": 0.8245420157909393}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.6742023527622223}]}, {"text": "AMR is a graph representation for the meaning of a sentence, in which noun phrases (NPs) are manually annotated with internal structure and semantic relations.", "labels": [], "entities": []}, {"text": "We extract NPs from the AMR corpus and construct a data set of NP semantic structures.", "labels": [], "entities": [{"text": "AMR corpus", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.7904120087623596}]}, {"text": "We also propose a transition-based algorithm which jointly identifies both the nodes in a semantic structure tree and semantic relations between them.", "labels": [], "entities": []}, {"text": "Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points.", "labels": [], "entities": [{"text": "NP semantic structure analysis", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.7667230814695358}]}], "introductionContent": [{"text": "Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering.", "labels": [], "entities": [{"text": "Semantic structure analysis of noun phrases (NPs)", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8840638664033678}, {"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.8151807487010956}, {"text": "question answering", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.8884382247924805}]}, {"text": "Among the previous works on NP analysis are internal NP structure analysis (, noun-noun relation analysis of noun compounds (, and predicate-argument analysis of noun compounds.", "labels": [], "entities": [{"text": "NP analysis", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9613428115844727}, {"text": "NP structure analysis", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7795120278994242}, {"text": "noun-noun relation analysis of noun compounds", "start_pos": 78, "end_pos": 123, "type": "TASK", "confidence": 0.8041470944881439}, {"text": "predicate-argument analysis of noun compounds", "start_pos": 131, "end_pos": 176, "type": "TASK", "confidence": 0.792513781785965}]}, {"text": "The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths).", "labels": [], "entities": [{"text": "NP structure analysis", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7986670732498169}]}, {"text": "In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation disaster!", "labels": [], "entities": [{"text": "noun-noun relation analysis", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.797905703385671}]}, {"text": "topic!: disaster prevention awareness in AMR.", "labels": [], "entities": [{"text": "disaster prevention awareness", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.8413552244504293}, {"text": "AMR", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.7230706810951233}]}, {"text": "Predicate-argument relation ARG1, noun-noun relation topic, and internal structure (disaster prevention) awareness are expressed.", "labels": [], "entities": [{"text": "Predicate-argument relation ARG1", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.7590845425923666}, {"text": "disaster prevention) awareness", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.75777368247509}]}, {"text": "purpose to a noun compound cooking pot, meaning that pot is used for cooking).", "labels": [], "entities": []}, {"text": "On the other hand, in predicate-argument analysis, the goal is to decide whether the modifier noun is the subject or the object of the head noun (e.g., car is the object of love in car lover, while child is the subject of behave in child behavior).", "labels": [], "entities": [{"text": "predicate-argument analysis", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7492902278900146}]}, {"text": "Previous NP researches have mainly focused on these different subproblems of NP analysis using different data sets, rather than targeting general NPs simultaneously.", "labels": [], "entities": [{"text": "NP analysis", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.8722178936004639}]}, {"text": "However, these subproblems of NP analysis tend to be highly intertwined when processing texts.", "labels": [], "entities": [{"text": "NP analysis", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9478948414325714}]}, {"text": "For the purpose of tackling the task of combined NP analysis, we make use of the Abstract Meaning Representation (AMR) corpus.", "labels": [], "entities": [{"text": "NP analysis", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9319504797458649}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.7449615846077601}]}, {"text": "AMR is a formalism of sentence semantic structure by directed, acyclic, and rooted graphs, in which semantic relations such as predicateargument relations and noun-noun relations are expressed.", "labels": [], "entities": [{"text": "sentence semantic structure", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7382886608441671}]}, {"text": "In this paper, we extract substructures corresponding to NPs (shown in) from the AMR Bank 1 , and create a data set of NP semantic structures.", "labels": [], "entities": [{"text": "AMR Bank 1", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9660693407058716}]}, {"text": "In general, AMR substructures are graphs.", "labels": [], "entities": [{"text": "AMR substructures", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8761219680309296}]}, {"text": "However, since we found out that NPs mostly form trees rather than graphs in the AMR Bank, we can assume that AMR substructures corresponding to NPs are trees.", "labels": [], "entities": [{"text": "AMR Bank", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9507843554019928}]}, {"text": "Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP.", "labels": [], "entities": [{"text": "predicting the AMR tree structure", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.6868388652801514}]}, {"text": "The previous method for AMR parsing takes a: Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts ().", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9733089506626129}]}, {"text": "In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.7554233372211456}, {"text": "POS tagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.8447645902633667}]}, {"text": "As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.7306514084339142}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.7501099705696106}, {"text": "AMR parsing", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.9292968213558197}]}, {"text": "To solve this problem, we extend a transitionbased dependency parsing algorithm, and propose a novel algorithm which jointly identifies the concepts and the relations in AMR trees.", "labels": [], "entities": [{"text": "transitionbased dependency parsing", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.65772012869517}]}, {"text": "Compared to the baseline, our method improves the performance of AMR analysis of NP semantic structures by 2.7 points, and using an external dictionary further boosts the performance by 7.1 points.", "labels": [], "entities": [{"text": "AMR analysis of NP semantic structures", "start_pos": 65, "end_pos": 103, "type": "TASK", "confidence": 0.868641326824824}]}], "datasetContent": [{"text": "We conduct an experiment using our NP data set).", "labels": [], "entities": [{"text": "NP data set", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8918134768803915}]}, {"text": "We use the implementation 2 of (Flanigan et al., 2014) as our baseline.", "labels": [], "entities": [{"text": "Flanigan et al., 2014)", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.9021978875001272}]}, {"text": "For the baseline, we use the features of the default settings.", "labels": [], "entities": []}, {"text": "The method by can only generate the concepts that appear in the training data.", "labels": [], "entities": []}, {"text": "On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in.", "labels": [], "entities": [{"text": "LEMMA", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9856957793235779}, {"text": "DICT PRED", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.7154797315597534}, {"text": "NOUN", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.4909922778606415}]}, {"text": "For a fair comparison, first, we only use the rules EMPTY and KNOWN.", "labels": [], "entities": [{"text": "EMPTY", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9908766150474548}, {"text": "KNOWN", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9823341369628906}]}, {"text": "Then, we add the rule LEMMA, which can generate a concept of the lemma of the all words between w(\u03c31) and w(\u03c30) \u222a all words between w(\u03c30) and \u03b20 {\"S\", \"S\" \u2022 rule(wi, c), \"S\" \u2022 rule(wi, c) \u2022 c} LEFT-REDUCE(r, n) {\"L-R\", \"L-R\" \u2022 r, \"L-R\" \u2022 r \u2022 n} RIGHT-REDUCE(r, n) {\"R-R\", \"R-R\"\u2022r, \"R-R\"\u2022r \u2022n} EMPTY-REDUCE {\"E-R\"}, which lists categorial variations of words (such as verb run for noun runner).", "labels": [], "entities": [{"text": "LEMMA", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9441896080970764}, {"text": "LEFT-REDUCE", "start_pos": 193, "end_pos": 204, "type": "METRIC", "confidence": 0.9824105501174927}, {"text": "RIGHT-REDUCE", "start_pos": 245, "end_pos": 257, "type": "METRIC", "confidence": 0.9632775187492371}]}, {"text": "We also use definitions of the predicates from PropBank (), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.936191976070404}, {"text": "AMR", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.7491011619567871}]}, {"text": "During the training, we use the max-violation perceptron () with beam size 8 and average the parameters.", "labels": [], "entities": []}, {"text": "During the testing, we also perform beam search with beam size 8.: Performance on concept identification 2013), which reports precision, recall, and F 1 -score for overlaps of nodes, edges, and roots in semantic structure graphs.", "labels": [], "entities": [{"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9996826648712158}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9991270899772644}, {"text": "F 1 -score", "start_pos": 149, "end_pos": 159, "type": "METRIC", "confidence": 0.9878771752119064}]}, {"text": "Compared to the baseline, our method improves both the precision and recall, resulting in an increasing of F 1 -score by 2.7 points.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9997716546058655}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9997661709785461}, {"text": "F 1 -score", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.98805932700634}]}, {"text": "When we add the LEMMA rule, the recall increases by 11.4 points because the LEMMA rule can generate concepts that do not appear in the training data, resulting in a further increase of F 1 -score by 5.2 points.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9997550845146179}, {"text": "F 1 -score", "start_pos": 185, "end_pos": 195, "type": "METRIC", "confidence": 0.9869589507579803}]}, {"text": "Finally, when we add the DICT rules, the F 1 -score improves further by 1.9 points.", "labels": [], "entities": [{"text": "DICT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.45896780490875244}, {"text": "F 1 -score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9865006357431412}]}, {"text": "shows the performance on concept identification.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7203045338392258}]}, {"text": "We report precision, recall, and F 1 -score against the correct set of concepts.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999478280544281}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9997305274009705}, {"text": "F 1 -score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9849095940589905}]}, {"text": "For each condition, we observe the same tendency in performance increases as.", "labels": [], "entities": []}, {"text": "Thus, we conclude that our method improves both the concept identification and relation identification performances.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.728132352232933}, {"text": "relation identification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.8252852559089661}]}], "tableCaptions": [{"text": " Table 6: Performance on NP semantic structure  analysis", "labels": [], "entities": [{"text": "NP semantic structure  analysis", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.775031328201294}]}, {"text": " Table 7: Performance on concept identification", "labels": [], "entities": [{"text": "concept identification", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7335633337497711}]}]}