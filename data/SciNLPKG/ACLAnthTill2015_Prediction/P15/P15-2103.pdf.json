{"title": [{"text": "Tackling Sparsity, the Achilles Heel of Social Networks: Language Model Smoothing via Social Regularization", "labels": [], "entities": [{"text": "Tackling Sparsity", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8676671385765076}, {"text": "Language Model Smoothing", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.5909928778807322}]}], "abstractContent": [{"text": "Online social networks nowadays have the worldwide prosperity, as they have revolutionized the way for people to discover, to share, and to diffuse information.", "labels": [], "entities": []}, {"text": "Social networks are powerful, yet they still have Achilles Heel: extreme data sparsi-ty.", "labels": [], "entities": [{"text": "Achilles Heel", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.9204308986663818}]}, {"text": "Individual posting documents, (e.g., a microblog less than 140 characters), seem to be too sparse to make a difference under various scenarios, while in fact they are quite different.", "labels": [], "entities": []}, {"text": "We propose to tackle this specific weakness of social networks by smoothing the posting document language model based on social regulariza-tion.", "labels": [], "entities": []}, {"text": "We formulate an optimization framework with asocial regularizer.", "labels": [], "entities": []}, {"text": "Experimental results on the Twitter dataset validate the effectiveness and efficiency of our proposed model.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8454082906246185}]}], "introductionContent": [{"text": "Along with Web 2.0 online social networks have revolutionized the way for people to discover, to share and to propagate information via peer-topeer interactions (.", "labels": [], "entities": []}, {"text": "Although powerful as social networks are, they still suffer from a severe weakness: extreme sparsity.", "labels": [], "entities": []}, {"text": "Due to the special characteristics of real-time propagation, the postings on social networks are either officially limited within a limit length (140 characters on Twitter), or generally quite short due to user preference.", "labels": [], "entities": [{"text": "real-time propagation", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7838000655174255}]}, {"text": "Given limited text data sampling, a language model estimation usually encounters with zero count problem when facing with data sparsity, which is not reliable.", "labels": [], "entities": []}, {"text": "Therefore, sparsity is regarded as the Achilles Heel of social networks and now we aim at tackling the bottleneck (.", "labels": [], "entities": []}, {"text": "Statistical language models have attracted much attention in research communities.", "labels": [], "entities": []}, {"text": "Till now much: 2 different sources to smooth document language models: texts (colored in yellow) and social contacts (colored in blue).", "labels": [], "entities": []}, {"text": "Each piece of texts is authored by a particular social network user.", "labels": [], "entities": []}, {"text": "work on language model smoothing has been investigated based on textual characteristics;).", "labels": [], "entities": [{"text": "language model smoothing", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6677039166291555}]}, {"text": "However, for social networks, texts are actually associated with users (as illustrated in).", "labels": [], "entities": []}, {"text": "We propose that social factors should be utilized as an augmentation to better smooth language models.", "labels": [], "entities": []}, {"text": "Here we propose an optimization framework with regularization for language model smoothing on social networks, using both textual information and the social structure.", "labels": [], "entities": [{"text": "language model smoothing", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6465181509653727}]}, {"text": "We believe the social factor is fundamental to smooth language models on social networks.", "labels": [], "entities": []}, {"text": "Our framework optimizes the smoothed language model to be closer to social neighbors in the online network, while avoid deviating too much from the original user language models.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We have proposed a balanced language model smoothing framework with optimization, using text information with social structure as a regularizer; \u2022 We have investigated an effective and efficient strategy to model the social information among social network users.", "labels": [], "entities": []}, {"text": "We evaluate the effect of our proposed language model smoothing model using datasets from Twitter.", "labels": [], "entities": []}, {"text": "Experimental results show that language model smoothing with social regularization is effective and efficient in terms of intrinsic evaluation by perplexity and running time: we show that the Achilles Heel of social networks could be to some extent tackled.", "labels": [], "entities": [{"text": "language model smoothing", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6306116183598837}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start by reviewing previous works.", "labels": [], "entities": []}, {"text": "Then we introduce the language model smoothing with social regularization and its optimization.", "labels": [], "entities": []}, {"text": "We describe the experiments and evaluation in the next section and finally draw the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Utilizing the data in), we establish the dataset of microblogs and the corresponding users from 9/29/2012 to 11/30/2012.", "labels": [], "entities": [{"text": "9/29/2012 to 11/30/2012", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.9051425186070529}]}, {"text": "We use roughly one month as the training set and the rest as testing set.", "labels": [], "entities": []}, {"text": "Based on this dataset, we group the posting documents with the same hashtag '#' into clusters as different datasets to evaluate).", "labels": [], "entities": []}, {"text": "We manually selected top-3 topics based on popularity (measured in the number of postings within the cluster) and to obtain broad coverage of different types: sports, technology, and general interests, as listed in.", "labels": [], "entities": []}, {"text": "Basically, the social network graph can be established from all posting documents and all users.", "labels": [], "entities": []}, {"text": "However, the data is noisy.", "labels": [], "entities": []}, {"text": "We first pre-filter the pointless babbles by applying the linguistic quality judgments (e.g., OOV ratio) (, and then remove inactive users that have less than one follower or followee and remove the users without any linkage to the remaining posting documents.", "labels": [], "entities": [{"text": "OOV ratio)", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9830975532531738}]}, {"text": "We remove stopwords and URLs, perform stemming, and build the graph after filtering.", "labels": [], "entities": []}, {"text": "We establish the language model smoothed with both text information and social factors.", "labels": [], "entities": []}, {"text": "We apply language perplexity to evaluate the smoothed language models.", "labels": [], "entities": []}, {"text": "The experimental procedure is as follows: given the topic clusters shown in, we remove the hashtags and compute its perplexity with respect to the current topic cluster, defined as a power function: Perplexity is actually an entropy based evaluation.", "labels": [], "entities": []}, {"text": "In this sense, the lower perplexity within the same topic cluster, the better performance in purity the topic cluster would have.: Perplexity in hashtag clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of dataset and topic clusters.", "labels": [], "entities": []}, {"text": " Table 2: Perplexity in hashtag clusters.", "labels": [], "entities": []}]}