{"title": [{"text": "Non-linear Learning for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.8242805600166321}]}], "abstractContent": [{"text": "Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.7822910298903784}]}, {"text": "The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data.", "labels": [], "entities": []}, {"text": "In this paper, we propose a non-linear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features.", "labels": [], "entities": []}, {"text": "A learning framework is presented for training the non-linear models.", "labels": [], "entities": []}, {"text": "We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance.", "labels": [], "entities": []}, {"text": "Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6846558054288229}]}], "introductionContent": [{"text": "One of the core problems in the research of statistical machine translation is the modeling of translation hypotheses.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6888617177804311}, {"text": "modeling of translation hypotheses", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.7225010991096497}]}, {"text": "Each modeling method defines a score of a target sentence e = e 1 e 2 ...e i ...e I , given a source sentence f = f 1 f 2 ...f j ...f J , where each e i is the ith target word and f j is the jth source word.", "labels": [], "entities": []}, {"text": "The well-known modeling method starts from the Source-Channel model)(Equation 1).", "labels": [], "entities": []}, {"text": "The scoring of e decomposes to the calculation of a translation model and a language model.", "labels": [], "entities": []}, {"text": "P r(e|f ) = P r(e)P r(f |e)/P r(f ) The modeling method is extended to log-linear models by, as shown in Equation 2, where h m (e|f ) is the mth feature function and \u03bb m is the corresponding weight.", "labels": [], "entities": []}, {"text": "Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by s L , is actually a linear combination of all features, as shown in Equation 3.", "labels": [], "entities": []}, {"text": "The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings).", "labels": [], "entities": []}, {"text": "It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane.", "labels": [], "entities": []}, {"text": "However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones.", "labels": [], "entities": []}, {"text": "Taking common features in atypical phrasebased ( or hierarchical phrasebased () machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses.", "labels": [], "entities": []}, {"text": "The phrase translation probability feature selects phrases that occurs more frequently in the training corpus, which sometimes is long with a lower translation probability, as in translating named entities or idioms; sometimes is short but with a high translation probability, as in translating verbs or pronouns.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7624257206916809}]}, {"text": "These three features jointly decide the choice of translations.", "labels": [], "entities": []}, {"text": "Simply use the weighted sum of their values may not be the best choice for modeling translations.", "labels": [], "entities": []}, {"text": "As a result, log-linear models may under-fit the data.", "labels": [], "entities": []}, {"text": "This under-fitting may prevents the further improvement of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9599707126617432}]}, {"text": "In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks.", "labels": [], "entities": [{"text": "translation hypotheses", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9067028164863586}]}, {"text": "The traditional features of a machine translation system are used as the input to the network.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7185690253973007}]}, {"text": "By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models.", "labels": [], "entities": []}, {"text": "(Section 3) Employing a neural network for SMT modeling has two issues to be tackled.", "labels": [], "entities": [{"text": "SMT modeling", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.9938386976718903}]}, {"text": "The first issue is the parameter learning.", "labels": [], "entities": []}, {"text": "Log-linear models rely on minimum error rate training (MERT) to achieve best performance.", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 26, "end_pos": 60, "type": "METRIC", "confidence": 0.8508995771408081}]}, {"text": "When the scoring function become non-linear, the intersection points of these non-linear functions could not be effectively calculated and enumerated.", "labels": [], "entities": []}, {"text": "Thus MERT is no longer suitable for learning the parameters.", "labels": [], "entities": [{"text": "MERT", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.7283602952957153}]}, {"text": "To solve the problem, we present a framework for effective training including several criteria to transform the training problem into a binary classification task, a unified objective function and an iterative training algorithm.", "labels": [], "entities": []}, {"text": "(Section 4) The second issue is the structure of neural network.", "labels": [], "entities": []}, {"text": "Single layer neural networks are equivalent to linear models; two-layer networks with sufficient nodes are capable of learning any continuous function.", "labels": [], "entities": []}, {"text": "Adding more layers into the network could model complex functions with less nodes, but also brings the problem of vanishing gradient).", "labels": [], "entities": []}, {"text": "We adapt a two-layer feed-forward neural network to keep the training process efficient.", "labels": [], "entities": []}, {"text": "We notice that one major problem that prevents a neural network training reaching a good solution is that there are too many local minimums in the parameter space.", "labels": [], "entities": []}, {"text": "Thus we discuss how to constrain the learning of neural networks with our intuitions and observations of the features.", "labels": [], "entities": []}, {"text": "(Section 5) Experiments are conducted to compare various settings and verify the effectiveness of our proposed learning framework.", "labels": [], "entities": []}, {"text": "Experimental results show that our framework could achieve better translation quality even with the same traditional features as previous linear models.", "labels": [], "entities": [{"text": "translation", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9560801386833191}]}, {"text": "2 Related work Many research has been attempting to bring nonlinearity into the training of SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9960254430770874}]}, {"text": "These efforts could be roughly divided into the following three categories.", "labels": [], "entities": []}, {"text": "The first line of research attempted to reinterpret original features via feature transformation or additional learning.", "labels": [], "entities": []}, {"text": "For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.707550048828125}]}, {"text": "used discretization to transform realvalued dense features into a set of binary indicator features.", "labels": [], "entities": []}, {"text": "learned new features using a semi-supervised deep auto encoder.", "labels": [], "entities": []}, {"text": "These work focus on the explicit representation of the features and usually employ extra learning procedure.", "labels": [], "entities": []}, {"text": "Our proposed method only takes the original features, with no transformation, as the input.", "labels": [], "entities": []}, {"text": "Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality.", "labels": [], "entities": [{"text": "Feature transformation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6947882175445557}]}, {"text": "The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work.", "labels": [], "entities": []}, {"text": "used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting.", "labels": [], "entities": [{"text": "MERT", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.4412049353122711}]}, {"text": "proposed an additive neural network which employed a two-layer neural network for embedding-based features.", "labels": [], "entities": []}, {"text": "To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO.", "labels": [], "entities": [{"text": "MERT", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.6425250172615051}]}, {"text": "Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models.", "labels": [], "entities": []}, {"text": "The third line of research attempted to add non-linear features/components into the log-linear learning framework.", "labels": [], "entities": []}, {"text": "Neural network based models are trained as language models), translation models ( ) or joint language and translation models ().", "labels": [], "entities": []}, {"text": "In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation systems with new designed features.", "labels": [], "entities": []}, {"text": "We believe additional improvement could be achieved by incorporating more features into our framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "This set experiments evaluates different training criteria discussed in Section 4.1.", "labels": [], "entities": []}, {"text": "We generate hypothesis-pair according to BW, BR and PW criteria, respectively, and perform training with these pairs.", "labels": [], "entities": [{"text": "BW", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9964403510093689}, {"text": "BR", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.7602790594100952}]}, {"text": "In the PW criterion, we use the sampling method of PRO (Hopkins and May, 2011) and get the 50 hypothesis pairs for each sentence.", "labels": [], "entities": [{"text": "PRO", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.5632646679878235}]}, {"text": "We use 20 hidden nodes for all three settings to make a fair comparison.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The first two rows compare training with and without the weighted combination of hypothesis pairs we discussed in Section 4.3.", "labels": [], "entities": []}, {"text": "As the result suggested, with the weighted combination of hypothesis pairs from previous iterations, the performance improves significantly on both test sets.", "labels": [], "entities": []}, {"text": "Although the system performance on the dev set varies, the performance on test sets are almost comparable.", "labels": [], "entities": []}, {"text": "This suggest that although the three training criteria are based on different assumptions, their are basically equivalent for training translation systems.", "labels": [], "entities": [{"text": "training translation", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.6107158213853836}]}, {"text": "We also compares the three training criteria in their number of new instances per iteration and final training accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.970576822757721}]}, {"text": "Compared to BR which tries to separate the best hypothesis from the rest hypotheses in the n-best set, and PW which tries to obtain a correct ranking of all hypotheses, BW only aims at separating the best and worst hypothesis of each iteration, which is a easier task for learning a classifiers.", "labels": [], "entities": [{"text": "BW", "start_pos": 169, "end_pos": 171, "type": "METRIC", "confidence": 0.9696085453033447}]}, {"text": "It requires the least training instances and achieves the best performance in training.", "labels": [], "entities": []}, {"text": "Note that, the accuracy for each system in are the accuracy each system achieves after training stops.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996017813682556}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9973904490470886}]}, {"text": "They are not calculated on the same set of instances, thus not directly comparable.", "labels": [], "entities": []}, {"text": "We use the differences inaccuracy as an indicator for the difficulties of the corresponding learning task.", "labels": [], "entities": []}, {"text": "For the rest of this paper, we use the BW criterion because it is much simpler compared to sampling method of PRO (Hopkins and May, 2011).", "labels": [], "entities": [{"text": "BW criterion", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9766879081726074}, {"text": "PRO", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7656421065330505}]}, {"text": "We make several comparisons of the network structures and compare them with a baseline hierarchical phrase-based translation system (HPB).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.7219667434692383}]}, {"text": "different systems 5 . All 5 two-layer feed forward neural networks models could achieve comparable or better performance comparing to the baseline system.", "labels": [], "entities": []}, {"text": "We can see that training a larger network may lead to better translation quality (from TLayer 20 and TLayer 30 to TLayer 50 ).", "labels": [], "entities": [{"text": "translation", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9530380964279175}]}, {"text": "However, increasing the number of hidden node to 100 and 200 does not bring further improvement.", "labels": [], "entities": []}, {"text": "One possible reason is that training a larger network with arbitrary connections brings in too many parameters which maybe difficult to train with limited training data.", "labels": [], "entities": []}, {"text": "TDN and GN are the two network structures proposed in Section 5.", "labels": [], "entities": [{"text": "TDN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9388088583946228}]}, {"text": "With the constraint that all input to the hidden node should be of degree 2, TDN performs comparable to the baseline system.", "labels": [], "entities": []}, {"text": "With the grouped feature, we could design networks such as GN, which shows significant improvement over the baseline systems (+0.57) and achieves the best performance among all neural systems.", "labels": [], "entities": []}, {"text": "5 TLayer20 is the same system as BW in shows statistics related to the efficiency issue of different systems.", "labels": [], "entities": [{"text": "BW", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.7099051475524902}]}, {"text": "The baseline system (HPB) uses MERT for training.", "labels": [], "entities": [{"text": "MERT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7123456001281738}]}, {"text": "HPB has a very small number of parameters and searches for the best parameters exhaustively in each iteration.", "labels": [], "entities": [{"text": "HPB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9525917768478394}]}, {"text": "The non-linear systems with few nodes (TLayer 20 and TLayer 30 ) train faster than HPB in each iteration because they perform back-propagation instead of exhaustive search.", "labels": [], "entities": []}, {"text": "We iterate 15 iterations for each non-linear system, while MERT takes about 10 rounds to reach its best performance.", "labels": [], "entities": [{"text": "MERT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.7376822233200073}]}, {"text": "When the number of nodes in the hidden layer increases (from 20 to 200), the number of parameters in the system also increases, which requires longer time to compute the score for each hypothesis and to update the parameters through backpropagation.", "labels": [], "entities": []}, {"text": "The network with 200 hidden nodes takes about twice the time to train for each iteration, compared to the linear system 6 . TDN and GN have larger numbers of hidden nodes.", "labels": [], "entities": [{"text": "GN", "start_pos": 132, "end_pos": 134, "type": "DATASET", "confidence": 0.8597570061683655}]}, {"text": "However, because of our intuitions in designing the structure of the networks, the degree of the hidden node is constrained.", "labels": [], "entities": []}, {"text": "So these two networks are sparser in parameters and take significant less training time than standard neural networks.", "labels": [], "entities": []}, {"text": "For example, GN has a comparable number of hidden nodes with TLayer 200 , but only has half of its parameters and takes about 70% time to train in each iteration.", "labels": [], "entities": [{"text": "GN", "start_pos": 13, "end_pos": 15, "type": "DATASET", "confidence": 0.8240354657173157}]}, {"text": "In other words, our proposed network structure provides more efficient training in these cases and achieve better results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU4 in percentage on different training criteria (\"BR\", \"BW\" and \"PW\" refer to experiments  with \"Best v.s. Rest\", \"Best v.s. Worst\" and \"Pairwise\" training criteria, respectively. \"BR c \" indicates  generate hypothesis pairs from n-best set of current iteration only presented in Section 4.3.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.998060405254364}, {"text": "BR", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9918128252029419}, {"text": "BW", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.71271151304245}]}, {"text": " Table 3: Comparison of different training criteria  in number of new instances per iteration and train- ing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.8964690566062927}]}, {"text": " Table 4: BLEU4 in percentage for comparing of systems using different network structures (HPB refers  to the baseline hierarchical phrase-based system. TLayer, TDN, GN refer to the standard 2-layer network,  Two-Degree Hidden Layer Network, Grouped Network, respectively. Subscript of TLayer indicates the  number of nodes in the hidden layer.) + ,  *  marks results that are significant better than the baseline  system with p < 0.01 and p < 0.05.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.997891366481781}]}, {"text": " Table 5: Comparison of network scales and training time of different systems, including the number of  nodes in the hidden layer, the number of parameters, the average training time per iteration (15 iterations).  The notations of systems are the same as in Table4.", "labels": [], "entities": []}]}