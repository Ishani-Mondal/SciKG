{"title": [{"text": "Co-training for Semi-supervised Sentiment Classification Based on Dual-view Bags-of-words Representation", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.9036802351474762}, {"text": "Representation", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.5855600237846375}]}], "abstractContent": [{"text": "A review text is normally represented as a bag-of-words (BOW) in sentiment classification.", "labels": [], "entities": [{"text": "BOW", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9158276915550232}, {"text": "sentiment classification", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.940079927444458}]}, {"text": "Such a simplified BOW model has fundamental deficiencies in modeling some complex linguistic phenomena such as negation.", "labels": [], "entities": []}, {"text": "In this work, we propose a dual-view co-training algorithm based on dual-view BOW representation for semi-supervised sentiment classification.", "labels": [], "entities": [{"text": "BOW", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.8990330100059509}, {"text": "sentiment classification", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7694306373596191}]}, {"text": "In dual-view BOW, we automatically construct antonymous reviews and model a review text by a pair of bags-of-words with opposite views.", "labels": [], "entities": []}, {"text": "We make use of the original and antonymous views in pairs, in the training, bootstrapping and testing process, all based on a joint observation of two views.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate the advantages of our approach , in meeting the two co-training requirements , addressing the negation problem , and enhancing the semi-supervised sentiment classification efficiency.", "labels": [], "entities": [{"text": "negation problem", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.914047122001648}, {"text": "sentiment classification", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.8142051994800568}]}], "introductionContent": [{"text": "In the past decade, there has been an explosion of user-generated subjective texts on the Internet in forms of online reviews, blogs and microblogs.", "labels": [], "entities": []}, {"text": "With the need of automatically identifying sentiments and opinions from those online texts, sentiment classification has attracted much attention in the field of natural language processing.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.9317026138305664}, {"text": "natural language processing", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.6523894468943278}]}, {"text": "Lots of previous research focused on the task of supervised sentiment classification.", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.6841291189193726}]}, {"text": "However, in some domains, it is hard to obtain a sufficient amount of labeled training data.", "labels": [], "entities": []}, {"text": "Manual annotation is also very expensive and time-consuming.", "labels": [], "entities": [{"text": "Manual annotation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6491684317588806}]}, {"text": "To address this problem, semi-supervised learning approaches were employed in sentiment classification, to reduce the need for labeled reviews by taking advantage of unlabeled reviews.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.9578381180763245}]}, {"text": "The dominating text representation method in both supervised and semi-supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.8209198117256165}]}, {"text": "For example, the BOW representations of two opposite reviews \"It works well\" and \"It doesn't work well\" are considered to be very similar by most statistical learning algorithms.", "labels": [], "entities": [{"text": "BOW", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9586398601531982}]}, {"text": "In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (.", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 3, "end_pos": 38, "type": "TASK", "confidence": 0.7638180454572042}, {"text": "negation", "start_pos": 93, "end_pos": 101, "type": "TASK", "confidence": 0.9662336707115173}]}, {"text": "Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts.", "labels": [], "entities": [{"text": "semi-supervised sentiment classification", "start_pos": 17, "end_pos": 57, "type": "TASK", "confidence": 0.683598667383194}]}, {"text": "For example, Aue and Gamon (2005) applied the na\u00a8\u0131vena\u00a8\u0131ve Bayes EM algorithm ().", "labels": [], "entities": []}, {"text": "applied a graph-based semi-supervised learning algorithm by (.", "labels": [], "entities": []}, {"text": "employed a co-training approach for cross-language sentiment classification.", "labels": [], "entities": [{"text": "cross-language sentiment classification", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.8890221913655599}]}, {"text": "employed cotraining with personal and impersonal views.", "labels": [], "entities": []}, {"text": "explored the use of label propagation (. As pointed by): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews' sentiment patterns.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7190589606761932}]}, {"text": "However, to the best knowledge, such investigations are very scarce in the research of semi-supervised sentiment classification.", "labels": [], "entities": [{"text": "semi-supervised sentiment classification", "start_pos": 87, "end_pos": 127, "type": "TASK", "confidence": 0.6589014629522959}]}, {"text": "In (, we have developed a dual sentiment analysis approach, which creates antonymous reviews and makes use of original and antonymous reviews together for supervised sentiment classification.", "labels": [], "entities": [{"text": "dual sentiment analysis", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.699247807264328}, {"text": "sentiment classification", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.6984996199607849}]}, {"text": "In this work, we propose a dual-view co-training approach based on dualview BOW representation for semi-supervised sentiment classification.", "labels": [], "entities": [{"text": "BOW", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.8724080324172974}, {"text": "sentiment classification", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.7593617141246796}]}, {"text": "Specifically, we model both the original and antonymous reviews by a pair of bags-of-words with opposite views.", "labels": [], "entities": []}, {"text": "Based on such a dual-view representation, we design a dual-view co-training approach.", "labels": [], "entities": []}, {"text": "The training, bootstrapping and testing processes are all performed by observing two opposite sides of one review.", "labels": [], "entities": []}, {"text": "That is, we consider not only how positive/negative the original review is, but also how negative/positive the antonymous review is.", "labels": [], "entities": []}, {"text": "In comparison with traditional methods, our dual-view co-training approach has the following advantages: \u2022 Effectively address the negation problem; \u2022 Automatically learn the associations among antonyms; \u2022 Better meet the two co-training requirements in).", "labels": [], "entities": [{"text": "negation problem", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.8984026312828064}]}], "datasetContent": [{"text": "We conduct the experiments on the multi-domain sentiment datasets, which were introduced in ( and have been widely used in sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.9302916526794434}]}, {"text": "It consists of four domains (Book, DVD, Electronics, and Kitchen) of reviews extracted from Amazon.com.", "labels": [], "entities": [{"text": "Book", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.941385805606842}]}, {"text": "Each of the four datasets contains 1,000 positive and 1,000 negative reviews.", "labels": [], "entities": []}, {"text": "Following the experimental settings used in (), we randomly separate all the reviews in each class into a labeled data set, a unlabeled data set, and a test set, with a proportion of 10%, 70% and 20%, respectively.", "labels": [], "entities": []}, {"text": "We report the averaged results of 10-fold cross-validation in terms of classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.9280834197998047}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9511927366256714}]}, {"text": "Note that our approach is a general framework that allows different classification algorithms.", "labels": [], "entities": []}, {"text": "Due to the space limitation, we only report the results by using logistic regression 3 . Note the similar conclusions can be obtained by using the other algorithms such as SVMs and na\u00a8\u0131vena\u00a8\u0131ve Bayes.", "labels": [], "entities": []}, {"text": "The LibLinear toolkit 4 is utilized, with a dual L2-regularized factor, and a default tradeoff parameter c.", "labels": [], "entities": []}, {"text": "Similar to), we carryout the experiments with the unigram features without feature selection.", "labels": [], "entities": []}, {"text": "Presence is used as the term weighting scheme as it was reported in () that it performed better than TF and TF-IDF.", "labels": [], "entities": [{"text": "Presence", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9202373027801514}, {"text": "TF", "start_pos": 101, "end_pos": 103, "type": "DATASET", "confidence": 0.6362578868865967}]}, {"text": "Finally, the paired t-test) is performed to test the significance of the difference be-  tween two systems, with a default significant level of 0.05.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The semi-supervised classification accu- racy of ten systems.", "labels": [], "entities": []}]}