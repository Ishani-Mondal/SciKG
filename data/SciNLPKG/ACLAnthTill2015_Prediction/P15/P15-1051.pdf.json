{"title": [{"text": "Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment", "labels": [], "entities": [{"text": "Document Enrichment", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7469878792762756}]}], "abstractContent": [{"text": "Document enrichment focuses on retrieving relevant knowledge from external resources , which is essential because text is generally replete with gaps.", "labels": [], "entities": [{"text": "Document enrichment", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.896620512008667}]}, {"text": "Since conventional work primarily relies on special resources , we instead use triples of Subject, Predicate, Object as knowledge and incorporate distributional semantics to rank them.", "labels": [], "entities": []}, {"text": "Our model first extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirich-let Allocation.", "labels": [], "entities": []}, {"text": "We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones.", "labels": [], "entities": []}, {"text": "Evaluated as a ranking problem, our model significantly out-performs multiple strong baselines.", "labels": [], "entities": []}, {"text": "Moreover , we conduct a task-based evaluation by incorporating these triples as additional features into document classification and enhances the performance by 3.02%.", "labels": [], "entities": [{"text": "document classification", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.6863308846950531}]}], "introductionContent": [{"text": "Document enrichment is the task of acquiring relevant background knowledge from external resources fora given document.", "labels": [], "entities": [{"text": "Document enrichment", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9197377860546112}]}, {"text": "This task is essential because, during the writing of text, some basic but well-known information is usually omitted by the author to make the document more concise.", "labels": [], "entities": []}, {"text": "For example, Baghdad is the capital of Iraq is omitted in.", "labels": [], "entities": []}, {"text": "A human will fill these gaps automatically with the background knowledge in his mind.", "labels": [], "entities": []}, {"text": "However, the machine lacks both the necessary background knowledge and the ability to select.", "labels": [], "entities": []}, {"text": "The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (, coreference resolution (, document cluster () and entity disambiguation.", "labels": [], "entities": [{"text": "document enrichment", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7407912015914917}, {"text": "coreference resolution", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.9375694096088409}, {"text": "entity disambiguation", "start_pos": 186, "end_pos": 207, "type": "TASK", "confidence": 0.7855680882930756}]}, {"text": "We can classify previous work into two classes according to the resources they rely on.", "labels": [], "entities": []}, {"text": "The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge).", "labels": [], "entities": []}, {"text": "Most research in this area relies on the text similarity () and structure information () between the mention and the Wikipedia page.", "labels": [], "entities": []}, {"text": "Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem.", "labels": [], "entities": []}, {"text": "Another line of work tries to improve the accuracy by introducing ontologies and structured knowledge bases such as WordNet (, which provide semantic information about words such as synonym ) and antonym.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9990424513816833}, {"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9208996295928955}]}, {"text": "However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice.", "labels": [], "entities": []}, {"text": "In contrast, we wish to seek the benefits of both coverage and accuracy from a better representation of background knowledge: triples of Subject, Predicate, Object (SPO).", "labels": [], "entities": [{"text": "coverage", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9546140432357788}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9976462721824646}]}, {"text": "According to, these triples, such as LeonardCohen, wasBornIn, Montreal, can be extracted automatically from Wikipedia and other sources, which is compatible with the RDF data model.", "labels": [], "entities": [{"text": "LeonardCohen", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9761941432952881}]}, {"text": "Moreover, by extracting these Global Ranking lobal anking S 1 : The coalition may never know if Iraqi president Saddam Hussein survived a U.S. airstrike yesterday.", "labels": [], "entities": [{"text": "Global Ranking lobal anking S 1", "start_pos": 30, "end_pos": 61, "type": "DATASET", "confidence": 0.9199126859505972}]}, {"text": "S 2 : A B-1 bomber dropped four 2,000-pound bombs on a building in a residential area of Baghdad . (b) Two omitted relevant pieces of background knowledge: An example of document enrichment: A source document about a U.S. airstrike omitting two important pieces of background knowledge which are acquired by our framework.", "labels": [], "entities": []}, {"text": "triples from multiple sources, we also get better coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.968346118927002}]}, {"text": "Therefore, one can expect that this representation is helpful for better document enrichment by incorporating both accuracy and coverage.", "labels": [], "entities": [{"text": "document enrichment", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7119848430156708}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.998843789100647}, {"text": "coverage", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.95208740234375}]}, {"text": "In fact, there is already evidence that this representation is helpful.", "labels": [], "entities": []}, {"text": "proposed a triple-based document enrichment framework which uses triples of SPO as background knowledge.", "labels": [], "entities": []}, {"text": "They first proposed a search enginebased method to evaluate the relatedness between every pair of triples, and then an iterative propagation algorithm was introduced to select the most relevant triples to a given source document (see Section 2), which achieved a good performance.", "labels": [], "entities": []}, {"text": "However, to evaluate the semantic relatedness between two triples, primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in turn limits its application in practice.", "labels": [], "entities": []}, {"text": "Moreover, they did not carryout any task-based evaluation, which makes it uncertain whether their method will be helpful in real applications.", "labels": [], "entities": []}, {"text": "Therefore, we instead use topic models, especially Latent Dirichlet Allocation (LDA), to encode distributional semantics of words and convert every triple into a real-valued vector, which is then used to evaluate the relatedness between a pair of triples.", "labels": [], "entities": []}, {"text": "We then incorporate these triples into the given source document and represent them together as a graph of triples.", "labels": [], "entities": []}, {"text": "Then a modified iterative propagation is carried out over the entire graph to select the most relevant triples of background knowledge to the given source document.", "labels": [], "entities": []}, {"text": "To evaluate our model, we conduct two series of experiments: (1) evaluation as a ranking problem, and (2) task-based evaluation.", "labels": [], "entities": []}, {"text": "We first treat this task as a ranking problem which inputs one document and outputs the top N most-relevant triples of background knowledge.", "labels": [], "entities": []}, {"text": "Second, we carryout a task-based evaluation by incorporating these relevant triples acquired by our model into the original model of document classification as additional features.", "labels": [], "entities": [{"text": "document classification", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.6923304945230484}]}, {"text": "We then perform a direct comparison between the classification models with and without these triples, to determine whether they are helpful or not.", "labels": [], "entities": []}, {"text": "On the first series of experiments, we achieve a MAP of 0.6494 and a P@N of 0.5597 in the best situation, which outperforms the strongest baseline by 5.87% and 17.21%.", "labels": [], "entities": [{"text": "MAP", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9993321299552917}, {"text": "P@N", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9260922273000082}]}, {"text": "In the task-based evaluation, the enriched model derived from the triples of background knowledge performs better by 3.02%, which demonstrates the effectiveness of our framework in real NLP applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our model, we conduct two series of experiments: (1) We first treat this task as a ranking problem, which takes a document as input and outputs the ranked triples of background knowledge, and evaluate the ranking performance by computing the scores of MAP and P@N.", "labels": [], "entities": [{"text": "MAP", "start_pos": 264, "end_pos": 267, "type": "METRIC", "confidence": 0.9577358365058899}]}, {"text": "(2) We also conduct a task-based evaluation, where document classification (see Section 4) is chosen as a demonstration, by enriching the background knowledge to the original framework as additional features and performing a direct comparison.", "labels": [], "entities": [{"text": "document classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.6956876963376999}]}, {"text": "Data preparation The data is composed of two parts: source documents and background knowledge.", "labels": [], "entities": [{"text": "Data preparation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6860413104295731}]}, {"text": "For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news 2 including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Military.", "labels": [], "entities": []}, {"text": "We then randomly but equally select 600 articles as the set of source documents from 9 topics without data bias.", "labels": [], "entities": []}, {"text": "We use all the other 16,599 documents of the same corpus as the source of background knowledge, and then introduce a wellknown Chinese open source tool ( to extract the triples of background knowledge from the raw text automatically.", "labels": [], "entities": []}, {"text": "So the background knowledge also distributes evenly across the same 9 topics.", "labels": [], "entities": []}, {"text": "We use the same tool to extract the triples of source documents too.", "labels": [], "entities": []}, {"text": "Baseline systems As argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval) and entity linking, as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link structure.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.7162505686283112}, {"text": "entity linking", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.7382718324661255}]}, {"text": "For better comparison, we implement three methods as baselines, which have been proved effective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA).", "labels": [], "entities": [{"text": "relevance evaluation", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.944192111492157}, {"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 185, "end_pos": 218, "type": "METRIC", "confidence": 0.8511975506941477}]}, {"text": "Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with http://www.sogou.com/labs/dl/c.html the publicly available tool word2vec 3 . Here we use ti , D, and w i to denote a triple of background knowledge, a source document, and the relevance oft i to D.", "labels": [], "entities": [{"text": "WE", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.8428813219070435}]}, {"text": "For VSM, we represent both ti and D with a tf-idf scheme first and compute w i as their cosinesimilarity.", "labels": [], "entities": [{"text": "VSM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.49194467067718506}]}, {"text": "For WE, we first convert both ti and the triples extracted from D into real-valued vectors with WE and then compute w i by accumulating all the cosine-similarities between ti and every triple from D.", "labels": [], "entities": []}, {"text": "For LDA, we represent ti as a vector with our model introduced in Section 3.1 and get the vector of D directly with LDA.", "labels": [], "entities": []}, {"text": "Then we evaluate their relevance oft i to D by computing the cosinesimilarity of two corresponding vectors.", "labels": [], "entities": []}, {"text": "Moreover, to determine whether our modified iterative propagation is helpful or not, we also compare our full model (Ours) against a simplified version without iterative propagation (Ours-S).", "labels": [], "entities": []}, {"text": "In Ours-S, we represent both ti and the triples extracted from D as real-valued vectors with our model introduced in Section 3.1.", "labels": [], "entities": []}, {"text": "Then we compute w i by accumulating all the cosine-similarities between ti and the triples extracted from D.", "labels": [], "entities": []}, {"text": "For all the baselines, we rank the triples of background knowledge according tow i , their relevance to D.", "labels": [], "entities": []}, {"text": "Experimental setup Previous research relies on manual annotation to evaluate the ranking performance (), which costs a lot, and in which it is difficult to get high consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 165, "end_pos": 176, "type": "METRIC", "confidence": 0.9745721220970154}]}, {"text": "In this paper, we carryout an automatic evaluation.", "labels": [], "entities": []}, {"text": "The corpus we used consists of 9 different classes, from which we extract triples of background knowledge.", "labels": [], "entities": []}, {"text": "So correspondingly, there will be 9 sets of triples too.", "labels": [], "entities": []}, {"text": "Then we randomly select 200 triples from every class and mix 200 \u00d7 9 = 1800 triples together as S, the set of triples of background knowledge.", "labels": [], "entities": []}, {"text": "For every document D to be enriched, our model selects the top N mostrelevant triples from Sand returns them to D as enrichments.", "labels": [], "entities": []}, {"text": "We treat a triplet i selected by our model as positive only if ti is extracted from the same class as D.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our model with two well-known criteria in ranking problem: MAP and P@N ().", "labels": [], "entities": [{"text": "MAP", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9456238150596619}]}, {"text": "Statistically significant differences of performance are determined using the two-tailed paired t-test computed at a 95% confidence level based on the average performance per source document.: The performance evaluated as a ranking task.", "labels": [], "entities": []}, {"text": "Here Ours corresponds to our full model, while Ours-S is a simplified version of our model without iterative propagation (see Section 3.2).", "labels": [], "entities": []}, {"text": "Results The performance of multiple models is shown in.", "labels": [], "entities": []}, {"text": "Overall, our full model Ours outperforms all the baseline systems significantly in every metric.", "labels": [], "entities": []}, {"text": "When evaluating the top 10 triples with the highest relevance weight, our framework outperforms the best baseline LDA by 4.4% in MAP and by 3.91% in P@N.", "labels": [], "entities": [{"text": "MAP", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.5028257966041565}]}, {"text": "When evaluating the top 5 triples, our framework performs even better and significantly outperforms the best baseline by 5.87% in MAP and by 17.", "labels": [], "entities": [{"text": "MAP", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.527271032333374}]}, {"text": "To analyze the results further, Ours-S, the simplified version of our model without iterative propagation, outperforms two strong baselines VSM and WE, which indicates the effectiveness of encoding distributional semantics.", "labels": [], "entities": []}, {"text": "However, the performance of this simplified model is not as good as that of LDA, because Ours-S evaluates the relevance with simple accumulation, which fails to capture the relatedness between multiple triples from the source document.", "labels": [], "entities": []}, {"text": "We tackle this problem by incorporating the modified iterative propagation over the entire triple graph into Ours, which achieves the best performance.", "labels": [], "entities": []}, {"text": "One possible problem is why WE has a poor performance, the reason of which lies in the setup of our evaluation, where we label positive and negative instances according to the class information of triples and documents.", "labels": [], "entities": [{"text": "WE", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.7351044416427612}]}, {"text": "This is better fit for topic model-based methods.", "labels": [], "entities": []}, {"text": "Discussion We further analyze the impact of the three modifications we made to the original model (see Section 3.2).", "labels": [], "entities": []}, {"text": "We first focus on the impact of decreasing the relevance weight of bk-nodes and increasing that of sd-nodes after every iteration.", "labels": [], "entities": []}, {"text": "As mentioned previously, we change their relevance weight according to a fixed ratio, which is important to the performance.", "labels": [], "entities": []}, {"text": "shows the performance of models with different ratios.", "labels": [], "entities": []}, {"text": "With any increase of the ratio, our model improves its performance in every metric, which shows the: The performance of our model with different ratios between sd-nodes and bk-nodes.", "labels": [], "entities": []}, {"text": "The performance remains stable from the value of 10:1, which is thus chosen as the final value in our experiments.", "labels": [], "entities": []}, {"text": "We then turn to the other two modifications about the edges between bk-nodes and the setup of propagation probability.", "labels": [], "entities": []}, {"text": "shows the performance of our full model and the simplified models without these two modifications.", "labels": [], "entities": []}, {"text": "With the edges between bk-nodes, our model improves the performance by 1.48% in MAP 5 and by 1.82% in P@5.", "labels": [], "entities": []}, {"text": "With the modified iterative propagation, we achieve a even greater improvement of 13.99% in MAP 5 and 24.27% in P@5.", "labels": [], "entities": []}, {"text": "All these improvements are statistically significant, which indicates the effectiveness of these modifications to the original model.: The performance of our full model (Full) and two simplified models without modifications:  Data preparation To carryout the task-based evaluation, we use the same Chinese corpus as that in previous experiments, which consists of 17,199 documents extracted from Internet news in 9 topics.", "labels": [], "entities": []}, {"text": "We also use the same tool (: The performance of document classification with (LDA+SVM+Ours-S, LDA+SVM+Ours) and without (others) background knowledge.", "labels": [], "entities": [{"text": "document classification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7230992615222931}]}, {"text": "without background knowledge to evaluate the impact of introducing background knowledge.", "labels": [], "entities": []}, {"text": "Baseline systems We first illustrate two baselines without background knowledge based on VSM and LDA.", "labels": [], "entities": [{"text": "VSM", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.7717403173446655}]}, {"text": "For VSM, the test document Dis represented as a bag of words, where the word distribution over candidate topics is trained on the same corpus mentioned previously.", "labels": [], "entities": [{"text": "VSM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5182495713233948}]}, {"text": "Then we evaluate the similarity between D and a candidate topic with cosine-similarity directly, where the topic with the highest similarity will be chosen as the final class.", "labels": [], "entities": []}, {"text": "We use two setups: (1) VSMone-hot represents a word as 1 if it appears in a document or topic, or 0 if not.", "labels": [], "entities": []}, {"text": "(2) VSM-tf-idf represents a word as the value of tf-idf.", "labels": [], "entities": []}, {"text": "For LDA, we re-implement the state-of-the-art system as another baseline, which represents D as a topic vector v din the parameter estimation step, and then introduces a SVM classifier to take v d as input and decide the final class in the inference step.", "labels": [], "entities": []}, {"text": "We also evaluate the impact of knowledge quality by proposing two different models to introduce background knowledge: our full model introduced in Section 3 (Ours), and a simplified version of our model without iterative propagation (Ours-S).", "labels": [], "entities": []}, {"text": "They have different performances on introducing background knowledge as shown in previous experiments (see Section 5.1).", "labels": [], "entities": []}, {"text": "We then conduct a direct comparison between the document classification models with these conditions, whose differing performances demonstrates the impact of different qualities of background knowledge on this task.", "labels": [], "entities": [{"text": "document classification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6995258629322052}]}, {"text": "We use P, R, F to evaluate the performance, which are computed as the micro-average over 9 topics.", "labels": [], "entities": []}, {"text": "Both models with background knowledge (LDA+SVM+Ours-S, LDA+SVM+Ours) outperform systems without knowledge, which shows that the introduction of background knowledge helps in better classifica-: The performance of document classification models with different thresholds.", "labels": [], "entities": [{"text": "document classification", "start_pos": 213, "end_pos": 236, "type": "TASK", "confidence": 0.6863416433334351}]}, {"text": "The knowledge whose relevance weight to the source document exceeds the threshold will be introduced as background knowledge.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).", "labels": [], "entities": []}, {"text": " Table 2: The performance of our full model (Full)  and two simplified models without modifications:", "labels": [], "entities": []}, {"text": " Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.", "labels": [], "entities": []}, {"text": " Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.", "labels": [], "entities": []}]}