{"title": [], "abstractContent": [{"text": "We develop an approach for generating deep (i.e, high-level) comprehension questions from novel text that bypasses the myriad challenges of creating a full semantic representation.", "labels": [], "entities": []}, {"text": "We do this by decomposing the task into an ontology-crowd-relevance workflow, consisting of first representing the original text in a low-dimensional ontology, then crowd-sourcing candidate question templates aligned with that space, and finally ranking potentially relevant templates fora novel region of text.", "labels": [], "entities": []}, {"text": "If ontological labels are not available, we infer them from the text.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of this method on a corpus of articles from Wikipedia alongside human judgments, and find that we can generate relevant deep questions with a precision of over 85% while maintaining a recall of 70%.", "labels": [], "entities": [{"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9980158805847168}, {"text": "recall", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.999334990978241}]}], "introductionContent": [{"text": "Questions area fundamental tool for teachers in assessing the understanding of their students.", "labels": [], "entities": []}, {"text": "Writing good questions, though, is hard work, and harder still when the questions need to be deep (i.e., high-level) rather than factoid-oriented.", "labels": [], "entities": []}, {"text": "These deep questions are the sort of open-ended queries that require deep thinking and recall rather than a rote response, that span significant amounts of content rather than a single sentence.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9936423301696777}]}, {"text": "Unsurprisingly, it is these deep questions that have the greatest educational value).", "labels": [], "entities": []}, {"text": "They are thus a key assessment mechanism fora spectrum of online educational options, from MOOCs to interactive tutoring systems.", "labels": [], "entities": []}, {"text": "As such, the problem of automatic question generation has long been of interest to the online education community), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring.", "labels": [], "entities": [{"text": "question generation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6931862831115723}, {"text": "question authoring", "start_pos": 219, "end_pos": 237, "type": "TASK", "confidence": 0.7329191267490387}]}, {"text": "Much work to date has focused on questions based on a single sentence of the text, and the ideal of creating deep, conceptual questions has remained elusive.", "labels": [], "entities": []}, {"text": "In this work, we hope to take a significant step towards this challenge by approaching the problem in a somewhat unconventional way.", "labels": [], "entities": []}, {"text": "While one might expect the natural path to generating deep questions to involve first extracting a semantic representation of the entire text, the state-of-the-art in this area is at too early a stage to achieve such a representation effectively.", "labels": [], "entities": []}, {"text": "Rather we take a step back from full understanding, and instead propose an ontology-crowd-relevance workflow for generating high-level questions, shown in.", "labels": [], "entities": []}, {"text": "This involves 1) decomposing a text into a meaningful, intermediate, low-dimensional ontology, 2) soliciting high-level templates from the crowd, aligned with this intermediate representation, and 3) fora target text segment, retrieving a subset of the collected templates based on its ontological categories and then ranking these questions by estimating the relevance of each to the text at hand.", "labels": [], "entities": []}, {"text": "In this work, we apply the proposed workflow to the Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.9460971653461456}]}, {"text": "For our ontology, we use a Cartesian product of article categories (derived from Freebase) and article section names (directly from Wikipedia) as the intermediate representation (e.g. category: Person, section: Early life), henceforth referred to as category-section pairs.", "labels": [], "entities": []}, {"text": "We use these pairs to prompt our crowd workers to create relevant templates; for instance, (Person, Early Life) might lead a worker to generate the question \"Who were the key influences on <Per-son> in their childhood?\", a good example of the sort of deep question that can't be answered from a single sentence in the article.", "labels": [], "entities": []}, {"text": "We also develop classifiers for inferring these categories when explicit or matching labels are not available.", "labels": [], "entities": []}, {"text": "Given a database of such category-section-specific question templates, we then train a binary classifier that can estimate the relevance of each to anew document.", "labels": [], "entities": []}, {"text": "We hypothesize that the resulting ranked questions will be both high-level and relevant, without requiring full machine understanding of the text -in other words, deep questions without deep understanding.", "labels": [], "entities": []}, {"text": "In the sections that follow, we detail the various components of this method and describe the experiments showing their efficacy at generating high-quality questions.", "labels": [], "entities": []}, {"text": "We begin by motivating our choice of ontology and demonstrating its coverage properties (Section 3).", "labels": [], "entities": []}, {"text": "We then describe our crowdsourcing methodology for soliciting questions and question-article relevance judgments (Section 4), and outline our model for determining the relevance of these questions to new text (Section 5).", "labels": [], "entities": []}, {"text": "After this we describe the two datasets that we construct for the evaluation of our approach and present quantitative results (Section 6) as well as examples of our output and an error analysis (Section 7) before concluding (Section 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the datasets used for training the relevance classifier in Section 5.2 (DATASET I) as well as for end-to-end performance on unlabeled text segments (DATASET II).", "labels": [], "entities": []}, {"text": "We then evaluate the performance on this second dataset under three settings: first, when the category and section are known, second, when those labels are unavailable, and third, when neither the labels nor the relevance classifier are available.", "labels": [], "entities": []}, {"text": "The first dataset (DATASET I) was intended for training and evaluating the relevance classifier, and for this we assumed the category and section labels were known.", "labels": [], "entities": []}, {"text": "As such, judgments were collected only for questions templates authored fora given article's actual category and section labels.", "labels": [], "entities": []}, {"text": "After filtering out annotations from unreliable workers (based on their pre-test results) as well as those with inter-annotator agreement below 60%, we were left with a set of 995 rated questions, spanning across two categories (Person and Location) and 50 sections per category (100 categorysection pairs total).", "labels": [], "entities": []}, {"text": "This corresponded to a total of 4439 relevance tuples (label, question, article) where label is a binary relevance rating aggregated via majority vote across multiple raters.", "labels": [], "entities": []}, {"text": "The relevance labels were skewed towards the positive (relevant) class with 63% relevant instances.", "labels": [], "entities": []}, {"text": "This is of course a mostly unrealistic data setting for applications of question generation (known category and section labels), but greatly useful in developing and evaluating the relevance classifier; we thus used this dataset only for that purpose (see Section 5.2 and).", "labels": [], "entities": [{"text": "question generation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7456295490264893}]}, {"text": "For an end-to-end evaluation we need to examine situations where the category and section labels are not available and we must rely on inference instead.", "labels": [], "entities": []}, {"text": "As this is the more typical use case for our method, it is critical to understand how the performance will be affected.", "labels": [], "entities": []}, {"text": "For DATASET II, then, we first sampled articles from the Wikipedia corpus at random (satisfying the constraints described in Section 3) and then performed category and section inference on the article segments.", "labels": [], "entities": [{"text": "DATASET II", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.49144551157951355}, {"text": "Wikipedia corpus", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.8726993501186371}]}, {"text": "The category c with the highest posterior probability was chosen as the inferred category, while all section types \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u0096 with a posterior probability greater than 0.6 were considered as sources for templates.", "labels": [], "entities": []}, {"text": "Only articles whose inferred category was Person or Location were considered, but given the noise in inference there was no guarantee that the true labels were of these categories.", "labels": [], "entities": []}, {"text": "We continued this process until we retrieved a total of 12 articles.", "labels": [], "entities": []}, {"text": "For each article segment in these 12, we drew a random subset of at most 20 question templates from our database matching the inferred category and section(s), then ordered them by their estimated relevance for presentation to judges.", "labels": [], "entities": []}, {"text": "We then solicited an additional 62 Mechanical Turk workers to a rating task setup according to the same protocol as for DATASET I.", "labels": [], "entities": [{"text": "DATASET I", "start_pos": 120, "end_pos": 129, "type": "TASK", "confidence": 0.5238568186759949}]}, {"text": "After aggregation and filtering in the same way, the second dataset contained a total 256 (label, question, article) relevance tuples, skewed towards the positive class with 72% relevant instances.", "labels": [], "entities": []}, {"text": "As our end-to-end task is framed as the retrieval of a set of relevant questions fora given article segment, we can measure performance in terms of an information retrieval-based metric.", "labels": [], "entities": []}, {"text": "Consider a user who supplies an article segment (the \"query\" in IR terms) for which she wants to generate a quiz: the system then presents a ranked list of retrieved questions, ordered according to their estimated relevance to the article.", "labels": [], "entities": []}, {"text": "As she makes her way down this ranked list of questions, adding a question at a time to the quiz (set Q), the behavior of the precision and recall (with respect to relevance to the article segment) of the questions in Q, summarizes the performance of the retrieval system (i.e. the Precision-Recall (PR) curve).", "labels": [], "entities": [{"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9985306262969971}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9867225289344788}, {"text": "Precision-Recall (PR) curve", "start_pos": 282, "end_pos": 309, "type": "METRIC", "confidence": 0.9183297753334045}]}, {"text": "We summarize the performance of our system by averaging the individual article segments' PR curves (linearly interpolated) from DATASET II, and present the average precision over bins of recall values in.", "labels": [], "entities": [{"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9964600205421448}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9848480820655823}]}, {"text": "We consider the following experimental conditions: \uf0b7 Known category/section, using relevance classifier (red): This is the casein which the actual category and section labels of the query article are known, and only the questions that match exactly in category and section are considered for relevance classification (i.e. added to Q if found relevant by the classifier).", "labels": [], "entities": [{"text": "relevance classification", "start_pos": 292, "end_pos": 316, "type": "TASK", "confidence": 0.7135434597730637}]}, {"text": "Recall is computed with respect to the total number of relevant questions in DATASET II, including those corresponding to sections different from the section label of the article.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9836781024932861}, {"text": "DATASET II", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.7982312142848969}]}, {"text": "\uf0b7 Inferred category/section, using relevance classifier (blue): This is the expected use case, where the category/section labels are not known.", "labels": [], "entities": []}, {"text": "Questions matching in category and section(s) to the inferred category and section of each article are considered and ranked in Q by their score from the relevance classifier.", "labels": [], "entities": []}, {"text": "Recall is computed with respect to the total number of relevant questions in DATASET II.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9845783114433289}, {"text": "DATASET II", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.7101516723632812}]}, {"text": "\uf0b7 Inferred category/section, ignoring relevance classifier (green): This is a baseline where we only use category/section inference and then retrieve questions from the database without filtering: all questions that match in inferred category and section(s) of the article are added to Q in a random ranking order, without performing relevance classification.", "labels": [], "entities": [{"text": "relevance classification", "start_pos": 334, "end_pos": 358, "type": "TASK", "confidence": 0.7179298400878906}]}, {"text": "As we examine, it is important to point out a subtlety in our choice to calculate recall of the known category/section condition (red bars) with respect to the set of all relevant questions, including those that are matched to sections different from the original (labeled) sections.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9988565444946289}]}, {"text": "While this condition by construction does not have access to questions of any other section, the resulting limitation in recall underlines the importance of performing section inference: without inference, we achieve a recall of no greater than 0.4.", "labels": [], "entities": [{"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9978986978530884}, {"text": "recall", "start_pos": 219, "end_pos": 225, "type": "METRIC", "confidence": 0.9971265196800232}]}, {"text": "As we had hypothesized, while the labels of the sections play an instrumental role in instructing the crowd to generate relevant questions, the resulting questions often tend to be relevant to content found under different but semantically related sections as well.", "labels": [], "entities": []}, {"text": "Leveraging the available questions of these related sections (by performing inference) boosts recall at the expense of only a small degree of precision (blue bars).", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9992647767066956}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9991725087165833}]}, {"text": "If we forgo relevance classification entirely, we get a constant precision of 0.74 (green bars) as mentioned in Section 5.2; it is clear that the relevance classifier results in a significant advantage.", "labels": [], "entities": [{"text": "relevance classification", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.9020185470581055}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9970978498458862}]}, {"text": "While there is a slight drop in precision when using inference, this is at least partly due to the constraints that were imposed during data-collection and relevance classifier training, i.e., all pairs of articles and questions belonged to the same category and section.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9989683628082275}]}, {"text": "While this constraint made the crowdsourcing methodology proposed in this work tractable, it also prevented the inclusion of training examples for sections that could potentially be inferred attest time.", "labels": [], "entities": []}, {"text": "One possible approach to remedy this would be sample from article segments that are similar in text (in terms of our distance metric) as opposed to only segments exactly matching in category and section.", "labels": [], "entities": []}], "tableCaptions": []}