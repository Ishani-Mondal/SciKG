{"title": [{"text": "Cross-lingual Dependency Parsing Based on Distributed Representations", "labels": [], "entities": [{"text": "Cross-lingual Dependency Parsing", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.596076895793279}]}], "abstractContent": [{"text": "This paper investigates the problem of cross-lingual dependency parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich language (e.g. English).", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.6564858059088389}]}, {"text": "Existing approaches typically don't include lexical features, which are not transferable across languages.", "labels": [], "entities": []}, {"text": "In this paper, we bridge the lexical feature gap by using distributed feature representations and their composition.", "labels": [], "entities": []}, {"text": "We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space.", "labels": [], "entities": []}, {"text": "Consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8000703454017639}]}, {"text": "Furthermore, our framework is able to incorporate additional useful features such as cross-lingual word clusters.", "labels": [], "entities": [{"text": "cross-lingual word clusters", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.5893068214257559}]}, {"text": "Our combined contributions achieve an average relative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 46, "end_pos": 70, "type": "METRIC", "confidence": 0.8224018414815267}, {"text": "labeled attachment score", "start_pos": 83, "end_pos": 107, "type": "METRIC", "confidence": 0.7359382112820944}, {"text": "English universal treebank", "start_pos": 162, "end_pos": 188, "type": "DATASET", "confidence": 0.8089659611384074}]}, {"text": "It also significantly out-performs McDonald et al.", "labels": [], "entities": []}, {"text": "(2013) augmented with projected cluster features on identical data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency Parsing has been one of NLP's longstanding central problems.", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9031296968460083}]}, {"text": "The majority of work on dependency parsing has been dedicated to resource-rich languages, such as English and Chinese.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8749682605266571}]}, {"text": "For these languages, there exist large-scale * This work was done while the author was visiting JHU.", "labels": [], "entities": [{"text": "JHU", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9864185452461243}]}, {"text": "annotated treebanks that can be used for supervised training of dependency parsers.", "labels": [], "entities": []}, {"text": "However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.9734352231025696}]}, {"text": "This fact has given rise to a number of research on unsupervised methods (), annotation projection methods (), and model transfer methods ) for predicting linguistic structures.", "labels": [], "entities": [{"text": "predicting linguistic structures", "start_pos": 144, "end_pos": 176, "type": "TASK", "confidence": 0.8759190638860067}]}, {"text": "In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages.", "labels": [], "entities": []}, {"text": "The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages.", "labels": [], "entities": []}, {"text": "To solve this problem,  build a delexicalized parser -a parser that only has non-lexical features.", "labels": [], "entities": []}, {"text": "A delexicalized parser makes sense in that POS tag features are significantly predictive for unlabeled dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6851057410240173}]}, {"text": "However, for labeled dependency parsing, especially for semantic-oriented dependencies like Stanfordtype dependencies, these nonlexical features are not predictive enough.", "labels": [], "entities": [{"text": "labeled dependency parsing", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6355369985103607}]}, {"text": "propose to learn cross-lingual word clusters from multilingual paralleled unlabeled data through word alignments, and apply these clusters as features for semi-supervised delexicalized parsing.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.7033674269914627}]}, {"text": "Word clusters can bethought as a kind of coarse-grained representations of words.", "labels": [], "entities": []}, {"text": "Thus, this approach partially fills the gap of lexical features in cross-lingual learning of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7577580213546753}]}, {"text": "This paper proposes a novel approach for crosslingual dependency parsing that is based on pure distributed feature representations.", "labels": [], "entities": [{"text": "crosslingual dependency parsing", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.7203340927759806}]}, {"text": "In contrast to the discrete lexical features used in traditional dependency parsers, distributed representations map symbolic features into a continuous representation space, that can be shared across languages.", "labels": [], "entities": []}, {"text": "Therefore, our model has the ability to utilize both lexical and non-lexical features naturally.", "labels": [], "entities": []}, {"text": "Specifically, our framework contains two primary components: \u2022 A neural network-based dependency parser.", "labels": [], "entities": []}, {"text": "We expect a non-linear model for dependency parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.81672003865242}]}, {"text": "propose a transition-based dependency parser using a neural network architecture, which is simple but works well on several datasets.", "labels": [], "entities": [{"text": "transition-based dependency parser", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.649784505367279}]}, {"text": "Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural network classifier.", "labels": [], "entities": []}, {"text": "We will provide explanations for the merits of this model in Section 3, as well as how we adapt it to the cross-lingual task.", "labels": [], "entities": []}, {"text": "\u2022 Cross-lingual word representation learning.", "labels": [], "entities": [{"text": "Cross-lingual word representation learning", "start_pos": 2, "end_pos": 44, "type": "TASK", "confidence": 0.7693622559309006}]}, {"text": "The key to filling the lexical feature gap is to project the representations of these features from different languages into a common vector space, preserving the translational equivalence.", "labels": [], "entities": []}, {"text": "We will study and compare two approaches of learning cross-lingual word representations in Section 4.", "labels": [], "entities": []}, {"text": "The first approach is robust projection, and the second approach is based on canonical correlation analysis.", "labels": [], "entities": [{"text": "robust projection", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.6331684738397598}]}, {"text": "Both approaches are simple to implement and are scalable to large data.", "labels": [], "entities": []}, {"text": "We evaluate our model on the universal multilingual treebanks.", "labels": [], "entities": []}, {"text": "Case studies include transferring from English to German, Spanish and French.", "labels": [], "entities": []}, {"text": "Experiments show that by incorporating lexical features, the performance of cross-lingual dependency parsing can be improved significantly.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.6500252385934194}]}, {"text": "By further embedding crosslingual cluster features, we achieve an average relative error reduction of 10.9% in labeled attachment score (LAS), as compared with the delexicalized parsers.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 74, "end_pos": 98, "type": "METRIC", "confidence": 0.8575363159179688}, {"text": "labeled attachment score (LAS)", "start_pos": 111, "end_pos": 141, "type": "METRIC", "confidence": 0.8770552674929301}]}, {"text": "It also significantly outperforms: An example labeled dependency tree.", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel and flexible cross-lingual learning framework for dependency parsing based on distributed representations, which can effectively incorporate both lexical and non-lexical features.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.811956912279129}]}, {"text": "\u2022 We present two novel and effective approaches for inducing cross-lingual word representations, that bridge the lexical feature gap in cross-lingual dependency parsing.", "labels": [], "entities": [{"text": "cross-lingual word representations", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.6051596105098724}, {"text": "cross-lingual dependency parsing", "start_pos": 136, "end_pos": 168, "type": "TASK", "confidence": 0.6656732161839803}]}, {"text": "\u2022 We show that cross-lingual word cluster features can be effectively embedded into our model, leading to significant additive improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "All of the parsing models are trained using the development data from English for early-stopping.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.974983274936676}]}, {"text": "lists the results of the cross-lingual transfer experiments for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9046233594417572}]}, {"text": "further summarizes each of the experimental gains detailed in.", "labels": [], "entities": []}, {"text": "Our delexicalized system obtains slightly lower performance than those reported in, because we're using Before this dataset was carried out, the CoNLL multilingual dependency treebanks) were often used for evaluation.", "labels": [], "entities": [{"text": "CoNLL multilingual dependency treebanks", "start_pos": 145, "end_pos": 184, "type": "DATASET", "confidence": 0.7866198569536209}]}, {"text": "However, the major problem is that the dependency annotations vary for different languages (e.g. the choice of lexical versus functional head), which makes it impossible to evaluate the LAS., in both absolute LAS gain and relative error reduction.", "labels": [], "entities": [{"text": "LAS.", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9207949638366699}, {"text": "absolute LAS gain", "start_pos": 200, "end_pos": 217, "type": "METRIC", "confidence": 0.78792405128479}]}, {"text": "All gains are statistically significant using MaltEval at p < 0.01. 12 greedy decoding and local training.", "labels": [], "entities": []}, {"text": "Our reimplementation of) attains comparable performance with MCD13.", "labels": [], "entities": [{"text": "MCD13", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.901606559753418}]}, {"text": "For all languages we consider in this study, by using cross-lingual word embeddings either from alignment-based projection or CCA, we obtain statistically significant improvements against the delexicalized system, both in UAS and LAS.", "labels": [], "entities": []}, {"text": "Interestingly, we notice that PROJ consistently performs better than CCA by a significant margin, and is comparable to McD13 * +Cluster.", "labels": [], "entities": [{"text": "PROJ", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.41243696212768555}, {"text": "McD13 * +Cluster", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.898504450917244}]}, {"text": "We will give further analysis to this observation in Section 5.3.1 and 5.3.2.", "labels": [], "entities": []}, {"text": "Our framework is flexible for incorporating richer features simply by embedding them into continuous vectors.", "labels": [], "entities": []}, {"text": "Thus we further embed the cross-lingual word cluster features into our model, together with the proposed cross-lingual word embeddings.", "labels": [], "entities": []}, {"text": "The cluster feature template used here is similar to the POS tag feature templates: As shown in, additive improvements are obtained for both PROJ and CCA.", "labels": [], "entities": [{"text": "PROJ", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.7300170063972473}]}, {"text": "Compared with our delexicalized system, the relative error is reduced by up to 13.1% in UAS, and up to 12.6% in LAS.", "labels": [], "entities": [{"text": "error", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.6868335008621216}, {"text": "UAS", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.5108173489570618}]}, {"text": "The combined system further outperforms McD13 * augmented with cluster features significantly .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Dimensions of feature embeddings.", "labels": [], "entities": []}, {"text": " Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  \u2020 indicates the averaged UAS/LAS.", "labels": [], "entities": [{"text": "Cross-lingual transfer dependency parsing", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.8167815655469894}, {"text": "unlabeled attachment score (UAS)", "start_pos": 146, "end_pos": 178, "type": "METRIC", "confidence": 0.7877330134312311}, {"text": "labeled attachment score  (LAS)", "start_pos": 183, "end_pos": 214, "type": "METRIC", "confidence": 0.8319121102492014}, {"text": "LAS", "start_pos": 372, "end_pos": 375, "type": "METRIC", "confidence": 0.7023321390151978}]}, {"text": " Table 6: Effect of robust projection.", "labels": [], "entities": []}, {"text": " Table 7: Effect of fine-tuning word embeddings.", "labels": [], "entities": []}, {"text": " Table 8: Comparison with existing bilingual word embeddings.  \u2021 For MTL and BIAE, we use their  released bilingual word embeddings.", "labels": [], "entities": [{"text": "MTL", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.6131500005722046}, {"text": "BIAE", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.7530364990234375}]}]}