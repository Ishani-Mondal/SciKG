{"title": [{"text": "Using prosodic annotations to improve coreference resolution of spoken text", "labels": [], "entities": [{"text": "coreference resolution of spoken text", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.924955153465271}]}], "abstractContent": [{"text": "This paper is the first to examine the effect of prosodic features on coreference resolution in spoken discourse.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9736186861991882}]}, {"text": "We test features from different prosodic levels and investigate which strategies can be applied.", "labels": [], "entities": []}, {"text": "Our results on the basis of manual prosodic labelling show that the presence of an accent is a helpful feature in a machine-learning setting.", "labels": [], "entities": []}, {"text": "Including prosodic boundaries and determining whether the accent is the nuclear accent further increases results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Noun phrase coreference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same discourse entities.", "labels": [], "entities": [{"text": "Noun phrase coreference resolution", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7873948067426682}]}, {"text": "Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 ( or in the SemEval shared task.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9302253723144531}, {"text": "CoNLL shared task 2012", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.7986057549715042}]}, {"text": "have shown that there are differences between written and spoken text wrt coreference resolution and that the performance typically drops when systems that have been developed for written text are applied on spoken text.", "labels": [], "entities": [{"text": "wrt coreference resolution", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.796016792456309}]}, {"text": "There has been considerable work on coreference resolution in written text, but comparatively little work on spoken text, with a few exceptions of systems for pronoun resolution in transcripts of spoken text e.g.,.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.9429281949996948}, {"text": "pronoun resolution", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7767635881900787}]}, {"text": "However, so far, prosodic information has not been taken into account.", "labels": [], "entities": []}, {"text": "The interaction between prosodic prominence and coreference has been investigated in several experimental and theoretical analyses); for German ().", "labels": [], "entities": [{"text": "prosodic prominence", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8188990354537964}, {"text": "coreference", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9649962186813354}]}, {"text": "There is a tendency for coreferent items, i.e. entities that have already been introduced into the discourse, to be deaccented, as the speaker assumes the entity to be salient in the listener's discourse model.", "labels": [], "entities": []}, {"text": "We can exploit this by including prominence features in the coreference resolver.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8679664134979248}]}, {"text": "Our prosodic features mainly aim at definite descriptions, where it is difficult for the resolver to decide whether the potential anaphor is actually anaphoric or not.", "labels": [], "entities": []}, {"text": "In these cases, accentuation is an important means to distinguish between given entities (often deaccented) and other categories (i.e. bridging anaphors, see below) that are typically accented, particularly for entities whose heads have a different lexeme than their potential antecedent.", "labels": [], "entities": []}, {"text": "Pronouns are not the case of interest here, as they are (almost) always anaphoric.", "labels": [], "entities": []}, {"text": "To make the intuitions clearer, Example (1), taken from Umbach (2002), shows the difference prominence can make: (1) John has an old cottage. a. Last year he reconstructed the SHED. b. Last year he reconSTRUCted the shed.", "labels": [], "entities": [{"text": "SHED.", "start_pos": 176, "end_pos": 181, "type": "TASK", "confidence": 0.6656567454338074}]}, {"text": "Due to the pitch accent on shed in (1a), it is quite obvious that the shed and the cottage refer to different entities; they exemplify a bridging relation, where the shed is apart of the cottage.", "labels": [], "entities": []}, {"text": "In (1b), however, the shed is deaccented, which has the effect that the shed and the cottage corefer.", "labels": [], "entities": []}, {"text": "We present a pilot study on German spoken text that uses manual prominence marking to show the principled usefulness of prosodic features for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.9819118678569794}]}, {"text": "In the long run and for application-based settings, of course, we do not want to rely on manual annotations.", "labels": [], "entities": []}, {"text": "This work is investigating the potential of prominence information and is meant to motivate the use of automatic prosodic features.", "labels": [], "entities": []}, {"text": "Our study deals with German data, but the prosodic properties are comparable to other West Germanic languages, like English or Dutch.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work on coreference resolution in spoken text that tests the theoretical claims regarding the interaction between coreference and prominence in a general, state-of-the-art coreference resolver, and shows that prosodic features improve coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution in spoken text", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.9023689746856689}, {"text": "coreference resolver", "start_pos": 220, "end_pos": 240, "type": "TASK", "confidence": 0.85688316822052}, {"text": "coreference resolution", "start_pos": 283, "end_pos": 305, "type": "TASK", "confidence": 0.9395415782928467}]}], "datasetContent": [{"text": "We perform our experiments using the IMS HotCoref system), a state-of-the-art coreference resolution system for English.", "labels": [], "entities": [{"text": "IMS HotCoref system", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.8343174258867899}, {"text": "coreference resolution", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.8090882301330566}]}, {"text": "As German is not a language that is featured in the standard resolver, we first had to adapt it.", "labels": [], "entities": [{"text": "resolver", "start_pos": 61, "end_pos": 69, "type": "TASK", "confidence": 0.9475530982017517}]}, {"text": "These adaptations include gender and number agreement, lemma-based (sub)string match and a feature that addresses German compounds, to name only a few.", "labels": [], "entities": [{"text": "gender and number agreement", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.545798271894455}]}, {"text": "2 2 To download the German coreference system, visit: www.ims.uni-stuttgart.de/forschung/ ressourcen/werkzeuge/HOTCorefDe.html For our experiments on prosodic features, we use the DIRNDL corpus 3 (ca.", "labels": [], "entities": [{"text": "DIRNDL corpus 3 (ca.", "start_pos": 180, "end_pos": 200, "type": "DATASET", "confidence": 0.9267437160015106}]}, {"text": "50.000 tokens, 3221 sentences), a radio news corpus annotated with both manual coreference and manual prosody labels ( . We adopt the official train, test and development split.", "labels": [], "entities": []}, {"text": "We decided to remove abstract anaphors (e.g. anaphors that refer to events or facts), which are not resolved by the system.", "labels": [], "entities": []}, {"text": "In all experiments, we only use predicted annotations and no gold mention boundary (GB) information as we aim at real end-to-end coreference resolution.", "labels": [], "entities": [{"text": "gold mention boundary (GB) information", "start_pos": 61, "end_pos": 99, "type": "METRIC", "confidence": 0.7688424927847726}, {"text": "coreference resolution", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.9367208182811737}]}, {"text": "On DIRNDL, our system achieves a CoNLL score of 47.93, which will serve as a baseline in our experiments.", "labels": [], "entities": [{"text": "DIRNDL", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.831451416015625}, {"text": "CoNLL score", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9812021553516388}]}, {"text": "To put the baseline in context, we also report performance on the German reference corpus T\u00fcBa-D/Z 5), which consists 3 http://www.ims.uni-stuttgart.de/ forschung/ressourcen/korpora/dirndl.html In this work, we have focused on improvements within the clearly defined field of coreference resolution, using prosodic features.", "labels": [], "entities": [{"text": "German reference corpus T\u00fcBa-D/Z 5", "start_pos": 66, "end_pos": 100, "type": "DATASET", "confidence": 0.8412815715585437}, {"text": "coreference resolution", "start_pos": 276, "end_pos": 298, "type": "TASK", "confidence": 0.9653690755367279}]}, {"text": "As one of the reviewers pointed out, the DIRNDL corpus additionally features manual two-level information status annotations according to the RefLex scheme (, which additionally distinguishes bridging anaphors, deictic expressions, and more.", "labels": [], "entities": [{"text": "DIRNDL corpus", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8982405364513397}]}, {"text": "Recent work on smaller datasets of read text has shown that there is a meaningful correspondence between information status classes and degrees of prosodic prominence, with regard to both pitch accent type and position (.", "labels": [], "entities": []}, {"text": "Moreover, information status classification has been identified as a task closely related to coreference resolution.", "labels": [], "entities": [{"text": "information status classification", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8854213754336039}, {"text": "coreference resolution", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9616838097572327}]}, {"text": "Integrating these approaches is a promising, though rather complex task, which we reserve for future work.", "labels": [], "entities": []}, {"text": "It might, furthermore, require more detailed prosodic analyses than are currently available in DIRNDL.", "labels": [], "entities": [{"text": "DIRNDL", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.9456866979598999}]}, {"text": "compares the performance of our system against CorZu (), a rule-based state-of-the-art system for German 9 (on the newest T\u00fcBa dataset).", "labels": [], "entities": [{"text": "German 9", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.926213949918747}, {"text": "T\u00fcBa dataset", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.9658183753490448}]}, {"text": "shows the effect of the respective features which are not informed about intonation boundaries) and those that are).", "labels": [], "entities": []}, {"text": "Features that achieved a significant improvement over the baseline are marked in boldface.", "labels": [], "entities": []}, {"text": "The best-performing feature in is the presence of a pitch accent in short NPs.", "labels": [], "entities": []}, {"text": "It can be seen that this feature has a negative effect when being applied on all NPs.", "labels": [], "entities": []}, {"text": "Presumably, this is because the system is misled to classify a higher number of complex anaphoric expressions as non-anaphoric, due to the presence of pitch accents.", "labels": [], "entities": []}, {"text": "This confirms our conjecture that long NPs will always contain some kind of accent and we cannot distinguish nu-6 http://stel.ub.edu/semeval2010-coref/ 7 Using the official CoNLL scorer v8.01, including singletons as they are part of T\u00fcBa 8 8 Using the official CoNLL scorer v8.01, not including singletons as T\u00fcBa 9 does not contain them.", "labels": [], "entities": [{"text": "CoNLL scorer v8.01", "start_pos": 173, "end_pos": 191, "type": "DATASET", "confidence": 0.850426971912384}, {"text": "CoNLL scorer v8.01", "start_pos": 262, "end_pos": 280, "type": "DATASET", "confidence": 0.9178719917933146}]}, {"text": "CorZu performance: Don Tuggener, personal communication.", "labels": [], "entities": []}, {"text": "We did not use CorZu for our experiments as the integration of prosodic information in a rulebased system is non-trivial.", "labels": [], "entities": []}, {"text": "We compute significance using the Wilcoxon signed rank test presents the performance of the features that are phonologically more informed.", "labels": [], "entities": []}, {"text": "Distinguishing between prenuclear and nuclear accents (NuclearType) is a feature that works best for short NPs where there is only one accent, while having a negative effect on all NPs.", "labels": [], "entities": [{"text": "NuclearType", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9145481586456299}]}, {"text": "Nuclear presence, however, works well for both versions (not distinguishing between n1 or n2 works for short NPs while n2 accents only works best for all NPs).", "labels": [], "entities": []}, {"text": "This feature achieves the overall best performance for both short NPs (48.76) and all NPs (48.88).", "labels": [], "entities": []}, {"text": "The NuclearBagOfAccents feature works quite well, too: this is a feature designed for NPs that have more than one accent and so it works best for complex NPs.", "labels": [], "entities": []}, {"text": "Combining the features did not lead to any improvements.", "labels": [], "entities": []}, {"text": "Overall, it becomes clear that one has to be very careful in terms of how the prosodic information is used.", "labels": [], "entities": []}, {"text": "In general, the presence of an accent works better than the distinction between certain accent types, and including intonation boundary information also contributes to the system's performance.", "labels": [], "entities": []}, {"text": "When including this information, we can observe that when we look at the presence of a pitch accent (the best-performing feature), the distinction between prenuclear and nuclear is an important one: not distinguishing between prenuclear and nuclear deteriorates results.", "labels": [], "entities": []}, {"text": "The results also seem to sug-gest that simpler features (like the presence or absence of a certain type of pitch accent) work best for simple (i.e. short) phrases.", "labels": [], "entities": []}, {"text": "For longer markables this effect turns into the negative.", "labels": [], "entities": []}, {"text": "This probably means that simple features cannot do justice to the complex prosody of longer NPs, which gets blurred.", "labels": [], "entities": []}, {"text": "The obvious solution is to define more complex features that approximate the rhythmic pattern (or even the prosodic contour) found on longer phrases, which however will require more data and, ideally, automatic prosodic annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SemEval Shared Task 2010 post-task  evaluation for track regular (on T\u00fcBa 8), includ- ing and excluding singletons", "labels": [], "entities": [{"text": "SemEval Shared Task", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8072206775347391}, {"text": "T\u00fcBa 8)", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.8865410486857096}]}, {"text": " Table 2: IMS HotCoref performance on T\u00fcBa 9  (no singletons), using regular preprocessing", "labels": [], "entities": [{"text": "IMS HotCoref", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.5380353182554245}, {"text": "T\u00fcBa 9", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.927057683467865}]}, {"text": " Table 3: CoNLL metric scores on DIRNDL for  different prosodic features (no singletons, signifi- cant results in boldface)", "labels": [], "entities": [{"text": "CoNLL metric", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.7494255900382996}]}]}