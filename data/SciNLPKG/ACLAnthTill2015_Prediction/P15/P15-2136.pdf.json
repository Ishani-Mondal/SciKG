{"title": [{"text": "Learning Summary Prior Representation for Extractive Summarization", "labels": [], "entities": [{"text": "Learning Summary Prior Representation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6911396756768227}, {"text": "Extractive Summarization", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7044358104467392}]}], "abstractContent": [{"text": "In this paper, we propose the concept of summary prior to define how much a sentence is appropriate to be selected into summary without consideration of its context.", "labels": [], "entities": []}, {"text": "Different from previous work using manually compiled document-independent features, we develop a novel summary system called PriorSum, which applies the enhanced convolutional neu-ral networks to capture the summary prior features derived from length-variable phrases.", "labels": [], "entities": []}, {"text": "Under a regression framework, the learned prior features are concate-nated with document-dependent features for sentence ranking.", "labels": [], "entities": []}, {"text": "Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines.", "labels": [], "entities": [{"text": "DUC generic summarization benchmarks", "start_pos": 19, "end_pos": 55, "type": "DATASET", "confidence": 0.8471431583166122}, {"text": "PriorSum", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9408468008041382}]}], "introductionContent": [{"text": "Sentence ranking, the vital part of extractive summarization, has been extensively investigated.", "labels": [], "entities": [{"text": "Sentence ranking", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8820992112159729}, {"text": "extractive summarization", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.793060839176178}]}, {"text": "Regardless of ranking models, feature engineering largely determines the final summarization performance.", "labels": [], "entities": []}, {"text": "Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity).", "labels": [], "entities": []}, {"text": "The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in.", "labels": [], "entities": []}, {"text": "Take the following two sentences as an example: 1.", "labels": [], "entities": []}, {"text": "Hurricane Emily slammed into Dominica on September 22, causing 3 deaths with its wind gusts up to 110 mph.", "labels": [], "entities": []}, {"text": "* Contribution during internship at Microsoft Research 2.", "labels": [], "entities": []}, {"text": "It was Emily, the hurricane which caused 3 deaths and armed with wind guests up to 110 mph, that slammed into Dominica on Tuesday.", "labels": [], "entities": [{"text": "Emily", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8303731679916382}]}, {"text": "The first sentence describes the major information of a hurricane.", "labels": [], "entities": []}, {"text": "With similar meaning, the second sentence uses an emphatic structure and is somewhat verbose.", "labels": [], "entities": []}, {"text": "Obviously the first one should be preferred fora news summary.", "labels": [], "entities": []}, {"text": "In this paper, we call such fact as summary prior nature 1 and learn document-independent features to reflect it.", "labels": [], "entities": []}, {"text": "In previous summarization systems, though not well-studied, some widely-used sentence ranking features such as the length and the ratio of stopwords, can be seen as attempts to measure the summary prior nature to a certain extent.", "labels": [], "entities": []}, {"text": "Notably, built a state-of-the-art summarization system through making use of advanced document-independent features.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9595344662666321}]}, {"text": "However, these document-independent features are usually hand-crafted, difficult to exhaust each aspect of the summary prior nature.", "labels": [], "entities": []}, {"text": "Meanwhile, items representing the same feature may contribute differently to a summary.", "labels": [], "entities": []}, {"text": "For example, \"September 22\" and \"Tuesday\" are both indicators of time, but the latter seldom occurs in a summary due to uncertainty.", "labels": [], "entities": []}, {"text": "In addition, to the best of our knowledge, document-independent features beyond word level (e.g., phrases) are seldom involved in current research.", "labels": [], "entities": []}, {"text": "The CTSUM system developed by is the most relevant to ours.", "labels": [], "entities": []}, {"text": "It attempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization.", "labels": [], "entities": [{"text": "certainty", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9789825677871704}]}, {"text": "To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR).", "labels": [], "entities": [{"text": "certainty score", "start_pos": 17, "end_pos": 32, "type": "METRIC", "confidence": 0.9761237800121307}, {"text": "Support Vector Regression (SVR)", "start_pos": 144, "end_pos": 175, "type": "METRIC", "confidence": 0.7999464770158132}]}, {"text": "How-ever, a low certainty score does not always represent low quality of being a summary sentence.", "labels": [], "entities": [{"text": "certainty score", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.9537167549133301}]}, {"text": "For example, the sentence below is from a topic about \"Korea nuclear issue\" in DUC 2004: Clinton acknowledged that U.S. is not yet certain that the suspicious underground construction project in North Korea is nuclear related.", "labels": [], "entities": [{"text": "Korea nuclear issue\" in DUC 2004", "start_pos": 55, "end_pos": 87, "type": "DATASET", "confidence": 0.619342931679317}]}, {"text": "The underlined phrases greatly reduce the certainty of this sentence according to's model.", "labels": [], "entities": [{"text": "certainty", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9972891807556152}]}, {"text": "But, in fact, this sentence can summarize the government's attitude and is salient enough in the related documents.", "labels": [], "entities": []}, {"text": "Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature.", "labels": [], "entities": [{"text": "certainty", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9514908790588379}]}, {"text": "To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature.", "labels": [], "entities": []}, {"text": "Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases.", "labels": [], "entities": [{"text": "latent feature representation", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6731589039166769}]}, {"text": "Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior.", "labels": [], "entities": []}, {"text": "PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1).", "labels": [], "entities": [{"text": "PriorSum", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8821039199829102}, {"text": "sentence regression", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.6869296580553055}]}, {"text": "We conduct extensive experiments on the and 2004 generic multi-document summarization datasets.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that our model outperforms stateof-the-art extractive summarization approaches.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.7234799265861511}]}, {"text": "Meanwhile, we analyze the different aspects supporting the summary prior in Section 3.3.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our work, we focus on the generic multidocument summarization task and carryout experiments on DUC 2001 2004 datasets.", "labels": [], "entities": [{"text": "generic multidocument summarization task", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.6366325169801712}, {"text": "DUC 2001 2004 datasets", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.9787247031927109}]}, {"text": "All the documents are from newswires and grouped into various thematic clusters.", "labels": [], "entities": []}, {"text": "The summary length is limited to 100 words (665 bytes for DUC 2004).", "labels": [], "entities": [{"text": "DUC 2004)", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9430922468503317}]}, {"text": "We use DUC 2003 data as the development set and conduct a 3-fold cross-validation on and 2004 datasets with two years of data as training set and one year of data as test set.", "labels": [], "entities": [{"text": "DUC 2003 data", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9690698186556498}]}, {"text": "We directly use the look-up table of 25-dimensional word embeddings trained by the model of.", "labels": [], "entities": []}, {"text": "These small word embeddings largely reduces model parameters.", "labels": [], "entities": []}, {"text": "The dimension l of the hidden documentindependent features is experimented in the range of, and the window sizes are experimented between 1 and 5.", "labels": [], "entities": []}, {"text": "Through parameter experiments on development set, we set l = 20 and m = 3 for PriorSum.", "labels": [], "entities": [{"text": "PriorSum", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9731500744819641}]}, {"text": "To update the weights W ht and w r , we apply the diagonal variant of AdaGrad with minibatches ().", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9594742059707642}]}, {"text": "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE, and take ROUGE-1 and ROUGE-2 as the main measures.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.7802838683128357}, {"text": "ROUGE-1", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9829787611961365}, {"text": "ROUGE-2", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9705386161804199}]}], "tableCaptions": [{"text": " Table 2: Comparison results (%) on DUC datasets.", "labels": [], "entities": [{"text": "Comparison", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9122328758239746}, {"text": "DUC datasets", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9850724637508392}]}]}