{"title": [{"text": "Early and Late Combinations of Criteria for Reranking Distributional Thesauri", "labels": [], "entities": [{"text": "Reranking Distributional Thesauri", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.8567735354105631}]}], "abstractContent": [{"text": "In this article, we first propose to exploit anew criterion for improving distri-butional thesauri.", "labels": [], "entities": []}, {"text": "Following a bootstrap-ping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classifier performing the reranking of a thesaurus.", "labels": [], "entities": []}, {"text": "Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "The work presented in this article aims at improving thesauri built following the distributional approach as implemented by.", "labels": [], "entities": []}, {"text": "A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words) or their reweighting, either by turning the weights of these components into ranks () or by adapting them through a bootstrapping method from the thesaurus to improve.", "labels": [], "entities": []}, {"text": "The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis, multi-prototype or exemplar-based models, neural approaches ( or the adoption of a Bayesian viewpoint (.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.6688786943753561}]}, {"text": "Our work follows, which proposed a different way from to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an initial thesaurus and training from them a classifier to rerank the semantic neighbors of the initial thesaurus entries.", "labels": [], "entities": []}, {"text": "More precisely, we propose anew criterion for this selection, based on the similarity relations of the components of similar compounds, and we show two modes -early and late -of combination of thesauri reranked from different criteria, including ours, leading to significant further improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first step of the work we present is the building of two distributional thesauri: the thesaurus of mono-terms to improve (A2ST) and a thesaurus of compounds (A2ST-comp).", "labels": [], "entities": []}, {"text": "Similarly to, they were both built from the AQUAINT-2 corpus, a 380 million-word corpus of news articles in English.", "labels": [], "entities": [{"text": "AQUAINT-2 corpus", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.9454865455627441}]}, {"text": "The building procedure, defined by, was also identical to, with distributional contexts compared with the Cosine measure and made of window-based lemmatized cooccurrents (1 word before and after) weighted by Positive Pointwise Mutual Information (PPMI).", "labels": [], "entities": []}, {"text": "For the thesaurus of compounds, a preprocessing step was added to identify nominal compounds in texts.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9625651240348816}]}, {"text": "This identification was done in two steps: first, a set of compounds were extracted from the AQUAINT-2 corpus by relying on a restricted set of morpho-syntactic patterns applied by the Multiword Expression Toolkit (mwetoolkit) (; then, the most frequent compounds in this set (frequency > 100) were selected as reference and their occurrences in the AQUAINT-2 corpus were identified by applying the longest-match strategy to the output of the TreeTagger part-of-speech tagger 1 . Finally, distributional contexts made of mono-terms and compounds were built as stated above and neighbors were found for 29,174 compounds.", "labels": [], "entities": [{"text": "AQUAINT-2 corpus", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.9541763961315155}, {"text": "AQUAINT-2 corpus", "start_pos": 350, "end_pos": 366, "type": "DATASET", "confidence": 0.9528406262397766}]}, {"text": "For our SVM models, we adopted the RBF kernel, as, and a grid search strategy for optimizing both the \u03b3 and C parameters by applying a 5-fold cross validation procedure to our training set and adopting the precision measure as the evaluation function to optimize.", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.6852267384529114}, {"text": "precision measure", "start_pos": 206, "end_pos": 223, "type": "METRIC", "confidence": 0.9811086356639862}]}, {"text": "The models were built with LIBSVM () and then applied to the neighbors of our initial thesaurus.", "labels": [], "entities": [{"text": "LIBSVM", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.8464331030845642}]}, {"text": "gives the results of the reranking for both the method we propose, compound (comp.", "labels": [], "entities": []}, {"text": "for short), with examples selected by rules and, and the one of, symmetry.", "labels": [], "entities": [{"text": "symmetry", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9533143043518066}]}, {"text": "In either case, they correspond to an intrinsic evaluation achieved by comparing the semantic neighbors of each thesaurus entry with the synonyms and related words of our Gold Standard resource for that entry.", "labels": [], "entities": [{"text": "Gold Standard resource", "start_pos": 171, "end_pos": 193, "type": "DATASET", "confidence": 0.8931965430577596}]}, {"text": "12,243 entries with frequency > 10 were present in this resource and evaluated in such away.", "labels": [], "entities": []}, {"text": "As the neighbors are ranked according to their similarity value with their entry, we adopted the classical evaluation measures of Information Retrieval by replacing documents with synonyms and queries with entries: R-precision (R-prec.), Mean Average Precision (MAP) and precision at different cut-offs (1, 5 and 10).", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.7362154424190521}, {"text": "Mean Average Precision (MAP)", "start_pos": 238, "end_pos": 266, "type": "METRIC", "confidence": 0.9682345986366272}, {"text": "precision", "start_pos": 271, "end_pos": 280, "type": "METRIC", "confidence": 0.998100221157074}]}, {"text": "More precisely, the initial row of gives the values of these measures for our initial thesaurus of mono-terms while its A2ST-comp row corresponds to the measures for our thesaurus of compounds.", "labels": [], "entities": [{"text": "A2ST-comp row", "start_pos": 120, "end_pos": 133, "type": "METRIC", "confidence": 0.9745895862579346}]}, {"text": "It should be note that in the case of the A2ST-comp thesaurus, the number of evaluated entries is very small, restricted to 813 entries, with also a very small number of reference synonyms by entry.", "labels": [], "entities": []}, {"text": "Hence, the results of the evaluation of A2ST-comp have to be considered with caution even if their high level for the very first semantic neighbors tends to confirm the positive impact of the low level of ambiguity of compounds compared to mono-terms.", "labels": [], "entities": [{"text": "A2ST-comp", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.6439789533615112}]}, {"text": "The two following rows gives the results of the thesauri built from the best models of ( ), B14-count for the count model, whose main parameters are close or identical to ours, and B14-predict for the predict model, built from (.", "labels": [], "entities": [{"text": "B14-count", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9825771450996399}, {"text": "B14-predict", "start_pos": 181, "end_pos": 192, "type": "METRIC", "confidence": 0.9784844517707825}]}, {"text": "These results first illustrate the known importance of corpus size, as the ( )'s corpus is more than 7 times larger than ours, and the fact that for building thesauri, the count model is superior to the predict model.", "labels": [], "entities": []}, {"text": "This last observation is confirmed by the results of the skip-gram model of () with its best parameters 2 for our corpus (5 th row), which clearly exhibits worst results than initial.", "labels": [], "entities": []}, {"text": "For this Mikolov thesaurus and the following reranked ones, each value corresponds to the difference between the measure for the considered thesaurus and the measure for the initial thesaurus.", "labels": [], "entities": []}, {"text": "All these differences were found statistically significant according to a paired Wilcoxon test with p-value < 0.05.: Evaluation of our initial thesaurus and its reranked versions (values = percentages).", "labels": [], "entities": []}, {"text": "The analysis of the next two rows of Table 2 first shows that each criterion used for reranking our initial thesaurus leads to a global increase of results.", "labels": [], "entities": []}, {"text": "The extent of this increase is quite similar for the two criteria: symmetry slightly outperforms compound but the difference is not significant.", "labels": [], "entities": [{"text": "symmetry", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9964624047279358}]}, {"text": "This increase is higher for P@{1,5,10} than for R-precision and MAP, which can be explained by the high number of synonyms and related words, 38.7 on average, that an entry of our initial thesaurus has in our reference.", "labels": [], "entities": [{"text": "MAP", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.7849571704864502}]}, {"text": "Hence, even a significant increase of P@{1,5,10} may have a modest impact on R-precision and MAP as the overall recall, equal to 9.8%, is low.", "labels": [], "entities": [{"text": "P@{1,5,10", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9667920668919882}, {"text": "R-precision", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9561575055122375}, {"text": "MAP", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9748368859291077}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9996212720870972}]}], "tableCaptions": [{"text": " Table 1: Selection of examples.", "labels": [], "entities": []}, {"text": " Table 1.  Concerning the method we propose,", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of our initial thesaurus and its  reranked versions (values = percentages).", "labels": [], "entities": []}]}