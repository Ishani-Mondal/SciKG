{"title": [], "abstractContent": [{"text": "Constituent parsing is typically modeled by a chart-based algorithm under prob-abilistic context-free grammars or by a transition-based algorithm with rich features.", "labels": [], "entities": [{"text": "Constituent parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8523700535297394}]}, {"text": "Previous models rely heavily on richer syntactic information through lex-icalizing rules, splitting categories, or memorizing long histories.", "labels": [], "entities": []}, {"text": "However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena.", "labels": [], "entities": []}, {"text": "We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure.", "labels": [], "entities": []}, {"text": "Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters.", "labels": [], "entities": [{"text": "transition-based neural constituent parsing", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.5891968011856079}, {"text": "F1 score", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9817243814468384}]}], "introductionContent": [{"text": "A popular parsing algorithm is a cubic time chartbased dynamic programming algorithm that uses probabilistic context-free grammars (PCFGs).", "labels": [], "entities": []}, {"text": "However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts.", "labels": [], "entities": []}, {"text": "To address this problem, various contexts are incorporated into the grammars through lexicalization) or category splitting either manually) or automatically ().", "labels": [], "entities": [{"text": "category splitting", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7358624339103699}]}, {"text": "Recently a rich feature set was introduced to capture the lexical contexts * The first author is now affiliated within each span without extra annotations in grammars ().", "labels": [], "entities": []}, {"text": "Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently.", "labels": [], "entities": []}, {"text": "show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing.", "labels": [], "entities": []}, {"text": "The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference ().", "labels": [], "entities": []}, {"text": "In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems.", "labels": [], "entities": []}, {"text": "Furthermore, the enriched models are still insufficient to capture various syntactic relations in texts due to the limited contexts represented in latent annotations or non-local features.", "labels": [], "entities": []}, {"text": "Recently introduced compositional vector grammar (CVG) to address the above limitations.", "labels": [], "entities": [{"text": "compositional vector grammar (CVG)", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.7549448062976202}]}, {"text": "However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural networkbased parser -transition-based neural constituent parsing (TNCP) -which can guarantee efficient search naturally.", "labels": [], "entities": [{"text": "transition-based neural constituent parsing (TNCP)", "start_pos": 56, "end_pos": 106, "type": "TASK", "confidence": 0.746686407497951}]}, {"text": "TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing.", "labels": [], "entities": []}, {"text": "More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network in backward direction).", "labels": [], "entities": []}, {"text": "The stack structure is also modeled similarly to RNNs, and its top item is updated using the previously constructed hidden representations saved in the stack.", "labels": [], "entities": []}, {"text": "The representations from both the stack and queue are combined with the representations propagated from the partially parsed tree structure inspired by the recursive neural networks of CVGs.", "labels": [], "entities": []}, {"text": "Parameters are estimated efficiently by a variant of max-violation () which considers the worst mistakes found during search and updates parameters based on the expected mistake.", "labels": [], "entities": []}, {"text": "Under similar settings, TCNP performs comparably to state-of-the-art parsers.", "labels": [], "entities": []}, {"text": "Experimental results obtained using the Wall Street Journal corpus of the English Penn Treebank achieved a labeled F1 score of 90.68%, and the result for the Penn Chinese Treebank was 84.33%.", "labels": [], "entities": [{"text": "Wall Street Journal corpus of the English Penn Treebank", "start_pos": 40, "end_pos": 95, "type": "DATASET", "confidence": 0.9282675385475159}, {"text": "F1 score", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9723862111568451}, {"text": "Penn Chinese Treebank", "start_pos": 158, "end_pos": 179, "type": "DATASET", "confidence": 0.9751245180765787}]}, {"text": "Our parser performs no reranking with computationally expensive models, employs no templates for feature engineering, and requires no additional monolingual data for reliable parameter estimation.", "labels": [], "entities": []}, {"text": "The source code and models will be made public 1 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of various state/word rep- resentation dimension size measured by labeled  F1(%). \"-32\" denotes the hidden state size m =  32. The numbers in bold indicate the best results  for each hidden state dimension.", "labels": [], "entities": [{"text": "word rep- resentation dimension size", "start_pos": 38, "end_pos": 74, "type": "METRIC", "confidence": 0.6801800777514776}, {"text": "F1", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9735187888145447}]}, {"text": " Table 2: Comparison of network structures mea- sured by labeled F1(%).", "labels": [], "entities": [{"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9291070103645325}]}, {"text": " Table 3: Comparison of loss functions measured  by labeled F1(%).", "labels": [], "entities": [{"text": "F1", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.891710638999939}]}, {"text": " Table 4: Comparison of different parsers on the  WSJ test data measured by labeled F1(%).", "labels": [], "entities": [{"text": "WSJ test data", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9357210993766785}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.896152913570404}]}, {"text": " Table 5: Comparison of different parsers on the  CTB test data measured by labeled F1(%).", "labels": [], "entities": [{"text": "CTB test data", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9151050448417664}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.8630629777908325}]}, {"text": " Table 6: Comparison of parsing speed by varying  beam size and hidden dimension; each cell shows  the number of sentences per second/labeled F1(%)  measured on the test data.", "labels": [], "entities": [{"text": "F1", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.8946682214736938}]}, {"text": " Table 7: Comparison of different parsers on the  WSJ test data measured by average number of er- rors per sentence; the numbers in bold indicate the  least errors in each error type.", "labels": [], "entities": [{"text": "WSJ test data", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9582223097483317}]}]}