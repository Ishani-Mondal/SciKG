{"title": [{"text": "The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation.", "labels": [], "entities": []}, {"text": "FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6644971370697021}]}, {"text": "In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs).", "labels": [], "entities": [{"text": "FOFE", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9292092323303223}]}, {"text": "Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular recurrent neural network (RNN) LMs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models play an important role in many applications like speech recognition, machine translation, information retrieval and nature language understanding.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7939615249633789}, {"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.806927502155304}, {"text": "information retrieval", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.8199044466018677}, {"text": "nature language understanding", "start_pos": 132, "end_pos": 161, "type": "TASK", "confidence": 0.758519450823466}]}, {"text": "Traditionally, the back-off n-gram models are the standard approach to language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7568469345569611}]}, {"text": "Recently, neural networks have been successfully applied to language modeling, yielding the stateof-the-art performance in many tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7999244332313538}]}, {"text": "In neural network language models (NNLM), the feedforward neural networks (FNN) and recurrent neural networks (RNN)) are two popular architectures.", "labels": [], "entities": []}, {"text": "The basic idea of NNLMs is to use a projection layer to project discrete words into a continuous space and estimate word conditional probabilities in this space, which maybe smoother to better generalize to unseen contexts.", "labels": [], "entities": []}, {"text": "FNN language models (FNN-LM) usually use a limited history within a fixed-size context window to predict the next word.", "labels": [], "entities": []}, {"text": "RNN language models (RNN-LM)) adopt a time-delayed recursive architecture for the hidden layers to memorize the long-term dependency in language.", "labels": [], "entities": []}, {"text": "Therefore, it is widely reported that RNN-LMs usually outperform FNNLMs in language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7443032264709473}]}, {"text": "While RNNs are theoretically powerful, the learning of RNNs needs to use the so-called back-propagation through time (BPTT) due to the internal recurrent feedback cycles.", "labels": [], "entities": [{"text": "back-propagation through time (BPTT)", "start_pos": 87, "end_pos": 123, "type": "METRIC", "confidence": 0.7029489179452261}]}, {"text": "The BPTT significantly increases the computational complexity of the learning algorithms and it may cause many problems in learning, such as gradient vanishing and exploding.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6481571793556213}]}, {"text": "More recently, some new architectures have been proposed to solve these problems.", "labels": [], "entities": []}, {"text": "For example, the long short term memory (LSTM) RNN) is an enhanced architecture to implement the recurrent feedbacks using various learnable gates, and it has obtained promising results on handwriting recognition and sequence modeling.", "labels": [], "entities": [{"text": "handwriting recognition", "start_pos": 189, "end_pos": 212, "type": "TASK", "confidence": 0.9423565864562988}, {"text": "sequence modeling", "start_pos": 217, "end_pos": 234, "type": "TASK", "confidence": 0.7835970818996429}]}, {"text": "Comparing with RNN-LMs, FNN-LMs can be learned in a simpler and more efficient way.", "labels": [], "entities": []}, {"text": "However, FNN-LMs cannot model the long-term dependency in language due to the fixed-size input window.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel encoding method for discrete sequences, named fixedsize ordinally-forgetting encoding (FOFE), which can almost uniquely encode any variable-length word sequence into a fixed-size code.", "labels": [], "entities": [{"text": "fixedsize ordinally-forgetting encoding (FOFE", "start_pos": 80, "end_pos": 125, "type": "METRIC", "confidence": 0.5037370085716247}]}, {"text": "Relying on a constant forgetting factor, FOFE can model the word order in a sequence based on a simple ordinally-forgetting mechanism, which uses the position of each word in the sequence.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.6129644513130188}]}, {"text": "Both the theoretical analysis and the experimental simulation have shown that FOFE can provide almost unique codes for variable-length word sequences as long as the forgetting factor is properly selected.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9465400576591492}]}, {"text": "In this work, we apply FOFE to neural network language models, where the fixedsize FOFE codes are fed to FNNs as input to predict next word, enabling FNN-LMs to model long-term dependency in language.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.8930637836456299}, {"text": "FOFE", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.8521584868431091}]}, {"text": "Experiments on two benchmark tasks, Penn Treebank Corpus (PTB) and Large Text Compression Benchmark (LTCB), have shown that FOFE-based FNN-LMs cannot only significantly outperform the standard fixed-input FNN-LMs but also achieve better performance than the popular RNN-LMs with or without using LSTM.", "labels": [], "entities": [{"text": "Penn Treebank Corpus (PTB)", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.9617445866266886}]}, {"text": "Moreover, our implementation also shows that FOFE based FNN-LMs can be learned very efficiently on GPUs without the complex BPTT procedure.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.8922464847564697}]}], "datasetContent": [{"text": "We have evaluated the FOFE method for NNLMs on two benchmark tasks: i) the Penn Treebank (PTB) corpus of about 1M words, following the same setup as).", "labels": [], "entities": [{"text": "FOFE", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.98768150806427}, {"text": "Penn Treebank (PTB) corpus", "start_pos": 75, "end_pos": 101, "type": "DATASET", "confidence": 0.9680218497912089}]}, {"text": "The vocabulary size is limited to 10k.", "labels": [], "entities": []}, {"text": "The preprocessing method and the way to split data into training/validation/test sets are the same as. ii) The Large Text Compression Benchmark (LTCB).", "labels": [], "entities": []}, {"text": "In LTCB, we use the enwik9 dataset, which is composed of the first 10 9 bytes of enwiki-20060303-pages-articles.xml.", "labels": [], "entities": [{"text": "LTCB", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9371573328971863}, {"text": "enwik9 dataset", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.9181771576404572}]}, {"text": "We split it into three parts: training (153M), validation (8.9M) and test (8.9M) sets.", "labels": [], "entities": []}, {"text": "We limit the vocabulary size to 80k for LTCB and replace all out-of-vocabulary words by <UNK>.", "labels": [], "entities": [{"text": "LTCB", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.7947068214416504}]}, {"text": "We have first evaluated the performance of the traditional FNN-LMs, taking the previous several words as input, denoted as n-gram FNN-LMs here.", "labels": [], "entities": []}, {"text": "We have trained neural networks with a linear projection layer (of 200 hidden nodes) and two hidden layers (of 400 nodes per layer).", "labels": [], "entities": []}, {"text": "All hidden units in networks use the rectified linear activation function, i.e., f (x) = max(0, x).", "labels": [], "entities": []}, {"text": "The nets are initialized based on the normalized initialization  in, without using any pre-training.", "labels": [], "entities": []}, {"text": "We use SGD with a mini-batch size of 200 and an initial learning rate of 0.4.", "labels": [], "entities": []}, {"text": "The learning rate is kept fixed as long as the perplexity on the validation set decreases by at least 1.", "labels": [], "entities": []}, {"text": "After that, we continue six more epochs of training, where the learning rate is halved after each epoch.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9522846639156342}]}, {"text": "The performance (in perplexity) of several n-gram FNN-LMs (from bigram to 6-gram) is shown in.", "labels": [], "entities": []}, {"text": "For the FOFE-FNNLMs, the net architecture and the parameter setting are the same as above.", "labels": [], "entities": [{"text": "FOFE-FNNLMs", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8972610831260681}]}, {"text": "The mini-batch size is also 200 and each minibatch is composed of several sentences up to 200 words (the last sentence maybe truncated).", "labels": [], "entities": []}, {"text": "All sentences in the corpus are randomly shuffled at the beginning of each epoch.", "labels": [], "entities": []}, {"text": "In this experiment, we first investigate how the forgetting factor \u03b1 may affect the performance of LMs.", "labels": [], "entities": []}, {"text": "We have trained two FOFE-FNNLMs: i) 1st-order (using z t as input to FNN for each time t; ii) 2nd-order (using both z t and z t\u22121 as input for each time t, with a forgetting factor varying between [0.0, 1.0].", "labels": [], "entities": [{"text": "FOFE-FNNLMs", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.6862086653709412}, {"text": "FNN", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9306500554084778}, {"text": "forgetting factor", "start_pos": 163, "end_pos": 180, "type": "METRIC", "confidence": 0.9585649371147156}]}, {"text": "Experimental results in have shown that a good choice of \u03b1 lies between [0.5, 0.8].", "labels": [], "entities": []}, {"text": "Using a too large or too small forgetting factor will hurt the performance.", "labels": [], "entities": []}, {"text": "A too small forgetting factor may limit the memory of the encoding while a too large \u03b1 may confuse LM with a far-away history.", "labels": [], "entities": [{"text": "memory", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.989138662815094}]}, {"text": "In the following experiments, we set \u03b1 = 0.7 for the rest experiments in this paper.", "labels": [], "entities": []}, {"text": "In, we have summarized the perplexities on the PTB test set for various models.", "labels": [], "entities": [{"text": "PTB test set", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.9246228734652201}]}, {"text": "The proposed FOFE-FNNLMs can significantly outperform the baseline FNN-LMs using the same architecture.", "labels": [], "entities": [{"text": "FOFE-FNNLMs", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.6832847595214844}]}, {"text": "For example, the perplexity of the baseline bigram FNNLM is 176, while the FOFE-  FNNLM can improve to 116.", "labels": [], "entities": [{"text": "FNNLM", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.6523162722587585}, {"text": "FOFE-  FNNLM", "start_pos": 75, "end_pos": 87, "type": "METRIC", "confidence": 0.6366665065288544}]}, {"text": "Moreover, the FOFE-FNNLMs can even overtake a well-trained RNNLM (400 hidden units) in and an LSTM in.", "labels": [], "entities": [{"text": "FOFE-FNNLMs", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.6631112694740295}]}, {"text": "It indicates FOFE-FNNLMs can effectively model the longterm dependency in language without using any recurrent feedback.", "labels": [], "entities": [{"text": "FOFE-FNNLMs", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.8822634220123291}]}, {"text": "At last, the 2nd-order FOFE-FNNLM can provide further improvement, yielding the perplexity of 108 on PTB.", "labels": [], "entities": [{"text": "FOFE-FNNLM", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.8510256409645081}, {"text": "perplexity", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.974047839641571}, {"text": "PTB", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.987234890460968}]}, {"text": "It also outperforms all higher-order FNN-LMs (4-gram, 5-gram and 6-gram), which are bigger in model size.", "labels": [], "entities": [{"text": "FNN-LMs", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7398918271064758}]}, {"text": "To our knowledge, this is one of the best reported results on PTB without model combination.", "labels": [], "entities": [{"text": "PTB", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.7190411686897278}]}, {"text": "We have further examined the FOFE based FNNLMs on a much larger text corpus, i.e. LTCB, which contains articles from Wikipedia.", "labels": [], "entities": [{"text": "FOFE based FNNLMs", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.6631835401058197}]}, {"text": "We have trained several baseline systems: i) two n-gram LMs (3-gram and 5-gram) using the modified Kneser-Ney smoothing without count cutoffs; ii) several traditional FNN-LMs with different model sizes and input context windows (bigram, trigram, 4-gram and 5-gram); iii) an RNN-LM with one hidden layer of 600 nodes using the toolkit in, in which we have further used a spliced sentence bunch in () to speedup the training on GPUs.", "labels": [], "entities": []}, {"text": "Moreover, we have examined four FOFE based FNN-LMs with various model sizes and input window sizes (two 1st-order FOFE models and two 2nd-order ones).", "labels": [], "entities": []}, {"text": "For all NNLMs, we have used an output layer of the full vocabulary (80k words).", "labels": [], "entities": []}, {"text": "In these experiments, we have used an initial learning rate of 0.01, and a bigger mini-batch of 500 for FNNLMMs and of 256 sentences for the RNN and FOFE models.", "labels": [], "entities": [{"text": "FNNLMMs", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.8845815658569336}, {"text": "FOFE", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.6044471263885498}]}, {"text": "Experimental results in have shown that the FOFE-based FNN-LMs can significantly outperform the baseline FNN-LMs (including some larger higher-order models) and also slightly overtake the popular RNN-based LM, yielding the best result (perplexity of 107) on the test set.", "labels": [], "entities": [{"text": "FOFE-based FNN-LMs", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.5710135847330093}]}], "tableCaptions": [{"text": " Table 1: Perplexities on PTB for various LMs.  Model  Test PPL  KN 5-gram (Mikolov, 2011)  141  FNNLM (Mikolov, 2012)  140  RNNLM (Mikolov, 2011)  123  LSTM (Graves, 2013)  117  bigram FNNLM  176  trigram FNNLM  131  4-gram FNNLM  118  5-gram FNNLM  114  6-gram FNNLM  113  1st-order FOFE-FNNLM  116  2nd-order FOFE-FNNLM  108", "labels": [], "entities": [{"text": "FNNLM", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.7739455103874207}, {"text": "RNNLM", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.7845649123191833}, {"text": "FOFE-FNNLM  116  2nd-order FOFE-FNNLM  108", "start_pos": 285, "end_pos": 327, "type": "METRIC", "confidence": 0.7624191403388977}]}, {"text": " Table 2: Perplexities on LTCB for various lan- guage models. [M*N] denotes the sizes of the in- put context window and projection layer.  Model  Architecture  Test PPL  KN 3-gram  - 156  KN 5-gram  - 132  [1*200]-400-400-80k  241  [2*200]-400-400-80k  155  FNN-LM [2*200]-600-600-80k  150  [3*200]-400-400-80k  131  [4*200]-400-400-80k  125  RNN-LM  [1*600]-80k  112  [1*200]-400-400-80k  120  FOFE  [1*200]-600-600-80k  115  FNN-LM [2*200]-400-400-80k  112  [2*200]-600-600-80k  107", "labels": [], "entities": [{"text": "FOFE", "start_pos": 395, "end_pos": 399, "type": "METRIC", "confidence": 0.9931849837303162}]}]}