{"title": [{"text": "Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification", "labels": [], "entities": [{"text": "Named Entity Classification", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6448697447776794}]}], "abstractContent": [{"text": "Entity classification, like many other important problems in NLP, involves learning classifiers over sparse high-dimensional feature spaces that result from the conjunction of elementary features of the entity mention and its context.", "labels": [], "entities": [{"text": "Entity classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.889919102191925}]}, {"text": "In this paper we develop a low-rank reg-ularization framework for training max-entropy models in such sparse conjunctive feature spaces.", "labels": [], "entities": []}, {"text": "Our approach handles con-junctive feature spaces using matrices and induces an implicit low-dimensional representation via low-rank constraints.", "labels": [], "entities": []}, {"text": "We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in controlling model capacity than standard techniques for linear classifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many important problems in NLP involve learning classifiers over sparse high-dimensional feature spaces that result from the conjunction of elementary features.", "labels": [], "entities": []}, {"text": "For example, to classify an entity in a document, it is standard to exploit features of the left and right context in which the entity occurs as well as spelling features of the entity mention itself.", "labels": [], "entities": []}, {"text": "These sets of features can be grouped into vectors which we call elementary feature vectors.", "labels": [], "entities": []}, {"text": "In our example, there will be one elementary feature vector for the left context, one for the right context and one for the features of the mention.", "labels": [], "entities": []}, {"text": "Observe that, when the elementary vectors consist of binary indicator features, the outer product of any pair of vectors represents all conjunctions of the corresponding elementary features.", "labels": [], "entities": []}, {"text": "Ideally, we would like to train a classifier that can leverage all conjunctions of elementary features, since among them there might be some that are discriminative for the classification task at hand.", "labels": [], "entities": []}, {"text": "However, allowing for such expressive high dimensional feature space comes at a cost: data sparsity becomes a key challenge and controlling the capacity of the model is crucial to avoid overfitting the training data.", "labels": [], "entities": []}, {"text": "The problem of data sparsity is even more severe when the goal is to train classifiers with minimal supervision, i.e. small training sets.", "labels": [], "entities": []}, {"text": "For example, in the entity classification setting we might be interested in training a classifier using only a small set of examples of each entity class.", "labels": [], "entities": [{"text": "entity classification", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7702241241931915}]}, {"text": "This is atypical scenario in an industrial setting, where developers are interested in classifying entities according to their own classification schema and can only provide a handful of examples of each class.", "labels": [], "entities": []}, {"text": "A standard approach to control the capacity of a linear classifier is to use 1 or 2 regularization on the parameter vector.", "labels": [], "entities": []}, {"text": "However, this type of regularization does not seem to be effective when dealing with sparse conjunctive feature spaces.", "labels": [], "entities": []}, {"text": "The main limitation is that 1 and 2 regularization cannot let the model give weight to conjunctions that have not been observed at training.", "labels": [], "entities": []}, {"text": "Without such ability it is unlikely that the model will generalize to novel examples, where most of the conjunctions will be unseen in the training set.", "labels": [], "entities": []}, {"text": "Of course, one could impose a strong prior on the weight vector so that it assigns weight to unseen conjunctions, but how can we build such a prior?", "labels": [], "entities": []}, {"text": "What kind of reasonable constraints can we put on unseen conjunctions?", "labels": [], "entities": []}, {"text": "Another common approach to handle high dimensional conjunctive feature spaces is to manually design the feature function so that it includes only a subset of \"relevant\" conjunctions.", "labels": [], "entities": []}, {"text": "But designing such a feature function can be time consuming and one might need to design anew feature function for each classification task.", "labels": [], "entities": []}, {"text": "Ideally, we would have a learning algorithm that does not require such feature engineering and that it can automatically leverage rich conjunctive feature spaces.", "labels": [], "entities": []}, {"text": "In this paper we present a solution to this problem by developing a regularization framework specifically designed for sparse conjunctive feature spaces.", "labels": [], "entities": []}, {"text": "Our approach results in a more effective way of controlling model capacity and it does not require feature engineering.", "labels": [], "entities": []}, {"text": "Our strategy is based on: \u2022 Employing tensors to define the scoring function of a max-entropy model as a multilinear form that computes weighted inner products between elementary vectors.", "labels": [], "entities": []}, {"text": "\u2022 Forcing the model to induce low-dimensional embeddings of elementary vectors via lowrank regularization on the tensor parameters.", "labels": [], "entities": []}, {"text": "The proposed regularization framework is based on a simple conceptual trick.", "labels": [], "entities": [{"text": "regularization", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9701859354972839}]}, {"text": "The standard approach to handle conjunctive feature spaces in NLP is to regard the parameters of the linear model as long vectors computing an inner product with a high dimensional feature representation that lists explicitly all possible conjunctions.", "labels": [], "entities": []}, {"text": "Instead, the parameters of our the model will be tensors and the compatibility score between an input pattern and a class will be defined as the sum of multilinear functions over elementary vectors.", "labels": [], "entities": [{"text": "compatibility score", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.9516320526599884}]}, {"text": "We then show that the rank 1 of the tensor has a very natural interpretation.", "labels": [], "entities": []}, {"text": "It can be seen as the intrinsic dimensionality of a latent embedding of the elementary feature vectors.", "labels": [], "entities": []}, {"text": "Thus by imposing a low-rank penalty on the tensor parameters we are encouraging the model to induce a lowdimensional projection of the elementary feature vectors . Using the rank itself as a regularization constraint in the learning algorithm would result in a non-convex optimization.", "labels": [], "entities": []}, {"text": "Instead, we follow a standard approach which is to use the nuclear norm as a convex relaxation of the rank.", "labels": [], "entities": []}, {"text": "In summary the main contributions of this paper are: \u2022 We develop anew regularization framework for training max-entropy models in high-dimensional sparse conjunctive feature spaces.", "labels": [], "entities": []}, {"text": "Since the proposed regularization implicitly induces a low dimensional embedding of feature vectors, our algorithm can also be seen as away of implicitly learning a latent variable model.", "labels": [], "entities": []}, {"text": "\u2022 We present a simple convex learning algorithm for training the parameters of the model.", "labels": [], "entities": []}, {"text": "\u2022 We conduct experiments on learning entity classifiers with minimal supervision.", "labels": [], "entities": [{"text": "learning entity classifiers", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6163340707619985}]}, {"text": "Our results show that the proposed regularization framework is better for sparse conjunctive feature spaces than standard 2 and 1 regularization.", "labels": [], "entities": []}, {"text": "These results make us conclude that encouraging the max-entropy model to operate on a low-dimensional space is an effective way of controlling the capacity of the model an ensure good generalization.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate our regularization framework for training models in highdimensional sparse conjunctive feature spaces.", "labels": [], "entities": []}, {"text": "We run experiments on learning entity classifiers with minimal supervision.", "labels": [], "entities": [{"text": "learning entity classifiers", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.629194845755895}]}, {"text": "We focus on classification of unseen entities to highlight the ability of the regularizer to generalize over conjunctions that are not observed at training.", "labels": [], "entities": [{"text": "classification of unseen entities", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8362502902746201}]}, {"text": "We simulate minimal supervision using the CoNLL-2003 Shared Task data, and compare the performance to 1 and 2 regularizers.", "labels": [], "entities": [{"text": "CoNLL-2003 Shared Task data", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.9117652177810669}]}], "tableCaptions": [{"text": " Table 1: For each entity class, the seed of entities for the 10-30 set, together with the number of mentions  in the training data that involve entities in the seed for various sizes of the seeds.", "labels": [], "entities": []}, {"text": " Table 2: Average-F1 of classification of unseen entity candidates on development data, using the 10-30  training seed and 2 regularization, for different conjunctive spaces (elementary only, full conjunctions,  all). Bag-of-words elementary features contain all clusters/PoS in separate windows to the left and to  the right of the candidate. N-grams elementary features contain all n-grams of clusters/PoS in separate  left and right windows (e.g. for size 3 it includes unigrams, bigrams and trigrams on each side).", "labels": [], "entities": []}, {"text": " Table 4: Results on the test for models trained with different sizes of the seed, using the parameters  and features that obtain the best evaluation results the development set. NN refers to nuclear norm  regularization, L1 and L2 refer to 1 and 2 regularization. Only test entities unseen at training are  considered. Avg. F1 is over PER, LOC, ORG and MISC, excluding O.", "labels": [], "entities": [{"text": "Avg. F1", "start_pos": 320, "end_pos": 327, "type": "METRIC", "confidence": 0.8642750978469849}, {"text": "PER", "start_pos": 336, "end_pos": 339, "type": "METRIC", "confidence": 0.9793034791946411}, {"text": "ORG", "start_pos": 346, "end_pos": 349, "type": "METRIC", "confidence": 0.9672183990478516}, {"text": "MISC", "start_pos": 354, "end_pos": 358, "type": "METRIC", "confidence": 0.5964000821113586}, {"text": "O", "start_pos": 370, "end_pos": 371, "type": "METRIC", "confidence": 0.9380490779876709}]}]}