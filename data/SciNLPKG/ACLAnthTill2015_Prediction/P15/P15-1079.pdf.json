{"title": [], "abstractContent": [{"text": "We achieve significant improvements in several syntax-based machine translation experiments using a string-to-tree variant of multi bottom-up tree transducers.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7168713212013245}]}, {"text": "Our new parameterized rule extraction algorithm extracts string-to-tree rules that can be discontiguous and non-minimal in contrast to existing algorithms for the tree-to-tree setting.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7146891206502914}]}, {"text": "The obtained models significantly outperform the string-to-tree component of the Moses framework in a large-scale empirical evaluation on several known translation tasks.", "labels": [], "entities": []}, {"text": "Our linguistic analysis reveals the remarkable benefits of discontiguous and non-minimal rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present an application of a variant of local multi bottom-up tree transducers (MBOTs) as proposed in to statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.6932821075121561}]}, {"text": "MBOTs allow discontinuities on the target language side since they have a sequence of target tree fragments instead of a singletree fragment in their rules.", "labels": [], "entities": [{"text": "MBOTs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.834941565990448}]}, {"text": "The original approach makes use of syntactic information on both the source and the target side (tree-to-tree) and a corresponding minimal rule extraction is presented in. implemented it as well as a decoder inside the Moses framework ( and demonstrated that the resulting tree-to-tree MBOT system significantly improved over its tree-to-tree baseline using minimal rules.", "labels": [], "entities": []}, {"text": "We can see at least two drawbacks in this approach.", "labels": [], "entities": []}, {"text": "First, experiments investigating the integration of syntactic information on both sides generally report quality deterioration.", "labels": [], "entities": []}, {"text": "For example,,, and noted that translation quality tends to decrease in tree-to-tree systems because the rules become too restrictive.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9725812077522278}]}, {"text": "Second, minimal rules (i.e., rules that cannot be obtained from other extracted rules) typically consist of a few lexical items only and are thus not the most suitable to translate idiomatic expressions and other fixed phrases.", "labels": [], "entities": [{"text": "translate idiomatic expressions and other fixed phrases", "start_pos": 171, "end_pos": 226, "type": "TASK", "confidence": 0.7654205475534711}]}, {"text": "To overcome these drawbacks, we abolish the syntactic information for the source side and develop a string-to-tree variant of MBOTs.", "labels": [], "entities": [{"text": "MBOTs", "start_pos": 126, "end_pos": 131, "type": "DATASET", "confidence": 0.9218264818191528}]}, {"text": "In addition, we develop anew rule extraction algorithm that can also extract non-minimal rules.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7250956892967224}]}, {"text": "In general, the number of extractable rules explodes, so our rule extraction places parameterized restrictions on the extracted rules in the same spirit as in.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7417646646499634}]}, {"text": "In this manner, we combine the advantages of the hierarchical phrase-based approach on the source side and the tree-based approach with discontinuiety on the target side.", "labels": [], "entities": []}, {"text": "We evaluate our new system in 3 large-scale experiments using translation tasks, in which we expect discontinuiety on the target.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.894415557384491}]}, {"text": "MBOTs are powerful but asymmetric models since discontinuiety is available only on the target.", "labels": [], "entities": [{"text": "MBOTs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7394430637359619}]}, {"text": "We chose to translate from English to German, Arabic, and Chinese.", "labels": [], "entities": []}, {"text": "In all experiments our new system significantly outperforms the string-to-tree syntax-based component) of Moses.", "labels": [], "entities": []}, {"text": "The (potentially) discontiguous rules of our model are very useful in these setups, which we confirm in a quantitative and qualitative analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We considered three reasonable baselines: (i) minimal MBOT, (ii) non-contiguous STSSG (), or (iii) a string-to-tree Moses system.", "labels": [], "entities": [{"text": "MBOT", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.6427969336509705}]}, {"text": "We decided against the minimal MBOT as a baseline since tree-to-tree systems generally get lower BLEU scores than string-to-tree systems.", "labels": [], "entities": [{"text": "MBOT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7392090559005737}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9991545677185059}]}, {"text": "We nevertheless present its BLEU scores (see).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9987899661064148}]}, {"text": "Unfortunately, we could not compare to because their decoder and rule extraction algorithms are not publicly available.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7681322991847992}]}, {"text": "Furthermore, we have the impression that their system does not scale well: \u2022 Only around 240,000 training sentences were used.", "labels": [], "entities": []}, {"text": "Our training data contains between 1.8M and 5.7M sentence pairs.", "labels": [], "entities": []}, {"text": "\u2022 The development and test set were length-   ratio filtered to sentences up to 50 characters.", "labels": [], "entities": [{"text": "length-   ratio", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.9271468917528788}]}, {"text": "We do not modify those sets.", "labels": [], "entities": []}, {"text": "\u2022 Only rules with at most one gap were allowed which would be equivalent to restrict the number of target tree fragments to 2 in our system.", "labels": [], "entities": []}, {"text": "Hence we decided to use a string-to-tree Moses system as baseline (see Section 6.1).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of numbers of extracted rules with respect to the different extraction algorithms.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results. The starred results  are statistically significant improvements over the  baseline (at confidence p < 1%).", "labels": [], "entities": []}, {"text": " Table 4: Number of rules per type used when decoding test (Lex = lexical rules; Struct = structural rules;  [dis]cont. = [dis]contiguous).", "labels": [], "entities": []}]}