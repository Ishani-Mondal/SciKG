{"title": [{"text": "Multi-Task Learning for Multiple Language Translation", "labels": [], "entities": [{"text": "Multiple Language Translation", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6700035134951273}]}], "abstractContent": [{"text": "In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7429376542568207}]}, {"text": "Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.7328833738962809}, {"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7316924482584}]}, {"text": "We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.7944610118865967}]}, {"text": "Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available.", "labels": [], "entities": []}, {"text": "Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation from one source language to multiple target languages at the same time is a difficult task for humans.", "labels": [], "entities": [{"text": "Translation from one source language", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8286544203758239}]}, {"text": "A person often needs to be familiar with specific translation rules for different language pairs.", "labels": [], "entities": []}, {"text": "Machine translation systems suffer from the same problems too.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7689144611358643}]}, {"text": "Under the current classic statistical machine translation framework, it is hard to share information across different phrase tables among different language pairs.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6273748775323232}]}, {"text": "Translation quality decreases rapidly when the size of training corpus for some minority language pairs becomes smaller.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9537400007247925}]}, {"text": "To conquer the problems described above, we propose a multi-task learning framework based on a sequence learning model to conduct machine translation from one source language to multiple target languages, inspired by the recently proposed neural machine translation(NMT) framework proposed by . Specifically, we extend the recurrent neural network based encoder-decoder framework to a multi-task learning model that shares an encoder across all language pairs and utilize a different decoder for each target language.", "labels": [], "entities": [{"text": "machine translation from one source language", "start_pos": 130, "end_pos": 174, "type": "TASK", "confidence": 0.8190964659055074}, {"text": "neural machine translation(NMT)", "start_pos": 239, "end_pos": 270, "type": "TASK", "confidence": 0.8373989462852478}]}, {"text": "The neural machine translation approach has recently achieved promising results in improving translation quality.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6801240146160126}, {"text": "translation", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9601231217384338}]}, {"text": "Different from conventional statistical machine translation approaches, neural machine translation approaches aim at learning a radically end-to-end neural network model to optimize translation performance by generalizing machine translation as a sequence learning problem.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.6570107042789459}, {"text": "neural machine translation", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.6748727560043335}, {"text": "machine translation", "start_pos": 222, "end_pos": 241, "type": "TASK", "confidence": 0.6908934861421585}]}, {"text": "Based on the neural translation framework, the lexical sparsity problem and the long-range dependency problem in traditional statistical machine translation can be alleviated through neural networks such as long shortterm memory networks which provide great lexical generalization and long-term sequence memorization abilities.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7576209008693695}, {"text": "statistical machine translation", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.6379464467366537}]}, {"text": "The basic assumption of our proposed framework is that many languages differ lexically but are closely related on the semantic and/or the syntactic levels.", "labels": [], "entities": []}, {"text": "We explore such correlation across different target languages and realize it under a multi-task learning framework.", "labels": [], "entities": []}, {"text": "We treat a separate translation direction as a sub RNN encode-decoder task in this framework which shares the same encoder (i.e. the same source language representation) across different translation directions, and use a different decoder for each specific target language.", "labels": [], "entities": []}, {"text": "In this way, this proposed multi-task learning model can make full use of the source language corpora across different language pairs.", "labels": [], "entities": []}, {"text": "Since the encoder part shares the same source language representation across all the translation tasks, it may learn semantic and structured predictive representations that cannot be learned with only a small amount of data.", "labels": [], "entities": []}, {"text": "Moreover, during training we jointly model the alignment and the translation process simultaneously for different language pairs under the same framework.", "labels": [], "entities": []}, {"text": "For example, when we simultaneously translate from English into Korean and Japanese, we can jointly learn latent similar semantic and structure information across Korea and Japanese because these two languages share some common language structures.", "labels": [], "entities": []}, {"text": "The contribution of this work is three folds.", "labels": [], "entities": []}, {"text": "First, we propose a unified machine learning framework to explore the problem of translating one source language into multiple target languages.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this problem has not been studied carefully in the statistical machine translation field before.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.672672281662623}]}, {"text": "Second, given large-scale training corpora for different language pairs, we show that our framework can improve translation quality on each target language as compared with the neural translation model trained on a single language pair.", "labels": [], "entities": []}, {"text": "Finally, our framework is able to alleviate the data scarcity problem, using language pairs with large-scale parallel training corpora to improve the translation quality of those with few parallel training corpus.", "labels": [], "entities": []}, {"text": "The following sections will be organized as follows: in section 2, related work will be described, and in section 3, we will describe our multi-task learning method.", "labels": [], "entities": []}, {"text": "Experiments that demonstrate the effectiveness of our framework will be described in section 4.", "labels": [], "entities": []}, {"text": "Lastly, we will conclude our work in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted two groups of experiments to show the effectiveness of our framework.", "labels": [], "entities": []}, {"text": "The goal of the first experiment is to show that multi-task learning helps to improve translation performance given enough training corpora for all language pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9658105969429016}]}, {"text": "In the second experiment, we show that for some resource-poor language pairs with a few parallel training data, their translation performance could be improved as well.", "labels": [], "entities": []}, {"text": "The Europarl corpus is a multi-lingual corpus including 21 European languages.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9901415109634399}]}, {"text": "Here we only choose four language pairs for our experiments.", "labels": [], "entities": []}, {"text": "The source language is English for all language pairs.", "labels": [], "entities": []}, {"text": "And the target languages are Spanish (Es), French (Fr), Portuguese (Pt) and Dutch (Nl).", "labels": [], "entities": []}, {"text": "To demonstrate the validity of our learning framework, we do some preprocessing on the training set.", "labels": [], "entities": []}, {"text": "For the source language, we use 30k of the most frequent words for source language vocabulary which is shared across different language pairs and 30k most frequent words for each target language.", "labels": [], "entities": []}, {"text": "Outof-vocabulary words are denoted as unknown words, and we maintain different unknown word labels for different languages.", "labels": [], "entities": []}, {"text": "For test sets, we also restrict all words in the test set to be from our training vocabulary and mark the OOV words as the corresponding labels as in the training data.", "labels": [], "entities": [{"text": "OOV", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9115842580795288}]}, {"text": "The size of training corpus in experiment 1 and 2 is listed in.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our method with EuroParl Common testset and WMT 2013 dataset.", "labels": [], "entities": [{"text": "EuroParl Common testset", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.9564494689305624}, {"text": "WMT 2013 dataset", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9341313640276591}]}, {"text": "BLEU-4 () is used as the evaluation metric.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9744855165481567}]}, {"text": "We evaluate BLEU scores on EuroParl Common test set with multi-task NMT models and single NMT models to demonstrate the validity of our multi-task learning framework.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.99763023853302}, {"text": "EuroParl Common test set", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.9881481677293777}]}, {"text": "On the WMT 2013 data sets, we compare performance of separately trained NMT models, multi-task NMT models and Moses.", "labels": [], "entities": [{"text": "WMT 2013 data sets", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.9615867584943771}]}, {"text": "We use the EuroParl Common test set as a development set in both neural machine translation experiments and Moses experiments.", "labels": [], "entities": [{"text": "EuroParl Common test set", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9864662140607834}, {"text": "neural machine translation", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.676307479540507}]}, {"text": "For single NMT models and multi-task NMT models, we select the best model with the highest BLEU score in the EuroParl Common testset and apply it to the WMT 2013 dataset.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9832286834716797}, {"text": "EuroParl Common testset", "start_pos": 109, "end_pos": 132, "type": "DATASET", "confidence": 0.9749740958213806}, {"text": "WMT 2013 dataset", "start_pos": 153, "end_pos": 169, "type": "DATASET", "confidence": 0.9259636998176575}]}, {"text": "Note that our experiment settings in NMT is equivalent with Moses, considering the same training corpus, development sets and test sets.", "labels": [], "entities": []}, {"text": "We report our results of three experiments to show the validity of our methods.", "labels": [], "entities": []}, {"text": "In the first experiment, we train multi-task learning model jointly on all four parallel corpora and compare BLEU scores with models trained separately on each parallel corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9990383386611938}]}, {"text": "In the second experiment, we utilize the same training procedures as Experiment 1, except that we mimic the situation where some parallel corpora are resource-poor and maintain only 15% data on two parallel training corpora.", "labels": [], "entities": []}, {"text": "In experiment 3, we test our learned model from experiment 1 and experiment 2 on WMT 2013 dataset.", "labels": [], "entities": [{"text": "WMT 2013 dataset", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.9886034925778707}]}, {"text": "show the case-insensitive BLEU scores on the Europarl common test data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9866883158683777}, {"text": "Europarl common test data", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.9891393333673477}]}, {"text": "Models learned from the multitask learning framework significantly outperform the models trained separately.", "labels": [], "entities": []}, {"text": "shows that given only 15% of parallel training corpus of English-Dutch and English-Portuguese, it is possible to improve translation performance on all the target languages as well.", "labels": [], "entities": [{"text": "translation", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.9670438170433044}]}, {"text": "This result makes sense because the correlated languages benefit from each other by sharing the same predictive structure, e.g. French, Spanish and Portuguese, all of which are from Latin.", "labels": [], "entities": []}, {"text": "We also notice that even though Dutch is from Germanic languages, it is also possible to increase translation performance under our multi-task learning framework which demonstrates the generalization of our model to multiple target languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.9670853018760681}]}], "tableCaptions": [{"text": " Table 1: Size of training corpus for different language pairs", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9734514355659485}]}, {"text": " Table 2: Size of test set in EuroParl Common  testset and WMT2013", "labels": [], "entities": [{"text": "EuroParl Common  testset", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.9691536625226339}, {"text": "WMT2013", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.8415160179138184}]}, {"text": " Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs", "labels": [], "entities": [{"text": "Multi-task neural translation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.685948371887207}]}, {"text": " Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.", "labels": [], "entities": [{"text": "Multi-task neural translation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6714409093062083}]}, {"text": " Table 5: Multi-task NMT v.s. single model v.s. moses on the WMT 2013 test set", "labels": [], "entities": [{"text": "WMT 2013 test set", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.9541473537683487}]}, {"text": " Table 7. The examples are from the", "labels": [], "entities": []}]}