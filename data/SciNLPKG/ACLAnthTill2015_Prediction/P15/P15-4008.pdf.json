{"title": [{"text": "A Web-based Collaborative Evaluation Tool for Automatically Learned Relation Extraction Patterns", "labels": [], "entities": [{"text": "Automatically Learned Relation Extraction Patterns", "start_pos": 46, "end_pos": 96, "type": "TASK", "confidence": 0.7453196048736572}]}], "abstractContent": [{"text": "Patterns extracted from dependency parses of sentences area major source of knowledge for most state-of-the-art relation extraction systems, but can be of low quality in distantly supervised settings.", "labels": [], "entities": [{"text": "Patterns extracted from dependency parses of sentences", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.713730092559542}, {"text": "relation extraction", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.738200768828392}]}, {"text": "We present a linguistic annotation tool that allows human experts to analyze and categorize automatically learned patterns, and to identify common error classes.", "labels": [], "entities": []}, {"text": "The annotations can be used to create datasets that enable machine learning approaches to pattern quality estimation.", "labels": [], "entities": [{"text": "pattern quality estimation", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.6360019048055013}]}, {"text": "We also present an experimental pattern error analysis for three semantic relations, where we find that between 24% and 61% of the learned dependency patterns are defective due to preprocessing or parsing errors, or due to violations of the distant supervision assumption.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parse trees of sentences have been shown to be very useful structures for relation extraction (RE), since they often capture syntactic and semantic properties of a relation and its arguments more compactly than more surfaceoriented representations.", "labels": [], "entities": [{"text": "Dependency parse trees of sentences", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8301043510437012}, {"text": "relation extraction (RE)", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8494361519813538}]}, {"text": "Typically, shortest-path or similar algorithms are used to extract a pattern from a sentence's dependency parse that connects the relation's arguments.", "labels": [], "entities": [{"text": "extract a pattern from a sentence's dependency parse", "start_pos": 59, "end_pos": 111, "type": "TASK", "confidence": 0.6742273370424906}]}, {"text": "Such patterns can be directly applied to parsed texts to identify novel instances of a relation (), or they can be used as features in a supervised learning approach (.", "labels": [], "entities": []}, {"text": "They are also useful by themselves, as linguistic resources that capture the different ways in which a given human language expresses semantic relations . In recent years, distant supervision has become a very important approach to relation extraction (, due to the availability of largescale structured knowledge bases such as Freebase (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 232, "end_pos": 251, "type": "TASK", "confidence": 0.9008736312389374}]}, {"text": "While typically yielding a high recall of relation mentions, distant supervision makes several strong assumptions that may significantly affect the quality of extracted dependency patterns.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9981978535652161}]}, {"text": "First, it assumes that for each relation tuple r i (e i 1 , . .", "labels": [], "entities": []}, {"text": ", e i k ) in a knowledge base, every sentence containing mentions of e i 1 , . .", "labels": [], "entities": []}, {"text": ", e i k (or a subset thereof) expresses the relation r i (.", "labels": [], "entities": []}, {"text": "This assumption typically does not hold for most sentences, i.e., entity mentions may co-occur without the sentence expressing the target relation.", "labels": [], "entities": []}, {"text": "Dependency patterns extracted from such sentences should be discarded to improve the precision of an RE system.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9992203712463379}, {"text": "RE", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.8350996971130371}]}, {"text": "Furthermore, distant supervision assumes that the knowledge base is complete: entity mention cooccurrences with no known relations are ignored or treated as negative training examples, lowering the discriminative capabilities of a learned model (.", "labels": [], "entities": []}, {"text": "Automatically estimating the quality of extracted patterns, e.g., by using data-driven statistical metrics, or by learning weights in a supervised setting, leads to indirect measures of pattern quality, but tells us only very little about the (grammatical) correctness and the semantic appropriateness of the patterns themselves.", "labels": [], "entities": []}, {"text": "We are hence interested in a more direct, expert-driven analysis of dependency patterns and their properties, which will hopefully guide us towards better automatic quality metrics.", "labels": [], "entities": []}, {"text": "To this end, we have developed a linguistic annotation tool, PatternJudge, that allows human experts to evaluate relation-specific dependency patterns and their associated source sentences.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are: \u2022 We present a linguistic annotation tool for human expert-driven quality control of dependency patterns (Section 3) \u2022 We describe an annotation process for pattern evaluation and the guidelines we developed for it (Section 4) \u2022 We present and discuss common error classes observed in an initial study of three semantic relations (Section 5)", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the pattern extraction approach described in Section 2 to create a dataset for 25 relations from the domains awards, business and personal relationships.", "labels": [], "entities": [{"text": "pattern extraction", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7501971125602722}]}, {"text": "We use Freebase as our knowledge base, and retrieve 200K relation instances as seed knowledge.", "labels": [], "entities": []}, {"text": "We then create a text corpus by querying Bing with the seeds as input, and retrieving the top 100 results per query.", "labels": [], "entities": []}, {"text": "From these documents, we extract more than 3M sentences mentioning a seed relation instance.", "labels": [], "entities": []}, {"text": "The resulting pattern dataset contains 1.5M unique patterns.", "labels": [], "entities": []}, {"text": "Since a manual evaluation of all these patterns would be too resource-intensive, we select a subset based on the pattern filtering algorithm proposed by.", "labels": [], "entities": []}, {"text": "We then sample a small set of sentences (3 \u2212 5) for each pattern, and conduct an initial pass over the data with human annotators that judge whether these sentences express the target relation or not.", "labels": [], "entities": []}, {"text": "We discard all patterns whose sentences do not express the relation.", "labels": [], "entities": []}, {"text": "The final dataset for manual evaluation consists of more than 8K patterns with all their source sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of pattern categories", "labels": [], "entities": [{"text": "Distribution of pattern categories", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.8429494947195053}]}, {"text": " Table 3: Distribution of error classes", "labels": [], "entities": []}]}