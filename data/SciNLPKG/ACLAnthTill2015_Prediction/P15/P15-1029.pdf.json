{"title": [{"text": "Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution", "labels": [], "entities": [{"text": "Incremental Reference Resolution", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.8600747187932333}]}], "abstractContent": [{"text": "An elementary way of using language is to refer to objects.", "labels": [], "entities": []}, {"text": "Often, these objects are physically present in the shared environment and reference is done via mention of perceivable properties of the objects.", "labels": [], "entities": []}, {"text": "This is a type of language use that is modelled well neither by logical semantics nor by distributional semantics, the former focusing on inferential relations between expressed propositions, the latter on similarity relations between words or phrases.", "labels": [], "entities": []}, {"text": "We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and 'dialogue-plausible' in that it computes meanings word-byword.", "labels": [], "entities": []}, {"text": "We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9991569519042969}]}], "introductionContent": [{"text": "The most basic, fundamental site of language use is co-located dialogue and referring to objects, as in Example (1), is a common occurrence in such a co-located setting.", "labels": [], "entities": []}, {"text": "(1) The green book on the left next to the mug.", "labels": [], "entities": []}, {"text": "Logical semantics) has little to say about this process -its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects.", "labels": [], "entities": []}, {"text": "Vector space approaches to distributional semantics () similarly focuses on something else, namely semantic similarity relations between words or phrases (e.g. finding closeness for \"coloured tome on the right of the cup\").", "labels": [], "entities": []}, {"text": "Neither approach by itself says anything about processing; typically, the assumption in applications is that fully presented phrases are being processed.", "labels": [], "entities": []}, {"text": "Lacking in these approaches is a notion of grounding of symbols in features of the world.", "labels": [], "entities": []}, {"text": "In this paper, we present an account of word and phrase meaning that is (a) perceptually grounded in that it provides a link between words and (computer) vision features of real images, (b) trainable, as that link is learned from examples of language use, (c) compositional in that the meaning of phrases is a function of that of its parts and composition is driven by structural analysis, and (d) 'dialogue-plausible' in that it computes meanings incrementally, word-by-word and can work with noisy input from an automatic speech recogniser (ASR).", "labels": [], "entities": []}, {"text": "We show that the approach performs well (with an accuracy of 65% on a reference resolution task out of 32 objects) on direct descriptions as well as target/landmark descriptions, even when trained with little data (less than 800 training examples).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9990891218185425}, {"text": "reference resolution task", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7922877669334412}]}, {"text": "In the following section we will give a background on reference resolution, followed by a description of our model.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7873262465000153}]}, {"text": "We will then describe the data we used and explain our evaluations.", "labels": [], "entities": []}, {"text": "We finish by giving results, providing some additional analysis, and discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task of the participant was to refer to that object using only speech, as if identifying it fora friend sitting next to the participant.", "labels": [], "entities": []}, {"text": "The wizard (experimentor) had an identical screen depicting the scene but not the selected object.", "labels": [], "entities": []}, {"text": "The wizard listened to the participant's RE and clicked on the object she thought was being referred on her screen.", "labels": [], "entities": [{"text": "RE", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.7728638052940369}]}, {"text": "If it was the target object, atone sounded and anew object was randomly chosen.", "labels": [], "entities": []}, {"text": "This constituted a single episode.", "labels": [], "entities": []}, {"text": "If a wrong object was clicked, a different tone sounded, the episode was flagged, and anew episode began.", "labels": [], "entities": []}, {"text": "At varied intervals, the participant was instructed to \"shuffle\" the board between episodes by moving around the pieces.", "labels": [], "entities": []}, {"text": "The first half of the allotted time constituted phase-1.", "labels": [], "entities": []}, {"text": "After phase-1 was complete, instructions for phase-2 were explained: the screen showed the target and also a landmark object, outlined in blue, near the target (again, see).", "labels": [], "entities": []}, {"text": "The participant was to refer to the target using the landmark.", "labels": [], "entities": []}, {"text": "(In the instructions, the concepts of landmark and target were explained in general terms.)", "labels": [], "entities": []}, {"text": "All other instructions remained the same as phase-1.", "labels": [], "entities": []}, {"text": "The target's identifier, which was always known beforehand, was always recorded.", "labels": [], "entities": []}, {"text": "For phase-2, the landmark's identifier was also recorded.", "labels": [], "entities": []}, {"text": "Nine participants (6 female, 3 male; avg. age of 22) took part in the study; the language of the study was German.", "labels": [], "entities": []}, {"text": "Phase-1 for one participant and phase-2 for another participant were not used due to misunderstanding and a technical difficulty.", "labels": [], "entities": []}, {"text": "This produced a corpus of 870 non-flagged episodes in total.", "labels": [], "entities": []}, {"text": "Even though each episode had 36 objects in the scene, all objects were not always recognised by the computer vision processing.", "labels": [], "entities": []}, {"text": "On average, 32 objects were recognized.", "labels": [], "entities": []}, {"text": "To obtain transcriptions, we used Google Web Speech (with a word error rate of 0.65, as determined by comparing to a hand transcribed sample) This resulted in 1587 distinct words, with 15.53 words on average per episode.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.8324124614397684}]}, {"text": "The objects were not manipulated in anyway during an episode, so the episode was guaranteed to remain static during a RE and a single image is sufficient to represent the layout of one episode's scene.", "labels": [], "entities": [{"text": "RE", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.7265783548355103}]}, {"text": "Each scene was processed using computer vision techniques to obtain low-level features for each (detected) object in the scene which were used for the word classifiers.", "labels": [], "entities": []}, {"text": "We annotated each episode's RE with a simple tagging scheme that segmented the RE into words that directly referred to the target, words that directly referred to the landmark (or multiple landmarks, in some cases) and the relation words.", "labels": [], "entities": []}, {"text": "For certain word types, additional information about the word was included in the tag if it described colour, shape, or spatial placement (denoted contributing REs in the evaluations below).", "labels": [], "entities": [{"text": "REs", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.9364542365074158}]}, {"text": "The direction of certain relation words was normalised (e.g., left-of should always denote a landmark-target relation).", "labels": [], "entities": []}, {"text": "This represents a minimal amount of \"syntactic\" information needed for the application of the classifiers and the composition of the phrase meanings.", "labels": [], "entities": []}, {"text": "We leave applying a syntactic parser to future work.", "labels": [], "entities": []}, {"text": "An example RE in the original German (as recognised by the ASR), English gloss, and tags for each word is given in. a. grauer stein\u00fcberstein\u00a8stein\u00fcber dem gr\u00fcnen m unten links b. gray block above the green m bottom left c. tc ts r l lc ls tf tf To obtain visual features of each object, we used the same simple computer-vision pipeline of object segmentation and contour reconstruction as used by, providing us with RGB representations for the colour and features such as skewness, number of edges etc.", "labels": [], "entities": [{"text": "RE", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.8485386967658997}, {"text": "ASR)", "start_pos": 59, "end_pos": 63, "type": "TASK", "confidence": 0.7869086265563965}, {"text": "object segmentation", "start_pos": 339, "end_pos": 358, "type": "TASK", "confidence": 0.7434552013874054}, {"text": "contour reconstruction", "start_pos": 363, "end_pos": 385, "type": "TASK", "confidence": 0.7607493102550507}]}, {"text": "Procedure We breakdown our data as follows: episodes where the target was referred directly via a 'simple reference' construction (DD; 410 episodes) and episodes where a target was referred via a landmark relation (RD; 460 episodes).", "labels": [], "entities": []}, {"text": "We also test with either knowledge about structure (simple or relational reference) provided (ST) or not (WO, for \"words-only\").", "labels": [], "entities": []}, {"text": "All results shown are from 10-fold cross validations averaged over 10 runs; where for evaluations labelled RD the training data always includes all of  Words were stemmed using the NLTK) Snowball Stemmer, reducing the vocabulary size to 1306.", "labels": [], "entities": [{"text": "NLTK) Snowball Stemmer", "start_pos": 181, "end_pos": 203, "type": "DATASET", "confidence": 0.7803891450166702}]}, {"text": "Due to sparsity, for relation words with a token count of less than 4 (found by ranging over values in a held-out set) relational features were piped into an UNK relation, which was used for unseen relations during evaluation (we assume the UNK relation would learn a general notion of 'nearness').", "labels": [], "entities": []}, {"text": "For the individual word classifiers, we always paired one negative example with one positive example.", "labels": [], "entities": []}, {"text": "For this evaluation, word classifiers for sr were given the following features: RGB values, HSV values, x and y coordinates of the centroids, euclidean distance of centroid from the center, and number of edges.", "labels": [], "entities": []}, {"text": "The relation classifiers received information relating two objects, namely the euclidean distance between them, the vertical and horizontal distances, and two binary features that denoted if the landmark was higher than/lower than or left/right of the target.", "labels": [], "entities": []}, {"text": "Metrics for Evaluation To give a picture of the overall performance of the model, we report accuracy (how often was the argmax the gold target) and mean reciprocal rank (MRR) of the gold target in the distribution overall the objects (like accuracy, higher MRR values are better; values range between 0 and 1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9996840953826904}, {"text": "mean reciprocal rank (MRR)", "start_pos": 148, "end_pos": 174, "type": "METRIC", "confidence": 0.8429336547851562}, {"text": "accuracy", "start_pos": 240, "end_pos": 248, "type": "METRIC", "confidence": 0.9991987347602844}]}, {"text": "The use of MRR is motivated by the assumption that in general, a good rank for the correct object is desirable, even if it doesn't reach the first position, as when integrated in a dialogue system this information might still be useful to formulate clarification questions.", "labels": [], "entities": [{"text": "MRR", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.738310694694519}]}, {"text": "(Random baseline of 1/32 or 3% not shown in plot.)", "labels": [], "entities": [{"text": "Random baseline", "start_pos": 1, "end_pos": 16, "type": "METRIC", "confidence": 0.9413557648658752}]}, {"text": "DD.WO shows how well the sr model performs using the whole utterances and not just the REs.", "labels": [], "entities": [{"text": "DD.WO", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.930475115776062}]}, {"text": "(Note that all evaluations are on noisy ASR transcriptions.)", "labels": [], "entities": [{"text": "ASR transcriptions", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9326983690261841}]}, {"text": "DD.ST adds structure by only considering words that are part of the actual RE, improving the results further.", "labels": [], "entities": [{"text": "DD.ST", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8841491937637329}]}, {"text": "The remaining sets evaluate the contributions of the rr model.", "labels": [], "entities": []}, {"text": "RD.ST (sr) does this indirectly, by including the target and landmark simple references, but not the model for the relations; the task here is to resolve target and landmark SRs as they are.", "labels": [], "entities": []}, {"text": "This provides the baseline for the next two evaluations, which include the relation model.", "labels": [], "entities": []}, {"text": "In RD.ST (sr+r), the model learns SRs from DD data and only relations from RD.", "labels": [], "entities": []}, {"text": "The performance is substantially better than the baseline without the relation model.", "labels": [], "entities": []}, {"text": "Performance is best finally for RD.ST (rr), where the landmark and target SRs in the training portion of RD also contribute to the word models.", "labels": [], "entities": [{"text": "RD.ST", "start_pos": 32, "end_pos": 37, "type": "TASK", "confidence": 0.896938145160675}]}, {"text": "The mean reciprocal rank scores follow a similar pattern and show that even though the target object was not the argmax of the distribution, on average it was high in the distribution.", "labels": [], "entities": []}, {"text": "For all evaluations, the average standard deviation across the 10 runs was very small (0.01), meaning the model was fairly stable, despite the possibility of one run having randomly chosen more discriminating negative examples.", "labels": [], "entities": []}, {"text": "Our conclusion from these experiments is that despite the small amount of training data and noise from ASR as well as the scene, the model is robust and yields respectable results.", "labels": [], "entities": [{"text": "ASR", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9197912216186523}]}, {"text": "Incremental Results shows how our rr model processes incrementally, by giving the average rank of the (gold) target at each increment for the REs with the most common length in our data (13 words, of which there were 64 examples).", "labels": [], "entities": []}, {"text": "A system that works incrementally would have a monotonically decreasing average rank as the utterance unfolds.", "labels": [], "entities": []}, {"text": "The overall trend as shown in that : Each plot represents how well selected words fit assumptions about their lexical semantics: the leftmost plot ecke (corner) yields higher probabilities as objects are closer to the corner; the middle plot gr\u00fcn (green) yields higher probabilities when the colour spectrum values are nearer to green; the rightmost plot\u00fcberplot\u00a8plot\u00fcber (above) yields higher probabilities when targets are nearer to a landmark set in the middle.", "labels": [], "entities": []}, {"text": "There is a slight increase between 6-7, though very small (a difference of 0.09).", "labels": [], "entities": []}, {"text": "Overall, these results seem to show that our model indeed works intersectively and \"zooms in\" on the intended referent.", "labels": [], "entities": []}], "tableCaptions": []}