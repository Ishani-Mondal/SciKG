{"title": [{"text": "Labeled Grammar Induction with Minimal Supervision", "labels": [], "entities": []}], "abstractContent": [{"text": "Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speech-tagged text.", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7108189264933268}]}, {"text": "These clean linguistic classes provide a very important, though unreal-istic, inductive bias.", "labels": [], "entities": []}, {"text": "Conversely, induced clusters are very noisy.", "labels": [], "entities": []}, {"text": "We show here, for the first time, that very limited human supervision (three frequent words per cluster) maybe required to induce labeled dependencies from automatically induced word clusters.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite significant progress on inducing part-of-speech (POS) tags from raw text () and a small number of notable exceptions), most approaches to grammar induction or unsupervised parsing ( are based on the assumption that gold POS tags are available to the induction system.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.736592561006546}]}, {"text": "Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (.", "labels": [], "entities": []}, {"text": "The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies.", "labels": [], "entities": []}, {"text": "train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar) lexicon for POS tags, while show that CCG lexicons can be induced automatically if POS tags are used to identify nouns and verbs.", "labels": [], "entities": []}, {"text": "However, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that the simple \"universal\" knowledge of can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature.", "labels": [], "entities": []}, {"text": "Specifically, we will provide a labeled evaluation of induced CCG parsers against the English and Chinese CCGbanks.", "labels": [], "entities": [{"text": "English and Chinese CCGbanks", "start_pos": 86, "end_pos": 114, "type": "DATASET", "confidence": 0.6734370067715645}]}, {"text": "To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7139596790075302}, {"text": "PASCAL Challenge on Grammar Induction", "start_pos": 215, "end_pos": 252, "type": "TASK", "confidence": 0.5903702676296234}]}, {"text": "The system of Christodoulopoulos et al.", "labels": [], "entities": []}, {"text": "(2012) was the only participant competing in the PAS-CAL Challenge that operated over raw text (instead of gold POS tags).", "labels": [], "entities": []}, {"text": "However, their approach did not outperform the six baseline systems provided.", "labels": [], "entities": []}, {"text": "These baselines were two versions of the DMV model () run on varying numbers of induced Brown clusters (described in section 2.1).", "labels": [], "entities": []}, {"text": "We will therefore compare against these baselines in our evaluation.", "labels": [], "entities": []}, {"text": "Outside of the shared task, Spitkovsky et al.", "labels": [], "entities": []}, {"text": "(2011) demonstrated impressive performance using Brown clusters but did not provide evaluation for languages other than English.", "labels": [], "entities": []}, {"text": "The system we propose here will use a coarsegrained labeling comprised of three classes, which makes it substantially simpler than traditional tagsets, and uses far fewer labeled tokens than is customary for weakly-supervised approaches).", "labels": [], "entities": []}], "datasetContent": [{"text": "We will focus first on producing CCG labeled predicate-argument dependencies for English and Chinese and will then apply our best settings to produce a comparison with the tree structures of the languages of the PASCAL Shared Task.", "labels": [], "entities": [{"text": "PASCAL Shared Task", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.4901059369246165}]}, {"text": "All languages will be trained on sentences of up to length 20 (not counting punctuation).", "labels": [], "entities": []}, {"text": "All cluster induction algorithms are treated as black boxes and run over the complete datasets in advance.", "labels": [], "entities": [{"text": "cluster induction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7272475957870483}]}, {"text": "This alleviates having to handle tagging of unknown words.", "labels": [], "entities": [{"text": "tagging of unknown words", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.85600645840168}]}, {"text": "To provide an intuition for the performance of the induced word clusters, we provide two standard metrics for unsupervised tagging: Many-to-one (M-1) A commonly used measure, M-1 relies on mapping each cluster to the most common POS tag of its words.", "labels": [], "entities": []}, {"text": "However, M-1 can be easily inflated by inducing more clusters.", "labels": [], "entities": []}, {"text": "V-Measure Proposed by, V-Measure (VM) measures the information-theoretic distance between two clusterings and has been shown to be robust to the number of induced clusters).", "labels": [], "entities": [{"text": "V-Measure (VM)", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.8110872507095337}]}, {"text": "Both of these metrics are known to be highly dependent on the gold annotation standards they are compared against, and may not correlate with downstream performance at parsing.", "labels": [], "entities": []}, {"text": "Of more immediate relevance to our task is the ability to accurately identify nouns and verbs: Noun, Verb, and Other Recall We measure the (token-based) recall of our three-way labeling scheme of clusters as noun/verb/other against the universal POS tags of each token.", "labels": [], "entities": [{"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9873248338699341}]}, {"text": "Experimental Setup For our primary experiments, we train and test our systems on the English and Chinese CCGbanks, and report directed labeled F1 (LF1) and undirected unlabeled F1 (UF1) over CCG dependencies).", "labels": [], "entities": [{"text": "English and Chinese CCGbanks", "start_pos": 85, "end_pos": 113, "type": "DATASET", "confidence": 0.6405021473765373}, {"text": "F1 (LF1)", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9193297773599625}, {"text": "F1", "start_pos": 177, "end_pos": 179, "type": "METRIC", "confidence": 0.8428166508674622}]}, {"text": "For the labeled evaluation, we follow the simplification of CCGbank categories proposed by: for English to remove morphosyntactic features, map NP to N and change VP modifiers (S\\NP)|(S\\NP) to sentential modifiers (S|S); for Chinese we map both M and QP to N.", "labels": [], "entities": []}, {"text": "In the CCG literature, UF1 is commonly used because undirected dependencies do not penalize argument vs. adjunct distinctions, e.g. for prepositional phrases.", "labels": [], "entities": []}, {"text": "For this reason we will include UF1 in the final test set evaluation.", "labels": [], "entities": [{"text": "UF1", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.7032597661018372}]}, {"text": "We use the published train/dev/test splits, using the dev set for choosing a cluster induction algorithm, and then present final performance on the test data.", "labels": [], "entities": []}, {"text": "We induce 36 tags for English and 37 for Chinese to match the number of tags present in the treebanks (excluding symbol and punctuation tags).", "labels": [], "entities": []}, {"text": "Results presents the parsing and tagging development results on the two CCG corpora.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9743745923042297}, {"text": "tagging", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.8636859059333801}]}, {"text": "In terms of tagging performance, we can see that the two hard clustering systems significantly outperform the HMM, but the relative performance of Brown and BMMM is mixed.", "labels": [], "entities": [{"text": "tagging", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9607900381088257}, {"text": "BMMM", "start_pos": 157, "end_pos": 161, "type": "DATASET", "confidence": 0.779891848564148}]}, {"text": "More importantly, we see that, at least for English, despite clear differences in tagging performance, the parsing results (LF1) are much more similar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 107, "end_pos": 114, "type": "TASK", "confidence": 0.9589598774909973}]}, {"text": "In Chinese, we see that the performance of the two hard clustering systems is almost identical, again, not representative of the differences in the tagging scores.", "labels": [], "entities": []}, {"text": "The N/V/O recall scores in both languages are equally poor predictors of parsing performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9321638941764832}]}, {"text": "However, these scores show that having only three labeled tokens per class is sufficient to capture most of the necessary distinctions for the HDP-CCG.", "labels": [], "entities": []}, {"text": "All of this confirms the observations of that POS tagging metrics are not correlated with parsing performance.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.7866958677768707}, {"text": "parsing", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.9583851099014282}]}, {"text": "However, since BMMM seems to have a slight overall advantage, we will be using it as our clustering system for the remaining experiments.", "labels": [], "entities": [{"text": "BMMM", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.6270289421081543}]}, {"text": "Since the goal of this work was to produce labeled syntactic structures, we also wanted to evaluate our performance against that of the HDP-CCG system that uses gold-standard POS tags.", "labels": [], "entities": []}, {"text": "As we can see in the last two columns of our development results in, our system is within 2/3 of the labeled performance of the gold-POS-based HDP-CCG . shows an example labeled syntactic structure induced by the model.", "labels": [], "entities": []}, {"text": "We can seethe system successfully learns to attach the final hertz equipment is a major supplier of rental equipment in the u.s.", "labels": [], "entities": []}, {"text": ", france , spain and the u.k . hertz equipment is a major supplier of rental equipment in the u.s.", "labels": [], "entities": [{"text": "france", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.5303680300712585}]}, {"text": ", france , spain and the u.k . hertz equipment is a major supplier of rental equipment in the u.s.", "labels": [], "entities": [{"text": "france", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.5303677916526794}]}, {"text": ", france , spain and the u.k . hertz equipment is a major supplier of rental equipment in the u.s.", "labels": [], "entities": [{"text": "france", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.5303677916526794}]}, {"text": ", france , spain and the u.k . hertz equipment is a major supplier of rental equipment in the u.s.", "labels": [], "entities": [{"text": "france", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.5303677916526794}]}, {"text": ", france , spain and the u.k . hertz equipment is a major supplier of rental equipment in the u.s.", "labels": [], "entities": [{"text": "france", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.5303677916526794}]}, {"text": ", france , spain and the u.k . hertz equipment is a major supplier of rental equipment in the u.s.", "labels": [], "entities": [{"text": "france", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.5303677916526794}]}, {"text": ", france , spain and the u.k . Hertz equipment is a major supplier of rental equipment Figure 1: A sample derivation from the WSJ Section 22 demonstrating the system is learning most of the correct categories of CCGbank but has incorrectly analyzed the determiner as a preposition.", "labels": [], "entities": [{"text": "WSJ Section 22", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.9595787127812704}]}, {"text": "prepositional phrase, but mistakes the verb for intransitive and treats the determiner a as a preposition.", "labels": [], "entities": []}, {"text": "The labeled and undirected recall for this parse are 5/8 and 7/8 respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9666774272918701}]}, {"text": "Experimental Setup During the PASCAL shared task, participants were encouraged to train over the complete union of the data splits.", "labels": [], "entities": [{"text": "PASCAL shared task", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.5868566830952963}]}, {"text": "We do the same here, use the dev set for choosing a HDP-CCG hyperparameter, and then present final results for comparison on the test section.", "labels": [], "entities": []}, {"text": "We vary the hyperparamter for this evaluation because the datasets fluctuate dramatically in size from 9K to 700K tokens on sentences up to length 20.", "labels": [], "entities": []}, {"text": "Rather than match all of the tagsets, we simply induce 49 (excluding punctuation) classes for every language.", "labels": [], "entities": []}, {"text": "The actual tagsets vary from 20 to 304 tags (median 39, mean 78).", "labels": [], "entities": []}, {"text": "Results We now present results for the 10 corpora of the PASCAL shared task (evaluated on all sentence lengths).", "labels": [], "entities": [{"text": "PASCAL shared task", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6105155746142069}]}, {"text": "presents the test performance for each language with the best hyperparameter chosen from the set {100, 1000, 2500}.", "labels": [], "entities": []}, {"text": "Also included are the best published results from the joint tag/dependency induction shared task (ST) as well as the results from Bisk and Hockenmaier (2013), the only existing numbers for multilingual CCG induction (BH) with gold partof-speech tags.", "labels": [], "entities": [{"text": "joint tag/dependency induction shared task (ST)", "start_pos": 54, "end_pos": 101, "type": "TASK", "confidence": 0.6002766400575638}, {"text": "multilingual CCG induction (BH)", "start_pos": 189, "end_pos": 220, "type": "TASK", "confidence": 0.7829638024171194}]}, {"text": "Note that the systems in ST do not have access to any gold-standard POS tags, whereas our system has access to the gold tags for  the three most frequent words of each cluster.", "labels": [], "entities": [{"text": "ST", "start_pos": 25, "end_pos": 27, "type": "DATASET", "confidence": 0.881238579750061}]}, {"text": "The languages are sorted by the number of nonpunctuation tokens in sentences of up to length 20.", "labels": [], "entities": []}, {"text": "Despite our average performance (34.2) being slightly higher than the shared task (31.8), the st. deviation is substantial (\u03c3 = 15.2 vs \u03c3 ST = 7.5).", "labels": [], "entities": []}, {"text": "It seems apparent from the results that while data sparsity may play a role in affecting performance, the more linguistically interesting thread appears to be morphology.", "labels": [], "entities": []}, {"text": "Czech is perhaps a prime example, as it has twice the data of the next largest language (700K tokens vs 336K in English), but our approach still performs poorly.", "labels": [], "entities": []}, {"text": "Finally, while we saw that the hard clustering systems outperformed the HMM for our experiments, this is perhaps best explained by analyzing the average number of gold fine-grained tags per lexical type in each of the corpora.", "labels": [], "entities": []}, {"text": "We found, counterintuitively, that the \"difficult\" languages had lower average number of tags per type (1.01 for Czech, 1.03 for Arabic) than English (1.17) which was the most ambiguous.", "labels": [], "entities": []}, {"text": "This is likely due to morphology distinguishing otherwise ambiguous lemmas.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.", "labels": [], "entities": [{"text": "labeled CCG-Dependency  performance (LF1)", "start_pos": 67, "end_pos": 108, "type": "METRIC", "confidence": 0.8292770435412725}]}, {"text": " Table 2: CCG parsing performance (LF1/UF1) on  the test set with and without gold tags.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6067183464765549}]}, {"text": " Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).", "labels": [], "entities": [{"text": "Tagging VM", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8410733342170715}, {"text": "Accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8718259334564209}, {"text": "PAS- CAL joint tag/dependency induction shared task", "start_pos": 234, "end_pos": 285, "type": "TASK", "confidence": 0.6226816475391388}]}]}