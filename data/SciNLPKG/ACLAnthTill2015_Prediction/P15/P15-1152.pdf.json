{"title": [{"text": "Neural Responding Machine for Short-Text Conversation", "labels": [], "entities": [{"text": "Neural Responding Machine", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8444859186808268}, {"text": "Short-Text Conversation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.6993249654769897}]}], "abstractContent": [{"text": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation.", "labels": [], "entities": [{"text": "Short-Text Conversation", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.7261929512023926}]}, {"text": "NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN).", "labels": [], "entities": [{"text": "NRM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8544231057167053}]}, {"text": "The NRM is trained with a large amount of one-round conversation data collected from a microblogging service.", "labels": [], "entities": [{"text": "NRM", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8439225554466248}]}, {"text": "Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 219, "end_pos": 228, "type": "TASK", "confidence": 0.9848440289497375}]}], "introductionContent": [{"text": "Natural language conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of commonsense knowledge.", "labels": [], "entities": [{"text": "Natural language conversation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6926422119140625}, {"text": "language understanding", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7282074093818665}]}, {"text": "Previous works in this direction mainly focus on either rule-based or learning-based methods (;).", "labels": [], "entities": []}, {"text": "These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system.", "labels": [], "entities": []}, {"text": "Recently due to the explosive growth of microblogging services such as Twitter and Weibo 2 , the amount of conversation data available on the web has tremendously increased.", "labels": [], "entities": []}, {"text": "This makes a data-driven approach to attack the conversation problem () possible.", "labels": [], "entities": []}, {"text": "Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer.", "labels": [], "entities": [{"text": "Short-Text Conversation (STC)", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.7735337734222412}]}, {"text": "The research on STC may shed light on understanding the complicated mechanism of natural language conversation.", "labels": [], "entities": [{"text": "STC", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9684332013130188}, {"text": "natural language conversation", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.6431272725264231}]}, {"text": "Previous methods for STC fall into two categories, 1) the retrieval-based method (, and 2) the statistical machine translation (SMT) based method ().", "labels": [], "entities": [{"text": "STC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9827744364738464}, {"text": "statistical machine translation (SMT)", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.7947855691115061}]}, {"text": "The basic idea of retrievalbased method is to pick a suitable response by ranking the candidate responses with a linear or non-linear combination of various matching features (e.g. number of shared words).", "labels": [], "entities": []}, {"text": "The main drawbacks of the retrieval-based method are the following \u2022 the responses are pre-existing and hard to customize for the particular text or requirement from the task, e.g., style or attitude.", "labels": [], "entities": []}, {"text": "\u2022 the use of matching features alone is usually not sufficient for distinguishing positive responses from negative ones, even after time consuming feature engineering.", "labels": [], "entities": []}, {"text": "(e.g., a penalty due to mismatched named entities is difficult to incorporate into the model) The SMT-based method, on the other hand, is generative.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 98, "end_pos": 107, "type": "TASK", "confidence": 0.9913927316665649}]}, {"text": "Basically it treats the response generation as a translation problem, in which the model is trained on a parallel corpus of post-response pairs.", "labels": [], "entities": [{"text": "response generation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7646344304084778}]}, {"text": "Despite its generative nature, the method is intrinsically unsuitable for response generation, because the responses are not semantically equivalent to the posts as in translation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7483658194541931}]}, {"text": "Actually one post can receive responses with completely different content, as manifested through the example in the fol-lowing figure: Post Having my fish sandwich right now UserA For god's sake, it is 11 in the morning UserB Enhhhh...", "labels": [], "entities": [{"text": "UserB", "start_pos": 220, "end_pos": 225, "type": "DATASET", "confidence": 0.8713313937187195}, {"text": "Enhhhh", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.5111716389656067}]}, {"text": "sounds yummy UserC which restaurant exactly?", "labels": [], "entities": []}, {"text": "Empirical studies also showed that SMT-based methods often yield responses with grammatical errors and in rigid forms, due to the unnecessary alignment between the \"source\" post and the \"target\" response ().", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.9932482242584229}]}, {"text": "This rigidity is still a serious problem in the recent work of (, despite its use of neural network-based generative model as features in decoding.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our models are trained on a corpus of roughly 4.4 million pairs of conversations from Weibo 3 .  To construct this million scale dataset, we first crawl hundreds of millions of post-response pairs, and then clean the raw data in a similar way as suggested in (, including 1) removing trivial responses like \"wow\", 2) filtering out potential advertisements, and 3) removing the responses after first 30 ones for topic consistency.", "labels": [], "entities": []}, {"text": "It can be seen that each post have 20 different responses on average.", "labels": [], "entities": []}, {"text": "In addition to the semantic gap between post and its responses, this is another key difference to a general parallel data set used for traditional translation.", "labels": [], "entities": []}, {"text": "We evaluate three different settings of NRM described in Section 3, namely NRM-glo, NRMloc, and NRM-hyb, and compare them to retrievalbased and SMT-based methods.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 144, "end_pos": 153, "type": "TASK", "confidence": 0.9782362580299377}]}, {"text": "We adopt human annotation to compare the performance of different models.", "labels": [], "entities": []}, {"text": "Five labelers with at least three-year experience of Sina Weibo are invited to do human evaluation.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.8829507231712341}]}, {"text": "Responses obtained from the five evaluated models are pooled and randomly permuted for each labeler.", "labels": [], "entities": []}, {"text": "The labelers are instructed to imagine that they were the authors of the original posts and judge whether a response (generated or retrieved) is appropriate and natural to a input post.", "labels": [], "entities": []}, {"text": "Three levels are assigned to a response with scores from 0 to 2: \u2022 Suitable (+2): the response is evidently an appropriate and natural response to the post; \u2022 Neutral (+1): the response can be a suitable response in a specific scenario; \u2022 Unsuitable (0): it is hard or impossible to find a scenario where response is suitable.", "labels": [], "entities": []}, {"text": "To make the annotation task operable, the suitability of generated responses is judged from the following five criteria: (a) Grammar and Fluency: Responses should be natural language and free of any fluency or grammatical errors;", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The results of evaluated methods. Mean score is the average value of annotated scores over all  annotations. (Rtr.-based means the retrieval-based method)", "labels": [], "entities": [{"text": "Mean score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9869585633277893}, {"text": "Rtr.-based", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9104985594749451}]}]}