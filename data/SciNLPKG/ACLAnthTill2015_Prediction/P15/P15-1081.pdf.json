{"title": [{"text": "Unifying Bayesian Inference and Vector Space Models for Improved Decipherment", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce into Bayesian decipherment abase distribution derived from similarities of word embeddings.", "labels": [], "entities": []}, {"text": "We use Dirich-let multinomial regression (Mimno and McCallum, 2012) to learn a mapping between ciphertext and plaintext word em-beddings from non-parallel data.", "labels": [], "entities": []}, {"text": "Experimental results show that the base distribution is highly beneficial to decipher-ment, improving state-of-the-art decipher-ment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9974333643913269}]}], "introductionContent": [{"text": "Tremendous advances in Machine Translation (MT) have been made since we began applying automatic learning techniques to learn translation rules automatically from parallel data.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8714898705482483}, {"text": "translation rules automatically from parallel data", "start_pos": 126, "end_pos": 176, "type": "TASK", "confidence": 0.7896646956602732}]}, {"text": "However, reliance on parallel data also limits the development and application of high-quality MT systems, as the amount of parallel data is far from adequate in low-density languages and domains.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9898534417152405}]}, {"text": "In general, it is easier to obtain non-parallel monolingual data.", "labels": [], "entities": []}, {"text": "The ability to learn translations from monolingual data can alleviate obstacles caused by insufficient parallel data.", "labels": [], "entities": []}, {"text": "Motivated by this idea, researchers have proposed different approaches to tackle this problem.", "labels": [], "entities": []}, {"text": "They can be largely divided into two groups.", "labels": [], "entities": []}, {"text": "The first group is based on the idea proposed by, in which words are represented as context vectors, and two words are likely to be translations if their context vectors are similar.", "labels": [], "entities": []}, {"text": "Initially, the vectors contained only context * Equal contribution words.", "labels": [], "entities": []}, {"text": "Later extensions introduced more features (, and used more abstract representation such as word embeddings (.", "labels": [], "entities": []}, {"text": "Another promising approach to solve this problem is decipherment.", "labels": [], "entities": []}, {"text": "It has drawn significant amounts of interest in the past few years ( and has been shown to improve end-to-end translation.", "labels": [], "entities": []}, {"text": "Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English.", "labels": [], "entities": []}, {"text": "Both approaches have been shown to improve quality of MT systems for domain adaptation) and low density languages).", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.989579975605011}, {"text": "domain adaptation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7385935485363007}]}, {"text": "Meanwhile, they have their own advantages and disadvantages.", "labels": [], "entities": []}, {"text": "While context vectors can take larger context into account, it requires high quality seed lexicons to learn a mapping between two vector spaces.", "labels": [], "entities": []}, {"text": "In contrast, decipherment does not depend on any seed lexicon, but only looks at a limited n-gram context.", "labels": [], "entities": []}, {"text": "In this work, we take advantage of both approaches and combine them in a joint inference process.", "labels": [], "entities": []}, {"text": "More specifically, we extend previous work in large scale Bayesian decipherment by introducing a better base distribution derived from similarities of word embedding vectors.", "labels": [], "entities": []}, {"text": "The main contributions of this work are: \u2022 We propose anew framework that combines the two main approaches to finding translations from monolingual data only.", "labels": [], "entities": []}, {"text": "\u2022 We develop anew base-distribution technique that improves state-of-the art decipherment accuracy by a factor of two for Spanish/English and Malagasy/English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9959591031074524}]}, {"text": "\u2022 We make our software available for future research, functioning as a kind of GIZA for non-parallel data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Spanish/English, Malagasy/English decipherment top-5 accuracy (%) of 5k and 10k most fre- quent word types", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9922278523445129}]}]}