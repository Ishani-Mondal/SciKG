{"title": [{"text": "CCG Supertagging with a Recurrent Neural Network", "labels": [], "entities": [{"text": "CCG Supertagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5194996297359467}]}], "abstractContent": [{"text": "Recent work on supertagging using a feed-forward neural network achieved significant improvements for CCG supertagging and parsing (Lewis and Steedman, 2014).", "labels": [], "entities": [{"text": "parsing", "start_pos": 123, "end_pos": 130, "type": "TASK", "confidence": 0.9498187303543091}]}, {"text": "However, their architecture is limited to considering local contexts and does not naturally model sequences of arbitrary length.", "labels": [], "entities": []}, {"text": "In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both su-pertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9990453124046326}, {"text": "parsing", "start_pos": 179, "end_pos": 186, "type": "TASK", "confidence": 0.9725207686424255}, {"text": "F1", "start_pos": 197, "end_pos": 199, "type": "METRIC", "confidence": 0.9893587827682495}, {"text": "CCGBank", "start_pos": 205, "end_pos": 212, "type": "DATASET", "confidence": 0.9612516760826111}]}], "introductionContent": [{"text": "Combinatory Categorial Grammar (CCG;) is a highly lexicalized formalism; the standard parsing model of  uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar (CCG", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.732575660943985}]}, {"text": "This makes accurate disambiguation of lexical types much more challenging.", "labels": [], "entities": [{"text": "disambiguation of lexical types", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.7658273726701736}]}, {"text": "However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging ().", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.6819873452186584}]}, {"text": "show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9767284393310547}]}, {"text": "In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (, since the derivation space of the parser is determined by the supertagger, at both train-*All work was completed before the author joined ing and test time.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9778023958206177}, {"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9719780683517456}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9611620306968689}]}, {"text": "enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found.", "labels": [], "entities": []}, {"text": "This strategy is used in the de facto C&C parser ( , and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (.", "labels": [], "entities": [{"text": "C&C parser", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.49957045167684555}]}, {"text": "Despite the effectiveness of supertagging, the most widely used model for this task ) has a number of drawbacks.", "labels": [], "entities": []}, {"text": "First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.99866783618927}]}, {"text": "Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words (.", "labels": [], "entities": []}, {"text": "And third, in order to reduce computational requirements and feature sparsity, each tagging decision is made without considering any potentially useful contextual information beyond a local context window.", "labels": [], "entities": []}, {"text": "introduced a feedforward neural network to supertagging, and addressed the first two problems mentioned above.", "labels": [], "entities": []}, {"text": "However, their attempt to tackle the third problem by pairing a conditional random field with their feed-forward tagger provided little accuracy improvement and vastly increased computational complexity, incurring a large efficiency penalty.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9989101886749268}]}, {"text": "We introduce a recurrent neural network-based (RNN) supertagging model to tackle all the above problems, with an emphasis on the third one.", "labels": [], "entities": []}, {"text": "RNNs are powerful models for sequential data, which can potentially capture long-term dependencies, based on an unbounded history of previous words ( \u00a72); similar to we only use distributed word representa-tions ( \u00a72.2).", "labels": [], "entities": []}, {"text": "Our model is highly accurate, and by integrating it with the C&C parser as its adaptive supertagger, we obtain substantial accuracy improvements, outperforming the feed-forward setup on both supertagging and parsing.", "labels": [], "entities": [{"text": "C&C parser", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.8369416445493698}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9991770386695862}, {"text": "parsing", "start_pos": 208, "end_pos": 215, "type": "TASK", "confidence": 0.9488173723220825}]}], "datasetContent": [{"text": "We follow the standard splits of for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set.", "labels": [], "entities": []}, {"text": "The Wikipedia corpus from and the Bioinfer corpus ( are used as two outof-domain test sets.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9733713865280151}, {"text": "Bioinfer corpus", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9451378881931305}]}, {"text": "We compare supertagging accuracy with the MaxEnt C&C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C&C parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9855499863624573}, {"text": "MaxEnt C&C supertagger", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9347671747207642}, {"text": "parsing", "start_pos": 162, "end_pos": 169, "type": "TASK", "confidence": 0.9642094969749451}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.7771307229995728}]}, {"text": "We use the same 425 supertag set used in both C&C and NN.", "labels": [], "entities": [{"text": "C&C and NN", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.8580329537391662}]}, {"text": "For L w , we use the scaled 50-dimensional Turian embeddings (n = 50 for L w ) as initialization.", "labels": [], "entities": []}, {"text": "We have experimented during development with using 100-dimensional embeddings and found no improvements in the resulting model.", "labels": [], "entities": []}, {"text": "Out , and are then scaled by their corresponding input vector size.", "labels": [], "entities": []}, {"text": "We experimented with context window sizes of 3, 5, 7, 9 and 11 during development and found a window size of 7 gives the best performing model on the dev set.", "labels": [], "entities": []}, {"text": "We use a fixed learning rate of 0.0025 and a hidden state size of 200.", "labels": [], "entities": []}, {"text": "To train the model, we optimize cross-entropy loss with stochastic gradient descent using minibatched backpropagation through time; the minibatch size for BPTT, again tuned on the dev set, is set to 9.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 155, "end_pos": 159, "type": "DATASET", "confidence": 0.5996944308280945}]}, {"text": "Without any regularization, we found cross-entropy error on the dev set started to increase while the error on the training set was continuously driven to a very small value.", "labels": [], "entities": []}, {"text": "With the suspicion of overfitting, we experimented with l 1 and l 2 regularization and learning rate decay but none of these techniques gave any noticeable improvements for our model.", "labels": [], "entities": [{"text": "learning rate decay", "start_pos": 87, "end_pos": 106, "type": "METRIC", "confidence": 0.8029059966405233}]}, {"text": "Following, we instead implemented word embedding dropout as a regularization for all the look-up tables, since the capacity of our tagging model mainly comes from the look-up tables, as in their system.", "labels": [], "entities": []}, {"text": "We observed more stable learning and better generalization of the trained model with dropout.", "labels": [], "entities": []}, {"text": "Similar to other forms of droput (), we randomly drop units and their connections to other units at training time.", "labels": [], "entities": []}, {"text": "Concretely, we apply a binary dropout mask to x t , with a dropout rate of 0.25, and attest time no mask is applied, but the input to the network, x t , at each word position is scaled by 0.75.", "labels": [], "entities": []}, {"text": "We experimented during development with different dropout rates, but found the above choice to be optimal in our setting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: 1-best tagging accuracy and speed com- parison on CCGBank Section 00 with a single  CPU core (1,913 sentences), tagging time in secs.", "labels": [], "entities": [{"text": "tagging", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9484497308731079}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9780840277671814}, {"text": "speed com- parison", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.8224143981933594}, {"text": "CCGBank Section 00", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.9427328705787659}]}, {"text": " Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&C \u03b2 levels  on CCGBank Section 00.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9952137470245361}, {"text": "CCGBank Section 00", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.9706278045972189}]}, {"text": " Table 3: 1-best tagging accuracy compari- son on CCGBank Section 23 (2,407 sentences),  Wikipedia (200 sentences) and Bio-GENIA (1,000  sentences).", "labels": [], "entities": [{"text": "accuracy compari- son", "start_pos": 25, "end_pos": 46, "type": "METRIC", "confidence": 0.8623927682638168}, {"text": "CCGBank Section 23", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.9724833766619364}, {"text": "Wikipedia", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.9730802774429321}]}, {"text": " Table 4: Parsing development results on CCGBank Section 00 (auto POS).", "labels": [], "entities": [{"text": "CCGBank Section 00", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9193764726320902}]}, {"text": " Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 230, "end_pos": 244, "type": "DATASET", "confidence": 0.9283045530319214}]}]}