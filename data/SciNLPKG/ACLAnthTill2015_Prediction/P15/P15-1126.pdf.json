{"title": [{"text": "Orthogonality of Syntax and Semantics within Distributional Spaces", "labels": [], "entities": []}], "abstractContent": [{"text": "A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations.", "labels": [], "entities": []}, {"text": "Investigating further, we find that performance on this task can be related to orthogonality within the space.", "labels": [], "entities": []}, {"text": "Explicitly designing such structure into a neu-ral network model results in representations that decompose into orthogonal semantic and syntactic subspaces.", "labels": [], "entities": []}, {"text": "We demonstrate that learning from word-order and morphological structure within En-glish Wikipedia text to enable this decomposition can produce substantial improvements on semantic-similarity, pos-induction and word-analogy tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional methods have become widely used across computational linguistics.", "labels": [], "entities": []}, {"text": "Recent applications include predicate clustering for question answering (, bilingual embeddings for machine translation ( and enhancing the coverage of POS tagging (.", "labels": [], "entities": [{"text": "predicate clustering", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.9033890068531036}, {"text": "question answering", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8824660778045654}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7945422828197479}, {"text": "POS tagging", "start_pos": 152, "end_pos": 163, "type": "TASK", "confidence": 0.7409870624542236}]}, {"text": "The popularity of these methods, stemming from their conceptual simplicity and wide applicability, motivates a deeper analysis of the structure of the representations they produce.", "labels": [], "entities": []}, {"text": "Commonly, these representations are made in a single vector space with similarity being the main structure of interest.", "labels": [], "entities": []}, {"text": "However, recent work by on a word-analogy task suggests that such spaces may have further useful internal regularities.", "labels": [], "entities": []}, {"text": "They found that semantic differences, such as between big and small, and also syntactic differences, as between big and bigger, were encoded consistently across their space.", "labels": [], "entities": []}, {"text": "In particular, they solved the word-analogy problems by exploiting the fact that equivalent relations tended to correspond to parallel vectordifferences.", "labels": [], "entities": []}, {"text": "In this paper, we investigate orthogonality between relations rather than parallelism.", "labels": [], "entities": []}, {"text": "While parallelism serves to ensure that the same relation is encoded consistently, our hypothesis is that orthogonality serves to ensure that distinct relations are clearly differentiable.", "labels": [], "entities": []}, {"text": "We focus specifically on semantic and syntactic relations as these are probably the most distinct classes of properties encoded in distributional spaces.", "labels": [], "entities": []}, {"text": "Empirically, we demonstrate that orthogonality predicts performance on the word-analogy task for three existing approaches to constructing word vectors.", "labels": [], "entities": []}, {"text": "We also attempt to enhance the weakest of these three models by imposing an orthogonal structure in its construction.", "labels": [], "entities": []}, {"text": "In these extensions, word representations decompose into orthogonal semantic and syntactic spaces, and we use word-order and morphology to drive this separation.", "labels": [], "entities": []}, {"text": "This decomposition also allows us to define a novel approach to solving the word-analogy problems and our extended models become competitive with the other two original models.", "labels": [], "entities": []}, {"text": "In addition, we show that the separate semantic and syntactic sub-spaces gain improved performance on semantic-similarity and POS-induction tasks respectively.", "labels": [], "entities": []}, {"text": "Our experiments here are based on models that construct vector-representations within a model that predicts the occurence of words in context.", "labels": [], "entities": []}, {"text": "In particular we focus on the CBOW and Skip-gram models of Mikolov etal.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8324295878410339}]}, {"text": "(2013b) and GloVe model.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.7334746718406677}]}, {"text": "These models share the property of producing a single general representation for each word, which can be utilized in a variety of tasks, from POS tagging to semantic role labelling.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 142, "end_pos": 153, "type": "TASK", "confidence": 0.8747884035110474}, {"text": "semantic role labelling", "start_pos": 157, "end_pos": 180, "type": "TASK", "confidence": 0.6692668696244558}]}, {"text": "In contrast, here we attempt to decompose the representations into separate seman- To motivate this decomposition, consider the analogical reasoning task that apply neural embeddings to.", "labels": [], "entities": []}, {"text": "In this task, given vectors for the words big, bigger and small, we try to predict the vector for smaller.", "labels": [], "entities": []}, {"text": "They find that in practice smaller \u2248 small + bigger \u2212 big produces an estimate that is frequently closer to the actual representation of smaller than any other word vector.", "labels": [], "entities": []}, {"text": "We can think of the vector bigger \u2212 big as representing the syntactic relation that holds between an adjective and its comparative.", "labels": [], "entities": []}, {"text": "Adding this syntactic structure to small thus ends up at, or near, the relevant comparative, smaller.", "labels": [], "entities": []}, {"text": "Alternatively, we could think of the vector small\u2212big as representing the semantic difference between small and big, and adding this relation to bigger produces a semantic transformation to smaller.", "labels": [], "entities": []}, {"text": "the question of what happens at the corners.", "labels": [], "entities": []}, {"text": "In other words, what is the relationship between the semantic differences, e.g. smaller \u2212 bigger, and the syntactic differences, e.g. smaller \u2212 small?", "labels": [], "entities": []}, {"text": "In this paper we explore the idea that such semantic and syntactic relations ought to be orthogonal to each other.", "labels": [], "entities": []}, {"text": "This hypothesis arises both from the intuition that such distinct types of information ought to be represented distinctly within our space and also from the observation that solving the word-analogy task requires that words can be uniquely identified by combining these vector differences and so small \u2212 big ought to be easily differentiable from bigger \u2212 big as these relations point to different end results starting from big.", "labels": [], "entities": []}, {"text": "Essentially, orthogonality will make better use of the volume within the space, spreading words with different semantic or syntactic characteristics further from each other.", "labels": [], "entities": []}, {"text": "In terms of predicting smaller from big, bigger and small, orthogonality of the relationship between smaller \u2212 bigger and smaller \u2212 small can be expressed in terms of their dot product: If all semantic relations were genuinely orthogonal to all syntactic relations, then their space would be decomposable into two orthogonal subspaces: one semantic, the other syntactic.", "labels": [], "entities": []}, {"text": "Any word representation, v, would then be the combination of a unique semantic vector, b, within the semantic subspace and a unique syntactic vector, s, within the syntactic subspace.", "labels": [], "entities": []}, {"text": "If b were given a representation in terms of e components, and sin terms off components, then v would have a representation in terms of d = e + f components which would just be the concatenation of the two sets of components, which we will represent in terms of the operator \u2295.", "labels": [], "entities": []}, {"text": "Achieving this differentiation within the representations requires that the model have a means of differentiating semantic and syntactic information in the raw text.", "labels": [], "entities": []}, {"text": "We consider two very simple approaches for this purpose, based on morphological and word order features.", "labels": [], "entities": []}, {"text": "Both these types of features have been previously employed in simple word co-occurrence models (e.g.,, with bag-of-words and Figure 2: CBOW model predicting wt from of a bag-of-words representation, b context , of a 4-word window around it. lemmatization being good for semantic applications, while sequential order and suffixes is more useful for syntax.", "labels": [], "entities": []}, {"text": "More recently, demonstrated that word order could be used to separate syntactic from semantic structure, but only within a simple bigram language model, rather than a neural network model, and without exploiting morphology.", "labels": [], "entities": []}, {"text": "Our enhanced models are based on Mikolov et al.'s (2013a) CBOW architecture, which is described in Section 2.", "labels": [], "entities": []}, {"text": "The novel extensions to it, employing a semantic-syntactic decomposition, are proposed in Section 3.", "labels": [], "entities": []}, {"text": "We then describe our evaluation tasks and provide their results in Sections 5 and 6 respectively.", "labels": [], "entities": []}, {"text": "These evaluations are based on the word-analogy dataset of, a noun-verb similarity task) and a POS clustering task.", "labels": [], "entities": [{"text": "POS clustering", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.7539786994457245}]}], "datasetContent": [{"text": "The hypothesis that orthogonality is useful to word vector representations is investigated empirically in two ways.", "labels": [], "entities": [{"text": "word vector representations", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6436964273452759}]}, {"text": "Firstly, we attempt to quantify the orthogonality that is already implicitly present in the original CBOW, Skip-gram and GloVe representations and relate that to their success in the word-analogy task.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8803754448890686}]}, {"text": "Secondly, the extensions described above are evaluated on a number of tasks in order to evaluate the benefits of their explicit orthogonality between components.", "labels": [], "entities": []}], "tableCaptions": []}