{"title": [{"text": "A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing", "labels": [], "entities": [{"text": "Transition-Based Dependency Parsing", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.6894265015920004}]}], "abstractContent": [{"text": "Neural probabilistic parsers are attractive for their capability of automatic feature combination and small data sizes.", "labels": [], "entities": [{"text": "Neural probabilistic parsers", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7583359877268473}]}, {"text": "A transition-based greedy neural parser has given better accuracies over its linear counterpart.", "labels": [], "entities": [{"text": "transition-based greedy neural parser", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.6483779773116112}, {"text": "accuracies", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9592671394348145}]}, {"text": "We propose a neural probabilistic structured-prediction model for transition-based dependency parsing, which integrates search and learning.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 66, "end_pos": 101, "type": "TASK", "confidence": 0.6091930568218231}]}, {"text": "Beam search is used for decoding, and contrastive learning is performed for maximizing the sentence-level log-likelihood.", "labels": [], "entities": []}, {"text": "In standard Penn Treebank experiments, the structured neural parser achieves a 1.8% accuracy improvement upon a competitive greedy neural parser baseline, giving performance comparable to the best linear parser.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.9804616570472717}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9988962411880493}]}], "introductionContent": [{"text": "Transition-based methods have given competitive accuracies and efficiencies for dependency parsing (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.871019572019577}]}, {"text": "These parsers construct dependency trees by using a sequence of transition actions, such as SHIFT and REDUCE, over input sentences.", "labels": [], "entities": [{"text": "REDUCE", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.8036383986473083}]}, {"text": "High accuracies are achieved by using a linear model and millions of binary indicator features.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9887853860855103}]}, {"text": "Recently, propose an alternative dependency parser using a neural network, which represents atomic features as dense vectors, and obtains feature combination automatically other than devising high-order features manually.", "labels": [], "entities": []}, {"text": "The greedy neural parser of gives higher accuracies compared to * Work done while the first author was visiting SUTD.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9949325919151306}]}, {"text": "the greedy linear MaltParser (), but lags behind state-of-the-art linear systems with sparse features (, which adopt global learning and beam search decoding ().", "labels": [], "entities": []}, {"text": "The key difference is that is a local classifier that greedily optimizes each action.", "labels": [], "entities": []}, {"text": "In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions.", "labels": [], "entities": [{"text": "structured neural probabilistic dependency parsing", "start_pos": 48, "end_pos": 98, "type": "TASK", "confidence": 0.6277813911437988}]}, {"text": "Following, beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate ().", "labels": [], "entities": []}, {"text": "Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial.", "labels": [], "entities": [{"text": "transition-based neural parsing", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.6900608142217001}]}, {"text": "A direct adaptation of the framework of under the neural probabilistic model setting does not yield good results.", "labels": [], "entities": []}, {"text": "The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron).", "labels": [], "entities": []}, {"text": "Due to the dense parameter space, for neural models, the scores of actions in a sequence are relatively more dependent than that in the linear models.", "labels": [], "entities": []}, {"text": "As a result, the log probability of an action sequence cannot be modeled just as the sum of log probabilities of each action in the sequence, which is the case of structured linear model.", "labels": [], "entities": []}, {"text": "We address the challenge by using a softmax function to directly model the distribution of action sequences.", "labels": [], "entities": []}, {"text": "Second, for the structured model above, maximum-likelihood training is computationally intractable, requiring summing overall possible action sequences, which is difficult for transition-based parsing.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 176, "end_pos": 200, "type": "TASK", "confidence": 0.599637359380722}]}, {"text": "To address this challenge, we take a contrastive learning approach.", "labels": [], "entities": []}, {"text": "Using the sum of log probabilities over the action sequences in the beam to approximate that overall possible action sequences.", "labels": [], "entities": []}, {"text": "In standard PennTreebank () evaluations, our parser achieves a significant accuracy improvement (+1.8%) over the greedy neural parser of, and gives the best reported accuracy by shift-reduce parsers.", "labels": [], "entities": [{"text": "PennTreebank", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.9221624135971069}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9987708926200867}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9893442988395691}]}, {"text": "The incremental neural probabilistic framework with global contrastive learning and beam search could be used in other structured prediction tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We set the following hyper-parameters according to the baseline greedy neural parser: embedding size d = 50, hidden layer size d h = 200, regularization parameter \u03bb = 10 \u22128 , initial learning rate of Adagrad \u03b1 = 0.01.", "labels": [], "entities": []}, {"text": "For the structured neural parser, beam size and mini-batch size are important to the parsing performance.", "labels": [], "entities": [{"text": "structured neural parser", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6677090724309286}]}, {"text": "We tune them on the development set.", "labels": [], "entities": []}, {"text": "Beam search enlarges the search space.", "labels": [], "entities": []}, {"text": "More importantly, the larger the beam is, the more accurate our training algorithm is. the Contrastive learning approximates the exact probabilities over exponential many action sequences by computing the relative probabilities over action sequences in the beam (Equation 18).", "labels": [], "entities": []}, {"text": "Therefore, the larger the beam is, the more accurate the relative probability is.", "labels": [], "entities": []}, {"text": "The first column of shows the accuracies of the structured neural parser on the development set with different beam sizes, which improves as the beam size increases.", "labels": [], "entities": []}, {"text": "We set the final beam size as 100 according to the accuracies on development set.: Comparison between sentence-level loglikelihood and ranking model. with beam search decoding.", "labels": [], "entities": []}, {"text": "The score of a whole action sequence is computed by the sum of log action probabilities (Equation 7).", "labels": [], "entities": []}, {"text": "As shown in the second column of, beam search can improve parsing slightly.", "labels": [], "entities": [{"text": "beam search", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9308516383171082}, {"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9863437414169312}]}, {"text": "When the beam size increases beyond 16, however, accuracy improvements stop.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9997135996818542}]}, {"text": "In contrast, by integrating beam search and global learning, our parsing performance benefits from large beam sizes much more significantly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9623715877532959}]}, {"text": "With abeam size as 16, the structured neural parser gives an accuracy close to that of baseline greedy parser 3 . When the beam size is 100, the structured neural parser outperforms baseline by 1.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.999428927898407}]}, {"text": "find that global learning and beam search should be used jointly for improving parsing using a linear transition-based model.", "labels": [], "entities": []}, {"text": "In particular, increasing the beam size, the accuracy of ZPar () increases significantly, but that of MaltParser does not.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.999708354473114}]}, {"text": "For structured neural parsing, our finding is similar: integrating search and learning is much more effective than using beam search only in decoding.", "labels": [], "entities": [{"text": "structured neural parsing", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6313666701316833}]}, {"text": "Our results in are obtained by using the same beam sizes for both training and testing.", "labels": [], "entities": []}, {"text": "Zhang and Nivre (2012) also find that for their lin- ear model, the best results are achieved by using the same beam sizes during training and testing.", "labels": [], "entities": []}, {"text": "We find that this observation does not apply to our neural parser.", "labels": [], "entities": []}, {"text": "In our case, a large training beam always leads to better results.", "labels": [], "entities": []}, {"text": "This is likely because a large beam improves contrastive learning.", "labels": [], "entities": []}, {"text": "As a result, our training beam size is set to 100 for the final test.", "labels": [], "entities": []}, {"text": "Parsing performance using neural networks is highly sensitive to the batch size of training.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9484619498252869}]}, {"text": "In greedy neural parsing), the accuracy on the development data improves from 85% to 91% by setting the batch size to 10 and 100000, respectively.", "labels": [], "entities": [{"text": "greedy neural parsing", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.6931880712509155}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995995163917542}]}, {"text": "In structured neural parsing, we fix the beam size as 100 and draw the accuracies on the development set by the training iteration.", "labels": [], "entities": [{"text": "structured neural parsing", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.6651016374429067}]}, {"text": "As shown in, in 5000 training iterations, the parsing accuracies improve as the iteration grows, yet different batch sizes result in different convergence accuracies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9589695334434509}]}, {"text": "With a batch size of 5000, the parsing accuracy is about 25% higher than with a batch size of 1 (i.e. SGD).", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9749533534049988}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9801657199859619}]}, {"text": "For the remaining experiments, we set batch size to 5000, which achieves the best accuracies on development testing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracies of structured neural parsing  and local neural classification parsing with differ- ent beam sizes.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.971236526966095}, {"text": "structured neural parsing", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.688113788763682}, {"text": "local neural classification parsing", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.769688069820404}]}, {"text": " Table 4: Comparison between sentence-level log- likelihood and ranking model.", "labels": [], "entities": [{"text": "sentence-level log- likelihood", "start_pos": 29, "end_pos": 59, "type": "METRIC", "confidence": 0.531729444861412}]}, {"text": " Table 5: Results on WSJ. Speed: sentences per  second.  \u2020: semi-supervised learning.  \u2021: joint  POS-tagging and dependency parsing models.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.760897696018219}, {"text": "Speed", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9732511043548584}, {"text": "dependency parsing", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7065414637327194}]}]}