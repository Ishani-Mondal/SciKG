{"title": [{"text": "A Convolution Kernel Approach to Identifying Comparisons in Text", "labels": [], "entities": [{"text": "Identifying Comparisons in Text", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.8825057446956635}]}], "abstractContent": [{"text": "Comparisons in text, such as in online reviews , serve as useful decision aids.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the task of identifying whether a comparison exists between a specific pair of entity mentions in a sentence.", "labels": [], "entities": []}, {"text": "This formulation is trans-formative, as previous work only seeks to determine whether a sentence is comparative , which is presumptuous in the event the sentence mentions multiple entities and is comparing only some, not all, of them.", "labels": [], "entities": []}, {"text": "Our approach leverages not only lexical features such as salient words, but also structural features expressing the relationships among words and entity mentions.", "labels": [], "entities": []}, {"text": "To model these features seamlessly, we rely on a dependency tree representation , and investigate the applicability of a series of tree kernels.", "labels": [], "entities": []}, {"text": "This leads to the development of anew context-sensitive tree kernel: Skip-node Kernel (SNK).", "labels": [], "entities": []}, {"text": "We further describe both its exact and approximate computations.", "labels": [], "entities": [{"text": "exact", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9447956085205078}]}, {"text": "Through experiments on real-life datasets, we evaluate the effectiveness of our kernel-based approach for comparison identification, as well as the utility of SNK and its approximations.", "labels": [], "entities": [{"text": "comparison identification", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.9121343493461609}]}], "introductionContent": [{"text": "When weighing various alternatives, users increasingly turn to the social media, by scouring online reviews, discussion forums, etc.", "labels": [], "entities": []}, {"text": "Our goal is to extract from such corpora those text snippets where users make direct comparisons of entities.", "labels": [], "entities": []}, {"text": "While sentiment analysis) maybe helpful in evaluating individual entities, comparison by the same author within a sentence provides an unambiguous and more equitable basis for the relative positions of two entities on some aspect.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.9410380125045776}]}, {"text": "For example, the sentence s 1 in, taken from an Amazon review about a digital camera, makes two distinct comparisons: #1) between \"A630\" and \"A-series cameras\" and #2) between \"A630\" and \"its competition\", with a clear sense of which entity mention is the greater on some aspect (\"larger\").", "labels": [], "entities": []}, {"text": "Moreover, comparisons maybe objective (e.g., larger) or subjective (e.g., better), while sentiments are primarily subjective.", "labels": [], "entities": []}, {"text": "Problem Given a sentence and a specific pair of entity mentions, we seek to determine if a comparison exists between those two mentions.", "labels": [], "entities": []}, {"text": "In previous work, the problem was formulated as identifying comparative sentences, i.e., those containing at least one comparison).", "labels": [], "entities": []}, {"text": "This is not ideal because a sentence may contain more than two entity mentions, and maybe comparing only some of them.", "labels": [], "entities": []}, {"text": "For instance, s 1 is comparative with respect to the pair (A630, A-series cameras) and the pair (A630, its competition), but not the pair (A-series cameras, its competition).", "labels": [], "entities": [{"text": "A630", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9758856892585754}, {"text": "A630", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9793344736099243}]}, {"text": "We therefore postulate that the more appropriate formulation is comparisons within sentences.", "labels": [], "entities": []}, {"text": "If a sentence compares two entities (A, B) with respect to some aspect Z, it should be possible to reformulate it into another sentence such as: \"A is better than B with respect to Z\").", "labels": [], "entities": []}, {"text": "Based on this definition, there is no comparison between (A-series cameras, its competition) in s 1 . Here, we adopt this apt definition with a slight restriction to make it more practical, and seek to identify such comparisons automatically.", "labels": [], "entities": []}, {"text": "We consider only sentences with at least two entity mentions involved in gradable comparisons, i.e., a clear sense of scaling in the comparison (e.g., A is better than B.).", "labels": [], "entities": []}, {"text": "Such comparisons are more useful in investigating the pros and cons of entities, as opposed to equative comparisons expressing parity between two mentions (e.g., A is as good as B.), or superlative comparisons expressing the primacy of an entity with respect to unknown reference entities (e.g., A is the best.).", "labels": [], "entities": []}], "datasetContent": [{"text": "Data For experiments, we compiled two annotated datasets in two domains: Digital Camera and Cell Phone from online review sentences.", "labels": [], "entities": []}, {"text": "The reviews were collected from Amazon and Epinions 2 . We identified the entity mentions through dictionary matching, followed by manual annotation to weed out false positives.", "labels": [], "entities": []}, {"text": "Each dictionary entry is a product name (e.g., Canon PowerShot D20, D7100) or a common product reference (e.g., this camera, that phone).", "labels": [], "entities": [{"text": "Canon PowerShot D20", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9135767420132955}]}, {"text": "The dataset includes only sentences that contain at least two entity mentions.", "labels": [], "entities": []}, {"text": "Every pair of entities within a sentence was annotated with a comparative label according to the definition given in Section 2.", "labels": [], "entities": []}, {"text": "A sentence is comparative if at least one pair of entities within it is in a comparative relation.", "labels": [], "entities": []}, {"text": "shows the dataset properties, in terms of the number sentences and the percentage that are comparative sentences, as well as the number of pairs of entity mentions and the percentage that are comparative relations.", "labels": [], "entities": []}, {"text": "There are more pairs than sentences, i.e., many sentences mention more than two entities.", "labels": [], "entities": []}, {"text": "This dataset subsumes the annotated gradable In this particular case, all features could have been computed by Lookahead Skip-node using preorder right-to-left DFS enumeration, although it may not be true in general.), into which we built Skip-node Kernel.", "labels": [], "entities": []}, {"text": "We further release a separate standalone library that we built, called Tree-SVM 4 , which does SVM optimization using the tree kernels described in this paper.", "labels": [], "entities": [{"text": "SVM optimization", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.947199672460556}]}, {"text": "The sentences were parsed and lemmatized with the use of the Stanford NLP software.", "labels": [], "entities": [{"text": "Stanford NLP software", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.9113449056943258}]}, {"text": "The experiments were done on 10 random data splits in 80:20 proportion of training vs. testing.", "labels": [], "entities": []}, {"text": "Performance is measured by using F 1 , which is the harmonic mean of precision P and recall R: The statistical significance 5 is measured by randomization test).", "labels": [], "entities": [{"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9903188049793243}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9936356544494629}, {"text": "recall R", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9782327115535736}, {"text": "statistical significance 5", "start_pos": 99, "end_pos": 125, "type": "METRIC", "confidence": 0.8258078296979269}]}, {"text": "The hyper-parameters, including the baselines', were optimized for F 1 through grid-search.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The dataset size for each domain.", "labels": [], "entities": []}, {"text": " Table 3: Comparison identification task", "labels": [], "entities": [{"text": "Comparison identification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.9052460789680481}]}, {"text": " Table 4: Comparative sentence identification task", "labels": [], "entities": [{"text": "Comparative sentence identification", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7444360454877218}]}, {"text": " Table 6: Tree kernels combined with bag-of-words", "labels": [], "entities": []}, {"text": " Table 7: Effectiveness: SNK vs. approximations", "labels": [], "entities": [{"text": "SNK", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9100123047828674}]}]}