{"title": [], "abstractContent": [{"text": "Discourse parsing is the process of discovering the latent relational structure of along form piece of text and remains a significant open challenge.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8502635657787323}]}, {"text": "One of the most difficult tasks in discourse parsing is the classification of implicit discourse relations.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6832739561796188}, {"text": "classification of implicit discourse relations", "start_pos": 60, "end_pos": 106, "type": "TASK", "confidence": 0.8165756702423096}]}, {"text": "Most state-of-the-art systems do not leverage the great volume of unlabeled text available on the web-they rely instead on human annotated training data.", "labels": [], "entities": []}, {"text": "By incorporating a mixture of labeled and unla-beled data, we are able to improve relation classification accuracy, reduce the need for annotated data, while still retaining the capacity to use labeled data to ensure that specific desired relations are learned.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.849927693605423}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9226898550987244}]}, {"text": "We achieve this using a latent variable model that is trained in a reduced dimensionality subspace using spectral methods.", "labels": [], "entities": []}, {"text": "Our approach achieves an F 1 score of 0.485 on the implicit relation labeling task for the Penn Discourse Treebank.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9911185105641683}, {"text": "Penn Discourse Treebank", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.983761211236318}]}], "introductionContent": [{"text": "Discourse parsing is a fundamental task in natural language processing that entails the discovery of the latent relational structure in a multi-sentence piece of text.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8414607644081116}]}, {"text": "Unlike semantic and syntactic parsing, which are used for single sentence parsing, discourse parsing is used to discover intersentential relations in longer pieces of text.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8013640940189362}, {"text": "single sentence parsing", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7785442471504211}, {"text": "discourse parsing", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7312555611133575}]}, {"text": "Without discourse, parsing methods can only be used to understand documents as sequences of unrelated sentences.", "labels": [], "entities": []}, {"text": "Unfortunately, manual annotation of discourse structure in text is costly and time consuming.", "labels": [], "entities": []}, {"text": "Multiple annotators are required for each relation to estimate inter-annotator agreement.", "labels": [], "entities": []}, {"text": "The Penn Discourse Treebank (PDTB) (. is one of the largest annotated discourse parsing datasets, with 16,224 implicit relations.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.9389976263046265}, {"text": "discourse parsing", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7603553831577301}]}, {"text": "However, this pales in comparison to unlabeled datasets that can include millions of sentences of text.", "labels": [], "entities": []}, {"text": "By augmenting a labeled dataset with unlabeled data, we can use a bootstrapping framework to improve predictive accuracy, and reduce the need for labeled data-which could make it much easier to port discourse parsing algorithms to new domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.8563092947006226}, {"text": "discourse parsing", "start_pos": 199, "end_pos": 216, "type": "TASK", "confidence": 0.7442850172519684}]}, {"text": "On the other hand, a fully unsupervised parser may not be desirable because in many applications specific discourse relations must be identified, which would be difficult to achieve without the use of labeled examples.", "labels": [], "entities": []}, {"text": "There has recently been growing interest in a breed of algorithms based on spectral decomposition, which are well suited to training with unlabeled data.", "labels": [], "entities": [{"text": "spectral decomposition", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7312276065349579}]}, {"text": "Spectral algorithms utilize matrix factorization algorithms such as Singular Value Decomposition (SVD) and rank factorization to discover low-rank decompositions of matrices or tensors of empirical moments.", "labels": [], "entities": [{"text": "Singular Value Decomposition (SVD)", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.7059319714705149}]}, {"text": "In many models, these decompositions allow us to identify the subspace spanned by a group of parameter vectors or the actual parameter vectors themselves.", "labels": [], "entities": []}, {"text": "For tasks where they can be applied, spectral methods provide statistically consistent results that avoid local maxima.", "labels": [], "entities": []}, {"text": "Also, spectral algorithms tend to be much faster-sometimes orders of magnitude faster-than competing approaches, which makes them ideal for tackling large datasets.", "labels": [], "entities": []}, {"text": "These methods can be viewed as inferring something about the latent structure of a domain-for example, in a hidden Markov model, the number of latent states and the sparsity pattern of the transition matrix are forms of latent structure, and spectral methods can recover both in the limit.", "labels": [], "entities": []}, {"text": "This paper presents a semi-supervised spectral model fora sequential relation labeling task for discourse parsing.", "labels": [], "entities": [{"text": "sequential relation labeling task", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.6880196258425713}, {"text": "discourse parsing", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7333602905273438}]}, {"text": "Besides the theoretically desirable properties mentioned above, we also demon-strate the practical advantages of the model with an empirical evaluation on the Penn Discourse Treebank (PDTB) () dataset, which yields an F 1 score of 0.485.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB) () dataset", "start_pos": 159, "end_pos": 200, "type": "DATASET", "confidence": 0.9365496039390564}, {"text": "F 1 score", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9897269010543823}]}, {"text": "This accuracy shows a 7-9 percentage point improvement over approaches that do not utilize unlabeled training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9994134902954102}]}], "datasetContent": [{"text": "This section defines the discourse parsing problem and discusses the characteristics of the PDTB.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.70322585105896}]}, {"text": "The PDTB consists of annotated articles from the Wall Street Journal and is used in our empirical evaluations.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.966352621714274}]}, {"text": "This is combined with the New York Times Annotated Corpus, which includes 1.8 million New York Times articles printed between 1987 and 2007.", "labels": [], "entities": [{"text": "New York Times Annotated Corpus", "start_pos": 26, "end_pos": 57, "type": "DATASET", "confidence": 0.7395385205745697}]}, {"text": "Discourse parsing can be reduced to three separate tasks.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8745148777961731}]}, {"text": "First, the text must be decomposed into elementary discourse units (EDUs), which mayor may not coincide with sentence boundaries.", "labels": [], "entities": []}, {"text": "The EDUs are often independent clauses that maybe connected with conjunctions.", "labels": [], "entities": []}, {"text": "After the text has been partitioned into EDUs, the discourse structure must be identified.", "labels": [], "entities": []}, {"text": "This requires us to identify all pairs of EDUs that will be connected with some discourse relation.", "labels": [], "entities": []}, {"text": "These relational links induce the skeletal structure of the discourse parse tree.", "labels": [], "entities": []}, {"text": "Finally, each connection identified in the previous step must be labeled using a known set of relations.", "labels": [], "entities": []}, {"text": "Examples of these discourse relations include concession, causal, and instantiation relations.", "labels": [], "entities": []}, {"text": "In the PDTB, only adjacent discourse units are connected with a discourse relation, so with this dataset we are considering parse sequences rather than parse trees.", "labels": [], "entities": []}, {"text": "In this work, we focus on the relation labeling task, as fairly simple methods perform quite well at the other two tasks ().", "labels": [], "entities": [{"text": "relation labeling task", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8951130112012228}]}, {"text": "We use the ground truth parse structures provided by the PDTB dataset, so as to isolate the error introduced by relation labeling in our results, but in practice a greedy structure learning algorithm can be used if the parse structures are not known a priori.", "labels": [], "entities": [{"text": "PDTB dataset", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.986526221036911}]}, {"text": "Some of the relations in the dataset are induced by specific connective words in the text.", "labels": [], "entities": []}, {"text": "For example, a contrast relation maybe explicitly revealed by the conjunction but.", "labels": [], "entities": []}, {"text": "Simple classifiers using only the text of the discourse connective with POS tags can find explicit relations with high accuracy ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9951156377792358}]}, {"text": "The following sentence shows an example of a more difficult implicit relation.", "labels": [], "entities": []}, {"text": "In this sentence, two EDUs are connected with an explanatory relation, shown in bold, although the connective word does not occur in the text.", "labels": [], "entities": []}, {"text": "\"But a few funds have taken other defensive steps.", "labels": [], "entities": []}, {"text": "Some have raised their cash positions to record levels.", "labels": [], "entities": []}, {"text": "[BECAUSE] High cash positions help buffer a fund when the market falls.\"", "labels": [], "entities": [{"text": "BECAUSE", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.728367030620575}]}, {"text": "We focus on the more difficult implicit relations that are not induced by coordinating connectives in the text.", "labels": [], "entities": []}, {"text": "The implicit relations have been shown to require more sophisticated feature sets including syntactic and linguistic information (.", "labels": [], "entities": []}, {"text": "The PDTB dataset includes 16,053 examples of implicit relations.", "labels": [], "entities": [{"text": "PDTB dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.967534065246582}]}, {"text": "A full list of the PDTB relations is available in ().", "labels": [], "entities": []}, {"text": "The relations are organized hierarchically into top level, types, and subtypes.", "labels": [], "entities": []}, {"text": "Our experiments focus on learning only up to level 2, as the level 3 (sub-type) relations are too specific and show only 80% inter-annotator agreement.", "labels": [], "entities": []}, {"text": "There are 16 level 2 relations in the PDTB, but the 5 least common relations only appear a handful of times in the dataset and are omitted from our tests, yielding 11 possible classes.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.9224562048912048}]}], "tableCaptions": []}