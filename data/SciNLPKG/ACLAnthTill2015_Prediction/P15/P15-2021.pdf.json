{"title": [{"text": "Lexicon Stratification for Translating Out-of-Vocabulary Words", "labels": [], "entities": []}], "abstractContent": [{"text": "A language lexicon can be divided into four main strata, depending on origin of words: core vocabulary words, fully-and partially-assimilated foreign words, and unassim-ilated foreign words (or transliterations).", "labels": [], "entities": []}, {"text": "This paper focuses on translation of fully-and partially-assimilated foreign words, called \"borrowed words\".", "labels": [], "entities": [{"text": "translation of fully-and partially-assimilated foreign words", "start_pos": 22, "end_pos": 82, "type": "TASK", "confidence": 0.8536667029062907}]}, {"text": "Borrowed words (or loanwords) are content words found in nearly all languages, occupying up to 70% of the vocabulary.", "labels": [], "entities": [{"text": "Borrowed words (or loanwords) are content words", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7286708752314249}]}, {"text": "We use models of lexical borrowing in machine translation as a pivoting mechanism to obtain translations of out-of-vocabulary loanwords in a low-resource language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6974343210458755}]}, {"text": "Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9977390766143799}]}], "introductionContent": [{"text": "Out-of-vocabulary (OOV) words area ubiquitous and difficult problem in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.8073517680168152}]}, {"text": "When a translation system encounters an OOV-a word that was not observed in the training data, and the trained system thus lacks its translation variants-it usually outputs the word just as it is in the source language, producing erroneous and disfluent translations.", "labels": [], "entities": []}, {"text": "All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9848073720932007}, {"text": "OOVs", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.8076205849647522}]}, {"text": "These are often named entities and neologisms.", "labels": [], "entities": []}, {"text": "However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words.", "labels": [], "entities": []}, {"text": "Procuring translations for OOVs has been a subject of active research for decades.", "labels": [], "entities": []}, {"text": "Translation of named entities is usually generated using transliteration techniques.", "labels": [], "entities": [{"text": "Translation of named entities", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9012846648693085}]}, {"text": "Extracting a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources).", "labels": [], "entities": [{"text": "Extracting a translation lexicon", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7292203456163406}]}, {"text": "In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists ().", "labels": [], "entities": []}, {"text": "Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages.", "labels": [], "entities": []}, {"text": "To achieve this goal, we replace transliteration by anew technique that captures more complex morpho-phonological transformations of historically-related words.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Swahili-English parallel corpus was crawled from the Global Voices project website 8 . To simulate resource-poor scenario for the Romanian-English language pair, we sample a parallel corpus of same size from the transcribed TED talks ().", "labels": [], "entities": [{"text": "Global Voices project website", "start_pos": 57, "end_pos": 86, "type": "DATASET", "confidence": 0.8393764197826385}]}, {"text": "To evalu- For Arabic and French we use the GlobalPhone pronunciation dictionaries () (we manually convert them to IPA).", "labels": [], "entities": [{"text": "GlobalPhone pronunciation dictionaries", "start_pos": 43, "end_pos": 81, "type": "DATASET", "confidence": 0.9111204544703165}]}, {"text": "For Swahili and Romanian we automatically construct pronunciation dictionaries using the Omniglot grapheme-to-IPA conversion rules at www.omniglot.com.", "labels": [], "entities": []}, {"text": "We assume that while parallel data is limited in the recipient language, monolingual data is available.", "labels": [], "entities": []}, {"text": "In all the MT experiments, we use the cdec 9 toolkit (, and optimize parameters with MERT.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.977833092212677}, {"text": "MERT", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9318985939025879}]}, {"text": "English 4-gram language models with Kneser-Ney smoothing are trained using KenLM) on the target side of the parallel training corpora and on the Gigaword corpus (.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 145, "end_pos": 160, "type": "DATASET", "confidence": 0.9564931988716125}]}, {"text": "Results are reported using case-insensitive BLEU with a single reference ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9841352105140686}]}, {"text": "We train three systems for each MT setup; reported BLEU scores are averaged over systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9837950468063354}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9987799525260925}]}, {"text": "The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification.", "labels": [], "entities": []}, {"text": "What is the overall improvement that can be achieved if we correctly translate all OOVs that were borrowed from another language?", "labels": [], "entities": []}, {"text": "What is the overall improvement that can be achieved if we correctly translate all OOVs?", "labels": [], "entities": []}, {"text": "We answer this question by defining \"upper bound\" experiments.", "labels": [], "entities": []}, {"text": "In the upper bound experiment we word-align all available parallel corpora, including dev and test sets, and extract from the alignments oracle translations of OOV words.", "labels": [], "entities": []}, {"text": "Then, we append the extracted OOV dictionaries to the training corpora and re-train SMT setups without OOVs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9845739006996155}]}, {"text": "Translation scores of the resulting system provide an upper bound of an improvement from correctly translating all OOVs.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8574573397636414}]}, {"text": "When we append oracle translations of the subset of OOV dictionaries, in particular translations of all OOVs for which the output of the borrowing system is www.cdec-decoder.org not empty, we obtain an upper bound that can be achieved using our method (if the borrowing system provided perfect outputs).", "labels": [], "entities": []}, {"text": "Understanding the upper bounds is relevant not only for our experiments, but for any experiments that involve augmenting translation dictionaries; however, we are not aware of prior work providing similar analysis of upper bounds, and we recommend this as a calibrating procedure for future work on OOV mitigation strategies.", "labels": [], "entities": [{"text": "OOV mitigation", "start_pos": 299, "end_pos": 313, "type": "TASK", "confidence": 0.9270468354225159}]}, {"text": "As described in \u00a72.2, we integrate translations of OOV loanwords in the translation model.", "labels": [], "entities": []}, {"text": "Due to data sparsity, we conjecture that non-OOVs that occur only few times in the training corpus can also lack appropriate translation candidates, i.e., these are targetlanguage OOVs.", "labels": [], "entities": []}, {"text": "We therefore run the borrowing system on OOVs and non-OOV words that occur less than 3 times in the training corpus.", "labels": [], "entities": []}, {"text": "We list in  Transliteration-augmented setups.", "labels": [], "entities": []}, {"text": "In addition to the standard baselines, we evaluate transliteration-augmented setups, where we replace the borrowing model by a transliteration model ().", "labels": [], "entities": []}, {"text": "The model is a linear-chain CRF where we label each source character with a sequence of target characters.", "labels": [], "entities": []}, {"text": "The features are label unigrams and bigrams, separately or conjoined with a moving window of source characters.", "labels": [], "entities": []}, {"text": "We employ the Swahili-Arabic and Romanian-French transliteration systems that were used as baselines in (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Dev and test corpora sizes.", "labels": [], "entities": []}, {"text": " Table 4: Sizes of translated lexicons extracted using pivoting  via borrowing and integrated in translation models.", "labels": [], "entities": []}, {"text": " Table 5: Sizes of translated lexicons extracted using pivoting  via transliteration and integrated in translation models.", "labels": [], "entities": []}, {"text": " Table 1: Statistics of the Swahili-English corpora and source-side OOV for 4K, 8K, 14K parallel training sentences.", "labels": [], "entities": [{"text": "OOV", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8435930013656616}]}, {"text": " Table 2: Statistics of the Romanian-English corpora and source-side OOV for 4K, 8K, 14K parallel training sentences.", "labels": [], "entities": [{"text": "OOV", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9403648972511292}]}, {"text": " Table 6: Swahili-English MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.7180538773536682}]}, {"text": " Table 7: Romanian-English MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.811962902545929}]}]}