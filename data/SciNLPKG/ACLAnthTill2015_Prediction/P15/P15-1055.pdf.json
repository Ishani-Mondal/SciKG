{"title": [{"text": "Learning to Explain Entity Relationships in Knowledge Graphs", "labels": [], "entities": [{"text": "Learning to Explain Entity Relationships in Knowledge Graphs", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7561335265636444}]}], "abstractContent": [{"text": "We study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions.", "labels": [], "entities": []}, {"text": "Our method extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities.", "labels": [], "entities": []}, {"text": "We model this task as a learning to rank problem for sentences and employ a rich set of features.", "labels": [], "entities": []}, {"text": "When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge graphs area powerful tool for supporting a large spectrum of search applications including ranking, recommendation, exploratory search, and web search (.", "labels": [], "entities": []}, {"text": "A knowledge graph aggregates information around entities across multiple content sources and links these entities together, while at the same time providing entity-specific properties (such as age or employer) and types (such as actor or movie).", "labels": [], "entities": []}, {"text": "Although there is a growing interest in automatically constructing knowledge graphs, e.g., from unstructured web data (, the problem of providing evidence on why two entities are related in a knowledge graph remains largely unaddressed.", "labels": [], "entities": []}, {"text": "Extracting and presenting evidence for linking two entities, however, is an important aspect of knowledge graphs, as it can enforce trust between the user and a search engine, which in turn can improve long-term user engagement, e.g., in the context of related entity recommendation (.", "labels": [], "entities": []}, {"text": "Although knowledge graphs exist that provide this functionality to a certain degree (e.g., when hovering over Google's suggested entities, see), to the best of our knowledge there is no previously published research on methods for entity relationship explanation.", "labels": [], "entities": [{"text": "entity relationship explanation", "start_pos": 231, "end_pos": 262, "type": "TASK", "confidence": 0.6985701123873392}]}, {"text": "Figure 1: Part of Google's search result page for the query \"barack obama\".", "labels": [], "entities": []}, {"text": "When hovering over the related entity \"Michelle Obama\", an explanation of the relationship between her and \"Barack Obama\" is shown.", "labels": [], "entities": []}, {"text": "In this paper we propose a method for explaining the relationship between two entities, which we evaluate on a newly constructed annotated dataset that we make publicly available.", "labels": [], "entities": []}, {"text": "In particular, we consider the task of explaining relationships between pairs of Wikipedia entities.", "labels": [], "entities": [{"text": "explaining relationships between pairs of Wikipedia entities", "start_pos": 39, "end_pos": 99, "type": "TASK", "confidence": 0.7404835820198059}]}, {"text": "We aim to infer a human-readable description for an entity pair given a relationship between the two entities.", "labels": [], "entities": []}, {"text": "Since Wikipedia does not explicitly define relationships between entities we use a knowledge graph to obtain these relations.", "labels": [], "entities": []}, {"text": "We cast our task as a sentence ranking problem: we automatically extract sentences from a corpus and rank them according to how well they describe a given relationship between a pair of entities.", "labels": [], "entities": []}, {"text": "For ranking purposes, we extract a rich set of features and use learning to rank to effectively combine them.", "labels": [], "entities": []}, {"text": "Our feature set includes both traditional information retrieval and natural language processing features that we augment with entity-dependent features.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.6985562890768051}]}, {"text": "These features leverage information from the structure of the knowledge graph.", "labels": [], "entities": []}, {"text": "On top of this, we use features that capture the presence in a sentence of the relationship of interest.", "labels": [], "entities": []}, {"text": "For our evaluation we focus on \"people\" entities and we use a large, manually annotated dataset of sentences.", "labels": [], "entities": []}, {"text": "The research questions we address are the following.", "labels": [], "entities": []}, {"text": "First, we ask what the effectiveness of state-of-the-art sentence retrieval models is for explaining a relationship between two entities (RQ1).", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.718305230140686}]}, {"text": "Second, we consider whether we can improve over sentence retrieval models by casting the task in a learning to rank framework (RQ2).", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.6973954737186432}]}, {"text": "Third, we examine whether we can further improve performance by using relationship-dependent models instead of a relationship-independent one (RQ3).", "labels": [], "entities": []}, {"text": "We complement these research questions with an error and feature analysis.", "labels": [], "entities": []}, {"text": "Our main contributions area robust and effective method for explaining entity relationships, detailed insights into the performance of our method and features, and a manually annotated dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the dataset, manual annotations, learning to rank algorithm, and evaluation metrics that we use to answer our research questions.", "labels": [], "entities": []}, {"text": "We draw entities and their relationships from a proprietary knowledge graph that is created from Wikipedia, Freebase, IMDB, and other sources, and that is used by the Yahoo web search engine.", "labels": [], "entities": []}, {"text": "We focus on \"people\" entities and relationships between them.", "labels": [], "entities": []}, {"text": "For our experiments we need to select a manageable set of entities, which we obtain as follows.", "labels": [], "entities": []}, {"text": "We consider a year of query logs from a large commercial search engine, count the number of times a user clicks on a Wikipedia article of an entity in the results page and perform stratified sampling of entities according to this distribution.", "labels": [], "entities": []}, {"text": "As we are bounded by limited resources for our manual assessments, we sample 1 476 entity pairs that together with nine unique relationship types form our experimental dataset.", "labels": [], "entities": []}, {"text": "We use an English Wikipedia dump dated July 8, 2013, containing approximately 4M articles, of which 50 638 belong to \"people\" entities that are also in our knowledge graph.", "labels": [], "entities": [{"text": "English Wikipedia dump dated July 8", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.9161060154438019}]}, {"text": "We extract sentences using the approach described in Section 4.1, resulting in 36 823 candidate sentences for our entities.", "labels": [], "entities": []}, {"text": "On average we have 24.94 sentences per entity pair (maximum 423 and minimum 0).", "labels": [], "entities": []}, {"text": "Because of the large variance, it is not feasible to obtain exhaustive annotations for all sentences.", "labels": [], "entities": []}, {"text": "We rank the sentences using R-TFISF and keep the top-10 sentences per entity pair for annotation.", "labels": [], "entities": []}, {"text": "This results in a total of 5 689 sentences.", "labels": [], "entities": []}, {"text": "Five human annotators provided relevance judgments, manually judging sentences based on how well they describe the relationship for an entity pair, for which we use a five-level graded relevance scale (perfect, excellent, good, fair, bad).", "labels": [], "entities": []}, {"text": "Of all relevance grades 8.1% is perfect, 15.69% excellent, 19.98% good, 8.05% fair, and 48.15% bad.", "labels": [], "entities": [{"text": "perfect", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9949746131896973}]}, {"text": "Out of 1 476 entity pairs, 1 093 have at least one sentence annotated as fair.", "labels": [], "entities": []}, {"text": "As is common in information retrieval evaluation, we discard entity pairs that have only \"bad\" sentences.", "labels": [], "entities": [{"text": "information retrieval evaluation", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.8460770646731058}]}, {"text": "We examine the difficulty of the task for human annotators by measuring inter-annotator agreement on a subset of 105 sentences that are judged by 3 annotators.", "labels": [], "entities": []}, {"text": "Fleiss' kappa is k = 0.449, which is considered to be moderate agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9578643441200256}]}, {"text": "We employ two main evaluation metrics in our experiments, NDCG) and ERR ().", "labels": [], "entities": [{"text": "ERR", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9839291572570801}]}, {"text": "The former measures the total accumulated gain from the top of the ranking that is discounted at lower ranks and is normalized by the ideal cumulative gain.", "labels": [], "entities": []}, {"text": "The latter models user behavior and measures the expected reciprocal rank at which a user will stop her search.", "labels": [], "entities": []}, {"text": "We consider these rankingbased graded evaluation metrics at two cut-off points: position 1, corresponding to showing a single sentence to a user, and 10, which accounts for users who might look at more results.", "labels": [], "entities": []}, {"text": "We report on NDCG@1, NDCG@10, ERR@1, ERR@10, and Exc@1, which indicates whether we have an \"excellent\" or \"perfect\" sentence at the top of the ranking.", "labels": [], "entities": [{"text": "ERR", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9228523373603821}]}, {"text": "Likewise, Per@1 indicates whether we have a \"perfect\" sentence at the top of the ranking (not all entity pairs have an excellent or a perfect sentence).", "labels": [], "entities": [{"text": "Per@1", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.895191470781962}]}, {"text": "We perform 5-fold cross validation and test for statistical significance using a paired two-tailed ttest.", "labels": [], "entities": []}, {"text": "We depict a significant difference in performance for p < 0.01 with (gain) and (loss) and for p < 0.05 with (gain) and (loss).", "labels": [], "entities": []}, {"text": "Boldface indicates the best score fora metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for five baseline variants. See text  for their description and significant differences.", "labels": [], "entities": []}, {"text": " Table 3: Results for the best baseline (B5) and the learning to rank method (LTR).", "labels": [], "entities": [{"text": "learning to rank method (LTR)", "start_pos": 53, "end_pos": 82, "type": "METRIC", "confidence": 0.5680313663823264}]}, {"text": " Table 4: Results for the best baseline (B5) and the learning to rank method (LTR), using all entity pairs  in the dataset, including those without any relevant sentences.", "labels": [], "entities": []}, {"text": " Table 5: Results for relationship-dependent models. Similar relationships are grouped together.", "labels": [], "entities": []}, {"text": " Table 6: Results using relationship-dependent  models, removing individual feature types.", "labels": [], "entities": []}]}