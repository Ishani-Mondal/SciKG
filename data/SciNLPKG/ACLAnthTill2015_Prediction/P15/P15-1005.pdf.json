{"title": [{"text": "Describing Images using Inferred Visual Dependency Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "The Visual Dependency Representation (VDR) is an explicit model of the spatial relationships between objects in an image.", "labels": [], "entities": [{"text": "Visual Dependency Representation (VDR)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7716787755489349}]}, {"text": "In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work.", "labels": [], "entities": [{"text": "VDR Parsing", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.4635133147239685}]}, {"text": "Our approach is to find the objects mentioned in a given description using a state-of-the-art object detector , and to use successful detections to produce training data.", "labels": [], "entities": []}, {"text": "The description of an unseen image is produced by first predicting its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR.", "labels": [], "entities": []}, {"text": "The performance of our approach is comparable to a state-of-the-art multimodal deep neural network in images depicting actions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Humans typically write the text accompanying an image, which is a time-consuming and expensive activity.", "labels": [], "entities": []}, {"text": "There are many circumstances in which people are well-suited to this task, such as captioning news articles where there are complex relationships between the modalities.", "labels": [], "entities": [{"text": "captioning news articles", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.9164857069651285}]}, {"text": "In this paper we focus on generating literal descriptions, which are rarely found alongside images because they describe what can easily be seen by others.", "labels": [], "entities": []}, {"text": "A computer that can automatically generate these literal descriptions, filling the gap left by humans, may improve access to existing image collections or increase information access for visually impaired users.", "labels": [], "entities": []}, {"text": "There has been an upsurge of research in this area, including models that rely on spatial relationships (, corpus-based relationships), spatial and visual attributes (), n-gram phrase fusion from Web-scale corpora), treesubstitution grammars (, selecting and combining phrases from large imagedescription collections (), using Visual Dependency Representations to capture spatial and corpus-based relationships, and in a generative framework over densely-labelled data).", "labels": [], "entities": []}, {"text": "The most recent developments have focused on deep learning the relationships between visual feature vectors and word-embeddings with language generation models based on recurrent neural networks or long-short term memory networks ().", "labels": [], "entities": []}, {"text": "An alternative thread of research has focused on directly pairing images with text, based on kCCA () or multimodal deep neural networks).", "labels": [], "entities": []}, {"text": "We revisit the Visual Dependency Representation, an intermediate structure that captures the spatial relationships between objects in an image.", "labels": [], "entities": []}, {"text": "Spatial context has been shown to be useful in object recognition and naming tasks because humans benefit from the visual world conforming to their expectations.", "labels": [], "entities": [{"text": "Spatial context", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.868405818939209}, {"text": "object recognition and naming tasks", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.7317645788192749}]}, {"text": "The spatial relationships defined in VDR are closely, but independently, related to cognitively plausible spatial templates and region connection calculus ().", "labels": [], "entities": []}, {"text": "In the image description task, explicitly modelling the spatial relationships between observed objects constrains how an image should be described.", "labels": [], "entities": [{"text": "image description task", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.7800562381744385}]}, {"text": "An example can be seen in, where the training VDR identifies the defining relationship between the man and the laptop, which maybe re-", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the generated descriptions using sentence-level) and BLEU4 (, which have been shown to have moderate correlation with humans ).", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9983687996864319}]}, {"text": "We adopt a jack-knifing evaluation methodology, which enables us to report human-human results (), using MultEval (Clark et al., 2011).", "labels": [], "entities": [{"text": "MultEval (Clark et al., 2011)", "start_pos": 105, "end_pos": 134, "type": "DATASET", "confidence": 0.7288074232637882}]}], "tableCaptions": [{"text": " Table 2: Sentence-level evaluation of the gen- erated descriptions.  VDR is comparable to  BRNN when the images exclusively depict actions  (VLT2K). In a more diverse data set, BRNN gener- ates better descriptions (Pascal1K).", "labels": [], "entities": [{"text": "VDR", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.6163644194602966}]}, {"text": " Table 3: Sentence-level scores when transferring  models directly between data sets with no retrain- ing. The VDR-based approach generates better de- scriptions in the Pascal1K data set if we transfer  the model from the VLT2K data set.", "labels": [], "entities": [{"text": "Pascal1K data set", "start_pos": 169, "end_pos": 186, "type": "DATASET", "confidence": 0.9455793102582296}, {"text": "VLT2K data set", "start_pos": 222, "end_pos": 236, "type": "DATASET", "confidence": 0.9540745417277018}]}]}