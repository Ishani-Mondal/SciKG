{"title": [{"text": "Training a Natural Language Generator From Unaligned Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel syntax-based natural language generation system that is train-able from unaligned pairs of input meaning representations and output sentences.", "labels": [], "entities": []}, {"text": "It is divided into sentence planning, which incrementally builds deep-syntactic dependency trees, and surface realization.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7068924307823181}, {"text": "surface realization", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7947818040847778}]}, {"text": "Sentence planner is based on A* search with a perceptron ranker that uses novel differing subtree updates and a simple future promise estimation; surface realization uses a rule-based pipeline from the Treex NLP toolkit.", "labels": [], "entities": [{"text": "Sentence planner", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9301574230194092}, {"text": "surface realization", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7332238554954529}, {"text": "Treex NLP toolkit", "start_pos": 202, "end_pos": 219, "type": "DATASET", "confidence": 0.9452289740244547}]}, {"text": "Our first results show that training from unaligned data is feasible, the outputs of our generator are mostly fluent and relevant .", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a novel approach to natural language generation (NLG) that does not require finegrained alignment in training data and uses deep dependency syntax for sentence plans.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.8169776995976766}]}, {"text": "We include our first results on the BAGEL restaurant recommendation data set of.", "labels": [], "entities": [{"text": "BAGEL restaurant recommendation data set", "start_pos": 36, "end_pos": 76, "type": "DATASET", "confidence": 0.7591486573219299}]}, {"text": "In our setting, the task of a natural language generator is that of converting an abstract meaning representation (MR) into a natural language utterance.", "labels": [], "entities": [{"text": "converting an abstract meaning representation (MR) into a natural language utterance", "start_pos": 68, "end_pos": 152, "type": "TASK", "confidence": 0.717305275110098}]}, {"text": "This corresponds to the sentence planning and surface realization NLG stages as described by.", "labels": [], "entities": [{"text": "sentence planning and surface realization NLG", "start_pos": 24, "end_pos": 69, "type": "TASK", "confidence": 0.7583113809426626}]}, {"text": "It also reflects the intended usage in a spoken dialogue system (SDS), where the NLG component is supposed to translate a system output action into a sentence.", "labels": [], "entities": [{"text": "spoken dialogue system (SDS)", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7046914100646973}]}, {"text": "While the content planning NLG stage has been used in SDS (e.g.,), we believe that deciding upon the contents of the system's utterance is generally a task for the dialogue manager.", "labels": [], "entities": [{"text": "SDS", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9629704356193542}]}, {"text": "We focus mainly on the sentence planning part in this work, and reuse an existing rule-based surface realizer to test the capabilities of the generator in an end-to-end setting.", "labels": [], "entities": []}, {"text": "Current NLG systems usually require a separate training data alignment step ().", "labels": [], "entities": [{"text": "training data alignment", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.6539299289385477}]}, {"text": "Many of them use a CFG or operate in a phrase-based fashion (), which limits their ability to capture long-range syntactic dependencies.", "labels": [], "entities": []}, {"text": "Our generator includes alignment learning into sentence planner training and uses deep-syntactic trees with a rule-based surface realization step, which ensures grammatical correctness of the outputs.", "labels": [], "entities": [{"text": "alignment learning", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.929010808467865}, {"text": "sentence planner training", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.786756287018458}]}, {"text": "Unlike previous approaches to trainable sentence planning (e.g.,;), our generator does not require a handcrafted base sentence planner.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7429746389389038}]}, {"text": "This paper is structured as follows: in Section 2, we describe the architecture of our generator.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 then provide further details on its main components.", "labels": [], "entities": []}, {"text": "In Section 5, we describe our experiments on the BAGEL data set, followed by an analysis of the results in Section 6.", "labels": [], "entities": [{"text": "BAGEL data set", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9297515551249186}]}, {"text": "Section 7 compares our generator to previous related works and Section 8 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we describe the data set used in our experiments, the needed preprocessing steps, and the settings of our generator specific to the data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation on the BAGEL data set (averaged over all ten cross-validation folds)", "labels": [], "entities": [{"text": "BAGEL data set", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8788801630338033}]}]}