{"title": [{"text": "Context-Dependent Translation Selection Using Convolutional Neural Network", "labels": [], "entities": [{"text": "Context-Dependent Translation Selection", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8052611152331034}]}], "abstractContent": [{"text": "We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages.", "labels": [], "entities": [{"text": "translation selection", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.9637756645679474}, {"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6504339575767517}]}, {"text": "The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language.", "labels": [], "entities": []}, {"text": "Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs.", "labels": [], "entities": []}, {"text": "We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrases and sentence-level contexts by using training examples from easy to difficult.", "labels": [], "entities": []}, {"text": "Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9990806579589844}]}], "introductionContent": [{"text": "Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (, which often fail to capture translation pairs which are grammatically and semantically similar.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.7973432888587316}]}, {"text": "To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space ().", "labels": [], "entities": []}, {"text": "The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have two baseline systems: \u2022 Baseline: The baseline system is an opensource system of the phrase-based modelMoses () with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model.", "labels": [], "entities": []}, {"text": "\u2022 CICM (context-independent convolutional matching) model: Following the previous works (), we calculate the matching degree of a phrase pair without considering any contextual information.", "labels": [], "entities": [{"text": "context-independent convolutional matching)", "start_pos": 8, "end_pos": 51, "type": "TASK", "confidence": 0.7443151324987411}]}, {"text": "Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative example.", "labels": [], "entities": []}, {"text": "The matching score is also introduced into Baseline as an additional feature.", "labels": [], "entities": [{"text": "matching score", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.8843715786933899}]}, {"text": "summaries the results of CDCMs trained from different curriculums.", "labels": [], "entities": []}, {"text": "No matter from which curriculum it is trained, the CDCM model significantly improves the translation quality on the overall test data (with gains of 1.0 BLEU points).", "labels": [], "entities": [{"text": "translation", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9413275122642517}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9983015060424805}]}, {"text": "The best improvement can be up to 1.4 BLEU points on MT04 with the fully trained CDCM.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9995362758636475}, {"text": "MT04", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8820706009864807}, {"text": "CDCM", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9365464448928833}]}, {"text": "As expected, the translation performance is consistently increased with curriculum growing.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9734879732131958}]}, {"text": "This indicates that the CDCM model indeed captures the desirable semantic information by the curriculum learning from easy to difficult.", "labels": [], "entities": []}, {"text": "Comparing with its context-independent counterpart (CICM, Row 2), the CDCM model shows significant improvement on all the test data consistently.", "labels": [], "entities": []}, {"text": "We contribute this to the incorporation of useful discriminative information embedded in the local context.", "labels": [], "entities": []}, {"text": "In addition, the performance of CICM is comparable with that of CDCM 1 . This is intuitive, because both of them try to capture the basic semantic similarity between the source and target phrase pair.", "labels": [], "entities": []}, {"text": "One of the hypotheses we tested in the course of this research was disproved.", "labels": [], "entities": []}, {"text": "We thought it likely that the difficult curriculum (CDCM 3 that distinguishs the correct translation from other candidates fora given context) would contribute most to the improvement, since this circumstance is more consistent with the real decoding procedure.", "labels": [], "entities": []}, {"text": "This turned out to be false, as shown in.", "labels": [], "entities": []}, {"text": "One possible reason is that the \"negative\" examples (other candidates for the same source phrase) may share the same semantic meaning with the positive one, thus give a wrong guide in the supervised training.", "labels": [], "entities": []}, {"text": "Constructing a reasonable set of negative examples that are more semantically different from the positive one is left for our future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of translation quality.  CDCM k denotes the CDCM model trained in the  kth curriculum in Alg. 1 (i.e., three levels of  curriculum training), CICM denotes its context- independent counterpart, and \"All\" is the com- bined test sets. The superscripts \u03b1 and \u03b2 indicate  statistically significant difference (p < 0.05) from  Baseline and CICM, respectively.", "labels": [], "entities": [{"text": "Alg. 1", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.8820432424545288}]}]}