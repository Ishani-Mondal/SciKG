{"title": [{"text": "The Impact of Listener Gaze on Predicting Reference Resolution", "labels": [], "entities": [{"text": "Predicting Reference Resolution", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.9832956393559774}]}], "abstractContent": [{"text": "We investigate the impact of listener's gaze on predicting reference resolution in situated interactions.", "labels": [], "entities": [{"text": "predicting reference resolution", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.8707219163576762}]}, {"text": "We extend an existing model that predicts to which entity in the environment listeners will resolve a referring expression (RE).", "labels": [], "entities": []}, {"text": "Our model makes use of features that capture which objects were looked at and for how long, reflecting listeners' visual behavior.", "labels": [], "entities": []}, {"text": "We improve a probabilistic model that considers a basic set of features for monitoring listeners' movements in a virtual environment.", "labels": [], "entities": []}, {"text": "Particularly, in complex referential scenes, where more objects next to the target are possible referents, gaze turns out to be beneficial and helps deciphering listen-ers' intention.", "labels": [], "entities": []}, {"text": "We evaluate performance at several prediction times before the listener performs an action, obtaining a highly significant accuracy gain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.998995840549469}]}], "introductionContent": [{"text": "Speakers tend to follow the listener's behavior in order to determine whether their communicated message was received and understood.", "labels": [], "entities": []}, {"text": "This phenomenon is known as grounding, it is well established in the dialogue literature, and it plays an important role in collaborative tasks and goal-oriented conversations.", "labels": [], "entities": []}, {"text": "Solving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels.", "labels": [], "entities": []}, {"text": "In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available.", "labels": [], "entities": []}, {"text": "While  studied instruction giving and following in virtual environments, examined pedestrian guidance in outdoor real environments.", "labels": [], "entities": [{"text": "instruction giving and following", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.6711481511592865}, {"text": "pedestrian guidance", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7272647023200989}]}, {"text": "Both studies investigate the interaction of human interlocutors but neither study exploits listeners' eye movements.", "labels": [], "entities": []}, {"text": "In contrast,  designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environment whose eye movements were tracked.", "labels": [], "entities": []}, {"text": "They outperformed similar systems in both successful reference resolution and listener confusion.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8374119699001312}]}, {"text": "attempted to predict the resolution of an RE, achieving good performance by combining two probabilistic log-linear models: a semantic model P sem that analyzes the semantics of a given instruction, and an observational model P obs that inspects the player's behavior.", "labels": [], "entities": [{"text": "predict the resolution of an RE", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.7099840641021729}]}, {"text": "However, they did not include listener's gaze.", "labels": [], "entities": []}, {"text": "They observed that the accuracy for P obs reaches its highest point at a relatively late stage in an interaction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9997261166572571}]}, {"text": "Similar observations are reported by: they compare listener gaze and an incremental update model (IUM) as predictors for the resolution of an RE, noting that gaze is more accurate before the onset of an utterance, whereas the model itself is more accurate afterwards.", "labels": [], "entities": [{"text": "incremental update model (IUM)", "start_pos": 72, "end_pos": 102, "type": "METRIC", "confidence": 0.7460594872633616}, {"text": "resolution of an RE", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8395912945270538}]}, {"text": "In this paper we report on the extension of the P obs model to also consider listener's visual behaviour.", "labels": [], "entities": []}, {"text": "More precisely we implement features that encode listener's eye movement patterns and evaluate their performance on a multi-modal data collection.", "labels": [], "entities": []}, {"text": "We show that such a model as it takes an additional communication channel provides more accurate predictions especially when dealing with complex scenes.", "labels": [], "entities": []}, {"text": "We also expand on concepts from the IUM, by applying the conclusions drawn from its behaviour to a dynamic task with a naturalistic interactive scenario.", "labels": [], "entities": []}], "datasetContent": [{"text": "The accuracy of our probabilistic models depends on the parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994157552719116}]}, {"text": "At different stages of an interaction the difficulty to predict an intended target varies as the visual context changes and in particular the number of visible objects.", "labels": [], "entities": []}, {"text": "As the weights of the features are optimized at time \u2212d train , it would be expected that testing also at time \u2212d test = \u2212d train yields the highest accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9977523684501648}]}, {"text": "However, the difficulty to make a prediction decreases as tb \u2212 d test approaches tb , i.e. as the player moves towards the intended target.", "labels": [], "entities": []}, {"text": "We expect that testing at \u2212d train works best, but we need to be able to update continuously.", "labels": [], "entities": []}, {"text": "Thus we also evaluate at other timepoints and test several combinations of the (d train , d test ) parameters.", "labels": [], "entities": []}, {"text": "Given the limited amount of eye-tracking data available in our corpus, we replaced the crosscorpora-challenge test setting from the original P obs study with a tenfold cross validation setup.", "labels": [], "entities": []}, {"text": "As training and testing were performed over instances of a certain minimum length according to, we first removed all instances with length less than max(d train , d test ), and then perform the cross validation split.", "labels": [], "entities": []}, {"text": "In this way we ensure that the number of instances in the folds are not unbalanced.", "labels": [], "entities": []}, {"text": "Moreover, each instance was classified as easy or hard depending on the number of visible objects at time tb . An instance was considered easy if no more than three objects were visible at that point, or hard otherwise.", "labels": [], "entities": []}, {"text": "For \u2212d test = 0, 59.5% of all instances are considered hard, but this proportion decreases as \u2212d test increases.", "labels": [], "entities": []}, {"text": "At \u2212d test = \u22126, the number of hard instances amounts to 72.7%.", "labels": [], "entities": []}, {"text": "We evaluated both the original P obs model and the P Eobs model on the same data set.", "labels": [], "entities": []}, {"text": "We also calculated accuracy values for each feature function, in order to test whether a single function could outperform P obs . We included as baselines two versions of P obs using only the features InRoom and Visual Salience proposed by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9988725781440735}]}, {"text": "The accuracy results on show our observations for \u22126 \u2264 \u2212d train \u2264 \u22122 and \u2212d train \u2264 \u2212d test \u2264 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996997117996216}]}, {"text": "The graph shows that P Eobs performs similarly as P obs on the easy instances, i.e. the eye-tracking features are not contributing in those scenarios.", "labels": [], "entities": []}, {"text": "However, P Eobs shows a consistent improvement on the hard instances over P obs . For each permutation of the training and testing parameters (d train , d test ), we obtain a set of episodes that fulfil the length criteria for the given parameters.", "labels": [], "entities": []}, {"text": "We apply P obs and P Eobs on the obtained set of instances and measure two corresponding accuracy values.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.999146580696106}]}, {"text": "We compared the accuracy values of P obs and P Eobs overall 25 different (d train , d test ) pairs, using a paired samples ttest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.999446451663971}]}, {"text": "The test indicated that the P Eobs performance (M = 83.72, SD = 3.56) is significantly better than the P obs performance (M = 79.33, SD = 3.89), (t(24) = 9.51, p < .001, Cohen s d = 1.17).", "labels": [], "entities": [{"text": "P Eobs performance", "start_pos": 28, "end_pos": 46, "type": "METRIC", "confidence": 0.7464989622433981}]}, {"text": "Thus eye-tracking features seem to be particularly helpful for predicting to which entity an RE is resolved in hard scenes.", "labels": [], "entities": [{"text": "predicting to which entity an RE", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.5357765704393387}]}, {"text": "The results also show a peak inaccuracy near the -3 seconds mark.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9735174179077148}]}, {"text": "We computed a 2x2 contingency table that contrasts correct and incorrect predictions for P obs and P Eobs , i.e. whether oi was classified as target objector not.", "labels": [], "entities": []}, {"text": "Data for this table was collected from all episode judgements for models trained at times in the [\u22126 sec., \u22123 sec.] range and tested at -3 seconds.", "labels": [], "entities": []}, {"text": "McNemar's test showed that the marginal row and column frequencies are significantly different (p < 0.05).", "labels": [], "entities": []}, {"text": "This peak is related to the average required time between an utterance and the resulting target manipulation.", "labels": [], "entities": []}, {"text": "This result shows that our model is more accurate precisely at points in time when we expect fixations to a target object.", "labels": [], "entities": []}], "tableCaptions": []}