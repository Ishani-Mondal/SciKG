{"title": [], "abstractContent": [{"text": "Meaning of a word varies from one domain to another.", "labels": [], "entities": [{"text": "Meaning", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8276386857032776}]}, {"text": "Despite this important domain dependence in word semantics , existing word representation learning methods are bound to a single domain.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.7331258952617645}]}, {"text": "Given a pair of source-target domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domain-specific aspects of word semantics.", "labels": [], "entities": []}, {"text": "First, we select a subset of frequent words that occur in both domains as pivots.", "labels": [], "entities": []}, {"text": "Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains.", "labels": [], "entities": []}, {"text": "Moreover, we propose a method to perform domain adaptation using the learnt word representations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7750106751918793}]}, {"text": "Our proposed method significantly outperforms competitive baselines including the state-of-the-art domain-insensitive word representations , and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.8145408630371094}]}], "introductionContent": [{"text": "Learning semantic representations for words is a fundamental task in NLP that is required in numerous higher-level NLP applications).", "labels": [], "entities": [{"text": "Learning semantic representations for words", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7832482099533081}]}, {"text": "Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words ().", "labels": [], "entities": [{"text": "Distributed word representations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8241406083106995}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.995162844657898}]}, {"text": "However, the meaning of a word often varies from one domain to another.", "labels": [], "entities": []}, {"text": "For example, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute fora portable electronic device.", "labels": [], "entities": []}, {"text": "However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight ().", "labels": [], "entities": []}, {"text": "However, existing word representation learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.7833849688371023}]}, {"text": "To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs.", "labels": [], "entities": []}, {"text": "Despite the successful applications of distributed word representation learning methods () most existing approaches are limited to learning only a single representation fora given word.", "labels": [], "entities": [{"text": "distributed word representation learning", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.6843355223536491}]}, {"text": "Although there have been some work on learning multiple prototype representations () fora word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used.", "labels": [], "entities": []}, {"text": "If we can learn separate representations fora word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (), cross-domain POS tagging (, crossdomain dependency parsing (, and domain adaptation of relation extractors (.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 161, "end_pos": 198, "type": "TASK", "confidence": 0.7199999888737997}, {"text": "POS tagging", "start_pos": 216, "end_pos": 227, "type": "TASK", "confidence": 0.7845194041728973}, {"text": "crossdomain dependency parsing", "start_pos": 231, "end_pos": 261, "type": "TASK", "confidence": 0.7527191837628683}, {"text": "domain adaptation of relation extractors", "start_pos": 269, "end_pos": 309, "type": "TASK", "confidence": 0.8034119725227356}]}, {"text": "We introduce the cross-domain word represen-tation learning task, where given two domains, (referred to as the source (S) and the target (T )) the goal is to learn two separate representations w Sand w T fora word w respectively from the source and the target domain that capture domainspecific semantic variations of w.", "labels": [], "entities": [{"text": "cross-domain word represen-tation learning", "start_pos": 17, "end_pos": 59, "type": "TASK", "confidence": 0.6967843621969223}]}, {"text": "In this paper, we use the term domain to represent a collection of documents related to a particular topic such as user-reviews in Amazon fora product category (e.g. books, dvds, movies, etc.).", "labels": [], "entities": []}, {"text": "However, a domain in general can be afield of study (e.g. biology, computer science, law, etc.) or even an entire source of information (e.g. twitter, blogs, news articles, etc.).", "labels": [], "entities": []}, {"text": "In particular, we do not assume the availability of any labeled data for learning word representations.", "labels": [], "entities": [{"text": "learning word representations", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.6144997874895731}]}, {"text": "This problem setting is closely related to unsupervised domain adaptation), which has found numerous useful applications such as, sentiment classification and POS tagging.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7710919082164764}, {"text": "sentiment classification", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.9653425216674805}, {"text": "POS tagging", "start_pos": 159, "end_pos": 170, "type": "TASK", "confidence": 0.8140082359313965}]}, {"text": "For example, in unsupervised cross-domain sentiment classification, we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained classifier to predict sentiment of the target domain's user reviews.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.7907359600067139}]}, {"text": "Although the distinction between the source and the target domains is not important for the word representation learning step, it is important for the domain adaptation tasks in which we subsequently evaluate the learnt word representations.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.7817868590354919}]}, {"text": "Following prior work on domain adaptation (), high-frequent features (unigrams/bigrams) common to both domains are referred to as domain-independent features or pivots.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7264338880777359}]}, {"text": "In contrast, we use non-pivots to refer to features that are specific to a single domain.", "labels": [], "entities": []}, {"text": "We propose an unsupervised cross-domain word representation learning method that jointly optimizes two criteria: (a) given a document d from the source or the target domain, we must accurately predict the non-pivots that occur ind using the pivots that occur ind, and (b) the source and target domain representations we learn for pivots must be similar.", "labels": [], "entities": [{"text": "cross-domain word representation learning", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.7001562491059303}]}, {"text": "The main challenge in domain adaptation is feature mismatch, where the features that we use for training a classifier in the source domain do not necessarily occur in the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7282886058092117}]}, {"text": "Consequently, prior work on domain adaptation () learn lower-dimensional mappings from non-pivots to pivots, thereby overcoming the feature mismatch problem.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7381410598754883}]}, {"text": "Criteria (a) ensures that word representations for domain-specific non-pivots in each domain are related to the word representations for domain-independent pivots.", "labels": [], "entities": []}, {"text": "This relationship enables us to discover pivots that are similar to target domain-specific non-pivots, thereby overcoming the feature mismatch problem.", "labels": [], "entities": []}, {"text": "On the other hand, criteria (b) captures the prior knowledge that high-frequent words common to two domains often represent domain-independent semantics.", "labels": [], "entities": []}, {"text": "For example, in sentiment classification, words such as excellent or terrible would express similar sentiment about a product irrespective of the domain.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.9508179426193237}]}, {"text": "However, if a pivot expresses different semantics in source and the target domains, then it will be surrounded by dissimilar sets of non-pivots, and reflected in the first criteria.", "labels": [], "entities": []}, {"text": "Criteria (b) can also be seen as a regularization constraint imposed on word representations to prevent overfitting by reducing the number of free parameters in the model.", "labels": [], "entities": []}, {"text": "Our contributions in this paper can be summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose a distributed word representation learning method that learns separate representations fora word for each domain in which it occurs.", "labels": [], "entities": [{"text": "distributed word representation learning", "start_pos": 15, "end_pos": 55, "type": "TASK", "confidence": 0.6883895769715309}]}, {"text": "To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method.", "labels": [], "entities": [{"text": "distributed word representation learning", "start_pos": 70, "end_pos": 110, "type": "TASK", "confidence": 0.6592060253024101}]}, {"text": "\u2022 Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier.", "labels": [], "entities": [{"text": "cross-domain sentiment classifier", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.6772447029749552}]}, {"text": "Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper-form a state-of-the-art domain-insensitive word representation learning method (, and several competitive baselines.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8077636361122131}]}, {"text": "In particular, our proposed cross-domain word representation learning method is not specific to a particular task such as sentiment classification, and in principle, can be in applied to a wide-range of domain adaptation tasks.", "labels": [], "entities": [{"text": "cross-domain word representation learning", "start_pos": 28, "end_pos": 69, "type": "TASK", "confidence": 0.6924108788371086}, {"text": "sentiment classification", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.9419769048690796}]}, {"text": "Despite this taskindependent nature of the proposed method, it achieves the best sentiment classification accuracies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.8257670998573303}, {"text": "cross-domain sentiment classification", "start_pos": 226, "end_pos": 263, "type": "TASK", "confidence": 0.7232715686162313}]}], "datasetContent": [{"text": "For train and evaluation purposes, we use the Amazon product reviews collected by for the four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K).", "labels": [], "entities": [{"text": "Amazon product reviews collected", "start_pos": 46, "end_pos": 78, "type": "DATASET", "confidence": 0.8810826241970062}]}, {"text": "There are 1000 positive and 1000 negative sentiment labeled reviews for each domain.", "labels": [], "entities": []}, {"text": "Moreover, each domain has on average 17, 547 unlabeled reviews.", "labels": [], "entities": []}, {"text": "We use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data, and the rest (200+200) for testing.", "labels": [], "entities": []}, {"text": "For validation purposes we use movie (source) and computer (target) domains, which were also collected by, but not part of the train/test domains.", "labels": [], "entities": []}, {"text": "Experiments conducted using this validation dataset revealed that the performance of the proposed method is relatively insensitive to the value of the regularization parameter \u03bb \u2208 [10 \u22123 , 10 3 ].", "labels": [], "entities": []}, {"text": "For the non-pivot prediction task we generate positive and negative instances using the procedure described in Section 3.2.", "labels": [], "entities": []}, {"text": "As atypical example, we have 88, 494 train instances from the books source domain and 141, 756 train instances from the target domain (1:5 ratio between positive and negative instances in each domain).", "labels": [], "entities": []}, {"text": "The number of pivots and non-pivots are set to NP = NS = NT = 500.", "labels": [], "entities": []}, {"text": "In, we compare the proposed method against two baselines (NA, InDomain), current state-of-the-art methods for unsupervised crossdomain sentiment classification (SFA, SCL), word representation learning (GloVe), and crossdomain similarity prediction (CS).", "labels": [], "entities": [{"text": "crossdomain sentiment classification", "start_pos": 123, "end_pos": 159, "type": "TASK", "confidence": 0.6901529928048452}, {"text": "word representation learning", "start_pos": 172, "end_pos": 200, "type": "TASK", "confidence": 0.7975078225135803}, {"text": "crossdomain similarity prediction (CS)", "start_pos": 214, "end_pos": 252, "type": "TASK", "confidence": 0.6871949732303619}]}, {"text": "The NA (noadapt) lower baseline uses a classifier trained on source labeled data to classify target test data without any domain adaptation.", "labels": [], "entities": []}, {"text": "The InDomain baseline is trained using the labeled data for the target domain, and simulates the performance we can expect to obtain if target domain labeled data were available.", "labels": [], "entities": [{"text": "InDomain baseline", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7570537328720093}]}, {"text": "Spectral Feature Alignment (SFA) ( and Structural Correspondence Learning (SCL) are the state-ofthe-art methods for cross-domain sentiment classification.", "labels": [], "entities": [{"text": "Spectral Feature Alignment (SFA)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8582954605420431}, {"text": "cross-domain sentiment classification", "start_pos": 116, "end_pos": 153, "type": "TASK", "confidence": 0.7718912959098816}]}, {"text": "However, those methods do not learn word representations.", "labels": [], "entities": []}, {"text": "We use Global Vector Prediction (GloVe), the current state-of-theart word representation learning method, to learn word representations separately from the source and target domain unlabeled data, and use the learnt representations in Eq.", "labels": [], "entities": []}, {"text": "In contrast to the joint word representations learnt by the proposed method, GloVe simulates the level of performance we would obtain by learning representations independently.", "labels": [], "entities": []}, {"text": "CS denotes the cross-domain vector prediction method proposed by.", "labels": [], "entities": [{"text": "cross-domain vector prediction", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6506287058194479}]}, {"text": "Although CS can be used to learn a vector-space translation matrix, it does not learn word representations.", "labels": [], "entities": [{"text": "vector-space translation matrix", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.7942990064620972}]}, {"text": "Vertical bars represent the classification accuracies (i.e. percentage of the correctly classified test instances) obtained by a particular method on target domain's test data, and Clopper-Pearson 95% binomial confidence intervals are superimposed.", "labels": [], "entities": []}, {"text": "Differences in data pre-processing (tokenization/lemmatization), selection (train/test splits), feature representation (unigram/bigram), pivot selection (MI/frequency), and the binary classification algorithms used to train the final classifier make it difficult to directly compare results published in prior work.", "labels": [], "entities": []}, {"text": "Therefore, we re-run the original algorithms on the same processed dataset under the same conditions such that any differences reported in can be directly attributable to the domain adaptation, or word-representation learning methods compared.", "labels": [], "entities": []}, {"text": "All methods use l 2 regularized logistic regression as the binary sentiment classifier, and the reg- ularization coefficients are set to their optimal values on the validation dataset.", "labels": [], "entities": []}, {"text": "SFA, SCL, and CS use the same set of 500 pivots as used by the proposed method selected using NPMI (Section 3.4).", "labels": [], "entities": [{"text": "SFA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8840073347091675}, {"text": "NPMI", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.9010598063468933}]}, {"text": "Dimensionality n of the representation is set to 300 for both GloVe and the proposed method.", "labels": [], "entities": []}, {"text": "From we see that the proposed method reports the highest classification accuracies in all 12 domain pairs.", "labels": [], "entities": []}, {"text": "Overall, the improvements of the proposed method over NA, GloVe, and CS are statistically significant, and is comparable with SFA, and SCL.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9682826399803162}]}, {"text": "The proposed method's improvement over CS shows the importance of predicting word representations instead of counting.", "labels": [], "entities": [{"text": "predicting word representations", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.8584804534912109}]}, {"text": "The improvement over GloVe shows that it is inadequate to simply apply existing word representation learning methods to learn independent word representations for the source and target domains.", "labels": [], "entities": []}, {"text": "We must consider the correspondences between the two domains as expressed by the pivots to jointly learn word representations.", "labels": [], "entities": []}, {"text": "As shown in, the proposed method reports superior accuracies over GloVe across different dimensionalities.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9901092052459717}]}, {"text": "Moreover, we see that when the dimensionality of the representations increases, initially accuracies increase in both methods and saturates after 200 \u2212 600 dimensions.", "labels": [], "entities": []}, {"text": "However, further increasing the dimensionality results in unstable and somewhat poor accuracies due to overfitting when training high-dimensional representations.", "labels": [], "entities": []}, {"text": "Although our word representations learnt by the proposed method are not specific to sentiment classification, the fact that it clearly outperforms SFA and SCL in all domain pairs is encouraging, and implies the wider-applicability of the proposed method for domain adaptation tasks beyond sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.9173277616500854}, {"text": "sentiment classification", "start_pos": 289, "end_pos": 313, "type": "TASK", "confidence": 0.8536294996738434}]}], "tableCaptions": []}