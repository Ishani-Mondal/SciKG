{"title": [{"text": "Efficient Disfluency Detection with Transition-based Parsing", "labels": [], "entities": [{"text": "Efficient Disfluency Detection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6399108668168386}, {"text": "Parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.44182291626930237}]}], "abstractContent": [{"text": "Automatic speech recognition (ASR) outputs often contain various disfluencies.", "labels": [], "entities": [{"text": "Automatic speech recognition (ASR) outputs", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8191546414579663}]}, {"text": "It is necessary to remove these disfluencies before processing downstream tasks.", "labels": [], "entities": []}, {"text": "In this paper, an efficient disfluency detection approach based on right-to-left transition-based parsing is proposed, which can efficiently identify disfluencies and keep ASR outputs grammatical.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7558209598064423}, {"text": "ASR outputs grammatical", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.9095275203386942}]}, {"text": "Our method exploits a global view to capture long-range dependencies for disfluency detection by integrating a rich set of syntactic and dis-fluency features with linear complexity.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8824032247066498}]}, {"text": "The experimental results show that our method outperforms state-of-the-art work and achieves a 85.1% f-score on the commonly used English Switchboard test set.", "labels": [], "entities": [{"text": "f-score", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9827936887741089}, {"text": "English Switchboard test set", "start_pos": 130, "end_pos": 158, "type": "DATASET", "confidence": 0.9502004832029343}]}, {"text": "We also apply our method to in-house annotated Chinese data and achieve a significantly higher f-score compared to the baseline of CRF-based approach.", "labels": [], "entities": [{"text": "f-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9968448877334595}]}], "introductionContent": [{"text": "With the development of the mobile internet, speech inputs have become more and more popular in applications where automatic speech recognition (ASR) is the key component to convert speech into text.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 115, "end_pos": 149, "type": "TASK", "confidence": 0.8019088804721832}]}, {"text": "ASR outputs often contain various disfluencies which create barriers to subsequent text processing tasks like parsing, machine translation and summarization.", "labels": [], "entities": [{"text": "ASR outputs", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9124331474304199}, {"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9552459716796875}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7850219905376434}, {"text": "summarization", "start_pos": 143, "end_pos": 156, "type": "TASK", "confidence": 0.9505846500396729}]}, {"text": "Usually, disfluencies can be classified into uncompleted words, filled pauses (e.g. \"uh\", \"um\"), discourse markers (e.g. \"I mean\"), editing terms (e.g. \"you know\") and repairs.", "labels": [], "entities": []}, {"text": "To identify and remove disfluencies, straightforward rules can be designed to tackle the former four classes of disfluencies since they often belong to a closed set.", "labels": [], "entities": []}, {"text": "However, the repair type disfluency poses particularly more difficult problems as their form is more arbitrary.", "labels": [], "entities": []}, {"text": "Typically, as shown in, a repair disfluency type consists of a reparandum (\"to Boston\") and a filled pause (\"um\"), followed by its repair (\"to Denver\").", "labels": [], "entities": [{"text": "repair disfluency", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6924811154603958}, {"text": "repair", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9628913998603821}]}, {"text": "This special structure of disfluency constraint, which exists in many languages such as English and Chinese, reflects the scenarios of spontaneous speech and conversation, where people often correct preceding words with following words when they find that the preceding words are wrong or improper.", "labels": [], "entities": []}, {"text": "This procedure might be interrupted and inserted with filled pauses when people are thinking or hesitating.", "labels": [], "entities": []}, {"text": "The challenges of detecting repair disfluencies are that reparandums vary in length, may occur everywhere, and are sometimes nested.", "labels": [], "entities": [{"text": "detecting repair disfluencies", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.7996570467948914}]}, {"text": "There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.9112304747104645}]}, {"text": "Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M 3 N) (, and prosodic features are also concerned in ().", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7885434031486511}, {"text": "sequence labeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.6790186762809753}]}, {"text": "These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances.", "labels": [], "entities": []}, {"text": "Recently, syntax-based models such as transitionbased parser have been used for detecting disflu-encies.", "labels": [], "entities": []}, {"text": "These methods can jointly perform dependency parsing and disfluency detection.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7686075568199158}, {"text": "disfluency detection", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7470535635948181}]}, {"text": "But in these methods, great efforts are made to distinguish normal words from disfluent words as decisions cannot be made imminently from left to right, leading to inefficient implementation as well as performance loss.", "labels": [], "entities": []}, {"text": "In this paper, we propose detecting disfluencies using a right-to-left transition-based dependency parsing (R2L parsing), where the words are consumed from right to left to build the parsing tree based on which the current word is predicted to be either disfluent or normal.", "labels": [], "entities": [{"text": "transition-based dependency parsing (R2L parsing)", "start_pos": 71, "end_pos": 120, "type": "TASK", "confidence": 0.7429718375205994}]}, {"text": "The proposed models cater to the disfluency constraint and integrate a rich set of features extracted from contexts of lexicons and partial syntactic tree structure, where the parsing model and disfluency predicting model are jointly calculated in a cascaded way.", "labels": [], "entities": [{"text": "disfluency predicting", "start_pos": 194, "end_pos": 215, "type": "TASK", "confidence": 0.6916740536689758}]}, {"text": "As shown in(b), while the parsing tree is being built, disfluency tags are predicted and attached to the disfluency nodes.", "labels": [], "entities": []}, {"text": "Our models are quite efficient with linear complexity of 2 * N (N is the length of input).", "labels": [], "entities": []}, {"text": "Intuitively, compared with previous syntaxbased work such as) that uses left-to-right transition-based parsing (L2R parsing) model, our proposed approach simplifies disfluency detection by sequentially processing each word, without going back to modify the pre-built tree structure of disfluency words.", "labels": [], "entities": [{"text": "left-to-right transition-based parsing (L2R parsing)", "start_pos": 72, "end_pos": 124, "type": "TASK", "confidence": 0.7756310445921761}, {"text": "disfluency detection", "start_pos": 165, "end_pos": 185, "type": "TASK", "confidence": 0.8424873054027557}]}, {"text": "As shown in(a), the L2R parsing based joint approach needs to cut the pre-built dependency link between \"did\" and \"he\" when \"was\" is identified as the repair of \"did\", which is never needed in our method as.", "labels": [], "entities": [{"text": "L2R parsing", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.46585290133953094}]}, {"text": "Furthermore, our method overcomes the deficiency issue in decoding of L2R parsing based joint method, meaning the number of parsing transitions for each hypothesis path is not identical to 2 * N , which leads to the failure of performing optimal search during decoding.", "labels": [], "entities": []}, {"text": "For example, the involvement of the extra cut operation in(a) destroys the competition scoring that accumulates over 2 * N transition actions among hypotheses in the standard transition-based parsing.", "labels": [], "entities": []}, {"text": "Although the heuristic score, such as the normalization of transition count), can be introduced, the total scores of all hypotheses are still not statistically comparable from a global view.", "labels": [], "entities": []}, {"text": "We conduct the experiments on English Switchboard corpus.", "labels": [], "entities": [{"text": "English Switchboard corpus", "start_pos": 30, "end_pos": 56, "type": "DATASET", "confidence": 0.9509025812149048}]}, {"text": "The results show that our method can achieve a 85.1% f-score with again of 0.7 point over state-of-the-art M 3 N labeling model in) and again of 1 point over state-of-the-art joint model proposed in).", "labels": [], "entities": [{"text": "f-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9912790656089783}]}, {"text": "We also apply our method on Chinese annotated data.", "labels": [], "entities": [{"text": "Chinese annotated data", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.6744181116422018}]}, {"text": "As there is no available public data in Chinese, we annotate 25k Chinese sentences manually for training and testing.", "labels": [], "entities": []}, {"text": "We achieve 71.2% f-score with 15 points gained compared to the CRF-based baseline, showing that our models are robust and language independent.", "labels": [], "entities": [{"text": "f-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9899469017982483}]}], "datasetContent": [{"text": "Our training data is the Switchboard portion of the English Penn Treebank ( corpus, which consists of telephone conversations about assigned topics.", "labels": [], "entities": [{"text": "English Penn Treebank ( corpus", "start_pos": 52, "end_pos": 82, "type": "DATASET", "confidence": 0.9600462913513184}]}, {"text": "As not all the Switchboard data has syntactic bracketing, we only use the subcorpus of PAESED/MRG/SWBD.", "labels": [], "entities": [{"text": "Switchboard data", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.857038676738739}, {"text": "PAESED", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9480563998222351}, {"text": "MRG/SWBD", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.6789489984512329}]}, {"text": "Following the experiment settings in), the training subcorpus contains directories 2 and 3 in PAESED/MRG/SWBD and directory 4 is split into test and development sets.", "labels": [], "entities": [{"text": "PAESED/MRG/SWBD", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.7659128546714783}]}, {"text": "We use the Stanford dependency converter) to get the dependency structure from the Switchboard corpus, as prove that Stanford converter is robust to the Switchboard data.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.8882476091384888}, {"text": "Switchboard data", "start_pos": 153, "end_pos": 169, "type": "DATASET", "confidence": 0.9075219333171844}]}, {"text": "For our Chinese experiments, no public Chinese corpus is available.", "labels": [], "entities": []}, {"text": "We annotate about 25k spoken sentences with only disfluency annotations according to the guideline proposed by.", "labels": [], "entities": []}, {"text": "In order to generate similar data format as English Switchboard corpus, we use Chinese dependency parsing trained on the Chinese Treebank corpus to parse the annotated data and use these parsed data for training and testing . For our Chinese experiment setting, we respectively select about 2k sentences for development and testing.", "labels": [], "entities": [{"text": "Chinese dependency parsing", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.6391719778378805}, {"text": "Chinese Treebank corpus", "start_pos": 121, "end_pos": 144, "type": "DATASET", "confidence": 0.9449657003084818}]}, {"text": "The rest are used for training.", "labels": [], "entities": []}, {"text": "To train the UT model, we create data format adaptation by replacing the original Shift and RightArc of disfluent words with Dis Shift and Dis RightArc, since they are just extensions of Shift and RightArc.", "labels": [], "entities": [{"text": "data format adaptation", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.6958346764246622}]}, {"text": "For the BCT model, disfluent words are directly depended to the root node and all their links and labels are removed.", "labels": [], "entities": [{"text": "BCT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.6828542947769165}]}, {"text": "We then link all the fluent children of disfluent words to parents of disfluent words.", "labels": [], "entities": []}, {"text": "We also remove partial words and punctuation from data to simulate speech recognizer results where such information is not available ( ).", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7001179456710815}]}, {"text": "Additionally, following, we remove all one token sentences as these sentences are trivial for disfluency detection, then lowercase the text and discard filled pauses like \"um\" and \"uh\".", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.7516542673110962}]}, {"text": "The evaluation metrics of disfluency detection are precision (Prec.), recall (Rec.) and f-score (F1).", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8269840776920319}, {"text": "precision (Prec.)", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.7685476541519165}, {"text": "recall (Rec.)", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.9352500438690186}, {"text": "f-score (F1)", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.831175908446312}]}, {"text": "For parsing accuracy metrics, we use unlabeled attachment score (UAS) and labeled attachment score (LAS).", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9717954993247986}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.802436888217926}, {"text": "unlabeled attachment score (UAS)", "start_pos": 37, "end_pos": 69, "type": "METRIC", "confidence": 0.7918349256118139}, {"text": "labeled attachment score (LAS)", "start_pos": 74, "end_pos": 104, "type": "METRIC", "confidence": 0.8945864041646322}]}, {"text": "For our primary comparison, we evaluate the widely used CRF labeling model, the state-of-the-art M 3 N model presented by which has been commonly used as baseline in previous works and the state-of-the-art L2R parsing based joint model proposed by Honnibal and Johnson (2014).", "labels": [], "entities": [{"text": "CRF labeling", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.658729612827301}]}], "tableCaptions": [{"text": " Table 2: Disfluency detection and parsing accuracies on English Switchboard data. The accuracy of  M 3 N refers to the result reported in (Qian and Liu, 2013). H&J is the L2R parsing based joint model in  (Honnibal and Johnson, 2014). The results of M 3 N  \u2020 come from the experiments with toolkit released by  Qian and Liu (2013) on our pre-processed corpus.", "labels": [], "entities": [{"text": "Disfluency detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9029576480388641}, {"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9473874568939209}, {"text": "English Switchboard data", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.8966341813405355}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9996160268783569}]}, {"text": " Table 4: Performance on different classes  of words. Dete.=determiner; Pron.=pronoun;  Conj.=conjunction; Prep.= preposition. feat.=new  disfluency features", "labels": [], "entities": [{"text": "Prep.", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9466180205345154}]}, {"text": " Table 5: Disfluency detection performance on  Chinese annotated data.", "labels": [], "entities": [{"text": "Disfluency detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8822404444217682}, {"text": "Chinese annotated data", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.7634631395339966}]}, {"text": " Table 1. The POS- tagger model that we implement for a pre-process  before parsing also uses structured perceptron for  training and can achieve a competitive accuracy of  96.7%. The beam size for both POS-tagger and  parsing is set to 5.", "labels": [], "entities": [{"text": "POS- tagger", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.6286913553873698}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9736500978469849}, {"text": "parsing", "start_pos": 219, "end_pos": 226, "type": "TASK", "confidence": 0.9621338248252869}]}, {"text": " Table 6: Performance of our parsers on different  test sets.", "labels": [], "entities": []}]}