{"title": [{"text": "Generative Incremental Dependency Parsing with Neural Networks", "labels": [], "entities": [{"text": "Generative Incremental Dependency Parsing", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7788299769163132}]}], "abstractContent": [{"text": "We propose a neural network model for scalable generative transition-based dependency parsing.", "labels": [], "entities": [{"text": "generative transition-based dependency parsing", "start_pos": 47, "end_pos": 93, "type": "TASK", "confidence": 0.8933514803647995}]}, {"text": "A probability distribution over both sentences and transition sequences is parameterised by a feed-forward neural network.", "labels": [], "entities": []}, {"text": "The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.999624490737915}, {"text": "generative dependency parsers", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.9245487252871195}, {"text": "UAS", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9023594856262207}]}, {"text": "Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.733674556016922}]}], "introductionContent": [{"text": "Transition-based dependency parsers that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy.", "labels": [], "entities": [{"text": "Transition-based dependency parsers", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.588965485493342}, {"text": "speed", "start_pos": 147, "end_pos": 152, "type": "METRIC", "confidence": 0.9889657497406006}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9881207942962646}]}, {"text": "Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance.", "labels": [], "entities": [{"text": "neural network transition-based dependency parsers", "start_pos": 9, "end_pos": 59, "type": "TASK", "confidence": 0.6360042452812195}]}, {"text": "However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically.", "labels": [], "entities": [{"text": "language understanding and generation", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.6768769547343254}]}, {"text": "Neural networks have also been shown to be powerful generative models for language modelling ( and machine translation).", "labels": [], "entities": [{"text": "language modelling", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7680512964725494}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7546440660953522}]}, {"text": "However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used.", "labels": [], "entities": []}, {"text": "In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model.", "labels": [], "entities": []}, {"text": "It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations ().", "labels": [], "entities": []}, {"text": "Generative models for graph-based dependency parsing are much less accurate than their discriminative counterparts.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.672050341963768}]}, {"text": "Syntactic language models based on PCFGs) and incremental parsing) have been proposed for speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8367094397544861}, {"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.8181290328502655}]}, {"text": "However, these models are also limited in either scalability, expressiveness, or both.", "labels": [], "entities": []}, {"text": "A generative transitionbased dependency parser based on recurrent neural networks obtains high accuracy, but training and decoding is prohibitively expensive.", "labels": [], "entities": [{"text": "generative transitionbased dependency parser", "start_pos": 2, "end_pos": 46, "type": "TASK", "confidence": 0.8035369217395782}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9991968274116516}]}, {"text": "We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam size depends on the uncertainty in the model (.", "labels": [], "entities": []}, {"text": "The model obtains 91.1% UAS on the WSJ, which is 0.2% UAS better than the previous highest accuracy generative dependency parser, while also being much more efficient.", "labels": [], "entities": [{"text": "UAS", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9986578226089478}, {"text": "WSJ", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8907902836799622}, {"text": "UAS", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9973107576370239}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9208314418792725}, {"text": "generative dependency parser", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.8331192135810852}]}, {"text": "As a language model its perplexity reaches 111.8, a 23% reduction over an ngram baseline, when combining supervised training with unsupervised fine-tuning.", "labels": [], "entities": []}, {"text": "Finally, we find that the model is able to generate sentences that display both local and syntactic coherence.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model for parsing and language modelling on the English Penn Treebank () WSJ parsing setup   Our neural network implementation is partly based on the OxLM neural language modelling framework ().", "labels": [], "entities": [{"text": "parsing and language modelling", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6336537301540375}, {"text": "English Penn Treebank", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.9626348813374838}, {"text": "WSJ parsing", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.5863179415464401}, {"text": "OxLM neural language modelling", "start_pos": 166, "end_pos": 196, "type": "TASK", "confidence": 0.6516221761703491}]}, {"text": "The model parameters are initialised randomly by drawing from a Gaussian distribution with mean 0 and variance 0.1, except for the bias weights, which are initialised by the unigram distributions of their output.", "labels": [], "entities": []}, {"text": "We use minibatches of size 128, the L2 regularization parameter is 10, and the word representation and hidden layer of size is 256.", "labels": [], "entities": []}, {"text": "The Adagrad learning rate is initialised to 0.05.", "labels": [], "entities": []}, {"text": "POS tags for the development and test sets are obtained with the Stanford POS tagger (, with 97.5% test set accuracy.", "labels": [], "entities": [{"text": "Stanford POS tagger", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.8525723417599996}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9882152080535889}]}, {"text": "Words that occur only once in the training data are treated as unknown words.", "labels": [], "entities": []}, {"text": "Unknown words are replaced by tokens representing morphological surface features (based on capitalization, numbers, punctuation and common suffixes) similar to those used in the implementation of generative constituency parsers ( .", "labels": [], "entities": [{"text": "generative constituency parsers", "start_pos": 196, "end_pos": 227, "type": "TASK", "confidence": 0.8621985912322998}]}], "tableCaptions": [{"text": " Table 2: Parsing accuracies using different neural  network activation functions.", "labels": [], "entities": [{"text": "Parsing accuracies", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8281433582305908}]}, {"text": " Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9518150091171265}]}, {"text": " Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.", "labels": [], "entities": []}, {"text": " Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.", "labels": [], "entities": [{"text": "WSJ Language modelling", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6362699667612711}]}]}