{"title": [{"text": "Leveraging Linguistic Structure For Open Domain Information Extraction", "labels": [], "entities": [{"text": "Open Domain Information Extraction", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.630805566906929}]}], "abstractContent": [{"text": "Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, inference , and other IE tasks.", "labels": [], "entities": [{"text": "open domain information extraction (open IE)", "start_pos": 29, "end_pos": 73, "type": "TASK", "confidence": 0.7872231379151344}, {"text": "question answering", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8746966421604156}]}, {"text": "Traditionally these are extracted using a large set of patterns ; however, this approach is brittle on out-of-domain text and long-range dependencies , and gives no insight into the sub-structure of the arguments.", "labels": [], "entities": []}, {"text": "We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences.", "labels": [], "entities": []}, {"text": "We then run natural logic inference over these short clauses to determine the maximally specific arguments for each candidate triple.", "labels": [], "entities": []}, {"text": "We show that our approach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task.", "labels": [], "entities": [{"text": "IE", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9257451295852661}, {"text": "TAC-KBP 2013 Slot Filling task", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.7574447870254517}]}], "introductionContent": [{"text": "Open information extraction (open IE) has been shown to be useful in a number of NLP tasks, such as question answering), relation extraction, and information retrieval.", "labels": [], "entities": [{"text": "Open information extraction (open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8100514709949493}, {"text": "question answering)", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8647827506065369}, {"text": "relation extraction", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.9404687285423279}, {"text": "information retrieval", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.8198014497756958}]}, {"text": "Conventionally, open IE systems search a collection of patterns over either the surface form or dependency tree of a sentence.", "labels": [], "entities": []}, {"text": "Although a small set of patterns covers most simple sentences (e.g., subject verb object constructions), relevant relations are often spread across clauses (see) or presented in a non-canonical form.", "labels": [], "entities": []}, {"text": "Systems like Ollie () approach this problem by using a bootstrapping method to create a large corpus of broad-coverage partially lexicalized patterns.", "labels": [], "entities": []}, {"text": "Although this is effective at capturing many of these patterns, it visits; the US): Open IE extractions produced by the system, alongside extractions from the stateof-the-art Ollie system.", "labels": [], "entities": [{"text": "IE extractions", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.8449702262878418}, {"text": "Ollie system", "start_pos": 175, "end_pos": 187, "type": "DATASET", "confidence": 0.9126017093658447}]}, {"text": "Generating coherent clauses before applying patterns helps reduce false matches such as (Honolulu; be born in; Hawaii).", "labels": [], "entities": []}, {"text": "Inference over the sub-structure of arguments, in turn, allows us to drop unnecessary information (e.g., of Austria), but only when it is warranted (e.g., keep fake in fake praise).", "labels": [], "entities": []}, {"text": "can lead to unintuitive behavior on out-of-domain text.", "labels": [], "entities": []}, {"text": "For instance, while Obama is president is extracted correctly by Ollie as (Obama; is; president), replacing is with are in cats are felines produces no extractions.", "labels": [], "entities": []}, {"text": "Furthermore, existing systems struggle at producing canonical argument forms -for example, in the argument Heinz Fischer of Austria is likely less useful for downstream applications than Heinz Fischer.", "labels": [], "entities": []}, {"text": "In this paper, we shift the burden of extracting informative and broad coverage triples away from this large pattern set.", "labels": [], "entities": []}, {"text": "Rather, we first pre-process the sentence in linguistically motivated ways to produce coherent clauses which are (1) logically entailed by the original sentence, and (2) easy to segment into open IE triples.", "labels": [], "entities": []}, {"text": "Our approach consists of two stages: we first learn a classifier for splitting a sentence into shorter utterances (Section 3), and then appeal to natural logic to maximally shorten these utterances while maintaining necessary context (Section 4.1).", "labels": [], "entities": []}, {"text": "A small set of 14 hand-crafted patterns can then be used to segment an utterance into an open IE triple.", "labels": [], "entities": []}, {"text": "We treat the first stage as a greedy search problem: we traverse a dependency parse tree recursively, at each step predicting whether an edge should yield an independent clause.", "labels": [], "entities": []}, {"text": "Importantly, in many cases na\u00a8\u0131velyna\u00a8\u0131vely yielding a clause on a dependency edge produces an incomplete utterance (e.g., Born in Honolulu, Hawaii, from).", "labels": [], "entities": []}, {"text": "These are often attributable to control relationships, where either the subject or object of the governing clause controls the subject of the subordinate clause.", "labels": [], "entities": []}, {"text": "We therefore allow the produced clause to sometimes inherit the subject or object of its governor.", "labels": [], "entities": []}, {"text": "This allows us to capture a large variety of long range dependencies with a concise classifier.", "labels": [], "entities": []}, {"text": "From these independent clauses, we then extract shorter sentences, which will produce shorter arguments more likely to be useful for downstream applications.", "labels": [], "entities": []}, {"text": "A natural framework for solving this problem is natural logic -a proof system built on the syntax of human language (see Section 4.1).", "labels": [], "entities": []}, {"text": "We can then observe that Heinz Fischer of Austria visits China entails that Heinz Fischer visits China.", "labels": [], "entities": []}, {"text": "On the other hand, we respect situations where it is incorrect to shorten an argument.", "labels": [], "entities": []}, {"text": "For example, No house cats have rabies should not entail that cats have rabies, or even that house cats have rabies.", "labels": [], "entities": []}, {"text": "When careful attention to logical validity is necessary -such as textual entailment -this approach captures even more subtle phenomena.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6775484830141068}]}, {"text": "For example, whereas all rabbits eat fresh vegetables yields (rabbits; eat; vegetables), the apparently similar sentence all young rabbits drink milk does not yield (rabbits; drink; milk).", "labels": [], "entities": []}, {"text": "We show that our new system performs well on areal world evaluation -the TAC KBP Slot Filling challenge.", "labels": [], "entities": [{"text": "TAC KBP Slot Filling", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.6240723356604576}]}, {"text": "We outperform both an official submission on open IE, and a baseline of replacing our extractor with Ollie, a state-ofthe-art open IE systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach in the context of a realworld end-to-end relation extraction task -the TAC KBP Slot Filling challenge.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7840703129768372}, {"text": "TAC KBP Slot Filling challenge", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.6188914060592652}]}, {"text": "In Slot Filling, we are given a large unlabeled corpus of text, a fixed schema of relations (see Section 5), and a set of query entities.", "labels": [], "entities": [{"text": "Slot Filling", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.9057431221008301}]}, {"text": "The task is to find all relation triples in the corpus that have as a subject the query entity, and as a relation one of the defined relations.", "labels": [], "entities": []}, {"text": "This can be viewed intuitively as populating Wikipedia Infoboxes from a large unstructured corpus of text.", "labels": [], "entities": []}, {"text": "We compare our approach to the University of Washington submission to.", "labels": [], "entities": []}, {"text": "Their system used OpenIE v4.0 (a successor to Ollie) run over the KBP corpus and then they generated a mapping from the extracted relations to the fixed schema.", "labels": [], "entities": [{"text": "KBP corpus", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.9562799632549286}]}, {"text": "Unlike our system, Open IE v4.0 employs a semantic role component extracting structured SRL frames, alongside a conventional open IE system.", "labels": [], "entities": []}, {"text": "Furthermore, the UW submission allows for extracting relations and entities from substrings of an open IE triple argument.", "labels": [], "entities": [{"text": "UW submission", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.870163232088089}]}, {"text": "For example, from the triple (Smith; was appointed; acting director of Acme Corporation), they extract that Smith is employed by Acme Corporation.", "labels": [], "entities": [{"text": "Acme Corporation", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.9665012955665588}, {"text": "Acme Corporation", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.9663083255290985}]}, {"text": "We disallow such extractions, passing the burden of finding correct precise extractions to the open IE system itself (see Section 4).", "labels": [], "entities": []}, {"text": "For entity linking, the UW submission uses Tom Lin's entity linker (; our submission uses the Illinois Wikifier () without the relational inference component, for efficiency.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.761663556098938}, {"text": "UW submission", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9386053681373596}, {"text": "Illinois Wikifier", "start_pos": 94, "end_pos": 111, "type": "DATASET", "confidence": 0.9070885181427002}]}, {"text": "For coreference, UW uses the Stanford coreference system (Lee et al., 2011); we employ a variant of the simple coref system described in).", "labels": [], "entities": [{"text": "coreference", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.956305205821991}, {"text": "UW", "start_pos": 17, "end_pos": 19, "type": "DATASET", "confidence": 0.9232276678085327}]}, {"text": "We report our results in.", "labels": [], "entities": []}, {"text": "UW Official refers to the official submission in the 2013 challenge; we show a 3.1 F 1 improvement (to 22.7 All results are reported with the anydoc flag set to true in the evaluation script, meaning that only the truth of the extracted knowledge base entry and not the associated provenance is scored.", "labels": [], "entities": [{"text": "UW Official", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9484897553920746}, {"text": "F 1 improvement", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.9631888071695963}]}, {"text": "In absence of human evaluators, this is in order to not penalize our system unfairly for extracting anew correct provenance.: A summary of our results on the endto-end KBP Slot Filling task.", "labels": [], "entities": [{"text": "KBP Slot Filling task", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.765943169593811}]}, {"text": "UW official is the submission made to the 2013 challenge.", "labels": [], "entities": [{"text": "UW official", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9567829072475433}]}, {"text": "The second row is the accuracy of Ollie embedded in our framework, and of Ollie evaluated with nominal relations from our system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9996553659439087}]}, {"text": "Lastly, we report our system, our system with nominal relations removed, and our system combined with an alternate names detector and rule-based website detector.", "labels": [], "entities": []}, {"text": "Comparable systems are marked with a dagger \u2020 or asterisk * . F 1 ) over this submission, evaluated using a comparable approach.", "labels": [], "entities": [{"text": "F 1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9708513021469116}]}, {"text": "A common technique in KBP systems but not employed by the official UW submission in 2013 is to add alternate names based on entity linking and coreference.", "labels": [], "entities": [{"text": "UW submission in 2013", "start_pos": 67, "end_pos": 88, "type": "DATASET", "confidence": 0.9737162292003632}]}, {"text": "Additionally, websites are often extracted using heuristic namematching as they are hard to capture with traditional relation extraction techniques.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7276059091091156}]}, {"text": "If we make use of both of these, our end-to-end accuracy becomes 28.2 F 1 . We attempt to remove the variance in scores from the influence of other components in an endto-end KBP system.", "labels": [], "entities": [{"text": "end-to-end", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9417127966880798}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.7765569090843201}]}, {"text": "We ran the Ollie open IE system () in an identical framework to ours, and report accuracy in.", "labels": [], "entities": [{"text": "Ollie open IE system", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.8668319135904312}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9775010347366333}]}, {"text": "Note that when an argument to an Ollie extraction contains a named entity, we take the argument to be that named entity.", "labels": [], "entities": [{"text": "Ollie extraction", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.6068908721208572}]}, {"text": "The low performance of this system can be partially attributed to its inability to extract nominal relations.", "labels": [], "entities": []}, {"text": "To normalize for this, we report results when the Ollie extractions are supplemented with the nominal relations produced by our system (Ollie + Nominal Rels in).", "labels": [], "entities": [{"text": "Ollie extractions", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.5328044891357422}]}, {"text": "Conversely, we can remove the nominal relation extractions from our system; in both cases we outperform Ollie on the task.", "labels": [], "entities": [{"text": "nominal relation extractions", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7572993636131287}]}, {"text": "Figure 4: A precision/recall curve for Ollie and our system (without nominals).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9995179176330566}, {"text": "recall curve", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.9468302130699158}, {"text": "Ollie", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.6530818939208984}]}, {"text": "For clarity, recall is plotted on a range from 0 to 0.15.", "labels": [], "entities": [{"text": "clarity", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9645054936408997}, {"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9993752837181091}]}], "tableCaptions": [{"text": " Table 4: A selection of the mapping from KBP to lemmatized open IE relations, conditioned on the types  of the arguments being correct. The top one or two relations are shown for 7 person and 6 organization  relations. Incorrect or dubious mappings are marked with an asterisk.", "labels": [], "entities": []}, {"text": " Table 5: A summary of our results on the end- to-end KBP Slot Filling task. UW official is the  submission made to the 2013 challenge. The sec- ond row is the accuracy of Ollie embedded in  our framework, and of Ollie evaluated with nom- inal relations from our system. Lastly, we report  our system, our system with nominal relations re- moved, and our system combined with an alternate  names detector and rule-based website detector.  Comparable systems are marked with a dagger  \u2020 or  asterisk  *  .", "labels": [], "entities": [{"text": "KBP Slot Filling task", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7182682305574417}, {"text": "UW official", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9716569185256958}, {"text": "sec- ond row", "start_pos": 140, "end_pos": 152, "type": "METRIC", "confidence": 0.7387735694646835}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9994856119155884}]}]}