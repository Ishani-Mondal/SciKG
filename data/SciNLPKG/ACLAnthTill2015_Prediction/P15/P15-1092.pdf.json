{"title": [], "abstractContent": [{"text": "Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information.", "labels": [], "entities": []}, {"text": "While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience.", "labels": [], "entities": [{"text": "SPs", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9642953872680664}, {"text": "human lexical acquisition", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7444102168083191}]}, {"text": "We present the first SP learning method that simultaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features.", "labels": [], "entities": [{"text": "SP learning", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9236650764942169}]}, {"text": "Our results show that it outperforms linguistic and visual models in isolation, as well as the existing SP induction approaches.", "labels": [], "entities": [{"text": "SP induction", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.9532585144042969}]}], "introductionContent": [{"text": "Selectional preferences (SPs) are the semantic constraints that a predicate places onto its arguments.", "labels": [], "entities": [{"text": "Selectional preferences (SPs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6173792243003845}]}, {"text": "This means that certain classes of entities are more likely to fill the predicate's argument slot than others.", "labels": [], "entities": []}, {"text": "For instance, while the sentences \"The authors wrote anew paper.\" and \"The cat is eating your sausage!\" sound natural and describe plausible real-life situations, the sentences \"The carrot ate the keys.\" and \"The law sang a driveway.\" appear implausible and difficult to interpret, as the arguments do not satisfy the verbs' common preferences.", "labels": [], "entities": []}, {"text": "SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation, resolving ambiguous syntactic attachments, semantic role labelling (, natural language inference (, and figurative language processing.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.7386866211891174}, {"text": "word sense disambiguation", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.670215000708898}, {"text": "resolving ambiguous syntactic attachments", "start_pos": 168, "end_pos": 209, "type": "TASK", "confidence": 0.7706413567066193}, {"text": "semantic role labelling", "start_pos": 211, "end_pos": 234, "type": "TASK", "confidence": 0.6410307089487711}, {"text": "figurative language processing", "start_pos": 272, "end_pos": 302, "type": "TASK", "confidence": 0.7489250898361206}]}, {"text": "Automatic acquisition of SPs from linguistic data has thus become an active area of research.", "labels": [], "entities": [{"text": "Automatic acquisition of SPs from linguistic", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.832548608382543}]}, {"text": "The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes), word clustering (, distributional similarity metrics, latent variable models, and neural networks (Van de Cruys, 2014).", "labels": [], "entities": [{"text": "word clustering", "start_pos": 211, "end_pos": 226, "type": "TASK", "confidence": 0.8187992870807648}]}, {"text": "Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs.", "labels": [], "entities": [{"text": "learning of SPs", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.5261182487010956}]}, {"text": "There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action.", "labels": [], "entities": []}, {"text": "This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world.", "labels": [], "entities": []}, {"text": "Multi-modal models of word meaning have thus enjoyed a growing interest in semantics ( ), outperforming purely text-based models in tasks such as similarity estimation (), predicting compositionality (, and concept categorization.", "labels": [], "entities": [{"text": "similarity estimation", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.6736343055963516}, {"text": "predicting compositionality", "start_pos": 172, "end_pos": 199, "type": "TASK", "confidence": 0.9657381772994995}]}, {"text": "However, to date these approaches relied on low-level image features such as color histograms or SIFT keypoints to represent the meaning of isolated words.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there has not yet been a multimodal semantic approach performing extraction of predicate-argument relations from visual data.", "labels": [], "entities": [{"text": "extraction of predicate-argument relations from visual data", "start_pos": 95, "end_pos": 154, "type": "TASK", "confidence": 0.820229172706604}]}, {"text": "In this paper, we propose the first SP model integrating information about predicate-argument interactions from text, images, and videos.", "labels": [], "entities": []}, {"text": "We expect it to outperform purely text-based models of SPs, which suffer from two problems: topic bias and figurative uses of words.", "labels": [], "entities": [{"text": "SPs", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9670426845550537}]}, {"text": "Such bias stems from the fact that we typically write about abstract topics and events, resulting in high coverage of abstract senses of words and comparatively lower coverage of the original physical senses).", "labels": [], "entities": []}, {"text": "For instance, the verb cut is used predominantly in the domains of economics and finance and its most frequent direct objects are cost and price, according to the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 163, "end_pos": 192, "type": "DATASET", "confidence": 0.9736080467700958}]}, {"text": "Predicate-argument distributions acquired from text thus tend to be skewed in favour of abstract domains and figurative uses, inadequately reflecting our daily experiences with cutting, which guide human acquisition of meaning.", "labels": [], "entities": []}, {"text": "Integrating predicate-argument relations observed in the physical world (in the form of image and video descriptions) with the more abstract text-based relations is likely to yield a more realistic semantic model, with real prospects of improving the performance of NLP applications that rely on SPs.", "labels": [], "entities": []}, {"text": "We use the BNC as an approximation of linguistic knowledge and a large collection of tagged images and videos from Flickr (www.flickr.com) as an approximation of perceptual knowledge.", "labels": [], "entities": [{"text": "BNC", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.7085639834403992}, {"text": "Flickr", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.9474973678588867}]}, {"text": "The human-annotated labels that accompany media on Flickr enable us to acquire predicate-argument cooccurrence information.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9522255063056946}]}, {"text": "Our experiments focus on verb preferences for their subjects and direct objects.", "labels": [], "entities": []}, {"text": "In summary, our method (1) performs word sense disambiguation and part-of-speech (PoS) tagging of Flickr tag sequences to extract verb-noun co-occurrence; (2) clusters nouns to induce SP classes using linguistic and visual features; (3) quantifies the strength of preference of a verb fora given class by interpolating linguistic and visual SP distributions.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6596565445264181}, {"text": "part-of-speech (PoS) tagging of Flickr tag sequences", "start_pos": 66, "end_pos": 118, "type": "TASK", "confidence": 0.6833726101451449}]}, {"text": "We investigate the impact of perceptual information at different levels -from none (purely text-based model) to 100% (purely visual model).", "labels": [], "entities": []}, {"text": "We evaluate our model directly against a dataset of human plausibility judgements of verbnoun pairs, as well as in the context of a semantic task: metaphor interpretation.", "labels": [], "entities": [{"text": "metaphor interpretation", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.7303218692541122}]}, {"text": "Our results show that the interpolated model combining linguistic and visual relations outperforms the purely linguistic model in both evaluation settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extract linguistic features for our model from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9622292518615723}]}, {"text": "In particular, we parse the corpus using the RASP parser () and extract subject-verb and verb-object relations from its dependency output.", "labels": [], "entities": []}, {"text": "These relations are then used as features for clustering to obtain SP classes, as well as to quantify the strength of association between a particular verb and a particular argument class.", "labels": [], "entities": []}, {"text": "For the visual features of our model, we mine the Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.9240954220294952}]}, {"text": "Flickr-100M contains 99.3 million images and 0.7 million videos with language tags annotated by users, enabling us to generalise SPs at a large scale.", "labels": [], "entities": [{"text": "Flickr-100M", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9384649395942688}]}, {"text": "The tags reflect how humans describe objects and actions from a visual perspective.", "labels": [], "entities": []}, {"text": "We first stem the tags and remove words that are absent in WordNet (typically named entities and misspellings), then identify their PoS based on their visual context and extract verb-noun cooccurrences.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.943220853805542}]}, {"text": "We evaluate the predicate-argument scores assigned by our models against a dataset of human plausibility judgements of verb-direct object pairs collected by.", "labels": [], "entities": []}, {"text": "Their dataset is balanced with respect to the frequency of verb-argument relations, as well as their plausibility and implausibility, thus creating a realistic SP evaluation task.", "labels": [], "entities": [{"text": "SP evaluation", "start_pos": 160, "end_pos": 173, "type": "TASK", "confidence": 0.9341384172439575}]}, {"text": "Keller and Lapata selected 30 predicates and matched each of them to three arguments from different co-occurrence frequency bands according to their BNC counts, e.g. divert attention (high frequency), divert water (medium) and divert fruit (low).", "labels": [], "entities": []}, {"text": "This constituted their dataset of Seen verb-noun pairs, 90 in total.", "labels": [], "entities": []}, {"text": "Each of the predicates was then also paired with three randomly selected arguments with which it did not occur in the BNC, creating the Unseen dataset.", "labels": [], "entities": [{"text": "BNC", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.9325591325759888}, {"text": "Unseen dataset", "start_pos": 136, "end_pos": 150, "type": "DATASET", "confidence": 0.9430670142173767}]}, {"text": "The pairs in both datasets were then rated for their plausibility by 27 human subjects, and their judgements were aggregated into a gold standard.", "labels": [], "entities": []}, {"text": "We compare the verb-argument scores generated by our linguistic (LSP), visual (VSP) and interpolated (ISP) SP models against these two datasets in terms of Pearson correlation coefficient, r, and Spearman rank correlation coefficient, \u03c1.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 156, "end_pos": 187, "type": "METRIC", "confidence": 0.9673835039138794}, {"text": "Spearman rank correlation coefficient", "start_pos": 196, "end_pos": 233, "type": "METRIC", "confidence": 0.6913899630308151}]}, {"text": "The selectional association score of the cluster to which a given noun belongs is taken to represent the preference score of the verb for this noun.", "labels": [], "entities": []}, {"text": "If a noun is not present in our argument clusters, we match it to its nearest cluster, as determined by its distributional similarity to the cluster centroid in terms of Jensen-Shannon divergence.", "labels": [], "entities": []}, {"text": "We first compare LSP, VSP and ISP with static and predicate-driven interpolation weights.", "labels": [], "entities": []}, {"text": "The results, presented in, demonstrate that the interpolated model outperforms both LSP and VSP used on their own.", "labels": [], "entities": []}, {"text": "The best performance is attained with the static interpolation weights of \u03bb LM = 0.8 (r = 0.540; \u03c1 = 0.728) and \u03bb LM = 0.9 (r = 0.548; \u03c1 = 0.699).", "labels": [], "entities": []}, {"text": "This suggests that while linguistic input plays a crucial role in SP induction (by providing both semantic and syntactic information), visual features further enhance the quality of SPs, as we expected.", "labels": [], "entities": [{"text": "SP induction", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.9916374981403351}]}, {"text": "However, the model based on visual features alone performs poorly on the dataset of.", "labels": [], "entities": []}, {"text": "This is partly explained by the fact that a number of verbs in this dataset are abstract verbs, whose visual representations in the Flickr data are sparse.", "labels": [], "entities": [{"text": "Flickr data", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9760223031044006}]}, {"text": "In addition, VSP (as other visual models used in isolation from text) is not syntaxaware and is unable to discriminate between different types of semantic relations.", "labels": [], "entities": []}, {"text": "VSP thus acquires sets of verb-argument relations that are closer in nature to scene descriptions and semantic frames than to lexico-syntactic paradigms.", "labels": [], "entities": []}, {"text": "shows the differences between linguistic and visual arguments of the verb kill ranked by LSP and VSP.", "labels": [], "entities": [{"text": "LSP", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9817051887512207}, {"text": "VSP", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.957438051700592}]}, {"text": "While LSP produces mainly semantic objects of kill, VSP output contains other types of arguments, such as weapon (instrument) and death (consequence).", "labels": [], "entities": []}, {"text": "Taking the argument classes produced by the linguistic model as a basis and then re-ranking LSP: (1) 0.523 girl other woman child person people; (2) 0.164 fleet soldier knight force rebel guard troops crew army pilot; (3) 0.133 sister daughter parent relative lover cousin friend wife mother husband brother father; (4) 0.048 being species sheep animal creature horse baby human fish male lamb bird rabbit [..]; (5) 0.045 victim bull teenager prisoner hero gang enemy rider offender youth killer thief [..]", "labels": [], "entities": []}, {"text": "Figure 5: Error analysis: Mixed subjects and direct objects of drink, assigned by the predicate-driven ISP them to incorporate visual statistics helps to avoid the above problem for the interpolated models, whose output corresponds to grammatical relations.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7859076261520386}]}, {"text": "However, static interpolation weights (emphasizing linguistic features over the visual ones for all verbs equally) outperformed the predicate-driven interpolation technique, attaining correlations of r = 0.548 and r = 0.476 respectively.", "labels": [], "entities": []}, {"text": "This is mainly due to the fact that some verbs are overrepresented in the visual data (e.g. the predicatedriven interpolation weight for the verb drink is \u03bb LM = 0.08).", "labels": [], "entities": []}, {"text": "As a result, candidate argument classes (selected based on syntactically-parsed linguistic input) are ranked predominantly based on visual statistics.", "labels": [], "entities": []}, {"text": "This makes it possible to emphasize incorrectly parsed arguments (such as subject relations in the direct object SP distribution and vice versa).", "labels": [], "entities": []}, {"text": "The predicate-driven ISP output for direct object SPs of drink, for instance, contains a mixture of subject and direct object classes, as shown in.", "labels": [], "entities": []}, {"text": "Using a static model with a high \u03bb LM weight helps to avoid such errors and, therefore, leads to a better performance.", "labels": [], "entities": []}, {"text": "In order to investigate the composition of the visual and linguistic datasets, we assess the average level of concreteness of the verbs and nouns present in the datasets.", "labels": [], "entities": []}, {"text": "We use the concreteness ratings from the MRC Psycholinguistic Database for this purpose.", "labels": [], "entities": [{"text": "MRC Psycholinguistic Database", "start_pos": 41, "end_pos": 70, "type": "DATASET", "confidence": 0.9154261946678162}]}, {"text": "In this database, nouns and  verbs are rated for concreteness on a scale from 100 (highly abstract) to 700 (highly concrete).", "labels": [], "entities": []}, {"text": "We map the verbs and nouns in our textual and visual corpora to their MRC concreteness scores.", "labels": [], "entities": [{"text": "MRC concreteness", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6107953786849976}]}, {"text": "We then calculate a dataset-wide concreteness score as an average of the concreteness scores of individual verbs and nouns weighted by their frequency in the respective corpus.", "labels": [], "entities": []}, {"text": "The average concreteness scores in the visual dataset were 506.4 (nouns) and 498.1 (verbs).", "labels": [], "entities": []}, {"text": "As expected, they are higher than the respective scores in the textual data: 433.1 (nouns) and.", "labels": [], "entities": []}, {"text": "In order to compare the types of actions that are common in each of the datasets, we map the verbs to their corresponding top level classes in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.9649398326873779}]}, {"text": "shows the comparison of prominent verb classes in visual and textual data.", "labels": [], "entities": []}, {"text": "One can see from the Figure that the visual dataset is well suited for representing motion, perception and contact, while abstract verbs related to e.g. communication, cognition, possession or change are more common in textual data.", "labels": [], "entities": []}, {"text": "We also compare the performance of our models to existing SP induction methods: the EM-based clustering method of, the vector space similarity-based method of and the LDA topic modelling approach of\u00b4O of\u00b4 of\u00b4O S\u00e9aghdha (2010) . The best ISP configuration (\u03bb LM = 0.9) outperforms all of these methods, as well as our own LSP, on the Seen dataset, confirming the positive contribution of visual features.", "labels": [], "entities": [{"text": "Seen dataset", "start_pos": 333, "end_pos": 345, "type": "DATASET", "confidence": 0.9607978165149689}]}, {"text": "However, it achieves less success on the Unseen data, where the methods of\u00b4Oof\u00b4 of\u00b4O S\u00e9aghdha (2010) and are leading.", "labels": [], "entities": [{"text": "Unseen data", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9345288574695587}]}, {"text": "This result speaks in favour of latent variable models for acquisition of SP estimates for rarely attested predicateargument pairs.", "labels": [], "entities": []}, {"text": "In turn, this suggests that integrating our ISP model (that currently outperforms others on more common pairs) with such techniques is likely to improve SP prediction across frequency bands.", "labels": [], "entities": [{"text": "SP prediction", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.983988493680954}]}, {"text": "In order to investigate the applicability of perceptually grounded SPs in wider NLP, we evaluate them in the context of an external semantic task -that of metaphor interpretation.", "labels": [], "entities": [{"text": "metaphor interpretation", "start_pos": 155, "end_pos": 178, "type": "TASK", "confidence": 0.7134595811367035}]}, {"text": "Since metaphor is based on transferring imagery and knowledge across domains -typically from more familiar domains of physical experiences to the sphere of vague and elusive abstract thought -metaphor interpretation provides an ideal framework for testing perceptually grounded SPs.", "labels": [], "entities": []}, {"text": "Our experiments rely on the metaphor interpretation method of, in which text-derived SPs area central component of the system.", "labels": [], "entities": [{"text": "metaphor interpretation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7489737868309021}]}, {"text": "We replace the SP component with our LSP and ISP (\u03bb LM = 0.8) models and compare their performance in the context of metaphor interpretation.", "labels": [], "entities": [{"text": "metaphor interpretation", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.8199230134487152}]}, {"text": "Shutova (2010) defined metaphor interpretation as a paraphrasing task, where literal paraphrases for metaphorical expressions are derived from corpus data using a set of statistical measures.", "labels": [], "entities": [{"text": "metaphor interpretation", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8238959014415741}]}, {"text": "For instance, their system interprets the metaphor \"a carelessly leaked report\" as \"a carelessly disclosed report\".", "labels": [], "entities": []}, {"text": "Focusing on metaphorical verbs in subject and direct object constructions, Shutova first applies a maximum likelihood model to extract and rank candidate paraphrases for the verb given the context, as follows: where f (i) is the frequency of the paraphrase on its own and f (w n , i) the co-occurrence frequency of the paraphrase with the context word w n . This results for their re-implementation reported by, who conducted a comprehensive evaluation of SP models on the plausibility data of model favours paraphrases that match the given context best.", "labels": [], "entities": []}, {"text": "These candidates are then filtered based on the presence of shared features with the metaphorical verb, as defined by their location and distance in the WordNet hierarchy.", "labels": [], "entities": []}, {"text": "All the candidates that have a common hypernym with the metaphorical verb within three levels of the WordNet hierarchy are selected.", "labels": [], "entities": []}, {"text": "This results in a set of paraphrases retaining the meaning of the metaphorical verb.", "labels": [], "entities": []}, {"text": "However, some of them are still figuratively used.", "labels": [], "entities": []}, {"text": "Shutova further applies an SP model to discriminate between figurative and literal paraphrases, treating a strong selectional preference fit as a likely indicator of literalness.", "labels": [], "entities": []}, {"text": "The candidates are re-ranked by the SP model, emphasizing the verbs whose preferences the noun in the context matches best.", "labels": [], "entities": []}, {"text": "We use LSP and ISP scores to perform this re-ranking step.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our models on this task using the metaphor paraphrasing gold standard of.", "labels": [], "entities": []}, {"text": "The dataset consists of 52 verb metaphors and their human-produced literal paraphrases.", "labels": [], "entities": []}, {"text": "Following Shutova, we evaluate the performance in terms of mean average precision (MAP), which measures the ranking quality of GS paraphrases across the dataset.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 59, "end_pos": 87, "type": "METRIC", "confidence": 0.9306908448537191}]}, {"text": "MAP is defined as follows: where M is the number of metaphorical expressions, N j is the number of correct paraphrases for the metaphorical expression j, P ji is the precision at each correct paraphrase (the number of correct paraphrases among the topi ranks).", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9991692304611206}]}, {"text": "As compared to the gold standard, ISP attains a MAP score of 0.65, outperforming both the LSP (MAP = 0.62) and the original system of Shutova", "labels": [], "entities": [{"text": "MAP score", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9872071444988251}, {"text": "LSP", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9807954430580139}, {"text": "MAP", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9178436398506165}]}], "tableCaptions": [{"text": " Table 1: Model comparison on the plausibility data  of Keller and Lapata (2003)", "labels": [], "entities": []}, {"text": " Table 2: Comparison to other SP induction meth- ods. * Results reported in O'Seaghdha (2010).", "labels": [], "entities": [{"text": "O'Seaghdha (2010)", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.9242232292890549}]}]}