{"title": [{"text": "Encoding Source Language with Convolutional Neural Network for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.732005313038826}]}], "abstractContent": [{"text": "The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 213, "end_pos": 216, "type": "TASK", "confidence": 0.9905154705047607}]}, {"text": "In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9669077396392822}]}, {"text": "With different guiding signals during decoding , our specifically designed convolu-tion+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation.", "labels": [], "entities": []}, {"text": "This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM.", "labels": [], "entities": []}, {"text": "Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average.", "labels": [], "entities": [{"text": "NIST Chinese-English translation tasks", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.7699561566114426}, {"text": "NNJM", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.9330726861953735}, {"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9988123178482056}]}], "introductionContent": [{"text": "Learning of continuous space representation for source language has attracted much attention in both traditional statistical machine translation (SMT) and neural machine translation (NMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 113, "end_pos": 150, "type": "TASK", "confidence": 0.7856275538603464}, {"text": "neural machine translation (NMT)", "start_pos": 155, "end_pos": 187, "type": "TASK", "confidence": 0.8147959013779958}]}, {"text": "Various models, mostly neural network-based, have been proposed for representing the source sentence, mainly as the encoder part in an encoder-decoder framework ().", "labels": [], "entities": []}, {"text": "There has been some quite recent work on encoding only \"relevant\" part of source sentence during the decoding process, most notably neural network joint model (NNJM) in), which extends the n-grams target language model by additionally taking a fixed-length window of source sentence, achieving state-of-the-art performance in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 326, "end_pos": 357, "type": "TASK", "confidence": 0.7009677588939667}]}, {"text": "In this paper, we propose novel convolutional architectures to dynamically encode the relevant information in the source language.", "labels": [], "entities": []}, {"text": "Our model covers the entire source sentence, but can effectively find and properly summarize the relevant parts, guided by the information from the target language.", "labels": [], "entities": []}, {"text": "With the guiding signals during decoding, our specifically designed convolution architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation.", "labels": [], "entities": []}, {"text": "This representation, together with target words, are fed to a deep neural network (DNN) to form a stronger NNJM.", "labels": [], "entities": []}, {"text": "Since our proposed joint model is purely lexicalized, it can be integrated into any SMT decoder as a feature.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.8868838846683502}]}, {"text": "Two variants of the joint model are also proposed, with coined name tagCNN and inCNN, with different guiding signals used from the decoding process.", "labels": [], "entities": [{"text": "inCNN", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9557300806045532}]}, {"text": "We integrate the proposed joint models into a state-of-the-art dependency-to-string translation system (Xie et al., 2011) to evaluate their effectiveness.", "labels": [], "entities": []}, {"text": "Experiments on NIST Chinese-English translation tasks show that our model is able to achieve significant improvements of +2.0 BLEU points on average over the baseline.", "labels": [], "entities": [{"text": "NIST Chinese-English translation tasks", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.763472318649292}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9987077713012695}]}, {"text": "Our model also outperforms's NNJM by up to +1.08 BLEU points.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9047969579696655}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9990096092224121}]}, {"text": "RoadMap: In the remainder of this paper, we start with a brief overview of joint language model in Section 2, while the convolutional encoders, as the key component of which, will be described in detail in Section 3.", "labels": [], "entities": []}, {"text": "Then in Section 4 we discuss the decoding algorithm with the proposed models.", "labels": [], "entities": []}, {"text": "The experiment results are reported in Section 5, followed by Section 6 and 7 for related work and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments in this Section are designed to answer the following questions: 1.", "labels": [], "entities": []}, {"text": "Are our tagCNN and inCNN joint language models able to improve translation quality, and are they complementary to each other?", "labels": [], "entities": []}, {"text": "2. Do inCNN and tagCNN benefit from their guiding signal, compared to a generic CNN?", "labels": [], "entities": []}, {"text": "3. For tagCNN, is it helpful to embed more dependency structure, e.g., dependency head of each affiliated word, as additional information?", "labels": [], "entities": []}, {"text": "4. Can our gating strategy improve the performance over max-pooling?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU-4 scores (%) on NIST MT04-test and MT05-test, of Moses (default settings),  dependency-to-string baseline system (Dep2Str), and different features on top of Dep2Str: neural  network joint model (BBN-JM), generic CNN, tagCNN, inCNN and the combination of tagCNN  and inCNN. The boldface numbers and superscript  *  indicate that the results are significantly  better (p<0.01) than those of the BBN-JM and the Dep2Str baseline respectively. \"+\" stands for  adding the corresponding feature to Dep2Str.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9994945526123047}, {"text": "NIST", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9613638520240784}, {"text": "MT04-test", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.6147365570068359}, {"text": "MT05-test", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9003427624702454}, {"text": "BBN-JM", "start_pos": 408, "end_pos": 414, "type": "DATASET", "confidence": 0.8684500455856323}, {"text": "Dep2Str baseline", "start_pos": 423, "end_pos": 439, "type": "DATASET", "confidence": 0.9169961810112}]}, {"text": " Table 2:  BLEU-4 scores (%) of tagCNN  model with dependency head words as addi- tional tags (tagCNN dep).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9988079071044922}]}]}