{"title": [{"text": "Exploring the Planet of the APEs: a Comparative Study of State-of-the-art Methods for MT Automatic Post-Editing", "labels": [], "entities": [{"text": "MT Automatic Post-Editing", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.8480213085810343}]}], "abstractContent": [{"text": "Downstream processing of machine translation (MT) output promises to be a solution to improve translation quality, especially when the MT system's internal decoding process is not accessible.", "labels": [], "entities": [{"text": "machine translation (MT) output", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.8560687303543091}]}, {"text": "Both rule-based and statistical automatic post-editing (APE) methods have been proposed over the years, but with contrasting results.", "labels": [], "entities": []}, {"text": "A missing aspect in previous evaluations is the assessment of different methods: i) under comparable conditions, and ii) on different language pairs featuring variable levels of MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 178, "end_pos": 180, "type": "TASK", "confidence": 0.9687192440032959}]}, {"text": "Fo-cusing on statistical APE methods (more portable across languages), we propose the first systematic analysis of two approaches.", "labels": [], "entities": [{"text": "APE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.6858275532722473}]}, {"text": "To understand their potential, we compare them in the same conditions over six language pairs having English as source.", "labels": [], "entities": []}, {"text": "Our results evidence consistent improvements on all language pairs, a relation between the extent of the gain and MT output quality, slight but statistically significant performance differences between the two methods, and their possible complementarity.", "labels": [], "entities": [{"text": "MT output quality", "start_pos": 114, "end_pos": 131, "type": "METRIC", "confidence": 0.7023187875747681}]}], "introductionContent": [{"text": "Automatic post-editing (APE) aims to correct systematic machine translation (MT) errors.", "labels": [], "entities": [{"text": "Automatic post-editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.575805789232254}, {"text": "correct systematic machine translation (MT)", "start_pos": 37, "end_pos": 80, "type": "TASK", "confidence": 0.7472120778901237}]}, {"text": "The problem is appealing for several reasons.", "labels": [], "entities": []}, {"text": "On one side, as pointed out by, APE systems can improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage.", "labels": [], "entities": [{"text": "MT output", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.9302606582641602}]}, {"text": "On the other side, and to our view more importantly, APE represents the only way to recover errors produced in \"black-box\" conditions in which the MT system is unknown or its internal decoding process is not accessible.", "labels": [], "entities": [{"text": "APE", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.47803619503974915}, {"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.9027928113937378}]}, {"text": "The task, firstly proposed by to cope with article selection in Japanese to English translation, has been later addressed in various ways.", "labels": [], "entities": [{"text": "article selection in Japanese to English translation", "start_pos": 43, "end_pos": 95, "type": "TASK", "confidence": 0.8826178993497577}]}, {"text": "On one side, rule-based methods () gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages.", "labels": [], "entities": []}, {"text": "On the other side, the statistical approach proposed by reached maturity in the work by  and inspired a number of further investigations, inter alia).", "labels": [], "entities": []}, {"text": "Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits.", "labels": [], "entities": [{"text": "error correction", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.6839470863342285}]}, {"text": "Their common trait is that the reported results are difficult to generalise.", "labels": [], "entities": []}, {"text": "Indeed, most of the works focus on evaluating a specific method, 1 which is typically applied to one single dataset fora given language pair.", "labels": [], "entities": []}, {"text": "As a result, the global landscape of the \"planet of the APEs\" is still blurred and open to more systematic explorations.", "labels": [], "entities": []}, {"text": "To shed light on the potential of statistical postediting, in this paper we examine two alternative approaches.", "labels": [], "entities": []}, {"text": "One is the method proposed in ( , which to date is the most widely used.", "labels": [], "entities": []}, {"text": "The other is the \"context-aware\" solution proposed in) which, to the best of our knowledge, represents the most significant variant of . The major contribution of our work is the first systematic analysis of different APE approaches, which are tested in controlled conditions over several language pairs.", "labels": [], "entities": [{"text": "APE", "start_pos": 218, "end_pos": 221, "type": "TASK", "confidence": 0.9549027681350708}]}, {"text": "To ensure the soundness of the analysis, our experimental setup consists of a dataset composed of the same English source sentences with automatic translations into six languages and respective manual post-edits by professional translators.", "labels": [], "entities": []}, {"text": "Overall, this represents the ideal condition to complement prior research with the missing answers to questions like: Q1: Does APE yield consistent MT quality improvements across different language pairs?", "labels": [], "entities": [{"text": "APE", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.6472483277320862}, {"text": "MT", "start_pos": 148, "end_pos": 150, "type": "TASK", "confidence": 0.9736286401748657}]}, {"text": "Q2: What is the relation between the original MT output quality and the APE results?", "labels": [], "entities": [{"text": "MT output", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.8522689938545227}, {"text": "APE", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.6365388035774231}]}, {"text": "Q3: Which of the two analysed APE methods has the highest potential?", "labels": [], "entities": [{"text": "APE", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.831923246383667}]}], "datasetContent": [{"text": "Some lessons learned from prior works on statistical APE methods) include: i) learning from human post-edits is more effective than learning from (independent) reference translations, ii) learning from (and applying APE to) domain-specific data is more promising than working on general-domain data, iii) correcting the output of rule-based MT systems is easier than improving translations from statistical MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 341, "end_pos": 343, "type": "TASK", "confidence": 0.6750741004943848}, {"text": "translations from statistical MT", "start_pos": 377, "end_pos": 409, "type": "TASK", "confidence": 0.5812289044260979}]}, {"text": "Our work capitalizes on these findings (we learn from domain-specific post-edited data and apply APE to statistical MT), but fills a gap of previous research: a fair comparative study between different methods in controlled conditions.", "labels": [], "entities": [{"text": "APE", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.8863089680671692}, {"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.8181878328323364}]}, {"text": "The key enabling factor is the availability, for the first time, of data consisting of the same source sentences, machinetranslated in several languages and post-edited by professional translators.", "labels": [], "entities": []}, {"text": "We experiment with the Autodesk PostEditing Data corpus, 5 which predominantly covers the domain of software user manuals.", "labels": [], "entities": [{"text": "Autodesk PostEditing Data corpus", "start_pos": 23, "end_pos": 55, "type": "DATASET", "confidence": 0.8929429203271866}]}, {"text": "English In Method 1, MGIZA++ is used to align f and f . In Method 2 it is used to align f and e, and then f #e and f .  To guarantee similar experimental conditions in the six language settings, we also train comparable target language models from external data (indeed, the 12.2K post-edits would not be enough to train reliable LMs).", "labels": [], "entities": []}, {"text": "We build our LMs from approximately 2.5M translations of the same English sentences collected from Europarl (, DGT-Translation Memory (), JRC Acquis (), OPUS IT (Tiedemann, ) and other Autodesk data common to all languages.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9854711294174194}, {"text": "JRC Acquis", "start_pos": 138, "end_pos": 148, "type": "DATASET", "confidence": 0.78083336353302}, {"text": "Autodesk data", "start_pos": 185, "end_pos": 198, "type": "DATASET", "confidence": 0.8250695765018463}]}, {"text": "We evaluate the APE methods based on their capability to reduce the distance between the MT output and a correct (fluent and adequate) translation.", "labels": [], "entities": [{"text": "APE", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7579222917556763}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9454055428504944}]}, {"text": "As a measure of the amount of the editing operations needed for the correction, TER and HTER) fit for our purpose.", "labels": [], "entities": [{"text": "TER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9986805319786072}, {"text": "HTER", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9911536574363708}]}, {"text": "TER and HTER measure the minimum edit distance between the MT output and its cor-: Performance of the MT baseline and the APE methods for each language pair.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9764677882194519}, {"text": "HTER", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.6533651351928711}]}, {"text": "Results for Method 2 marked with the \" * \" symbol are statistically significant compared to Method 1.", "labels": [], "entities": []}, {"text": "This can be either a reference translation created independently from the MT output (TER) or a human post-edition obtained by manually correcting the MT output (HTER).", "labels": [], "entities": [{"text": "MT output (TER)", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.601697313785553}]}, {"text": "For the sake of simplicity, henceforth we will use the term TER to refer to both situations (though, when measuring the distance between the MT output and its human post-edition the actual metric is the HTER).", "labels": [], "entities": [{"text": "TER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9944491386413574}, {"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.8262212872505188}, {"text": "HTER", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.7406659126281738}]}, {"text": "Similar to all previous works on APE, our baseline is the MT output as is.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.6758525967597961}]}, {"text": "Hence, baseline scores for each language pair correspond to the TER computed between the original MT output (produced by the \"black-box\" Autodesk inhouse system) and the human post-edits.", "labels": [], "entities": [{"text": "TER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9973607659339905}]}, {"text": "lists our results, with language pairs ordered according to the respective baseline TER.", "labels": [], "entities": [{"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9950090646743774}]}], "tableCaptions": [{"text": " Table 1: Data statistics for each language.", "labels": [], "entities": []}, {"text": " Table 2: Performance of the MT baseline and the APE methods for each language pair. Results for  Method 2 marked with the \"  *  \" symbol are statistically significant compared to Method 1.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.864285945892334}]}]}