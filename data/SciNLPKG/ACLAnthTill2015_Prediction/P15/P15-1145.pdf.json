{"title": [{"text": "Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a general framework to incorporate semantic knowledge into the popular data-driven learning process of word embeddings to improve the quality of them.", "labels": [], "entities": []}, {"text": "Under this framework, we represent semantic knowledge as many ordinal ranking inequalities and formulate the learning of semantic word embed-dings (SWE) as a constrained optimization problem, where the data-derived objective function is optimized subject to all ordinal knowledge inequality constraints extracted from available knowledge resources such as Thesaurus and Word-Net.", "labels": [], "entities": [{"text": "Word-Net", "start_pos": 370, "end_pos": 378, "type": "DATASET", "confidence": 0.8635135293006897}]}, {"text": "We have demonstrated that this constrained optimization problem can be efficiently solved by the stochastic gradient descent (SGD) algorithm, even fora large number of inequality constraints.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 97, "end_pos": 130, "type": "TASK", "confidence": 0.7121114234129587}]}, {"text": "Experimental results on four standard NLP tasks, including word similarity measure, sentence completion, name entity recognition , and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7781578004360199}, {"text": "name entity recognition", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.7532124718030294}, {"text": "TOEFL synonym selection", "start_pos": 139, "end_pos": 162, "type": "TASK", "confidence": 0.7105935414632162}]}], "introductionContent": [{"text": "Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP).", "labels": [], "entities": [{"text": "Distributed word representation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.755671501159668}, {"text": "word embedding)", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.6281825800736746}, {"text": "natural language processing (NLP)", "start_pos": 155, "end_pos": 188, "type": "TASK", "confidence": 0.7789381941159567}]}, {"text": "In recent years, it has been widely used in various NLP tasks, including neural language model, sequence labelling tasks), machine translation, and antonym selection (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8526200652122498}, {"text": "antonym selection", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.7424243688583374}]}, {"text": "Typically, word vectors are learned based on the distributional hypothesis, which assumes that words with a similar context tend to have a similar meaning.", "labels": [], "entities": []}, {"text": "Under this hypothesis, various models, such as the skip-gram model ( and GloVe model), have been proposed to leverage the context of each word in large corpora to learn word embeddings.", "labels": [], "entities": []}, {"text": "These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks.", "labels": [], "entities": []}, {"text": "However, they still suffer from some major limitations.", "labels": [], "entities": []}, {"text": "In particular, these corpus-based methods usually fail to capture the precise meanings for many words.", "labels": [], "entities": []}, {"text": "For example, some semantically related but dissimilar words may have similar contexts, such as synonyms and antonyms.", "labels": [], "entities": []}, {"text": "As a result, these corpus-based methods may lead to some antonymous word vectors being located much closer in the learned embedding space than many synonymous words.", "labels": [], "entities": []}, {"text": "Moreover, as word representations are mainly learned based on the co-occurrence information, the learned word embeddings do not capture the accurate relationship between two semantically similar words if either one appears less frequently in the corpus.", "labels": [], "entities": []}, {"text": "To address these issues, some recent work has been proposed to incorporate prior lexical knowledge (WordNet, PPDB, etc.) or knowledge graph (Freebase, etc.) into word representations.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.9379174709320068}]}, {"text": "Such knowledge enhanced word embedding methods have achieved considerable improvements on various natural language processing tasks, like).", "labels": [], "entities": []}, {"text": "These methods attempt to increase the semantic similarities between words belonging to one semantic category or to explicitly model the semantic relationships between different words.", "labels": [], "entities": []}, {"text": "For example, have proposed anew learning objective function to enhance word embeddings by combining neural models and a prior knowledge measure from semantic resources.", "labels": [], "entities": []}, {"text": "have recently proposed to leverage morphological, syntactic, and semantic knowledge to improve the learning of word embeddings.", "labels": [], "entities": []}, {"text": "Besides, a novel framework has been proposed in () to take advantage of both relational and categorical knowledge to learn high-quality word representations, where two regularization functions are used to model the relational and categorical knowledge respectively.", "labels": [], "entities": []}, {"text": "More recently, a retrofitting technique has been introduced in) to improve semantic vectors by leveraging lexicon-derived relational information in a post-processing stage.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew and flexible method to incorporate semantic knowledge into the corpus-based learning of word embeddings.", "labels": [], "entities": []}, {"text": "In our approach, we propose to represent semantic knowledge as many word ordinal ranking inequalities.", "labels": [], "entities": []}, {"text": "Furthermore, these inequalities are cast as semantic constraints in the optimization process to learn semantically sensible word embeddings.", "labels": [], "entities": []}, {"text": "The proposed method has several advantages.", "labels": [], "entities": []}, {"text": "Firstly, many different types of semantic knowledge can all be represented as a number of such ranking inequalities, such as synonymantonym, hyponym-hypernym and etc.", "labels": [], "entities": []}, {"text": "Secondly, these inequalities can be easily extracted from many existing knowledge resources, such as Thesaurus, WordNet and knowledge graphs.", "labels": [], "entities": [{"text": "Thesaurus", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.9182149767875671}, {"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.8938451409339905}]}, {"text": "Moreover, the ranking inequalities can also be manually generated by human annotation because ranking orders is much easier for human annotators than assigning specific scores.", "labels": [], "entities": []}, {"text": "Next, we present a flexible learning framework to learn distributed word representation based on the ordinal semantic knowledge.", "labels": [], "entities": []}, {"text": "By solving a constrained optimization problem using the efficient stochastic gradient descent algorithm, we can obtain semantic word embedding enhanced by the ordinal knowledge constraints.", "labels": [], "entities": []}, {"text": "Experiments on four popular natural language processing tasks, including word similarity, sentence completion, name entity recognition and synonym selection, have all demonstrated that the proposed method can learn good semantically sensible word embeddings.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.7526794672012329}, {"text": "sentence completion", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7616414725780487}, {"text": "name entity recognition", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.7446531256039938}, {"text": "synonym selection", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.9054358899593353}]}], "datasetContent": [{"text": "In this section, we report all experiments conducted to evaluate the effectiveness of the proposed semantic word embeddings (SWE).", "labels": [], "entities": [{"text": "semantic word embeddings (SWE)", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.6432613631089529}]}, {"text": "Here we compare the performance of the proposed SWE model with the conventional skip-gram baseline model on four popular natural language processing tasks, including word similarity measure, sentence completion, name entity recognition, and synonym selection.", "labels": [], "entities": [{"text": "SWE", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.910965621471405}, {"text": "sentence completion", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.7620205879211426}, {"text": "name entity recognition", "start_pos": 212, "end_pos": 235, "type": "TASK", "confidence": 0.753533144791921}, {"text": "synonym selection", "start_pos": 241, "end_pos": 258, "type": "TASK", "confidence": 0.9410275220870972}]}, {"text": "In the following, we first describe the experimental setup, training corpora, semantic knowledge databases.", "labels": [], "entities": []}, {"text": "Next, we report the experimental results on these four NLP tasks.", "labels": [], "entities": []}, {"text": "Note that the SWE training codes and scripts are made publicly available at http: //home.ustc.edu.cn/ \u02dc quanliu/.", "labels": [], "entities": [{"text": "SWE training", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.7959635555744171}]}], "tableCaptions": [{"text": " Table 1: Spearman results on the WordSim-353  Task.", "labels": [], "entities": [{"text": "Spearman", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9422929286956787}, {"text": "WordSim-353  Task", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9062698185443878}]}, {"text": " Table 2: Results on Sentence Completion Task.", "labels": [], "entities": [{"text": "Sentence Completion Task", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.9533252914746603}]}, {"text": " Table 3. The nota- tion \"Gaz\" stands for gazetteers that are added into  the NER system as an auxiliary feature. For the  SWE model, we experiment two configurations by  adding gazetteers or not (denoted by \"IsGaz\" and  \"NoGaz\" respectively).", "labels": [], "entities": []}, {"text": " Table 3: F1 scores on the CoNLL03 NER task.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993205070495605}, {"text": "CoNLL03 NER task", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.737903892993927}]}, {"text": " Table 4: The TOEFL synonym selection task.", "labels": [], "entities": [{"text": "TOEFL synonym selection task", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.7656600028276443}]}]}