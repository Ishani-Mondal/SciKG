{"title": [{"text": "Sparse, Contextually Informed Models for Irony Detection: Exploiting User Communities, Entities and Sentiment", "labels": [], "entities": [{"text": "Irony Detection", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7185692340135574}]}], "abstractContent": [{"text": "Automatically detecting verbal irony (roughly, sarcasm) in online content is important for many practical applications (e.g., sentiment detection), but it is difficult.", "labels": [], "entities": [{"text": "Automatically detecting verbal irony (roughly, sarcasm) in online content", "start_pos": 0, "end_pos": 73, "type": "TASK", "confidence": 0.8471532190839449}, {"text": "sentiment detection", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.9716437757015228}]}, {"text": "Previous approaches have relied predominantly on signal gleaned from word counts and grammatical cues.", "labels": [], "entities": []}, {"text": "But such approaches fail to exploit the context in which comments are embedded.", "labels": [], "entities": []}, {"text": "We thus propose a novel strategy for verbal irony classification that exploits contex-tual features, specifically by combining noun phrases and sentiment extracted from comments with the forum type (e.g., conservative or liberal) to which they were posted.", "labels": [], "entities": [{"text": "verbal irony classification", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.839972456296285}]}, {"text": "We show that this approach improves verbal irony classification performance.", "labels": [], "entities": [{"text": "verbal irony classification", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.8380316297213236}]}, {"text": "Furthermore, because this method generates a very large feature space (and we expect predictive contextual features to be strong but few), we propose a mixed regularization strategy that places a sparsity-inducing 1 penalty on the contextual feature weights on top of the 2 penalty applied to all model coefficients.", "labels": [], "entities": []}, {"text": "This increases model sparsity and reduces the variance of model performance.", "labels": [], "entities": [{"text": "variance", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9630590081214905}]}], "introductionContent": [], "datasetContent": [{"text": "For our development dataset, we used a subset of the reddit irony corpus () comprising annotated comments from the progressive and conservative subreddits.", "labels": [], "entities": []}, {"text": "We also report results from experiments performed using a separate, held-out portion of this data, which we did not use during model refinement.", "labels": [], "entities": []}, {"text": "Furthermore, we later present results on comments from the atheism and Christianity subreddits (we did not use this data during model development, either).", "labels": [], "entities": []}, {"text": "The development dataset includes 1,825 annotated comments (876 and 949 from the progressive and conservative subreddits, respectively).", "labels": [], "entities": []}, {"text": "These comprise 5,625 sentences in total, each of which was independently labeled by three annotators as having been intended ironically or not.", "labels": [], "entities": []}, {"text": "For additional details on the annotation process, see).", "labels": [], "entities": []}, {"text": "For simplicity, we consider a sentence to be 'ironic' (y = 1) when at least two of the three annotators designated it as such, and 'unironic' (y = \u22121) otherwise.", "labels": [], "entities": []}, {"text": "Using this criteria, 286 (5%) of the labeled sentences are labeled 'ironic'.", "labels": [], "entities": []}, {"text": "The test portion of the political dataset comprises 996 annotated comments (409 progressive and 587 conservative comments), totalling 2,884 sentences.", "labels": [], "entities": []}, {"text": "Using the same criteria as above -at least 2/3 annotators labeling a given sentence as 'ironic' -we have 154 'ironic' sentences (again about 5%).", "labels": [], "entities": []}, {"text": "The 'religion' dataset (comments from atheism and Christianity) contains 1,682 labeled comments comprising 5615 sentences (2,966 and 2,649 from the atheism and Christian subreddits, respectively); 313 (\u223c6%) were deemed 'ironic'.", "labels": [], "entities": []}, {"text": "We recorded results from 500 independently performed experiments on random train (80%)/test (20%) splits of the data.", "labels": [], "entities": []}, {"text": "These splits were performed at the comment (rather than sentence) level, so as not to test on sentences belonging to comments encountered in the training set.", "labels": [], "entities": []}, {"text": "We measured performance, however, at the sentence level (often only a single sentence in a given comment will have been labeled as 'ironic').", "labels": [], "entities": []}, {"text": "Our baseline approach is a standard squared-2 regularized log-loss linear model (fit via SGD) that leverages uni-and bi-grams and features indicating grammatical cues, such as exclamation points and emoticons.", "labels": [], "entities": []}, {"text": "We also experiment with a model that includes inferred sentiment indicators, but not context.", "labels": [], "entities": []}, {"text": "We performed standard English stopwording, and we used Term Frequency InverseDocument Frequency (TF-IDF) feature weighting.", "labels": [], "entities": []}, {"text": "For the gradient descent procedure, we used a decaying learning rate (specifically, 1 t , where t is the update count).", "labels": [], "entities": [{"text": "gradient descent", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7838935256004333}, {"text": "decaying learning rate", "start_pos": 46, "end_pos": 68, "type": "METRIC", "confidence": 0.7195955316225687}]}, {"text": "We performed a coarse grid search to find values for \u03b1 that maximize F 1 on the training datasets.", "labels": [], "entities": [{"text": "F 1", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9906373918056488}]}, {"text": "We took five full passes over the training data before terminating descent.", "labels": [], "entities": []}, {"text": "We report paired recalls and precisions, as observed on each random train/test split of the data.", "labels": [], "entities": [{"text": "recalls", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9484053254127502}, {"text": "precisions", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9152866005897522}]}, {"text": "The former is defined as T PT P +F N and the latter as T PT P +F P , where T P denotes the true positive count, F N the number of false negatives and F P the false positive count.", "labels": [], "entities": []}, {"text": "We report these separately -rather than collapsing into F 1 -because it is not clear that one would value recall and precision equally for irony detection, and because this allows us to tease out how the models differ in performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9991145730018616}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9907089471817017}, {"text": "irony detection", "start_pos": 139, "end_pos": 154, "type": "TASK", "confidence": 0.9452680349349976}]}, {"text": "Notably, for example, sentiment and context features both improve recall, but the latter does so without harming precision. and Table 2 summarize the performance of the different approaches over 500 independently performed train/test splits of the political development corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9987390637397766}, {"text": "precision.", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9977802634239197}]}, {"text": "For reference, a random chance strategy (which predicts 'ironic' with probability equal to the observed prevalence) achieves a median recall of 0.048 and a median precision of 0.047.", "labels": [], "entities": [{"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9687634706497192}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.804512619972229}]}, {"text": "shows histograms of the observed absolute differences between the baseline linear clas- sifier and the proposed augmentations.", "labels": [], "entities": []}, {"text": "Adding the proposed features (which capitalize on sentiment and NNP-mentions on specific subreddits) increases absolute median recall by 3.4 percentage points (a relative gain of \u223c12%).", "labels": [], "entities": [{"text": "absolute median", "start_pos": 111, "end_pos": 126, "type": "METRIC", "confidence": 0.8589898645877838}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.7741250991821289}]}, {"text": "And this is achieved without sacrificing precision (in contrast to exploiting only sentiment).", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9965168237686157}]}, {"text": "Furthermore, as we can see in, the proposed regularization strategy shrinks the variance of the classifier.", "labels": [], "entities": []}, {"text": "This variance reduction is achieved through greater model sparsity, as can be seen in, which improves interpretability.", "labels": [], "entities": []}, {"text": "We note that leveraging only an 1 regularization penalty (with the full feature-set) results in very poor performance (median recall and precision of 0.05 and 0.09, respectively).", "labels": [], "entities": [{"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9564180374145508}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.997089684009552}]}, {"text": "Similarly, the elastic-net strategy () (in which we do not specify which features to apply the 1 penalty to), here achieves a median recall of 0.11 and a median precision of 0.07.", "labels": [], "entities": [{"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9596934914588928}, {"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.8279709815979004}]}, {"text": "reports results on the held-out political test dataset, achieved after training the models on the entirety of the development corpus.", "labels": [], "entities": [{"text": "political test dataset", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.7310267488161722}]}, {"text": "To account for the variance inherent to inference via SGD, we performed 100 runs of the SGD procedure and report median results from these runs.", "labels": [], "entities": []}, {"text": "These results mostly agree with those reported for the development corpus: the proposed strategy improves median recall on the held-out corpus by nearly 4.0 percentage points, at a median cost of about 1 point in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9332904815673828}, {"text": "precision", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.9973944425582886}]}, {"text": "By contrast, sentiment alone provides a 2% absolute improvement in recall at +0.010; +0.000 (+0.000, +0.021) -0.002; -0.002 (-0.007, +0.004) NNP+ \u00d7 sent.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.999247670173645}]}, {"text": "To assess the general applicability of the proposed approach, we also evaluate the method on comments from a separate pair of polarized communities: atheism and Christianity, as described in Section 4.1.", "labels": [], "entities": []}, {"text": "This dataset was not used during model development.", "labels": [], "entities": []}, {"text": "We follow the experimental setup described in Section 4.2.", "labels": [], "entities": []}, {"text": "In this case, capitalizing on the NNP \u00d7 subreddit features produces a mean 2.3% absolute gain in recall (median: 2.4%) over the baseline approach, with a (very) slight gain in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9954413175582886}, {"text": "precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9983333945274353}]}, {"text": "The 1 2 approach achieves a lower expected gain in recall (median: 1.5%), but again shrinks the variance w.r.t. model performance (see.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.999525785446167}]}, {"text": "Moreover, as we show in, this is achieved with a much more compact (sparser) model.", "labels": [], "entities": []}, {"text": "We note that for the religion data, inferred sentiment features do not seem to improve performance, in contrast to the results on the political subreddits.", "labels": [], "entities": []}, {"text": "At present, we are not sure why this is the case.", "labels": [], "entities": []}, {"text": "These results demonstrate that introducing features that encode entities and user communities (NNPs \u00d7 subreddit) improve recall for irony detection in comments addressing relatively diverse topics (politics and religion).", "labels": [], "entities": [{"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9988023042678833}, {"text": "irony detection in comments addressing relatively diverse topics (politics and religion)", "start_pos": 132, "end_pos": 220, "type": "TASK", "confidence": 0.7382602462401757}]}], "tableCaptions": [{"text": " Table 1. The \u00d7 symbol denotes interactions; + indicates  addition. The proposed contextual features substantially improve recall, with little to no loss in precision. Moreover, in general,  the 12 regularization approach reduces variance. (We note that in constructing histograms we have excluded a handful of  points -never more than 1% -where the difference exceeded 0.15).", "labels": [], "entities": [{"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9989551305770874}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9980935454368591}]}, {"text": " Table 4: Results on the held-out political dataset, using the entire development corpus as a training set. Abbreviations are as  described in the caption for", "labels": [], "entities": [{"text": "Abbreviations", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.965395450592041}]}]}