{"title": [{"text": "Open IE as an Intermediate Structure for Semantic Tasks", "labels": [], "entities": [{"text": "Open IE", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.49983255565166473}]}], "abstractContent": [{"text": "Semantic applications typically extract information from intermediate structures derived from sentences, such as dependency parse or semantic role labeling.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7323367595672607}, {"text": "semantic role labeling", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.5932983855406443}]}, {"text": "In this paper , we study Open Information Extrac-tion's (Open IE) output as an additional intermediate structure and find that for tasks such as text comprehension, word similarity and word analogy it can be very effective.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.7794721722602844}, {"text": "word analogy", "start_pos": 185, "end_pos": 197, "type": "TASK", "confidence": 0.7897189259529114}]}, {"text": "Specifically, for word analogy, Open IE-based embeddings surpass the state of the art.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.781874030828476}]}, {"text": "We suggest that semantic applications will likely benefit from adding Open IE format to their set of potential sentence-level structures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic applications, such as QA or summarization, typically extract sentence features from a derived intermediate structure.", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9371545314788818}]}, {"text": "Common intermediate structures include: (1) Lexical representations, in which features are extracted from the original word sequence or the bag of words, (2) Stanford dependency parse trees, which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments).", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 245, "end_pos": 273, "type": "TASK", "confidence": 0.8039316634337107}]}, {"text": "For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (), by using short dependency paths as features to compare their syntactic relationships (, or by using SRL to compare their predicate-argument structures.", "labels": [], "entities": []}, {"text": "Ina seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases ().", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE) extracts coherent propositions from a sentence", "start_pos": 46, "end_pos": 130, "type": "TASK", "confidence": 0.8286960812715384}]}, {"text": "We observe that while Open IE is primarily used as an end goal in itself (e.g.,), it also makes certain structural design choices which differ from those made by dependency or SRL.", "labels": [], "entities": []}, {"text": "For example, Open IE chooses different predicate and argument boundaries and assigns different relations between them.", "labels": [], "entities": []}, {"text": "Given the differences between Open IE and other intermediate structures (see Section 2), a research question arises: Can certain downstream applications gain additional benefits from utilizing Open IE structures?", "labels": [], "entities": []}, {"text": "To answer this question we quantitatively evaluate the use of Open IE output against other dominant structures (Sections 3 and 4).", "labels": [], "entities": []}, {"text": "For each of text comprehension, word similarity and word analogy tasks, we choose a state-of-the-art algorithm in which we can easily swap the intermediate structure while preserving the algorithmic computations over the features extracted from it.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7736519575119019}, {"text": "word analogy", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.7102946043014526}]}, {"text": "We find that in several tasks Open IE substantially outperforms other structures, suggesting that it can provide an additional set of useful sentence-level features.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.5898391753435135}]}], "datasetContent": [{"text": "In our experiments we use MaltParser () for dependency parsing, and ClearNLP (Choi and Palmer, 2011) for SRL.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9004694819450378}, {"text": "SRL", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.7317001223564148}]}, {"text": "To obtain Open-IE structures, we use the recent Open IE-4 system 2 which produces n-ary extractions of both verb-based relation phrases using SRLIE (an improvement over) and nominal relations using regular expressions.", "labels": [], "entities": [{"text": "SRLIE", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.7067927718162537}]}, {"text": "SRLIE first processes sentences using SRL and then uses hand-coded rules to convert SRL frames and associated dependency parses to open extractions.", "labels": [], "entities": [{"text": "SRLIE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8647233843803406}, {"text": "SRL frames and associated dependency parses", "start_pos": 84, "end_pos": 127, "type": "TASK", "confidence": 0.5650366296370825}]}, {"text": "We choose these tools as they are on par with state-of-the-art in their respective fields, and therefore represent the current available off-the-shelf intermediate structures for semantic applications.", "labels": [], "entities": []}, {"text": "Furthermore, Open IE-4 is based on ClearNLP's SRL, allowing fora direct comparison.", "labels": [], "entities": [{"text": "ClearNLP's SRL", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9238843321800232}]}, {"text": "For SRL systems, we take argument boundaries as their complete parse subtrees.", "labels": [], "entities": [{"text": "SRL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9704766273498535}]}, {"text": "Results on Text Comprehension Task We report results (in percentage of correct answers) on the whole of MC500 dataset (ignoring train-devtest split) since all our methods are unsupervised.", "labels": [], "entities": [{"text": "Text Comprehension Task", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8007404406865438}, {"text": "MC500 dataset", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.9858729839324951}]}, {"text": "shows the accuracies obtained on the multiple-choice questions, categorized by single (the question can be answered based on a sin-   gle story sentence) , multiple (multiple sentences needed) and all (single + multiple).", "labels": [], "entities": []}, {"text": "In this task, we find that Open IE and dependency edges substantially outperform lexical and SRL.", "labels": [], "entities": []}, {"text": "We conjecture that SRL's weak performance is due to its treatment of infinitives and multi-word predicates as different propositions (see Section 2).", "labels": [], "entities": [{"text": "SRL", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9715588092803955}]}, {"text": "This adds noise by wrongly counting partial matching between predications, as exemplified in.", "labels": [], "entities": []}, {"text": "The gain over the lexical approach can be explained by the ability to capture longer range relations than the fixed size window.", "labels": [], "entities": []}, {"text": "In our results Open IE slightly improves over dependency.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.354745015501976}]}, {"text": "This can be traced back to the different structural choices depicted in Section 2 -Open IE counts matches at the proposition level while the dependency variant may count path matches over unrelated sentence parts.", "labels": [], "entities": []}, {"text": "The differences between the performance of Open IE and all other systems were found to be statistically significant (p < 0.01).", "labels": [], "entities": [{"text": "Open IE", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.7336670756340027}]}, {"text": "Results on Similarity and Analogy Tasks For these tasks, we train the various word embeddings As expected, all sentence-level intermediate structures perform best on the single partition, yet results show that some of the questions from the multiple partition may also be answered correctly using information from a single sentence.", "labels": [], "entities": []}, {"text": "We experimented with various window sizes and found that window size of the length of the current candidateanswer performed best.", "labels": [], "entities": []}, {"text": "on a Wikipedia dump (August 2013 dump), containing 77.5M sentences and 1.5B tokens.", "labels": [], "entities": [{"text": "Wikipedia dump (August 2013 dump)", "start_pos": 5, "end_pos": 38, "type": "DATASET", "confidence": 0.9066702808652606}]}, {"text": "We used the default hyperparameters from: 300 dimensions, skip gram with negative sampling of size 5.", "labels": [], "entities": []}, {"text": "Lexical embeddings were trained with 5-gram contexts.", "labels": [], "entities": []}, {"text": "Performance is measured using Spearman's \u03c1, in order to assess the correlation of the predictions to the gold annotations, rather than comparing their values directly.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.5930538574854533}]}, {"text": "compares the results on the word similarity task using cosine similarity between embeddings as the similarity predictor.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.789667010307312}]}, {"text": "For the ws353 test set we report results on the whole corpus (full) as well as on the partition suggested by) into relatedness (mainly meronymholonym) and similarity (synonyms, antonyms, or hyponym-hypernym).", "labels": [], "entities": [{"text": "ws353 test set", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8963025212287903}]}, {"text": "We find that Open IE-based embeddings consistently do well; performing best across all test sets, except for simlex999.", "labels": [], "entities": []}, {"text": "Analysis reveals that Open IE's ability to represent multi-word predicates and arguments allows it to naturally incorporate both notions of similarity.", "labels": [], "entities": []}, {"text": "Context words originating from the same Open IE slot (either predicate or argument) are lexically close and indicate domainsimilarity, whereas context words from other elements in the tuple express semantic relationships, and target functional similarity.", "labels": [], "entities": []}, {"text": "Thus, Open IE performs better on word-pairs which exhibit both topical and functional similarity, such as (latinist, classicist), or (provincialism, narrow-mindedness), which were taken from the Luong test set.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 6, "end_pos": 13, "type": "TASK", "confidence": 0.6083599328994751}, {"text": "Luong test set", "start_pos": 195, "end_pos": 209, "type": "DATASET", "confidence": 0.815944661696752}]}, {"text": "further illustrates this dual capturing of both types of similarity in Open IE space.", "labels": [], "entities": []}, {"text": "Our results also reiterate previous findingslexical contexts do well on domain-similarity test sets ().", "labels": [], "entities": []}, {"text": "The results on the simlex999 test set can be explained by its focus on functional similarity, previously identified as better captured by dependency contexts ().", "labels": [], "entities": [{"text": "simlex999 test set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.7729577620824178}]}, {"text": "For the Word analogy task we use the Google () and the Microsoft corpora (), which are composed of \u223c 195K and 8K instances respectively.", "labels": [], "entities": [{"text": "Word analogy", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.7238229364156723}, {"text": "Google", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9331697225570679}]}, {"text": "We obtain the analogy vectors using both the additive and multiplicative measures (.", "labels": [], "entities": []}, {"text": "shows the results -Open IE obtains the best accuracies by vast margins (p < 0.01), for reasons simi- lar to the word similarity tasks.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.5800295472145081}]}, {"text": "To our knowledge, Open IE results on both analogy datasets surpass the state of the art.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.5661600232124329}]}, {"text": "An example (from the Microsoft test set) which supports the observation regarding Open IE embeddings space is (gentlest:gentler, loudest:?), for which only Open IE answers correctly as louder, while lexical respond with higher-pitched (domain similar to loudest), and dependency with thinnest (functionally similar to loudest).", "labels": [], "entities": [{"text": "Microsoft test set", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.8079007466634115}]}, {"text": "Our Open-IE embeddings are freely available and we note that these can serve as plug-in features for other NLP applications, as demonstrated in ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance in word similarity tasks  (Spearman's \u03c1)", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7767144441604614}]}, {"text": " Table 3: Performance in word analogy tasks (per- centage of correct answers)", "labels": [], "entities": [{"text": "word analogy tasks", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8538455764452616}]}]}