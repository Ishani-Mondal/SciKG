{"title": [{"text": "A Data-Driven, Factorization Parser for CCG Dependency Structures", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model.", "labels": [], "entities": []}, {"text": "Three types of fac-torization together with different higher-order features are designed to capture different syntacto-semantic properties of functor-argument dependencies.", "labels": [], "entities": []}, {"text": "Integrating heterogeneous factorizations results in intractability in decoding.", "labels": [], "entities": []}, {"text": "We propose a principled method to obtain optimal graphs based on dual decomposition.", "labels": [], "entities": []}, {"text": "Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result.", "labels": [], "entities": [{"text": "f-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.939798891544342}, {"text": "CCGBank data", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9903374314308167}, {"text": "error reduction", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.9825088381767273}]}, {"text": "which yields a significant improvement over the best published result in the literature.", "labels": [], "entities": []}, {"text": "Our implementation is available at http://www.icst.", "labels": [], "entities": []}, {"text": "pku.edu.cn/lcwm/grass.", "labels": [], "entities": [{"text": "pku.edu.cn/lcwm/grass", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.8683554291725158}]}], "introductionContent": [{"text": "Combinatory Categorial) is a linguistically expressive grammar formalism which has a transparent yet elegant interface between syntax and semantics.", "labels": [], "entities": []}, {"text": "By assigning each lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (), providing a useful approximation to the underlying meaning representations.", "labels": [], "entities": []}, {"text": "To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information).", "labels": [], "entities": []}, {"text": "Such semantic-oriented dependency structures have been shown very helpful for NLP ap- * Email correspondence.", "labels": [], "entities": [{"text": "NLP ap- * Email correspondence", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.5578407297531763}]}, {"text": "plications e.g. Question Answering ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7271191775798798}]}, {"text": "Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers.", "labels": [], "entities": []}, {"text": "The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity.", "labels": [], "entities": []}, {"text": "Robustness and efficiency, thus, are two major problems for handling practical tasks.", "labels": [], "entities": [{"text": "efficiency", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9717900156974792}]}, {"text": "To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary ().", "labels": [], "entities": []}, {"text": "In the past decade, the techniques for datadriven dependency parsing has made a great progress (.", "labels": [], "entities": [{"text": "datadriven dependency parsing", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.6405620574951172}]}, {"text": "The major advantage of the data-driven architecture is complementary to the grammardriven one.", "labels": [], "entities": []}, {"text": "On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences.", "labels": [], "entities": []}, {"text": "On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective and 1-endpoint-crossing trees, can be of low complexity.", "labels": [], "entities": []}, {"text": "This paper proposes anew data-driven dependency parser that efficiently produces globally optimal CCG dependency graphs according to a discriminative, factorization model.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7203817814588547}]}, {"text": "The design of the factorization is motivated by three essential properties of the CCG dependencies.", "labels": [], "entities": []}, {"text": "First, all arguments associated with the same predicate are highly correlated due to the nature that they approximates type-logical semantics.", "labels": [], "entities": []}, {"text": "Second, all predicates govern the same argument exhibit the hybrid syntactic/semantic, i.e. head-complementadjunct, relationships.", "labels": [], "entities": []}, {"text": "Finally, the CCG dependency graphs are not but look very much like trees, which have many good computational properties.", "labels": [], "entities": []}, {"text": "Simultaneously modeling the three properties yields intrinsically heterogeneous factorizations over the same graph, and hence results in intractability in decoding.", "labels": [], "entities": []}, {"text": "Inspired by ( , we employ dual decomposition to perform principled decoding.", "labels": [], "entities": []}, {"text": "Though not always, we can obtain the optimal solution most of time.", "labels": [], "entities": []}, {"text": "The time complexity of our parser is O(n 3 ) when various 1st-and 2nd-order features are incorporated.", "labels": [], "entities": [{"text": "O", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9907958507537842}]}, {"text": "We conduct experiments on English CCGBank.", "labels": [], "entities": [{"text": "English CCGBank", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9231616258621216}]}, {"text": "Though our parser does not use any grammar information, including both lexical categories and syntactic derivations, it produces very accurate CCG dependency graphs with respect to both token and complete matching.", "labels": [], "entities": []}, {"text": "Our parser obtains an unlabeled f-score of 93.23, resulting in, perhaps surprisingly, an error reduction of up to 6.5% over the best published performance reported in (Auli and).", "labels": [], "entities": [{"text": "f-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9505535960197449}, {"text": "error reduction", "start_pos": 89, "end_pos": 104, "type": "METRIC", "confidence": 0.9820610284805298}]}, {"text": "Our work indicates that highquality data-driven parsers can be built for producing more general dependency graphs, rather than trees.", "labels": [], "entities": []}, {"text": "Nevertheless, empirical evaluation indicates that explicitly or implicitly using tree-structured information plays an essential role.", "labels": [], "entities": []}, {"text": "The result also suggests that a wider range of complicated linguistic phenomena beyond surface syntax can be well modeled even without explicitly using grammars.", "labels": [], "entities": []}, {"text": "Our algorithm is also applicable to other graphstructured representations, e.g. HPSG predicateargument analysis).", "labels": [], "entities": [{"text": "HPSG predicateargument analysis", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6954737901687622}]}], "datasetContent": [{"text": "CCGbank is a translation of the Penn Treebank into a corpus of CCG derivations (Hockenmaier.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9318671822547913}, {"text": "Penn Treebank", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9943242371082306}, {"text": "Hockenmaier.", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.9410661160945892}]}, {"text": "CCGbank pairs syntactic derivations with sets of word-word dependencies which approximate the underlying functorargument structure.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9000157117843628}]}, {"text": "Our experiments were performed using CCGBank which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23).", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9537639617919922}]}, {"text": "We also use the syntactic dependency trees provided by the CCGBank to obtain necessary information for graph parsing.", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9634314179420471}, {"text": "graph parsing", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.6628511995077133}]}, {"text": "However, different from experiments in the CCG parsing literature, we use no grammar information.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.741865873336792}]}, {"text": "Neither lexical categories nor CCG derivations are utilized.", "labels": [], "entities": []}, {"text": "All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger, and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in () as well as a 2nd-order graph-based parser 2.", "labels": [], "entities": []}, {"text": "The accuracy of these preprocessors is shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996730089187622}]}, {"text": "We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times.", "labels": [], "entities": []}, {"text": "For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion.", "labels": [], "entities": []}, {"text": "Previous research on dependency parsing shows that structured perceptron) is one of the strongest discriminative learning algorithms.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.836069643497467}]}, {"text": "To estimate \u03b8's of different models, we utilize the averaged perceptron algorithm.", "labels": [], "entities": []}, {"text": "We implement our own the predicate-and argument-centric models.", "labels": [], "entities": []}, {"text": "To perform tree parsing, we re-use the open-source implementation provided by the mate-tool.", "labels": [], "entities": [{"text": "tree parsing", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.6695823669433594}]}, {"text": "See the source code attached for details.", "labels": [], "entities": []}, {"text": "We set iteration 5 to train predicate-and argument-centric models and 10 for the tree approximation model.", "labels": [], "entities": []}, {"text": "To perform dual decomposition, we set the maximum iteration 200.: Parsing performance on the development data.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.888491690158844}]}, {"text": "The column \"Tree\" denotes the parsers that give dependency tree of development set: no tree (No), transition-based (Tr) or graph-based (Gr).", "labels": [], "entities": []}, {"text": "\"PC,\" \"AC\" and \"TA\" in the second column denotes the predicate-centric, the argument-centric and the tree approximation models, respectively.", "labels": [], "entities": [{"text": "TA", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9890022277832031}]}, {"text": "summarizes parsing performance on development set with different configurations.", "labels": [], "entities": []}, {"text": "We report unlabeled precision (UP), recall (UR), fscore (UF) as well as complete match (UEM).", "labels": [], "entities": [{"text": "precision (UP)", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.9575541317462921}, {"text": "recall (UR)", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9545896202325821}, {"text": "fscore (UF)", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9599119573831558}, {"text": "complete match (UEM)", "start_pos": 72, "end_pos": 92, "type": "METRIC", "confidence": 0.9203614830970764}]}, {"text": "It can be clearly seen that the data-driven models obtains high-quality graphs with respect to token match.", "labels": [], "entities": []}, {"text": "Even without any syntactic information (see the top block associated with \"No Tree\"), our parser with all three factorization models obtains an f-score of 92.5.", "labels": [], "entities": []}, {"text": "when assisted by a syntactic parser, this figure goes up to over 93.1.", "labels": [], "entities": []}, {"text": "If the predicate-or argument-centric model is applied by itself, either one can achieve a competitive accuracy, especially when syntactic features are utilized.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9980294108390808}]}], "tableCaptions": [{"text": " Table 1: The accuracy of the POS tagger and the  UAS of the syntax tree parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996674060821533}, {"text": "POS tagger", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.6567216962575912}, {"text": "UAS", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9868654608726501}]}, {"text": " Table 2: Parsing performance on the development  data. The column \"Tree\" denotes the parsers that  give dependency tree of development set: no tree  (No), transition-based (Tr) or graph-based (Gr).  \"PC,\" \"AC\" and \"TA\" in the second column de- notes the predicate-centric, the argument-centric  and the tree approximation models, respectively.", "labels": [], "entities": []}, {"text": " Table 3: Comparing the state-of-art with our mod- els on test set.", "labels": [], "entities": []}]}