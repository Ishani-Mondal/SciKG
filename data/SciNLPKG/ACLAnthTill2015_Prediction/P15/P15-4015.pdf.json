{"title": [{"text": "LEXenstein: A Framework for Lexical Simplification", "labels": [], "entities": [{"text": "LEXenstein", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7382779121398926}, {"text": "Lexical Simplification", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8610311448574066}]}], "abstractContent": [{"text": "Lexical Simplification consists in replacing complex words in a text with simpler alternatives.", "labels": [], "entities": [{"text": "Lexical Simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9150223135948181}]}, {"text": "We introduce LEXen-stein, the first open source framework for Lexical Simplification.", "labels": [], "entities": [{"text": "Lexical Simplification", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8875240087509155}]}, {"text": "It covers all major stages of the process and allows for easy benchmarking of various approaches.", "labels": [], "entities": []}, {"text": "We test the tool's performance and report comparisons on different datasets against the state of the art approaches.", "labels": [], "entities": []}, {"text": "The results show that combining the novel Substitution Selection and Substitution Ranking approaches introduced in LEXenstein is the most effective approach to Lexical Simplification.", "labels": [], "entities": [{"text": "Lexical Simplification", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.9123979210853577}]}], "introductionContent": [{"text": "The goal of a Lexical Simplification (LS) approach is to replace complex words and expressions in a given text, often a sentence, with simpler alternatives of equivalent meaning in context.", "labels": [], "entities": [{"text": "Lexical Simplification (LS)", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.8692043423652649}]}, {"text": "Although very intuitive, this is a challenging task since the substitutions must preserve both the original meaning and the grammaticality of the sentence being simplified.", "labels": [], "entities": []}, {"text": "The LS task has been gaining significant attention since the late 1990's, thanks to the positive influence of the early work presented by  and).", "labels": [], "entities": [{"text": "LS task", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.8699747025966644}]}, {"text": "More recently, the LS task at SemEval-2012 ( ) has given LS wider visibility.", "labels": [], "entities": [{"text": "LS task at SemEval-2012", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7295096069574356}]}, {"text": "Participants had the opportunity to compare their approaches in the task of ranking candidate substitutions, all of which were already known to fit the context, according to their \"simplicity\".", "labels": [], "entities": []}, {"text": "Despite its growth in popularity, the inexistence of tools to support the process and help researchers to build upon has been hampering progress in the area.", "labels": [], "entities": []}, {"text": "We were only able to find one tool for LS: a set of scripts designed for the training and testing of ranking models provided by (Jauhar and Specia, 2012) . However, they cover only one step of the process.", "labels": [], "entities": [{"text": "LS", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9726729989051819}]}, {"text": "In an effort to tackle this issue, we present LEXenstein: a framework for Lexical Simplification development and benchmarking.", "labels": [], "entities": [{"text": "Lexical Simplification development", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.9181747039159139}]}, {"text": "LEXenstein is an easy-to-use framework that provides simplified access to many approaches for several sub-tasks of the LS pipeline, which is illustrated in.", "labels": [], "entities": [{"text": "LEXenstein", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7830392122268677}]}, {"text": "Its current version includes methods for the three main sub-tasks in the pipeline: Substitution Generation, Substitution Selection and Substitution Ranking.", "labels": [], "entities": [{"text": "Substitution Generation", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.9544143378734589}, {"text": "Substitution Selection", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.9276969134807587}]}], "datasetContent": [{"text": "Since one of the goals of LEXenstein is to facilitate the benchmarking LS approaches, it is crucial that it provides evaluation methods.", "labels": [], "entities": []}, {"text": "This module includes functions for the evaluation of all subtasks, both individually and in combination.", "labels": [], "entities": []}, {"text": "It contains four Python classes: GeneratorEvaluator: Provides evaluation metrics for SG methods.", "labels": [], "entities": []}, {"text": "It requires a gold-standard in the VICTOR format and a set of generated substitutions.", "labels": [], "entities": [{"text": "VICTOR format", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.836810827255249}]}, {"text": "It returns the Potential, Precision and F-measure, where Potential is the proportion of instances for which at least one of the substitutions generated is present in the gold-standard, Precision the proportion of generated instances which are present in the gold-standard, and F-measure their harmonic mean.", "labels": [], "entities": [{"text": "Potential", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9878095388412476}, {"text": "Precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9840938448905945}, {"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9969353675842285}, {"text": "Potential", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9765204787254333}, {"text": "F-measure", "start_pos": 277, "end_pos": 286, "type": "METRIC", "confidence": 0.9935652613639832}]}, {"text": "SelectorEvaluator: Provides evaluation metrics for SS methods.", "labels": [], "entities": []}, {"text": "It requires a gold-standard in the VICTOR format and a set of selected substitutions.", "labels": [], "entities": [{"text": "VICTOR format", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.7957731783390045}]}, {"text": "It returns the Potential, Precision and Fmeasure of the SS approach, as defined above.", "labels": [], "entities": [{"text": "Potential", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9994490742683411}, {"text": "Precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9973403811454773}, {"text": "Fmeasure", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9973107576370239}]}, {"text": "RankerEvaluator: Provides evaluation metrics for SR methods.", "labels": [], "entities": [{"text": "SR", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9669877290725708}]}, {"text": "It requires a gold-standard in the VICTOR format and a set of ranked substitutions.", "labels": [], "entities": [{"text": "VICTOR format", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8151303231716156}]}, {"text": "It returns the TRank-at-1:3 and Recall-at-1:3 metrics ( , where Trank-at-i is the proportion of instances for which a candidate of gold-rank r \u2264 i was ranked first, and Recall-at-i the proportion of candidates of gold-rank r \u2264 i that are ranked in positions p \u2264 i.", "labels": [], "entities": [{"text": "TRank-at-1:3", "start_pos": 15, "end_pos": 27, "type": "METRIC", "confidence": 0.932861328125}, {"text": "Recall-at-1:3", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9982038736343384}, {"text": "Trank-at-i", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9939184188842773}, {"text": "Recall-at-i", "start_pos": 169, "end_pos": 180, "type": "METRIC", "confidence": 0.9154948592185974}]}, {"text": "PipelineEvaluator: Provides evaluation metrics for the entire LS pipeline.", "labels": [], "entities": []}, {"text": "It requires as input a gold-standard in the VICTOR format and a set of ranked substitutions which have been generated and selected by a given set of approaches.", "labels": [], "entities": [{"text": "VICTOR format", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8511961996555328}]}, {"text": "It returns the approaches' Precision, Accuracy and Change Proportion, where Precision is the proportion of instances for which the highest ranking substitution is not the target complex word itself and is in the gold-standard, Accuracy is the proportion of instances for which the highest ranking substitution is in the gold-standard, and Change Proportion is the proportion of instances for which the highest ranking substitution is not the target complex word itself.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9909366965293884}, {"text": "Precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9748503565788269}, {"text": "Accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9979793429374695}]}, {"text": "In this Section, we discuss the results obtained in four benchmarking experiments.", "labels": [], "entities": []}, {"text": "In this experiment we evaluate the performance of different combinations of SS and SR approaches in selecting suitable substitutions for complex words from the ones produced by all generators combined.", "labels": [], "entities": []}, {"text": "Rankers and selectors are configured in the same way as they were in the experiments in Sections 3.3 and 3.2.", "labels": [], "entities": []}, {"text": "The gold-standard used is LexMturk, and the performance metric used is the combination's Precision: the proportion of times in which the candidate ranked highest is not the target complex word itself and belongs to the goldstandard list.", "labels": [], "entities": [{"text": "LexMturk", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.937930166721344}, {"text": "Precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9974374771118164}]}, {"text": "Results are shown in  the highest performance in the pipeline evaluation.", "labels": [], "entities": []}, {"text": "Interestingly, the SVMRanker, which performed very well in the individual evaluation of Section 3.3, was outperformed by all three baselines in this experiment.", "labels": [], "entities": [{"text": "SVMRanker", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.8627005815505981}]}], "tableCaptions": [{"text": " Table 1: SG benchmarking results", "labels": [], "entities": [{"text": "SG benchmarking", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8473561406135559}]}, {"text": " Table 2: SS benchmarking results", "labels": [], "entities": [{"text": "SS", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.573066234588623}]}, {"text": " Table 3: SR benchmarking results", "labels": [], "entities": [{"text": "SR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.5279789566993713}]}, {"text": " Table 4: Round-trip benchmarking results", "labels": [], "entities": []}]}