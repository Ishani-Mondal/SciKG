{"title": [{"text": "Exploiting Image Generality for Lexical Entailment Detection", "labels": [], "entities": [{"text": "Exploiting Image Generality", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.773204505443573}, {"text": "Lexical Entailment Detection", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.889420747756958}]}], "abstractContent": [{"text": "We exploit the visual properties of concepts for lexical entailment detection by examining a concept's generality.", "labels": [], "entities": [{"text": "lexical entailment detection", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7868336141109467}]}, {"text": "We introduce three unsupervised methods for determining a concept's generality, based on its related images, and obtain state-of-the-art performance on two standard semantic evaluation datasets.", "labels": [], "entities": []}, {"text": "We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequency-based baseline.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.693619042634964}]}], "introductionContent": [{"text": "Automatic detection of lexical entailment is useful fora number of NLP tasks including search query expansion (, recognising textual entailment (), metaphor detection (, and text generation.", "labels": [], "entities": [{"text": "Automatic detection of lexical entailment", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7736888647079467}, {"text": "search query expansion", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6152240435282389}, {"text": "metaphor detection", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.9189944863319397}, {"text": "text generation", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.7805272936820984}]}, {"text": "Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym.", "labels": [], "entities": [{"text": "detecting lexical entailment", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.8886115749677023}]}, {"text": "For example, bird is more general than eagle, having a broader intension and a larger extension.", "labels": [], "entities": []}, {"text": "This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution.", "labels": [], "entities": []}, {"text": "A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics ().", "labels": [], "entities": []}, {"text": "Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a range of tasks, including modelling semantic similarity and conceptual relatedness).", "labels": [], "entities": []}, {"text": "In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks.", "labels": [], "entities": []}, {"text": "We hypothesize that visual representations can be particularly useful for lexical entailment detection.", "labels": [], "entities": [{"text": "lexical entailment detection", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.8039528528849283}]}, {"text": "have shown that sets of images corresponding to terms at higher levels in the WordNet hierarchy have greater visual variability than those at lower levels.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9025323987007141}]}, {"text": "We exploit this tendency using sets of images returned by Google's image search.", "labels": [], "entities": []}, {"text": "The intuition is that the set of images returned for animal will consist of pictures of different kinds of animals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consist only of images of owls, as can be seen in.", "labels": [], "entities": []}, {"text": "Here we evaluate three different vision-based methods for measuring term generality on the semantic tasks of hypernym detection and hypernym directionality.", "labels": [], "entities": [{"text": "term generality", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.6909608393907547}, {"text": "hypernym detection", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7036145478487015}, {"text": "hypernym directionality", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.7435503602027893}]}, {"text": "Using this simple yet effective unsupervised approach, we obtain state-of-the-art results compared with supervised algorithms which use linguistic data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Examples for evaluation datasets.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy. For WBLESS and BIBLESS we  report results for multi-modal \u00b5 c , with visual-only  \u00b5 c in brackets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987562894821167}, {"text": "WBLESS", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.8036841154098511}, {"text": "BIBLESS", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.71028071641922}]}]}