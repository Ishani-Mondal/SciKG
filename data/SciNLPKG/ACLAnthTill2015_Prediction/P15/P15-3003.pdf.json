{"title": [{"text": "Learning representations for text-level discourse parsing", "labels": [], "entities": [{"text": "text-level discourse parsing", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.621750275293986}]}], "abstractContent": [{"text": "In the proposed doctoral work we will design an end-to-end approach for the challenging NLP task of text-level discourse parsing.", "labels": [], "entities": [{"text": "text-level discourse parsing", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.5994536578655243}]}, {"text": "Instead of depending on mostly hand-engineered sparse features and independent components for each subtask, we propose a unified approach completely based on deep learning architectures.", "labels": [], "entities": []}, {"text": "To train more expressive representations that capture communicative functions and semantic roles of discourse units and relations between them, we will jointly learn all discourse parsing subtasks at different layers of our architecture and share their intermediate representations.", "labels": [], "entities": [{"text": "discourse parsing subtasks", "start_pos": 170, "end_pos": 196, "type": "TASK", "confidence": 0.8189960916837057}]}, {"text": "By combining unsupervised training of word embed-dings with our layer-wise multi-task learning of higher representations we hope to reach or even surpass performance of current state-of-the-art methods on annotated English corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern algorithms for natural language processing (NLP) are based on statistical machine learning and require a computationally convenient representation of input data.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.8189551730950674}]}, {"text": "Unfortunately real-world plain text is usually represented as an unstructured sequence of words with complex relations between them.", "labels": [], "entities": []}, {"text": "Therefore it is extremely important to discover good representations in the form of informative text features.", "labels": [], "entities": []}, {"text": "In NLP such features are almost always handengineered sparse features and require expensive human labor and expert knowledge to construct.", "labels": [], "entities": []}, {"text": "They are usually based on lexicons or features extracted by other NLP subtasks and have the form of hand-engineered extraction rules, regular expressions, lemmatization, part-of-speech (POS) tags, positions or lengths of arguments, tense forms, syntactic parse trees, and similar.", "labels": [], "entities": []}, {"text": "Although such features are specific fora given language, domain, and task, they work well enough for simple NLP tasks, like named entity recognition or POS tagging.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.6116401553153992}, {"text": "POS tagging", "start_pos": 152, "end_pos": 163, "type": "TASK", "confidence": 0.7754799425601959}]}, {"text": "Nevertheless, the ability to learn text features and representations automatically would have a lot of potential to improve state-of-the-art performance on more challenging NLP tasks, such as text-level discourse parsing.", "labels": [], "entities": [{"text": "text-level discourse parsing", "start_pos": 192, "end_pos": 220, "type": "TASK", "confidence": 0.5931504766146342}]}, {"text": "This may even be more important for languages where progress in NLP is still lacking.", "labels": [], "entities": []}, {"text": "Variants of deep learning architectures have been shown to provide a different approach to learning in which latent features are automatically learned as distributed dense vectors.", "labels": [], "entities": []}, {"text": "They managed to represent meaningful relations with word), POS and dependency tag), sentence, and document) embeddings and achieved surprising results fora number of NLP tasks.", "labels": [], "entities": []}, {"text": "It has been shown that both unsupervised pre-training () and multitask learning) significantly improve their performance in the absence of hand-engineered features.", "labels": [], "entities": []}, {"text": "This makes them especially interesting for the problem of text-level discourse parsing.", "labels": [], "entities": [{"text": "text-level discourse parsing", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.6340295871098837}]}], "datasetContent": [], "tableCaptions": []}