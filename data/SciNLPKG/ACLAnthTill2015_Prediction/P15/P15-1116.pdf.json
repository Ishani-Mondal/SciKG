{"title": [], "abstractContent": [{"text": "We present an extension to incremental shift-reduce parsing that handles discon-tinuous constituents, using a linear clas-sifier and beam search.", "labels": [], "entities": []}, {"text": "We achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F 1 on TiGer).", "labels": [], "entities": [{"text": "accurate", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9737766981124878}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.8946494460105896}]}], "introductionContent": [{"text": "Discontinuous constituents consist of more than one continuous block of tokens.", "labels": [], "entities": []}, {"text": "They arise through phenomena which traditionally in linguistics would be analyzed as being the result of some kind of \"movement\", such as extraposition or topicalization.", "labels": [], "entities": []}, {"text": "The occurrence of discontinuous constituents does not necessarily depend on the degree of freedom in word order that a language allows for.", "labels": [], "entities": []}, {"text": "They can be found, e.g., in almost equal proportions in English and German treebank data ().", "labels": [], "entities": [{"text": "English and German treebank data", "start_pos": 56, "end_pos": 88, "type": "DATASET", "confidence": 0.6614241480827332}]}, {"text": "Generally, discontinuous constituents are accounted for in treebank annotation.", "labels": [], "entities": []}, {"text": "One annotation method consists of using trace nodes that denote the source of a movement and are co-indexed with the moved constituent.", "labels": [], "entities": []}, {"text": "Another method is to annotate discontinuities directly by allowing for crossing branches.", "labels": [], "entities": []}, {"text": "shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1).", "labels": [], "entities": []}, {"text": "The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached.", "labels": [], "entities": []}, {"text": "framed as a separate pre-, post-or in-processing task to PCFG parsing; see particularly for more details.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.7699039280414581}]}, {"text": "Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures.", "labels": [], "entities": []}, {"text": "Transformations have been proposed by, who use complex edge labels that encode paths between lexical heads, and recently by, who use edge labels to encode the attachment order of modifiers to heads.", "labels": [], "entities": []}, {"text": "Direct parsing of discontinuous constituents can be done with Linear Context-Free Rewriting System (LCFRS), an extension of CFG which allows its non-terminals to cover more than one continuous block.", "labels": [], "entities": [{"text": "parsing of discontinuous constituents", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.8416641354560852}, {"text": "CFG", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.9352545738220215}]}, {"text": "LCFRS parsing is expensive: CYK chart parsing with a binarized grammar can be done in O(n 3k ) where k is the block degree, the maximal number of continuous blocks a non-terminal can cover (.", "labels": [], "entities": [{"text": "LCFRS parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7645918428897858}, {"text": "CYK chart parsing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6774301330248514}]}, {"text": "For atypical treebank LCFRS, k \u2248 3, instead of k = 1 for PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.9215930104255676}]}, {"text": "In order to improve on otherwise impractical parsing times, LCFRS chart parsers employ different strategies to speedup search: Kallmeyer and Maier (2013) use A * search; van Cranenburgh (2012) and van use a coarse-to-fine strategy in combination with DataOriented Parsing; use a novel cost estimation to rank parser items.", "labels": [], "entities": [{"text": "parsing", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9620673656463623}, {"text": "LCFRS chart parsers", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.6835471987724304}]}, {"text": "apply a treebank transformation which limits the block degree and therewith also the parsing complexity.", "labels": [], "entities": []}, {"text": "Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an \"easy-first\" approach in the style of.", "labels": [], "entities": []}, {"text": "In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words.", "labels": [], "entities": []}, {"text": "Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words.", "labels": [], "entities": [{"text": "projective dependency parsing", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7130891680717468}]}, {"text": "The reordering can be done online during parsing with a \"swap\" operation that allows to process input words out of order.", "labels": [], "entities": []}, {"text": "This idea can be transferred, because also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals.", "labels": [], "entities": []}, {"text": "uses an adaptive gradient method to train his parser.", "labels": [], "entities": []}, {"text": "He reports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9529159665107727}]}, {"text": "In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.720149427652359}]}, {"text": "While the structured perceptron for parsing has first been used by, classifier-based incremental shift-reduce parsing has been taken up by.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9758416414260864}, {"text": "classifier-based incremental shift-reduce parsing", "start_pos": 68, "end_pos": 117, "type": "TASK", "confidence": 0.5652049034833908}]}, {"text": "A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by.", "labels": [], "entities": [{"text": "shift-reduce constituency parsing", "start_pos": 101, "end_pos": 134, "type": "TASK", "confidence": 0.592007557551066}]}, {"text": "A similar strategy has been shown to work well for CCG parsing, too.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.8299722075462341}]}, {"text": "In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following and) and extend it such that it can create trees with crossing branches (following).", "labels": [], "entities": []}, {"text": "We present strategies to improve performance on discontinuous structures, such as anew feature set.", "labels": [], "entities": []}, {"text": "Our parser is very fast (up to 640 sent./sec.), and produces accurate results.", "labels": [], "entities": []}, {"text": "In our evaluation, where we pay particular attention to the parser performance on discontinuous structures, we show among other things that surprisingly, a grammarbased parser has an edge over a shift-reduce approach concerning the reconstruction of discontinuous constituents.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In subsection 2.1, we introduce the general parser architecture; the subsections 2.2 and 2.3 introduce the features we use and our strategy for handling discontinuous structures.", "labels": [], "entities": []}, {"text": "Section 3 presents and discusses the experimental results, section 4 concludes the article.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our parser is implemented in Java.", "labels": [], "entities": []}, {"text": "We run all our experiments with Java 8 on an Intel Core i5, allocating 15 GB per experiment.", "labels": [], "entities": []}, {"text": "All experiments are carried outwith gold POS tags, as in previous work on shift-reduce constituency parsing (.", "labels": [], "entities": [{"text": "shift-reduce constituency parsing", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.6321890354156494}]}, {"text": "Grammatical function labels are discarded.", "labels": [], "entities": []}, {"text": "For the evaluation, we use the corresponding module of discodop.", "labels": [], "entities": []}, {"text": "We report several metrics (as implemented in discodop): \u2022 Extended labeled bracketing, in which a bracket fora single node consists of its label and a set of pairs of indices, delimiting the continuous blocks it covers.", "labels": [], "entities": [{"text": "Extended labeled bracketing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6286794741948446}]}, {"text": "We do not include the root node in the evaluation and ignore punctuation.", "labels": [], "entities": []}, {"text": "We report labeled precision, recall and F 1 , as well as exact match (all brackets correct).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9480754137039185}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9997337460517883}, {"text": "F 1", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9903990924358368}, {"text": "exact", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9991437196731567}, {"text": "match", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.5968810319900513}]}, {"text": "\u2022 Leaf-ancestor (, for which we consider all paths from leaves to the root.", "labels": [], "entities": []}, {"text": "\u2022 Tree edit distance, which consists of the minimum edit distance between gold tree and parser output.", "labels": [], "entities": [{"text": "Tree edit distance", "start_pos": 2, "end_pos": 20, "type": "METRIC", "confidence": 0.6663722991943359}]}, {"text": "Aside from a full evaluation, we also evaluate only the constituents that are discontinuous.).", "labels": [], "entities": []}, {"text": "We run further experiments with rparse 5 ( to facilitate a comparison with a grammar-based parser.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results NEGRA, beam size 8", "labels": [], "entities": [{"text": "NEGRA", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.5313481688499451}]}, {"text": " Table 2: Results NEGRA TED and Leaf-Ancestor", "labels": [], "entities": [{"text": "NEGRA TED", "start_pos": 18, "end_pos": 27, "type": "TASK", "confidence": 0.6997228562831879}]}, {"text": " Table 3: Results TIGER, beam size 4", "labels": [], "entities": [{"text": "TIGER", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.982269287109375}]}, {"text": " Table 5: Results NEGRA rparse", "labels": [], "entities": [{"text": "NEGRA rparse", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.8076302707195282}]}, {"text": " Table 6: Results TIGERHN, sentence length \u2264 40", "labels": [], "entities": [{"text": "TIGERHN", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9526143670082092}]}]}