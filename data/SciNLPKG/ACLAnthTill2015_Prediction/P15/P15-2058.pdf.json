{"title": [{"text": "Semantic Clustering and Convolutional Neural Network for Short Text Categorization", "labels": [], "entities": [{"text": "Short Text Categorization", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.5854063530762991}]}], "abstractContent": [{"text": "Short texts usually encounter data sparsi-ty and ambiguity problems in representations for their lack of context.", "labels": [], "entities": []}, {"text": "In this paper , we propose a novel method to model short texts based on semantic clustering and convolutional neural network.", "labels": [], "entities": []}, {"text": "Particularly , we first discover semantic cliques in embedding spaces by a fast clustering algorithm.", "labels": [], "entities": []}, {"text": "Then, multi-scale semantic units are detected under the supervision of semantic cliques, which introduce useful external knowledge for short texts.", "labels": [], "entities": []}, {"text": "These meaningful semantic units are combined and fed into convolutional layer, followed by max-pooling operation.", "labels": [], "entities": []}, {"text": "Experimental results on two open benchmarks validate the effectiveness of the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conventional texts mining methods based on bagof-words (BoW) easily encounter data sparsity and ambiguity problems in short text modeling, which ignore semantic relations between words (.", "labels": [], "entities": []}, {"text": "How to acquire effective representation for short text has been an active research issue.", "labels": [], "entities": []}, {"text": "In order to overcome the weakness of BoW, researchers have proposed to expand the representation of short text using latent semantics, where the words are mapped to distributional representations by Latent Dirichlet Allocation (LDA) () and its extensions.", "labels": [], "entities": [{"text": "BoW", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8976989388465881}]}, {"text": "presented a general framework to expand the short and sparse text by appending topic names discovered using LDA.", "labels": [], "entities": []}, {"text": "presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity.", "labels": [], "entities": []}, {"text": "However, the methods discussed above still view apiece of text as BoW.", "labels": [], "entities": [{"text": "BoW", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9029014110565186}]}, {"text": "Therefore, they are not effective in capturing finegrained semantic information for short texts modeling.", "labels": [], "entities": [{"text": "short texts modeling", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.625001053015391}]}, {"text": "Recently, neural network related methods have received much attention, including learning word embeddings () and performing semantic composition to obtain phrase or sentence level representations).", "labels": [], "entities": []}, {"text": "For learning word embedding, the training objective of continuous Skip-gram model) is to predict its context.", "labels": [], "entities": [{"text": "learning word embedding", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6610599855581919}]}, {"text": "Thus, the cooccurrence information can be effectively used to describe a word, and each component of word embedding might have a semantic or grammatical interpretation.", "labels": [], "entities": []}, {"text": "In embedding spaces, semantically close words are likely to cluster together and form semantic cliques (or word embedding cliques).", "labels": [], "entities": []}, {"text": "Moreover, the embedding spaces exhibit linear structure that the word vectors can be meaningfully combined using simple additive operation, for example: The above examples indicate that the additive composition can often produce meaningful results.", "labels": [], "entities": []}, {"text": "In Equation (1), the token \u2032 Berlin \u2032 can be viewed that it has an embedding offset vec (Capital) to the token \u2032 Germany \u2032 in embedding spaces.", "labels": [], "entities": []}, {"text": "Furthermore, the embedding offsets represent the syntactical and semantic relations among words.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to model short texts using semantic clustering and convolutional neural network (CNN).", "labels": [], "entities": []}, {"text": "Firstly, the fast clustering algorithm (), based on searching density peaks, is utilized to cluster word embeddings and discover semantic cliques, as shown in.", "labels": [], "entities": []}, {"text": "Then semantic composition is performed over n-gram embeddings to: Fast clustering based on density peaks of embeddings detect candidate Semantic Units 1 (abbr. to SUs) appearing in short texts.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.773847907781601}]}, {"text": "The part of candidate SUs meeting the preset threshold are chosen to constitute semantic matrices, which are used as input for the CNN, otherwise dropout.", "labels": [], "entities": []}, {"text": "In this stage, semantic cliques are used as supervision information, which guarantee meaningful SUs can be extracted.", "labels": [], "entities": []}, {"text": "The motivation of our work is to introduce extra knowledge by pre-trained word embeddings and fully exploit the contextual information of short texts to improve their representations.", "labels": [], "entities": []}, {"text": "The main contributions include: (1) semantic cliques are discovered using fast clustering method based on searching density peaks; (2) for fine-tuning multiscale SUs, the semantic cliques are used to supervise the selection stage.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The related works are briefly reviewed in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 introduces the semantic clustering based on fast searching density peaks.", "labels": [], "entities": [{"text": "semantic clustering", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6927644610404968}]}, {"text": "Section 4 describes the architecture of the proposed method.", "labels": [], "entities": []}, {"text": "Section 5 demonstrates the effectiveness of our method with experiments.", "labels": [], "entities": []}, {"text": "Finally, concluding remarks are offered in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are conducted on two benchmarks: Google Snippets ( and TREC (.", "labels": [], "entities": [{"text": "TREC", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.6219035983085632}]}, {"text": "Google Snippets This dataset consists of 10,060 training snippets and 2,280 test snippets from 8 categories.", "labels": [], "entities": []}, {"text": "On average, each snippet has 18.07 words.", "labels": [], "entities": []}, {"text": "TREC The TREC questions dataset contains 6 different question types.", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6264166235923767}, {"text": "TREC questions dataset", "start_pos": 9, "end_pos": 31, "type": "DATASET", "confidence": 0.7480490108331045}]}, {"text": "The training dataset consists of 5,452 labeled questions whereas the test dataset consists of 500 questions.", "labels": [], "entities": []}, {"text": "Three pre-trained word embeddings for initializing the lookup table are summarized in.", "labels": [], "entities": []}, {"text": "To discover semantic cliques, we take \u03c1 min = 16 and \u03b4 min = 1.54.", "labels": [], "entities": []}, {"text": "Through our experiments, 6 kernel matrices in convolutional layer, K = 3 for max pooling, and mini-batch size of 100 are used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of word embeddings", "labels": [], "entities": []}, {"text": " Table 2: The classification accuracy of proposed  method against other models", "labels": [], "entities": [{"text": "classification", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9013805389404297}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9081002473831177}]}]}