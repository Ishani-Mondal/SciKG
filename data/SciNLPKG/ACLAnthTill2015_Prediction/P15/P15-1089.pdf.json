{"title": [], "abstractContent": [{"text": "In this paper, we propose a syllable-based method for tweet normalization to study the cognitive process of non-standard word creation in social media.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8526722192764282}]}, {"text": "Assuming that syllable plays a fundamental role in forming the non-standard tweet words, we choose syllable as the basic unit and extend the conventional noisy channel model by incorporating the syllables to represent the word-to-word transitions at both word and syllable levels.", "labels": [], "entities": []}, {"text": "The syllables are used in our method not only to suggest more candidates, but also to measure similarity between words.", "labels": [], "entities": []}, {"text": "Novelty of this work is threefold: First, to the best of our knowledge, this is an early attempt to explore syllables in tweet normalization.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7163194119930267}]}, {"text": "Second, our proposed normalization method relies on unlabeled samples, making it much easier to adapt our method to handle non-standard words in any period of history.", "labels": [], "entities": [{"text": "normalization", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9665777087211609}]}, {"text": "And third, we conduct a series of experiments and prove that the proposed method is advantageous over the state-of-art solutions for tweet normalization.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.8208354115486145}]}], "introductionContent": [{"text": "Due to the casual nature of social media, there exists a large number of non-standard words in text expressions which make it substantially different from formal written text.", "labels": [], "entities": []}, {"text": "It is reported in ( ) that more than 4 million distinct out-of-vocabulary (OOV) tokens are found in the Edinburgh Twitter corpus (.", "labels": [], "entities": [{"text": "Edinburgh Twitter corpus", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.9850457111994425}]}, {"text": "This variation poses challenges when performing natural language processing (NLP) tasks) based on such texts.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.7620958685874939}]}, {"text": "Tweet normalization, aiming at converting these OOV non-standard words into their in-vocabulary (IV) formal forms, is therefore viewed as a very important pre-processing task.", "labels": [], "entities": [{"text": "Tweet normalization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9617617726325989}]}, {"text": "Researchers focus their studies in tweet normalization at different levels.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.906776487827301}]}, {"text": "A character-level tagging system is used in to solve deletion-based abbreviation.", "labels": [], "entities": [{"text": "character-level tagging", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.6393072158098221}, {"text": "deletion-based abbreviation", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.7205519378185272}]}, {"text": "It was further extended in ( ) using more characters instead of Y or N as labels.", "labels": [], "entities": []}, {"text": "The character-level machine translation (MT) approach (Pennell and Liu, 2011) was modified in (Li and Liu, 2012a) into character-block.", "labels": [], "entities": [{"text": "character-level machine translation (MT)", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7890401631593704}]}, {"text": "While a string edit distance method was introduced in ( to represent word-level similarity, and this orthographical feature has been adopted in, and.", "labels": [], "entities": []}, {"text": "Challenges are encountered in these different levels of tweet normalization.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8284828066825867}]}, {"text": "In the characterlevel sequential labeling systems, features are required for every character and their combinations, leading to much more noise into the later reverse table look-up process ( ).", "labels": [], "entities": [{"text": "characterlevel sequential labeling", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.6266232629617056}]}, {"text": "In the character-block level MT systems equal number of blocks and their corresponding phonetic symbols are required for alignment (.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.8567907810211182}, {"text": "alignment", "start_pos": 121, "end_pos": 130, "type": "TASK", "confidence": 0.9652705192565918}]}, {"text": "This strict restriction can result in a great difficulty in training set construction and a loss of useful information.", "labels": [], "entities": [{"text": "training set construction", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6692078610261282}]}, {"text": "Finally, word-level normalization methods cannot properly model how non-standard words are formed, and some patterns or consistencies within words can be omitted and altered.", "labels": [], "entities": [{"text": "word-level normalization", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.7207502126693726}]}, {"text": "We observe the cognitive process that, given non-standard words like tmr, people tend to first segment them into syllables like t-m-r.", "labels": [], "entities": []}, {"text": "Then they will find the corresponding standard word with syllables like to-mor-row.", "labels": [], "entities": []}, {"text": "Inspired by this cognitive observation, we propose a syllable based tweet normalization method, in which nonstandard words are first segmented into syllables.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7053811699151993}]}, {"text": "Since we cannot predict the writers deterministic intention in using tmr as a segmentation of tm-r (representing tim-er) or t-m-r (representing to-mor-row), every possible segmentation form is considered.", "labels": [], "entities": []}, {"text": "Then we represent similarity of standard syllables and non-standard syllables using an exponential potential function.", "labels": [], "entities": []}, {"text": "After every transition probabilities of standard syllable and non-standard syllable are assigned, we then use noisy channel model and Viterbi decoder to search for the most possible standard candidate in each tweet sentence.", "labels": [], "entities": []}, {"text": "Our empirical study reveals that syllable is a proper level for tweet normalization.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8097243309020996}]}, {"text": "The syllable is similar to character-block but it represents phonetic features naturally because every word is pronounced with syllables.", "labels": [], "entities": []}, {"text": "Our syllable-based tweet normalization method utilizes effective features of both character-and word-level: (1) Like characterlevel, it can capture more detailed information about how non-standard words are generated; (2) Similar to word-level, it reduces a large amount of noisy candidates.", "labels": [], "entities": [{"text": "syllable-based tweet normalization", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6013343135515848}]}, {"text": "Instead of using domain-specific resources, our method makes good use of standard words to extract linguistic features.", "labels": [], "entities": []}, {"text": "This makes our method extendable to new normalization tasks or domains.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: previous work in tweet normalization are reviewed and discussed in Section 2.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8353011906147003}]}, {"text": "Our approach is presented in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 and Section 5, we provide implementation details and results.", "labels": [], "entities": []}, {"text": "Then we make some analysis of the results in Section 6.", "labels": [], "entities": []}, {"text": "This work is finally concluded in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two labeled twitter datasets in existence to evaluate our tweet normalization method.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7412241399288177}]}, {"text": "\u2022 LexNorm1.1 contains 549 complete tweets with 1184 non-standard tokens (558 unique word type) (Han and Baldwin, 2011).", "labels": [], "entities": [{"text": "LexNorm1.1", "start_pos": 2, "end_pos": 12, "type": "DATASET", "confidence": 0.9447663426399231}]}, {"text": "\u2022 LexNorm1.2 is a revised version of LexNorm1.1.", "labels": [], "entities": [{"text": "LexNorm1.2", "start_pos": 2, "end_pos": 12, "type": "DATASET", "confidence": 0.97403484582901}, {"text": "LexNorm1.1", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.981581449508667}]}, {"text": "Some inconsistencies and errors in LexNorm1.1 are corrected and some more non-standard words are properly recovered.", "labels": [], "entities": [{"text": "LexNorm1.1", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9591098427772522}]}, {"text": "In both datasets, to-be-normalized non-standard words are detected manually as well as the corresponding standard words.", "labels": [], "entities": []}, {"text": "Here we use precision, recall and F-score to evaluate our method.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9997296929359436}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9996564388275146}, {"text": "F-score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9992493987083435}]}, {"text": "As normalization methods on these datasets focused on the labeled nonstandard words, recall is the proportion of words requiring normalization which are normalized correctly; precision is the proportion of normalizations which are correct.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9993312358856201}, {"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9992510676383972}]}, {"text": "When we perform the tweet normalization methods, every error is both a false positive and false negative, so in the task, precision equals to recall.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7694418430328369}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9995286464691162}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9982189536094666}]}], "tableCaptions": [{"text": " Table 3: Experiment results of the tweet normalization methods.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7738457322120667}]}]}