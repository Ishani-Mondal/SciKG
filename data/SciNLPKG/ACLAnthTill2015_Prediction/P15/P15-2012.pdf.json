{"title": [{"text": "Zoom: a corpus of natural language descriptions of map locations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an experiment to elicit referring expressions from human subjects for research in natural language generation and related fields, and preliminary results of a computational model for the generation of these expressions.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6921006639798483}]}, {"text": "Unlike existing resources of this kind, the resulting data set-the Zoom corpus of natural language descriptions of map locations-takes into account a domain that is significantly closer to real-world applications than what has been considered in previous work, and addresses more complex situations of reference, including contexts with different levels of detail, and instances of singular and plural reference produced by speakers of Spanish and Portuguese.", "labels": [], "entities": [{"text": "Zoom corpus of natural language descriptions of map locations-takes", "start_pos": 67, "end_pos": 134, "type": "DATASET", "confidence": 0.8703193598323398}]}], "introductionContent": [{"text": "Referring Expression Generation (REG) is the computational task of producing adequate natural language descriptions (e.g., pronouns, definite descriptions, proper names, etc.) of domain entities.", "labels": [], "entities": [{"text": "Referring Expression Generation (REG) is the computational task of producing adequate natural language descriptions (e.g., pronouns, definite descriptions, proper names, etc.) of domain entities", "start_pos": 0, "end_pos": 194, "type": "Description", "confidence": 0.8113204501569271}]}, {"text": "In particular, the issue of how to determine the semantic contents of definite descriptions (e.g., 'the Indian restaurant on 5th street', 'the restaurant we went to last night', etc.) has received significant attention in the field, and it is also the focus of the present work.", "labels": [], "entities": [{"text": "determine the semantic contents of definite descriptions", "start_pos": 35, "end_pos": 91, "type": "TASK", "confidence": 0.7913220354488918}]}, {"text": "The input to a REG algorithm is a context set C containing an intended referent rand a number of distractor objects.", "labels": [], "entities": []}, {"text": "All objects are represented as attribute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (.", "labels": [], "entities": []}, {"text": "The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (.", "labels": [], "entities": []}, {"text": "Properties are selected for inclusion in L according to multiple -and often conflicting -criteria, including discriminatory power (i.e., the ability to rule out distractors) as in), domain preferences) and many others.", "labels": [], "entities": []}, {"text": "A description that conveys more information than what is strictly required for disambiguation is said to be overspecified ().", "labels": [], "entities": []}, {"text": "For a review of the research challenges in REG, see . Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale & Reiter Incremental algorithm in.", "labels": [], "entities": [{"text": "REG", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9743335843086243}, {"text": "REG", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9602234363555908}]}, {"text": "The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g.,).", "labels": [], "entities": [{"text": "machine learning (ML)", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6658073782920837}]}, {"text": "A possible explanation for the small interest in ML for REG maybe the relatively low availability of data.", "labels": [], "entities": [{"text": "ML", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9732444286346436}, {"text": "REG", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8716354966163635}]}, {"text": "While research in many fields may benefit from the wide availability of text corpora (e.g., obtainable from the web), research in REG usually requires highly specialised data -hereby called REG corpora -conveying not only referring expressions produced by human speakers, but also a fully-annotated representation of the context (i.e., all objects and their semantic properties) within which the expressions have been produced.", "labels": [], "entities": []}, {"text": "REG corpora such as TUNA () and GRE3D3) are useful both to gain general insights on human language production, and to benefit from data-intensive computational techniques such as ML.", "labels": [], "entities": []}, {"text": "However, being usually the final product of controlled experiments involving human subjects, REG cor-pora tend to address highly specific research questions.", "labels": [], "entities": []}, {"text": "For instance, GRE3D3 is largely devoted to the investigation of relational referring expressions () in simple visual scenes involving geometric shapes, as in 'the large ball next to the red cube'.", "labels": [], "entities": [{"text": "GRE3D3", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.740787148475647}]}, {"text": "As a result, and despite the usefulness of these resources to a large body of work in REG, further research questions will usually require the collection of new data.", "labels": [], "entities": [{"text": "REG", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9078952670097351}]}, {"text": "In this paper we introduce the Zoom corpus of referring expressions.", "labels": [], "entities": []}, {"text": "Zoom addresses a domain that is considerably closer to real-world applications (namely, city maps in different degrees of detail represented by zoom levels) than what has been considered in previous work, involving both singular and plural reference, and making extensive use of relational properties.", "labels": [], "entities": []}, {"text": "Moreover, Zoom descriptions were produced by both Spanish and Portuguese speakers, which will allow (to the best of our knowledge, for the first time) a comprehensive study of the REG surface realisation subtask in these languages, and enable research on the issues of human variation in REG ().", "labels": [], "entities": [{"text": "REG surface realisation subtask", "start_pos": 180, "end_pos": 211, "type": "TASK", "confidence": 0.8638327568769455}]}], "datasetContent": [{"text": "We designed a web-based experiment to collect natural language descriptions of map locations in both Spanish and Portuguese.", "labels": [], "entities": []}, {"text": "The collected data set comprises a corpus of referring expressions for research in REG and related fields.", "labels": [], "entities": [{"text": "REG", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9451248645782471}]}, {"text": "The situations of reference under consideration make use of map scenes in two degrees of detail (represented by low and high zoom levels), and address instances of singular and plural reference.", "labels": [], "entities": []}, {"text": "A fragment of the experiment interface is shown in.", "labels": [], "entities": []}, {"text": "In what follows we illustrate the use of the Zoom corpus as training and test data fora simple machine learning approach to REG adapted from).", "labels": [], "entities": [{"text": "Zoom corpus", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.855314701795578}]}, {"text": "The goal of this evaluation is to provide reference results for future comparison with purpose-built REG algorithms, and not to present a complete REG solution for the Zoom domain or others.", "labels": [], "entities": []}, {"text": "The present model consists of 12 binary classifiers representing whether individual referential attributes should be selected for inclusion in an output description.", "labels": [], "entities": []}, {"text": "The classifiers correspond to atomic attributes of the target and first landmark object (type, name and others), and relations.", "labels": [], "entities": []}, {"text": "Referential attributes of other landmark objects were not modelled due to data sparsity and also to reduce computational costs.", "labels": [], "entities": []}, {"text": "For similar reasons, the multivalue between relation is also presently disregarded, and 'corner' relations involving two landmarks (e.g., two streets) will be modelled as two independent classification tasks.", "labels": [], "entities": []}, {"text": "Only two learning features are considered by each classifier: landmarkCount, which represents the number of landmark objects near the main target, and distractorCount, which represents the number of objects of the same type as the target within the relevant context in the map.", "labels": [], "entities": []}, {"text": "For other possible features applicable to this task, see, for instance, (dos Santos.", "labels": [], "entities": []}, {"text": "From the outcome of the 12 binary classifiers, a description is built by considering atomic target attributes in the first place.", "labels": [], "entities": []}, {"text": "All attributes that correspond to a positive prediction are selected for inclusion in the output description.", "labels": [], "entities": []}, {"text": "Next, relations are considered.", "labels": [], "entities": []}, {"text": "If no relation is predicted, the algorithm terminates by returning anatomic description of the main target object.", "labels": [], "entities": []}, {"text": "If the description includes a relation, the corresponding landmark object is selected, and the algorithm is called recursively to describe it as well.", "labels": [], "entities": []}, {"text": "Since every attribute that corresponds to a positive prediction is always selected, the algorithm does not regard uniqueness as a stop condition.", "labels": [], "entities": []}, {"text": "As a result, the output description may convey a certain amount of overspecification.", "labels": [], "entities": []}, {"text": "For evaluation purposes, we used the subset of singular descriptions from the Portuguese portion of the corpus, comprising 821 descriptions.", "labels": [], "entities": [{"text": "Portuguese portion of the corpus", "start_pos": 78, "end_pos": 110, "type": "DATASET", "confidence": 0.6608540415763855}]}, {"text": "Evaluation was carried out by comparing the corpus description with the system output to measure overall accuracy (i.e., the number of exact matches between the two descriptions), Dice and MASI () coefficients.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9961093068122864}, {"text": "Dice", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9863345623016357}, {"text": "MASI", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.9854002594947815}]}, {"text": "Following), we built a REG model using support vector machines with radial basis function kernel.", "labels": [], "entities": []}, {"text": "The classifiers were trained and tested using 6-fold cross validation.", "labels": [], "entities": []}, {"text": "Optimal parameters were selected using grid search as follows: for each step in the main k-fold validation, one fold was reserved for testing, and the remaining k \u2212 1 folds were subject to a secondary cross-validation procedure in which different parameter combinations were attempted.", "labels": [], "entities": []}, {"text": "The C parameter was assigned the values 1, 10, 100 and 1000, and \u03b3 was assigned 1, 0.1, 0.001 and 0.0001.", "labels": [], "entities": []}, {"text": "The best-performing parameter set was selected to build a classifier trained from the k \u2212 1 fold, and tested on the test data.", "labels": [], "entities": []}, {"text": "This was repeated for every iteration of the main cross-validation procedure.", "labels": [], "entities": []}, {"text": "summarises the results obtained by the REG algorithm built from SVM classifiers, those obtained by a baseline system representing a relational extension of the Dale & Reiter Incremental Algorithm, and by a Random selection strategy.", "labels": [], "entities": []}, {"text": "We compare accuracy scores obtained by every algorithm pair using the chi-square test, and we compare Dice scores using Wilcoxon's signedrank test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9992396831512451}]}, {"text": "In terms of overall accuracy, the SVM approach outperforms both alternatives.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9993738532066345}]}, {"text": "The difference from the second best-performing algorithm (i.e., the Incremental approach) is significant (\u03c7 2 = 79.87, df=1, p<0.0001).", "labels": [], "entities": []}, {"text": "Only in terms of Dice scores a small effect in the opposite direction is observed (T=137570.5, p= 0.01413).", "labels": [], "entities": [{"text": "Dice", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.8611335754394531}, {"text": "T", "start_pos": 83, "end_pos": 84, "type": "METRIC", "confidence": 0.9980924725532532}]}, {"text": "We also assessed the performance of the individual classifiers.", "labels": [], "entities": []}, {"text": "shows these results as measured by precision (P), recall (R), F1-measure (F1) and area under the ROC curve (AUC).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.942369818687439}, {"text": "recall (R)", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9570382237434387}, {"text": "F1-measure (F1)", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.8871933370828629}, {"text": "area", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9438828825950623}, {"text": "ROC curve (AUC)", "start_pos": 97, "end_pos": 112, "type": "METRIC", "confidence": 0.9570940613746644}]}, {"text": "From these results we notice that highly frequent attributes (e.g., target type and landmark name) were classified with high accuracy, whereas others (e.g., multivalue attributes and relations) were not.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9973897337913513}]}], "tableCaptions": [{"text": " Table 1: Comparison with existing REG corpora", "labels": [], "entities": []}]}