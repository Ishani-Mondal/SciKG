{"title": [], "abstractContent": [{"text": "This paper proposes a novel lifelong learning (LL) approach to sentiment classification.", "labels": [], "entities": [{"text": "lifelong learning (LL)", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7347771048545837}, {"text": "sentiment classification", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.9678325057029724}]}, {"text": "LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning.", "labels": [], "entities": []}, {"text": "In this paper, we first discuss LL in general and then LL for sentiment classification in particular.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.9507566392421722}]}, {"text": "The proposed LL approach adopts a Bayesian optimization framework based on stochas-tic gradient descent.", "labels": [], "entities": []}, {"text": "Our experimental results show that the proposed method out-performs baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment classification is the task of classifying an opinion document as expressing a positive or negative sentiment. and provided good surveys of the existing research.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9586415588855743}]}, {"text": "In this paper, we tackle sentiment classification from a novel angle, lifelong learning (LL), or lifelong machine learning.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.9777757227420807}, {"text": "lifelong learning (LL)", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7391181230545044}]}, {"text": "This learning paradigm aims to learn as humans do: retaining the learned knowledge from the past and use the knowledge to help future learning.", "labels": [], "entities": []}, {"text": "Although many machine learning topics and techniques are related to LL, e.g., lifelong learning, transfer learning, multi-task learning, never-ending learning, selftaught learning (, and online learning, there is still no unified definition for LL.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.9011605978012085}]}, {"text": "Based on the prior work and our research, to build an LL system, we believe that we need to answer the following key questions: 1.", "labels": [], "entities": []}, {"text": "What information should be retained from the past learning tasks?", "labels": [], "entities": []}, {"text": "2. What forms of knowledge will be used to help future learning?", "labels": [], "entities": []}, {"text": "3. How does the system obtain the knowledge?", "labels": [], "entities": []}, {"text": "4. How does the system use the knowledge to help future learning?", "labels": [], "entities": []}, {"text": "Motivated by these questions, we present the following definition of lifelong learning (LL).", "labels": [], "entities": [{"text": "lifelong learning (LL)", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7149294495582581}]}, {"text": "Definition (Lifelong Learning): A learner has performed learning on a sequence of tasks, from 1 to N \u2212 1.", "labels": [], "entities": []}, {"text": "When faced with the N th task, it uses the knowledge gained in the past N \u2212 1 tasks to help learning for the N th task.", "labels": [], "entities": []}, {"text": "An LL system thus needs the following four general components: 1.", "labels": [], "entities": []}, {"text": "Past Information Store (PIS): It stores the information resulted from the past learning.", "labels": [], "entities": []}, {"text": "This may involve sub-stores for information such as (1) the original data used in each past task, (2) intermediate results from the learning of each past task, and (3) the final model or patterns learned from the past task, respectively.", "labels": [], "entities": []}, {"text": "2. Knowledge Base (KB): It stores the knowledge mined or consolidated from PIS (Past Information Store).", "labels": [], "entities": []}, {"text": "This requires a knowledge representation scheme suitable for the application.", "labels": [], "entities": []}, {"text": "3. Knowledge Miner (KM).", "labels": [], "entities": [{"text": "Knowledge Miner (KM)", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.5584930717945099}]}, {"text": "It mines knowledge from PIS (Past Information Store).", "labels": [], "entities": [{"text": "PIS (Past Information Store)", "start_pos": 24, "end_pos": 52, "type": "DATASET", "confidence": 0.8002877434094747}]}, {"text": "This mining can be regarded as a meta-learning process because it learns knowledge from information resulted from learning of the past tasks.", "labels": [], "entities": []}, {"text": "The knowledge is stored to KB (Knowledge Base).", "labels": [], "entities": []}, {"text": "4. Knowledge-Based Learner (KBL): Given the knowledge in KB, this learner is able to leverage the knowledge and/or some information in PIS for the new task.", "labels": [], "entities": [{"text": "Knowledge-Based Learner (KBL)", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6258651375770569}]}, {"text": "Based on this, we can define lifelong sentiment classification (LSC): A learner has performed a sequence of supervised sentiment classification tasks, from 1 to N \u2212 1, where each task consists of a set of training documents with positive and negative polarity labels.", "labels": [], "entities": [{"text": "lifelong sentiment classification (LSC)", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.8397767742474874}, {"text": "sentiment classification", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.7024195492267609}]}, {"text": "Given the N th task, it uses the knowledge gained in the past N \u2212 1 tasks to learn a better classifier for the N th task.", "labels": [], "entities": []}, {"text": "It is useful to note that although many researchers have used transfer learning for supervised sentiment classification, LL is different from the classic transfer learning or domain adaptation.", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.6767114996910095}]}, {"text": "Transfer learning typically uses labeled training data from one (or more) source domain(s) to help learning in the target domain that has little or no labeled data).", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9540479779243469}]}, {"text": "It does not use the results of the past learning or knowledge mined from the results of the past learning.", "labels": [], "entities": []}, {"text": "Further, transfer learning is usually inferior to traditional supervised learning when the target domain already has good training data.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.9775275886058807}]}, {"text": "In contrast, our target (or future) domain/task has good training data and we aim to further improve the learning using both the target domain training data and the knowledge gained in past learning.", "labels": [], "entities": []}, {"text": "To be consistent with prior research, we treat the classification of one domain as one learning task.", "labels": [], "entities": []}, {"text": "One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled training data.", "labels": [], "entities": [{"text": "target domain classification", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.6955706079800924}]}, {"text": "The key reason is that the training data may not be fully representative of the test data due to the sample selection bias).", "labels": [], "entities": []}, {"text": "In few real-life applications, the training data are fully representative of the test data.", "labels": [], "entities": []}, {"text": "For example, in a sentiment classification application, the test data may contain some sentiment words that are absent in the training data of the target domain, while these sentiment words have appeared in some past domains.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.9440546631813049}]}, {"text": "So the past domain knowledge can provide the prior polarity information in this situation.", "labels": [], "entities": []}, {"text": "Like most existing sentiment classification papers (, this paper focuses on binary classification, i.e., positive (+) and negative (\u2212) polarities.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.9250805974006653}]}, {"text": "But the proposed method is also applicable to multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.8381209373474121}]}, {"text": "To embed and use the knowledge in building the target domain classifier, we propose a novel optimization method based on the Na\u00a8\u0131veNa\u00a8\u0131ve Bayesian (NB) framework and stochastic gradient descent.", "labels": [], "entities": []}, {"text": "The knowledge is incorporated using penalty terms in the optimization formulation.", "labels": [], "entities": []}, {"text": "This paper makes three contributions: 1.", "labels": [], "entities": []}, {"text": "It proposes a novel lifelong learning approach to sentiment classification, called lifelong sentiment classification (LSC).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.9685438275337219}, {"text": "lifelong sentiment classification (LSC)", "start_pos": 83, "end_pos": 122, "type": "TASK", "confidence": 0.7986390888690948}]}, {"text": "2. It proposes an optimization method that uses penalty terms to embed the knowledge gained in the past and to deal with domain dependent sentiment words to build a better classifier.", "labels": [], "entities": []}, {"text": "3. It creates a large corpus containing reviews from 20 diverse product domains for extensive evaluation.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate the superiority of the proposed method.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created a large corpus containing reviews from 20 types of diverse products or domains crawled from Amazon.com (i.e., 20 datasets).", "labels": [], "entities": []}, {"text": "The names of product domains are listed in.", "labels": [], "entities": []}, {"text": "Each domain contains 1,000 reviews.", "labels": [], "entities": []}, {"text": "Following the existing work of other researchers (), we treat reviews with rating > 3 as positive and reviews with rating < 3 as negative.", "labels": [], "entities": []}, {"text": "The datasets are publically available at the authors websites.", "labels": [], "entities": []}, {"text": "Natural class distribution: We keep the natural (or skewed) distribution of the positive and negative reviews to experiment with the real-life situation.", "labels": [], "entities": []}, {"text": "F1-score is used due to the imbalance.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9883887767791748}]}, {"text": "Balanced class distribution: We also created a balance dataset with 200 reviews (100 positive and 100 negative) in each domain dataset.", "labels": [], "entities": []}, {"text": "This set is smaller because of the small number of negative reviews in each domain.", "labels": [], "entities": []}, {"text": "Accuracy is used for evaluation in this balanced setting.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9907350540161133}]}, {"text": "We used unigram features with no feature selection in classification.", "labels": [], "entities": []}, {"text": "We followed () to deal with negation words.", "labels": [], "entities": []}, {"text": "For evaluation, each domain is treated as the target domain with the rest 19 domains as the past domains.", "labels": [], "entities": []}, {"text": "All the models are evaluated using 5-fold cross validation.", "labels": [], "entities": []}, {"text": "We compare our proposed LSC model with Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB), SVM 1 , and CLF (.", "labels": [], "entities": []}, {"text": "Note that NB and SVM can only work on a single domain data.", "labels": [], "entities": []}, {"text": "To have a comprehensive comparison, they are fed with three types of training data: a) labeled training data from the target domain only, denoted by NB-T and SVM-T; b) labeled training data from all past source domains only, denoted by NB-S and SVM-S; c) merged (labeled) training data from all past domains and the target domain, referred to as NB-ST and SVM-ST.", "labels": [], "entities": [{"text": "NB-ST", "start_pos": 346, "end_pos": 351, "type": "DATASET", "confidence": 0.9456854462623596}]}, {"text": "For LSC, we empirically set \u03c3 = 6 and \u03c4 = 6.", "labels": [], "entities": []}, {"text": "The learning rate \u03bb and regularization coefficient \u03b1 are set to 0.1 empirically.", "labels": [], "entities": [{"text": "learning rate \u03bb", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8011410236358643}]}, {"text": "\u03bb is set to 1 for (Laplace) smoothing.", "labels": [], "entities": []}, {"text": "shows the average F1-scores for the negative class in the natural class distribution, and shows the average accuracies in the balanced class distribution.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9976898431777954}]}, {"text": "We can clearly see that our proposed model LSC achieves the best performance in both cases.", "labels": [], "entities": []}, {"text": "In general, NB-S (and SVM-S) are worse than NB-T (and SVM-T), both of which are worse than NB-ST (and SVM-ST).", "labels": [], "entities": []}, {"text": "This shows that simply merging both past domains and the target domain data is slightly beneficial.", "labels": [], "entities": []}, {"text": "Note that the average F1-score for the positive class is not shown as all classifiers perform very well because the positive class is the majority class (while our model performs slightly better than the baselines).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.998664140701294}]}, {"text": "The improvements of the proposed LSC model overall baselines in both cases are statistically significant using paired t-test (p < 0.01 compared to NB-ST and CLF, p < 0.0001 compared to the others).", "labels": [], "entities": [{"text": "NB-ST", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.9525681138038635}]}, {"text": "In the balanced class setting (Table 3), CLF performs better than NB-T and SVM-T, which is consistent with the results in (.", "labels": [], "entities": []}, {"text": "However, it is still worse than our LSC model.", "labels": [], "entities": []}, {"text": "Effects of #Past Domains.", "labels": [], "entities": []}, {"text": "shows the effects of our model using different number of past domains.", "labels": [], "entities": []}, {"text": "We clearly see that LSC performs better with more past domains, showing it indeed has the ability to accumulate knowledge and use the knowledge to build better classifiers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Names of the 20 product domains and the proportion of negative reviews in each domain.", "labels": [], "entities": []}, {"text": " Table 2: Natural class distribution: Average F1-score of the negative class over 20 domains. Negative  class is the minority class and thus harder to classify.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9832034111022949}]}, {"text": " Table 3: Balanced class distribution: Average accuracy over 20 domains for each system.", "labels": [], "entities": [{"text": "Average", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9540668725967407}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.8899129629135132}]}]}