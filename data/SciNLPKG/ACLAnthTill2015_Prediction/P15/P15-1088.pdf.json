{"title": [{"text": "Multiple Many-to-Many Sequence Alignment for Combining String-Valued Variables: A G2P Experiment", "labels": [], "entities": [{"text": "Sequence Alignment", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.73702272772789}]}], "abstractContent": [{"text": "We investigate multiple many-to-many alignments as a primary step in integrating supplemental information strings in string transduction.", "labels": [], "entities": []}, {"text": "Besides outlining DP based solutions to the multiple alignment problem, we detail an approximation of the problem in terms of multiple sequence segmentations satisfying a coupling constraint.", "labels": [], "entities": [{"text": "multiple alignment problem", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.8200137813886007}]}, {"text": "We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information.", "labels": [], "entities": []}], "introductionContent": [{"text": "String-to-string translation (string transduction) is the problem of converting one string x over an alphabet \u03a3 into another stringy over a possibly different alphabet \u0393.", "labels": [], "entities": [{"text": "String-to-string translation (string transduction)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.7599451492230097}]}, {"text": "The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration ( , lemmatization (, and spelling error correction).", "labels": [], "entities": [{"text": "string-to-string translation in natural language processing (NLP)", "start_pos": 35, "end_pos": 100, "type": "TASK", "confidence": 0.7516931394735972}, {"text": "graphemeto-phoneme conversion", "start_pos": 105, "end_pos": 134, "type": "TASK", "confidence": 0.7337653487920761}, {"text": "spelling error correction", "start_pos": 237, "end_pos": 262, "type": "TASK", "confidence": 0.5706252952416738}]}, {"text": "The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data.", "labels": [], "entities": []}, {"text": "Thereby, all state-of-the-art modelings we are aware of (e.g.,) proceed by first aligning the string pairs (x, y) in the training data.", "labels": [], "entities": []}, {"text": "Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence ph oe n ix f in I ks: Sample monotone many-to-many alignment between x = phoenix and y = finIks.", "labels": [], "entities": []}, {"text": "characters maybe matched up with several y sequence characters; illustrates.", "labels": [], "entities": []}, {"text": "Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, string-to-string translation maybe seen as a sequence labeling (tagging) problem in which x (sub-)sequence characters are observed variables and y (sub-)sequence characters are hidden states.", "labels": [], "entities": [{"text": "string-to-string translation", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.7634885907173157}, {"text": "sequence labeling (tagging)", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.7857317090034485}]}, {"text": "In this work, we extend the problem of classical string-to-string translation by assuming that, at training time, we have available (M + 2)-tuples of strings {(x, \u02c6 y (1) , . .", "labels": [], "entities": [{"text": "string-to-string translation", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7921213805675507}]}, {"text": ", \u02c6 y (M ) , y)}, where x is the input string, \u02c6 y (m) , for 1 \u2264 m \u2264 M , are supplemental information strings, and y is the desired output string; attest time, we wish to predict y from (x, \u02c6 y (1) , . .", "labels": [], "entities": []}, {"text": "Generally, we may think of\u02c6yof\u02c6 of\u02c6y , . .", "labels": [], "entities": []}, {"text": ", \u02c6 y (M ) as arbitrary strings over arbitrary alphabets \u03a3 (m) , for 1 \u2264 m \u2264 M . For example, x might be a letter-string and\u02c6yand\u02c6 and\u02c6y (m) might be a transliteration of x in language L m (cf.).", "labels": [], "entities": []}, {"text": "Alternatively, and this is our model scenario in the current work, x might be a letter input string and\u02c6yand\u02c6 and\u02c6y (m) might be the predicted string of phonemes, given x, produced by an (offline) system Tm . This situation is outlined in.", "labels": [], "entities": []}, {"text": "In the table, we also illustrate a multiple (monotone) many-to-many alignment of (x, \u02c6 y (1) , . .", "labels": [], "entities": []}, {"text": ", \u02c6 y (M ) , y).", "labels": [], "entities": []}, {"text": "By this, we mean an alignment where (1) subsequences of all M + 2 strings maybe matched up with each other (many-to-many alignments), and where (2) the matching up of subsequences obeys monotonicity.", "labels": [], "entities": []}, {"text": "Note that such a multiple alignment generalizes classical monotone many-to-many alignments between pairs of strings, as shown in.", "labels": [], "entities": []}, {"text": "Furthermore, such an alignment may apparently be quite useful.", "labels": [], "entities": []}, {"text": "For instance, while none of the strings\u02c6ystrings\u02c6 strings\u02c6y (m) in the table equals the true phonetic transcription y of x, taking a position-wise majority vote of the multiple alignment of (\u02c6 y (1) , . .", "labels": [], "entities": []}, {"text": ", \u02c6 y (M ) ) yields y.", "labels": [], "entities": []}, {"text": "Moreover, analogously as in the case of pairs of aligned strings, we may perceive the so extended stringto-string translation problem as a sequence labeling task once (x, \u02c6 y (1) , . .", "labels": [], "entities": [{"text": "stringto-string translation", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.7596487998962402}]}, {"text": ", \u02c6 y (M ) , y) are multiply aligned, but now, with additional observed variables (or features), namely, (sub-)sequence characters of each string\u02c6ystring\u02c6 string\u02c6y . To further motivate our approach, consider the situation of training anew G2P system on the basis of, e.g.,).", "labels": [], "entities": []}, {"text": "For each letter form in its database, Combilex provides a corresponding phonetic transcription.", "labels": [], "entities": []}, {"text": "Now, suppose that, in addition, we can poll an external knowledge source such as Wiktionary for (its) phonetic transcriptions of the respective Combilex letter words as outlined in  tral question we want to answer is: can we train a system using this additional information which performs better than the 'baseline' system that ignores the extra information?", "labels": [], "entities": []}, {"text": "Clearly, a system with more information should not perform worse than a system with less information (unless the additional information is highly noisy), but it is a priori not clear at all how the extra information can be included, as note: output predictions maybe in distinct alphabets and/or follow different conventions, and simple rule-based conversions may even deteriorate a baseline system's performance.", "labels": [], "entities": []}, {"text": "Their solution to the problem is to let the baseline system output its n-best phonetic transcriptions, and then to re-rank these n-best predictions via an SVM reranker trained on the supplemental representations x = schizo sch i z o \u02c6 y (1) = skaIz@U s k aI z @U\u02c6y @U\u02c6 @U\u02c6y (2) = saIz@U s -aI z @U\u02c6y @U\u02c6 @U\u02c6y (3) = skIts@ s k I ts @ \u02c6 y (4) = Sits@U Si ts @U\u02c6y @U\u02c6 @U\u02c6y (5) = skIts@ s k I ts @ y = skIts@U s k I ts @U: Left: Input string x, predictions of 5 systems, and output stringy.", "labels": [], "entities": []}, {"text": "Right: A multiple many-to-many alignment of (x, \u02c6 y (1) , . .", "labels": [], "entities": []}, {"text": ", \u02c6 y (5) , y).", "labels": [], "entities": []}, {"text": "Skips are marked by a dash ('-').", "labels": [], "entities": []}, {"text": "Our approach is much different from this: we character (or substring) align the supplemental information strings with the input letter strings and then sequentially transduce input character substrings as in the standard G2P approach, but where the sequential transducer is aware of the corresponding subsequences of the supplemental information strings.", "labels": [], "entities": []}, {"text": "Our goals in the current work are first, in Section 2, to formally introduce the multiple manyto-many alignment problem, which, to our knowledge, has not yet been formally considered, and to indicate how it can be solved (by standard extensions of well-known DP recursions).", "labels": [], "entities": [{"text": "multiple manyto-many alignment problem", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.6935896575450897}]}, {"text": "Secondly, we outline an 'approximation algorithm', also in Section 2, with much better runtime complexity, to solving the multiple many-to-many alignment problem.", "labels": [], "entities": [{"text": "multiple many-to-many alignment problem", "start_pos": 122, "end_pos": 161, "type": "TASK", "confidence": 0.6842531934380531}]}, {"text": "This proceeds by optimally segmenting individual strings to align under the global constraint that the number of segments must agree across strings.", "labels": [], "entities": []}, {"text": "Thirdly, we demonstrate experimentally, in Section 5, that multiple many-tomany alignments maybe an extremely useful first step in boosting the performance of a G2P model.", "labels": [], "entities": []}, {"text": "In particular, we show that by conjoining abase system with additional systems very high performance increases can be achieved.", "labels": [], "entities": []}, {"text": "We also investigate the effects of using our introduced approximation algorithm instead of 'exactly' determining alignments.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 3, present data and systems in Section 4 and conclude in Section 6. 2 Mult.", "labels": [], "entities": [{"text": "Mult", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.7411935329437256}]}, {"text": "Models We now formally define the problem of multiply aligning several strings in a monotone and manyto-many alignment manner.", "labels": [], "entities": []}, {"text": "For notational convenience, in this section, let the N strings to align be denoted by w 1 , . .", "labels": [], "entities": []}, {"text": ", w N (rather than x, \u02c6 y (m) , y, etc.).", "labels": [], "entities": []}, {"text": "Let each w n , for 1 \u2264 n \u2264 N , bean arbitrary string over some alphabet \u03a3 (n) . Let n = |w n | denote the length of w n . Moreover, assume that a set S \u2286 N n=1 {0, . .", "labels": [], "entities": []}, {"text": ", n }\\{0 N } of allowable steps is specified, where 0 N = (0, . .", "labels": [], "entities": []}, {"text": ", 0 N times ).", "labels": [], "entities": []}, {"text": "We interpret the elements of S as follows: if (s 1 , s 2 , . .", "labels": [], "entities": []}, {"text": ", s N ) \u2208 S, then subsequences of w 1 of lengths 1 , subsequences of w 2 of lengths 2 , . .", "labels": [], "entities": []}, {"text": "., subsequences of w N of lengths N maybe matched up with each other.", "labels": [], "entities": []}, {"text": "In other words, S defines the types of valid 'many-to-many match-up operations'.", "labels": [], "entities": []}, {"text": "While we could drop S from consideration and simply allow every possible matching up of character subsequences, it is convenient to introduce S because algorithmic complexity may then be specified in terms of S, and by choosing particular S, one may retrieve special cases otherwise considered in the literature (see next section).", "labels": [], "entities": []}, {"text": "As indicated, for us, a multiple alignment of (w 1 , . .", "labels": [], "entities": []}, {"text": ", w N ) is any scheme such that (|w 1,i | , . .", "labels": [], "entities": []}, {"text": ", |w N,i |) \u2208 S, for all i = 1, . .", "labels": [], "entities": []}, {"text": ", k, and such that w n = w n,1 \u00b7 \u00b7 \u00b7 w n,k , for all 1 \u2264 n \u2264 N . Let A S = A S (w 1 , . .", "labels": [], "entities": []}, {"text": ", w N ) denote the set of all multiple alignments of (w 1 , . .", "labels": [], "entities": []}, {"text": "For an alignment a \u2208 A S , denote by score(a) = f (a) the score of alignment a under alignment model f , where f : We now investigate solutions to the problem of finding the alignment with maximal score under different choices of alignment models f , i.e., we search to efficiently solve Unigram alignment model For our first alignment model f , we assume that f (a), fora \u2208 A S , is the score 1 Here, denotes the Cartesian product of sets.", "labels": [], "entities": []}, {"text": "In the case of two strings, this is sometimes denoted in the manner M -N (e.g., 3-2, 1-0), indicating that M characters of one string maybe matched up with N characters of the other string.", "labels": [], "entities": []}, {"text": "Analogously, we could write here s1-s2-s3-\u00b7 \u00b7 \u00b7 . fora real-valued similarity function is the sum of the similarity scores of the matched-up subsequences (w 1,i , . .", "labels": [], "entities": []}, {"text": ", w N,i ), ignoring context.", "labels": [], "entities": []}, {"text": "Due to this independence assumption, solving maximization problem in Eq. under specification (2) is straightforward via a dynamic programming (DP) recursion.", "labels": [], "entities": []}, {"text": "To do so, define by M S,sim 1 (i 1 , i 2 , . .", "labels": [], "entities": []}, {"text": ", i N ) the score of the best alignment, under alignment model f = sim 1 and set of steps S, of (w 1 (1 : This recurrence directly leads to a DP algorithm, shown in Algorithm 1, for computing the score of the best alignment of (w 1 , . .", "labels": [], "entities": []}, {"text": ", w N ); the actual alignment can be found by storing pointers to the maximizing steps taken.", "labels": [], "entities": []}, {"text": "If similarity evaluations sim 1 (w 1,i , . .", "labels": [], "entities": []}, {"text": ", w N,i ) are thought of as taking constant time, this algorithm's run time is O( N n=1 n \u00b7 |S|).", "labels": [], "entities": []}, {"text": "When = 1 = \u00b7 \u00b7 \u00b7 = n and |S| = N \u2212 1 ('worst case' size of S), then the algorithm's runtime is thus O( 2N ), which quickly becomes untractable as N , the number of strings to align, increases.", "labels": [], "entities": [{"text": "O", "start_pos": 100, "end_pos": 101, "type": "METRIC", "confidence": 0.9654418230056763}]}, {"text": "Of course, the unigram alignment model could be generalized to an m-gram alignment model.", "labels": [], "entities": []}, {"text": "An m-gram alignment model would exhibit worstcase runtime complexity of O( (m+1)N ) under analogous DP recursions as for the unigram model.", "labels": [], "entities": [{"text": "O", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.9634430408477783}]}], "datasetContent": [{"text": "We now describe two sets of experiments, a controlled experiment on the Combilex data set where we can design our offline/black box systems ourselves and where the black box systems are trained on a similar distribution as the baseline and the extended baseline systems.", "labels": [], "entities": [{"text": "Combilex data set", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9560513297716776}]}, {"text": "In particular, the black box systems operate on the same output alphabet as the extended baseline systems, which constitutes an 'ideal' situation.", "labels": [], "entities": []}, {"text": "Thereafter, we investigate how our extended baseline system performs in a 'real-world' scenario: we train a system on Combilex that has as supplemental information corresponding Wiktionary (and PTE, as explained below) transcriptions.", "labels": [], "entities": [{"text": "PTE", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.8037654757499695}]}, {"text": "Throughout, we use as accuracy measures for all our systems word accuray (WACC).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9991186261177063}]}, {"text": "Word accuracy is defined as the number of correctly transcribed strings among all transcribed strings in a test sample.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.8615396022796631}]}, {"text": "WACC is a strict measure that penalizes even tiny deviations from the gold-standard transcriptions, but has nowadays become standard in G2P.", "labels": [], "entities": [{"text": "WACC", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5894689559936523}]}, {"text": "In our first set of experiments, we let our offline/black box systems be the Sequitur G2P modeling toolkit) (S) and the Phonetisaurus modeling toolkit) (P).", "labels": [], "entities": []}, {"text": "We train them on disjoint sets of 20,000 grapheme-to-phoneme Combilex string pairs each.", "labels": [], "entities": []}, {"text": "The performance of these two systems, on the test set of size 28,000, is indicated in.", "labels": [], "entities": []}, {"text": "Next, we train BASELINE on disPhonetisaurus Sequitur WACC 72.12 71.70: Word-accuracy (in %) on the test data, for the two systems indicated.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9943969249725342}, {"text": "disPhonetisaurus Sequitur WACC 72.12 71.70", "start_pos": 27, "end_pos": 69, "type": "DATASET", "confidence": 0.8965773701667785}, {"text": "Word-accuracy", "start_pos": 71, "end_pos": 84, "type": "METRIC", "confidence": 0.8772530555725098}]}, {"text": "joint sets (disjoint from both the training sets of P and S) of size 2,000, 5,000, 10,000 and 20,000.", "labels": [], "entities": []}, {"text": "Making BASELINE's training sets disjoint from the training sets of the offline systems is both realistic (since a black box system would typically follow a partially distinct distribution from one's own training set distribution) and also prevents the extended baseline systems from fully adapting to the predictions of either P or S, whose training set accuracy is an upward biased representation of their true accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 354, "end_pos": 362, "type": "METRIC", "confidence": 0.8214737176895142}, {"text": "accuracy", "start_pos": 412, "end_pos": 420, "type": "METRIC", "confidence": 0.9515252113342285}]}, {"text": "As baseline extensions, we consider the systems BASELINE+P (+P), and BASELINE+P+S (+P+S).", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.6878244876861572}, {"text": "BASELINE", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.7264953851699829}]}, {"text": "8 Results are shown in.", "labels": [], "entities": []}, {"text": "We see that conjoining the base system with the predictions of the offline Phonetisaurus and Sequitur models substantially increases the baseline WACC, especially in the case of little training data.", "labels": [], "entities": [{"text": "WACC", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.31913119554519653}]}, {"text": "In fact, WACC increases hereby almost 100% when the baseline system is complemented by\u02c6yby\u02c6 by\u02c6y (P) and\u02c6yand\u02c6 and\u02c6y . As training set size increases, differences become less and less pronounced.", "labels": [], "entities": [{"text": "WACC", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.8316059708595276}]}, {"text": "Eventually, we would expect them to drop to zero, since beyond some training set size, the additional features may provide no new information.", "labels": [], "entities": []}, {"text": "We also note that conjoining the two systems is more valuable than conjoining only one system, and, in, that the models which are based on exact multiple alignments outperform the models based on approximate alignments, but not We omit BASELINE+S since it yielded similar results as BASELINE+P.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.8455524444580078}]}, {"text": "In fact, in follow-up work, we find that the additional information may also confuse the base system when training set sizes are large enough. by a wide margin.", "labels": [], "entities": []}, {"text": "Concerning differences in alignments between the two alignment types, exact vs. approximate, an illustrative example where the approximate model fails and the exact model does not is ('false' alignment based on the approximate model indicated): r ee n t er ed r i E n t @' rd r i E n t @' rd which nicely captures the inability of the approximate model to account for correlations between the matched-up subsequences.", "labels": [], "entities": []}, {"text": "That is, while the segmentations of the three shown sequences appear acceptable, a matching of graphemic t with phonemic n, etc., seems quite unlikely.", "labels": [], "entities": []}, {"text": "Still, it is very promising to see that these differences in alignment quality translate into very small differences in overall string-to-string translation model performance, as outlines.", "labels": [], "entities": []}, {"text": "Namely, differences in WACC are typically on the level of 1% or less (always in favor of the exact alignment model).", "labels": [], "entities": [{"text": "WACC", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.8868927955627441}]}, {"text": "This is a very important finding, as it indicates that string-to-string translation need not be (severely) negatively impacted by switching to the approximate alignment model, a tractable alternative to the exact models, which quickly become practically infeasible as the number of strings to align increases.", "labels": [], "entities": [{"text": "string-to-string translation", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.7241567075252533}]}, {"text": "To test whether our approach may also succeed in a 'real-world setting', we use as offline/black box systems GA Wiktionary transcriptions of our input forms as well as PhotoTransEdit (PTE) transcriptions, 10 a lexicon-based G2P system which offers both GA and RP (received pronunciation) transcription of English strings.", "labels": [], "entities": [{"text": "RP", "start_pos": 260, "end_pos": 262, "type": "METRIC", "confidence": 0.9686082005500793}]}, {"text": "We train and test on input strings for which both Combilex and PTE transcriptions are available, and for which both Combilex and Wiktionary transcriptions are available.", "labels": [], "entities": [{"text": "PTE", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.6795826554298401}]}, {"text": "11 Test set sizes are about 1,500 in the case of PTE and 3,500 in the case of Wiktionary.", "labels": [], "entities": [{"text": "PTE", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.5499085783958435}, {"text": "Wiktionary", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9294462203979492}]}, {"text": "We only test here the performance of the exact alignment method, noting that, as before, approximate alignments produced slightly weaker results.", "labels": [], "entities": []}, {"text": "Clearly, Wiktionary and PTE differ from the Combilex data.", "labels": [], "entities": [{"text": "PTE", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.7666193246841431}, {"text": "Combilex data", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9366344809532166}]}, {"text": "First, both Wiktionary and PTE use different numbers of phonemic symbols than Combilex, as  arise from the fact that, e.g., lengthening of vowels is indicated by two output letters in some data sets and only one in others.", "labels": [], "entities": [{"text": "PTE", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7923189997673035}]}, {"text": "Also, phonemic transcription conventions differ, as becomes most strikingly evident in the case of RP vs. GA transcriptions - illustrates.", "labels": [], "entities": []}, {"text": "Finally, Wiktionary has many more phonetic symbols than the other datasets, a finding that we attribute to its crowd-sourced nature and lacking of normalization.", "labels": [], "entities": []}, {"text": "Despite these differences in phonemic annotation standards between Combilex, Wiktionary and PTE, we observe that conjoining input strings with predicted Wiktionary or PTE transcriptions via multiple alignments leads to very good improvements in WACC over only using the input string as information source.", "labels": [], "entities": [{"text": "PTE", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.8715252876281738}, {"text": "WACC", "start_pos": 245, "end_pos": 249, "type": "TASK", "confidence": 0.7159286737442017}]}, {"text": "Indeed, as shown in, for PTE, WACC increases by as much as 80% in case of small training sample (1,099 string pairs) and as much as 37% in case of medium-sized training sample (2,687 string pairs).", "labels": [], "entities": [{"text": "PTE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.5401216745376587}, {"text": "WACC", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9713515043258667}]}, {"text": "Thus, comparing with the previous situation of homogenous systems, we also observe that the gain from including heterogeneous system is relatively weaker, as we would expect due to distinct underlying assumptions, but still impressive.", "labels": [], "entities": []}, {"text": "Performance increases when including Wiktionary are slightly lower, most likely because it constitutes a very heterogenous source of phonetic transcriptions with user-idiosyncratic annotations (however, training set sizes are also different", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Sizes of phonetic inventaries of different  data sets.", "labels": [], "entities": []}, {"text": " Table 7: Top: WACC in % for baseline CRF  model and the models that integrate PTE in the  GA versions and RP versions, respectively. Bot- tom: BASELINE and BASELINE+Wiktionary.", "labels": [], "entities": [{"text": "WACC", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.976805567741394}, {"text": "PTE", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8845454454421997}, {"text": "BASELINE", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.939859926700592}]}]}