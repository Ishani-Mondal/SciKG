{"title": [{"text": "Learning Lexical Embeddings with Syntactic and Lexicographic Knowledge", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose two improvements on lexical association used in embedding learning: factorizing individual dependency relations and using lexicographic knowledge from monolingual dictionaries.", "labels": [], "entities": []}, {"text": "Both proposals provide low-entropy lexical co-occurrence information, and are empirically shown to improve embedding learning by performing notably better than several popular embedding models in similarity tasks.", "labels": [], "entities": [{"text": "embedding learning", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8693176507949829}]}, {"text": "1 Lexical Embeddings and Relatedness Lexical embeddings are essentially real-valued distributed representations of words.", "labels": [], "entities": []}, {"text": "As a vector-space model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks.", "labels": [], "entities": []}, {"text": "In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pen-nington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014).", "labels": [], "entities": []}, {"text": "To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Eu-clidean similarity between the embeddings of related words.", "labels": [], "entities": []}, {"text": "Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer.", "labels": [], "entities": []}, {"text": "The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models.", "labels": [], "entities": []}, {"text": "In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts.", "labels": [], "entities": []}, {"text": "Although supported by the distributional hypothesis (Harris, 1954), this definition suffers from two major limitations.", "labels": [], "entities": []}, {"text": "Firstly, the window frame size is usually rather small (for efficiency and sparsity considerations), which increases the false negative rate by missing long-distance dependencies.", "labels": [], "entities": [{"text": "false negative rate", "start_pos": 121, "end_pos": 140, "type": "METRIC", "confidence": 0.8219276666641235}]}, {"text": "Secondly , a window frame can (and often does) span across different constituents in a sentence, resulting in an increased false positive rate by associating unrelated words.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 123, "end_pos": 142, "type": "METRIC", "confidence": 0.8445235093434652}]}, {"text": "The problem is worsened as the size of the window increases since each false-positive n-gram will appear in two subsuming false-positive (n + 1)-grams.", "labels": [], "entities": []}, {"text": "Several existing studies have addressed these limitations of window-based contexts.", "labels": [], "entities": []}, {"text": "Nonetheless , we hypothesize that lexical embedding learning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge.", "labels": [], "entities": []}, {"text": "We will show that implementation of these ideas brings notable improvement in lexical similarity tasks.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Correlation between human judgement  and cosine similarity of embeddings (trained on  the Gigaword corpus) on six similarity datasets.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.9610551297664642}]}, {"text": " Table 2: Lexical similarity performance of  relation-independent models (trained on the 17M  corpus) combining top two best-performing rela- tions for each POS.", "labels": [], "entities": [{"text": "17M  corpus", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9783948063850403}]}, {"text": " Table 2. The combined results  improve over the best relation-dependent mod- els for all categories except for SL a (adjectives),  where only the top-performing relation-dependent  model (amod \u22121 ) yielded statistically significant  results and thus, results are worsened by com- bining the second-best relation-dependent source  dobj \u22121 (which is essentially noise). Compar- ing to baselines, the relation-independent model  achieves better results in four out of the six cat-", "labels": [], "entities": []}, {"text": " Table 3: Lexical similarity performance of mod- els using dictionary definitions and compared to  word2vec trained on the Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.9460994303226471}]}]}