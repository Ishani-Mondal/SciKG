{"title": [{"text": "Joint Dependency Parsing and Multiword Expression Tokenisation", "labels": [], "entities": [{"text": "Multiword Expression Tokenisation", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7076991697152456}]}], "abstractContent": [{"text": "Complex conjunctions and determiners are often considered as pretokenized units in parsing.", "labels": [], "entities": []}, {"text": "This is not always realistic, since they can be ambiguous.", "labels": [], "entities": []}, {"text": "We propose a model for joint dependency parsing and multiword expressions identification, in which complex function words are represented as individual tokens linked with morphological dependencies.", "labels": [], "entities": [{"text": "joint dependency parsing", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6469304064909617}, {"text": "multiword expressions identification", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.7279330094655355}]}, {"text": "Our graph-based parser includes standard second-order features and verbal subcategoriza-tion features derived from a syntactic lexicon .We train it on a modified version of the French Treebank enriched with morphological dependencies.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 177, "end_pos": 192, "type": "DATASET", "confidence": 0.9327710866928101}]}, {"text": "It recognizes 81.79% of ADV+que conjunctions with 91.57% precision, and 82.74% of de+DET determiners with 86.70% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9980393052101135}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9949710369110107}]}], "introductionContent": [{"text": "Standard NLP tool suites for text analysis are often made of several processes that are organized as a pipeline, in which the input of a process is the output of the preceding one.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7612410187721252}]}, {"text": "Among these processes, one commonly finds a tokenizer, which segments a sentence into words, a part-of-speech (POS) tagger, which associates to every word a part-of-speech tag, and a syntactic parser, which builds a parse tree for the sentence . These three processes correspond to three formal operations on the string: segmentation into linguistically relevant units (words), tagging the words with POS tags and linking the (word, POS) pairs by means of syntactic dependencies.", "labels": [], "entities": [{"text": "segmentation into linguistically relevant units (words)", "start_pos": 321, "end_pos": 376, "type": "TASK", "confidence": 0.7568272054195404}]}, {"text": "This setup is clearly not ideal, as some decisions are made too early in the pipeline (.", "labels": [], "entities": []}, {"text": "More specifically, some tokenization and tagging choices are difficult to make without taking syntax into account.", "labels": [], "entities": []}, {"text": "To avoid the pitfall of premature decisions, probabilistic tokenizers and taggers can produce several solutions in the form of lattices ().", "labels": [], "entities": []}, {"text": "Such approaches usually lead to severe computational overhead due to the huge search space in which the parser looks for the optimal parse tree.", "labels": [], "entities": []}, {"text": "Besides, the parser might be biased towards short solutions, as it compares scores of trees associated to sequences of different lengths.", "labels": [], "entities": []}, {"text": "This problem is particularly hard when parsing multiword expressions (MWEs), that is, groups of tokens that must be treated as single units.", "labels": [], "entities": [{"text": "parsing multiword expressions (MWEs)", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.8922393421332041}]}, {"text": "The solution we present in this paper is different from the usual pipeline.", "labels": [], "entities": []}, {"text": "We propose to jointly parse and tokenize MWEs, transforming segmentation decisions into linking decisions.", "labels": [], "entities": [{"text": "tokenize MWEs", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.7311239838600159}]}, {"text": "Our experiments concentrate on two difficult tokenization cases.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.9859597682952881}]}, {"text": "Hence, it is the parser that will choose, in such cases, whether to group or not several tokens.", "labels": [], "entities": []}, {"text": "Our first target phenomenon is the family of ADV+que constructions, a type of complex conjunction in French.", "labels": [], "entities": []}, {"text": "They are formed by adverbs like bien (well) or ainsi (likewise) followed by the subordinative conjunction que (that).", "labels": [], "entities": []}, {"text": "They function like English complex conjunctions so that and now that.", "labels": [], "entities": []}, {"text": "Due to their structure, ADV+que constructions are generally ambiguous, like in the following examples: 1.", "labels": [], "entities": []}, {"text": "Je mange bien que je n'aie pas faim I eat although I am not hungry 2.", "labels": [], "entities": []}, {"text": "Je pense bien que je n'ai pas faim I think indeed that I am not hungry In example 1, the sequence bien que forms a complex conjunction (although) whereas in example 2, the adverb bien (indeed) modifies the verb pense (think), and the conjunction que (that) introduces the sentential complement je n'ai pas faim (I am not hungry).", "labels": [], "entities": []}, {"text": "In treebanks, the different readings are represented through the use of wordswith-spaces in the case of complex conjunctions.", "labels": [], "entities": []}, {"text": "Our second target phenomenon is the family of partitive articles which are made of the preposition de (of ) followed by the definite determiner le, la, l' or les 2 (the).", "labels": [], "entities": []}, {"text": "These de+DET constructions are ambiguous, as shown in the following examples: 3.", "labels": [], "entities": [{"text": "DET", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9744395613670349}]}, {"text": "Il boit de la b\u00ec ere He drinks some beer 4.", "labels": [], "entities": []}, {"text": "Il parle de lab\u00ec ere I talks about the beer In example 3, the sequence de la forms a determiner (some) whereas in example 4, de is a preposition (about) and la is the determiner (the) of the nounb\u00ec ere b\u00ec ere).", "labels": [], "entities": []}, {"text": "We focus on these constructions for two reasons.", "labels": [], "entities": []}, {"text": "First, because they are extremely frequent.", "labels": [], "entities": []}, {"text": "For instance, in the frWaC corpus, from a total of 54.8M sentences, 1.15M sentences (2.1%) contain one or more occurrences of our target ADV+que constructions and 26.7M sentences (48.6%) contain a de+DET construction (see.", "labels": [], "entities": [{"text": "frWaC corpus", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.968407392501831}]}, {"text": "Moreover, in a corpus of 370 M words in French, 3 des is the 7 th most frequent word.", "labels": [], "entities": []}, {"text": "Second, because they are perfect examples of phenomena which are difficult to process by a tokenizer.", "labels": [], "entities": []}, {"text": "In order to decide, in example 1, that bien que is a complex subordinate conjunction, non-trivial morphological, lexical and syntactic clues must betaken into account, such as the subcategorization frame of the verb of the principal clause and the mood of the subordinate clause.", "labels": [], "entities": []}, {"text": "All these clues are difficult to take into account during tokenization, where the syntactic structure of the sentence is not yet explicit.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.9769374132156372}]}, {"text": "Ask the parser to perform tokenization will not always solve the problem.", "labels": [], "entities": []}, {"text": "Even state-of-the-art parsers can fail to predict the right structure for the cases we are dealing with.", "labels": [], "entities": []}, {"text": "The main reason is that they are trained on treebanks of limited size, and some lexico-syntactic phenomena cannot be well modeled.", "labels": [], "entities": []}, {"text": "This brings us to the second topic of this paper, which is the integration of external linguistic resources in a treebank-trained probabilistic parser.", "labels": [], "entities": []}, {"text": "We show that, in order to cor-2 Sequences de le and de les do not appear as such in French.", "labels": [], "entities": []}, {"text": "They have undergone a morpho-phonetic process known as amalgamation and are represented as tokens du and des.", "labels": [], "entities": []}, {"text": "In our pipeline, they are artificially detokenized.", "labels": [], "entities": []}, {"text": "3 Newspaper Le Monde from 1986 to 2002.", "labels": [], "entities": [{"text": "3 Newspaper Le Monde from 1986 to 2002", "start_pos": 0, "end_pos": 38, "type": "DATASET", "confidence": 0.908722274005413}]}, {"text": "rectly solve the two problems at hand, the parser must have access to lexico-syntactic information that can be found in a syntactic lexicon.", "labels": [], "entities": []}, {"text": "We propose a simple way to introduce such information in the parser by defining new linguistic features that blend smoothly with treebank features used by the parser when looking for the optimal parse tree.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 describes related work on MWE parsing.", "labels": [], "entities": [{"text": "MWE parsing", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9568284153938293}]}, {"text": "Section 3 proposes away to represent multiword units by means of syntactic dependencies.", "labels": [], "entities": []}, {"text": "In Section 4, we briefly describe the parser that has been used in this work, and in Section 5, we propose away to integrate a syntactic lexicon into the parser.", "labels": [], "entities": []}, {"text": "Section 6 describes the data sets used for the experiments, which results are presented and discussed in Section 7.", "labels": [], "entities": []}, {"text": "Section 8 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the proposed model to verify the linguistic plausibility and computational feasibility of using MORPH links to represent syntactically idiosyncratic MWEs in a dependency parser enriched with subcat features.", "labels": [], "entities": []}, {"text": "Therefore, we train a probabilistic dependency parsing model on modified treebank, representing ADV+que and de+DET constructions using this special syntactic relation instead of pretokenization.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7339943647384644}]}, {"text": "Furthermore, in addition to regular features learned from the treebank, we also introduce and evaluate subcat features based on a lexicon of verbal valency, which helps identifying subordinative clauses and de prepositional phrases (see Section 5).", "labels": [], "entities": []}, {"text": "We evaluate parsing precision and MWE identification on a test treebank and, more importantly, on a dataset built specifically to study the representation of our target constructions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9739171862602234}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9297006130218506}, {"text": "MWE identification", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9719662368297577}]}, {"text": "All experiments used the NLP tool suite MACAON 5 , which comprises a second-order graph-based parser.", "labels": [], "entities": []}, {"text": "We evaluate our models on two aspects: parsing quality and MWE identification).", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9833027720451355}, {"text": "MWE identification", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.961920291185379}]}, {"text": "First, we use standard parsing attachment scores to verify whether our models impact parsing performance in general.", "labels": [], "entities": [{"text": "parsing attachment", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9079931974411011}]}, {"text": "We compare the generated dependency trees with the reference in the test portion of the FTB, reporting the proportion of matched links, both in terms of structure -unlabeled attachment score (UAS) -and of labeled links -labeled attachment score (LAS).", "labels": [], "entities": [{"text": "FTB", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.9103753566741943}, {"text": "structure -unlabeled attachment score (UAS)", "start_pos": 153, "end_pos": 196, "type": "METRIC", "confidence": 0.7900157794356346}, {"text": "labeled links -labeled attachment score (LAS)", "start_pos": 205, "end_pos": 250, "type": "METRIC", "confidence": 0.8097344570689731}]}, {"text": "Since our focus is on MWE parsing, we are also interested in MWE identification metrics.", "labels": [], "entities": [{"text": "MWE parsing", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9528860747814178}, {"text": "MWE identification", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9627217054367065}]}, {"text": "We focus on words whose dependency label is MORPH and calculate the proportion of correctly predicted MORPH links among those in the parser output (precision), among those in the reference (recall) and the F1 average.", "labels": [], "entities": [{"text": "MORPH", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9588633179664612}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9922164678573608}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9893552660942078}, {"text": "F1", "start_pos": 206, "end_pos": 208, "type": "METRIC", "confidence": 0.9988638162612915}]}, {"text": "Since some of the phenomena are quite rare in the FTB test portion, we focus on the MORPH dataset, which contains around 100 instances of each target construction.", "labels": [], "entities": [{"text": "FTB test", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7274604737758636}, {"text": "MORPH dataset", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.7731100618839264}]}, {"text": "We compare our approach with two simple baselines.", "labels": [], "entities": []}, {"text": "The first one consists in pretokenizing ADV+que systematically as a single token, while de+DET is systematically left as two separate tokens.", "labels": [], "entities": [{"text": "DET", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.8738003373146057}]}, {"text": "This baseline emulates the behavior of most parsing pipelines, which deal with functional complex words during tokenization.", "labels": [], "entities": [{"text": "parsing pipelines", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.9149009585380554}]}, {"text": "This corresponds to choosing the majority classes in the last row of.", "labels": [], "entities": []}, {"text": "For ADV+que, the precision of the baseline is 56.4%.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9997057318687439}]}, {"text": "If we assume recall is 100%, this yields an F1 score of 72.2%.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9995880722999573}, {"text": "F1 score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9875328540802002}]}, {"text": "For de+DET, however, recall is 0% since no MORPH link is predicted at all.", "labels": [], "entities": [{"text": "DET", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.7670592069625854}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996836185455322}, {"text": "MORPH link", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9663640558719635}]}, {"text": "Therefore, we only look at the baseline's precision of 63.5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9990231990814209}]}, {"text": "A second, slightly more sophisticated baseline, consists in choosing the majority class for each individual construction and average precisions over the constructions.", "labels": [], "entities": [{"text": "precisions", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.992643415927887}]}, {"text": "In this case, the average precision is 75.3% for ADV+que and 76.6% for de+DET.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9993719458580017}]}, {"text": "We compare our model to the one proposed by.", "labels": [], "entities": []}, {"text": "We used the pretrained model available as part of the Stanford parser 9 . Their model outputs constituent trees, which were automatically converted to unlabeled dependency structures.", "labels": [], "entities": []}, {"text": "We ignore the nature of the dependency link, only checking whether the target construction elements are linked in the correct order.", "labels": [], "entities": []}, {"text": "Our experiments use the MACAON tool suite.", "labels": [], "entities": [{"text": "MACAON tool", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.8712702989578247}]}, {"text": "For the FTB, gold POS and gold lemmas are given as input to the parser.", "labels": [], "entities": [{"text": "FTB", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8942522406578064}]}, {"text": "In the case of the MORPH dataset, for which we do not have gold POS and lemmas, they are predicted by MACAON.", "labels": [], "entities": [{"text": "MORPH dataset", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.8995682895183563}, {"text": "POS", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9619380831718445}, {"text": "MACAON", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.6425938606262207}]}, {"text": "The first best prediction is given as input to the parser..", "labels": [], "entities": []}, {"text": "It is therefore difficult draw clear conclusions concerning the task of predicting the MORPH dependency.", "labels": [], "entities": [{"text": "MORPH", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.5871844291687012}]}, {"text": "The precision and recall have nevertheless been reported.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9997077584266663}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.999743640422821}]}, {"text": "The recall is perfect (all MORPH dependencies have been predicted) and the the precision is reasonable (the parser overpredicts a little).", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9994764924049377}, {"text": "MORPH", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9847827553749084}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9997072815895081}]}, {"text": "The table also shows that the use of subcat features is not beneficial, as attachment scores as well as precision decrease.", "labels": [], "entities": [{"text": "attachment", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.97190922498703}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9997007846832275}]}, {"text": "The decrease of precision is misleading, though, due to the small number of occurrences it has been computed on. displays the precision, recall and F1 of the prediction of the MORPH dependency on the 730 ADV+que sentences of the MORPH dataset, without and with the use of subcat features.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9992021918296814}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9996564388275146}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.999097466468811}, {"text": "F1", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.9994763731956482}, {"text": "MORPH dataset", "start_pos": 229, "end_pos": 242, "type": "DATASET", "confidence": 0.8890374004840851}]}, {"text": "The scores obtained are lower than the same experiments on the FTB.Precision is higher than recall, which indicates that the parser has a tendency to underpredict.", "labels": [], "entities": [{"text": "FTB.Precision", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.8696836829185486}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9996156692504883}]}, {"text": "We also present the precision of the two baselines described in Section 6.2.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9996053576469421}]}, {"text": "Only in two cases the per-construction majority baseline (indiv.) outperforms our parser without subcat features.", "labels": [], "entities": []}, {"text": "These two constructions do not tend to form complex conjunctions, that is, the parser overgenerates MORPH dependencies.", "labels": [], "entities": []}, {"text": "Here, subcat features help increasing precision, systematically outperforming the baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.998914361000061}]}], "tableCaptions": [{"text": " Table 1: Annotations for ADV+que combinations  in MORPH dataset: number of annotated sen- tences, proportion (%) of complex conjunction  uses (MORPH) and other uses, number of occur- rences in frWaC.", "labels": [], "entities": [{"text": "MORPH dataset", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.8983129560947418}]}, {"text": " Table 2: Annotations for de+DET combina- tions MORPH dataset: number of annotated sen- tences, proportion (%) of complex determiner uses  (MORPH) and other uses, number of occurrences  in frWaC.", "labels": [], "entities": [{"text": "DET combina- tions", "start_pos": 29, "end_pos": 47, "type": "METRIC", "confidence": 0.8723190873861313}, {"text": "MORPH dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.7778428196907043}, {"text": "frWaC", "start_pos": 189, "end_pos": 194, "type": "DATASET", "confidence": 0.9179509282112122}]}, {"text": " Table 3: Number of verbs in DicoValence per  value of subcat feature.", "labels": [], "entities": []}, {"text": " Table 5: MORPH link prediction for ADV+que constructions: precision of global majority baseline, preci- sion of individual per-construction baseline, precision of Green et al. (2013) constituent parser, precision,  recall and F1 of our dependency parser without and with subcat features.", "labels": [], "entities": [{"text": "MORPH link prediction", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.659273495276769}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9989193677902222}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.998965859413147}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9995259046554565}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9995843768119812}, {"text": "F1", "start_pos": 227, "end_pos": 229, "type": "METRIC", "confidence": 0.9995193481445312}]}, {"text": " Table 6: Attachment scores, count, precision and  recall of the MORPH dependency for de+DET in  FTB test, without and with subcat features (SF).", "labels": [], "entities": [{"text": "Attachment", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9828737378120422}, {"text": "count", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.997058629989624}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9996122717857361}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9995822310447693}, {"text": "MORPH", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9284006953239441}, {"text": "DET", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.6661256551742554}, {"text": "FTB test", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.8665787875652313}]}, {"text": " Table 7: MORPH link prediction for de+DET constructions: precision of global majority baseline, preci- sion of individual per-construction baseline, precision of Green et al. (2013) constituent parser, precision,  recall and F1 of our dependency parser without and with subcat features.", "labels": [], "entities": [{"text": "MORPH link prediction", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6749886274337769}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.99892657995224}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9989877343177795}, {"text": "precision", "start_pos": 203, "end_pos": 212, "type": "METRIC", "confidence": 0.9995493292808533}, {"text": "recall", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.9995918869972229}, {"text": "F1", "start_pos": 226, "end_pos": 228, "type": "METRIC", "confidence": 0.9995427131652832}]}]}