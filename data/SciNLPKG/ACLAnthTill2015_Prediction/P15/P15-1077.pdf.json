{"title": [{"text": "Gaussian LDA for Topic Models with Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language.", "labels": [], "entities": []}, {"text": "In this paper we replace LDA's param-eterization of \"topics\" as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space.", "labels": [], "entities": []}, {"text": "This encourages the model to group words that area priori known to be semantically related into topics.", "labels": [], "entities": []}, {"text": "To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decom-positions of covariance matrices of the posterior predictive distributions.", "labels": [], "entities": []}, {"text": "We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis-Hastings step.", "labels": [], "entities": []}, {"text": "Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS).", "labels": [], "entities": [{"text": "NIPS", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.9295433163642883}]}, {"text": "Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA.", "labels": [], "entities": []}, {"text": "Quantitatively , our technique outperforms existing models at dealing with OOV words in held-out documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Latent Dirichlet Allocation (LDA) is a Bayesian technique that is widely used for inferring the topic structure in corpora of documents.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.917299771308899}]}, {"text": "It conceives of a document as a mixture of a small number of topics, and topics as a (relatively sparse) distribution over word types ().", "labels": [], "entities": []}, {"text": "These priors are remarkably effective at producing useful *Both student authors had equal contribution. results.", "labels": [], "entities": []}, {"text": "However, our intuitions tell us that while documents may indeed be conceived of as a mixture of topics, we should further expect topics to be semantically coherent.", "labels": [], "entities": []}, {"text": "Indeed, standard human evaluations of topic modeling performance are designed to elicit assessment of semantic coherence.", "labels": [], "entities": []}, {"text": "However, this prior preference for semantic coherence is not encoded in the model, and any such observation of semantic coherence found in the inferred topic distributions is, in some sense, accidental.", "labels": [], "entities": []}, {"text": "In this paper, we develop a variant of LDA that operates on continuous space embeddings of wordsrather than word types-to impose a prior expectation for semantic coherence.", "labels": [], "entities": []}, {"text": "Our approach replaces the opaque word types usually modeled in LDA with continuous space embeddings of these words, which are generated as draws from a multivariate Gaussian.", "labels": [], "entities": []}, {"text": "How does this capture our preference for semantic coherence?", "labels": [], "entities": []}, {"text": "Word embeddings have been shown to capture lexico-semantic regularities in language: words with similar syntactic and semantic properties are found to be close to each other in the embedding space.", "labels": [], "entities": []}, {"text": "Since Gaussian distributions capture a notion of centrality in space, and semantically related words are localized in space, our Gaussian LDA model encodes a prior preference for semantically coherent topics.", "labels": [], "entities": []}, {"text": "Our model further has several advantages.", "labels": [], "entities": []}, {"text": "Traditional LDA assumes a fixed vocabulary of word types.", "labels": [], "entities": []}, {"text": "This modeling assumption drawback as it cannot handle out of vocabulary (OOV) words in \"held out\" documents.", "labels": [], "entities": []}, {"text": "proposed an approach to address this problem by drawing topics from a Dirichlet Process with abase distribution overall possible character strings (i.e., words).", "labels": [], "entities": []}, {"text": "While this model can in principle handle unseen words, the only bias toward being included in a particular topic comes from the topic assignments in the rest of the document.", "labels": [], "entities": []}, {"text": "Our model can exploit the contiguity of semantically similar words in the embedding space and can assign high topic probability to a word which is similar to an existing topical word even if it has never been seen before.", "labels": [], "entities": []}, {"text": "The main contributions of our paper are as follows: We propose anew technique for topic modeling by treating the document as a collection of word embeddings and topics itself as multivariate Gaussian distributions in the embedding space ( \u00a73).", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.8429614007472992}]}, {"text": "We explore several strategies for collapsed Gibbs sampling and derive scalable algorithms, achieving asymptotic speed-up over the na\u00a8\u0131vena\u00a8\u0131ve implementation ( \u00a74).", "labels": [], "entities": []}, {"text": "We qualitatively show that our topics make intuitive sense and quantitatively demonstrate that our model captures a better representation of a document in the topic space by outperforming other models in a classification task ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate our Word Vector Topic Model on various experimental tasks.", "labels": [], "entities": []}, {"text": "Specifically we wish to determine: \u2022 Is our model is able to find coherent and meaningful topics?", "labels": [], "entities": []}, {"text": "\u2022 Is our model able to infer the topic distribution of a held-out document even when the document contains words which were previously unseen?", "labels": [], "entities": []}, {"text": "We run our experiments 4 on two datasets 20-NEWSGROUP 5 and NIPS 6 . All the datasets were tokenized and lowercased with cdec (Dyer et al., 2010).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top words of some topics from Gaussian-LDA and multinomial LDA on 20-newsgroups for  K = 50. Words in Gaussian LDA are ranked based on density assigned to them by the posterior predic- tive distribution. The last row for each method indicates the PMI score (w.r.t. Wikipedia co-occurence)  of the topics fifteen highest ranked words.", "labels": [], "entities": [{"text": "PMI score", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9437726140022278}]}, {"text": " Table 3: This table shows the Average L 1 Devia- tion, Average L 2 Deviation, Average L \u221e Devia- tion for the difference of the topic distribution of  the actual document and the synthetic document  on the NIPS corpus. Compared to infvoc, G-LDA  achieves a lower deviation of topic distribution in- ferred on the synthetic documents with respect to  actual document. The full size of the test corpus is  174.", "labels": [], "entities": [{"text": "Average L \u221e Devia- tion", "start_pos": 79, "end_pos": 102, "type": "METRIC", "confidence": 0.8946697016557058}, {"text": "NIPS corpus", "start_pos": 207, "end_pos": 218, "type": "DATASET", "confidence": 0.9827199578285217}, {"text": "G-LDA", "start_pos": 240, "end_pos": 245, "type": "METRIC", "confidence": 0.8600571751594543}]}]}