{"title": [{"text": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "labels": [], "entities": []}], "abstractContent": [{"text": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models.", "labels": [], "entities": [{"text": "Natural language generation of coherent long texts", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.7942846672875541}]}, {"text": "In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs.", "labels": [], "entities": []}, {"text": "We introduce an LSTM model that hierarchically builds an embedding fora paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph.", "labels": [], "entities": []}, {"text": "We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in away that preserve syntactic, semantic, and discourse coherence.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.7729263305664062}]}, {"text": "While only a first step toward generating coherent text units from neural models , our work has the potential to significantly impact natural language generation and summarization 1 .", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.6515496869881948}, {"text": "summarization", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.9904639720916748}]}], "introductionContent": [{"text": "Generating coherent text is a central task in natural language processing.", "labels": [], "entities": [{"text": "Generating coherent text", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8515767852465311}, {"text": "natural language processing", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6448416511217753}]}, {"text": "A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory ( or Discourse Representation Theory (, for extracting these relations from text units, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (; Barzilay and Lee, Code for models described in this paper are available at www.stanford.edu/ \u02dc jiweil/. 2004;, inter alia).", "labels": [], "entities": [{"text": "Discourse Representation Theory", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.7730066776275635}]}, {"text": "However, applying these to text generation remains difficult.", "labels": [], "entities": [{"text": "text generation", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.84376260638237}]}, {"text": "To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text.", "labels": [], "entities": []}, {"text": "Identifying increasingly sophisticated human-developed features maybe insufficient for capturing these patterns.", "labels": [], "entities": []}, {"text": "But developing neuralbased alternatives has also been difficult.", "labels": [], "entities": []}, {"text": "Although neural representations for sentences can capture aspects of coherent sentence structure), it's not clear how they could help in generating more broadly coherent text.", "labels": [], "entities": []}, {"text": "Recent LSTM models) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation) or parsing ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7011063098907471}, {"text": "parsing", "start_pos": 157, "end_pos": 164, "type": "TASK", "confidence": 0.966336190700531}]}, {"text": "This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express.", "labels": [], "entities": []}, {"text": "Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents?", "labels": [], "entities": []}, {"text": "In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words.", "labels": [], "entities": [{"text": "sequenceto-sequence generation tasks", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.8162993788719177}]}, {"text": "Multi-text generation tasks like summarization could work in a similar way: the system reads a collection of input sentences, and is then asked to generate meaningful texts with certain properties (such as-for summarizationbeing succinct and conclusive).", "labels": [], "entities": [{"text": "Multi-text generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7764274477958679}, {"text": "summarization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9879539012908936}]}, {"text": "Just as the local semantic and syntactic compositionally of words can be captured by LSTM models, can the com-positionally of discourse releations of higher-level text units (e.g., clauses, sentences, paragraphs, and documents) be captured in a similar way, with clues about how text units connect with each another stored in the neural compositional matrices?", "labels": [], "entities": []}, {"text": "In this paper we explore a first step toward this task of neural natural language generation.", "labels": [], "entities": [{"text": "neural natural language generation", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.6553213074803352}]}, {"text": "We focus on the component task of training a paragraph (document)-to-paragraph (document) autoencoder to reconstruct the input text sequence from a compressed vector representation from a deep learning model.", "labels": [], "entities": []}, {"text": "We develop hierarchical LSTM models that arranges tokens, sentences and paragraphs in a hierarchical structure, with different levels of LSTMs capturing compositionality at the tokentoken and sentence-to-sentence levels.", "labels": [], "entities": []}, {"text": "We offer in the following section to a brief description of sequence-to-sequence LSTM models.", "labels": [], "entities": []}, {"text": "The proposed hierarchical LSTM models are then described in Section 3, followed by experimental results in Section 4, and then a brief conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implement the proposed autoencoder on two datasets, a highly domain specific dataset consisting of hotel reviews and a general dataset extracted from Wkipedia.", "labels": [], "entities": [{"text": "Wkipedia", "start_pos": 153, "end_pos": 161, "type": "DATASET", "confidence": 0.9834702610969543}]}, {"text": "Hotel Reviews We use a subset of hotel reviews crawled from TripAdvisor.", "labels": [], "entities": []}, {"text": "We consider only reviews consisting sentences ranging from 50 to 250 words; the model has problems dealing with extremely long sentences, as we will discuss later.", "labels": [], "entities": []}, {"text": "We keep a vocabulary set consisting of the 25,000 most frequent words.", "labels": [], "entities": []}, {"text": "A special \"<unk>\" token is used to denote all the remaining less frequent tokens.", "labels": [], "entities": []}, {"text": "Reviews that consist of more than 2 percent of unknown words are discarded.", "labels": [], "entities": []}, {"text": "Our training dataset is comprised of roughly 340,000 reviews; the testing set is comprised of 40,000 reviews.", "labels": [], "entities": []}, {"text": "Dataset details are shown in.", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8801908493041992}]}, {"text": "Wikipedia We extracted paragraphs from Wikipedia corpus that meet the aforementioned length requirements.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9001704752445221}]}, {"text": "We keep atop frequent vocabulary list of 120,000 words.", "labels": [], "entities": []}, {"text": "Paragraphs with larger than 4 percent of unknown words are discarded.", "labels": [], "entities": []}, {"text": "The training dataset is comprised of roughly 500,000 paragraphs and testing contains roughly 50,000.", "labels": [], "entities": []}, {"text": "We need to measure the closeness of the output (candidate) to the input (reference).", "labels": [], "entities": []}, {"text": "We first adopt two standard evaluation metrics, ROUGE) and BLEU ().", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9978804588317871}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9988887906074524}]}, {"text": "ROUGE is a recall-oriented measure widely used in the summarization literature.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9655578136444092}, {"text": "recall-oriented measure", "start_pos": 11, "end_pos": 34, "type": "METRIC", "confidence": 0.9723885655403137}, {"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9831277132034302}]}, {"text": "It measures the n-gram recall between the candidate text and the reference text(s).", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9682442545890808}]}, {"text": "In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: where count match denotes the number of n-grams co-occurring in the input and output.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9808482229709625}, {"text": "count match", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.9583144783973694}]}, {"text": "We report ROUGE-1, 2 and W (based on weighted longest common subsequence).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9945318698883057}]}, {"text": "BLEU Purely measuring recall will inappropriately reward long outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9750403165817261}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9975021481513977}]}, {"text": "BLEU is designed to address such an issue by emphasizing precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9634709358215332}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9975171089172363}]}, {"text": "n-gram precision scores for our situation are given by: precision n = gram n \u2208output count match (gram n ) gram n \u2208output count(gram n ) (17) BLEU then combines the average logarithm of precision scores with exceeded length penalization.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9642853736877441}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9966986775398254}, {"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9985324144363403}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9894099235534668}, {"text": "exceeded length penalization", "start_pos": 208, "end_pos": 236, "type": "METRIC", "confidence": 0.6969815691312155}]}, {"text": "Coherence Evaluation Neither BLEU nor ROUGE attempts to evaluate true coherence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9970648884773254}, {"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9797195792198181}]}, {"text": "There is no generally accepted and readily available coherence evaluation metric.", "labels": [], "entities": []}, {"text": "Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case.", "labels": [], "entities": []}, {"text": "Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent (, we compare generated outputs with input documents in terms of how much original text order is preserved.", "labels": [], "entities": []}, {"text": "We develop a grid evaluation metric similar to the entity transition algorithms in ().", "labels": [], "entities": []}, {"text": "The key idea of Barzilay and Lapata's models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences.", "labels": [], "entities": []}, {"text": "We represent each sentence as a featurevector consisting of verbs and nouns in the sentence.", "labels": [], "entities": []}, {"text": "Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors.", "labels": [], "entities": [{"text": "sentence-tosentence F1 scores", "start_pos": 74, "end_pos": 103, "type": "METRIC", "confidence": 0.6299124956130981}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9898409247398376}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9778717160224915}, {"text": "ROUGE", "start_pos": 152, "end_pos": 157, "type": "METRIC", "confidence": 0.9861435890197754}, {"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9920377731323242}]}, {"text": "Note that multiple output sentences can be matched to one input Input-Wiki washington was unanimously elected President by the electors in both the 1788 -1789 and 1792 elections . he oversaw the creation of a strong, well-financed national government that maintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , and won acceptance among Americans of all types . washington established many forms in government still used today , such as the cabinet system and inaugural address . his retirement after two terms and the peaceful transition from his presidency to that of john adams established a tradition that continued up until franklin d . roosevelt was elected to a third term . washington has been widely hailed as the \" father of his country \" even during his lifetime.", "labels": [], "entities": []}, {"text": "Assume that sentence s i output is aligned with sentence s i input , where i and i denote position index fora output sentence and its aligned input.", "labels": [], "entities": []}, {"text": "The penalization score L is then given by: Equ.", "labels": [], "entities": [{"text": "penalization score L", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.7445406317710876}, {"text": "Equ", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9787843823432922}]}, {"text": "18 can be interpreted as follows: (j \u2212 i) denotes the distance in terms of position index between two outputted sentences indexed by j and i, and (j \u2212 i ) denotes the distance between their mirrors in inputs.", "labels": [], "entities": []}, {"text": "As we wish to penalize the degree of permutation in terms of text order, we penalize the absolute difference between the two computed distances.", "labels": [], "entities": []}, {"text": "This metric is also relevant to the overall performance of prediction and recall: an irrelevant output will be aligned to a random input, thus being heavily penalized.", "labels": [], "entities": [{"text": "prediction", "start_pos": 59, "end_pos": 69, "type": "TASK", "confidence": 0.9672216773033142}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9528668522834778}]}, {"text": "The deficiency of the proposed metric is that it concerns itself only with a semantic perspective on coherence, barely considering syntactical issues.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the Datasets. W, S and D re- spectively represent number of words, number of  sentences, and number of documents/paragraphs.  For example, \"S per D\" denotes average number  of sentences per document.", "labels": [], "entities": []}, {"text": " Table 3: Results for three models on two datasets. As with coherence score L, smaller values signifies  better performances.", "labels": [], "entities": []}]}