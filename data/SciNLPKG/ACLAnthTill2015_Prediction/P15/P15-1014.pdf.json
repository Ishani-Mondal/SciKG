{"title": [{"text": "Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations", "labels": [], "entities": [{"text": "Learning Word Representations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6587943037350973}]}], "abstractContent": [{"text": "Vector space representation of words has been widely used to capture fine-grained linguistic regularities, and proven to be successful in various natural language processing tasks in recent years.", "labels": [], "entities": [{"text": "Vector space representation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8250020941098531}]}, {"text": "However, existing models for learning word representations focus on either syntagmatic or paradigmatic relations alone.", "labels": [], "entities": []}, {"text": "In this paper , we argue that it is beneficial to jointly modeling both relations so that we cannot only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mutual enhancement between these two types of relations.", "labels": [], "entities": []}, {"text": "We propose two novel dis-tributional models for word representation using both syntagmatic and paradigmatic relations via a joint training objective.", "labels": [], "entities": [{"text": "word representation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7717627286911011}]}, {"text": "The proposed models are trained on a public Wikipedia corpus, and the learned representations are evaluated on word analogy and word similarity tasks.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.7584579288959503}]}, {"text": "The results demonstrate that the proposed models can perform significantly better than all the state-of-the-art baseline methods on both tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word.", "labels": [], "entities": []}, {"text": "The representations can be used as basic features in a variety of applications, such as information retrieval (, named entity recognition), question answering (, disambiguation, and parsing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7892249822616577}, {"text": "named entity recognition)", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.7447633743286133}, {"text": "question answering", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.8857063055038452}, {"text": "parsing", "start_pos": 182, "end_pos": 189, "type": "TASK", "confidence": 0.9554414749145508}]}, {"text": "A common paradigm for acquiring such representations is based on the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "Based on this hypothesis, various models on learning word representations have been proposed during the last two decades.", "labels": [], "entities": [{"text": "learning word representations", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.6040402154127756}]}, {"text": "According to the leveraged distributional information, existing models can be grouped into two categories.", "labels": [], "entities": []}, {"text": "The first category mainly concerns the syntagmatic relations among the words, which relate the words that co-occur in the same text region.", "labels": [], "entities": []}, {"text": "For example, \"wolf\" is close to \"fierce\" since they often co-occur in a sentence, as shown in.", "labels": [], "entities": []}, {"text": "This type of models learn the distributional representations of words based on the text region that the words occur in, as exemplified by Latent Semantic Analysis (LSA) model) and Non-negative Matrix Factorization (NMF) model).", "labels": [], "entities": []}, {"text": "The second category mainly captures paradigmatic relations, which relate words that occur with similar contexts but may not cooccur in the text.", "labels": [], "entities": []}, {"text": "For example, \"wolf\" is close to \"tiger\" since they often have similar context words.", "labels": [], "entities": []}, {"text": "This type of models learn the word representations based on the surrounding words, as exemplified by the Hyperspace Analogue to Language (HAL) model, Continuous Bag-of-Words (CBOW) model and SkipGram (SG) model.", "labels": [], "entities": []}, {"text": "In this work, we argue that it is important to take both syntagmatic and paradigmatic relations into account to build a good distributional model.", "labels": [], "entities": []}, {"text": "Firstly, in distributional meaning acquisition, it is expected that a good representation should be able to encode a bunch of linguistic properties.", "labels": [], "entities": [{"text": "distributional meaning acquisition", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.6991485555966696}]}, {"text": "For example, it can put semantically related words close (e.g., \"microsoft\" and \"office\"), and also be able to capture syntactic regularities like \"big is to bigger as deep is to deeper\".", "labels": [], "entities": []}, {"text": "Obviously, these linguistic properties are related to both syntagmatic and paradigmatic relations, and cannot be well modeled by either alone.", "labels": [], "entities": []}, {"text": "Secondly, syntagmatic and paradigmatic relations are complimentary rather than conflicted in representation learning.", "labels": [], "entities": []}, {"text": "That is relating the words that co-occur within the same text region (e.g., \"wolf\" and \"fierce\" as well as \"tiger\" and \"fierce\") can better relate words that occur with similar contexts (e.g., \"wolf\" and \"tiger\"), and vice versa.", "labels": [], "entities": []}, {"text": "Based on the above analysis, we propose two new distributional models for word representation using both syntagmatic and paradigmatic relations.", "labels": [], "entities": [{"text": "word representation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7819247543811798}]}, {"text": "Specifically, we learn the distributional representations of words based on the text region (i.e., the document) that the words occur in as well as the surrounding words (i.e., word sequences within some window size).", "labels": [], "entities": []}, {"text": "By combining these two types of relations either in a parallel or a hierarchical way, we obtain two different joint training objectives for word representation learning.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.8804670770963033}]}, {"text": "We evaluate our new models in two tasks, i.e., word analogy and word similarity.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.8034573197364807}]}, {"text": "The experimental results demonstrate that the proposed models can perform significantly better than all of the state-ofthe-art baseline methods in both of the tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first describe our experimental settings including the corpus, hyper-parameter selections, and baseline methods.", "labels": [], "entities": []}, {"text": "Then we compare our models with baseline methods on two tasks, i.e., word analogy and word similarity.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 69, "end_pos": 81, "type": "TASK", "confidence": 0.7943237721920013}, {"text": "word similarity", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.6985462158918381}]}, {"text": "After that, we conduct some case studies to show that our model can better capture both syntagmatic and paradigmatic relations and how it improves the performances on semantic tasks.", "labels": [], "entities": []}, {"text": "We select Wikipedia, the largest online knowledge base, to train our models.", "labels": [], "entities": []}, {"text": "We adopt the publicly available April 2010 dump 3, which is also used by).", "labels": [], "entities": [{"text": "April 2010 dump 3", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.7340944483876228}]}, {"text": "The corpus in total has 3, 035, 070 articles and about 1 billion tokens.", "labels": [], "entities": []}, {"text": "In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters . Following the practice in (), we set context window size as 10 and use 10 negative samples.", "labels": [], "entities": []}, {"text": "The noise distributions for context and words are set as the same as used in (), p nw (w) \u221d #(w) 0.75 . We also adopt the same linear learning rate strategy described in (, where the initial learning rate of PDC model is 0.05, and We compare our models with various state-ofthe-art models including C&W), GCANLM, CBOW, SG (Mikolov et al., 2013a), GloVe (), PV-DM, PV-DBOW () and HPCA ().", "labels": [], "entities": [{"text": "HPCA", "start_pos": 379, "end_pos": 383, "type": "DATASET", "confidence": 0.8678091168403625}]}, {"text": "For C&W, GCANLM 6 , GloVe and HPCA, we use the word embeddings they provided.", "labels": [], "entities": [{"text": "C&W", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7879316012064616}, {"text": "GCANLM 6", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.8393061459064484}, {"text": "GloVe", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.859657347202301}, {"text": "HPCA", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9246238470077515}]}, {"text": "For CBOW and SG model, we reimplement these two models since the original word2vec tool uses SGD but cannot shuffle the data.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7492783665657043}]}, {"text": "Besides, we also implement PV-DM and PV-DBOW models due to ( has not released source codes.", "labels": [], "entities": []}, {"text": "We train these four models on the same dataset with the same hyper-parameter settings as our models for fair comparison.", "labels": [], "entities": []}, {"text": "The statistics of the corpora used in baseline models are shown in.", "labels": [], "entities": []}, {"text": "Moreover, since different papers report different dimensionality, to be fair, we conduct evaluations on three dimensions (i.e., 50, 100, 300) to cover the publicly available results 7 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora used in baseline models.", "labels": [], "entities": []}, {"text": " Table 2: Results on the word analogy task. Un- derlined scores are the best within groups of the  same dimensionality, while bold scores are the  best overall.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8370673060417175}]}, {"text": " Table 3: Target words and their 5 most similar  words under different representations. Words in  italic often co-occur with the target words, while  words in bold are substitutable to the target words.", "labels": [], "entities": []}]}