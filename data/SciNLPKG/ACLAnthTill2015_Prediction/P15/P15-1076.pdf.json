{"title": [{"text": "Trans-dimensional Random Fields for Language Modeling", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7374804317951202}]}], "abstractContent": [{"text": "Language modeling (LM) involves determining the joint probability of words in a sentence.", "labels": [], "entities": [{"text": "Language modeling (LM)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8558516442775727}]}, {"text": "The conditional approach is dominant, representing the joint probability in terms of conditionals.", "labels": [], "entities": []}, {"text": "Examples include n-gram LMs and neural network LMs.", "labels": [], "entities": []}, {"text": "An alternative approach, called the random field (RF) approach, is used in whole-sentence maximum entropy (WSME) LMs.", "labels": [], "entities": [{"text": "whole-sentence maximum entropy (WSME) LMs", "start_pos": 75, "end_pos": 116, "type": "TASK", "confidence": 0.7178022350583758}]}, {"text": "Although the RF approach has potential benefits, the empirical results of previous WSME models are not satisfactory.", "labels": [], "entities": [{"text": "RF", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9744945764541626}, {"text": "WSME", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.7957859635353088}]}, {"text": "In this paper, we revisit the RF approach for language modeling, with a number of innovations.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8277508616447449}]}, {"text": "We propose a trans-dimensional RF (TDRF) model and develop a training algorithm using joint stochastic approximation and trans-dimensional mixture sampling.", "labels": [], "entities": []}, {"text": "We perform speech recognition experiments on Wall Street Journal data, and find that our TDRF models lead to performances as good as the recurrent neural network LMs but are computationally more efficient in computing sentence probability.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.801899790763855}, {"text": "Wall Street Journal data", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.9635801166296005}]}], "introductionContent": [{"text": "Language modeling is crucial fora variety of computational linguistic applications, such as speech recognition, machine translation, handwriting recognition, information retrieval and soon.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7202396839857101}, {"text": "speech recognition", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7702845335006714}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8144439160823822}, {"text": "handwriting recognition", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.9079099893569946}, {"text": "information retrieval", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.837681382894516}]}, {"text": "It involves determining the joint probability p(x) of a sentence x, which can be denoted as a pair x = (l, x l ), where l is the length and x l = (x 1 , . .", "labels": [], "entities": []}, {"text": ", x l ) is a sequence of l words.", "labels": [], "entities": []}, {"text": "Currently, the dominant approach is conditional modeling, which decomposes the joint probability of x l into a product of conditional probabilities 1 And the joint probability of x is modeled as p(x) = by using the chain rule, p(x i |x 1 , . .", "labels": [], "entities": [{"text": "conditional modeling", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.8670017421245575}]}, {"text": ", x i\u22121 ).", "labels": [], "entities": []}, {"text": "To avoid degenerate representation of the conditionals, the history of xi , denoted ash i = (x 1 , \u00b7 \u00b7 \u00b7 , x i\u22121 ), is reduced to equivalence classes through a mapping \u03c6(h i ) with the assumption p(x i |h i ) \u2248 p(x i |\u03c6(h i )).", "labels": [], "entities": []}, {"text": "Language modeling in this conditional approach consists of finding suitable mappings \u03c6(h i ) and effective methods to estimate p(x i |\u03c6(h i )).", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6616586595773697}]}, {"text": "A classic example is the traditional n-gram LMs with \u03c6(h i ) = (x i\u2212n+1 , . .", "labels": [], "entities": []}, {"text": ", x i\u22121 ).", "labels": [], "entities": []}, {"text": "Various smoothing techniques are used for parameter estimation.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.6517148464918137}]}, {"text": "Recently, neural network LMs, which have begun to surpass the traditional n-gram LMs, also follow the conditional modeling approach, with \u03c6(h i ) determined by a neural network (NN), which can be either a feedforward NN or a recurrent.", "labels": [], "entities": []}, {"text": "Remarkably, an alternative approach is used in whole-sentence maximum entropy (WSME) language modeling ().", "labels": [], "entities": [{"text": "whole-sentence maximum entropy (WSME) language modeling", "start_pos": 47, "end_pos": 102, "type": "TASK", "confidence": 0.7032905407249928}]}, {"text": "Specifically, a WSME model has the form: Here f (x) is a vector of features, which can be arbitrary computable functions of x, \u03bb is the corresponding parameter vector, and Z is the global normalization constant.", "labels": [], "entities": [{"text": "WSME", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.8600010275840759}]}, {"text": "Although WSME models have the potential benefits of being able to naturally express sentence-level phenomena and integrate features from a variety of knowledge where EOS is a special token placed at the end of every sentence.", "labels": [], "entities": []}, {"text": "Thus the distribution of the sentence length is implicitly modeled.", "labels": [], "entities": []}, {"text": "sources, their performance results ever reported are not satisfactory ().", "labels": [], "entities": []}, {"text": "The WSME model defined in (3) is basically a Markov random field (MRF).", "labels": [], "entities": [{"text": "WSME", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.7240617871284485}]}, {"text": "A substantial challenge in fitting MRFs is that evaluating the gradient of the log likelihood requires high-dimensional integration and hence is difficult even for moderately sized models, let alone the language model (3).", "labels": [], "entities": [{"text": "MRFs", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.7858723998069763}]}, {"text": "The sampling methods previously tried for approximating the gradient are the Gibbs sampling, the Independence MetropolisHasting sampling and the importance sampling ().", "labels": [], "entities": [{"text": "Independence MetropolisHasting sampling", "start_pos": 97, "end_pos": 136, "type": "DATASET", "confidence": 0.7724917332331339}, {"text": "importance sampling", "start_pos": 145, "end_pos": 164, "type": "METRIC", "confidence": 0.9377913177013397}]}, {"text": "Simple applications of these methods are hardly able to work efficiently for the complex, high-dimensional distribution such as (3), and hence the WSME models are in fact poorly fitted to the data.", "labels": [], "entities": []}, {"text": "This is one of the reasons for the unsatisfactory results of previous WSME models.", "labels": [], "entities": [{"text": "WSME", "start_pos": 70, "end_pos": 74, "type": "TASK", "confidence": 0.560997486114502}]}, {"text": "In this paper, we propose anew language model, called the trans-dimensional random field (TDRF) model, by explicitly taking account of the empirical distributions of lengths.", "labels": [], "entities": []}, {"text": "This formulation subsequently enables us to develop a powerful Markov chain Monte Carlo (MCMC) technique, called trans-dimensional mixture sampling and then propose an effective training algorithm in the framework of stochastic approximation (SA) ().", "labels": [], "entities": []}, {"text": "The SA algorithm involves jointly updating the model parameters and normalization constants, in conjunction with trans-dimensional MCMC sampling.", "labels": [], "entities": []}, {"text": "Section 2 and 3 present the model definition and estimation respectively.", "labels": [], "entities": [{"text": "estimation", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.8621006011962891}]}, {"text": "Furthermore, we make several additional innovations, as detailed in Section 4, to enable successful training of TDRF models.", "labels": [], "entities": []}, {"text": "First, the diagonal elements of hessian matrix are estimated during SA iterations to rescale the gradient, which significantly improves the convergence of the SA algorithm.", "labels": [], "entities": []}, {"text": "Second, word classing is introduced to accelerate the sampling operation and also improve the smoothing behavior of the models through sharing statistical strength between similar words.", "labels": [], "entities": [{"text": "word classing", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.7382427453994751}]}, {"text": "Finally, multiple CPUs are used to parallelize the training of our RF models.", "labels": [], "entities": []}, {"text": "In Section 5, speech recognition experiments are conducted to evaluate our TDRF LMs, compared with the traditional 4-gram LMs and the recurrent neural network LMs (RNNLMs)) which have emerged as anew stateof-art of language modeling.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7649922668933868}]}, {"text": "We explore the use of a variety of features based on word and class information in TDRF LMs.", "labels": [], "entities": []}, {"text": "In terms of word error rates (WERs) for speech recognition, our TDRF LMs alone can outperform the KN-smoothing 4-gram LM with 9.1% relative reduction, and perform comparably to the RNNLM with a slight 0.5% relative reduction.", "labels": [], "entities": [{"text": "word error rates (WERs)", "start_pos": 12, "end_pos": 35, "type": "METRIC", "confidence": 0.8448902020851771}, {"text": "speech recognition", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.751127690076828}]}, {"text": "To our knowledge, this result represents the first strong empirical evidence supporting the power of using the whole-sentence language modeling approach.", "labels": [], "entities": [{"text": "whole-sentence language modeling", "start_pos": 111, "end_pos": 143, "type": "TASK", "confidence": 0.6782726645469666}]}, {"text": "Our open-source TDRF toolkit is released publicly 2 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The PPLs on the PTB test data. The class  number is 200.", "labels": [], "entities": [{"text": "PTB test data", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9840829769770304}]}, {"text": " Table 3: The WERs and PPLs on the WSJ'92 test  data. \"#feat\" denotes the feature number. Differ- ent TDRF models with class number 100/200/500  are reported (denoted by \"100c\"/\"200c\"/\"500c\")", "labels": [], "entities": [{"text": "WERs", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9702053666114807}, {"text": "WSJ'92 test  data", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.9835139513015747}]}]}