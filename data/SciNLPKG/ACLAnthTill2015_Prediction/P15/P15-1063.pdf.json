{"title": [{"text": "Omnia Mutantur, Nihil Interit: Connecting Past with Present by Find- ing Corresponding Terms across Time", "labels": [], "entities": [{"text": "Find- ing Corresponding Terms across Time", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.686534196138382}]}], "abstractContent": [{"text": "In the current fast-paced world, people tend to possess limited knowledge about things from the past.", "labels": [], "entities": []}, {"text": "For example, some young users may not know that Walkman played similar function as iPod does nowadays.", "labels": [], "entities": []}, {"text": "In this paper, we approach the temporal correspondence problem in which, given an input term (e.g., iPod) and the target time (e.g. 1980s), the task is to find the counterpart of the query that existed in the target time.", "labels": [], "entities": []}, {"text": "We propose an approach that transforms word contexts across time based on their neural network representations.", "labels": [], "entities": []}, {"text": "We then experimentally demonstrate the effectiveness of our method on the New York Times Annotated Corpus.", "labels": [], "entities": [{"text": "New York Times Annotated Corpus", "start_pos": 74, "end_pos": 105, "type": "DATASET", "confidence": 0.8140399098396301}]}], "introductionContent": [{"text": "What music device 30 years ago played similar role as iPod does nowadays?", "labels": [], "entities": []}, {"text": "Who are today's Beatles?", "labels": [], "entities": []}, {"text": "Who was a counterpart of President Chirac in 1988?", "labels": [], "entities": []}, {"text": "These and many other similar questions maybe difficult to answer by average users (especially, by young ones).", "labels": [], "entities": []}, {"text": "This is because people tend to possess less knowledge about the past than about the contemporary time.", "labels": [], "entities": []}, {"text": "In this work we propose an effective method to solve the problem of finding counterpart terms across time.", "labels": [], "entities": []}, {"text": "In particular, for an input pair of a term (e.g., iPod) and the target time (e.g. 1980s), we find the corresponding term that existed in the target time (walkman).", "labels": [], "entities": []}, {"text": "We consider temporal counterparts to be terms which are semantically similar, yet, which existed in different time.", "labels": [], "entities": []}, {"text": "Knowledge of temporal counterparts can help to alleviate the problem of terminology gap for users searching within temporal document collections such as archives.", "labels": [], "entities": []}, {"text": "For example, given a user's query and the target time frame, anew modified query that represents the same meaning could be suggested to improve search results.", "labels": [], "entities": []}, {"text": "Essentially, it would mean letting searchers use the knowledge they possess on the current world to perform search within unknown collections such as ones containing documents from the distant past.", "labels": [], "entities": []}, {"text": "Furthermore, solving temporal correspondence problem can help timeline construction, temporal summarization, reference forecasting and can have applications in education.", "labels": [], "entities": [{"text": "timeline construction", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.8162569403648376}, {"text": "temporal summarization", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.4886467903852463}, {"text": "reference forecasting", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.8487263321876526}]}, {"text": "The problem of temporal counterpart detection is however not trivial.", "labels": [], "entities": [{"text": "temporal counterpart detection", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.7777902682622274}]}, {"text": "The key difficulty comes from the change of the entire context that results in low overlap of context across time.", "labels": [], "entities": []}, {"text": "In other words, it is difficult to find temporal counterpart terms by directly comparing context vectors across time.", "labels": [], "entities": []}, {"text": "This fact is nicely portrayed by the Latin proverb: \"omnia mutantur, nihil interit\" (in English: \"everything changes, nothing perishes\") which indicates that there are no completely static things, yet, many things and concepts are still similar across time.", "labels": [], "entities": []}, {"text": "Another challenge is the lack of training data.", "labels": [], "entities": []}, {"text": "If we have had enough training pairs of input terms and their temporal counterparts, then it would have become possible to represent the task as atypical machine learning problem.", "labels": [], "entities": []}, {"text": "However, it is difficult to collect multiple training pairs over various domains and for arbitrary time.", "labels": [], "entities": []}, {"text": "In view of the challenges mentioned above, we propose an approach that transforms term representations from one vector space (e.g., one derived from the present documents) to another vector space (e.g., one obtained from the past documents).", "labels": [], "entities": []}, {"text": "Terms in both the vector spaces are represented by the distributed vector representation ().", "labels": [], "entities": []}, {"text": "Our method then matches the terms by comparing their relative positions in the vector spaces of different time periods alleviating the problem of low overlap between word contexts overtime.", "labels": [], "entities": []}, {"text": "It also does not require to manually prepare seed pairs of temporal counterparts.", "labels": [], "entities": []}, {"text": "We further improve this method by automatically generating reference points that more precisely represent target terms in the form of local graphs.", "labels": [], "entities": []}, {"text": "In result, our approach consists of finding global and local correspondence between terms overtime.", "labels": [], "entities": []}, {"text": "To sum up, we make the following contributions in this paper: (1) we propose an efficient method to find temporal counterparts by transforming the representation of terms within different temporal spaces, (2) we then enhance the global correspondence method by considering also the local context of terms (local correspondence) and (3) we perform extensive experiments on the New York Times Annotated Corpus, including the search from the present to the past and vice versa, which prove the effectiveness of our approach.", "labels": [], "entities": [{"text": "New York Times Annotated Corpus", "start_pos": 376, "end_pos": 407, "type": "DATASET", "confidence": 0.6669864356517792}]}], "datasetContent": [{"text": "We use the Mean Reciprocal Rank (MRR) as a main metric to evaluate the ranked search results for each method.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 11, "end_pos": 37, "type": "METRIC", "confidence": 0.9645298421382904}]}, {"text": "MRR is expressed as the mean of the inverse ranks for each test where a correct result appears.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8753820657730103}]}, {"text": "It is calculated as follows: where ranki is the rank of a correct counterpart at the i-th test.", "labels": [], "entities": []}, {"text": "N is the number of query-answer pairs.", "labels": [], "entities": []}, {"text": "MRR's values range between.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6380985379219055}]}, {"text": "The higher the value, the more correct the method is.", "labels": [], "entities": []}, {"text": "Besides MRR, we also report precision @1, @5, @10 and @20.", "labels": [], "entities": [{"text": "MRR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9662331342697144}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9996969699859619}]}, {"text": "They are equal to the rates of tests in which the correct counterpart term tc was found in the top 1, 5, 10 and 20 results, respectively.", "labels": [], "entities": []}, {"text": "We prepare three baselines: (1) Bag of words approach (BOW) without transformation: this method directly compares the context of the query in the base time with the context of the candidate term in the target time.", "labels": [], "entities": [{"text": "BOW", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.801416277885437}]}, {"text": "We use it to examine whether the distributed vector representation and transformation are necessary.", "labels": [], "entities": []}, {"text": "(2) Latent Semantic Indexing (LSI) without transformation (LSI-Com): we first merge the documents in the base time and the documents in the target time.", "labels": [], "entities": [{"text": "Latent Semantic Indexing (LSI)", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6304369866847992}]}, {"text": "Then, we train LSI on such combined collection to represent each term by the same distribution of detected topics.", "labels": [], "entities": []}, {"text": "We next search for the terms that exist in the target period and that are also semantically similar to the queried terms by comparing their vector q tc  representations.", "labels": [], "entities": []}, {"text": "The purpose of using LSI-Com is to check the need for the transformation overtime.", "labels": [], "entities": []}, {"text": "(3) Latent Semantic Indexing (LSI) with transformation (LSI-Tran): we train two LSI models separately on the documents in the base time and the documents in the target time.", "labels": [], "entities": [{"text": "Latent Semantic Indexing (LSI)", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6043916443983713}]}, {"text": "Then we train the transformation matrix in the same way as we did for our proposed methods.", "labels": [], "entities": []}, {"text": "Lastly, fora given input query, we compare its transformed vector representation with terms in the target time.", "labels": [], "entities": []}, {"text": "LSI-Tran is used to investigate if LSI can bean alternative for the vector representation under our transformation scenario.", "labels": [], "entities": []}, {"text": "All our methods use the neural network based term representation.", "labels": [], "entities": []}, {"text": "The first one is the method without considering the local context graph called GT (see Sec. 2).", "labels": [], "entities": []}, {"text": "By testing it we want to investigate the necessity of transforming the context of the query in the target time.", "labels": [], "entities": []}, {"text": "We also test the three variants of the proposed approach that applies the local graph (explained in Sec. 3).", "labels": [], "entities": []}, {"text": "The first one, LT-Lex, constructs the local graph by using the hypernyms of terms.", "labels": [], "entities": [{"text": "LT-Lex", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.8742597699165344}]}, {"text": "LTCooc applies term co-occurrence to select the reference points.", "labels": [], "entities": [{"text": "LTCooc", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9446300864219666}]}, {"text": "Finally, LT-Clust clusters the context terms by their semantic meanings and selects the most common term from each cluster.", "labels": [], "entities": []}, {"text": "First, we look at the results of finding temporal counterparts in.", "labels": [], "entities": []}, {"text": "The average scores for each method are shown in. shows detailed results for few example queries.", "labels": [], "entities": []}, {"text": "The main finding is that all our methods outperform the baselines when measured by MRR and by the precisions at different ranks.", "labels": [], "entities": [{"text": "MRR", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.8547973036766052}, {"text": "precisions", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9905784726142883}]}, {"text": "In the following subsections we discuss the results in detail.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example results where q is the input term and tc is the matching temporal counterpart. The  numbers are the ranks of the correct temporal counterpart in the results ranked by each method. Since  we output only the top 1000 results, ranks lower than 1000 are represented as 1000+.", "labels": [], "entities": []}, {"text": " Table 2: Results of searching from present to past  (", "labels": [], "entities": []}, {"text": " Table 3: Average scores of searching from past to", "labels": [], "entities": [{"text": "Average scores", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9239979386329651}]}, {"text": " Table 4: Results of searching from present to past  (", "labels": [], "entities": []}, {"text": " Table 5: Results of searching from past to present", "labels": [], "entities": []}]}