{"title": [], "abstractContent": [{"text": "Precisely evaluating the quality of a translation against human references is a challenging task due to the flexible word ordering of a sentence and the existence of a large number of synonyms for words.", "labels": [], "entities": []}, {"text": "This paper proposes to evaluate translations with distributed representations of words and sentences.", "labels": [], "entities": []}, {"text": "We study several metrics based on word and sentence representations and their combination.", "labels": [], "entities": []}, {"text": "Experiments on the WMT metric task shows that the metric based on the combined representations achieves the best performance, outperforming the state-of-the-art translation metrics by a large margin.", "labels": [], "entities": [{"text": "WMT metric task", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.6152036984761556}]}, {"text": "In particular , training the distributed representations only needs a reasonable amount of mono-lingual, unlabeled data that is not necessary drawn from the test domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references.", "labels": [], "entities": [{"text": "Automatic machine translation (MT) evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7959799425942558}]}, {"text": "They allow rapid comparisons between different systems and enable the tuning of parameter values during system training.", "labels": [], "entities": []}, {"text": "Many machine translation metrics have been proposed in recent years, such as BLEU (), NIST), TER (), Meteor (Banerjee and) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7712827026844025}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9982183575630188}, {"text": "TER", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9873637557029724}, {"text": "MEANT", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.753460705280304}]}, {"text": "Precisely evaluating translation, however, is not easy.", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.683586597442627}]}, {"text": "This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7332263886928558}]}, {"text": "One straightforward solution to improve the evaluation quality is to increase the number of various references.", "labels": [], "entities": []}, {"text": "Nevertheless, it is expensive to create multiple references.", "labels": [], "entities": []}, {"text": "In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables have been used.", "labels": [], "entities": []}, {"text": "For example, Meteor (Banerjee and) uses WordNet; TER-Plus () and Meteor Universal) deploy paraphrasing tables.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9713384509086609}, {"text": "TER-Plus", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.7673208117485046}]}, {"text": "These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9983892440795898}]}, {"text": "This paper leverages recent developments on distributed representations to address the above mentioned two challenges.", "labels": [], "entities": []}, {"text": "A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (.", "labels": [], "entities": []}, {"text": "For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space.", "labels": [], "entities": []}, {"text": "We propose to evaluate the translations with different word and sentence representations.", "labels": [], "entities": []}, {"text": "Specifically, we investigate the use of three widely deployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive autoencoder.", "labels": [], "entities": []}, {"text": "In particular, to leverage the different advantages and focuses, in terms of benefiting evaluation, of various representations, we concatenate the three representations to form one vector representation for each sentence.", "labels": [], "entities": []}, {"text": "Our experiments on the WMT metric task show that the metric based on the concatenated representation outperforms several state-of-the-art machine translation metrics, by a large margin on both segment and system-level.", "labels": [], "entities": [{"text": "WMT metric task", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.823550800482432}, {"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.6974398195743561}]}, {"text": "Furthermore, our results also indicate that the representation based metrics are robust to a variety of training conditions, such as the data volume and domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on the WMT metric task data.", "labels": [], "entities": [{"text": "WMT metric task data", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.748205691576004}]}, {"text": "Development sets include WMT 2011 all-to-English, and English-to-all submissions.", "labels": [], "entities": [{"text": "WMT 2011", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8148676156997681}]}, {"text": "Test sets contain Englishto-all submissions.", "labels": [], "entities": []}, {"text": "The languages \"all\" include French, Spanish, German and Czech.", "labels": [], "entities": []}, {"text": "For training the word embedding and recursive auto-encoder model, we used WMT 2013 training data 2 . We compared our metrics with smoothed BLEU (mteval-v13a), TER 3 , Meteor v1.0 4 , and Meteor Universal (i.e. v1.5) . We used the default settings for all these four metrics.", "labels": [], "entities": [{"text": "WMT 2013 training data", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9404241144657135}, {"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9891762137413025}, {"text": "TER", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.8715482354164124}]}, {"text": "When considering the representation based metrics, we tuned all the parameters to maximize the system-level \u03b3 score for all representation based metrics on the dev sets.", "labels": [], "entities": []}, {"text": "We tuned the weights for combining the three vectors automatically, using the downhill simplex method as described in).", "labels": [], "entities": []}, {"text": "The weights are 1 for the RAE vector, about 0.1 for the word embedding vector, and around 0.01 for the one-hot vector, respectively.", "labels": [], "entities": [{"text": "RAE", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.6114675998687744}]}, {"text": "We tuned other parameters manually.", "labels": [], "entities": []}, {"text": "Specifically, we set n equal to 2 for the one-hot n-gram representation, the vector size of the recursive auto-encoder to 10, and the vector size of word embeddings to 80.", "labels": [], "entities": []}, {"text": "Following WMT 2013's metric task, to measure the correlation with human judgment, we use Kendall's rank correlation coefficient \u03c4 for the segment level, and Pearson's correlation coefficient (\u03b3 in the below tables and figures) for the system-level respectively.", "labels": [], "entities": [{"text": "WMT 2013", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.8790818154811859}, {"text": "Kendall's rank correlation coefficient \u03c4", "start_pos": 89, "end_pos": 129, "type": "METRIC", "confidence": 0.7531656871239344}, {"text": "Pearson's correlation coefficient", "start_pos": 157, "end_pos": 190, "type": "METRIC", "confidence": 0.9522904604673386}]}], "tableCaptions": [{"text": " Table 1: Correlations with human judgment on  WMT data for Into-English and Out-of-English  task. Results are averaged on all test sets.", "labels": [], "entities": [{"text": "WMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.891975462436676}]}]}