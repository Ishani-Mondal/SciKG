{"title": [], "abstractContent": [{"text": "In this paper, we apply the concept of pre-training to hidden-unit conditional random fields (HUCRFs) to enable learning on unlabeled data.", "labels": [], "entities": []}, {"text": "We present a simple yet effective pre-training technique that learns to associate words with their clusters , which are obtained in an unsuper-vised manner.", "labels": [], "entities": []}, {"text": "The learned parameters are then used to initialize the supervised learning process.", "labels": [], "entities": []}, {"text": "We also propose a word clustering technique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7804003357887268}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9988896250724792}]}, {"text": "We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.7228480130434036}, {"text": "named entity recognition (NER)", "start_pos": 127, "end_pos": 157, "type": "TASK", "confidence": 0.7954476376374563}, {"text": "part-of-speech (POS) tagging tasks", "start_pos": 163, "end_pos": 197, "type": "TASK", "confidence": 0.6845294833183289}]}], "introductionContent": [{"text": "Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems), conditional random fields (CRFs) () still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9984911680221558}, {"text": "sequence tagging", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7247546166181564}]}, {"text": "The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features.", "labels": [], "entities": [{"text": "CRF training", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.9562064111232758}]}, {"text": "Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming.", "labels": [], "entities": []}, {"text": "Pre-training has been widely used in deep learning () and is one of the distinguishing advantages of deep learning models.", "labels": [], "entities": []}, {"text": "The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase.", "labels": [], "entities": []}, {"text": "The empirical results ( suggest that unsupervised pre-training has the regularization effect on the learning process and also results in a model parameter configuration that places the model near the basins of attraction of minima that support better generalization.", "labels": [], "entities": []}, {"text": "While pre-training became a standard steps in many deep learning model training recipes, it has not been applied to the family of CRFs.", "labels": [], "entities": []}, {"text": "There were several reasons for that; (i) the shallow and linear nature of basic CRF model topology, which limits their expressiveness to the inner product between data and model parameters, and (ii) Lack of a training criterion and configuration to employ pre-training on unlabeled data in a task independent way.", "labels": [], "entities": [{"text": "CRF model topology", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7679840127627054}]}, {"text": "Hidden-unit CRFs (HUCRFs) of Maaten et al.", "labels": [], "entities": []}, {"text": "(2011) provide a deeper model topology and improve the expressive power of the CRFs but it does not address how to train them in a task independent way using unlabeled data.", "labels": [], "entities": []}, {"text": "In this paper, we present an effective technique for pre-training of HUCRFs that can potentially lead to accuracy gains over HUCRF and basic linear chain CRF models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9987971782684326}]}, {"text": "We cluster words in the text and treat clusters as pseudo-labels to train an HUCRF.", "labels": [], "entities": []}, {"text": "Then we transfer the parameters corresponding to observations to initialize the training process on labeled data.", "labels": [], "entities": []}, {"text": "The intuition behind this is that words that are clustered together tend to assume the same labels.", "labels": [], "entities": []}, {"text": "Therefore, learning the model parameters to assign the correct cluster ID to each word should accrue to assigning the correct task specific label during supervised learning.", "labels": [], "entities": []}, {"text": "This pre-training step significantly reduces the challenges in training a high-performance HUCRF by (i) acquiring abroad feature coverage from unlabeled data and thus improving the generalization of the model to unseen events, (ii) finding a good a initialization point for the model parameters, and (iii) regularizing the parameter learning by minimizing variance and introducing a bias towards configurations of the parameter space that are useful for unsupervised learning.", "labels": [], "entities": []}, {"text": "We also propose a word clustering technique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7775319814682007}]}, {"text": "For example, the resulting clusters can differentiate the instance of \"bank\" in the sense of financial institutions and the land alongside the river.", "labels": [], "entities": []}, {"text": "This is an important point as different senses of a word are likely to have a different task specific tag.", "labels": [], "entities": []}, {"text": "Putting them in different clusters would enable the HU-CRF model to learn the distinction in terms of label assignment.", "labels": [], "entities": []}], "datasetContent": [{"text": "To validate the effectiveness of our pre-training method, we experiment on three sequence labeling tasks: semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7341855764389038}, {"text": "named entity recognition (NER)", "start_pos": 124, "end_pos": 154, "type": "TASK", "confidence": 0.7727469553550085}, {"text": "part-of-speech (POS) tagging", "start_pos": 160, "end_pos": 188, "type": "TASK", "confidence": 0.6529560446739197}]}, {"text": "We used L-BFGS for training CRFs 1 and the averaged perceptron for training HUCRFs.", "labels": [], "entities": [{"text": "HUCRFs", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8867459893226624}]}, {"text": "The number of hidden variables was set to 500.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of slot F1 scores on nine personal assistant domains. The numbers in boldface  are the best performing method. Subscripts mean the following: G = random initialization from a  Gaussian distribution with variance 10 \u22124 , R = pre-training with Restricted Boltzmann Machine (RBM)  using contrastive divergence of", "labels": [], "entities": []}, {"text": " Table 2: F1 Score for NER task and Accuracy for  POS task.", "labels": [], "entities": [{"text": "F1 Score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9791142046451569}, {"text": "NER task", "start_pos": 23, "end_pos": 31, "type": "TASK", "confidence": 0.8718658685684204}, {"text": "Accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9994996786117554}]}]}