{"title": [{"text": "A Distributional Analysis of a Lexicalized Statistical Parsing Model", "labels": [], "entities": [{"text": "Distributional Analysis of a Lexicalized Statistical Parsing", "start_pos": 2, "end_pos": 62, "type": "TASK", "confidence": 0.677756633077349}]}], "abstractContent": [{"text": "This paper presents some of the first data visualiza-tions and analysis of distributions fora lexicalized statistical parsing model, in order to better understand their nature.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.6710413992404938}]}, {"text": "In the course of this analysis, we have paid particular attention to parameters that include bilexical dependencies.", "labels": [], "entities": []}, {"text": "The prevailing view has been that such statistics are very informative but suffer greatly from sparse data problems.", "labels": [], "entities": []}, {"text": "By using a parser to constrain-parse its own output, and by hypothesizing and testing for distributional similarity with back-off distributions, we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable inso-far as making parse decisions.", "labels": [], "entities": []}, {"text": "Finally, our analysis has provided for the first time an effective way to do parameter selection fora generative lexicalized statistical parsing model.", "labels": [], "entities": [{"text": "generative lexicalized statistical parsing", "start_pos": 102, "end_pos": 144, "type": "TASK", "confidence": 0.8864982277154922}]}], "introductionContent": [{"text": "Lexicalized statistical parsing models, such as those built by,, and, have been enormously successful, but they also have an enormous complexity.", "labels": [], "entities": [{"text": "Lexicalized statistical parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6850864291191101}]}, {"text": "Their success has often been attributed to their sensitivity to individual lexical items, and it is precisely this incorporation of lexical items into features or parameter schemata that gives rise to their complexity.", "labels": [], "entities": []}, {"text": "In order to help determine which features are helpful, the somewhat crude-buteffective method has been to compare a model's overall parsing performance with and without a feature.", "labels": [], "entities": []}, {"text": "Often, it has seemed that features that are derived from linguistic principles result in higherperforming models).", "labels": [], "entities": []}, {"text": "While this maybe true, it is clearly inappropriate to highlight ex post facto the linguistically-motivated features and rationalize their inclusion and state how effective they are.", "labels": [], "entities": []}, {"text": "A rigorous analysis of features or parameters in relation to the entire model is called for.", "labels": [], "entities": []}, {"text": "Accordingly, this work aims to provide a thorough analysis of the nature of the parameters in a Collins-style parsing model, with particular focus on the two parameter classes that generate lexicalized modifying nonterminals, for these are where all a sentence's words are generated except for the headword of the entire sentence; also, these two parameter classes have by far the most parameters and suffer the most from sparse data problems.", "labels": [], "entities": []}, {"text": "In spite of using a Collins-style model as the basis for analysis, throughout this paper, we will attempt to present information that is widely applicable because it pertains to properties of the widely-used) and lexicalized parsing models in general.", "labels": [], "entities": []}, {"text": "This work also sheds light on the much-discussed \"bilexical dependencies\" of statistical parsing models.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.6846693158149719}]}, {"text": "Beginning with the seminal work at IBM (, and continuing with such lexicalist approaches as, these features have been lauded for their ability to approximate a word's semantics as a means to override syntactic preferences with semantic ones).", "labels": [], "entities": []}, {"text": "However, the work of showed that, with an approximate reimplementation of Collins' Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy.", "labels": [], "entities": [{"text": "Collins' Model 1", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9451674620310465}, {"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.875931978225708}]}, {"text": "The prevailing assumption was then that such bilexical statistics were not useful for making syntactic decisions, although it was not entirely clear why.", "labels": [], "entities": []}, {"text": "Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding), appearing to confirm the original result.", "labels": [], "entities": []}, {"text": "However, the present work will show that such statistics do get frequently used for the highest-probability parses, but that when a Collinsstyle model generates modifier words, the bilexical parameters are so similar to their back-off distributions as to provide almost no extra predictive information.", "labels": [], "entities": [{"text": "Collinsstyle", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.927101194858551}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Average counts and diversities of histories of the P M and P M w parameter classes. c and d are  average history count and diversity, respectively. n = c", "labels": [], "entities": []}, {"text": " Table 3: Average entropies for each parameter class.", "labels": [], "entities": []}, {"text": " Table 4. This value not only  confirms but quantifies the long-held intuition that  PP-attachment requires more than just the local  phrasal context; it is, e.g., precisely why the PP- specific features of", "labels": [], "entities": []}, {"text": " Table 4: Entropy distribution statistics for P M and P M w .", "labels": [], "entities": []}, {"text": " Table 6: Parsing results on Sections 00 and 23 with  Collins' Model 3, our emulation of Collins' Model  2 and the reduced version at a threshold of 0.06. LR  = labeled recall, LP = labeled precision.", "labels": [], "entities": [{"text": "Collins' Model 3", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9555108944574991}, {"text": "LR", "start_pos": 155, "end_pos": 157, "type": "METRIC", "confidence": 0.9722938537597656}, {"text": "recall", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.8987873792648315}, {"text": "precision", "start_pos": 190, "end_pos": 199, "type": "METRIC", "confidence": 0.8312807679176331}]}]}