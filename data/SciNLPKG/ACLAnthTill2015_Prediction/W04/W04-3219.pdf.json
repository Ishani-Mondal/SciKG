{"title": [{"text": "Monolingual Machine Translation for Paraphrase Generation", "labels": [], "entities": [{"text": "Monolingual Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7996009389559428}]}], "abstractContent": [{"text": "We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 9, "end_pos": 46, "type": "TASK", "confidence": 0.7667452444632848}]}, {"text": "The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web.", "labels": [], "entities": []}, {"text": "Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.", "labels": [], "entities": [{"text": "Alignment Error Rate (AER)", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.8832073112328848}]}, {"text": "A monotone phrasal decoder generates contextual replacements.", "labels": [], "entities": []}, {"text": "Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scal-ability than the current best-of-breed paraphrasing approaches.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.8770623207092285}, {"text": "coverage", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9859405755996704}]}], "introductionContent": [{"text": "The ability to categorize distinct word sequences as \"meaning the same thing\" is vital to applications as diverse as search, summarization, dialog, and question answering.", "labels": [], "entities": [{"text": "summarization", "start_pos": 125, "end_pos": 138, "type": "TASK", "confidence": 0.9721093773841858}, {"text": "question answering", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.857532411813736}]}, {"text": "Recent research has treated paraphrase acquisition and generation as a machine learning problem.", "labels": [], "entities": [{"text": "paraphrase acquisition and generation", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.920683965086937}]}, {"text": "We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.7656903266906738}]}, {"text": "That is, we seek to identify the optimal paraphrase T* of a sentence S by finding: We describe and evaluate an SMT-based paraphrase generation system that utilizes a monotone phrasal decoder to generate meaning-preserving paraphrases across multiple domains.", "labels": [], "entities": [{"text": "SMT-based paraphrase generation", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.9169366558392843}]}, {"text": "By adopting at the outset a paradigm geared toward generating sentences, this approach overcomes many problems encountered by task-specific approaches.", "labels": [], "entities": []}, {"text": "In particular, we show that SMT techniques can be extended to paraphrase given sufficient monolingual parallel data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9968630075454712}]}, {"text": "We show that a huge corpus of comparable and alignable sentence pairs can be culled from ready-made topical/temporal clusters of news articles gathered on a daily basis from thousands of sources on the World Wide Web, thereby permitting the system to operate outside the narrow domains typical of existing systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimented with several methods for extracting a parallel sentence-aligned corpus from news clusters using word alignment error rate, or AER, as an evaluation metric.", "labels": [], "entities": [{"text": "word alignment error rate", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.6595433130860329}, {"text": "AER", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.9898712038993835}]}, {"text": "A brief summary of these experiments is provided in.", "labels": [], "entities": []}, {"text": "To evaluate the quality of generation, we followed the lead of.", "labels": [], "entities": []}, {"text": "We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below).", "labels": [], "entities": [{"text": "MSA", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.948650062084198}, {"text": "WordNet", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9166930913925171}]}, {"text": "Since the size of this data set made it difficult to obtain statistically significant results, we also included 141 randomly selected sentences from held-out clusters.", "labels": [], "entities": []}, {"text": "We then produced paraphrases with each of the following systems and compared them with MSA and WN: \u2022 WN+LM: WordNet with a trigram LM \u2022 CL: Statistical clusters with a trigram LM \u2022 PR: The top 5 sentence rewrites produced by Phrasal Replacement.", "labels": [], "entities": [{"text": "Phrasal Replacement", "start_pos": 225, "end_pos": 244, "type": "DATASET", "confidence": 0.8401451706886292}]}, {"text": "For the sake of consistency, we did not use the judgments provided by Barzilay and Lee; instead we had two raters judge whether the output from each system was a paraphrase of the input sentence.", "labels": [], "entities": [{"text": "consistency", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9758753776550293}]}, {"text": "The raters were presented with an input sentence and an output paraphrase from each system in random order to prevent bias toward any particular judgment.", "labels": [], "entities": []}, {"text": "Since, on our first pass, we found inter-rater agreement to be somewhat low (84%), we asked the raters to make a second pass of judgments on those where they disagreed; this significantly improved agreement (96.9%).", "labels": [], "entities": [{"text": "agreement", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9959020018577576}]}, {"text": "The results of this final evaluation are summarized in. shows that PR can produce rewordings that are evaluated as plausible paraphrases more frequently than those generated by either baseline techniques or MSA.", "labels": [], "entities": [{"text": "PR", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9506525993347168}]}, {"text": "The WordNet baseline performs quite poorly, even in combination with a trigram language model: the language model does not contribute significantly to resolving lexical selection.", "labels": [], "entities": [{"text": "WordNet baseline", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9360930025577545}]}, {"text": "The performance of CL is likewise abysmal-again a language model does nothing to help.", "labels": [], "entities": []}, {"text": "The poor performance of these synonymbased techniques indicates that they have little value except as a baseline.", "labels": [], "entities": []}, {"text": "The PR model generates plausible paraphrases for the overwhelming majority of test sentences, indicating that even the relatively high AER for non-identical words is not an obstacle to successful generation.", "labels": [], "entities": [{"text": "AER", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9982869029045105}]}, {"text": "Moreover, PR was able to generate a paraphrase for all 200 sentences (including the 59 MSA examples).", "labels": [], "entities": []}, {"text": "The correlation between acceptability and PR sentence rank validates both the ranking algorithm and the evaluation methodology.", "labels": [], "entities": []}, {"text": "In, the PR model scores significantly better than MSA in terms of the percentage of paraphrase candidates accepted by raters.", "labels": [], "entities": [{"text": "PR", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.945280909538269}]}, {"text": "Moreover, PR generates at least five (and often hundreds more) distinct paraphrases for each test sentence.", "labels": [], "entities": []}, {"text": "Such perfect coverage on this dataset is perhaps fortuitous, but is nonetheless indicative of scalability.", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9452455639839172}]}, {"text": "By contrast report being able to generate paraphrases for only 59 out of 484 sentences in their training (test?) set, a total of 12%.", "labels": [], "entities": []}, {"text": "One potential concern is that PR paraphrases usually involve simple substitutions of words and short phrases (a mean edit distance of 2.9 on the top ranked sentences), whereas MSA outputs more complex paraphrases (reflected in a mean edit distance of 25.8).", "labels": [], "entities": [{"text": "PR paraphrases", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.9718784391880035}]}, {"text": "This is reflected in, which provides a breakdown of four dimensions of interest, as provided by one of our independent evaluators.", "labels": [], "entities": []}, {"text": "Some 47% of MSA paraphrases involve significant reordering, such as an active-passive alternation, whereas the monotone PR decoder precludes anything other than minor transpositions within phrasal replacements.", "labels": [], "entities": []}, {"text": "Should these facts be interpreted to mean that MSA, with its more dramatic rewrites, is ultimately more ambitious than PR?", "labels": [], "entities": [{"text": "MSA", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.602495551109314}]}, {"text": "We believe that the opposite is true.", "labels": [], "entities": []}, {"text": "A close look at MSA suggests that it is similar in spirit to example-based machine translation techniques that rely on pairing entire sentences in source and target languages, with the translation step limited to local adjustments of the target sentence (e.g..", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.754798412322998}]}, {"text": "When an input sentence closely matches a template, results can be stunning.", "labels": [], "entities": []}, {"text": "However, MSA achieves its richness of substitution at the cost of generality.", "labels": [], "entities": []}, {"text": "Inspection reveals that 15 of the 59 MSA paraphrases, or 25.4%, are based on a single high-frequency, domain-specific template (essentially a running tally of deaths in the Israeli-Palestinian conflict).", "labels": [], "entities": [{"text": "MSA paraphrases", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8012497127056122}]}, {"text": "Unless one is prepared to assume that similar templates can be found for most sentence types, scalability and domain extensibility appear beyond the reach of MSA.", "labels": [], "entities": []}, {"text": "In addition, since MSA templates pair entire sentences, the technique can produce semantically different output when there is a mismatch in information content among template training sentences.", "labels": [], "entities": []}, {"text": "Consider the third and fourth rows of, which indicate the extent of embellishment and lossiness found in MSA paraphrases and the topranked PR paraphrases.", "labels": [], "entities": []}, {"text": "Particularly noteworthy is the lossiness of MSA seen in row 4.", "labels": [], "entities": [{"text": "MSA", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.5939400792121887}]}, {"text": "illustrates a case where the MSA paraphrase yields a significant reduction in information, while PR is more conservative in its replacements.", "labels": [], "entities": []}, {"text": "While the substitutions obtained by the PR model remain for the present relatively modest, they are not trivial.", "labels": [], "entities": []}, {"text": "Changing a single content word is a legitimate form of paraphrase, and the ability to paraphrase across an arbitrarily large sentence set and arbitrary domains is a desideratum of paraphrase research.", "labels": [], "entities": []}, {"text": "We have demonstrated that the SMT-motivated PR method is capable of generating acceptable paraphrases for the overwhelming majority of sentences in abroad domain.", "labels": [], "entities": [{"text": "SMT-motivated PR", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.9650198221206665}]}, {"text": "43 / 59 = 73% 31 / 100 = 31%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. AER on the Lev12 corpus", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9979270696640015}, {"text": "Lev12", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.981719434261322}]}, {"text": " Table 2. Human acceptability judgments", "labels": [], "entities": [{"text": "Human acceptability judgments", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5974749724070231}]}, {"text": " Table 3. Qualitative analysis of paraphrases", "labels": [], "entities": [{"text": "Qualitative analysis of paraphrases", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8946749418973923}]}]}