{"title": [{"text": "Named Entity Recognition in Biomedical Texts using an HMM Model", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7076953798532486}]}], "abstractContent": [{"text": "Although there exists a huge number of biomedical texts online, there is alack of tools good enough to help people get information or knowledge from them.", "labels": [], "entities": []}, {"text": "Named entity Recognition (NER) becomes very important for further processing like information retrieval, information extraction and knowledge discovery.", "labels": [], "entities": [{"text": "Named entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7506626347700754}, {"text": "information retrieval", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7959343194961548}, {"text": "information extraction", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.8320497572422028}, {"text": "knowledge discovery", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7802352905273438}]}, {"text": "We introduce a Hidden Markov Model (HMM) for NER, with a word similarity-based smoothing.", "labels": [], "entities": [{"text": "NER", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.885117769241333}]}, {"text": "Our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data.", "labels": [], "entities": [{"text": "word similarity-based smoothing", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6454809109369913}]}, {"text": "While many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information.", "labels": [], "entities": [{"text": "word formation, prefix, suffix and abbreviation information automatically from biomedical texts", "start_pos": 157, "end_pos": 252, "type": "TASK", "confidence": 0.787988007068634}, {"text": "word distribution", "start_pos": 272, "end_pos": 289, "type": "TASK", "confidence": 0.7185078561306}]}], "introductionContent": [{"text": "In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc.", "labels": [], "entities": [{"text": "Message Understanding Conference (MUC)", "start_pos": 7, "end_pos": 45, "type": "TASK", "confidence": 0.7860972384611765}, {"text": "Named entity Recognition", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6646716197331747}]}, {"text": "Many researchers adapt their systems from MUC to the biomedical domain, such as,, (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (,, (Lee et al 2003) and (.", "labels": [], "entities": []}, {"text": "As opposed to rule-based systems, machine learning-based systems could train their models on labeled data.", "labels": [], "entities": []}, {"text": "But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems.", "labels": [], "entities": []}, {"text": "This work requires domain specific knowledge.", "labels": [], "entities": []}, {"text": "How to get the domain knowledge automatically is a question that has not been fully investigated.", "labels": [], "entities": []}, {"text": "Our system is built on an HMM model with the words themselves as the features.", "labels": [], "entities": []}, {"text": "Huge unlabeled corpus is gathered from MEDLINE.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9535976648330688}]}, {"text": "Word similarity information is computed from the corpus and we use a word similarity-based smoothing to overcome the data sparseness.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment results are shown in  The Baseline2 outperforms Baseline1 because it prevents from using low frequency unigrams, and our system outperforms Baseline1 and Baseline2 because it prevents from using low frequency bigrams and unigrams.", "labels": [], "entities": []}, {"text": "Our system benefits from huge unlabeled corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Similar words for \"IL-0\"", "labels": [], "entities": []}]}