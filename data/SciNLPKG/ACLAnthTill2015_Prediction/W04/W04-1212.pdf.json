{"title": [{"text": "Classification from Full Text: A Comparison of Canonical Sections of Scientific Papers", "labels": [], "entities": []}], "abstractContent": [{"text": "The accelerating growth in biomedical literature has stimulated activity on automated classification of and information extraction from this literature.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.8077315092086792}]}, {"text": "The work described here attempts to improve on an earlier classification study associating biological articles to GO codes.", "labels": [], "entities": []}, {"text": "It demonstrates the need, under particular assumptions, for more access to full text articles and for the use of Part-of-Speech tagging.", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.792912632226944}]}], "introductionContent": [{"text": "The accelerating growth in biomedical literature is stimulating efforts both to screen individual papers quickly for useful information and to use aggregations of papers for the collective information they provide.", "labels": [], "entities": []}, {"text": "Aggregative use may involve what one might call \"binning\" classification, where one decides which of N bins an entity should be slotted into ().", "labels": [], "entities": [{"text": "binning\" classification", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.8704962531725565}]}, {"text": "Most often these tasks have been done on titles and abstracts, simply because that is what is most freely available.", "labels": [], "entities": []}, {"text": "However the nature of titles and abstracts means that they may lack information that is relevant to the task.", "labels": [], "entities": []}, {"text": "The present study considers this issue, taking as its starting point work done by Raychaudhuri, Chang,.", "labels": [], "entities": []}, {"text": "In this work: (i) articles were associated with GO codes; and then (ii) GO codes were assigned to new genes on the basis of the GO-code associations with articles about related genes.", "labels": [], "entities": []}, {"text": "This paper reconsiders the basis for Step (i), to see if this can be done more accurately, using full text rather than titles and abstracts.", "labels": [], "entities": []}, {"text": "investigated how statistical natural language techniques could be applied to assign GO codes to genes using the titles and abstracts of articles about related genes.", "labels": [], "entities": []}, {"text": "GO codes are terms drawn from three controlled vocabularies (biological processes, cellular components and molecular functions) developed by the Gene Ontology Consortium ().", "labels": [], "entities": []}, {"text": "The Gene Ontology Consortium's aim is for gene products to be described in a consistent manner across independent databases and species.", "labels": [], "entities": []}, {"text": "Each controlled vocabulary is organised as a directed acyclic graph (DAG).", "labels": [], "entities": []}, {"text": "The GO codes that Raychaudhuri et al. chose to assign to articles (and hence to genes) were from the biological process vocabulary, an approximate horizontal cut through the biological process DAG.", "labels": [], "entities": []}, {"text": "However, some departures were made from the horizontal when the authors found difficulties in precisely defining the associated literature.", "labels": [], "entities": []}, {"text": "In departing from the horizontal, parent/child dependencies were introduced.", "labels": [], "entities": []}, {"text": "For example, GO code transport is a parent of intracellular protein traffic.", "labels": [], "entities": [{"text": "GO code transport", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.5618712504704794}]}, {"text": "MEDLINE queries on these GO codes were manually created in order to retrieve approximately 1000 articles related to each topic.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6588112711906433}]}, {"text": "The queries contained both MeSH terms and keywords.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.7626943588256836}]}, {"text": "Medical Subject Headings (MeSH)) is a controlled vocabulary from the National Library of Medicine used to aid indexing and searching biological information.", "labels": [], "entities": [{"text": "Medical Subject Headings (MeSH))", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6950252751509348}, {"text": "National Library of Medicine", "start_pos": 69, "end_pos": 97, "type": "DATASET", "confidence": 0.925863727927208}]}, {"text": "MEDLINE articles are indexed by MeSH headings, among other annotations.", "labels": [], "entities": [{"text": "MEDLINE articles", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9475932419300079}, {"text": "MeSH headings", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.8550329506397247}]}, {"text": "The PubMed search tool allows a user to specify desired search fields, of which Raychaudhuri et al. used title (TI), Major MeSH Heading (MAJR), MeSH Heading (MH) and date of publication (DP).", "labels": [], "entities": [{"text": "title (TI)", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9403585195541382}, {"text": "Major MeSH Heading (MAJR)", "start_pos": 117, "end_pos": 142, "type": "METRIC", "confidence": 0.7611763775348663}, {"text": "MeSH Heading (MH)", "start_pos": 144, "end_pos": 161, "type": "METRIC", "confidence": 0.6758472383022308}, {"text": "date of publication (DP)", "start_pos": 166, "end_pos": 190, "type": "METRIC", "confidence": 0.8575723667939504}]}, {"text": "The first three fields were used to specify the subject of the article while the DP field was used to limit the number of articles retrieved to approximately 1000.", "labels": [], "entities": [{"text": "DP field", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9254343807697296}]}, {"text": "Raychaudhuri et al. experimented with three machine learning approaches (Naive Bayes, K-Nearest Neighbours and Maximum Entropy) to classify articles according to the 21 GO codes.", "labels": [], "entities": []}, {"text": "Each classifier was trained on articles retrieved using the described queries from 1999 and earlier, and tested on articles from 2000.", "labels": [], "entities": []}, {"text": "Maximum Entropy was found to be the most successful at classifying articles, achieving 72.83% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9813101291656494}]}], "datasetContent": [{"text": "Classification results are shown in.", "labels": [], "entities": []}, {"text": "This table shows the recall, precision and F-score for each section of text and for each of the four word-bag types.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9995331764221191}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9992076754570007}, {"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.999110758304596}]}, {"text": "The first line of the table corresponds to Raychaudhuri et al.'s strategy using Naive Bayes instead of Maximum Entropy.", "labels": [], "entities": []}, {"text": "The F-score is calculated giving equal weighting to recall and precision.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.999359667301178}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9994669556617737}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9993007183074951}]}, {"text": "Titles achieved the best F-score -this occurs because the precision was much higher than the other sections.", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9989351630210876}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9996147155761719}]}, {"text": "This is not unexpected since there would be very little room for false indicators in the relatively short Title section.", "labels": [], "entities": [{"text": "Title section", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.936773270368576}]}, {"text": "The other sections have more scope for introducing negative indicators.", "labels": [], "entities": []}, {"text": "Titles consistently had lower recall in comparison with the other sections.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9996777772903442}]}, {"text": "Obviously a title can only convey the one or two main points of an article and not include every relevant topic.", "labels": [], "entities": []}, {"text": "The Methods section was expected to fare worse than other sections, since it contains more technical data, such as investigative techniques, chemicals and measurements, than information about biological processes.", "labels": [], "entities": []}, {"text": "However, performance on the Methods section was on a par with the Abstract and Introduction, suggesting that the Methods sections may give the reasoning behind certain experiments.", "labels": [], "entities": [{"text": "Abstract", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.5726010799407959}]}, {"text": "This is in contrast to conclusion that the Methods section was not valuable for the extraction of keywords relating to biological concepts compared with the other sections.", "labels": [], "entities": [{"text": "extraction of keywords relating to biological concepts", "start_pos": 84, "end_pos": 138, "type": "TASK", "confidence": 0.7768220560891288}]}, {"text": "The Introduction section can conceivably contain any type of information, including similar/opposite studies, ultimate goal of the present study, other processes related to the gene(s)/protein(s) in question and so can have many positive and negative indicators of category.", "labels": [], "entities": []}, {"text": "Thus, a similar performance to Abstracts and Methods is not to be unexpected.", "labels": [], "entities": []}, {"text": "The Results section generally produced the worst performance.", "labels": [], "entities": []}, {"text": "This could be considered surprising since here is where one would expect the proof of biological processes occurring in experimentation.", "labels": [], "entities": []}, {"text": "This outcome maybe because no reasoning is made about the results at this point.", "labels": [], "entities": []}, {"text": "Also there are frequent indicators against a category, when a biological process is found not to be affected in the experiment and is so stated, e.g. \"Biogenesis of the vacuole is not obviously disturbed in aut9 cells\" ().", "labels": [], "entities": []}, {"text": "Explicit negative information is im-portant for biologists, so that they do not to waste resources by repeating work that has already been investigated.", "labels": [], "entities": []}, {"text": "On the other hand, it has an adverse affect on classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.9556635022163391}]}, {"text": "The whole full text achieved both the best recall and the worst precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.999639630317688}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9987812638282776}]}, {"text": "The full text has maximum potential for including positive indicators of biological process just as it has maximum potential for including misleading indicators.", "labels": [], "entities": []}, {"text": "All individual sections except Titles underperformed in comparison with the baseline of Title and Abstract with regard to equally-weighted Fscore.", "labels": [], "entities": [{"text": "Abstract", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.818555474281311}, {"text": "Fscore", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.878791332244873}]}, {"text": "(See Section 4 for discussion of alternatives to equally-weighted F-score.)", "labels": [], "entities": [{"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9859419465065002}]}, {"text": "Similarly, nouns, stemmed words and stemmed nouns all produced a lower equally-weighted F-score than did the baseline of bag of words.", "labels": [], "entities": [{"text": "F-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9956466555595398}]}, {"text": "Discussion was the only section dataset to outperform the Title and Abstract with regards to recall, while no section significantly bettered Title and Abstract on precision except Titles alone.", "labels": [], "entities": [{"text": "Abstract", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.4661415219306946}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9984845519065857}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9990348815917969}]}, {"text": "While both training and testing on nouns and stems increased the performance compared to simply using a bag of words, combining these two techniques seemed to interfere with their individual usefulness.", "labels": [], "entities": []}, {"text": "The combination -first retrieving the nouns and then stemming them -achieved an increase in recall compared with just stemming, however recall was decreased compared with just using nouns.", "labels": [], "entities": [{"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9991874098777771}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9993221759796143}]}, {"text": "The trend between classifying with words and nouns differed depending on whether they are stemmed or not.", "labels": [], "entities": []}, {"text": "Recall generally increased and precision generally decreased when going from classifying with whole words to classifying with stemmed words.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.970916748046875}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9996410608291626}]}, {"text": "In contrast, recall decreased and precision increased when going from classifying with nouns to classifying with stemmed nouns.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9995323419570923}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.999631404876709}]}], "tableCaptions": [{"text": " Table 1: Comparison of individual Recall scores for previous and present studies using bag of words.", "labels": [], "entities": [{"text": "Recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.7646968364715576}]}, {"text": " Table 2: Average Recall / Precision / F-score percentages of classification of full text and individual sections  using the four NLP strategies.", "labels": [], "entities": [{"text": "Average Recall / Precision /", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7897451162338257}, {"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.5641284584999084}]}]}