{"title": [{"text": "Assigning Domains to Speech Recognition Hypotheses", "labels": [], "entities": [{"text": "Assigning Domains to Speech Recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7100638031959534}]}], "abstractContent": [{"text": "We present the results of experiments aimed at assigning domains to speech recognition hypotheses (SRH).", "labels": [], "entities": [{"text": "speech recognition hypotheses (SRH)", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.8285146107276281}]}, {"text": "The methods rely on high-level linguistic representations of SRHs as sets of ontological concepts.", "labels": [], "entities": []}, {"text": "We experimented with two domain models and evaluated their performance against a statistical, word-based model.", "labels": [], "entities": []}, {"text": "Our hand-annotated and tf*idf-based models yielded a precision of 88,39% and 82,59% respectively, compared to 93,14% for the word-based base-line model.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9992972612380981}]}, {"text": "These results are explained in terms of our experimental setup.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We performed a number of annotation experiments.", "labels": [], "entities": []}, {"text": "The purpose of these experiments was to: \u2022 investigate the reliability of the annotations; \u2022 create a domain model based on human annotations; \u2022 produce a training dataset for statistical classifiers; \u2022 set a Gold Standard as a test dataset for the evaluation.", "labels": [], "entities": []}, {"text": "All annotation experiments were conducted on data collected in hidden-operator tests following the paradigm described in.", "labels": [], "entities": []}, {"text": "Subjects were asked to verbalize a predefined intention in each of their turns, the system's reaction was simulated by a human operator.", "labels": [], "entities": []}, {"text": "We collected utterances from 29 subjects in 8 dialogues with the system each.", "labels": [], "entities": []}, {"text": "All user turns were recorded in separate audio files.", "labels": [], "entities": []}, {"text": "These audio files were processed by two versions of our dialogue system with different speech recognition modules.", "labels": [], "entities": []}, {"text": "Data describing our corpora is given in: Descriptive corpus statistics.", "labels": [], "entities": []}, {"text": "The corpora obtained from these experiments were further transformed into a set of annotation files, which can be read into GUI-based annotation tools, e.g., MMAX.", "labels": [], "entities": [{"text": "MMAX", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.9254128932952881}]}, {"text": "This tool can be adopted for annotating different levels of information, e.g., semantic coherence and domains of utterances, the best speech recognition hypothesis in the N-best list, as well as domains of individual concepts.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7531503140926361}]}, {"text": "The two annotators were trained with the help of an annotation manual.", "labels": [], "entities": []}, {"text": "A reconciled version of both annotations resulted in the Gold Standard.", "labels": [], "entities": [{"text": "the", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.6993429064750671}, {"text": "Gold Standard", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.7777908146381378}]}, {"text": "In the following, we present the results of our annotation experiments.", "labels": [], "entities": []}, {"text": "The first experiment was aimed at annotating the speech recognition hypotheses (SRH) from Dataset 1 w.r.t. their domains.", "labels": [], "entities": [{"text": "speech recognition hypotheses (SRH)", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.8170631527900696}]}, {"text": "In the first stage, the annotators labeled randomly mixed SRHs, i.e. SRHs without discourse context, for their semantic coherence as coherent or incoherent.", "labels": [], "entities": []}, {"text": "In the second stage, coherent SRHs were labeled for their domains, resulting in a corpus of 1511 hypotheses labeled for at least one domain category.", "labels": [], "entities": []}, {"text": "The numbers for ambiguous domain attributions can be found in.", "labels": [], "entities": []}, {"text": "The class distribution is given in. presents the Kappa coefficient values computed for individual categories.", "labels": [], "entities": []}, {"text": "P(A) is the percentage of agreement between annotators.", "labels": [], "entities": [{"text": "P(A)", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9598419964313507}]}, {"text": "P(E) is the percentage we expect them to agree by chance.", "labels": [], "entities": [{"text": "P(E)", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9538851529359818}]}, {"text": "The annotations are generally considered to be reliable if K > 0.8.", "labels": [], "entities": []}, {"text": "This is true for all classes except those which occur very rarely on our data.", "labels": [], "entities": []}, {"text": "As will be evident from Section 4.1, each SRH can be mapped to a set of possible interpretations, which are called conceptual representations (CR).", "labels": [], "entities": []}, {"text": "In this experiment, the best conceptual representation and the domains of coherent SRHs from Dataset 2 were annotated.", "labels": [], "entities": [{"text": "SRHs from Dataset 2", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.7205626592040062}]}, {"text": "As our system operates on the basis of CR, it is necessary to disambiguate them in a preprocessing step.", "labels": [], "entities": [{"text": "CR", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9564564228057861}]}, {"text": "867 SRHs used in this experiment are mapped to 2853 CR, i.e. on average each SRH is mapped to 3.29 CR.", "labels": [], "entities": []}, {"text": "The annotators' agreement on the task of determining the best CR reached ca.", "labels": [], "entities": []}, {"text": "For the task of domain annotation, again, we computed the absolute agreement, when the annotators agreed on all domains fora given SRH.", "labels": [], "entities": [{"text": "domain annotation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.675605982542038}, {"text": "SRH", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.9656797647476196}]}, {"text": "92.5% (SRHabs, see).", "labels": [], "entities": [{"text": "SRHabs", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.7240539193153381}]}, {"text": "The agreement on individual domain decisions (6936 overall) yielded ca.", "labels": [], "entities": []}, {"text": "98.92% (SRHindiv, see).", "labels": [], "entities": [{"text": "SRHindiv", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8175579905509949}]}, {"text": "As the suggests, annotating utterances with domains is an easier task for humans than annotating ontological concepts with the same information.", "labels": [], "entities": []}, {"text": "One possible reason for this is that even for an isolated SRH of an utterance there is at least some local context available, which clarifies its high-level meaning to some extent.", "labels": [], "entities": []}, {"text": "An isolated concept has no defining context whatsoever.", "labels": [], "entities": []}, {"text": "The evaluation of the algorithms and domain models presented herein poses a methodological problem.", "labels": [], "entities": []}, {"text": "As stated in Section 3.3, the annotators were allowed to assign 1 or more domains to an SRH, so the number of domain categories varies in the Gold Standard data.", "labels": [], "entities": [{"text": "Gold Standard data", "start_pos": 142, "end_pos": 160, "type": "DATASET", "confidence": 0.9653673768043518}]}, {"text": "The output of DOMSCORE, however, is a set with confidence values for all domains ranging from 0 to 1.", "labels": [], "entities": [{"text": "DOMSCORE", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.5329135656356812}]}, {"text": "To the best of our knowledge, there exists no evaluation method that allows the straightforward evaluation of these confidence sets against the varying number of binary domain decisions.", "labels": [], "entities": []}, {"text": "As a consequence, we restricted the evaluation to the subset of 758 SRHs unambiguously annotated fora single domain in Dataset 2.", "labels": [], "entities": []}, {"text": "For each SRH we compared the recognized domain of its best CR with the annotated domain.", "labels": [], "entities": []}, {"text": "This recognized domain is the one that was scored the highest confidence by DOMSCORE.", "labels": [], "entities": [{"text": "confidence", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9624924659729004}, {"text": "DOMSCORE", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9212397933006287}]}, {"text": "In this way we measured the precision on recognizing the best domain of an SRH.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9994731545448303}]}, {"text": "The best conceptual representation of an SRH had been previously disambiguated by humans as reported in Section 3.3.", "labels": [], "entities": [{"text": "SRH", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9062734842300415}]}, {"text": "Alternatively, this kind of disambiguation can be performed automatically, e.g., with the help of the system presented in.", "labels": [], "entities": []}, {"text": "The system scores semantic coherence of SRHs, where the best CR is the one with the highest semantic coherence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The first and the sec- ond system's runs are referred to as Dataset 1 and  Dataset 2 respectively.", "labels": [], "entities": []}, {"text": " Table 1: Descriptive corpus statistics.", "labels": [], "entities": []}, {"text": " Table 2. The class distribution is given  in", "labels": [], "entities": []}, {"text": " Table 2: Multiple domain assignments in Dataset 1.", "labels": [], "entities": []}, {"text": " Table 3: Class distribution for domain assignments.", "labels": [], "entities": []}, {"text": " Table 4: Kappa coefficient for separate domains.", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9715573489665985}]}, {"text": " Table 5: Matrix DM anno derived from human anno- tations.", "labels": [], "entities": []}, {"text": " Table 6: Matrix DM tf  * idf derived from the anno- tated corpus.", "labels": [], "entities": []}, {"text": " Table 7: Domain scores on the basis of DM anno .", "labels": [], "entities": []}, {"text": " Table 8: Domain scores on the basis of DM tf  * idf .", "labels": [], "entities": []}, {"text": " Table 9: Class distribution in the evaluation dataset.", "labels": [], "entities": []}]}