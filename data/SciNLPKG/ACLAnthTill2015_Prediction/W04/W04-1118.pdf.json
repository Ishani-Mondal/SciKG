{"title": [{"text": "Do We Need Chinese Word Segmentation for Statistical Machine Translation?", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.5451802909374237}, {"text": "Statistical Machine Translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.874455451965332}]}], "abstractContent": [{"text": "In Chinese texts, words are not separated by white spaces.", "labels": [], "entities": []}, {"text": "This is problematic for many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7081136777997017}]}, {"text": "The standard approach is to segment the Chinese character sequence into words.", "labels": [], "entities": []}, {"text": "Here, we investigate Chi-nese word segmentation for statistical machine translation.", "labels": [], "entities": [{"text": "Chi-nese word segmentation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6746959984302521}, {"text": "statistical machine translation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.6924000680446625}]}, {"text": "We pursue two goals: the first one is the maximization of the final translation quality ; the second is the minimization of the manual effort for building a translation system.", "labels": [], "entities": []}, {"text": "The commonly used method forgetting the word boundaries is based on a word segmenta-tion tool and a predefined monolingual dictionary.", "labels": [], "entities": [{"text": "forgetting the word boundaries", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.8355030119419098}]}, {"text": "To avoid the dependence of the translation system on an external dictionary, we have developed a system that learns a domain-specific dictionary from the parallel training corpus.", "labels": [], "entities": []}, {"text": "This method produces results that are comparable with the predefined dictionary.", "labels": [], "entities": []}, {"text": "Further more, our translation system is able to work without word segmentation with only a minor loss in translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9596063494682312}, {"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7302304357290268}]}], "introductionContent": [{"text": "In Chinese texts, words composed of single or multiple characters, are not separated by white spaces, which is different from most of the western languages.", "labels": [], "entities": []}, {"text": "This is problematic for many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7081134766340256}]}, {"text": "Therefore, the usual method is to segment a Chinese character sequence into Chinese \"words\".", "labels": [], "entities": []}, {"text": "Many investigations have been performed concerning Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.5856333573659261}]}, {"text": "For example,) developed a Chinese word segmenter using a manually segmented corpus.", "labels": [], "entities": [{"text": "Chinese word segmenter", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.636913518110911}]}, {"text": "The segmentation rules were learned automatically from this corpus. and) used a method that does not rely on a dictionary or a manually segmented corpus.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9690349698066711}]}, {"text": "The characters of the unsegmented Chinese text are grouped into pairs with the highest value of mutual information.", "labels": [], "entities": []}, {"text": "This mutual information can be learned from an unsegmented Chinese corpus.", "labels": [], "entities": []}, {"text": "We will present anew method for segmenting the Chinese text without using a manually segmented corpus or a predefined dictionary.", "labels": [], "entities": []}, {"text": "In statistical machine translation, we have a bilingual corpus available, which is used to obtain a segmentation of the Chinese text in the following way.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6770376761754354}]}, {"text": "First, we train the statistical translation models with the unsegmented bilingual corpus.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.5640871524810791}]}, {"text": "As a result, we obtain a mapping of Chinese characters to the corresponding English words for each sentence pair.", "labels": [], "entities": []}, {"text": "By using this mapping, we can extract a dictionary automatically.", "labels": [], "entities": []}, {"text": "With this self-learned dictionary, we use a segmentation tool to obtain a segmented Chinese text.", "labels": [], "entities": []}, {"text": "Finally, we retrain our translation system with the segmented corpus.", "labels": [], "entities": []}, {"text": "Additionally, we have performed experiments without explicit word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7542821168899536}]}, {"text": "In this case, each Chinese character is interpreted as one \"word\".", "labels": [], "entities": []}, {"text": "Based on word groups, our machine translation system is able to work without a word segmentation, while having only a minor translation quality relative loss of less than 5%.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7270143330097198}, {"text": "word segmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.6987403780221939}]}], "datasetContent": [{"text": "So far, in machine translation research, a single generally accepted criterion for the evaluation of the experimental results does not exist.", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.87850288550059}]}, {"text": "We have used three automatic criteria.", "labels": [], "entities": []}, {"text": "For the test corpus, we have four references available.", "labels": [], "entities": []}, {"text": "Hence, we compute all the following criteria with respect to multiple references.", "labels": [], "entities": []}, {"text": "\u2022 WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence into the reference sentence.", "labels": [], "entities": [{"text": "WER (word error rate)", "start_pos": 2, "end_pos": 23, "type": "METRIC", "confidence": 0.8476704061031342}, {"text": "WER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9945435523986816}]}, {"text": "\u2022 PER (position-independent word error rate): A shortcoming of the WER is that it requires a perfect word order.", "labels": [], "entities": [{"text": "PER", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9972148537635803}, {"text": "position-independent word error rate)", "start_pos": 7, "end_pos": 44, "type": "METRIC", "confidence": 0.7197307765483856}, {"text": "WER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7456610202789307}]}, {"text": "The word order of an acceptable sentence can be different from that of the target sentence, so that the WER measure alone could be misleading.", "labels": [], "entities": [{"text": "WER measure", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.9848631024360657}]}, {"text": "The PER compares the words in the two sentences ignoring the word order.", "labels": [], "entities": [{"text": "PER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9520605206489563}]}, {"text": "\u2022 BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9864911735057831}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9988828301429749}]}, {"text": "The BLEU score measures accuracy, i.e. large BLEU scores are better.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9694723188877106}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995906949043274}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9987201690673828}]}, {"text": "The evaluation is performed on the LDC corpus described in Section 3.", "labels": [], "entities": [{"text": "LDC corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.8891532719135284}]}, {"text": "The translation performance of the three systems is summarized in for the three evaluation criteria WER, PER and BLEU.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9616897106170654}, {"text": "WER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9973841309547424}, {"text": "PER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9920201897621155}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9969450831413269}]}, {"text": "We observe that the translation quality with the learned segmentation is similar to that with the LDC segmentation.", "labels": [], "entities": []}, {"text": "The WER of the system with the learned segmentation is somewhat better, but PER and BLEU are slightly worse.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9989666938781738}, {"text": "PER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9981098175048828}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9982988238334656}]}, {"text": "We conclude that it is possible to learn a domain-specific dictionary for Chinese word segmentation from a bilingual corpus.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.6739019155502319}]}, {"text": "Therefore the translation system is independent of a predefined dictionary, which maybe unsuitable fora certain task.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9701022505760193}]}, {"text": "The translation system using no segmentation performs slightly worse.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9720218777656555}]}, {"text": "For example, for the WER there is a loss of about 2% relative compared to the system with the LDC segmentation.", "labels": [], "entities": [{"text": "WER", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.45305335521698}]}], "tableCaptions": [{"text": " Table 1: Statistics of training and test corpus.  For each of the two languages, there is a set of 20  special characters, such as digits, punctuation  marks and symbols like \"()%$...\"", "labels": [], "entities": []}, {"text": " Table 3: Statistics of word lengths in the LDC  dictionary and in the learned dictionary.", "labels": [], "entities": [{"text": "LDC  dictionary", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8828885853290558}]}, {"text": " Table 4: Translation performance of different  segmentation methods (all numbers in percent).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9798091650009155}]}]}