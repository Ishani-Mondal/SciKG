{"title": [{"text": "Linguistic Preprocessing for Distributional Classification of Words", "labels": [], "entities": [{"text": "Distributional Classification of Words", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.8964130282402039}]}], "abstractContent": [{"text": "The paper is concerned with automatic classification of new lexical items into synonymic sets on the basis of their co-occurrence data obtained from a corpus.", "labels": [], "entities": [{"text": "automatic classification of new lexical items into synonymic sets", "start_pos": 28, "end_pos": 93, "type": "TASK", "confidence": 0.8415457606315613}]}, {"text": "Our goal is to examine the impact that different types of linguistic preprocessing of the co-occurrence material have on the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9110239148139954}]}, {"text": "The paper comparatively studies several preprocessing techniques frequently used for this and similar tasks and makes conclusions about their relative merits.", "labels": [], "entities": []}, {"text": "We find that a carefully chosen preprocessing procedure achieves a relative effectiveness improvement of up to 88% depending on the classification method in comparison to the window-based context delineation, along with using much smaller feature space.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the fast development of text mining technologies, automated management of lexical resources is presently an important research issue.", "labels": [], "entities": [{"text": "text mining", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8474339246749878}]}, {"text": "A particular text mining task often requires a lexical database (e.g., a thesaurus, dictionary, or a terminology) with a specific size, topic coverage, and granularity of encoded meaning.", "labels": [], "entities": [{"text": "text mining task", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7878819604714712}]}, {"text": "That is why a lot of recent NLP and AI research has been focusing on finding ways to speedily build or extend a lexical resource ad hoc for an application.", "labels": [], "entities": []}, {"text": "One attractive idea to address this problem is to elicit the meanings of new words automatically from a corpus relevant to the application domain.", "labels": [], "entities": []}, {"text": "To do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.859155535697937}]}, {"text": "The approach is now being actively explored fora wide range of semantics-related tasks including automatic construction of thesauri, their enrichment (), acquisition of bilingual lexica from nonaligned ( and nonparallel corpora), learning of information extraction patterns from un-annotated text (.", "labels": [], "entities": [{"text": "learning of information extraction patterns from un-annotated text", "start_pos": 230, "end_pos": 296, "type": "TASK", "confidence": 0.7511937022209167}]}, {"text": "However, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.", "labels": [], "entities": []}, {"text": "In order to improve robustness, recent research has attempted a variety of ways to incorporate external knowledge into the distributional model.", "labels": [], "entities": []}, {"text": "In this paper we investigate the impact produced by the introduction of different types of linguistic knowledge into the model.", "labels": [], "entities": []}, {"text": "Linguistic knowledge, i.e., the knowledge about linguistically relevant units of text and relations holding between them, is a particularly convenient way to enhance the distributional model.", "labels": [], "entities": []}, {"text": "On the one hand, although describing the \"surface\" properties of the language, linguistic notions contain conceptual information about the units of text they describe.", "labels": [], "entities": []}, {"text": "It is therefore reasonable to expect that the linguistic analysis of the context of a word yields additional evidence about its meaning.", "labels": [], "entities": []}, {"text": "On the other hand, linguistic knowledge is relatively easy to obtain: linguistic analyzers (lemmatizers, PoS-taggers, parsers, etc) do not require expensive hand-encoded resources, their application is not restricted to particular domains, and their performance is not dependent on the amount of the textual data.", "labels": [], "entities": []}, {"text": "All these characteristics fit very well with the strengths of the distributional approach: while enhancing it with external knowledge, linguistic analyzers do not limit its coverage and portability.", "labels": [], "entities": []}, {"text": "This or that kind of linguistic preprocessing is carried out in many previous applications of the approach.", "labels": [], "entities": []}, {"text": "However, these studies seldom motivate the choice of a particular preprocessing procedure, concentrating rather on optimization of other parameters of the methodology.", "labels": [], "entities": []}, {"text": "Very few studies exist that analyze and compare different techniques for linguistically motivated extraction of distributional data.", "labels": [], "entities": []}, {"text": "The goal of this paper is to exploire in detail a range of variables in the morphological and syntactic processing of the context information and reveal the merits and drawbacks of their particular settings.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the preprocessing methods understudy.", "labels": [], "entities": []}, {"text": "Section 3 describes the settings for their empirical evaluation.", "labels": [], "entities": []}, {"text": "Section 4 details the experimental results.", "labels": [], "entities": []}, {"text": "Section 5 discusses related work.", "labels": [], "entities": []}, {"text": "Section 6 summarizes the results and presents the conclusions from the study.", "labels": [], "entities": []}], "datasetContent": [{"text": "The preprocessing techniques were evaluated on the task of automatic classification of nouns into semantic classes.", "labels": [], "entities": [{"text": "automatic classification of nouns into semantic classes", "start_pos": 59, "end_pos": 114, "type": "TASK", "confidence": 0.7644546968596322}]}, {"text": "The evaluation of each preprocessing method consisted in the following.", "labels": [], "entities": []}, {"text": "A set of nouns N each belonging to one semantic class c\u2208C was randomly split into ten equal parts.", "labels": [], "entities": []}, {"text": "Co-occurrence data on the nouns was collected and preprocessed using a particular method under analysis.", "labels": [], "entities": []}, {"text": "Then each noun n\u2208N was represented as a vector of distributional features: n r = (v n,1 , v n,2 , \u2026 v n,i ), where the values of the features are the frequencies of n occurring in the lexical context corresponding to v.", "labels": [], "entities": []}, {"text": "At each experimental run, one of the ten subsets of the nouns was used as the test data and the remaining ones as the train data.", "labels": [], "entities": []}, {"text": "The reported effectiveness measures are microaveraged precision scores averaged over the ten runs.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.8360469341278076}]}, {"text": "The statistical significance of differences between performance of particular preprocessing methods reported below was estimated by means of the onetailed paired t-test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Different kinds of features derived from  prepositional phrases involving target nouns.", "labels": [], "entities": []}, {"text": " Table 2. Syntactically-defined types of features.", "labels": [], "entities": []}, {"text": " Table 3. Combinations of syntactically-defined  feature types.", "labels": [], "entities": []}, {"text": " Table 4. Morphological preprocessing of verbs,  adjectives, and nouns.", "labels": [], "entities": [{"text": "Morphological preprocessing of verbs", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.9406217783689499}]}, {"text": " Table 5. Distributional features derived from the  morphological analysis of context words.", "labels": [], "entities": []}, {"text": " Table 4: The effect of removing rare context words.", "labels": [], "entities": []}]}