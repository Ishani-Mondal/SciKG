{"title": [{"text": "Error Measures and Bayes Decision Rules Revisited with Applications to POS Tagging", "labels": [], "entities": [{"text": "Error Measures", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7268806844949722}, {"text": "POS Tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.734880656003952}]}], "abstractContent": [{"text": "Starting from first principles, we re-visit the statistical approach and study two forms of the Bayes decision rule: the common rule for minimizing the number of string errors and a novel rule for minimizing the number of symbols errors.", "labels": [], "entities": []}, {"text": "The Bayes decision rule for minimizing the number of string errors is widely used, e.g. in speech recognition, POS tagging and machine translation, but its justification is rarely questioned.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7986034452915192}, {"text": "POS tagging", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.8299282789230347}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7899022102355957}]}, {"text": "To minimize the number of symbol errors as is more suitable fora task like POS tagging, we show that another form of the Bayes decision rule can be derived.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.7200666964054108}]}, {"text": "The major purpose of this paper is to show that the form of the Bayes decision rule should not betaken for granted (as it is done in virtually all statistical NLP work), but should be adapted to the error measure being used.", "labels": [], "entities": []}, {"text": "We present first experimental results for POS tagging tasks.", "labels": [], "entities": [{"text": "POS tagging tasks", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.9097259044647217}]}], "introductionContent": [{"text": "Meanwhile, the statistical approach to natural language processing (NLP) tasks like speech recognition, POS tagging and machine translation has found widespread use.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 39, "end_pos": 78, "type": "TASK", "confidence": 0.7879784277507237}, {"text": "speech recognition", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7665526568889618}, {"text": "POS tagging", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.8335394859313965}, {"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7840542197227478}]}, {"text": "There are three ingredients to any statistical approach to NLP, namely the Bayes decision rule, the probability models (like trigram model, HMM, ...) and the training criterion (like maximum likelihood, mutual information, ...).", "labels": [], "entities": []}, {"text": "The topic of this paper is to re-consider the form of the Bayes decision rule.", "labels": [], "entities": []}, {"text": "In virtually all NLP tasks, the specific form of the Bayes decision rule is never questioned, and the decision rule is adapted from speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7019312828779221}]}, {"text": "In speech recognition, the typical decision rule is to maximize the sentence probability overall possible sentences.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8046139776706696}]}, {"text": "However, this decision rule is optimal for the sentence error rate and not for the word error rate.", "labels": [], "entities": []}, {"text": "This difference is rarely studied in the literature.", "labels": [], "entities": []}, {"text": "As a specific NLP task, we will consider partof-speech (POS) tagging.", "labels": [], "entities": [{"text": "partof-speech (POS) tagging", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.5822632789611817}]}, {"text": "However, the problem addressed comes up in any NLP task which is tackled by the statistical approach and which makes use of a Bayes decision rule.", "labels": [], "entities": []}, {"text": "Other prominent examples are speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8373228013515472}, {"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8028846383094788}]}, {"text": "The advantage of the POS tagging task is that it will be easier to handle from the mathematical point of view and will result in closedform solutions for the decision rules.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.8787573377291361}]}, {"text": "From this point-of-view, the POS tagging task serves as a good opportunity to illustrate the key concepts of the statistical approach to NLP.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.855859100818634}]}, {"text": "Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60's and 70's (, before they started to find widespread use in the 80's.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9693544805049896}]}, {"text": "To the best of our knowledge, the 'standard' version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 165, "end_pos": 176, "type": "TASK", "confidence": 0.9268181920051575}]}, {"text": "There are only two research groups that do not take this type of decision rule for granted:: In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.7943878769874573}, {"text": "maximum likelihood tagging", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.5775921742121378}]}, {"text": "The spirit of this method is similar to that of this work.", "labels": [], "entities": []}, {"text": "However, this method is mentioned as an aside and its implications for the Bayes decision rule and the statistical approach are not addressed.", "labels": [], "entities": []}, {"text": "Part of this work goes back to ( who considered a problem in coding theory.", "labels": [], "entities": [{"text": "coding theory", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9623547792434692}]}, {"text": "(: The error measure considered by the authors is the word error rate in speech recognition, i.e. the edit distance.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.8195291558901469}, {"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6870173215866089}]}, {"text": "Due to the mathematical complexity of this error measure, the authors resort to numeric approximations to compute the Bayes risk (see next section).", "labels": [], "entities": [{"text": "Bayes risk", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.8860450983047485}]}, {"text": "Since this approach does not results in explicit closed-form equations and involves many numeric approximations, it is not easy to draw conclusions from this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Of course, there have already been many papers about POS tagging using statistical methods.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.936300665140152}]}, {"text": "The goal of the experiments is to compare the two decision rules and to analyze the differences in performance.", "labels": [], "entities": []}, {"text": "As the results for the WSJ corpus will show, both the trigram method and the maximum entropy method have an tagging error rate of 3.0% to 3.5% and are thus comparable to the best results reported in the literature, e.g..", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.9332769215106964}, {"text": "tagging error rate", "start_pos": 108, "end_pos": 126, "type": "METRIC", "confidence": 0.7386853496233622}]}], "tableCaptions": [{"text": " Table 1: WSJ corpus statistics.", "labels": [], "entities": [{"text": "WSJ corpus statistics", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.9117771983146667}]}, {"text": " Table 2: MTP corpus statistics.", "labels": [], "entities": [{"text": "MTP corpus", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.7535444796085358}]}, {"text": " Table 3: POS tagging error rates [%] for WSJ task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6976599395275116}, {"text": "WSJ task", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.5643852949142456}]}, {"text": " Table 4: POS tagging error rates [%] for MTP task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7148629724979401}, {"text": "MTP task", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.7300914227962494}]}]}