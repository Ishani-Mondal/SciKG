{"title": [{"text": "A Rhetorical Status Classifier for Legal Text Summarisation", "labels": [], "entities": [{"text": "Rhetorical Status Classifier", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.7397151788075765}, {"text": "Legal Text Summarisation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6084327598412832}]}], "abstractContent": [{"text": "We describe a classifier which determines the rhetorical status of sentences in texts from a corpus of judgments of the UK House of Lords.", "labels": [], "entities": [{"text": "UK House of Lords", "start_pos": 120, "end_pos": 137, "type": "DATASET", "confidence": 0.8811790645122528}]}, {"text": "Our sum-marisation system is based on the work of Teufel and Moens where sentences are classified for rhetorical status to aid sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7211329936981201}]}, {"text": "We experiment with a variety of linguistic features with results comparable to Teufel and Moens, thereby demonstrating the feasibility of porting this kind of system to anew domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Law reports form an interesting domain for automatic summarisation.", "labels": [], "entities": [{"text": "automatic summarisation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6549138128757477}]}, {"text": "They are texts which record the proceedings of a court and, due to the role that precedents play in English law, easy access to them is essential fora wide range of people.", "labels": [], "entities": []}, {"text": "For this reason, they are frequently manually summarised by legal experts, with summaries varying according to target audience (e.g. students, solicitors).", "labels": [], "entities": []}, {"text": "In the SUM project, we are exploring methods for generating flexible summaries of legal documents, taking as our point of departure the approach to automatic summarisation (henceforth T&M).", "labels": [], "entities": [{"text": "SUM", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8698058724403381}, {"text": "summaries of legal documents", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.8137439489364624}]}, {"text": "We have chosen to work with law reports for three main reasons: (a) the existence of manual summaries means that we have evaluation material for the final summarisation system; (b) the existence of differing target audiences allows us to explore the issue of tailored summaries; and (c) the texts have much in common with the academic papers that T&M worked with, while remaining challengingly different in many respects.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.9080949425697327}, {"text": "T&M", "start_pos": 347, "end_pos": 350, "type": "DATASET", "confidence": 0.7922993103663126}]}, {"text": "Our general aims are comparable with those of the SALOMON project (, which also deals with summarisation of legal texts, but our choice of methodology is designed to test the portability of the T&M approach to anew domain.", "labels": [], "entities": [{"text": "summarisation of legal texts", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.9241467714309692}, {"text": "T&M", "start_pos": 194, "end_pos": 197, "type": "TASK", "confidence": 0.5546070337295532}]}, {"text": "The T&M approach is an instance of what Sp\u00e4rck Jones (1999) terms text extraction where a summary typically consists of sentences selected from the source text, with some smoothing to increase the coherence between the sentences.", "labels": [], "entities": [{"text": "text extraction", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7453583329916}]}, {"text": "Since the academic texts they use are rather long and the aim is to produce flexible summaries of varying length and for various audiences, T&M go beyond simple sentence selection and classify source sentences according to their rhetorical status (e.g. a description of the main result, a criticism of someone else's work, etc.).", "labels": [], "entities": [{"text": "T&M", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.743109921614329}]}, {"text": "With sentences classified in this manner, different kinds of summaries can be generated.", "labels": [], "entities": []}, {"text": "Sentences can be reordered, since they have rhetorical roles associated with them, or they can be suppressed if a user is not interested in certain types of rhetorical roles.", "labels": [], "entities": []}, {"text": "In the second stage of our project we will explore techniques for sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8357524573802948}]}, {"text": "Following the T&M methodology, we will annotate sentences in the corpus for 'relevance'.", "labels": [], "entities": [{"text": "T&M methodology", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.729505330324173}]}, {"text": "For our corpus we hope to be able to compute relevance by using automatic techniques to pair up sentences from manually created abstracts with sentences in the source text.", "labels": [], "entities": []}, {"text": "The addition of this layer of annotation will provide the training and testing material for sentence extraction, with the rhetorical role labels helping to constrain the type of summary generated.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8038528859615326}]}, {"text": "In this paper we focus on our rhetorical status classifier.", "labels": [], "entities": [{"text": "rhetorical status classifier", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.813774844010671}]}, {"text": "This is a key part of the summarisation process and our work can bethought of as a test of portability of the T&M approach to anew domain.", "labels": [], "entities": [{"text": "summarisation process", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.9029838144779205}, {"text": "T&M", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7482486168543497}]}, {"text": "At the same time, our methods differ in important respects from those of T&M and in reporting our work we will attempt to draw comparisons wherever possible.", "labels": [], "entities": [{"text": "T&M", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8626682559649149}]}, {"text": "In Section 2 we describe the House of Lords corpus we have gathered and annotated.", "labels": [], "entities": [{"text": "House of Lords corpus", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.5973683446645737}]}, {"text": "We explain the rhetorical role annotation scheme that we have developed and contrast it with the T&M scheme for academic articles.", "labels": [], "entities": [{"text": "rhetorical role annotation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8069934447606405}, {"text": "T&M", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.5016579329967499}]}, {"text": "We provide inter-annotation agreement results for the annotation scheme.", "labels": [], "entities": []}, {"text": "In Section 2.3 we give an overview of the tools and techniques we have used in the automatic linguistic processing of the judgments.", "labels": [], "entities": [{"text": "automatic linguistic processing of the judgments", "start_pos": 83, "end_pos": 131, "type": "TASK", "confidence": 0.7780934820572535}]}, {"text": "Section 3 describes our sentence classifier.", "labels": [], "entities": []}, {"text": "In Section 3.1 we review the kinds of features that can be used by a classifier and describe the set of features used in our experiments.", "labels": [], "entities": []}, {"text": "In Section 3.2 we present the results of experiments with four classifiers and discuss the relative effectiveness of the methods and the feature sets.", "labels": [], "entities": []}, {"text": "Finally, in Section 4 we draw some conclusions and outline future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Micro-averaged F-score results for rhetorical classification", "labels": [], "entities": [{"text": "Micro-averaged", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9250733852386475}, {"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8199288845062256}, {"text": "rhetorical classification", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.9912479519844055}]}]}