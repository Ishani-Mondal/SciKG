{"title": [{"text": "Phrase Pair Rescoring with Term Weightings for Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase Pair Rescoring", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7455396850903829}, {"text": "Statistical Machine Translation", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.8417636156082153}]}], "abstractContent": [{"text": "We propose to score phrase translation pairs for statistical machine translation using term weight based models.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6821411848068237}, {"text": "statistical machine translation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.6718182265758514}]}, {"text": "These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs.", "labels": [], "entities": []}, {"text": "The translation probability is then modeled by similarity functions defined in a vector space.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9673148989677429}]}, {"text": "Two similarity functions are compared.", "labels": [], "entities": []}, {"text": "Using these models in a statistical machine translation task shows significant improvements.", "labels": [], "entities": [{"text": "statistical machine translation task", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.7158155515789986}]}], "introductionContent": [{"text": "Words can be classified as content and functional words.", "labels": [], "entities": []}, {"text": "Content words like verbs and proper nouns are more informative than function words like \"to'' and \"the''.", "labels": [], "entities": []}, {"text": "In machine translation, intuitively, the informative content words should be emphasized more for better adequacy of the translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7689282298088074}]}, {"text": "However, the standard statistical translation approach does not take account how informative and thereby, how important a word is, in its translation model.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6864352822303772}]}, {"text": "One reason is the difficulty to measure how informative a word is.", "labels": [], "entities": []}, {"text": "Another problem is to integrate it naturally into the existing statistical machine translation framework, which typically is built on word alignment models, like the well-known IBM alignment models (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6254801650842031}, {"text": "word alignment", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.6998386532068253}]}, {"text": "In recent years there has been a strong tendency to incorporate phrasal translation into statistical machine translation.", "labels": [], "entities": [{"text": "phrasal translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7140983492136002}, {"text": "statistical machine translation", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.7042352358500162}]}, {"text": "It directly translates an n-gram from the source language into an mgram in the target language.", "labels": [], "entities": []}, {"text": "The advantages are obvious: It has built-in local context modeling, and provides reliable local word reordering.", "labels": [], "entities": []}, {"text": "It has multi-word translations, and models a word's conditional fertility given a local context.", "labels": [], "entities": []}, {"text": "It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries.", "labels": [], "entities": []}, {"text": "In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.676907479763031}]}, {"text": "The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as ().", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8027266561985016}]}, {"text": "However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models.", "labels": [], "entities": [{"text": "phrase translation pairs", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7730061213175455}]}, {"text": "The leads to the question what probability should be assigned to those phrase translations.", "labels": [], "entities": []}, {"text": "Different approaches have been suggested as using relative frequencies (, calculate probabilities based on a statistical word-to-word dictionary ( ) or use a linear interpolation of these scores (.", "labels": [], "entities": []}, {"text": "In this paper we investigate a different approach with takes the information content of words better into account.", "labels": [], "entities": []}, {"text": "Term weighting based vector models are proposed to encode the translation quality.", "labels": [], "entities": []}, {"text": "The advantage is that term weights, such as tf.idf, are useful to model the informativeness of words.", "labels": [], "entities": []}, {"text": "Highly informative content words usually have high tf.idf scores.", "labels": [], "entities": []}, {"text": "In information retrieval this has been successfully applied to capture the relevance of a document to a query, by representing both query and documents as term weight vectors and use for example the cosine distance to calculate the similarity between query vector and document vector.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7710487842559814}]}, {"text": "The idea now is to consider the source phrase as a \"query\", and the different target phrases extracted from the bilingual corpus as translation candidates as a relevant \"documents\".", "labels": [], "entities": []}, {"text": "The cosine distance is then a natural choice to model the translation probability.", "labels": [], "entities": [{"text": "translation", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.951200008392334}]}, {"text": "Our approach is to apply term weighting schemes to transform source and target phrases into term vectors.", "labels": [], "entities": []}, {"text": "Usually content words in both source and target languages will be emphasized by large term weights.", "labels": [], "entities": []}, {"text": "Thus, good phrase translation pairs will share similar contours, or, to express it in a different way, will be close to each other in the term weight vector space.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7087554931640625}]}, {"text": "A similarity function is then defined to approximate translation probability in the vector space.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: in Section 2, our phrase-based statistical machine translation system is introduced; in Section 3, a phrase translation score function based on word translation probabilities is explained, as this will be used as a baseline system; in Section 4, a vector model based on tf.idf is proposed together with two similarity functions; in Section 5, length regularization and smoothing schemes are explained briefly; in Section 6, the translation experiments are presented; and Section 7 concludes with a discussion.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.5656429678201675}, {"text": "phrase translation", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.6882410645484924}]}], "datasetContent": [{"text": "Experiments were carried out on the so-called large data track Chinese-English TIDES translation task, using the June 2002 test data.", "labels": [], "entities": [{"text": "TIDES translation task", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7693496445814768}, {"text": "June 2002 test data", "start_pos": 113, "end_pos": 132, "type": "DATASET", "confidence": 0.7305390685796738}]}, {"text": "The training data used to train the statistical lexicon and to extract the phrase translation pairs was selected from a 120 million word parallel corpus in such away as to cover the phrases in test sentences.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.6935240030288696}]}, {"text": "The restricted training corpus contained then approximately 10 million words..", "labels": [], "entities": []}, {"text": "A trigram model was built on 20 million words of general newswire text, using the SRILM toolkit.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.8813477456569672}]}, {"text": "Decoding was carried out as described in section 2.2.", "labels": [], "entities": [{"text": "Decoding", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.7547786235809326}]}, {"text": "The test data consists of 878 Chinese sentences or 24,337 words afterword segmentation.", "labels": [], "entities": []}, {"text": "There are four human translations per Chinese sentence as references.", "labels": [], "entities": []}, {"text": "Both NIST score and Bleu score (in percentage) are reported for adequacy and fluency aspects of the translation quality.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 5, "end_pos": 15, "type": "DATASET", "confidence": 0.6588575541973114}, {"text": "Bleu score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9824329912662506}]}], "tableCaptions": []}