{"title": [{"text": "Creating a Test Corpus of Clinical Notes Manually Tagged for Part-of-Speech Information", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a project whose main goal is to construct a corpus of clinical text manually annotated for part-of-speech information.", "labels": [], "entities": []}, {"text": "We describe and discuss the process of training three domain experts to perform linguistic annotation.", "labels": [], "entities": []}, {"text": "We list some of the challenges as well as encouraging results pertaining to inter-rater agreement and consistency of annotation.", "labels": [], "entities": [{"text": "consistency", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9688143730163574}]}, {"text": "We also present preliminary experimental results indicating the necessity for adapting state-of-the-art POS taggers to the sublanguage domain of medical text.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.7717776298522949}]}], "introductionContent": [{"text": "Having reliable part-of-speech (POS) information is critical to successful implementation of Natural Language Processing (NLP) techniques for processing unrestricted text in the biomedical domain.", "labels": [], "entities": []}, {"text": "State-of-the-art automated POS taggers achieve accuracy of 93% -98% and the most successful implementations are based on statistical approaches to POS tagging.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8570725917816162}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9995118379592896}, {"text": "POS tagging", "start_pos": 147, "end_pos": 158, "type": "TASK", "confidence": 0.8954704999923706}]}, {"text": "Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead.", "labels": [], "entities": []}, {"text": "The prime public domain examples of such implementations include the Trigrams'n'Tags tagger, Xerox tagger () and LT POS tagger.", "labels": [], "entities": [{"text": "Trigrams'n'Tags tagger", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.6276503801345825}, {"text": "LT POS tagger", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.6645380258560181}]}, {"text": "Maximum Entropy (MaxEnt) based taggers also seem to perform very well (Ratnaparkhi 1996, Jason Baldridge, Tom Morton, and Gann Bierner http://maxent.sourceforge.net ).", "labels": [], "entities": []}, {"text": "One of the issues with statistical POS taggers is that most of them need a representative amount of hand-labeled training data either in the form of a comprehensive lexicon and a corpus of untagged data or a large corpus of text annotated for POS or a combination of the two.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.8726797997951508}]}, {"text": "Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.7533257901668549}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9342167973518372}, {"text": "Penn Treebank data", "start_pos": 90, "end_pos": 108, "type": "DATASET", "confidence": 0.9934513370196024}]}, {"text": "The texts in Treebank represent the general English domain.", "labels": [], "entities": []}, {"text": "It is not entirely clear how representative the general English language vocabulary and structure are of a specialized subdomain such as clinical reports.", "labels": [], "entities": []}, {"text": "A well-recognized problem is that the accuracy of all current POS taggers drops dramatically on unknown words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9994032382965088}, {"text": "POS taggers", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.843461662530899}]}, {"text": "For example, while the TnT tagger performs at 97% accuracy on known words in the Treebank, the accuracy drops to 89% on unknown words).", "labels": [], "entities": [{"text": "TnT tagger", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.6693560183048248}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9967446327209473}, {"text": "Treebank", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.8718881607055664}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9992474317550659}]}, {"text": "The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word \"guesser\").", "labels": [], "entities": [{"text": "LT POS tagger", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.5970464746157328}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9952273368835449}]}, {"text": "The overall results for both of these taggers are much closer to the high end of the spectrum because the rate of the unknown words in the tests performed on the Penn Treebank corpus is generally relatively low -2.9%.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 162, "end_pos": 182, "type": "DATASET", "confidence": 0.9959725141525269}]}, {"text": "From these results, we can conclude that the higher the rate of unknown vocabulary, the lower the overall accuracy will be, necessitating the adaptation of the taggers trained on Penn Treebank to sublanguage domains with vocabulary that is substantially different from the one represented by the Penn Treebank corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9993754029273987}, {"text": "Penn Treebank", "start_pos": 179, "end_pos": 192, "type": "DATASET", "confidence": 0.9951274991035461}, {"text": "Penn Treebank corpus", "start_pos": 296, "end_pos": 316, "type": "DATASET", "confidence": 0.9960086146990458}]}, {"text": "Based on the observable differences between the clinical and the general English discourse and POS tagging accuracy results on unknown vocabulary, it is reasonable to assume that a tagger trained on general English may not perform as well on clinical notes, where the percentage of unknown words will increase.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.6546550542116165}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.8381178379058838}]}, {"text": "To test this assumption, a \"gold standard\" corpus of clinical notes needs to be manually annotated for POS information.", "labels": [], "entities": [{"text": "POS information", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.6822883784770966}]}, {"text": "The issues with the annotation process constitute the primary focus of this paper.", "labels": [], "entities": []}, {"text": "We describe an effort to train three medical coding experts to mark the text of clinical notes for part-of-speech information.", "labels": [], "entities": []}, {"text": "The motivation for using medical coders rather than trained linguists is threefold.", "labels": [], "entities": []}, {"text": "First of all, due to confidentiality restrictions, in order to develop a corpus of hand labeled data from clinical notes one can only use personnel authorized to access patient information.", "labels": [], "entities": []}, {"text": "The only way to avoid it, is to anonymize the notes prior to POS tagging which in itself is a difficult and expensive process.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7219108939170837}]}, {"text": "Second, medical coding experts are well familiar with clinical discourse, which helps especially with annotating medicine specific vocabulary.", "labels": [], "entities": []}, {"text": "Third, the fact that POS tagging can be viewed as a classification task makes the medical coding experts highly suitable because their primary occupation and expertise is in classifying patient records for subsequent retrieval.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.8872489631175995}]}, {"text": "We show that, given a good set of guidelines, medical coding experts can be trained in a limited amount of time to perform a linguistic task such as POS annotation at a high level of agreement on both clinical notes and Penn Treebank data.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 220, "end_pos": 238, "type": "DATASET", "confidence": 0.9914037783940634}]}, {"text": "Finally, we report on a set of training experiments performed with the TnT tagger) using the Penn Treebank as well as the newly developed medical corpus..", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9963634312152863}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Annotator agreement results based on 5  clinical notes", "labels": [], "entities": []}, {"text": " Table 3 Syntactic category distribution in the  corpus of clinical notes.", "labels": [], "entities": []}, {"text": " Table 4 Correctness results for the Treebank  model.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9812668561935425}]}]}