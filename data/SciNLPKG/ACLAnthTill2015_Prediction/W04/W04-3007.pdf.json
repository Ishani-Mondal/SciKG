{"title": [{"text": "Robustness Issues in a Data-Driven Spoken Language Understanding System", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6433228154977163}]}], "abstractContent": [{"text": "Robustness is a key requirement in spoken language understanding (SLU) systems.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.8196304142475128}]}, {"text": "Human speech is often ungrammatical and ill-formed, and there will frequently be a mismatch between training and test data.", "labels": [], "entities": []}, {"text": "This paper discusses robustness and adaptation issues in a statistically-based SLU system which is entirely data-driven.", "labels": [], "entities": []}, {"text": "To test robustness, the system has been tested on data from the Air Travel Information Service (ATIS) domain which has been artificially corrupted with varying levels of additive noise.", "labels": [], "entities": [{"text": "Air Travel Information Service (ATIS) domain", "start_pos": 64, "end_pos": 108, "type": "DATASET", "confidence": 0.932836577296257}]}, {"text": "Although the speech recognition performance degraded steadily, the system did not fail catastrophically.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8452599048614502}]}, {"text": "Indeed, the rate at which the end-to-end performance of the complete system degraded was significantly slower than that of the actual recognition component.", "labels": [], "entities": []}, {"text": "Ina second set of experiments, the ability to rapidly adapt the core understanding component of the system to a different application within the same broad domain has been tested.", "labels": [], "entities": []}, {"text": "Using only a small amount of training data, experiments have shown that a semantic parser based on the Hidden Vector State (HVS) model originally trained on the ATIS corpus can be straightforwardly adapted to the somewhat different DARPA Communicator task using standard adaptation algorithms.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 161, "end_pos": 172, "type": "DATASET", "confidence": 0.9393920600414276}]}, {"text": "The paper concludes by suggesting that the results presented provide initial support to the claim that an SLU system which is statistically-based and trained entirely from data is intrinsically robust and can be readily adapted to new applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken language is highly variable as different people use different words and sentence structures to convey the same meaning.", "labels": [], "entities": [{"text": "Spoken language", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8970534205436707}]}, {"text": "Also, many utterances are grammaticallyincorrect or ill-formed.", "labels": [], "entities": []}, {"text": "It thus remains an open issue as to how to provide robustness for large populations of nonexpert users in spoken dialogue systems.", "labels": [], "entities": []}, {"text": "The key component of a spoken language understanding (SLU) system is the semantic parser, which translates the users' utterances into semantic representations.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.8047436773777008}]}, {"text": "Traditionally, most semantic parser systems have been built using hand-crafted semantic grammar rules and so-called robust parsing () is used to handle the ill-formed user input in which word patterns corresponding to semantic tokens are used to fill slots in different semantic frames in parallel.", "labels": [], "entities": []}, {"text": "The frame with the highest score then yields the selected semantic representation.", "labels": [], "entities": []}, {"text": "Formally speaking, the robustness of language (recognition, parsing, etc.) is a measure of the ability of human speakers to communicate despite incomplete information, ambiguity, and the constant element of surprise.", "labels": [], "entities": [{"text": "recognition, parsing", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.605453352133433}]}, {"text": "In this paper, two aspects of SLU system performance are investigated: noise robustness and adaptability to different applications.", "labels": [], "entities": [{"text": "SLU", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9648100137710571}]}, {"text": "For the former, we expect that an SLU system should maintain acceptable performance when given noisy input speech data.", "labels": [], "entities": []}, {"text": "This requires, the understanding components of the SLU system to be able to correctly interpret the meaning of an utterance even when faced with recognition errors.", "labels": [], "entities": []}, {"text": "For the latter, the SLU system should be readily adaptable to a different application using a relatively small set (e.g. less than 100) of adaptation utterances.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "An overview of our data-driven SLU system is outlined in section 2.", "labels": [], "entities": []}, {"text": "Experimental results on performance under a range of SNRs are then presented in section 3.", "labels": [], "entities": [{"text": "SNRs", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9240373373031616}]}, {"text": "Section 4 discusses adaptation of the HVS model to new applications.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental setup used to evaluate the SLU system was similar to that described in).", "labels": [], "entities": []}, {"text": "As mentioned in section 2, the SLU system consists of three main components, a standard HTK-based HMM recognizer, the HVS semantic parser, and the TAN dialogue act (DA) decoder.", "labels": [], "entities": [{"text": "HTK-based HMM recognizer", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7069165110588074}, {"text": "HVS semantic parser", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.6473911305268606}]}, {"text": "Each of the three major components are trained separately.", "labels": [], "entities": []}, {"text": "The acoustic speech signal in the ATIS training data is modelled by extracting 39 features every 10ms: 12 cepstra, energy, and their first and second derivatives.", "labels": [], "entities": [{"text": "ATIS training data", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.8883551359176636}]}, {"text": "This data is then used to train the speaker-independent, continuous speech recognizer.", "labels": [], "entities": [{"text": "continuous speech recognizer", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.688730259736379}]}, {"text": "The HVS semantic parser is trained on the unannotated utterances using EM constrained by the domain-specific lexical class information and the dominance relations built into the abstract annotations ().", "labels": [], "entities": [{"text": "HVS semantic parser", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6429082651933035}]}, {"text": "In the case of ATIS, the lexical classes can be extracted automatically from the relational database, whilst abstract semantic annotations for each utterance are automatically derived from the accompanying SQL queries of the training utterances.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8227826356887817}]}, {"text": "The dialogue act decoder is trained using the main topics or goals and the key semantic concepts extracted automatically from the reference SQL queries Performance is measured at both the component and the system level.", "labels": [], "entities": []}, {"text": "For the former, the recognizer is evaluated byword error rate, the parser by concept slot retrieval rate using an F-measure metric, and the dialog act decoder by detection rate.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.947096049785614}]}, {"text": "The overall system performance is measured using the standard NIST \"query answer\" rate.", "labels": [], "entities": [{"text": "NIST \"query answer\" rate", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.6220388313134512}]}, {"text": "In the expriments reported here, car noise from the NOISEX-92 () database was added to the ATIS-3 NOV93 and DEC94 test sets.", "labels": [], "entities": [{"text": "NOISEX-92 () database", "start_pos": 52, "end_pos": 73, "type": "DATASET", "confidence": 0.8968486785888672}, {"text": "ATIS-3 NOV93", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.7782030403614044}, {"text": "DEC94 test sets", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.8863644003868103}]}, {"text": "In order to obtain different SNRs, the noise was scaled accordingly before adding to the speech signal.", "labels": [], "entities": [{"text": "SNRs", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.9050928950309753}]}, {"text": "Robust spoken language understanding components should be able to compensate for the weakness of the speech recognizer.", "labels": [], "entities": [{"text": "Robust spoken language understanding", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6687798202037811}]}, {"text": "That is, ideally they should be capable of generating the correct meaning of an utterance even if it is recognized wrongly by a speech recognizer.", "labels": [], "entities": []}, {"text": "At minimum, the performance of the understanding components should degrade gracefully as recognition accuracy degrades.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9694371223449707}]}, {"text": "gives the system performance on the corrupted test data with additive noise ranging from 25dB to 10dB SNR.", "labels": [], "entities": [{"text": "SNR", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9762770533561707}]}, {"text": "The label \"clean\" in the X-axis denotes the original clean speech data without additive noise.", "labels": [], "entities": []}, {"text": "Note that the recognition results on the corrupted test data were obtained directly using the original clean speech HMM models without retraining for the noisy conditions.", "labels": [], "entities": []}, {"text": "The upper portion of shows the end-toend performance in terms of query answer error rate for the NOV93 and DEC94 test sets.", "labels": [], "entities": [{"text": "query answer error rate", "start_pos": 65, "end_pos": 88, "type": "METRIC", "confidence": 0.6082483232021332}, {"text": "NOV93", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9847250580787659}, {"text": "DEC94 test sets", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.9455779592196146}]}, {"text": "For easy reference, WER is also shown.", "labels": [], "entities": [{"text": "WER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9959598183631897}]}, {"text": "The individual component performance, F-measure for the HVS semantic parser and dialogue act (DA) detection accuracy for the DA decoder, are illustrated in the lower portion of.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9978011250495911}, {"text": "HVS semantic parser", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.6203014751275381}, {"text": "dialogue act (DA) detection", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.5743906497955322}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.6505551934242249}]}, {"text": "For each test set, the performance on the rescored word hypotheses is given as well.", "labels": [], "entities": []}, {"text": "This incorporates the semantic parse scores into the acoustic and language modelling likelihoods to rescore the 25-best word lists from the speech recognizer.", "labels": [], "entities": [{"text": "semantic parse", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.6901272088289261}]}, {"text": "It can be observed that the system gives fairly stable performance at high SNRs and then the recognition accuracy degrades rapidly in the presence of increasing noise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9407932162284851}]}, {"text": "At 20dB SNR, the WER for the NOV93 test set increases by 1.6 times relative to clean whilst the query answer error rate increases by only 1.3 times.", "labels": [], "entities": [{"text": "SNR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9360914826393127}, {"text": "WER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9984090924263}, {"text": "NOV93 test set", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9852576653162638}, {"text": "query answer error rate", "start_pos": 96, "end_pos": 119, "type": "METRIC", "confidence": 0.6230849847197533}]}, {"text": "On decreasing the SNR to 15dB, the system performance degrades significantly.", "labels": [], "entities": [{"text": "SNR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.7615554332733154}]}, {"text": "The WER increases by 3.1 times relative to clean but the query answer error rate increases by only 1.7 times.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9970580339431763}, {"text": "query answer error rate", "start_pos": 57, "end_pos": 80, "type": "METRIC", "confidence": 0.5860923081636429}]}, {"text": "Similar figures were obtained for the DEC94 test set.", "labels": [], "entities": [{"text": "DEC94 test set", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.9712604880332947}]}, {"text": "The above suggests that the end-to-end performance measured in terms of answer error rate degrades more slowly compared to the recognizer WER as the noise level increases.", "labels": [], "entities": [{"text": "answer error rate", "start_pos": 72, "end_pos": 89, "type": "METRIC", "confidence": 0.7479405601819357}, {"text": "recognizer WER", "start_pos": 127, "end_pos": 141, "type": "TASK", "confidence": 0.5848978608846664}]}, {"text": "This demonstrates that the statisticallybased understanding components of the SLU system, the semantic parser and the dialogue act decoder, are relatively robust to degrading recognition performance.", "labels": [], "entities": []}, {"text": "Regarding the individual component performance, the dialogue act detection accuracy appears to be less sensitive to decreasing SNR.", "labels": [], "entities": [{"text": "dialogue act detection", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7800285816192627}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.7565211653709412}, {"text": "SNR", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.7187609076499939}]}, {"text": "This is probably a consequence of the fact that the Bayesian networks are setup to respond to only the presence or absence of semantic concepts or slots, regardless of the actual values assigned to them.", "labels": [], "entities": []}, {"text": "In another words, the performance of the dialogue act decoder is not affected by the mis-recognition of individual words, but only by a failure to detect the presence of a semantic concept.", "labels": [], "entities": []}, {"text": "It can also be observed from that the F-measure needs to be better than 85% in order to achieve acceptable end-to-end performance.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9978317618370056}]}, {"text": "To test the portability of the statistical parser, the initial experiments reported here are focussed on assessing the adaptability of the HVS model when it is tested in a domain which covers broadly similar concepts, but comprises rather different speaking styles.", "labels": [], "entities": []}, {"text": "To this end, the flight information subset of the DARPA Communicator Travel task has been used as the target domain).", "labels": [], "entities": [{"text": "DARPA Communicator Travel task", "start_pos": 50, "end_pos": 80, "type": "DATASET", "confidence": 0.6638840436935425}]}, {"text": "By limiting the test in this way, we ensure that the dimensionalities of the HVS model parameters remain the same and no new semantic concepts are introduced by the adaptation training data.", "labels": [], "entities": []}, {"text": "The baseline HVS parser was trained on the ATIS corpus using 4978 utterances selected from the contextindependent (Class A) training data in the ATIS-2 and ATIS-3 corpora.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9847451150417328}, {"text": "ATIS-2", "start_pos": 145, "end_pos": 151, "type": "DATASET", "confidence": 0.944075882434845}, {"text": "ATIS-3 corpora", "start_pos": 156, "end_pos": 170, "type": "DATASET", "confidence": 0.8530733585357666}]}, {"text": "The vocabulary size of the ATIS training corpus is 611 and there are altogether 110 semantic concepts defined.", "labels": [], "entities": [{"text": "ATIS training corpus", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.6688056786855062}]}, {"text": "The parser model was then adapted using utterances relating to flight reservation from the DARPA Communicator data.", "labels": [], "entities": [{"text": "flight reservation", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.6763251572847366}, {"text": "DARPA Communicator data", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.9473806222279867}]}, {"text": "Although the latter bears similarities to the ATIS data, it contains utterances of a different style and is often more complex.", "labels": [], "entities": [{"text": "ATIS data", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.9751023650169373}]}, {"text": "For example, Communicator contains utterances on multiple flight legs, information which is not available in ATIS.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.95107102394104}]}, {"text": "To compare the adapted ATIS parser with an in-domain Communicator parser, a HVS model was trained from scratch using 10682 Communicator training utterances.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.8032267689704895}]}, {"text": "The vocabulary size of the in-domain Communicator training data is 505 and a total of 99 semantic concepts have been defined.", "labels": [], "entities": []}, {"text": "For all tests, a set of 1017 Communicator test utterances was used.", "labels": [], "entities": []}, {"text": "lists the recall, precision, and F-measure results obtained when tested on the 1017 utterance DARPA Communicator test set.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9995068311691284}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9991815686225891}, {"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.999349057674408}, {"text": "1017 utterance DARPA Communicator test set", "start_pos": 79, "end_pos": 121, "type": "DATASET", "confidence": 0.6655910015106201}]}, {"text": "The baseline is the unadapted HVS parser trained on the ATIS corpus only.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9690086543560028}]}, {"text": "The indomain results are obtained using the HVS parser trained solely on the 10682 DARPA training data.", "labels": [], "entities": [{"text": "DARPA training data", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.86404021581014}]}, {"text": "The other rows of the table give the parser performance using MAP and log-linear interpolation based adaptation of the baseline model using 50 randomly selected adaptation utterances.", "labels": [], "entities": [{"text": "MAP", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.5363290309906006}]}], "tableCaptions": [{"text": " Table 1: Performance comparison of adaptation using  MAP or log-linear interpolation.", "labels": [], "entities": []}]}