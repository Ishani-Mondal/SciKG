{"title": [{"text": "A Phrase-Based HMM Approach to Document/Abstract Alignment", "labels": [], "entities": [{"text": "Document/Abstract Alignment", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.667658656835556}]}], "abstractContent": [{"text": "We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts.", "labels": [], "entities": []}, {"text": "Such alignments are critical for the development of statistical sum-marization systems that can be trained on large corpora of document/abstract pairs.", "labels": [], "entities": []}, {"text": "Our model, which is based on a novel Phrase-Based HMM, outper-forms both the Cut & Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7811547815799713}]}], "introductionContent": [{"text": "There area wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts.", "labels": [], "entities": []}, {"text": "Detailed studies of such pairs show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing.", "labels": [], "entities": [{"text": "summarizing texts", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8987424969673157}]}, {"text": "Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations.", "labels": [], "entities": [{"text": "document/abstract alignment", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6610302329063416}]}, {"text": "To get around directly tackling this problem, researchers in text summarization have employed one of several techniques.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7929049134254456}]}, {"text": "Some researchers () have developed simple statistical models for aligning documents and headlines.", "labels": [], "entities": [{"text": "aligning documents and headlines", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.7736974060535431}]}, {"text": "These models, which implement IBM Model 1 (, treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines.", "labels": [], "entities": []}, {"text": "As our results show, these models are too weak for capturing the operations that are employed by humans in summarizing texts beyond the headline level.", "labels": [], "entities": [{"text": "summarizing texts", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.9293032288551331}]}, {"text": "Other researchers have developed models that make unreasonable assumptions about the data, which lead to the utilization of a very small percent of available data.", "labels": [], "entities": []}, {"text": "For instance, the document and sentence compression models of) assume that sentences/documents can be summarized only through deletion of contiguous text segments.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7422710359096527}]}, {"text": "Knight and Marcu found that from a corpus of 39, 060 abstract sentences, only 1067 sentence extracts existed: a recall of only 2.7%.", "labels": [], "entities": [{"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.999303936958313}]}, {"text": "An alternate techinque employed in a large variety of systems is to treat the summarization problem as a sentence extraction problem.", "labels": [], "entities": [{"text": "summarization problem", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.9347126483917236}, {"text": "sentence extraction problem", "start_pos": 105, "end_pos": 132, "type": "TASK", "confidence": 0.7986269195874532}]}, {"text": "Such systems can be trained either on human constructed extracts or extracts generated automatically from document/abstract pairs (see) for two such approaches).", "labels": [], "entities": []}, {"text": "None of these techniques is adequate.", "labels": [], "entities": []}, {"text": "Even fora relatively simple sentence from an abstract, we can see that none of the assumptions listed above holds.", "labels": [], "entities": []}, {"text": "In, we observe several phenomena: \u2022 Alignments can occur at the granularity of words and at the granularity of phrases.", "labels": [], "entities": []}, {"text": "\u2022 The ordering of phrases in an abstract can be different from the ordering in the document.", "labels": [], "entities": []}, {"text": "\u2022 Some abstract words do not have direct correspondents in the document, and some document words are never used.", "labels": [], "entities": []}, {"text": "It is thus desirable to be able to automatically construct alignments between documents and their abstracts, so that the correspondences between the pairs are obvious.", "labels": [], "entities": []}, {"text": "One might be initially tempted to use readily-available machine translation systems like GIZA++ to perform such  alignments.", "labels": [], "entities": []}, {"text": "However, as we will show, the alignments produced by such a system are inadequate for this task.", "labels": [], "entities": []}, {"text": "The solution that we propose to this problem is an alignment model based on a novel mathematical structure we call the Phrase-Based HMM.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe an intrinsic evaluation of the PBHMM document/abstract alignment model.", "labels": [], "entities": [{"text": "PBHMM document/abstract alignment", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.7382028937339783}]}, {"text": "All experiments in this paper are done on the ZiffDavis corpus (statistics are in).", "labels": [], "entities": [{"text": "ZiffDavis corpus", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.816038578748703}]}, {"text": "In order to judge the quality of the alignments produced by a system, we first need to create a set of \"gold standard\" alignments.", "labels": [], "entities": []}, {"text": "Two human annotators manually constructed such alignments between documents and their abstracts.", "labels": [], "entities": []}, {"text": "Software for assisting this process was developed and is made freely available.", "labels": [], "entities": []}, {"text": "An annotation guide, which explains in detail the document/abstract alignment process was also prepared and is freely available.", "labels": [], "entities": [{"text": "document/abstract alignment", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6550653055310249}]}, {"text": "In order to establish a baseline alignment model, we used the IBM Model 4 () and the HMM model as implemented in the GIZA++ package.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.9000757137934366}, {"text": "GIZA++ package", "start_pos": 117, "end_pos": 131, "type": "DATASET", "confidence": 0.8617432514826456}]}, {"text": "We modified this slightly to allow longer inputs and higher fertilities.", "labels": [], "entities": []}, {"text": "Such translation models require that input be in sentence-aligned form.", "labels": [], "entities": []}, {"text": "In the summarization task, however, one abstract sentence often corresponds to multiple document sentences.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.9222531020641327}]}, {"text": "In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by.", "labels": [], "entities": []}, {"text": "In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human.", "labels": [], "entities": []}, {"text": "Each pair was ranked as 0 (document sentences contain little-to-none of the information in the abstract sentence), 1 (document sentences contain some of the information in the abstract sentence) or 2 (document sentences contain all of the information).", "labels": [], "entities": []}, {"text": "Of the twenty random examples, none were labeled as 0; five were labeled as 1; and 15 were labeled as 2, giving a mean rating of 1.75.", "labels": [], "entities": []}, {"text": "We ran experiments using the document sentences as both the source and the target language in GIZA++.", "labels": [], "entities": []}, {"text": "When document sentences were used as the target language, each abstract word needed to produce many document words, leading to very high fertilities.", "labels": [], "entities": []}, {"text": "However, since each target word is generated independently, this led to very flat rewrite tables and, hence, to poor results.", "labels": [], "entities": []}, {"text": "Performance increased dramatically by using the document as the source language and the abstract as the target language.", "labels": [], "entities": []}, {"text": "In all MT cases, the corpus was appended with one-word sentence pairs for each word where that word is translated as itself.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9914724230766296}]}, {"text": "In the two basic models, HMM and Model 4, the abstract sentence is the source language and the document sentences are the target language.", "labels": [], "entities": []}, {"text": "To alleviate the fertility problem, we also ran experiments with the translation going in the opposite direction.", "labels": [], "entities": []}, {"text": "These are called HMMflipped and Model 4-flipped, respectively.", "labels": [], "entities": [{"text": "HMMflipped", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.8718003034591675}]}, {"text": "These tend to out-perform the original translation direction.", "labels": [], "entities": []}, {"text": "In all of these setups, 5 iterations of Model 1 were run, followed by 5 iterations of the HMM model.", "labels": [], "entities": []}, {"text": "In the Model 4 cases, 5 iterations of Model 4 were run, following the HMM.", "labels": [], "entities": []}, {"text": "We also tested alignments using the Cut and Paste summary decomposition method), based on a non-trainable HMM.", "labels": [], "entities": []}, {"text": "Briefly, the Cut and Paste HMM searches for long contiguous blocks of words in the document and abstract that are identical (up to stem).", "labels": [], "entities": []}, {"text": "The longest such sequences are aligned.", "labels": [], "entities": []}, {"text": "By fixing a length cutoff of n and ignoring sequences of length less than n, one can arbitrarily increase the precision of this method.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9994598031044006}]}, {"text": "We found that n = 2 yields the best balance between precision and recall (and the highest F-measure).", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.999584972858429}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9989433884620667}, {"text": "F-measure", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9990890026092529}]}, {"text": "The results of these experiments are shown under the header \"Cut & Paste.\"", "labels": [], "entities": []}, {"text": "It clearly outperforms all of the MT-based models.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 34, "end_pos": 42, "type": "TASK", "confidence": 0.686481237411499}]}, {"text": "While the PBHMM is based on a dynamic programming algorithm, the effective search space in this model is enormous, even for moderately sized document/abstract pairs.", "labels": [], "entities": [{"text": "PBHMM", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.9030574560165405}]}, {"text": "We selected the 2000 shortest document/abstract pairs from the Ziff-Davis corpus for training; however, only 12 of the hand-annotated documents were included in this set, so we additionally added the other 33 hand-annotate documents to this set, yielding 2033 document/abstract pairs.", "labels": [], "entities": []}, {"text": "We then performed sentence extraction on this corpus exactly as in the MT case, using the technique of (Marcu, 1999).", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7886243164539337}, {"text": "MT case", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.810148298740387}]}, {"text": "The relevant data for this corpus is in.", "labels": [], "entities": []}, {"text": "We also restrict the state-space with abeam, sized at 50% of the unrestricted state-space.", "labels": [], "entities": []}, {"text": "The PBHMM system was then trained on this abstract/extract corpus.", "labels": [], "entities": [{"text": "PBHMM", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.812447190284729}]}, {"text": "The precision/recall results are shown in  One common precision mistake made by the PBHMM system is to accidentally align words on the summary side to words on the document side, when the summary word should be null-aligned.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991087317466736}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9610007405281067}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9842484593391418}]}, {"text": "The PBHMM O system is an oracle system in which system-produced alignments are removed for summary words that should be null-aligned (according to the hand-annotated data).", "labels": [], "entities": []}, {"text": "Doing this results in a rather significant gain in SoftP score.", "labels": [], "entities": [{"text": "SoftP score", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.5822495073080063}]}, {"text": "As we can see from, none of the machine translation models is well suited to this task, achieving, at best, an F-score of 0.298.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7386420667171478}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9994970560073853}]}, {"text": "The Cut & Paste method performs significantly better, which is to be expected, since it is designed specifically for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.9871683120727539}]}, {"text": "As one would expect, this method achieves higher precision than recall, though not by very much.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9991874098777771}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9989664554595947}]}, {"text": "Our method significantly outperforms both the IBM models and the Cut & Paste method, achieving a precision of 0.456 and a recall nearing 0.7, yielding an overall F-score of 0.548.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.999265730381012}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9995543360710144}, {"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9995319843292236}]}], "tableCaptions": [{"text": " Table 4: Ziff-Davis extract corpus statistics", "labels": [], "entities": []}, {"text": " Table 5: Results on the Ziff-Davis corpus", "labels": [], "entities": [{"text": "the Ziff-Davis corpus", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.7643382052580515}]}]}