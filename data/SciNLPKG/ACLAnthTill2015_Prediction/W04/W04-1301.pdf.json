{"title": [{"text": "A quantitative evaluation of naturalistic models of language acquisition; the efficiency of the Triggering Learning Algorithm compared to a Categorial Grammar Learner", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7026292532682419}]}], "abstractContent": [{"text": "Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7394651174545288}]}, {"text": "The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner's hypothesis space.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.75166916847229}]}, {"text": "This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999).", "labels": [], "entities": []}, {"text": "We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions.", "labels": [], "entities": [{"text": "TLA", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.6775124073028564}]}, {"text": "We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors.", "labels": [], "entities": []}, {"text": "However, the CGL converges more reliably than the TLA.", "labels": [], "entities": []}, {"text": "We discuss the trade-off of efficiency against more reliable convergence to the target grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "A normal child acquires the language of her environment without any specific training.", "labels": [], "entities": []}, {"text": "claims that, given the \"relatively slight exposure\" to examples and \"remarkable complexity\" of language, it would be \"an extraordinary intellectual achievement\" fora child to acquire a language if not specifically designed to do so.", "labels": [], "entities": []}, {"text": "His Argument from the Poverty of the Stimulus suggests that if we know X, and X is undetermined by learning experience then X must be innate.", "labels": [], "entities": []}, {"text": "For an example consider structure dependency in language syntax: A question in English can be formed by inverting the auxiliary verb and subject noun-phrase: (1a) \"Dinah was drinking a saucer of milk\"; (1b) \"was Dinah drinking a saucer of milk?\"", "labels": [], "entities": []}, {"text": "Upon exposure to this example, a child could hypothesize infinitely many question-formation rules, such as: (i) swap the first and second words in the sentence; (ii) front the first auxiliary verb; (iii) front words beginning with w.", "labels": [], "entities": []}, {"text": "The first two of these rules are refuted if the child encounters the following: (2a) \"the cat who was grinning at Alice was disappearing\"; (2b) \"was the cat who was grinning at Alice disappearing?\"", "labels": [], "entities": []}, {"text": "If a child is to converge upon the correct hypothesis unaided she must be exposed to sufficient examples so that all false hypotheses are refuted.", "labels": [], "entities": []}, {"text": "Unfortunately such examples are not readily available in child-directed speech; even the constructions in examples (2a) and (2b) are rare).", "labels": [], "entities": []}, {"text": "To compensate for this lack of data Chomsky suggests that some principles of language are already available in the child's mind.", "labels": [], "entities": []}, {"text": "For example, if the child had innately \"known\" that all grammar rules are structurally-dependent upon syntax she would never have hypothesized rules (i) and (iii).", "labels": [], "entities": []}, {"text": "Thus, Chomsky theorizes that a human mind contains a Universal Grammar which defines a hypothesis-space of \"legal\" grammars.", "labels": [], "entities": []}, {"text": "1 This hypothesis-space must be both large enough to contain grammar's for all of the world's languages and small enough to ensure successful acquisition given the sparsity of data.", "labels": [], "entities": []}, {"text": "Language acquisition is the process of searching the hypothesis-space for the grammar that most closely describes the language of the environment.", "labels": [], "entities": [{"text": "Language acquisition", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.689457356929779}]}, {"text": "With estimates of the number of living languages being around 6800) it is not sensible to model the hypothesis-space of grammars explicitly, rather it must be modeled parametrically.", "labels": [], "entities": []}, {"text": "Language acquisition is then the process of setting these parameters.", "labels": [], "entities": [{"text": "Language acquisition", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7171187996864319}]}, {"text": "suggested that parameters should represent points of variation between languages, however the only requirement for parameters is that they define the current hypothesisspace.", "labels": [], "entities": []}, {"text": "The properties of the parameters used by this learner (the CGL) are as follows: (1) Parameters are lexical; (2) Parameters are inheritance based; (3) Parameter setting is statistical.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}