{"title": [{"text": "A Statistical Model for Hangeul-Hanja Conversion in Terminology Domain", "labels": [], "entities": []}], "abstractContent": [{"text": "Sino-Korean words, which are historically borrowed from Chinese language, could be represented with both Hanja (Chinese characters) and Hangeul (Korean characters) writings.", "labels": [], "entities": []}, {"text": "Previous Korean Input Method Editors (IMEs) provide only a simple dictionary-based approach for Hangeul-Hanja conversion.", "labels": [], "entities": [{"text": "Korean Input Method Editors (IMEs)", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.6005879470280239}, {"text": "Hangeul-Hanja conversion", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.8351863920688629}]}, {"text": "This paper presents a sentence-based statistical model for Hangeul-Hanja conversion, with word tokenization included as a hidden process.", "labels": [], "entities": [{"text": "Hangeul-Hanja conversion", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8380038738250732}, {"text": "word tokenization", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.6958656013011932}]}, {"text": "As a result, we reach 91.4% of character accuracy and 81.4% of word accuracy in terminology domain, when only very limited Hanja data is available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9572688341140747}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.766012966632843}]}], "introductionContent": [{"text": "More than one half of the Korean words are Sino-Korean words.", "labels": [], "entities": []}, {"text": "These words are historically borrowed from Chinese language, could be represented with both Hanja and Hangeul writings.", "labels": [], "entities": []}, {"text": "Hanja writing is rarely used in modern Korean language, but still plays important roles in the word sense disambiguation (WSD) and word origin tracing, especially in the terminology, proper noun and compound noun domain.", "labels": [], "entities": [{"text": "Hanja writing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7287793457508087}, {"text": "word sense disambiguation (WSD)", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.7850209772586823}, {"text": "word origin tracing", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7324226101239523}]}, {"text": "Automatic Hangeul-Hanja conversion is very difficult for system because of several reasons.", "labels": [], "entities": [{"text": "Hangeul-Hanja conversion", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8903391063213348}]}, {"text": "There are 473 Hangeul characters (syllables) have Hanja correspondences, map to 4888 common Hanja characters.", "labels": [], "entities": []}, {"text": "Each of these Hangeul characters could correspond to from one to sixty-four Hanja characters, so it is difficult to system to select the correct Hanja correspondence.", "labels": [], "entities": []}, {"text": "Besides that, the sino-Korean Hangeul characters/words could be also native Korean characters/words according to their meaning.", "labels": [], "entities": []}, {"text": "For example, \" \" writing formats.", "labels": [], "entities": []}, {"text": "It means a compound word tokenization should be included as a preprocessing in Hangeul-Hanja conversion.", "labels": [], "entities": [{"text": "Hangeul-Hanja conversion", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7009122669696808}]}, {"text": "Automatic Hangeul-Hanja conversion also suffers from another problem, that there are no enough Hanja corpora for statistical approach.", "labels": [], "entities": [{"text": "Hangeul-Hanja conversion", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8375404179096222}]}, {"text": "In modern Korean language, only few sino-Korean words are written in Hanja writing generally, and the same sino-Korean word with the same meaning could be in either Hangeul or Hanja writing even in the same text.", "labels": [], "entities": []}, {"text": "This paper presents a sentence-based statistical model for Hangeul-Hanja conversion.", "labels": [], "entities": [{"text": "Hangeul-Hanja conversion", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8266220092773438}]}, {"text": "The model includes a transfer model (TM) and a language model (LM), in which word tokenization is included as a hidden process for compound word tokenization.", "labels": [], "entities": []}, {"text": "To find answer for the issues like adapt the model to character or word level, or limit the conversion target to only noun or expand it to other Part of Speech (POS) tags, a series of experiments has been performed.", "labels": [], "entities": []}, {"text": "As a result, our system shows significant better result with only very limited Hanja data, when we compare it to the dictionary-based conversion approach used in commercial products.", "labels": [], "entities": [{"text": "dictionary-based conversion", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.6267490088939667}]}, {"text": "In the following of this paper: Section 2 discusses related works.", "labels": [], "entities": []}, {"text": "Section 3 describes our model.", "labels": [], "entities": []}, {"text": "Section 4 discusses several factors considered in the model implementation and experiment design.", "labels": [], "entities": []}, {"text": "Section 5 gives the evaluation approaches and a series of experiment results.", "labels": [], "entities": []}, {"text": "Section 6 presents our conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "This chapter shows the experiments on the model in equation 3 and some different implementations we have discussed above.", "labels": [], "entities": []}, {"text": "There are two parts in the experiments, first one is mostly related to word level model implementation, in which the basic issues like language resource utilization and POS tag restriction, and some word level related issues like bigram or unigram for LM in word level are tested.", "labels": [], "entities": [{"text": "word level model implementation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6239228025078773}, {"text": "POS tag restriction", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.6075136760870615}]}, {"text": "The second part is mostly character level related.", "labels": [], "entities": []}, {"text": "Several evaluation standards are employed in the experiments.", "labels": [], "entities": []}, {"text": "The adopted standards and evaluation approaches are reported in the first section of the experiments.", "labels": [], "entities": []}, {"text": "We use several evaluation standards in the experiments.", "labels": [], "entities": []}, {"text": "To reflect the readability from the user viewpoint, we adopt word and phrase (sentence) level accuracy, precision and recall; to compare the automatic conversion result with the standard result -from the developer viewpoint, Dice-coefficient based similarity calculation is employed also; to compare with previous Chinese Pinyin input method, a character based accuracy evaluation is also adopted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9729070663452148}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9995841383934021}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9994063377380371}, {"text": "accuracy", "start_pos": 361, "end_pos": 369, "type": "METRIC", "confidence": 0.9029500484466553}]}, {"text": "An automatic evaluation and analysis system is developed to support large scale experiments.", "labels": [], "entities": []}, {"text": "The system compares the automatic result to the standard one, and performs detailed error analysis using a decision tree.", "labels": [], "entities": []}, {"text": "In this part, the basic issues like language resource utilization and POS tag restriction, and the word level related issues, like bigram or unigram for LM are performed.", "labels": [], "entities": [{"text": "POS tag restriction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7014581561088562}]}, {"text": "The objects of the first experiment are, firstly, compare a simple LM based statistical approach with the baseline -dictionary based approach; secondly, see if large dictionary is better than small dictionary in dictionary based conversion; thirdly, see if Chinese corpus does help to the HangeulHanja conversion.", "labels": [], "entities": []}, {"text": "A small dictionary based conversion (Dic), large dictionary based conversion (BigDic), a unigram (Unigram) and a bigram based (Bigram) word level conversion, are performed to compared to the each other.", "labels": [], "entities": []}, {"text": "The small dictionary Dic has 56,000 HangeulHanja entries; while the large dictionary BigDic contains 280,000 Hangeul-Hanja entries.", "labels": [], "entities": [{"text": "HangeulHanja", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.8795692920684814}, {"text": "BigDic", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9390248656272888}]}, {"text": "The unigram and bigram are extracted from Chinese data C.", "labels": [], "entities": [{"text": "Chinese data C", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8099075754483541}]}, {"text": "The test set is a small test set with 90 terms (180 content words) from terminology domain.", "labels": [], "entities": []}, {"text": "Word level precision and recall with F1-measure are employed as evaluation standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.8789138793945312}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9993948936462402}, {"text": "F1-measure", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9985516667366028}]}, {"text": "From the result shows in table 1, we can get the conclusions that, 1) compare to the small dictionary, large dictionary reaches better F1-measure because of the enhancement in recall, although the precision is slightly low downed because of more Hanja candidates forgiven Hangeul entry; 2) Statistical approach shows obvious better result than the dictionary based approach, although it is only a very simple LM; 3) Chinese data does help to the Hangeul-Hanja conversion.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9992913007736206}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9987195730209351}, {"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9989435076713562}]}, {"text": "We have to evaluation its impact by comparing it with other Hanja data in further experiments.", "labels": [], "entities": [{"text": "Hanja data", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.8465903699398041}]}, {"text": "4) Bigram shows similar result with unigram in word level conversion, it shows that data sparseness problem is still very serious.", "labels": [], "entities": [{"text": "word level conversion", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6582333544890085}]}, {"text": "In the character level experiments, first, we compare the character level model with baseline dictionary based approach; Second, compare the character level model with the word level model; Third, to find out the best TM weight for the character level model.", "labels": [], "entities": []}, {"text": "This part of experiments uses anew test set, which has 1,000 terms in it (2,727 content words; 3.9 Hanja candidates per sino-Korean word in average).", "labels": [], "entities": []}, {"text": "The user data U has 12,000 HangeulHanja term pairs in it.", "labels": [], "entities": [{"text": "HangeulHanja", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9454742670059204}]}, {"text": "U is from the same domain of the test set (computer science and electronic engineering domain at here), but there is no overlap with the test set (so it is a opened test).", "labels": [], "entities": []}, {"text": "Several different evaluation standards are employed.", "labels": [], "entities": []}, {"text": "As the first column of table 4, \"CA\", \"WA\" and \"SA\" mean character, word, sentence (terms) accuracy, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.974023163318634}]}, {"text": "\"Sim\" is the similarity based evaluation, and F1 is the value of word level F1-measure which is from word precision/recall evaluation.", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9991726279258728}, {"text": "word level F1-measure", "start_pos": 65, "end_pos": 86, "type": "METRIC", "confidence": 0.6673481861750284}]}, {"text": "The first row of table 4 shows the HangeulHanja conversion approach with the employed data and TM weight \u03b1.", "labels": [], "entities": [{"text": "HangeulHanja", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8889889121055603}, {"text": "TM weight \u03b1", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.8215214014053345}]}, {"text": "\"Dic\" is the baseline dictionary based approach; \"w\" means word level model; \"D\" means dictionary data (extracted from the large dictionary with 400,000 Hangeul-Hangeul and Hangeul-Hanja entries), U means user data described above, C means Chinese data.", "labels": [], "entities": [{"text": "Hangeul-Hangeul", "start_pos": 153, "end_pos": 168, "type": "DATASET", "confidence": 0.9587754011154175}]}, {"text": "The digital value like \".5\" is TM weight.", "labels": [], "entities": [{"text": "TM weight", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9272251129150391}]}, {"text": "So, as an example, \"wDUC1\" means word model with \u03b1=1 and using all data resources D, U and C; \"DU.2\" means character model with \u03b1=0.2 and using data D and U.", "labels": [], "entities": []}, {"text": "From the table 4, we can get the conclusions that, 1) all statistical model based approaches shows obviously better performance than the baseline dictionary based approach \"Dic\" (Dic others).", "labels": [], "entities": []}, {"text": "2) In most cases, character models show better results than word model).", "labels": [], "entities": []}, {"text": "But when there is no user data, word mode is better than character model (wD1 D.5).", "labels": [], "entities": []}, {"text": "3) Among character models, the TM with \u03b1=1 shows the best result (\"DU1\" \"DU.x\").", "labels": [], "entities": []}, {"text": "4) User data has positive impact on the performance (\"Dw1 DUCw1\", \"D.5 DU.5\"), and it is especially important to the character model (\"D.5 DU.5\").", "labels": [], "entities": [{"text": "Dw1", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.6205138564109802}, {"text": "DUCw1", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.42688044905662537}]}, {"text": "It is because character model may cause more noise because of word tokenization error when there is no user data.", "labels": [], "entities": [{"text": "word tokenization", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.6497475653886795}]}, {"text": "From the table 4, we can seethe best result is gotten from character based TM with using dictionary and user data D, U (\"DU1\").", "labels": [], "entities": []}, {"text": "The best character accuracy is 91.4%, when the word accuracy is 81.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9739698767662048}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8013237714767456}]}, {"text": "The character accuracy is lower than the typing and language model based Chinese Pinyin IME, which was 95% in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9918248057365417}]}, {"text": "But consider that in our experiment, there is almost no Hanja data except dictionary, and also consider the extra difficulty from terminology domain, this comparision result is quite understandable.", "labels": [], "entities": []}, {"text": "Our experiment also shows that, compare to using only LM like it in, TM shows significantly better result in character accuracy (from 81.0% to 91.4% in our experiment: \"DU0\" \"DU1\", table 4).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9659888744354248}]}, {"text": "Our user evaluation also shows that, to the terminology domain, the automatic conversion result from the system shows even better quality than the draft result from untrained human translator.", "labels": [], "entities": []}, {"text": "shows the trends of different evaluation standards in the same experiment shown in table 4.", "labels": [], "entities": []}, {"text": "We can see character accuracy \"CA\" shows similar trend with similarity based standard \"Sim\", while word accuracy \"WA\" and sentence (terms) accuracy \"SA\" show similar trends with F1-measure \"F1\", in which \"F1\"is based on word precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.793982744216919}, {"text": "word accuracy \"WA\"", "start_pos": 99, "end_pos": 117, "type": "METRIC", "confidence": 0.7726932168006897}, {"text": "F1-measure \"F1", "start_pos": 178, "end_pos": 192, "type": "METRIC", "confidence": 0.742369016011556}, {"text": "F1", "start_pos": 205, "end_pos": 207, "type": "METRIC", "confidence": 0.9865812063217163}, {"text": "recall", "start_pos": 239, "end_pos": 245, "type": "METRIC", "confidence": 0.9794741868972778}]}, {"text": "From the user viewpoint, word/sentence accuracy and F1-measure reflects readability better than character accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9574270248413086}, {"text": "F1-measure", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9988908171653748}]}, {"text": "It is because, if there is a character wrongly converted in a word, it affects the readability of whole word but not only that character's.", "labels": [], "entities": []}, {"text": "However, character accuracy is more important to the system evaluation, especially for the character level model implementation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9870949387550354}]}, {"text": "It is because the character accuracy can reflect the system performance in full detail than the word or sentence (term) based one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9810968041419983}]}], "tableCaptions": [{"text": " Table 1. Base line (small dic vs. large dic) vs.  Statistical approach (unigram vs. bigram)", "labels": [], "entities": []}, {"text": " Table 2. POS tag constraint and language resource  evaluation", "labels": [], "entities": [{"text": "POS tag constraint", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6373744408289591}, {"text": "language resource  evaluation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.6138547559579214}]}, {"text": " Table 3. TM weight in word model", "labels": [], "entities": []}, {"text": " Table 4: Character level model vs. word level  model vs. base line (dictionary based approach)", "labels": [], "entities": []}]}