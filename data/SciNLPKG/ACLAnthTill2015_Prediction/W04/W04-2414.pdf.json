{"title": [{"text": "Memory-based semantic role labeling: Optimizing features, algorithm, and output", "labels": [], "entities": [{"text": "Memory-based semantic role labeling", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5914976820349693}]}], "abstractContent": [], "introductionContent": [{"text": "In this paper we interpret the semantic role labeling problem as a classification task, and apply memory-based learning to it in an approach similar to and for grammatical relation labeling.", "labels": [], "entities": [{"text": "semantic role labeling problem", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.6933507174253464}, {"text": "grammatical relation labeling", "start_pos": 160, "end_pos": 189, "type": "TASK", "confidence": 0.6196768979231516}]}, {"text": "We apply feature selection and algorithm parameter optimization strategies to our learner.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7358668744564056}, {"text": "algorithm parameter optimization", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.6632934212684631}]}, {"text": "In addition, we investigate the effect of two innovations: (i) the use of sequences of classes as classification output, combined with a simple voting mechanism, and (ii) the use of iterative classifier stacking which takes as input the original features and a pattern of outputs of a first-stage classifier.", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.7259714603424072}]}, {"text": "Our claim is that both methods avoid errors in sequences of predictions typically made by simple classifiers that are unaware of their previous or subsequent decisions in a sequence.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Features used in the different runs mentioned  in Table 1. The numbers mentioned for words, part-of- speech tags, chunk tags, named entity tags and output  classes show the position of the tokens with respect to  the focus token (0). Distances are measured in chunks,  NP chunks, VP chunks and words. In all other table en- tries, + denotes selection and -omission.", "labels": [], "entities": []}, {"text": " Table 3: Parameters of the machines learner that were  used in the different runs mentioned in Table 1. More  information about the parameters and their values can be  found in Daelemans et al. (2003).", "labels": [], "entities": []}, {"text": " Table 4. T. M. Cover and P. E. Hart. 1967. Nearest neighbor  pattern classification. Institute of Electrical and Elec-", "labels": [], "entities": [{"text": "1967. Nearest neighbor  pattern classification", "start_pos": 38, "end_pos": 84, "type": "TASK", "confidence": 0.5909030735492706}]}]}