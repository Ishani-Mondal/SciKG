{"title": [{"text": "ROUGE: A Package for Automatic Evaluation of Summaries", "labels": [], "entities": [{"text": "Evaluation of Summaries", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7378549774487814}]}], "abstractContent": [{"text": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9742845892906189}]}, {"text": "It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.", "labels": [], "entities": []}, {"text": "The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally evaluation of summarization i nvolves human judgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content).", "labels": [], "entities": []}, {"text": "However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) would require over 3,000 hours of human efforts.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 140, "end_pos": 179, "type": "TASK", "confidence": 0.6649602899948756}]}, {"text": "This is very expensive and difficult to conduct in a frequent basis.", "labels": [], "entities": []}, {"text": "Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years.", "labels": [], "entities": [{"text": "summaries automatically", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8427685499191284}, {"text": "summarization research", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.9170801937580109}]}, {"text": "For example, proposed three content-based evaluation methods that measure similarity between summaries.", "labels": [], "entities": []}, {"text": "These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence.", "labels": [], "entities": [{"text": "unit overlap", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.8317964673042297}]}, {"text": "However, they did not show how the results of these automatic evaluation methods correlate to human judgments.", "labels": [], "entities": []}, {"text": "Following the successful application of automatic evaluation methods, such as BLEU (), in machine translation evaluation, showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9924499988555908}, {"text": "machine translation evaluation", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.8622162938117981}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9921218156814575}]}, {"text": "In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9907271862030029}, {"text": "summaries", "start_pos": 74, "end_pos": 83, "type": "TASK", "confidence": 0.8415936827659607}]}, {"text": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9742845892906189}]}, {"text": "It includes several automatic evaluation methods that measure the similarity between summaries.", "labels": [], "entities": []}, {"text": "We describe ROUGE-N in Section 2, ROUGE-L in Section 3, ROUGE-W in Section 4, and ROUGE-S in Section 5.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.931957483291626}, {"text": "ROUGE-L", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9025529026985168}, {"text": "ROUGE-W", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9159370064735413}, {"text": "ROUGE-S", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.8706512451171875}]}, {"text": "Section 6 shows how these measures correlate with human judgments using, and 2003 data.", "labels": [], "entities": []}, {"text": "Section 7 concludes this paper and discusses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the effectiveness of ROUGE measures, we compute the correlation between ROUGE assigned summary scores and human assigned summary scores.", "labels": [], "entities": [{"text": "ROUGE assigned summary scores", "start_pos": 82, "end_pos": 111, "type": "METRIC", "confidence": 0.7494360208511353}]}, {"text": "The intuition is that a good evaluation measure should assign a good score to a good summary and a bad score to a bad summary.", "labels": [], "entities": []}, {"text": "The ground truth is based on human assigned scores.", "labels": [], "entities": []}, {"text": "Acquiring human judgments are usually very expensive; fortunately, we have  manual summaries (CASE set), stemmed 4 version of the summaries (STEM set), and stopped version of the summaries (STOP set).", "labels": [], "entities": [{"text": "Acquiring human judgments", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.750743548075358}, {"text": "CASE set)", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.8741828203201294}]}, {"text": "For example, we computed ROUGE scores for the 12 systems participated in the DUC 2001 single document summarization evaluation using the CASE set with single reference and then calculated the three correlation scores for these 12 systems' ROUGE scores vs. human assigned average coverage scores.", "labels": [], "entities": [{"text": "DUC 2001 single document summarization evaluation", "start_pos": 77, "end_pos": 126, "type": "DATASET", "confidence": 0.8916093508402506}, {"text": "CASE set", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.7578261494636536}]}, {"text": "After that we repeated the process using multiple references and then using STEM and STOP sets.", "labels": [], "entities": []}, {"text": "Therefore, 2 (multi or single) x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spearman, or Kendall) = 18 data points were collected for each ROUGE measure and each DUC task.", "labels": [], "entities": []}, {"text": "To assess the significance of the results, we applied bootstrap resampling technique to estimate 95% confidence intervals for every correlation computation.", "labels": [], "entities": []}, {"text": "17 ROUGE measures were tested for each run using ROUGE evaluation package v1.2.1: ROUGE-N with N = 1 to 9, R OUGE-L, ROUGE-W with weighting factor \u03b1 = 1.2, ROUGE-S and ROUGE-SU with maximum skip distance d skip = 1, 4, and 9.", "labels": [], "entities": [{"text": "OUGE-L", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.6973077654838562}]}, {"text": "Due to limitation of space, we only report correlation analysis results based on Pearson's correlation coefficient.", "labels": [], "entities": [{"text": "correlation analysis", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7175321280956268}, {"text": "Pearson's correlation coefficient", "start_pos": 81, "end_pos": 114, "type": "METRIC", "confidence": 0.7646643072366714}]}, {"text": "Correlation analyses based on Spearman's and Kendall's correlation coefficients are tracking Pearson's very closely and will be posted later at the ROUGE website 5 for reference.", "labels": [], "entities": [{"text": "Kendall's correlation", "start_pos": 45, "end_pos": 66, "type": "METRIC", "confidence": 0.5918350120385488}, {"text": "ROUGE website 5", "start_pos": 148, "end_pos": 163, "type": "DATASET", "confidence": 0.8986507455507914}]}, {"text": "The critical value 6 for Pearson's correlation is 0.632 at 95% confidence with 8 degrees of freedom.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 25, "end_pos": 46, "type": "METRIC", "confidence": 0.7300612926483154}]}, {"text": "shows the Pearson's correlation coefficients of the 17 ROUGE measures vs. human judgments on DUC 2001 and 2002 100 words single document summarization data.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8408541083335876}, {"text": "ROUGE", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9647201299667358}, {"text": "DUC 2001 and 2002 100 words single document summarization data", "start_pos": 93, "end_pos": 155, "type": "DATASET", "confidence": 0.9410436868667602}]}, {"text": "The best values in each column are marked with dark (green) color and statistically equivalent values to the best values are marked with gray.", "labels": [], "entities": []}, {"text": "We found that correlations were not affected by stemming or removal of stopwords in this data set, ROUGE-2 performed better among the ROUGE-N variants, ROUGE-L, ROUGE-W, and ROUGE-S were all performing well, and using multiple references improved performance though not much.", "labels": [], "entities": []}, {"text": "All ROUGE measures achieved very good correlation with human judgments in the DUC 2002 data.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9064253568649292}, {"text": "DUC 2002 data", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9838597178459167}]}, {"text": "This might due to the double sample size in DUC 2002 for each system.", "labels": [], "entities": [{"text": "DUC 2002", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.943390816450119}]}, {"text": "shows the correlation analysis results on the DUC 2003 single document very short summary data.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9496062397956848}, {"text": "DUC 2003 single document", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.9725034534931183}]}, {"text": "We found that ROUGE-1, ROUGE-L, ROUGE- in this data set, using multiple references did not improve correlations.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.929294764995575}, {"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9771290421485901}]}, {"text": "In A1, A2, and A3, we show correlation analysis results on, and 2003 100 words multi-document summarization data.", "labels": [], "entities": [{"text": "A2", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.8809109926223755}, {"text": "correlation", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9209296107292175}]}, {"text": "The results indicated that using multiple references improved correlation and exclusion of stopwords usually improved performance.", "labels": [], "entities": [{"text": "correlation", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9642677903175354}]}, {"text": "ROUGE-1, 2, and 3 performed fine but were not consistent.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6757562160491943}]}, {"text": "ROUGE-1, ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGE-SU9 with stopword removal had correlation above 0.70.", "labels": [], "entities": []}, {"text": "ROUGE-L and ROUGE-W did notwork well in this set of data.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9356678128242493}, {"text": "ROUGE-W", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9605534076690674}]}, {"text": "C, D1, D2, E1, E2, and F show the correlation analyses using multiple references on the rest of DUC data.", "labels": [], "entities": [{"text": "F", "start_pos": 23, "end_pos": 24, "type": "METRIC", "confidence": 0.9979285001754761}, {"text": "correlation", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9690572023391724}, {"text": "DUC data", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9489452242851257}]}, {"text": "These results again suggested that exclusion of stopwords achieved better performance especially in multi-document summaries of 50 words.", "labels": [], "entities": []}, {"text": "Better correlations (> 0.70) were observed on long summary tasks, i.e. 200 and 400 words summaries.", "labels": [], "entities": []}, {"text": "The relative performance of ROUGE measures followed the pattern of the 100 words multi-document summarization task.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.6596599221229553}, {"text": "multi-document summarization task", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.6240649521350861}]}, {"text": "Comparing the results in 1 and 2, we found that correlation values in the multidocument tasks rarely reached high 90% except in long summary tasks.", "labels": [], "entities": [{"text": "correlation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9847188591957092}]}, {"text": "One possible explanation of this outcome is that we did not have large amount of samples for the multi-document tasks.", "labels": [], "entities": []}, {"text": "In the single document summarization tasks we had over 100 samples; while we only had about 30 samples in the multi-document tasks.", "labels": [], "entities": []}, {"text": "The only tasks that had over 30 samples was from DUC 2002 and the correlations of ROUGE measures with human judgments on the 100 words summary task were much better and more stable than similar tasks in.", "labels": [], "entities": [{"text": "DUC 2002", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9524878263473511}, {"text": "ROUGE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9907570481300354}]}, {"text": "Statistically stable human judgments of system performance might not be obtained due to lack of samples and this in turn caused instability of correlation analyses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's correlations of 17 ROUGE  measure scores vs. human judgments for the DUC  2001 and 2002 100 words single document sum- marization tasks", "labels": [], "entities": [{"text": "ROUGE  measure scores", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.9515448212623596}, {"text": "DUC  2001 and 2002 100 words single document sum- marization", "start_pos": 89, "end_pos": 149, "type": "TASK", "confidence": 0.8441797278144143}]}, {"text": " Table 3: Pearson's correlations of 17 ROUGE measure scores vs. human judgments for  the", "labels": [], "entities": [{"text": "ROUGE measure scores", "start_pos": 39, "end_pos": 59, "type": "METRIC", "confidence": 0.9491991400718689}]}]}