{"title": [{"text": "Evaluation of Restricted Domain Question-Answering Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "Question-Answering (QA) evaluation efforts have largely been tailored to open-domain systems.", "labels": [], "entities": [{"text": "Question-Answering (QA) evaluation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6602923989295959}]}, {"text": "The TREC QA test collections contain newswire articles and the accompanying queries cover a wide variety of topics.", "labels": [], "entities": [{"text": "TREC QA test collections", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8881843239068985}]}, {"text": "While some apprehension about the limitations of restricted-domain systems is no doubt justified, the strict promotion of unlimited domain QA evaluations may have some unintended consequences.", "labels": [], "entities": []}, {"text": "Simply applying the open domain QA evaluation paradigm to a restricted-domain system poses problems in the areas of test question development, answer key creation, and test collection construction.", "labels": [], "entities": [{"text": "test question development", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.6478493511676788}, {"text": "answer key creation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7931238214174906}, {"text": "test collection construction", "start_pos": 168, "end_pos": 196, "type": "TASK", "confidence": 0.7812563379605612}]}, {"text": "This paper examines the evaluation requirements of restricted domain systems.", "labels": [], "entities": []}, {"text": "It incorporates evaluation criteria identified by users of an operational QA system in the aerospace engineering domain.", "labels": [], "entities": []}, {"text": "While the paper demonstrates that user-centered task-based evaluations are required for restricted domain systems, these evaluations are found to be equally applicable to open domain systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Text REtrieval Conference (TREC) organized the first QA evaluation (QA track) in 1999) and annual evaluations of this nature are ongoing (Voorhees, to appear).", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7545409550269445}]}, {"text": "While the tasks and answer requirements have varied slightly from year to year, the purpose behind QA evaluations remains the same: to move from the traditional document retrieval to actual information retrieval by providing an answer to a question rather than a ranked list of relevant documents.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.7560140788555145}]}, {"text": "The track was originally intended to bring together the fields of Information Extraction (IE) and Information Retrieval (IR).", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.8023821473121643}, {"text": "Information Retrieval (IR)", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.8444855332374572}]}, {"text": "This legacy still continues in the factoid questions that require an IE type answer snippet in response, e.g.: \"What country is the Aswan High Dam located in?\"", "labels": [], "entities": [{"text": "Aswan High Dam", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.8054910103480021}]}, {"text": "This style of QA evaluation is spreading with very similar evaluations in Asia) and Europe (.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.9321833848953247}]}, {"text": "Although these evaluations have a multilingual slant, they are strongly modeled after the TREC QA track.", "labels": [], "entities": [{"text": "TREC QA track", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.6637299557526907}]}, {"text": "Typical QA systems that participate in these evaluations classify the questions into types which determine what kind of answer is required.", "labels": [], "entities": []}, {"text": "After an initial retrieval of documents pertaining to the question, some form of text processing is then applied to identify possible answer sentences in the documents.", "labels": [], "entities": []}, {"text": "Sentences that are near or contain keywords from the original question and contain the desired answer pattern are selected for answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.835944414138794}]}, {"text": "Since it is difficult for systems to determine which part of the sentence is the correct answer, especially if it contains multiple extractions of the desired type, many systems have resorted to redundancy tactics ().", "labels": [], "entities": []}, {"text": "These systems use the Web as an answer verification tool by choosing the answer that appears most often together with the question keywords.", "labels": [], "entities": [{"text": "answer verification", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.789388120174408}]}, {"text": "While this technique is very successful in open domain evaluations, restricted-domain systems do not have the luxury of using redundancy, making these evaluations inappropriate for systems such as these.", "labels": [], "entities": []}, {"text": "Our QA system participated in the three earlier TREC evaluations, e.g. ().", "labels": [], "entities": []}, {"text": "However, after starting work in the restricted-domain of re-usable launch vehicles, we found that the TREC evaluation no longer suited our system development needs and maintaining two different QA systems was too costly.", "labels": [], "entities": []}], "datasetContent": [{"text": "When it came time to evaluate the KAAS system, we initially defaulted to the TREC style QA evaluation with short, fact-based questions, adjudicated answers to these questions, and a test collection in which to find those answers.", "labels": [], "entities": [{"text": "TREC", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8944693207740784}]}, {"text": "This choice of evaluation was not surprising since early versions of our system grew out of that environment.", "labels": [], "entities": []}, {"text": "However, it quickly became apparent that this evaluation style posed problems for our restricted-domain, specific purpose system.", "labels": [], "entities": []}, {"text": "Developing a set of test questions was easier said than done.", "labels": [], "entities": []}, {"text": "Unlike the open domain evaluations, where test questions can be mined from question logs (Encarta, Excite, AskJeeves), no question sets are at the disposal of restricteddomain evaluators.", "labels": [], "entities": []}, {"text": "To build a set of test questions, we hired two sophomore aerospace engineering students.", "labels": [], "entities": []}, {"text": "Based on class project papers of the previous semester and examples of TREC questions, the students were asked to create as many short factoid questions as they could, i.e \"What is APAS?\"", "labels": [], "entities": [{"text": "APAS?\"", "start_pos": 181, "end_pos": 187, "type": "TASK", "confidence": 0.6904830038547516}]}, {"text": "However, the real user questions that we collected later did not look anything like the short test questions in this initial evaluation set.", "labels": [], "entities": []}, {"text": "The user questions were much more complex, e.g. \"How difficult is it to mold and shape graphite-epoxies compared with alloys or ceramics that maybe used for thermal protective applications?\"", "labels": [], "entities": []}, {"text": "A more in depth analysis of KAAS question types can be found in.", "labels": [], "entities": [{"text": "KAAS question types", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.7427910566329956}]}, {"text": "Establishing answers for the initial test questions proved difficult as well.", "labels": [], "entities": []}, {"text": "The students did fine at collecting the questions that they had while reading the papers, but lacked sufficient domain expertise to establish answer correctness.", "labels": [], "entities": []}, {"text": "Another issue was determining recall because it wasn't always clear whether the (small) corpus simply did not contain the answer or whether the system was notable to find it.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9644731879234314}]}, {"text": "A third student, a doctoral student in aerospace engineering, was hired to help with these issues.", "labels": [], "entities": []}, {"text": "To facilitate automatic evaluation we wanted to represent the answers in simple patterns but found that complex answers are not necessarily suitable for such a representation, even though patterns have proven feasible for TREC systems.", "labels": [], "entities": []}, {"text": "While a newswire document collection for general domain evaluation is easy to find, a collection in our specialized domain needed to be created from scratch.", "labels": [], "entities": [{"text": "general domain evaluation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.7465342084566752}]}, {"text": "Not only did the collection of documents take time, the conversion of most of these documents to text proved to be quite an unexpected hurdle as well.", "labels": [], "entities": []}, {"text": "As is evident, the TREC style QA evaluation did not suit our restricted domain system.", "labels": [], "entities": [{"text": "TREC style QA evaluation", "start_pos": 19, "end_pos": 43, "type": "DATASET", "confidence": 0.5086694210767746}]}, {"text": "It also leaves out the user entirely.", "labels": [], "entities": []}, {"text": "While informationbased evaluations are necessary to establish the ability of the system to answer questions correctly, we felt that they were not sufficient for evaluating a system with real users.", "labels": [], "entities": []}, {"text": "Restricted domain systems tend to be situated not only within a specific domain, but also within a certain user community and within a specific task domain.", "labels": [], "entities": []}, {"text": "A generic evaluation is neither sufficient nor suitable fora restricted domain system.", "labels": [], "entities": []}, {"text": "The environment in which KAAS is situated should drive the evaluation.", "labels": [], "entities": [{"text": "KAAS", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.767872154712677}]}, {"text": "Unlike many of the systems that participate in a TREC QA evaluation, the KAAS system has to function in real time with real users, not in batch mode with surrogate relevance assessors.", "labels": [], "entities": [{"text": "TREC QA evaluation", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7700811227162679}]}, {"text": "This brings with it additional evaluation criteria such as utility and system speed (.", "labels": [], "entities": []}, {"text": "KAAS users were asked in two separate surveys about their use and experiences with the system.", "labels": [], "entities": [{"text": "KAAS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8238290548324585}]}, {"text": "The surveys were part of larger scale, cross-university course evaluations which looked at the students' perceptions of distance learning, collaboration at a distance, the collaborative software package, the KAAS, and each participating faculty member.", "labels": [], "entities": [{"text": "KAAS", "start_pos": 208, "end_pos": 212, "type": "DATASET", "confidence": 0.8910338878631592}]}, {"text": "While there was some structure and guidance in the user survey of the QA system, it was minimal and the survey is mainly characterized by the open nature of the responses.", "labels": [], "entities": []}, {"text": "There were 25 to 30 students participating in each full course survey, but since we do not have the actual surveys that were turned in, we are not certain as to exactly how many students completed the survey section on the KAAS.", "labels": [], "entities": [{"text": "KAAS", "start_pos": 223, "end_pos": 227, "type": "DATASET", "confidence": 0.6392582058906555}]}, {"text": "However, it appears that most, if not all of the students provided feedback.", "labels": [], "entities": []}, {"text": "Given the free text nature of the responses, it was decided that the three researchers would do a content analysis of the responses and independently derive a set of evaluation dimensions that they detected in the students' responses.", "labels": [], "entities": []}, {"text": "Through content analysis of the user responses and follow-up discussion, we identified 5 main areas of importance to KAAS users when using the system: system performance, answers, database content, display, and expectations (see.", "labels": [], "entities": []}, {"text": "Each of the categories will be described in more detail below.", "labels": [], "entities": []}, {"text": "If we consider a restricted domain QA system as a system developed fora certain application, it is clear that these systems require a situated evaluation.", "labels": [], "entities": []}, {"text": "The evaluation has to be situated in the task, domain, and user community for which the system is developed.", "labels": [], "entities": []}, {"text": "How then can a restricted domain system best be evaluated?", "labels": [], "entities": []}, {"text": "We believe that the evaluation should be driven by the dimensions identified by the users as important: system performance, answers, database content, display, and expectations.", "labels": [], "entities": [{"text": "display", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.9336808323860168}]}, {"text": "The system should be evaluated on its performance.", "labels": [], "entities": []}, {"text": "How many seconds does it take to answer a question?", "labels": [], "entities": []}, {"text": "Once the speed is known, one can determine how long users are willing to wait for an answer.", "labels": [], "entities": []}, {"text": "It may very well be that the answer-finding capability of a system will need to be simplified in order to speedup the system and satisfy its users.", "labels": [], "entities": []}, {"text": "Similarly, tests to determine robustness need to be part of the system performance evaluation.", "labels": [], "entities": []}, {"text": "Users tend to shy away from systems that are periodically unavailable or slow to a crawl during peak usage hours.", "labels": [], "entities": []}, {"text": "Systems should also be evaluated on their answer providing ability.", "labels": [], "entities": [{"text": "answer providing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.845133513212204}]}, {"text": "This evaluation should include measures for answer completeness, accuracy, and relevancy.", "labels": [], "entities": [{"text": "answer completeness", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8100056052207947}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9990900754928589}]}, {"text": "Test questions should be within the domain of the QA system in order to test the answer quality for that domain.", "labels": [], "entities": []}, {"text": "Answers to certain questions require a more finegrained scoring procedure: answers that are explanations or summaries or biographies or comparative evaluations cannot be meaningfully rated as simply right or wrong.", "labels": [], "entities": []}, {"text": "The answer providing capability should be evaluated in light of the task or purpose of the system.", "labels": [], "entities": []}, {"text": "For example, users of the KAAS are learners in the field and are not well served with exact answer snippets.", "labels": [], "entities": [{"text": "KAAS", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.7574643492698669}]}, {"text": "For their task, they need answer context information to be able to learn from the answer text.", "labels": [], "entities": []}, {"text": "The evaluation should also include measures of the Database Content.", "labels": [], "entities": []}, {"text": "Rather than assuming relevancy of a collection, it should be evaluated whether the content is regularly updated, whether the contents are of acceptable quality to the users, and whether the coverage of the restricted domain is extensive enough.", "labels": [], "entities": []}, {"text": "Another system component that should be evaluated is the User Interface.", "labels": [], "entities": [{"text": "User Interface", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.5966170430183411}]}, {"text": "Is the system easy to use?", "labels": [], "entities": []}, {"text": "Does the interface provide clear guidance and/or assistance to the user?", "labels": [], "entities": []}, {"text": "Does it allow users to search in multiple ways?", "labels": [], "entities": []}, {"text": "Finally, it maybe pertinent to evaluate how far the system goes in living up to user expectations.", "labels": [], "entities": []}, {"text": "Although it is impossible to satisfy everybody, the system developers need to know whether there is a large discrepancy between user expectations and the actual system, since this may influence the use of the system.", "labels": [], "entities": []}, {"text": "Clearly, open-domain systems would benefit from the evaluation dimensions discussed in Section 4.", "labels": [], "entities": []}, {"text": "The difference would be that the test questions used for evaluation would be general rather than tailored to a specific domain.", "labels": [], "entities": []}, {"text": "Additionally, it maybe harder to evaluate the database content (i.e. the collection) fora general domain system than would be the case for restricted-domain systems.", "labels": [], "entities": []}, {"text": "To make open-domain evaluations more applicable to restricted-domain systems, they could be extended to include metrics about answer speed, and the ability of answering within a certain task.", "labels": [], "entities": []}, {"text": "For example, the evaluation could include system performance to get an indication as to how much processing time, given certain hardware, is required in getting the answers.", "labels": [], "entities": []}, {"text": "As for answer correctness itself, it maybe interesting to require extensive use of task scenarios that would determine aspects such as answer length and level of detail.", "labels": [], "entities": [{"text": "answer correctness", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.9375302791595459}]}, {"text": "It may also be desirable to evaluate runs without redundancy techniques separately.", "labels": [], "entities": []}, {"text": "Ideally, users would be incorporated into the evaluation to assess the user interface and the ability of the system to assist them in completion of a certain task.", "labels": [], "entities": []}], "tableCaptions": []}