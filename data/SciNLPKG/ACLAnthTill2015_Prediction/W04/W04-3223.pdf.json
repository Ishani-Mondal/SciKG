{"title": [{"text": "Incremental Feature Selection and 1 Regularization for Relaxed Maximum-Entropy Modeling", "labels": [], "entities": [{"text": "Incremental Feature Selection", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7286485234896342}]}], "abstractContent": [{"text": "We present an approach to bounded constraint-relaxation for entropy maximization that corresponds to using a double-exponential prior or 1 reg-ularizer in likelihood maximization for log-linear models.", "labels": [], "entities": [{"text": "entropy maximization", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8397889137268066}]}, {"text": "We show that a combined incremental feature selection and regularization method can be established for maximum entropy modeling by a natural incorporation of the regularizer into gradient-based feature selection, following Perkins et al.", "labels": [], "entities": []}, {"text": "This provides an efficient alternative to standard 1 regularization on the full feature set, and a mathematical justification for thresholding techniques used in likelihood-based feature selection.", "labels": [], "entities": [{"text": "likelihood-based feature selection", "start_pos": 162, "end_pos": 196, "type": "TASK", "confidence": 0.631802370150884}]}, {"text": "Also, we motivate an extension to n-best feature selection for linguistic features sets with moderate redundancy, and present experimental results showing its advantage over 0 , 1-best 1 , 2 regularization and over standard incremental feature selection for the task of maximum-entropy parsing.", "labels": [], "entities": [{"text": "maximum-entropy parsing", "start_pos": 270, "end_pos": 293, "type": "TASK", "confidence": 0.6652919948101044}]}], "introductionContent": [{"text": "The maximum-entropy (ME) principle, which prescribes choosing the model that maximizes the entropy out of all models that satisfy given feature constraints, can be seen as a built-in regularization mechanism that avoids overfitting the training data.", "labels": [], "entities": []}, {"text": "However, it is only a weak regularizer that cannot avoid overfitting in situations where the number of training examples is significantly smaller than the number of features.", "labels": [], "entities": []}, {"text": "In such situations some features will occur zero times on the training set and receive negative infinity weights, causing the assignment of zero probabilities for inputs including those features.", "labels": [], "entities": []}, {"text": "Similar assignment of (negative) infinity weights happens to features that are pseudominimal (or pseudo-maximal) on the training set (see), that is, features whose value on correct parses always is less (or greater) than or equal to their value on all other parses.", "labels": [], "entities": []}, {"text": "Also, if large features sets are generated automatically from conjunctions of simple feature tests, many features will be redundant.", "labels": [], "entities": []}, {"text": "Besides overfitting, large feature sets also create the problem of increased time and space complexity.", "labels": [], "entities": []}, {"text": "Common techniques to deal with these problems are regularization and feature selection.", "labels": [], "entities": [{"text": "regularization", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9700090289115906}, {"text": "feature selection", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7804271578788757}]}, {"text": "For ME models, the use of an 2 regularizer, corresponding to imposing a Gaussian prior on the parameter values, has been proposed by and.", "labels": [], "entities": []}, {"text": "Feature selection for ME models has commonly used simple frequencybased cut-off, or likelihood-based feature induction as introduced by Della.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7349945604801178}]}, {"text": "Whereas 2 regularization produces excellent generalization performance and effectively avoids numerical problems, parameter values almost never decrease to zero, leaving the problem of inefficient computation with the full feature set.", "labels": [], "entities": []}, {"text": "In contrast, feature selection methods effectively decrease computational complexity by selecting a fraction of the feature set for computation; however, generalization performance suffers from the ad-hoc character of hard thresholds on feature counts or likelihood gains.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7470871806144714}]}, {"text": "proposed a technique based on 1 regularization that embeds feature selection into regularization such that both a precise assessment of the reliability of features and the decision about inclusion or deletion of features can be done in the same framework.", "labels": [], "entities": []}, {"text": "Feature sparsity is produced by the polyhedral structure of the 1 norm which exhibits a gradient discontinuity at zero that tends to force a subset of parameter values to be exactly zero at the optimum.", "labels": [], "entities": []}, {"text": "Since this discontinuity makes optimization a hard numerical problem, standard gradient-based techniques for estimation cannot be applied directly.", "labels": [], "entities": [{"text": "estimation", "start_pos": 109, "end_pos": 119, "type": "TASK", "confidence": 0.9641803503036499}]}, {"text": "presents a specialized optimization algorithm for 1 regularization for linear least-squares regression called the Lasso algorithm. and employ standard iterative scaling and conjugate gradient techniques, however, for regularization a simplified one-sided exponential prior is employed which is non-zero only for non-negative parameter values.", "labels": [], "entities": [{"text": "1 regularization", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.6651929020881653}]}, {"text": "In these approaches the full feature space is considered in estimation, so savings in computational complexity are gained only in applications of the resulting sparse models.", "labels": [], "entities": []}, {"text": "presented an approach that combines 1 based regularization with incremental feature selection.", "labels": [], "entities": []}, {"text": "Their basic idea is to start with a model in which almost all weights are zero, and iteratively decide, by comparing regularized feature gradients, which weight should be adjusted away from zero in order to decrease the regularized objective function by the maximum amount.", "labels": [], "entities": []}, {"text": "The 1 regularizer is thus used directly for incremental feature selection, which on the one hand makes feature selection fast, and on the other hand avoids numerical problems for zero-valued weights since only non-zero weights are included in the model.", "labels": [], "entities": []}, {"text": "Besides the experimental evidence presented in these papers, recently a theoretical account on the superior sample complexity of 1 over 2 regularization has been presented by, showing logarithmic versus linear growth in the number of irrelevant features for 1 versus 2 regularized logistic regression.", "labels": [], "entities": []}, {"text": "In this paper, we apply 1 regularization to loglinear models, and motivate our approach in terms of maximum entropy estimation subject to relaxed constraints.", "labels": [], "entities": []}, {"text": "We apply the gradient-based feature selection technique of to our framework, and improve its computational complexity by an n-best feature inclusion technique.", "labels": [], "entities": []}, {"text": "This extension is tailored to linguistically motivated feature sets where the number of irrelevant features is moderate.", "labels": [], "entities": []}, {"text": "In experiments on real-world data from maximum-entropy parsing, we show the advantage of n-best 1 regularization over 2 , 1 , 0 regularization and standard incremental feature selection in terms of better computational complexity and improved generalization performance.", "labels": [], "entities": [{"text": "maximum-entropy parsing", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7279402017593384}]}], "datasetContent": [{"text": "As shown in, for feature selection from linguistically motivated feature sets with only a moderate amount of truly redundant features, it is crucial to choose the right number n of features to be added in each grafting step.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7107337862253189}]}, {"text": "The number of conjugate gradient iterations decreases rapidly in the number of features added at each step, whereas F-score evaluated on the test set does not decrease (or increases slightly) until more than 100 features are added in each step.", "labels": [], "entities": [{"text": "F-score", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9962982535362244}]}, {"text": "100-best grafting thus reduces estimation time by a factor of 10 at no loss in F-score compared to 1-best grafting.", "labels": [], "entities": [{"text": "estimation time", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.902875542640686}, {"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9988778233528137}]}, {"text": "Further increasing n results in a significant drop in F-score, while smaller n is computationally expensive, and also shows slight overtraining effects.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9989612102508545}]}, {"text": "In another experiment we tried to assess the relative contribution of regularization and incremental feature selection to the 1 -grafting technique.", "labels": [], "entities": []}, {"text": "Results of this experiments are shown in.", "labels": [], "entities": []}, {"text": "In this experiment we applied incremental feature selection using the gradient test described above to unregularized maximum-likelihood estimation (mleifs) and 2 -regularized maximum-likelihood estimation ( 2 -ifs).", "labels": [], "entities": []}, {"text": "Threshold parameters \u03b3 are adjusted on the heldout set, in addition to and independent of regularization parameters such as the variance of the Gaussian prior.", "labels": [], "entities": [{"text": "Threshold", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9810715317726135}]}, {"text": "Results are compared to 1 -regularized grafting as presented above.", "labels": [], "entities": []}, {"text": "For all runs a number of 100 features to be added in each grafting step is chosen.", "labels": [], "entities": []}, {"text": "The best result for the mle-ifs run is achieved at a threshold of 25, yielding an F-score of 78.8%.", "labels": [], "entities": [{"text": "mle-ifs", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.498477041721344}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9997263550758362}]}, {"text": "This shows that incremental feature selection is a powerful tool to avoid overfitting.", "labels": [], "entities": []}, {"text": "A further improvement in F-score to 79.1% is achieved by combining incremental feature selection with the 2 regularizer at a variance of 0.1 for the Gaussian prior and a threshold of 15.", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9988917708396912}]}, {"text": "Both runs provide excellent compression rates and convergence times.", "labels": [], "entities": [{"text": "compression", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9065123796463013}, {"text": "convergence", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9043444991111755}]}, {"text": "However, they are still outperformed by the 1 run that achieves a slight improvement in F-score to 79.3% and a slightly better runtime.", "labels": [], "entities": [{"text": "F-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9997692704200745}]}, {"text": "Furthermore, by integrating regularization naturally into thresholding for feature selection, a separate thresholding parameter is avoided in 1 -based incremental feature selection.", "labels": [], "entities": []}, {"text": "A theoretical account of the savings in computational complexity that can be achieved by nbest grafting can be given as follows.", "labels": [], "entities": []}, {"text": "This results in a cost of \u2248 mtp for feature testing, and \u2248 1 3 cmn 2 t 3 \u03c4 for optimization.", "labels": [], "entities": []}, {"text": "If we assume that tn s, this indicates considerable savings compared to both 1-best grafting and standard gradient-based optimization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F-score, compression, number of iterations,  and elapsed time for unregularized and standardized  maximum-likelihood estimation, and 0 , 1 , and 2  regularization on test split of PARC 700 dependency  bank.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9974138736724854}, {"text": "compression", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9580742716789246}, {"text": "PARC 700 dependency  bank", "start_pos": 190, "end_pos": 215, "type": "DATASET", "confidence": 0.9622465968132019}]}, {"text": " Table 3: F-score, compression, number of itera- tions, and elapsed time for gradient-based incre- mental feature selection without regularization, and  with 2 , and 1 regularization on test split of PARC  700 dependency bank.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9955022931098938}, {"text": "compression", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9303649067878723}, {"text": "PARC  700 dependency bank", "start_pos": 200, "end_pos": 225, "type": "DATASET", "confidence": 0.9628073871135712}]}]}