{"title": [{"text": "Supervised Word Sense Disambiguation with Support Vector Machines and Multiple Knowledge Sources", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7385689119497935}]}], "abstractContent": [{"text": "We participated in the SENSEVAL-3 English lexical sample task and multilingual lexical sample task.", "labels": [], "entities": [{"text": "SENSEVAL-3 English lexical sample task", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.7403383851051331}]}, {"text": "We adopted a supervised learning approach with Support Vector Machines, using only the official training data provided.", "labels": [], "entities": []}, {"text": "No other external resources were used.", "labels": [], "entities": []}, {"text": "The knowledge sources used were part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.", "labels": [], "entities": []}, {"text": "For the translation and sense subtask of the multilingual lexical sample task, the English sense given for the target word was also used as an additional knowledge source.", "labels": [], "entities": [{"text": "translation and sense subtask", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.7921874821186066}]}, {"text": "For the English lexical sample task, we obtained fine-grained and coarse-grained score (for both recall and precision) of 0.724 and 0.788 respectively.", "labels": [], "entities": [{"text": "coarse-grained score", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.9354868829250336}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9989750385284424}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9847372174263}]}, {"text": "For the multilingual lexical sample task, we obtained recall (and precision) of 0.634 for the translation subtask, and 0.673 for the translation and sense subtask.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9995598196983337}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9978755712509155}]}], "introductionContent": [{"text": "This paper describes the approach adopted by our systems which participated in the English lexical sample task and the multilingual lexical sample task of SENSEVAL-3.", "labels": [], "entities": []}, {"text": "The goal of the English lexical sample task is to predict the correct sense of an ambiguous English word , while that of the multilingual lexical sample task is to predict the correct Hindi (target language) translation of an ambiguous English (source language) word . The multilingual lexical sample task is further subdivided into two subtasks: the translation subtask, as well as the translation and sense subtask.", "labels": [], "entities": [{"text": "Hindi (target language) translation of an ambiguous English (source language) word", "start_pos": 184, "end_pos": 266, "type": "TASK", "confidence": 0.7556320269902547}]}, {"text": "The distinction is that for the translation and sense subtask, the English sense of the target ambiguous word is also provided (for both training and test data).", "labels": [], "entities": []}, {"text": "In all, we submitted 3 systems: system nusels for the English lexical sample task, system nusmlst for the translation subtask, and system nusmlsts for the translation and sense subtask.", "labels": [], "entities": []}, {"text": "All systems were based on the supervised word sense disambiguation (WSD) system of, and used Support Vector Machines (SVM) learning.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.7878989726305008}]}, {"text": "Only the training examples provided in the official training corpus were used to train the systems, and no other external resources were used.", "labels": [], "entities": []}, {"text": "In particular, we did not use any external dictionary or the sample sentences in the provided dictionary.", "labels": [], "entities": []}, {"text": "The knowledge sources used included part-ofspeech (POS) of neighboring words, single words in the surrounding context, local collocations, and syntactic relations, as described in.", "labels": [], "entities": []}, {"text": "For the translation and sense subtask of the multilingual lexical sample task, the English sense given for the target word was also used as an additional knowledge source.", "labels": [], "entities": [{"text": "translation and sense subtask", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.7921875715255737}]}, {"text": "All features encoding these knowledge sources were used, without any feature selection.", "labels": [], "entities": []}, {"text": "We next describe SVM learning and the combined knowledge sources adopted.", "labels": [], "entities": [{"text": "SVM learning", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9152851700782776}]}, {"text": "Much of the description follows that of.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recall SE-2 0.656 SE-1 (with dictionary examples) 0.796 SE-1 (without dictionary examples) 0.776: Micro-averaged recall on SENSEVAL-3 test data  Before participating in SENSEVAL-3, we evaluated our WSD system on the English lexical sample task of SENSEVAL-2 and SENSEVAL-1.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.880508303642273}, {"text": "SENSEVAL-3 test data", "start_pos": 123, "end_pos": 143, "type": "DATASET", "confidence": 0.6843316952387491}]}, {"text": "The microaveraged, fine-grained recall overall SENSEVAL-2 test words and all SENSEVAL-1 test words are given in.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9552430510520935}, {"text": "SENSEVAL-2", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.7073580622673035}]}, {"text": "In SENSEVAL-1, some example sentences are provided with the dictionary entries of the words used in the evaluation.", "labels": [], "entities": []}, {"text": "We provide the recall on SENSEVAL-1 test data with and without the use of such additional dictionary examples in training.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9995137453079224}, {"text": "SENSEVAL-1 test data", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.7136140068372091}]}, {"text": "On both SENSEVAL-2 and SENSEVAL-1 test data, the accuracy figures we obtained, as reported in, are higher than the best official test scores reported on both evaluation data sets.", "labels": [], "entities": [{"text": "SENSEVAL-2", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.6350449323654175}, {"text": "SENSEVAL-1 test data", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.8849960168202718}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.999566376209259}]}], "tableCaptions": [{"text": " Table 1. Each POS  noun, verb, or adjective is illustrated by one exam- ple. For each example, (a) shows", "labels": [], "entities": []}]}