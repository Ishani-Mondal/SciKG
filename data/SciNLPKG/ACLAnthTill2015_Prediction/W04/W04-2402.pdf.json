{"title": [{"text": "Semantic Lexicon Construction: Learning from Unlabeled Data via Spectral Analysis", "labels": [], "entities": [{"text": "Semantic Lexicon Construction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8115818500518799}, {"text": "Spectral Analysis", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8246268928050995}]}], "abstractContent": [{"text": "This paper considers the task of automatically collecting words with their entity class labels, starting from a small number of labeled examples ('seed' words).", "labels": [], "entities": []}, {"text": "We show that spectral analysis is useful for compensating for the paucity of labeled examples by learning from unlabeled data.", "labels": [], "entities": [{"text": "spectral analysis", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8610108196735382}]}, {"text": "The proposed method significantly outperforms a number of methods that employ techniques such as EM and co-training.", "labels": [], "entities": []}, {"text": "Furthermore, when trained with 300 labeled examples and unlabeled data, it rivals Naive Bayes classifiers trained with 7500 labeled examples .", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity detection plays an important role in information extraction systems.", "labels": [], "entities": [{"text": "Entity detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8414977490901947}, {"text": "information extraction", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.8565604090690613}]}, {"text": "Whether entity recognizers employ machine learning techniques or rule-based approaches, it is useful to have a gazetteer of words 1 that reliably suggest target entity class membership.", "labels": [], "entities": [{"text": "entity recognizers", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.6585978269577026}]}, {"text": "This paper considers the task of generating such gazetteers from a large unannotated corpus with minimal manual effort.", "labels": [], "entities": []}, {"text": "Starting from a small number of labeled examples (seeds), e.g., \"car\", \"plane\", \"ship\" \u00a1 labeled as vehicles, we seek to automatically collect more of these.", "labels": [], "entities": []}, {"text": "This task is sometimes called the semi-automatic construction of semantic lexicons, e.g. (. A common trend in prior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration.", "labels": [], "entities": []}, {"text": "The aim of bootstrapping is to compensate for the paucity of labeled examples.", "labels": [], "entities": []}, {"text": "However, its potential danger is label 'contamination' -namely, wrongly (automatically) labeled examples may misdirect the succeeding iterations.", "labels": [], "entities": []}, {"text": "Also, low frequency words are known to be problematic.", "labels": [], "entities": []}, {"text": "They do not provide sufficient corpus statistics (e.g., how frequently the word occurs as the subject of \"said\"), for adequate label prediction.", "labels": [], "entities": [{"text": "label prediction", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.8117398321628571}]}, {"text": "By contrast, we focus on improving feature vector representation for use in standard linear classifiers.", "labels": [], "entities": []}, {"text": "To counteract data sparseness, we employ subspace projection where subspaces are derived by singular value decomposition (SVD).", "labels": [], "entities": [{"text": "subspace projection", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.815041720867157}]}, {"text": "In this paper, we generally call such SVDbased subspace construction spectral analysis.", "labels": [], "entities": [{"text": "SVDbased subspace construction spectral analysis", "start_pos": 38, "end_pos": 86, "type": "TASK", "confidence": 0.7357435047626495}]}, {"text": "Latent Semantic Indexing (LSI)) is a well-known application of spectral analysis to word-by-document matrices.", "labels": [], "entities": [{"text": "Latent Semantic Indexing (LSI))", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.726583997408549}, {"text": "spectral analysis", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7399599552154541}]}, {"text": "Formal analyses of LSI were published relatively recently, e.g., ().", "labels": [], "entities": [{"text": "LSI", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7879132032394409}]}, {"text": "show the factors that may affect LSI's performance by analyzing the conditions under which the LSI subspace approximates an optimum subspace.", "labels": [], "entities": [{"text": "LSI", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9351938962936401}]}, {"text": "Our theoretical basis is partly derived from this analysis.", "labels": [], "entities": []}, {"text": "In particular, we replace the abstract notion of 'optimum subspace' with a precise definition of a subspace useful for our task.", "labels": [], "entities": []}, {"text": "The essence of spectral analysis is to capture the most prominently observed vector directions (or sub-vectors) into a subspace.", "labels": [], "entities": [{"text": "spectral analysis", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8737493753433228}]}, {"text": "Hence, we should apply spectral analysis only to 'good' feature vectors so that useful portions are captured into the subspace, and then factor out 'harmful' portions of all the vectors via subspace projection.", "labels": [], "entities": []}, {"text": "We first formalize the notion of harmful portions of the commonly used feature vector representation.", "labels": [], "entities": []}, {"text": "Experimental results show that this new strategy significantly improves label prediction performance.", "labels": [], "entities": [{"text": "label prediction", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.8409892022609711}]}, {"text": "For instance, when trained with 300 labeled examples and unlabeled data, the proposed method rivals Naive Bayes classifiers trained with 7500 labeled examples.", "labels": [], "entities": []}, {"text": "In general, generation of labeled training data involves expensive manual effort, while unlabeled data can be easily obtained in large amounts.", "labels": [], "entities": []}, {"text": "This fact has motivated supervised learning with unlabeled data, such as co-training (e.g.,).", "labels": [], "entities": []}, {"text": "The method we propose (called Spectral) can also be regarded as exploiting unlabeled data for supervised learning.", "labels": [], "entities": []}, {"text": "The main difference from co-training or popular EM-based approaches is that the process of learning from unlabeled data (via spectral analysis) does not use any class information.", "labels": [], "entities": []}, {"text": "It encodes learned information into feature vectors -which essentially serves as prediction of unseen feature occurrences -for use in supervised classification.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.728003203868866}]}, {"text": "The absence of class information during the learning process may seem to be disadvantageous.", "labels": [], "entities": []}, {"text": "On the contrary, our experiments show that Spectral consistently outperforms all the tested methods that employ techniques such as EM and co-training.", "labels": [], "entities": [{"text": "Spectral", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.880233645439148}, {"text": "EM", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.8804686665534973}]}, {"text": "We formalize the problem in Section 2, and propose the method in Section 3.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 4.", "labels": [], "entities": []}, {"text": "Experiments are reported in Section 5, and we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We study Spectral's performance in comparison with the algorithms discussed in the previous sections.", "labels": [], "entities": []}, {"text": "Prior semantic lexicon studies (e.g., TR) note that the choice of seeds is critical -i.e., seeds should be highfrequency words so that methods are provided with plenty of feature information to bootstrap with.", "labels": [], "entities": []}, {"text": "In practice, this can be achieved by first extracting the most frequent words from the target corpus and manually labeling them for use as seeds.", "labels": [], "entities": []}, {"text": "To simulate this practical situation, we split the above 10K words into a labeled set and an unlabeled set 11 , by choosing the most frequent words as the labeled set, where Note that approximately 80% of the seeds are negative examples ('Others').", "labels": [], "entities": []}, {"text": "As we assume that test data is known at the time of training, we use the unlabeled set as both unlabeled data and test data.", "labels": [], "entities": []}, {"text": "To study performance dependency on the choice of seeds, we made labeled/unlabeled splits randomly.", "labels": [], "entities": []}, {"text": "shows results of Spectral and the best-performing baseline algorithms.", "labels": [], "entities": [{"text": "Spectral", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.506196916103363}]}, {"text": "The average results over five runs using different seeds are shown.", "labels": [], "entities": []}, {"text": "All the methods (except Spectral) exhibit the same tendency.", "labels": [], "entities": []}, {"text": "That is, performance on random seeds is lower than that on high-frequency seeds, and the degradation is larger when the number of seeds is small.", "labels": [], "entities": []}, {"text": "This is not surprising since a small number of randomly chosen seeds provide much less information (corpus statistics) than high frequency seeds.", "labels": [], "entities": []}, {"text": "However, Spectral's perfor-", "labels": [], "entities": []}], "tableCaptions": []}