{"title": [{"text": "Combining Neural Networks and Statistics for Chinese Word sense disambiguation", "labels": [], "entities": [{"text": "Chinese Word sense disambiguation", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.623784177005291}]}], "abstractContent": [{"text": "The input of network is the key problem for Chinese Word sense disambiguation utilizing the Neural Network.", "labels": [], "entities": [{"text": "Chinese Word sense disambiguation", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.5423519164323807}]}, {"text": "This paper presents an input model of Neural Network that calculates the Mutual Information between contextual words and ambiguous word by using statistical method and taking the contextual words to certain number beside the ambiguous word according to (-M, +N).", "labels": [], "entities": []}, {"text": "The experiment adopts triple-layer BP Neural Network model and proves how the size of training set and the value of M and N affect the performance of Neural Network model.", "labels": [], "entities": []}, {"text": "The experimental objects are six pseudowords owning three word-senses constructed according to certain principles.", "labels": [], "entities": []}, {"text": "Tested accuracy of our approach on a close-corpus reaches 90.31%,, and 89.62% on a open-corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.999711811542511}]}, {"text": "The experiment proves that the Neural Network model has good performance on Word sense disambiguation.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6437111099561056}]}], "introductionContent": [{"text": "It is general that one word with many senses in natural language.", "labels": [], "entities": []}, {"text": "According statistics, there are about 42% ambiguous words in Chinese corpus . Word sense disambiguation (WSD) is a method to determine the sense of ambiguous word given the context circumstance.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.7696472952763239}]}, {"text": "WSD, a long-standing problem in NLP, has been a very active research topic,, which can be well applied in many NLP systems, such as Information Retrieval, Text Mining, Machine Translation, Text Categorization, Text Summarization, Speech Recognition, Text to Speech, and soon.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8171576857566833}, {"text": "Information Retrieval", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.7844499349594116}, {"text": "Text Mining", "start_pos": 155, "end_pos": 166, "type": "TASK", "confidence": 0.7747075259685516}, {"text": "Machine Translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7963194847106934}, {"text": "Text Categorization", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.7303906679153442}, {"text": "Text Summarization", "start_pos": 210, "end_pos": 228, "type": "TASK", "confidence": 0.8031544387340546}, {"text": "Speech Recognition", "start_pos": 230, "end_pos": 248, "type": "TASK", "confidence": 0.7970017790794373}]}, {"text": "With rising of Corpus linguistics, the machine learning methods based on statistics are booming.", "labels": [], "entities": [{"text": "Corpus linguistics", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7297844588756561}]}, {"text": "These methods draw the support from the high-powered computers, get the statistics of large real-world corpus, find and acquire knowledge of linguistics automatically.", "labels": [], "entities": []}, {"text": "They deal with all change by invariability, thus it is easy to trace the evaluation and development of natural language.", "labels": [], "entities": []}, {"text": "So the statistic methods of NLP has attracted the attention of professional researchers and become the mainstream bit by bit.", "labels": [], "entities": []}, {"text": "Corpus-based Statistical approaches are Decision Tree), Decision List, Genetic Algorithm, Naive-Bayesian Classifier)\u3001Maximum Entropy Model, and soon.", "labels": [], "entities": []}, {"text": "Corpus-based statistical approaches can be divided into supervised and unsupervised according to whether training corpus is sense-labeled text.", "labels": [], "entities": []}, {"text": "Supervised learning methods have the good learning ability and can get better accuracy in WSD experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9991055130958557}, {"text": "WSD", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9554484486579895}]}, {"text": "Obviously the data sparseness problem is a bottleneck for supervised learning algorithm.", "labels": [], "entities": []}, {"text": "If you want to get better learning and disambiguating effect, you can enlarge the size and smooth the data of training corpus.", "labels": [], "entities": []}, {"text": "According to practical demand, it would spend much more time and manpower to enlarge the size of training corpus.", "labels": [], "entities": []}, {"text": "Smoothing data is merely a subsidiary measure.", "labels": [], "entities": [{"text": "Smoothing", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.961566686630249}]}, {"text": "The sufficient large size of training corpus is still the foundation to get a satisfied effect in WSD experiment.", "labels": [], "entities": [{"text": "WSD", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.8758454918861389}]}, {"text": "Unsupervised WSD never depend on tagged corpus and could realize the training of large real corpus coming from all kinds of applying field.", "labels": [], "entities": [{"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9028199911117554}]}, {"text": "So researchers begin to pay attention to this kind of methods).", "labels": [], "entities": []}, {"text": "The kind of methods can overcome the sparseness problem in a degree.", "labels": [], "entities": []}, {"text": "It is obvious that the two kinds of methods based on statistic have their own advantages and disadvantages, and cannot supersede each other.", "labels": [], "entities": []}, {"text": "This paper researches the Chinese WSD using the model of artificial neural network and investigates the effect on WSD from input model of neural network constructed by the context words and the size of training corpus.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.47843775153160095}, {"text": "WSD", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.8469105362892151}]}], "datasetContent": [{"text": "In order to analyze the effect that the extent of training corpus influences the meaning distinguish ability of neural network, this article trains the model of neural network using the experimental corpus individually, C 1 , C2 and C 3 , and makes the close and open test for 6 ambiguities separately.", "labels": [], "entities": []}, {"text": "The close test means the corpus are same in test and training.", "labels": [], "entities": []}, {"text": "The experiment is divided into two groups according to the extracting method of contextual feature words..1 shows the result of the first experiment which extracts the contextual feature words using the method of \uff08-5\uff0c+ 5\uff09.", "labels": [], "entities": []}, {"text": "In addition, the first experiment investigates that the extent of training corpus (the number of training samples big or small) influences the ability to distinguish the models.", "labels": [], "entities": []}, {"text": "The second experiment investigates emphatically the effect that the method to collect the feature words influences the ability to distinguish BP model.", "labels": [], "entities": []}, {"text": "With the growing of training sample, the experimental results of open test increase steadily, except ambiguous word W2 (a little bit difference).", "labels": [], "entities": []}, {"text": "The experimental data prove the growing of training samples rise the correct percentage.", "labels": [], "entities": []}, {"text": "However, when the rising reaches to a certain degree, more rising is not good for the improvement of model.", "labels": [], "entities": []}, {"text": "What's more, the effect of noise is more and more remarkable.", "labels": [], "entities": []}, {"text": "That decreases the model's ability of differentiation in a certain degree.", "labels": [], "entities": []}, {"text": "On the other hand, after the growing of training corpus, the linguistic phenomenon around ambiguities is richer and richer, more and more complex.", "labels": [], "entities": []}, {"text": "That makes it harder to determine the meaning.", "labels": [], "entities": [{"text": "determine the meaning", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7103031675020853}]}, {"text": "This article emphasizes on the collecting method of contextual feature words in experiment two, in other words, the effect that the different values of M and N influence the model of BP network.", "labels": [], "entities": []}, {"text": "The experimental results (table 4.1 and 4.5) tell us that the context windows influence the correct percentage heavily.", "labels": [], "entities": []}, {"text": "The correct percentage increases almost by leaps and bounds from (-3\uff0c + 3) to\uff08-5\uff0c+5\uff09.", "labels": [], "entities": [{"text": "correct percentage", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9626307189464569}]}, {"text": "The discrepancy is obvious despite close test or open test.", "labels": [], "entities": []}, {"text": "The correct percentage increase again to \uff08-10\uff0c + 10\uff09 , in which the close test of ambiguous word W6 is more than 90% and 89.62% the close test, with the exception of W1 which open testis slightly special.", "labels": [], "entities": [{"text": "correct", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9845038652420044}]}, {"text": "That illustrates the more widely the context windows open, the more the effective information is caught to benefit the WSD more.", "labels": [], "entities": [{"text": "WSD", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.7990076541900635}]}, {"text": "Comparing the four feature methods of collection, including \uff08-3\uff0c+ 7\uff09,\uff08-7\uff0c+ 3\uff09,\uff08-4\uff0c+ 6\uff09 and \uff08-6\uff0c + 4\uff09 with \uff08-5\uff0c + 5\uff09 , the number of feature words besides the ambiguous word is various and the experimental results (table 4.1 and 4.5) are not same, although the windows are same.", "labels": [], "entities": []}, {"text": "Among them, the correct percentage of \uff08-5\uff0c+ 5\uff09is the highest.", "labels": [], "entities": [{"text": "correct percentage", "start_pos": 16, "end_pos": 34, "type": "METRIC", "confidence": 0.9612008631229401}]}, {"text": "And that of\uff08-4\uff0c+ 6\uff09and\uff08-6\uff0c+ 4\uff09is better than that of\uff08-3\uff0c+ 7\uff09and\uff08-7\uff0c+ 3\uff09a bit.", "labels": [], "entities": []}, {"text": "That shows the more balanceable the feature words besides ambiguous word, the more advantageous to judge meaning, and the better the experimental results.", "labels": [], "entities": []}, {"text": "In addition, some experimental results of open test are better than that of close test.", "labels": [], "entities": []}, {"text": "The main reason is the experimental corpus of open testis smaller than training corpus.", "labels": [], "entities": []}, {"text": "So the contextual meanings of ambiguous word in experimental corpus are rather explicit.", "labels": [], "entities": []}, {"text": "Thereby, that explains why should be this kind of experimental result.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table  3.2 show the overall number of ambiguous word  and percentage of ambiguous word having 2~4  meanings in all ambiguous word. These two charts  indicate that verb is most active in Chinese and its  average number of meanings is most, about 2.56.  The percentage of ambiguous word having 2~4  meanings is most in all ambiguous word.", "labels": [], "entities": []}, {"text": " Table 3.1 the average number of a Chinese", "labels": [], "entities": []}, {"text": " Table 3.3 the total number of the feature -vector  sample of ambiguous word  Training corpus are 105,000 lines, and each line  is a paragraph, totally about 10,000,000 words.", "labels": [], "entities": []}, {"text": " Table 4.1 The contrast chat of experimental result  for six ambiguities  ambiguities is showed in table 4.2 (close test), table  4.3 (open test), and table 4.4. Considering the  length of this article, table 4.2 and table 4.3 shows  the detailed data, and table 4.4 is brief.  Training set  pseudo- words  C 1  C 2  C 3  sense 1 0.9226  0.8169 0.8991  sense 2 0.5513  0.8017 0.6872  sense 3 0.8027  0.9564 0.9510  W 1", "labels": [], "entities": []}, {"text": " Table 4.3 The result of W 1 and W 6 in open test  under the different training corpus", "labels": [], "entities": []}, {"text": " Table 4.5 the experimental result under different  feature collecting method", "labels": [], "entities": [{"text": "feature collecting", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7929944097995758}]}, {"text": " Table 5.1 The number of experimental samples", "labels": [], "entities": []}]}