{"title": [{"text": "Modeling Category Structures with a Kernel Function", "labels": [], "entities": [{"text": "Modeling Category Structures", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8323917388916016}]}], "abstractContent": [{"text": "We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization.", "labels": [], "entities": [{"text": "TOP", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9308823347091675}]}, {"text": "Ina number of categoriza-tion tasks including text categorization, negative examples are usually more common than positive examples and there maybe several different types of negative examples.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7654891908168793}]}, {"text": "Therefore, we construct a TOP kernel, regarding the prob-abilistic model of negative examples as a mixture of several component models respectively corresponding to given categories.", "labels": [], "entities": []}, {"text": "Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time.", "labels": [], "entities": []}, {"text": "We also show that the computational advantage is shared by a more general class of models.", "labels": [], "entities": []}, {"text": "In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, Support Vector Machines (SVMs) have been actively studied because of their high generalization ability).", "labels": [], "entities": []}, {"text": "In the formulation of SVMs, functions which measure the similarity of two examples take an important role.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.9338368773460388}]}, {"text": "These functions are called kernel functions.", "labels": [], "entities": []}, {"text": "The usual dot-product of two vectors respectively corresponding to two examples is often used.", "labels": [], "entities": []}, {"text": "Although some variants to the usual dot-product are sometimes used (for example, higher-order polynomial kernels and RBF kernels), the distribution of examples is not taken into account in such kernels.", "labels": [], "entities": []}, {"text": "However, new types of kernels have more recently been proposed; they are based on the probability distribution of examples.", "labels": [], "entities": []}, {"text": "One is Fisher kernels).", "labels": [], "entities": []}, {"text": "The other is TOP (Tangent vector Of the Posterior log-odds) kernels ( ).", "labels": [], "entities": [{"text": "TOP", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.993175208568573}]}, {"text": "While Fisher kernels are constructed on the basis of a generative model of data, TOP kernels are based on the classposterior probability, that is, the probability that the positive class occurs given an example.", "labels": [], "entities": []}, {"text": "However, in order to use those kernels, we have to select a probabilistic model of data.", "labels": [], "entities": []}, {"text": "The selection of a model will affect categorization result.", "labels": [], "entities": []}, {"text": "The present paper provides one solution to this issue.", "labels": [], "entities": []}, {"text": "Specifically, we proposed one type of TOP kernel, because it has been reported that TOP kernels perform better than Fisher kernels in terms of categorization accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9714756608009338}]}, {"text": "We briefly explain our kernel.", "labels": [], "entities": []}, {"text": "We focus on negative examples in binary classification.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7475012838840485}]}, {"text": "Negative examples are usually more common than positive examples.", "labels": [], "entities": []}, {"text": "There maybe several different types of negative examples.", "labels": [], "entities": []}, {"text": "Furthermore, the categories of negative examples are sometimes explicitly given (for example, the situation where we are given documents, each of which has one of three categories \"sports\",\"politics\" and \"economics\", and we are to extract documents with \"politics\").", "labels": [], "entities": []}, {"text": "In such a situation, the probabilistic model of negative examples can be regarded as a mixture of several component models.", "labels": [], "entities": []}, {"text": "We effectively use this property.", "labels": [], "entities": []}, {"text": "Although many other models can be used, we propose a model based on the separating hyperplanes in the original feature space.", "labels": [], "entities": []}, {"text": "Specifically, a one-dimensional Gaussian-type function normal to a hyperplane corresponds to a category.", "labels": [], "entities": []}, {"text": "The negative class is then expressed as a kind of Gaussian mixture.", "labels": [], "entities": []}, {"text": "The reason for the selection of this model is that the resulting kernel has an advantage in computational time.", "labels": [], "entities": []}, {"text": "The kernel based on this mixture model, what we call Hyperplane-based TOP (HP-TOP) kernel, can be computed efficiently in spite of its high dimensionality.", "labels": [], "entities": []}, {"text": "We later show that the computational advantage is shared by a more general class of models.", "labels": [], "entities": []}, {"text": "In the experiments of text categorization, in which SVMs are used as classifiers, our kernel outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model proposed by in terms of categorization accuracy.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7424579858779907}]}], "datasetContent": [{"text": "Through experiments of text categorization, we empirically compare the HP-TOP kernel with the linear kernel and the PLSI-based Fisher kernel.", "labels": [], "entities": []}, {"text": "We use Reuters-21578 dataset 2 with ModApte-split ().", "labels": [], "entities": [{"text": "Reuters-21578 dataset", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.9647963643074036}]}, {"text": "In addition, we delete some texts from the result of ModAptesplit, because those texts have no text body.", "labels": [], "entities": [{"text": "ModAptesplit", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.8877102732658386}]}, {"text": "After the deletion, we obtain 8815 training examples and 3023 test examples.", "labels": [], "entities": []}, {"text": "The words that occur less than five times in the whole training set are excluded from the original feature set.", "labels": [], "entities": []}, {"text": "We do not use all the 8815 training examples.", "labels": [], "entities": []}, {"text": "The size of the actual training data ranges from 1000 to 8000.", "labels": [], "entities": []}, {"text": "For each dataset size, experiments are executed 10 times with different training sets.The result is evaluated with Fmeasures for the most frequent 10 categories.", "labels": [], "entities": [{"text": "Fmeasures", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9965264201164246}]}, {"text": "The total number of categories is actually 116.", "labels": [], "entities": []}, {"text": "However, for small categories, reliable statistics cannot be obtained.", "labels": [], "entities": []}, {"text": "For this reason, we regard the remaining categories other than the 10 most frequent categories as one category.", "labels": [], "entities": []}, {"text": "Therefore, the model for negative examples is a mixture of 10 component models (9 out of the 10 most frequent categories and the new category consisting of the remaining categories).", "labels": [], "entities": []}, {"text": "We assume uniform priors for categories as in ( ).", "labels": [], "entities": []}, {"text": "We computed the Fisher kernels with different numbers (10, 20 and 30) of latent classes and added them together to make a robust kernel.", "labels": [], "entities": []}, {"text": "After the learning in the original feature space, the parameters for the probability distributions are estimated with 2 Available from http://www.daviddlewis.com/resources/.", "labels": [], "entities": []}, {"text": "maximum likelihood estimation as in Equations and, followed by the learning with the proposed kernel.", "labels": [], "entities": []}, {"text": "We used an SVM package, TinySVM 3 , for SVM computation.", "labels": [], "entities": [{"text": "SVM computation", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.8377631902694702}]}, {"text": "The soft-margin parameter C was set to 1.0 (other values of C showed no significant changes in results).", "labels": [], "entities": []}, {"text": "The result is shown in (for macro-average) and (for micro-average).", "labels": [], "entities": []}, {"text": "The HP-TOP kernel outperforms the linear kernel and the PLSI-based Fisher kernel for every number of examples.", "labels": [], "entities": []}, {"text": "At each number of examples, we conducted a Wilcoxon Signed Rank test with 5% significance-level, for the HP-TOP kernel and the linear kernel, since these two are better than the other.", "labels": [], "entities": [{"text": "Wilcoxon Signed Rank test", "start_pos": 43, "end_pos": 68, "type": "METRIC", "confidence": 0.6211494654417038}, {"text": "significance-level", "start_pos": 77, "end_pos": 95, "type": "METRIC", "confidence": 0.9772995710372925}]}, {"text": "The test shows that the difference between the two methods is significant for the training data sizes 1000 to 5000.", "labels": [], "entities": []}, {"text": "The superiority of the HP-TOP kernel for small training datasets supports our expectation that the enrichment of feature set will lead to better performance for few active words.", "labels": [], "entities": []}, {"text": "Although we also expected that the effect of word sense disambiguation would improve accuracy for large training datasets, the experiments do not provide us with an empirical evidence for the expectation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.723757823308309}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9981580376625061}]}, {"text": "One possible reason is that Gaussian-type functions do not reflect the actual distribution of data.", "labels": [], "entities": []}, {"text": "We leave its further investigation as future research.", "labels": [], "entities": []}, {"text": "In this experimental setting, the PLSI-based Fisher kernel did notwork well in terms of categorization accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.971259355545044}]}, {"text": "However, this Fisher kernel will perform better when the number of labeled examples is small and a number of unlabeled examples are available, as reported by.", "labels": [], "entities": []}, {"text": "We also measured computational time of each method ( . This result empirically shows that the HP-TOP kernel outperforms the PLSI-based Fisher kernel in terms of computational time as theoretically expected in Section 5.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The categories and their sizes of Reuters-21578", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.6589757800102234}]}]}