{"title": [{"text": "A Qualitative Comparison of Scientific and Journalistic Texts from the Perspective of Extracting Definitions", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we highlight a selection of features of scientific text which distinguish it from news stories.", "labels": [], "entities": []}, {"text": "We argue that features such as structure, selective use of past tense, voice and stylistic conventions can affect question answering in the scientific domain.", "labels": [], "entities": [{"text": "question answering", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8588070571422577}]}, {"text": "We demonstrate this through qualitative observations made while working on retrieving definitions to terms related to salmon fish.", "labels": [], "entities": []}], "introductionContent": [{"text": "An information retrieval system informs on the existence (or non-existence) and whereabouts of documents relating to the request of a user.", "labels": [], "entities": []}, {"text": "On the other hand, a question answering (QA) system allows a user to ask a question in natural language and receive a concise answer, possibly with a validating context.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8486405551433563}]}, {"text": "Questions asking about definitions of terms (i.e., 'What is X?')", "labels": [], "entities": []}, {"text": "occur frequently in the query logs of search engines.", "labels": [], "entities": []}, {"text": "However, due to their complexity, recent work in the field of question answering has largely neglected them and concentrated instead on answering factoid questions for which the answer is a single word or short phrase).", "labels": [], "entities": [{"text": "question answering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8429631292819977}]}, {"text": "Much of this work has been motivated by the question answering track of the Text REtrieval Conference (TREC), which evaluates systems by providing them with a common challenge.", "labels": [], "entities": [{"text": "question answering", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7135530859231949}, {"text": "Text REtrieval Conference (TREC)", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.7709682782491049}]}, {"text": "Ina recent project inspired by our experiences in TREC (), a system was built for extracting definitions of technical terms from scientific texts.", "labels": [], "entities": [{"text": "extracting definitions of technical terms from scientific texts", "start_pos": 82, "end_pos": 145, "type": "TASK", "confidence": 0.8335887938737869}]}, {"text": "The topic was salmon fish biology, a very different one from that of news articles.", "labels": [], "entities": [{"text": "salmon fish biology", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.625405083100001}]}, {"text": "What, then, is the effect of domain on the applicability of QA?", "labels": [], "entities": []}, {"text": "In this paper we attempt to answer this question, focusing on definitions and drawing on our findings from previous projects.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: First, we review recent related work.", "labels": [], "entities": []}, {"text": "Second, we summarise the objectives, methods and findings of the SOK-I QA project, named after the sockeye salmon.", "labels": [], "entities": [{"text": "SOK-I QA", "start_pos": 65, "end_pos": 73, "type": "TASK", "confidence": 0.7221042513847351}]}, {"text": "Third, we compare the characteristics of scientific text with those of newspaper articles illustrating our points with examples from our SOK-I collection as well from the New York Times, CLEF 1994 Los Angeles Times collection and AQUAINT corpus.", "labels": [], "entities": [{"text": "SOK-I collection", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.7696717381477356}, {"text": "CLEF 1994 Los Angeles Times collection", "start_pos": 187, "end_pos": 225, "type": "DATASET", "confidence": 0.8190838793913523}, {"text": "AQUAINT corpus", "start_pos": 230, "end_pos": 244, "type": "DATASET", "confidence": 0.8940427303314209}]}, {"text": "Fourth, we discuss the implications that these have for definitional QA.", "labels": [], "entities": [{"text": "definitional QA", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8311959505081177}]}, {"text": "Finally, we draw conclusions from the study.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}