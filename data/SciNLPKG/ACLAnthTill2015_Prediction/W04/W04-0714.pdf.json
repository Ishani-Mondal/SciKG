{"title": [{"text": "Topic Identification in Chinese Based on Centering Model", "labels": [], "entities": [{"text": "Topic Identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8799187242984772}]}], "abstractContent": [{"text": "In this paper we are concerned with identifying the topics of sentences in Chinese texts.", "labels": [], "entities": []}, {"text": "The key elements of the centering model of local discourse coherence are employed to identify the topic which is the most salient element in a Chinese sentence.", "labels": [], "entities": []}, {"text": "Due to the phenomenon of zero anaphora occurring in Chinese texts frequently, in addition to the centering model, we further employ the constraint rules to identify the antecedents of zero anaphors.", "labels": [], "entities": []}, {"text": "Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge, we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics from sentences.", "labels": [], "entities": [{"text": "parsing sentences", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.9004921317100525}]}], "introductionContent": [{"text": "One of the most striking characteristics in a topic-prominent language like Chinese is the important element, \"topic,\" in a sentence which can represent what the sentence is about (.", "labels": [], "entities": []}, {"text": "That is, if we can identify topics from Chinese sentences, we can obtain the most information embedded in the text.", "labels": [], "entities": []}, {"text": "In this paper, we tend to identify the topic of each utterance within a discourse based on the centering model.", "labels": [], "entities": []}, {"text": "However, in many natural languages, elements that can be easily deduced by the reader are frequently omitted from expressions in texts.", "labels": [], "entities": []}, {"text": "The elimination of anaphoric expressions is termed zero anaphor (ZA) which often occurs in topic position in a Chinese sentence, due to their prominence in discourse.", "labels": [], "entities": [{"text": "zero anaphor (ZA)", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.7290015459060669}]}, {"text": "Accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.9129168093204498}, {"text": "zero anaphora resolution", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6809061169624329}]}, {"text": "There are several methods of anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7559469640254974}]}, {"text": "One method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained ().", "labels": [], "entities": []}, {"text": "Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge.", "labels": [], "entities": []}, {"text": "However, it is very labor-intensive and time-consuming to construct a domain knowledge base.", "labels": [], "entities": []}, {"text": "Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate ().", "labels": [], "entities": []}, {"text": "This method can sort out the above problems.", "labels": [], "entities": []}, {"text": "However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information.", "labels": [], "entities": []}, {"text": "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8225677311420441}, {"text": "partof-speech (POS) tagger", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.613310581445694}]}, {"text": "The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream.", "labels": [], "entities": []}, {"text": "The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6640723645687103}]}, {"text": "In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the details of shallow parsing we employed.", "labels": [], "entities": []}, {"text": "In Section 4 we explain our ZA resolution method based on the centering model and the constraint rules.", "labels": [], "entities": [{"text": "ZA resolution", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.8295205533504486}]}, {"text": "The method of topic identification in Chinese sentences is illustrated in Section 5.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.9054482281208038}]}, {"text": "In the last section the conclusions are made.", "labels": [], "entities": []}, {"text": "In the centering theory (, the 'attentional state' was identified as a basic component of discourse structure that consisted of two levels of focusing: global and local.", "labels": [], "entities": []}, {"text": "For Grosz and Sidner, the centering theory provided a model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses.", "labels": [], "entities": []}, {"text": "In the centering model, each utterance U in a discourse segment has two structures associated with it, called forwardlooking centers, Cf (U), and backward-looking center, Cb (U).", "labels": [], "entities": []}, {"text": "The forward-looking centers of U n , Cf (U n ), depend only on the expressions that constitute that utterance.", "labels": [], "entities": []}, {"text": "They are not constrained by features of any previous utterance in the discourse segment (DS), and the elements of Cf (U n ) are partially ordered to reflect relative prominence in U n ., assume that grammatical roles are the major determinant for ranking the forward-looking centers, with the order \"Subject > Object(s) > Others\".", "labels": [], "entities": []}, {"text": "The superlative element of Cf (U n ) may become the Cb of the following utterance, Cb (U n+1 ).", "labels": [], "entities": []}, {"text": "In addition to the structures for centers, Cb , and Cf , the centering model specifies a set of constraints and rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiment of ZA resolution, we use a test corpus which is a collection of 150 news articles contained 998 paragraphs, 4631 utterances, and 40884 Chinese words.", "labels": [], "entities": [{"text": "ZA resolution", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9446336030960083}]}, {"text": "By employing the ZA Triple Rules and ZA identification constraints mentioned previously, zero anaphors occur in topic or subject, and object positions can be detected.", "labels": [], "entities": [{"text": "ZA identification", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6485618948936462}]}, {"text": "Because the ZA Triple Rules cover each possible topic or subject, and object omission cases, the result shows that the zero anaphors are over detected and the precision rate (PR) is 84% calculated using equation 1.", "labels": [], "entities": [{"text": "precision rate (PR)", "start_pos": 159, "end_pos": 178, "type": "METRIC", "confidence": 0.9569804310798645}]}, {"text": "The main errors of ZA detection occur in the experiment when parsing inverted sentences and non-anaphoric cases (e.g. exophora or cataphora)).", "labels": [], "entities": [{"text": "ZA detection", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9327177107334137}, {"text": "parsing inverted sentences", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.8845431009928385}]}, {"text": "Cataphora is similar to anaphora, the difference being the direction of the reference.", "labels": [], "entities": []}, {"text": "In this paper, we do not deal with the case that the referent of a zero anaphor is in the following utterances, but we can detect about 60% cataphora in the test corpus by employing ZA identification constraint 1.", "labels": [], "entities": []}, {"text": "In the phase of antecedent identification, we take the output of employing the ZA Triple Rules and ZA identification constraints, and further to identify the antecedents of zero anaphors by using antecedent identification rule based on the centering model.", "labels": [], "entities": [{"text": "antecedent identification", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.6992896348237991}]}, {"text": "For example, in the discourse segment (5), the zero anaphors are detected in the utterances (5b) and (5c).", "labels": [], "entities": []}, {"text": "According to the antecedent identification rule, the noun phrase, \ud97b\udf59 'Kee-lung General Hospital,' whose grammatical role is corresponding to the zero anaphor i 1 \u03c6 in (5b) is identified as the antecedent.", "labels": [], "entities": [{"text": "Kee-lung General Hospital", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.8438471357027689}]}, {"text": "Subsequently, the antecedent of the zero anaphor  huo weishengshu renke wei banli wailao tijian yiyuan (Kee-lung General Hospital) i obtain Department-of-Health certify to-be handle foreign-laborer physical-examination hospital (Kee-lung General Hospital) i is certified by Department of Health as a hospital which can handle physical examinations of foreign laborers.", "labels": [], "entities": []}, {"text": "The recall rates (RR) and precision rates (PR) of ZA resolution is 70% and 60.3% respectively calculated using equation 2 and equation 3.", "labels": [], "entities": [{"text": "recall rates (RR)", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.976304292678833}, {"text": "precision rates (PR)", "start_pos": 26, "end_pos": 46, "type": "METRIC", "confidence": 0.986602783203125}]}, {"text": "Errors occur in the phase when a zero anaphor refers to an entity other than the corresponding grammatical role or the antecedent of the zero anaphor in the preceding utterance.", "labels": [], "entities": []}], "tableCaptions": []}