{"title": [{"text": "A Linear Programming Formulation for Global Inference in Natural Language Tasks", "labels": [], "entities": [{"text": "Linear Programming Formulation", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.6854319969813029}, {"text": "Global Inference in Natural Language Tasks", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.6812315483887991}]}], "abstractContent": [{"text": "Given a collection of discrete random variables representing outcomes of learned local predic-tors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints.", "labels": [], "entities": []}, {"text": "Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc.", "labels": [], "entities": []}, {"text": "We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations.", "labels": [], "entities": []}, {"text": "Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the \"human-like\" quality of the inferences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9993137121200562}]}], "introductionContent": [{"text": "Natural language decisions often depend on the outcomes of several different but mutually dependent predictions.", "labels": [], "entities": []}, {"text": "These predictions must respect some constraints that could arise from the nature of the data or from domain or task specific conditions.", "labels": [], "entities": []}, {"text": "For example, in part-ofspeech tagging, a sentence must have at least one verb, and cannot have three consecutive verbs.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7583924531936646}]}, {"text": "These facts can be used as constraints.", "labels": [], "entities": []}, {"text": "In named entity recognition, \"no entities can overlap\" is a common constraint used in various works.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6418224771817526}]}, {"text": "Efficient solutions to problems of these sort have been given when the constraints on the predictors are sequential).", "labels": [], "entities": []}, {"text": "These solutions can be categorized into the following two frameworks.", "labels": [], "entities": []}, {"text": "Learning global models trains a probabilistic model under the constraints imposed by the domain.", "labels": [], "entities": []}, {"text": "Examples include variations of HMMs, conditional models and sequential variations of Markov random fields ().", "labels": [], "entities": []}, {"text": "The other framework, inference with classifiers, views maintaining constraints and learning classifiers as separate processes.", "labels": [], "entities": []}, {"text": "Various local classifiers are trained without the knowledge of constraints.", "labels": [], "entities": []}, {"text": "The predictions are taken as input on the inference procedure which then finds the best global prediction.", "labels": [], "entities": []}, {"text": "In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally.", "labels": [], "entities": []}, {"text": "Typically, efficient inference procedures in both frameworks rely on dynamic programming (e.g., Viterbi), which works well in sequential data.", "labels": [], "entities": []}, {"text": "However, in many important problems, the structure is more general, resulting in computationally intractable inference.", "labels": [], "entities": []}, {"text": "Problems of these sorts have been studied in computer vision, where inference is generally performed over low level measurements rather than over higher level predictors (.", "labels": [], "entities": []}, {"text": "This work develops a novel inference with classifiers approach.", "labels": [], "entities": []}, {"text": "Rather than being restricted on sequential data, we study a fairly general setting.", "labels": [], "entities": []}, {"text": "The problem is defined in terms of a collection of discrete random variables representing binary relations and their arguments; we seek an optimal assignment to the variables in the presence of the constraints on the binary relations between variables and the relation types.", "labels": [], "entities": []}, {"text": "The key insight to this solution comes from recent techniques developed for approximation algorithms (.", "labels": [], "entities": [{"text": "approximation algorithms", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.9357165098190308}]}, {"text": "Following this work, we model inference as an optimization problem, and show how to cast it as a linear program.", "labels": [], "entities": []}, {"text": "Using existing numerical packages, which are able to solve very large linear programming problems in a very short time 1 , inference can be done very quickly.", "labels": [], "entities": []}, {"text": "Our approach could be contrasted with other ap-proaches to sequential inference or to general Markov random field approaches ().", "labels": [], "entities": []}, {"text": "The key difference is that in these approaches, the model is learned globally, under the constraints imposed by the domain.", "labels": [], "entities": []}, {"text": "In our approach, predictors do not need to be learned in the context of the decision tasks, but rather can be learned in other contexts, or incorporated as background knowledge.", "labels": [], "entities": []}, {"text": "This way, our approach allows the incorporation of constraints into decisions in a dynamic fashion and can therefore support task specific inferences.", "labels": [], "entities": []}, {"text": "The significance of this is clearly shown in our experimental results.", "labels": [], "entities": []}, {"text": "We develop our models in the context of natural language inferences and evaluate it hereon the problem of simultaneously recognizing named entities and relations between them.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe below two experiments on the problem of simultaneously recognizing entities and relations.", "labels": [], "entities": []}, {"text": "In the first, we view the task as a knowledge acquisition task -we let the system read sentences and identify entities and relations among them.", "labels": [], "entities": [{"text": "knowledge acquisition task", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7639192541440328}]}, {"text": "Given that this is a difficult task which may require quite often information beyond the sentence, we consider also a \"forced decision\" task, in which we simulate a question answering situationwe ask the system, say, \"who killed whom\" and evaluate it on identifying correctly the relation and its arguments, given that it is known that somewhere in this sentence this relation is active.", "labels": [], "entities": []}, {"text": "In addition, this evaluation exhibits the ability of our approach to incorporate task specific constraints at decision time.", "labels": [], "entities": []}, {"text": "Our experiments are based on the TREC data set (which consists of articles from WSJ, AP, etc.) that we annotated for named entities and relations.", "labels": [], "entities": [{"text": "TREC data set", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8218269745508829}, {"text": "WSJ", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9715938568115234}, {"text": "AP", "start_pos": 85, "end_pos": 87, "type": "DATASET", "confidence": 0.5051077008247375}]}, {"text": "In order to effectively observe the interaction between relations and entities, we picked 1437 sentences that have at least one active relation.", "labels": [], "entities": []}, {"text": "Among those sentences, there are 5336 entities, and 19048 pairs of entities (binary relations).", "labels": [], "entities": []}, {"text": "Entity labels include 1685 persons, 1968 locations, 978 organizations and 705 others.", "labels": [], "entities": []}, {"text": "Relation labels include 406 located in, 394 work for, 451 orgBased in, 521 live in, 268 kill, and 17007 none.", "labels": [], "entities": []}, {"text": "Note that most pairs of entities have no active relations at all.", "labels": [], "entities": []}, {"text": "Therefore, relation none significantly outnumbers others.", "labels": [], "entities": []}, {"text": "Examples of each relation label and the constraints between a relation variable and its two entity arguments are shown as follows.", "labels": [], "entities": []}, {"text": "In order to focus on the evaluation of our inference procedure, we assume the problem of segmentation (or phrase detection)) is solved, and the entity boundaries are given to us as input; thus we only concentrate on their classifications.", "labels": [], "entities": [{"text": "phrase detection))", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7735505600770315}]}], "tableCaptions": [{"text": " Table 2: Results of Entity Classification", "labels": [], "entities": [{"text": "Entity Classification", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8238066136837006}]}]}