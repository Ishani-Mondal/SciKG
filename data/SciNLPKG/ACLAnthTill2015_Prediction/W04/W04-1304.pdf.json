{"title": [{"text": "Grammatical Inference and First Language Acquisition", "labels": [], "entities": [{"text": "Grammatical Inference", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8742357194423676}, {"text": "First Language Acquisition", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6349490582942963}]}], "abstractContent": [{"text": "One argument for parametric models of language has been learnability in the context of first language acquisition.", "labels": [], "entities": [{"text": "first language acquisition", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.6661208868026733}]}, {"text": "The claim is made that \"logical\" arguments from learnability theory require non-trivial constraints on the class of languages.", "labels": [], "entities": []}, {"text": "Initial formal-isations of the problem (Gold, 1967) are however inapplicable to this particular situation.", "labels": [], "entities": []}, {"text": "In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts.", "labels": [], "entities": []}, {"text": "We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice.", "labels": [], "entities": []}, {"text": "Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria.", "labels": [], "entities": []}, {"text": "We then discuss the applicability of these results to parametric and non-parametric models.", "labels": [], "entities": []}], "introductionContent": [{"text": "For some years, the relevance of formal results in grammatical inference to the empirical question of first language acquisition by infant children has been recognised.", "labels": [], "entities": [{"text": "grammatical inference", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7288856357336044}, {"text": "first language acquisition", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6325072546799978}]}, {"text": "Unfortunately, for many researchers, with a few notable exceptions, this begins and ends with Gold's famous negative results in the identification in the limit paradigm.", "labels": [], "entities": [{"text": "identification", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.9638274908065796}]}, {"text": "This paradigm, though still widely used in the grammatical inference community, is clearly of limited relevance to the issue at hand, since it requires the model to be able to exactly identify the target language even when an adversary can pick arbitrarily misleading sequences of examples to provide.", "labels": [], "entities": []}, {"text": "Moreover, the paradigm as stated has no bounds on the amount of data or computation required for the learner.", "labels": [], "entities": []}, {"text": "In spite of the inapplicability of this particular paradigm, in a suitable analysis there are quite strong arguments that bear directly on this problem.", "labels": [], "entities": []}, {"text": "Grammatical inference is the study of machine learning of formal languages.", "labels": [], "entities": [{"text": "Grammatical inference", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.878460705280304}]}, {"text": "It has avast formal vocabulary and has been applied to a wide selection of different problems, where the \"languages\" understudy can be (representations of) parts of natural languages, sequences of nucleotides, moves of a robot, or some other sequence data.", "labels": [], "entities": []}, {"text": "For any conclusions that we draw from formal discussions to have any applicability to the real world, we must be sure to select, or construct, from the rich set of formal devices available an appropriate formalisation.", "labels": [], "entities": []}, {"text": "Even then, we should be very cautious about making inferences about how the infant child must or cannot learn language: subsequent developments in GI might allow a more nuanced description in which these conclusions are not valid.", "labels": [], "entities": []}, {"text": "The situation is complicated by the fact that the field of grammtical inference, much like the wider field of machine learning in general, is in a state of rapid change.", "labels": [], "entities": []}, {"text": "In this paper we hope to address this problem by justifying the selection of the appropriate learning framework starting by looking at the actual situation the child is in, rather than from an a priori decision about the right framework.", "labels": [], "entities": []}, {"text": "We will not attempt a survey of grammatical inference techniques; nor shall we provide proofs of the theorems we use here.", "labels": [], "entities": []}, {"text": "Arguments based on formal learnability have been used to support the idea of parameter based theories of language.", "labels": [], "entities": []}, {"text": "As we shall see below, under our analysis of the problem these arguments are weak.", "labels": [], "entities": []}, {"text": "Indeed, they are more pertinent to questions about the autonomy and modularity of language learning: the question whether learning of some level of linguistic knowledge -morphology or syntax, for example -can take place in isolation from other forms of learning, such as the acquisition of word meaning, and without interaction, grounding and soon.", "labels": [], "entities": []}, {"text": "Positive results can help us to understand how humans might learn languages by outlining the class of algorithms that might be used by humans, considered as computational systems at a suitable abstract level.", "labels": [], "entities": []}, {"text": "Conversely, negative results might be helpful if they could demonstrate that no algorithms of a certain class could perform the task -in this case we could know that the human child learns his language in some other way.", "labels": [], "entities": []}, {"text": "We shall proceed as follows: after briefly describing FLA, we describe the various elements of a model of learning, or framework.", "labels": [], "entities": [{"text": "FLA", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.7852311134338379}]}, {"text": "We then make a series of decisions based on the empirical facts about FLA, to construct an appropriate model or models, avoiding unnecessary idealisation wherever possible.", "labels": [], "entities": [{"text": "FLA", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.6349185109138489}]}, {"text": "We proceed to some strong negative results, well-known in the GI community that bear on the questions at hand.", "labels": [], "entities": []}, {"text": "The most powerful of these appears to apply quite directly to our chosen model.", "labels": [], "entities": []}, {"text": "We then discuss an interesting algorithm which shows that this can be circumvented, at least fora subclass of regular languages.", "labels": [], "entities": []}, {"text": "Finally, after discussing the possibilities for extending this result to all regular languages, and beyond, we conclude with a discussion of the implications of the results presented for the distinction between parametric and non-parametric models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}