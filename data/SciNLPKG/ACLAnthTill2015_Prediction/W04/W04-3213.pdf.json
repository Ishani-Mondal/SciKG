{"title": [], "abstractContent": [{"text": "We present an unsupervised method for labelling the arguments of verbs with their semantic roles.", "labels": [], "entities": []}, {"text": "Our bootstrapping algorithm makes initial unam-biguous role assignments, and then iteratively updates the probability model on which future assignments are based.", "labels": [], "entities": []}, {"text": "A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model.", "labels": [], "entities": []}, {"text": "We achieve 50-65% reduction in the error rate over an informed baseline, indicating the potential of our approach fora task that has heretofore relied on large amounts of manually generated training data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9742105305194855}]}], "introductionContent": [{"text": "Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g.,).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.8022130727767944}]}, {"text": "In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants.", "labels": [], "entities": [{"text": "labelling the semantic roles of the arguments of a verb", "start_pos": 15, "end_pos": 70, "type": "TASK", "confidence": 0.7497017681598663}]}, {"text": "Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g.,;.", "labels": [], "entities": [{"text": "automatic semantic role labelling", "start_pos": 90, "end_pos": 123, "type": "TASK", "confidence": 0.5533885732293129}]}, {"text": "These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet () or PropBank ( as training data, which are expensive to produce, are limited in size, and may not be representative.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.8896812796592712}, {"text": "PropBank", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.9366587400436401}]}, {"text": "We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.6725945075352987}]}, {"text": "To achieve this, we take a \"bootstrapping\" approach (e.g.,), which initially makes only the role assignments that are unambiguous according to a verb lexicon.", "labels": [], "entities": []}, {"text": "We then iteratively: create a probability model based on the currently annotated semantic roles, use this probability model to assign roles that are deemed to have sufficient evidence, and add the newly labelled arguments to our annotated set.", "labels": [], "entities": []}, {"text": "As we iterate, we gradually both grow the size of the annotated set, and relax the evidence thresholds for the probability model, until all arguments have been assigned roles.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first unsupervised semantic role labelling system applied to general semantic roles in a domain-general corpus.", "labels": [], "entities": []}, {"text": "Ina similar vein of work, Riloff and colleagues) used bootstrapping to learn \"case frames\" for verbs, but their approach has been applied in very narrow topic domains with topic-specific roles.", "labels": [], "entities": []}, {"text": "In other work, has explored unsupervised methods to discover role-slot mappings for verbs, but not to apply this knowledge to label text with roles.", "labels": [], "entities": []}, {"text": "Our approach also differs from earlier work in its novel use of classes of information in backing off to less specific role probabilities (in contrast to using simple subsets of information, as in).", "labels": [], "entities": []}, {"text": "If warranted, we base our decisions on the probability of a role given the verb, the syntactic slot (syntactic argument position), and the noun occurring in that slot.", "labels": [], "entities": []}, {"text": "For example, the assignment to the first argument of sentence (1) above maybe based on . When backing off from this probability, we use statistics over more general classes of information, such as conditioning over the semantic class of the verb instead of the verb itself-for this example, psychological state verbs.", "labels": [], "entities": []}, {"text": "Our approach yields a very simple probability model which emphasizes classbased generalizations.", "labels": [], "entities": []}, {"text": "The first step in our algorithm is to use the verb lexicon to determine the argument slots and the roles available for them.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss the lexicon we use, and our initial steps of syntactic frame matching and \"unambiguous\" role assignment.", "labels": [], "entities": [{"text": "syntactic frame matching", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.6131592293580373}, {"text": "role assignment", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.702685609459877}]}, {"text": "This unambiguous data is leveraged by using those role assignments as the basis for the initial estimates for the probability model described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents the algorithm which brings these two components together, iteratively updating the probability estimates as more and more data is labelled.", "labels": [], "entities": []}, {"text": "In Section 5, we describe details of the materials and methods used for the experiments presented in Section 6.", "labels": [], "entities": []}, {"text": "Our results show a large improvement over an informed baseline.", "labels": [], "entities": []}, {"text": "This kind of unsupervised approach to role labelling is quite new, and we conclude with a discussion of limitations and on-going work in Section 7.", "labels": [], "entities": [{"text": "role labelling", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.772484302520752}]}], "datasetContent": [{"text": "Of over 960K slots we extracted from the corpus, 120K occurred with one of 54 target verbs.", "labels": [], "entities": []}, {"text": "Of these, our validation data consisted of 278 slots, and our test data of 554 slots.", "labels": [], "entities": []}, {"text": "We focus on the analysis of test data; the pattern on the validation data was nearly identical in all respects.", "labels": [], "entities": []}, {"text": "The target slots fall into several categories, depending on the human judgements: argument slots, adjunct slots, and \"bad\" slots (chunking errors).", "labels": [], "entities": []}, {"text": "We report detailed analysis over the slots identified as arguments.", "labels": [], "entities": []}, {"text": "We also report overall accuracy if adjunct and \"bad\" slots are included in the slots to be labelled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994654059410095}]}, {"text": "This comparison is similar to that made by and others, either using arguments as delimited in the FrameNet corpus, or having to automatically locate argument boundaries.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.9313712120056152}]}, {"text": "Furthermore, we report results over individ-ual slot classes (subject, object, indirect object, and PP object), as well as overall slots.", "labels": [], "entities": []}, {"text": "We report results after the \"unambiguous\" data is assigned, and at the end of the algorithm, when no more slots can be labelled.", "labels": [], "entities": []}, {"text": "At either of these steps it is possible for some slots to have been assigned and some to remain unassigned.", "labels": [], "entities": []}, {"text": "Rather than performing a simple precision/recall analysis, we report a finer grained elaboration that gives a more precise picture of the results.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9992163181304932}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.938605546951294}]}, {"text": "For the assigned slots, we report percent correct (of total, not of assigned) and percent incorrect.", "labels": [], "entities": [{"text": "correct", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.897894024848938}]}, {"text": "For the unassigned slots, we report percent \"possible\" (i.e., slots whose candidate list contains the correct role) and percent \"impossible\" (i.e., slots whose candidate list does not contain the correct role-and which may in fact be empty).", "labels": [], "entities": []}, {"text": "All these percent figures are out of all argument slots (for the first set of results), and out of all slots (for the second set); see.", "labels": [], "entities": []}, {"text": "Correctness is determined by the human judgements on the chunked slots, as reported above.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9777488708496094}]}, {"text": "Using our notion of slot class, we compare our results to a baseline that assigns all slots the role with the highest probability for that slot class, , as used in other work.", "labels": [], "entities": []}, {"text": "We are using a very different verb lexicon, corpus, and human standard than in previous research.", "labels": [], "entities": []}, {"text": "The closest work is that of which maps FrameNet roles to a set of 18 thematic roles very similar to our roles, and also operates on a subset of the BNC (albeit manually rather than randomly selected).", "labels": [], "entities": [{"text": "BNC", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.9239577651023865}]}, {"text": "We mention the performance of their method where appropriate below.", "labels": [], "entities": []}, {"text": "However, our results are compared to human annotation of chunked data, while theirs (and other supervised results) are compared to manually annotated full sentences.", "labels": [], "entities": []}, {"text": "Our percentage correct values therefore do not take into account argument constituents that are simply missed by the chunker.", "labels": [], "entities": []}, {"text": "In this section, we focus on argument slots as identified by our human judges (the first panel of results in the table).", "labels": [], "entities": []}, {"text": "There area number of things to note.", "labels": [], "entities": []}, {"text": "First, our performance on these slots is very high, 90.1% correct at the end of the algorithm, with 7.0% incorrect, and delimited arguments, others train, as well as test, only on such arguments.", "labels": [], "entities": []}, {"text": "In our approach, all previously annotated slots are used in the iterative training of the probability model.", "labels": [], "entities": []}, {"text": "Thus, even when we report results on argument slots only, adjunct and \"bad\" slots may have induced errors in their labelling.", "labels": [], "entities": []}, {"text": "only 2.9% left unassigned.", "labels": [], "entities": []}, {"text": "(The latter have null candidate lists.)", "labels": [], "entities": []}, {"text": "This is a 56% reduction in error rate over the baseline.", "labels": [], "entities": [{"text": "error rate", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9901045858860016}]}, {"text": "Second, we see that even after the initial unambiguous role assignment step, the algorithm achieves close to the baseline percent correct.", "labels": [], "entities": [{"text": "role assignment", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7711064517498016}]}, {"text": "Furthermore, over 96% of the initially assigned roles are correct.", "labels": [], "entities": []}, {"text": "This means that much of the work in narrowing down the candidate lists is actually being preformed during frame matching.", "labels": [], "entities": [{"text": "frame matching", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.6958137452602386}]}, {"text": "It is noteworthy that such a simple method of choosing the initial candidates can be so useful, and it would seem that even supervised methods might benefit from employing such an explicit use of the lexicon to narrow down role candidates fora slot.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: An example of frame matching.", "labels": [], "entities": [{"text": "frame matching", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.835881233215332}]}, {"text": " Table 3: Evaluation of test data on 554 identified arguments (see Section 6.2) and on all 672 target slots (see  Section 6.4).", "labels": [], "entities": []}]}