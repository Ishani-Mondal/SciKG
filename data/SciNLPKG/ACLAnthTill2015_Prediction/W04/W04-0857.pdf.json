{"title": [{"text": "Generative Models for Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7252390384674072}]}], "abstractContent": [{"text": "This paper describes the four entries from the University of Utah in the semantic role labeling task of SENSEVAL-3.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.6966821253299713}]}, {"text": "All the entries took a statistical machine learning approach, using the subset of the FrameNet corpus provided by SENSEVAL-3 as training data.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.95234814286232}]}, {"text": "Our approach was to develop a model of natural language generation from semantics , and train the model using maximum likelihood and smoothing.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6770462393760681}]}, {"text": "Our models performed satisfactorily in the competition, and can flexibly handle varying permutations of provided versus inferred information .", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal in the SENSEVAL-3 semantic role labeling task is to identify roles and optionally constituent boundaries for each role, given a natural language sentence, target, and frame.", "labels": [], "entities": [{"text": "SENSEVAL-3 semantic role labeling task", "start_pos": 16, "end_pos": 54, "type": "TASK", "confidence": 0.9012529730796814}]}, {"text": "The Utah approach to this task is to apply machine learning techniques to create a model capable of semantically analyzing unseen sentences.", "labels": [], "entities": []}, {"text": "We have developed a set of generative models) that have the advantages of flexibility, power, and ease of applicability for semi-supervised learning scenarios.", "labels": [], "entities": []}, {"text": "We can supplement any of the generative models with a constituent classifier that determines, given a sentence and parse, which parse constituents are most likely to correspond to a role.", "labels": [], "entities": []}, {"text": "We apply the combination to the \"hard,\" or restricted version of the role labeling task, in which the system is provided only with the sentence, target, and frame, and must determine which sentence constituents to label with roles.", "labels": [], "entities": [{"text": "role labeling task", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7773013214270273}]}, {"text": "We discuss our overall model, the constituent classifier we use in the hard task, and the classifier's use at role-labeling time.", "labels": [], "entities": []}, {"text": "We entered four sets of answers, as discussed in Section 5.", "labels": [], "entities": []}, {"text": "The first two correspond to the \"easy\" task, in which the rolebearing constituents -those parts of the sentence corresponding to a role -are provided to the system with the target and frame.", "labels": [], "entities": []}, {"text": "The second two are variants for the \"hard\" task.", "labels": [], "entities": []}, {"text": "Finally, we discuss Future Work and conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The SENSEVAL-3 committee chose 40 of the most frequent 100 frames from FrameNet II for the competition.", "labels": [], "entities": []}, {"text": "In experiments with validation sets, our algorithm performed better using only the SENSEVAL-3 training data, as opposed to also using sentences from the remaining frames, so all our models were trained only on that data.", "labels": [], "entities": [{"text": "SENSEVAL-3 training data", "start_pos": 83, "end_pos": 107, "type": "DATASET", "confidence": 0.638109028339386}]}, {"text": "We calculated performance using SENSEVAL-3's scoring software.", "labels": [], "entities": []}, {"text": "We submitted two set of answers for each task.", "labels": [], "entities": []}, {"text": "We summarize each system's performance in Table 1.", "labels": [], "entities": []}, {"text": "For the easy task, we used both the grouped (FEG Easy) and first order (FirstOrder Easy) models.", "labels": [], "entities": [{"text": "FEG", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.77239590883255}]}, {"text": "The grouped model performed better on experiments with validation sets, perhaps due to the fact that many frames have a small number of possible  role permutations corresponding to a given number of constituents.", "labels": [], "entities": []}, {"text": "In less artificial conditions this version would be less flexible in incorporating both relevant and irrelevant constituents.", "labels": [], "entities": []}, {"text": "For the hard task, we used only the first order model, due both to its greater flexibility and to the low precision of our classifier: if all positively classified constituents were passed to the group model, the sequence length would be greater than any seen at training time, when only correct constituents are given to the labeler.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9965198040008545}]}, {"text": "We used both the cost sensitive classifier (CostSens Hard ) and the regular constituent classifier to filter constituents (Hard ).", "labels": [], "entities": []}, {"text": "There is a precision/recall tradeoff in using the different classifiers.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9993745684623718}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.984969973564148}]}, {"text": "We were surprised how poorly our labeler was performing on validation sets as we prepared our results.", "labels": [], "entities": []}, {"text": "We found out that our classifier was omitting about 70% of the role-bearing constituents from consideration, because they only matched a parse constituent at a pre-terminal node.", "labels": [], "entities": []}, {"text": "We fixed this bug after submission, learned anew constituent classifier, and used the same role labeler as before.", "labels": [], "entities": []}, {"text": "The improved results are shown in Table 2.", "labels": [], "entities": []}, {"text": "Note that our recall has an upper limit of 85.8% due to mismatches between roles and parse tree constituents.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9989247918128967}]}], "tableCaptions": [{"text": " Table 2: Newer System Scores.", "labels": [], "entities": []}]}