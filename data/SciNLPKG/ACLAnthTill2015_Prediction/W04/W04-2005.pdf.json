{"title": [{"text": "EVALUATING GETARUNS PARSER WITH GREVAL TEST SUITE", "labels": [], "entities": [{"text": "GETARUNS", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.34388482570648193}, {"text": "PARSER", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.6392128467559814}, {"text": "GREVAL TEST SUITE", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.7425337235132853}]}], "abstractContent": [{"text": "GREVAL, the test suite of 500 English sentences taken from SUSANNE Corpus and made available by John Carroll and Ted Briscoe at their website, has been used to test the performance of a symbolic linguistically-based parser called GETARUNS presented in (Delmonte, 2002).", "labels": [], "entities": [{"text": "GREVAL", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9645963311195374}, {"text": "SUSANNE Corpus", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.8145202994346619}]}, {"text": "GETARUNS is a symbolic linguistically-based parser written in Prolog Horn clauses which uses a strong deterministic policy by means of a lookahead mechanism and a WFST.", "labels": [], "entities": [{"text": "GETARUNS", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.6794872879981995}, {"text": "WFST", "start_pos": 163, "end_pos": 167, "type": "DATASET", "confidence": 0.886731743812561}]}, {"text": "The grammar allows the specification of linguistic rules in a highly declarative mode: it works topdown and by making a heavy use of linguistic knowledge may achieve an almost complete deterministic policy: in this sense it is equivalent to an LR parser.", "labels": [], "entities": []}, {"text": "The results obtained fare higher that the ones reported in (Preis, 2003) and this we argue is due basically to the symbolic rule-based approach: we reach 96% precision (coverage) and 84% recall (accuracy).", "labels": [], "entities": [{"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9983561635017395}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9992420673370361}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.7709946632385254}]}, {"text": "We assume that from a psycholinguistic point of view, parsing requires setting up a number of disambiguating strategies, to tell arguments apart from adjuncts and reduce the effects of backtracking.", "labels": [], "entities": [{"text": "parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.9676286578178406}]}, {"text": "To do that the system is based on LFG theoretical framework and uses Grammatical Functions information to help the parser cope with syntactic ambiguity.", "labels": [], "entities": []}, {"text": "In the paper we shall comment on some shortcomings of the Greval corpus annotation and more in general we shall criticize some aspects of the Dependency Structure representation.", "labels": [], "entities": [{"text": "Greval corpus annotation", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.7669230103492737}, {"text": "Dependency Structure representation", "start_pos": 142, "end_pos": 177, "type": "TASK", "confidence": 0.5035137136777242}]}], "introductionContent": [{"text": "In this paper we will present the parser used by the system GETARUN and discuss its performance with the test suite called GREVAL setup by Carroll & Briscoe.", "labels": [], "entities": [{"text": "GETARUN", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8315644860267639}, {"text": "GREVAL", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9684901833534241}]}, {"text": "We will also discuss the mapping algorithm from LFG to Dependency Grammatical Relations (DGRs), which we have been obliged to develop in order to be able to evaluate our parser.", "labels": [], "entities": []}, {"text": "Greval is a benchmark for parser evaluation based on Grammatical Relations in a Head Dependency Structure style output, i.e. a word based HeadDependent flat representation enriched with Grammatical Relation information, where each relation is represented as follows, Relation(introducer,head,dependent) Relation(introducer,head,dependent,deep-relation) where a deep relation is introduced basically for passive constructions, dative shift, and potentially other structures, according to the \"Movement\" approach invoked by chomskians.", "labels": [], "entities": [{"text": "Greval", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9418125748634338}]}, {"text": "The annotation adopted by the authors is a surface level GRs approach where for instance, in cases of Locative Inversion as in sentence 284, (1) Here, in the old days -when they had come to seethe moon or displays of fireworks -sat the king and his court while priests, soldiers, and other members of the party lounged in the smaller alcoves between.", "labels": [], "entities": []}, {"text": "the SUBJect NP 'the king and his court\" is assigned to DOBJ and then receives an additional deeprelation label as NCSUBJ to indicate its original deep structure position.", "labels": [], "entities": [{"text": "DOBJ", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8178414702415466}, {"text": "NCSUBJ", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.967358410358429}]}, {"text": "However the same relation is \"wrongly\" marked as NCSUBJ in a subsequent case of Locative Inversion (the only other one, sentence 445), which we report below, (2) In his stead is a milquetoast version known as the corporation.", "labels": [], "entities": [{"text": "NCSUBJ", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9728710651397705}, {"text": "Locative Inversion", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8287594616413116}]}, {"text": "where the inverted subject NP \"a milquetoast version\" is annotated straightforwardly as NCSUBJ.", "labels": [], "entities": [{"text": "NCSUBJ", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.9667907953262329}]}, {"text": "The inconsistency denounced by this case of double annotation lingers on other types of ambiguous GRs that we will comment below.", "labels": [], "entities": []}, {"text": "In our experiment, for reasons already explained in () and further commented below, we restricted our mapping algorithm to all \"predonly\" f-structures, i.e. only to semantic heads with primary GRs.", "labels": [], "entities": []}, {"text": "This is because we assume that the most difficult task a parser is faced with when parsing a sentence is to operate the argument-adjunct distinction: thus the subset of GRs we will take into account is the following: NCSUBJ, DOBJ, IOBJ, ARG_MOD, XCOMP, CCOMP Difficulties in building up a comparable version of our output, include the inconsistencies present in the non typographical text distributed for the test.", "labels": [], "entities": [{"text": "NCSUBJ", "start_pos": 217, "end_pos": 223, "type": "DATASET", "confidence": 0.877685546875}]}, {"text": "We also had to give up using the internally provided tool for evaluation because our system builds multiword expressions which are almost totally absent in the annotated Gold version.", "labels": [], "entities": []}, {"text": "So even though the authors admit to the need of improving this aspect, its lack makes it impossible for real systems to use automatic evaluation tools.", "labels": [], "entities": []}], "datasetContent": [{"text": "The parser parses 89% of all text top down: then it parses 9.3% of the remaining linguistic material bottom up and adds it up to the parsed portion of the current sentence.", "labels": [], "entities": []}, {"text": "That may produce wrong results in case a list has been partially parsed by the top down parser.", "labels": [], "entities": []}, {"text": "But it produces right results whenever any additional complete subordinate or coordinate sentence structure has been leftover -which constitutes the majority of cases.", "labels": [], "entities": []}, {"text": "Overall almost the whole text -98.3% -is turned into semantically consistent structures which have already undergone Pronominal Binding at sentence level in their DAG structural representation.", "labels": [], "entities": [{"text": "Pronominal Binding", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7438831031322479}, {"text": "DAG structural representation", "start_pos": 163, "end_pos": 192, "type": "TASK", "confidence": 0.6574837267398834}]}, {"text": "We find it very important to remark the fact that the performance of our parser is mainly to be appreciate for the high coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9735646843910217}]}, {"text": "None of the statistically and stochastically based parser reported under    The impression one gets from the performance of statistically and stochastically based parsers is that they are inherently unable to cope with deep linguistic information.", "labels": [], "entities": []}, {"text": "They are certainly impossible to undergo substantial improvements.", "labels": [], "entities": []}, {"text": "On the contrary, rule based parsers would benefit from additional subcategorization frames as in our case: and for all those constructions which require setting up of new additional peripheral rules in the grammar, they would typically increase their coverage, as did our parser.", "labels": [], "entities": []}, {"text": "As a last comment, we started evaluating subsets of GREVAL corpus with the online version of \"Connexor\" dependency parser, on the assumption that that version would be identical or even better than the one commercially available.", "labels": [], "entities": [{"text": "GREVAL corpus", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.904718816280365}]}, {"text": "We did that because this parser is regarded the best dependency parser on the market.", "labels": [], "entities": []}, {"text": "We tried out a subset of 50 sentences, and on a first perusal of the output we discovered that only 40 sentences contained correct, and fully connected representations.", "labels": [], "entities": []}, {"text": "The remaining 10 sentences either presented unconnected heads, or misconnected ones due to wrong attachments.", "labels": [], "entities": []}, {"text": "So, even though word-level parsing maybe more effective as to the number of connections (constituents) safely produced, without leaving off any fragment or skimmed fragment, it is nonetheless faced with the hard task of recomposing clause level control mechanisms which in atop down constituency-based parser are given for granted.", "labels": [], "entities": [{"text": "word-level parsing", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7123329937458038}]}, {"text": "The F-measure derived from our P and R according to the usual formula: 2rp F1(r,p) = ---------r+p is 89.38%, which is by far higher than the 75% reported in) as being the best result obtained by linguistic parsers today.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9920520782470703}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.6378639936447144}]}, {"text": "We are currently experimenting with a \"mildly\" topdown/bottomup version of the parser in which rather than starting from Clause level we search recursively for Arguments and Adjuncts.", "labels": [], "entities": []}, {"text": "In other words, we look for fully semantically interpreted constituents in which choice for argumenthood has already been partially performed.", "labels": [], "entities": []}, {"text": "In addition to collecting Arguments/Adjuncts, the new parser scatters in the output list punctuation marks and coordinate/subordinate words which are deemed responsible to determine clause boundaries.", "labels": [], "entities": []}, {"text": "To that aim, we devised a procedure for clause creation under the restriction that a main tensed Verb constituent complex has been found.", "labels": [], "entities": [{"text": "clause creation", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.8599537312984467}]}, {"text": "This can be iterated on the input list and the procedure may decide to fuse portion of the output list which have been left stranded without independent clause status, and append it to the preceding prospective clause.", "labels": [], "entities": []}, {"text": "Interpretation procedures follows by recovering subcategorization frames for the main tensed verb and assignment of grammatical function and semantic roles takes place.", "labels": [], "entities": []}, {"text": "The Clause level procedure is then followed by an Utterance level procedure that produces simple utterance or complex ones -coordinated or subordinated -according to the Clause input list.", "labels": [], "entities": []}, {"text": "We experimented the new version of the parser with the Greval Corpus and discovered that in some cases it was much slower than the fully topdown version.", "labels": [], "entities": [{"text": "Greval Corpus", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9243700206279755}]}, {"text": "However, we also recovered parsing time in highly ambiguous and complex sentences, where the \"mildly\" bottomup parser actually followed a totally linear behaviour: no increase in computation time resulted and the performance is only conditioned by the number of words/number of argumentsadjuncts/number of clauses to build.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9750490188598633}]}, {"text": "We haven't been able to compute this proportion systematically but will do so in the future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Grammatical Relations produced by GETARUNS with Precision and Recall", "labels": [], "entities": [{"text": "Grammatical Relations", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9171347916126251}, {"text": "GETARUNS", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.6629031300544739}]}, {"text": " Table 3: GR Precisions and Recalls as derived from (Preis, 2003)", "labels": [], "entities": [{"text": "GR Precisions", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6503494679927826}, {"text": "Recalls", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9317029714584351}]}]}