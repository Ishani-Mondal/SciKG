{"title": [], "abstractContent": [{"text": "Accurate dependency recovery has recently been reported fora number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG).", "labels": [], "entities": []}, {"text": "However, overall figures give no indication of a parser's performance on specific constructions, nor how suitable a parser is for specific applications.", "labels": [], "entities": []}, {"text": "In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text.", "labels": [], "entities": [{"text": "object extraction dependencies", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.785355011622111}, {"text": "WSJ text", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.8831889033317566}]}, {"text": "We also show how the parser can be used to parse questions for Question Answering.", "labels": [], "entities": [{"text": "parse questions", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.8859359622001648}, {"text": "Question Answering", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.6970569640398026}]}, {"text": "The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to anew domain, by creating new labelled data at the lexical category level only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992096424102783}]}, {"text": "Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy.", "labels": [], "entities": [{"text": "question parsing", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.6951198130846024}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.8721602559089661}]}], "introductionContent": [{"text": "Several wide-coverage statistical parsers have recently been developed for Combinatory Categorial) and applied to the WSJ Penn Treebank (.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 118, "end_pos": 135, "type": "DATASET", "confidence": 0.9057738780975342}]}, {"text": "One motivation for using CCG is the recovery of the long-range dependencies inherent in phenomena such as coordination and extraction.", "labels": [], "entities": []}, {"text": "Recovery of these dependencies is important for NLP tasks which require semantic interpretation and for processing text which contains a high frequency of such cases, e.g. Wh-questions fed to a Question Answering (QA) system.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.8145885944366456}]}, {"text": "One shortcoming of treebank parsers such as and is that they typically produce phrase-structure trees containing only local syntactic information.", "labels": [], "entities": []}, {"text": "uses post-processing methods to insert \"empty\" nodes into the trees, and use preprocessing methods to determine where discontinuities are likely to appear in the sentence.", "labels": [], "entities": []}, {"text": "In contrast, the CCG parsers detect long-range dependencies as an integral part of the parsing process.", "labels": [], "entities": [{"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9658138155937195}]}, {"text": "The CCG parser used here) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second.", "labels": [], "entities": [{"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9985685348510742}, {"text": "WSJ text", "start_pos": 132, "end_pos": 140, "type": "DATASET", "confidence": 0.9060854613780975}]}, {"text": "Thus the parser should be useful for large-scale NLP tasks.", "labels": [], "entities": []}, {"text": "However, the overall accuracy figures give no indication of the parser's performance on specific constructions, nor how suitable the parser is for specific applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9994246959686279}]}, {"text": "In this paper we give a detailed evaluation for object extraction dependencies and show how the parser can be used to parse questions for QA.", "labels": [], "entities": [{"text": "object extraction dependencies", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.8252641558647156}]}, {"text": "We find that the parser performs well on the object extraction cases found in the Penn Treebank, given the difficulty of the task.", "labels": [], "entities": [{"text": "object extraction", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7292885631322861}, {"text": "Penn Treebank", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9956396818161011}]}, {"text": "In contrast, the parser performs poorly on questions from TREC, due to the small number of questions in the Penn Treebank.", "labels": [], "entities": [{"text": "TREC", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8371058702468872}, {"text": "Penn Treebank", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9942962229251862}]}, {"text": "This motivates the remainder of the paper, in which we describe the creation of new training data consisting of labelled questions.", "labels": [], "entities": []}, {"text": "Crucially, the questions are labelled at the lexical category level only, and not at the derivation level, making the creation of new labelled data relatively easy.", "labels": [], "entities": []}, {"text": "The parser uses a supertagger to assign lexical categories to words, and the supertagger can be adapted to the new question domain by training on the newly created data.", "labels": [], "entities": []}, {"text": "We find that using the original parsing model with the new supertagger model dramatically increases parsing accuracy on TREC questions, producing a parser suitable for use in a QA system.", "labels": [], "entities": [{"text": "parsing", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.9554957747459412}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9276374578475952}, {"text": "TREC questions", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.5780317485332489}]}, {"text": "For evaluation we focus on What questions used in the TREC competitions.", "labels": [], "entities": []}, {"text": "As well as giving an overall evaluation on this test set, we also consider a number of object extraction cases.", "labels": [], "entities": [{"text": "object extraction", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.78374382853508}]}, {"text": "The creation of new training data at the lexical category level alone is a technique which could be used to rapidly port the parser to other domains.", "labels": [], "entities": []}, {"text": "This technique may also be applicable to other lexicalised grammar formalisms, such as Tree Adjoining Grammar ().", "labels": [], "entities": []}], "datasetContent": [{"text": "A development set was created by randomly selecting 171 questions.", "labels": [], "entities": []}, {"text": "For development purposes the remaining 1,000 questions were used for training; these were also used as a final cross-validation training/test set.", "labels": [], "entities": []}, {"text": "The average length of the tokenised questions in the whole corpus is 8.6 tokens.", "labels": [], "entities": []}, {"text": "The lexical category set used by the parser contains all categories which occur at least 10 times in CCGbank, giving a set of 409 categories.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9499368071556091}]}, {"text": "In creating the question corpus we used a small number of new category types, of which 3 were needed to cover common question constructions.", "labels": [], "entities": []}, {"text": "One of these, (S[wq]/(S[dcl]\\NP))/N, applies to What, as in the second example in.", "labels": [], "entities": []}, {"text": "This category does appear in CCGbank, but so infrequently that it is not part of the parser's lexical category set.", "labels": [], "entities": []}, {"text": "Two more apply to question words like did and is; for example, (S[q]/(S[pss]\\NP))/NP applies to is in What instrument is Ray Charles best known for playing?, and (S[q]/PP)/NP applies to is in What city in Florida is Sea World in?.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of What categories in questions", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9848226308822632}]}, {"text": " Table 2: Accuracy of supertagger on dev question data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998019814491272}]}, {"text": " Table 3: Parser category accuracy on dev data", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.7188896536827087}]}]}