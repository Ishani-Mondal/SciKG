{"title": [{"text": "Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based?", "labels": [], "entities": [{"text": "Chinese Part-of-Speech Tagging", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5391691525777181}]}], "abstractContent": [{"text": "Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.", "labels": [], "entities": [{"text": "Chinese part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5599047889312109}]}, {"text": "However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.8255598545074463}, {"text": "word segmentation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.6983994692564011}]}, {"text": "We could perform Chinese POS tagging strictly afterword segmentation (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-at-once approach).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.6637004613876343}, {"text": "afterword segmentation", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7032574862241745}, {"text": "word segmentation", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7315816730260849}, {"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.7472350597381592}]}, {"text": "Also, we could choose to assign POS tags on a word-byword basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based).", "labels": [], "entities": []}, {"text": "This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.5904724299907684}]}, {"text": "We found that while the all-at-once, character-based approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9988585710525513}]}, {"text": "As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.", "labels": [], "entities": [{"text": "Chinese word segmenter", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.5541144410769144}, {"text": "SIGHAN 2003 word segmenters", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.6858099550008774}]}], "introductionContent": [{"text": "Most corpus-based language processing research has focused on the English language.", "labels": [], "entities": [{"text": "corpus-based language processing", "start_pos": 5, "end_pos": 37, "type": "TASK", "confidence": 0.7056934237480164}]}, {"text": "Theoretically, we should be able to just port corpus-based, machine learning techniques across different languages since the techniques are largely language independent.", "labels": [], "entities": []}, {"text": "However, in practice, the special characteristics of different languages introduce complications.", "labels": [], "entities": []}, {"text": "For Chinese in particular, words are not demarcated in a Chinese sentence.", "labels": [], "entities": []}, {"text": "As such, we need to perform word segmentation before we can proceed with other tasks such as part-of-speech (POS) tagging and parsing, since one POS tag is assigned to each Chinese word (i.e., all characters in a Chinese word have the same POS tag), and the leaves of a parse tree fora Chinese sentence are words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7150428295135498}, {"text": "part-of-speech (POS) tagging", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6969884276390076}, {"text": "parsing", "start_pos": 126, "end_pos": 133, "type": "TASK", "confidence": 0.8821905851364136}]}, {"text": "To build a Chinese POS tagger, the following questions naturally arise: (1) Should we perform Chinese POS tagging strictly afterword segmentation in two separate phases (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-at-once approach)?", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.5994013398885727}, {"text": "word segmentation", "start_pos": 211, "end_pos": 228, "type": "TASK", "confidence": 0.7211728096008301}, {"text": "POS tagging", "start_pos": 233, "end_pos": 244, "type": "TASK", "confidence": 0.7417303919792175}]}, {"text": "(2) Should we assign POS tags on a word-byword basis (like in English), making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based)?", "labels": [], "entities": []}, {"text": "This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.5904710590839386}]}, {"text": "We analyze the performance of the different approaches in our attempt to find the best approach.", "labels": [], "entities": []}, {"text": "To our knowledge, our work is the first to systematically investigate such issues in Chinese POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.6522234976291656}]}], "datasetContent": [{"text": "To evaluate the accuracy of our word segmenter, we carried out 10-fold cross validation (CV) on the 250K-word Penn Chinese Treebank (CTB) () version 3.0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9992857575416565}, {"text": "word segmenter", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7198162972927094}, {"text": "cross validation (CV)", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.7553642749786377}, {"text": "Penn Chinese Treebank (CTB) () version 3.0", "start_pos": 110, "end_pos": 152, "type": "DATASET", "confidence": 0.9545237223307291}]}, {"text": "The Java opennlp maximum entropy package from sourceforge 1 was used in our implementation, and training was done with a feature cutoff of 2 and 100 iterations.", "labels": [], "entities": []}, {"text": "The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure ( Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter's output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999017596244812}, {"text": "word segmentation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7527249157428741}, {"text": "recall (R)", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9524537622928619}, {"text": "precision (P)", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.9594305604696274}, {"text": "Fmeasure", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9995635151863098}, {"text": "Recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.6728911399841309}, {"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9991859793663025}]}, {"text": "gives the word segmentation Fmeasure of our word segmenter based on 10-fold CV on the 250K-word CTB.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6555871814489365}, {"text": "Fmeasure", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9756590127944946}]}, {"text": "Our word segmenter achieves an average F-measure of 95.1%.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7043264210224152}, {"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9991436004638672}]}, {"text": "This accuracy compares favorably with 1 http://maxent.sourceforge.net, which reported 94.6% word segmentation F-measure using his full parser without additional lexical features, and about 94.9% 2 word segmentation F-measure using only word boundaries information, no POS tags or constituent labels, but with lexical features derived from a 58K-entry word list.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9993225336074829}, {"text": "word segmentation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.6402890831232071}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.748936116695404}, {"text": "F-measure", "start_pos": 215, "end_pos": 224, "type": "METRIC", "confidence": 0.8334105610847473}]}, {"text": "The average training time taken to train on 90% of the 250K-word CTB was 12 minutes, while testing on 10% of CTB took about 1 minute.", "labels": [], "entities": []}, {"text": "The running times reported in this paper were all obtained on an Intel Xeon 2.4GHz computer with 2GB RAM.", "labels": [], "entities": []}, {"text": "Experim ent Num ber Word Seg F-Measure(%): CTB 10-fold CV word segmentation Fmeasure for our word segmenter As further evaluation, we tested our word segmenter on all the 4 test corpora (CTB, Academia Sinica (AS), Hong Kong CityU (HK), and Peking University (PK)) of the closed track of the 2003 ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff.", "labels": [], "entities": [{"text": "CV word segmentation", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.6215074261029562}, {"text": "Fmeasure", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.5262995958328247}, {"text": "ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff", "start_pos": 296, "end_pos": 370, "type": "TASK", "confidence": 0.6606697184698922}]}, {"text": "For each of the 4 corpora, we trained our word segmenter on only the official released training data of that corpus.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7081150114536285}]}, {"text": "Training was conducted with feature cutoff of 2 and 100 iterations (these parameters were obtained by cross validation on the training set), except for the AS corpus where we used cutoff 3 since the AS training corpus was too big to train with cutoff 2.", "labels": [], "entities": [{"text": "AS corpus", "start_pos": 156, "end_pos": 165, "type": "DATASET", "confidence": 0.6895488798618317}]}, {"text": "shows our word segmenter's Fmeasure (based on the official word segmentation scorer of  We also compared the F-measure of our word segmenter on CTB O , the open category of the CTB corpus, where participants were free to use any available resources and were not restricted to only the official released training data of CTB.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9572208523750305}, {"text": "word segmentation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.6892952769994736}, {"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.992866039276123}, {"text": "CTB O", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.6389430463314056}, {"text": "CTB corpus", "start_pos": 177, "end_pos": 187, "type": "DATASET", "confidence": 0.8821615874767303}, {"text": "CTB", "start_pos": 320, "end_pos": 323, "type": "DATASET", "confidence": 0.9430851936340332}]}, {"text": "On this CTB O task, we used as additional training data the AS training corpus provided by SIGHAN, after converting the AS training corpus to GB encoding.", "labels": [], "entities": [{"text": "AS training corpus", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.6296298106511434}, {"text": "SIGHAN", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.7824546694755554}]}, {"text": "We found that with this additional AS training data added to the original 3 Last ranked participant of SIGHAN CTB (closed) with F-measure 73.2% is not shown in due to space constraint.", "labels": [], "entities": [{"text": "AS", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9811806678771973}, {"text": "SIGHAN CTB", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.7946953475475311}, {"text": "F-measure", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9958674907684326}]}, {"text": "official released CTB training data of SIGHAN, our word segmenter achieved an F-measure of 92.2%, higher than the best reported F-measure in the CTB open task.", "labels": [], "entities": [{"text": "CTB training data", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.7941394249598185}, {"text": "SIGHAN", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.5248485207557678}, {"text": "word segmenter", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7058315128087997}, {"text": "F-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9990324974060059}, {"text": "F-measure", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9765310883522034}]}, {"text": "With sufficient training data, our word segmenter can perform very well.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.753997266292572}]}, {"text": "In our evaluation, we also found that the additional features we introduced in Section 2.2 and the post-processing step consistently improved average word segmentation F-measure, when evaluated on the 4 SIGHAN test corpora in the closed track.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.6476913392543793}, {"text": "F-measure", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.7339733242988586}, {"text": "SIGHAN test corpora", "start_pos": 203, "end_pos": 222, "type": "DATASET", "confidence": 0.7044043640295664}]}, {"text": "The additional features improved F-measure by an average of about 0.4%, and the post-processing step added on top of the use of all features further improved Fmeasure by 0.3% (i.e., fora cumulative total of 0.7% increase in F-measure).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.942415714263916}, {"text": "Fmeasure", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.8541013598442078}, {"text": "F-measure", "start_pos": 224, "end_pos": 233, "type": "METRIC", "confidence": 0.8176248073577881}]}, {"text": "10-fold CV on CTB was carried out again, using unsegmented test sentences as input to the program.", "labels": [], "entities": [{"text": "CTB", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9490022659301758}]}, {"text": "shows the word segmentation Fmeasure, while shows the POS tagging accuracy achieved by this approach.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6649805754423141}, {"text": "Fmeasure", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8467851877212524}, {"text": "POS tagging", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.6763861775398254}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9146517515182495}]}, {"text": "With an allat-once, character-based approach, an average word segmentation F-measure of 95.2% and an average POS tagging accuracy of 91.9% was achieved.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.6468303948640823}, {"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.5317535996437073}, {"text": "POS tagging", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.6889356374740601}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8447611331939697}]}, {"text": "The average training timing was 3 hours, while testing took about 20 minutes.", "labels": [], "entities": [{"text": "timing", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.7548897862434387}]}, {"text": "There is a slight improvement in word segmentation and POS tagging accuracy using this approach, compared to the one-at-a-time, character-based approach.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7576329410076141}, {"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7845231890678406}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.8855941295623779}]}, {"text": "When a paired t-test was carried out at the level of significance 0.01, the all-at-once approach was found to be significantly better than the one-at-a-time approach for POS tagging accuracy, although the difference was insignificant for word segmentation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 170, "end_pos": 181, "type": "TASK", "confidence": 0.8410216867923737}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.8842633962631226}, {"text": "word segmentation", "start_pos": 238, "end_pos": 255, "type": "TASK", "confidence": 0.766008585691452}]}, {"text": "Experim ent Num ber POS Accuracy(%): CTB 10-fold CV POS tagging accuracy using an all-at-once approach However, the time required for training and testing is increased significantly for the all-atonce approach.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9417586922645569}, {"text": "CTB", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9333931803703308}, {"text": "POS tagging", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.49014346301555634}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9806293249130249}]}, {"text": "When efficiency is a major consideration, or if high quality hand-segmented text is available, the one-at-a-time, characterbased approach could indeed be a worthwhile compromise, performing only slightly worse than the all-at-once approach.", "labels": [], "entities": []}, {"text": "summarizes the methods investigated in this paper.", "labels": [], "entities": []}, {"text": "Total testing time includes both word segmentation and POS tagging on 10% of CTB data.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7771302163600922}, {"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7341482788324356}, {"text": "CTB data", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9495396614074707}]}, {"text": "Note that an all-atonce, word-based approach is not applicable as word segmentation requires character features to determine the word boundaries.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7172149121761322}]}], "tableCaptions": [{"text": " Table 1: Summary table on the various methods  investigated for POS tagging", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9208486974239349}]}]}