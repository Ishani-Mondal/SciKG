{"title": [{"text": "A Study of Text Categorization for Model Organism Databases", "labels": [], "entities": []}], "abstractContent": [{"text": "One of the routine tasks for model organism database curators is to identify and associate research articles to database entries.", "labels": [], "entities": [{"text": "model organism database curators", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.630817636847496}]}, {"text": "Such task can be considered as text categorization which has been studied in the general English domain.", "labels": [], "entities": []}, {"text": "The task can be decomposed into two text categoriza-tion subtasks: i) finding relevant articles associating with specific model organisms , and ii) routing the articles to specific entries or specific areas.", "labels": [], "entities": []}, {"text": "In this paper, we investigated the first subtask and designed a study using existing reference information available at four well-known model organism databases and investigated the problem of identifying relevant articles for these organisms.", "labels": [], "entities": []}, {"text": "We used features obtained from abstract text and titles.", "labels": [], "entities": []}, {"text": "Additionally, we studied the determination power of other MEDLINE citation fields (e.g., Authors, MeshHeadings, Journals).", "labels": [], "entities": []}, {"text": "Furthermore, we compared three supervised machine learning techniques on predicting to which organism the article belongs.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the accelerated accumulation of genetic information associated with popularly used genetic model organisms for the human genome project such as laboratory mouse, C. elegans, fruit fly, and Saccharomyces, model organism databases that contain curated genetic information specifically to the associated organism have been initialized and evolved to provide a central place for researchers to seek condense genetic information.", "labels": [], "entities": []}, {"text": "At the same time, a rich amount of genetic and biomedical information associated with these model organisms are published through scientific literature.", "labels": [], "entities": []}, {"text": "One of the routine curation tasks for the database curators is to associate research articles to specific genetic entries in the databases and identify key information mentioned in the articles.", "labels": [], "entities": []}, {"text": "For example, a regular practice of mouse genetic database curators is to scan the current scientific literature, extract and enter the relevant information into databases.", "labels": [], "entities": []}, {"text": "In Saccharomyces Genome Database (SGD, Saccharomyces cerevisiae: http://www.yeastgenome.org), database curators are currently in the process of revising the information associated with the Description field of an entry to ensure that the Description (which usually is a concise summary of the function and biological context of the associated entry) contains the most up-to-date information and is written in a consistent style.", "labels": [], "entities": []}, {"text": "One of the current objectives of WormBase (http://www.wormbase.org) is to systematically curate the C. elegans literature.", "labels": [], "entities": [{"text": "WormBase", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.7070951461791992}]}, {"text": "However, manually scanning scientific articles is a labor intensive task.", "labels": [], "entities": [{"text": "scanning scientific articles", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.8123206893603007}]}, {"text": "Meanwhile, the outcome maybe incomplete, i.e., curators may miss some critical papers.", "labels": [], "entities": []}, {"text": "Additionally, more than 2000 completed references are added daily to MEDLINE alone.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.7538629770278931}]}, {"text": "It seems impossible to be always up-to-date.", "labels": [], "entities": []}, {"text": "The task of associating research articles with specific entries can be decomposed into two subtasks: i) categorizing articles into several categories where articles with the same category are about the same model organism, and ii) associating the articles to specific entries or specific areas.", "labels": [], "entities": []}, {"text": "Finding relevant articles specific to a particular model organism is a case of information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7480790913105011}]}, {"text": "A simple way to retrieve relevant articles about a model organism is to retrieve articles containing terms that represent that organism.", "labels": [], "entities": []}, {"text": "For example, if the term \"C. elegans\" appears in a paper, most likely, the paper is relevant to C. elegans.", "labels": [], "entities": []}, {"text": "Another way is to apply supervised machine learning techniques on a list of category-labeled documents.", "labels": [], "entities": []}, {"text": "In this paper, we designed a study on retrieving relevant articles using MEDLINE reference information obtained from four model organism databases aiming to answer the following questions: \u2022 Can we just use keywords to retrieve relevant MEDLINE references instead of using complicated machine learning techniques?", "labels": [], "entities": []}, {"text": "\u2022 How accurate is the retrieval when we use various MEDLINE fields such as Authors, MeshHeadings etc to retrieve articles?", "labels": [], "entities": []}, {"text": "Which kind of feature representations has the best performance?", "labels": [], "entities": []}, {"text": "\u2022 Which kind of machine learning algorithm is suitable for categorizing the articles to the appropriate categories?", "labels": [], "entities": []}, {"text": "\u2022 How good is the MEDLINE citation information when we used category-labeled documents obtained in the past to predict the category of new documents?", "labels": [], "entities": [{"text": "MEDLINE citation", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.5410798192024231}]}, {"text": "In the following, we first provide background information about applying supervised machine learning techniques on text categorization.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7823897302150726}]}, {"text": "We then describe materials and methods.", "labels": [], "entities": []}, {"text": "Finally, we present our results and discussions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each year from 1990 to 2003, we trained a classifier using citations published in all previous years and tested using citations in the current year.", "labels": [], "entities": []}, {"text": "lists the detail about the training set and the test set for each year.", "labels": [], "entities": [{"text": "detail", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.968319833278656}]}, {"text": "We experimented the following feature representations: stemmed words from AbstractText, stemmed words from Title, Author, MeshHeading, and Journals.", "labels": [], "entities": []}, {"text": "Since some of the MEDLINE fields maybe empty (such as some citations do not contain abstracts), also provides the number of non-applicable references each year fora given feature representation method.", "labels": [], "entities": []}, {"text": "From, we found that every citation has a title.", "labels": [], "entities": []}, {"text": "However, there are about 6.4% of citations (5,647 out of 88,281) that do not have abstracts.", "labels": [], "entities": []}, {"text": "For each feature representation, we applied three supervised learning algorithms (i.e., Na\u00efve Bayes learning, Decision List learning, Support Vector Machine).", "labels": [], "entities": []}, {"text": "For each combination of machine learning algorithm and feature representation, we computed the performance using the F-measure, which is defined as 2*P*R/(P+R), where P is the precision (the number of citations predicted correctly to the total number of citations being predicted) and R is the recall (the number of citations predicted correctly to the total number of citations).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9686510562896729}, {"text": "precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9991200566291809}, {"text": "recall", "start_pos": 294, "end_pos": 300, "type": "METRIC", "confidence": 0.9995641112327576}]}, {"text": "We then sorted the feature representations according to their F-measures and gradually combined them into several complex feature representations.", "labels": [], "entities": []}, {"text": "The feature vector of a complex feature representation is formed by simply combining the feature vector of its members.", "labels": [], "entities": []}, {"text": "For example, suppose the feature vector of feature representation using stemmed words from the title contains an element A and the feature vector of feature representation using stemmed words from the abstract contains an element B, then the feature vector of the complex representation obtained by combining stemmed words from title and stemmed words from abstracts will contain the two elements: Title: A and Abstract: B. These feature representations were then combined with the machine learning algorithm that has the best overall performance to build text categorization classifiers.", "labels": [], "entities": []}, {"text": "Similarly, we evaluated these complex feature representations using citations published in all previous years as training citations and tested using citations published in the current year.", "labels": [], "entities": []}, {"text": "shows the detail F-measure obtained for each combination of machine learning algorithm, year, and feature representation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9950483441352844}]}, {"text": "Among them, Support Vector machine along with stemmed words in abstracts achieved the best F-measure (i.e., 90.5%).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9979698061943054}]}, {"text": "Decision list learning along with stemmed words in titles achieved the second best F-measure (i.e., 90.1%).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9990063309669495}]}, {"text": "Feature representation using Mesh Headings along with Decision list learning or Support Vector machine has the third best F-measure (i.e., 88.7%).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9977070093154907}]}, {"text": "Feature representation using Author combined with Support Vector Machine has an F-measure of 71.8%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9997000694274902}]}, {"text": "Feature representation using Journals has the lowest Fmeasure (i.e., 62.1%).", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9994978904724121}]}, {"text": "From, we can see that Support Vector Machine has the best performance for almost each feature representation.", "labels": [], "entities": []}], "tableCaptions": []}