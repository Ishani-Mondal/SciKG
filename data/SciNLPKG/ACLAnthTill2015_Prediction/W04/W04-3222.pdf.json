{"title": [{"text": "The Leaf Projection Path View of Parse Trees: Exploring String Kernels for HPSG Parse Selection", "labels": [], "entities": [{"text": "HPSG Parse Selection", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7312737107276917}]}], "abstractContent": [{"text": "We present a novel representation of parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree.", "labels": [], "entities": []}, {"text": "This representation allows us to achieve significantly higher accuracy in the task of HPSG parse selection than standard models, and makes the application of string kernels natural.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9967842102050781}, {"text": "HPSG parse selection", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.7761984070142111}]}, {"text": "We define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 110, "end_pos": 130, "type": "TASK", "confidence": 0.809915691614151}]}, {"text": "We apply SVM ranking models and achieve an exact sentence accuracy of 85.40% on the Redwoods corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.745229959487915}, {"text": "Redwoods corpus", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.9804685413837433}]}], "introductionContent": [{"text": "In this work we are concerned with building statistical models for parse disambiguation -choosing a correct analysis out of the possible analyses fora sentence.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.9302761852741241}]}, {"text": "Many machine learning algorithms for classification and ranking require data to be represented as real-valued vectors of fixed dimensionality.", "labels": [], "entities": [{"text": "classification and ranking", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.6953464647134145}]}, {"text": "Natural language parse trees are not readily representable in this form, and the choice of representation is extremely important for the success of machine learning algorithms.", "labels": [], "entities": [{"text": "Natural language parse trees", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6568005755543709}]}, {"text": "For a large class of machine learning algorithms, such an explicit representation is not necessary, and it suffices to devise a kernel function \u00a2 \u00a1 \u00a4 \u00a3 \u00a6 \u00a5 \u00a8 \u00a7 \u00a9 which measures the similarity between inputs \u00a3 and \u00a7 . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features.", "labels": [], "entities": [{"text": "\u00a9", "start_pos": 160, "end_pos": 161, "type": "METRIC", "confidence": 0.8503785133361816}]}, {"text": "In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees, similar to the features of the best performing lexicalized generative parsing models.", "labels": [], "entities": [{"text": "discriminative natural language parsing", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.6474504321813583}, {"text": "generative parsing", "start_pos": 215, "end_pos": 233, "type": "TASK", "confidence": 0.8382433950901031}]}, {"text": "Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models ().", "labels": [], "entities": []}, {"text": "Another approach has been to define tree kernels: for example, in), the allsubtrees representation of parse trees is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees.", "labels": [], "entities": []}, {"text": "Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in ().", "labels": [], "entities": []}, {"text": "Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.7902393043041229}]}, {"text": "A good overview of kernels for structured data can be found in ().", "labels": [], "entities": []}, {"text": "Here we propose anew representation of parse trees which (i) allows the localization of broader useful context, (ii) paves the way for exploring kernels, and (iii) achieves superior disambiguation accuracy compared to models that use tree representations centered around context-free rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9828071594238281}]}, {"text": "Compared to the usual notion of discriminative models (placing classes on rich observed data) discriminative PCFG parsing with plain context free rule features may look naive, since most of the features (in a particular tree) make no reference to observed input at all.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 109, "end_pos": 121, "type": "TASK", "confidence": 0.7086974680423737}]}, {"text": "The standard way to address this problem is through lexicalization, which puts an element of the input on each tree node, so all features do refer to the input.", "labels": [], "entities": []}, {"text": "This paper explores an alternative way of achieving this that gives a broader view of tree contexts, extends naturally to exploring kernels, and performs better.", "labels": [], "entities": []}, {"text": "We represent parse trees as lists of paths (leaf projection paths) from words to the top level of the tree, which includes both the head-path (where the word is a syntactic head) and the non-head path.", "labels": [], "entities": []}, {"text": "This allows us to capture for example cases of non-head dependencies which were also discussed by and were used to motivate large subtree features, such as \"more careful than his sister\" where \"careful\" is analyzed as head of the adjective phrase, but \"more\" licenses the \"than\" comparative clause.", "labels": [], "entities": []}, {"text": "This representation of trees as lists of projection paths (strings) allows us to explore string kernels on these paths and combine them into tree kernels.", "labels": [], "entities": []}, {"text": "We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus ( ).", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8936316668987274}, {"text": "Redwoods corpus", "start_pos": 178, "end_pos": 193, "type": "DATASET", "confidence": 0.9587869048118591}]}, {"text": "HPSG is a modern constraint-based lexicalist (or \"unification\") grammar formalism.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9504364132881165}]}, {"text": "We build discriminative models using Support Vector Machines for ranking.", "labels": [], "entities": []}, {"text": "We compare our proposed representation to previous approaches and show that it leads to substantial improvements inaccuracy.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experimental results using different string kernels and different feature annotation of parse trees. with a linear kernel in that space.", "labels": [], "entities": []}, {"text": "Since the feature maps are not especially expensive for the kernels used here, we chose to solve the problem in its primal form.", "labels": [], "entities": []}, {"text": "We were not aware of the existence of any fast software packages that could solve SVM ranking problems in the dual formulation.", "labels": [], "entities": [{"text": "SVM ranking", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.8677776753902435}]}, {"text": "It is possible to convert the ranking problem into a classification problem using pairs of trees as shown in.", "labels": [], "entities": []}, {"text": "We have taken this approach in more recent work using string kernels requiring very expensive feature maps.", "labels": [], "entities": []}, {"text": "We performed experiments using the version of the Redwoods corpus which was also used in the work of ( . We discarded the unambiguous sentences from the training and test sets.", "labels": [], "entities": [{"text": "Redwoods corpus", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.9799854457378387}]}, {"text": "All models were trained and tested using 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "Accuracy results are reported as percentage of sentences where the correct analysis was ranked first by the model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9946030974388123}]}, {"text": "The structure of the experiments section is as follows.", "labels": [], "entities": []}, {"text": "First we describe the results from a controlled experiment using a limited number of features, and aimed at comparing models using local rule features to models using leaf projection paths in Section 4.1.", "labels": [], "entities": []}, {"text": "Next we describe models using more sophisticated string kernels on projection paths in Section 4.2.", "labels": [], "entities": []}, {"text": "In the present experiments, we have limited the derivation tree node annotation to the features listed in: Annotated features of derivation tree nodes.", "labels": [], "entities": [{"text": "derivation tree node annotation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.7999902963638306}, {"text": "Annotated", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9422479271888733}]}, {"text": "The examples are from one node in the head path of the word let in. are potentially helpful for disambiguation, and incorporating more useful features is a next step for this work.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.9684545397758484}]}, {"text": "However, given the size of the corpus, a single model cannot usefully profit from a large number of features.", "labels": [], "entities": []}, {"text": "Previous work () has explored combining multiple classifiers using different features.", "labels": [], "entities": []}, {"text": "We report results from such an experiment as well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of models using the leaf projec- tion path and rule representations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9857913255691528}]}, {"text": " Table 2: Annotated features of derivation tree  nodes. The examples are from one node in the head  path of the word let in", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the Repetition kernel to", "labels": [], "entities": [{"text": "Repetition", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9249274134635925}]}, {"text": " Table 4: Accuracy of models using projection paths  keyed by le-type or both word and le-type. Numbers  of features are shown in thousands.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.972924530506134}]}]}