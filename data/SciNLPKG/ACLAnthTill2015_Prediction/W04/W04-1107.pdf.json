{"title": [{"text": "Chinese Chunking with another Type of Spec", "labels": [], "entities": [{"text": "Chinese Chunking", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8108562529087067}]}], "abstractContent": [{"text": "Spec is a critical issue for automatic chunking.", "labels": [], "entities": [{"text": "Spec", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7053839564323425}, {"text": "automatic chunking", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.5042550563812256}]}, {"text": "This paper proposes a solution of Chinese chunking with another type of spec, which is not derived from a complete syntactic tree but only based on the un-bracketed, POS tagged corpus.", "labels": [], "entities": [{"text": "Chinese chunking", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.5491285920143127}, {"text": "POS tagged corpus", "start_pos": 166, "end_pos": 183, "type": "DATASET", "confidence": 0.7540734012921652}]}, {"text": "With this spec, a chunked data is built and HMM is used to build the chunker.", "labels": [], "entities": []}, {"text": "TBL-based error correction is used to further improve chunking performance.", "labels": [], "entities": [{"text": "TBL-based error correction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5718146959940592}]}, {"text": "The average chunk length is about 1.38 tokens, F measure of chunking achieves 91.13%, labeling accuracy alone achieves 99.80% and the ratio of crossing brackets is 2.87%.", "labels": [], "entities": [{"text": "F measure", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9855151474475861}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9769495129585266}]}, {"text": "We also find that the hardest point of Chinese chunking is to identify the chunking boundary inside noun-noun sequences 1 .", "labels": [], "entities": [{"text": "Chinese chunking", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.642796665430069}]}], "introductionContent": [{"text": "Abney (1991) has proposed chunking as a useful and relative tractable median stage that is to divide sentences into non-overlapping segments only based on superficial analysis and local information.", "labels": [], "entities": [{"text": "chunking", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.9657918214797974}]}, {"text": "() represent chunking as tagging problem and the CoNLL2000 shared task) is now the standard evaluation task for chunking English.", "labels": [], "entities": []}, {"text": "Their work has inspired many others to study chunking for other human languages.", "labels": [], "entities": [{"text": "chunking", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.9704087376594543}]}, {"text": "Besides the chunking algorithm, spec (the detailed definitions of all chunk types) is another critical issue for automatic chunking development.", "labels": [], "entities": [{"text": "automatic chunking development", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.6378932297229767}]}, {"text": "The well-defined spec can induce the chunker to perform well.", "labels": [], "entities": []}, {"text": "Currently chunking specs are defined as some rules or one program to extract phrases from Treebank such as and) in order to save the cost of manual annotation.", "labels": [], "entities": []}, {"text": "We name it as Treebank-derived spec.", "labels": [], "entities": [{"text": "Treebank-derived spec.", "start_pos": 14, "end_pos": 36, "type": "DATASET", "confidence": 0.9717621604601542}]}, {"text": "However, we find that it is more valuable to compile another type of chunking spec according to the observation from un-bracketed corpus instead of Treebank.", "labels": [], "entities": [{"text": "Treebank", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.9748237729072571}]}, {"text": "Based on the problems of chunking Chinese that are found with our observation, we explain the reason why another type of spec is needed and then propose our spec in which the shortening and extending strategies are used to resolve these problems.", "labels": [], "entities": [{"text": "chunking Chinese", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.8771584630012512}]}, {"text": "We also compare our spec with a Treebank-derived spec which is derived from Chinese Treebank (CTB)).", "labels": [], "entities": [{"text": "Chinese Treebank (CTB))", "start_pos": 76, "end_pos": 99, "type": "DATASET", "confidence": 0.9656049370765686}]}, {"text": "An annotated chunking corpus is built with the spec and then a chunker is also constructed accordingly.", "labels": [], "entities": []}, {"text": "For annotation, we adopt a two-stage processing, in which text is first chunked manually and then the potential inconsistent annotations are checked semi-automatically with a tool.", "labels": [], "entities": []}, {"text": "For the chunker, we use HMM model and TBL (Transform-based Learning) based error correction to further improve chunking performance.", "labels": [], "entities": [{"text": "TBL", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9051206111907959}]}, {"text": "With our spec the overall average length of chunks arrives 1.", "labels": [], "entities": [{"text": "length", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.7999127507209778}]}, {"text": "tokens, in open test, the chunking F measure achieves 91.13% and 95.45% if under-combining errors are not counted.", "labels": [], "entities": [{"text": "F measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.8558351695537567}]}, {"text": "We also find the hardest point of Chinese chunking is to identify the chunking boundary inside a noun-noun sequence.", "labels": [], "entities": [{"text": "Chinese chunking", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6303458958864212}]}, {"text": "In the remainder of this paper section 2 describes some problems in chunking Chinese text, section 3 discusses the reason why another type of spec is needed and proposes our chunking spec, section 4 discusses the annotation of our chunking corpus, section 5 describes chunking model, section 6 gives experiment results, section 7, 8 recall some related work and give our conclusions respectively.", "labels": [], "entities": [{"text": "chunking Chinese text", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8547495603561401}]}], "datasetContent": [{"text": "The performance of chunking is commonly measured with three figures: precision (P), recall (R) and F measure that are defined in CoNLL2000.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9437952637672424}, {"text": "recall (R)", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9611156135797501}, {"text": "F measure", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.989951878786087}, {"text": "CoNLL2000", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.9435034394264221}]}, {"text": "Besides these, we also use two other measurements to evaluate the performance of bracketing and labeling respectively: RCB(ratio of crossing brackets), that is the percentage of the found brackets which cross the correct brackets; LA(labeling accuracy), that is the percentage of the found chunks which have the correct labels.", "labels": [], "entities": [{"text": "RCB", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9827122688293457}, {"text": "LA", "start_pos": 231, "end_pos": 233, "type": "METRIC", "confidence": 0.9973592162132263}, {"text": "accuracy", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.6434754729270935}]}, {"text": "The average length (ALen) of chunks for each type is the average number of tokens in each chunk of given type.", "labels": [], "entities": [{"text": "average length (ALen)", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.8878562092781067}]}, {"text": "The overall average length is the average number of tokens in each chunk.", "labels": [], "entities": []}, {"text": "To be more disinterested, outside tokens (including outside punctuations) are also concerned and each of them is counted as one chunk.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The observation of several common structural ambiguities during Chinese chunking", "labels": [], "entities": [{"text": "Chinese chunking", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.6434578597545624}]}, {"text": " Table 3:The information of data set  Table 4 shows the chunking performance of  close test and open test when HMM and ten folds", "labels": [], "entities": []}, {"text": " Table 4:The overall performance of chunking  As can be seen, the performance of open test  doesn't drop much. For open test, HMM achieves  6.9% F improvement, 3.4% RCB reduction on  baseline; error correction gets another 2.7% F  improvement, 0.3% RCB reduction. Labeling  accuracy is so high even with the baseline, which  indicates that the hard point of chunking is to  identify the boundaries of each chunk.", "labels": [], "entities": [{"text": "F improvement", "start_pos": 145, "end_pos": 158, "type": "METRIC", "confidence": 0.9880560338497162}, {"text": "RCB reduction", "start_pos": 165, "end_pos": 178, "type": "METRIC", "confidence": 0.9236668944358826}, {"text": "F", "start_pos": 228, "end_pos": 229, "type": "METRIC", "confidence": 0.997397780418396}, {"text": "accuracy", "start_pos": 274, "end_pos": 282, "type": "METRIC", "confidence": 0.989203155040741}]}, {"text": " Table 5:The result of each type with our spec  All the chunking errors could be classified into  four types: wrong labeling, under-combining, over- combining and overlapping.", "labels": [], "entities": []}, {"text": " Table 6:The distribution of chunking errors  With comparison we also use some other  learning methods, MBL", "labels": [], "entities": [{"text": "MBL", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.47485285997390747}]}, {"text": " Table 7 shows the result. As seen, without  error correction all these models do not perform  well and our HMM gets the best performance.  MBL  SVM  TBL  HMM  F(%)  85.31  86.25  86.92  88.39  Table 7:Comparison with different algorithms", "labels": [], "entities": [{"text": "MBL  SVM  TBL  HMM", "start_pos": 140, "end_pos": 158, "type": "DATASET", "confidence": 0.6297798901796341}, {"text": "F", "start_pos": 160, "end_pos": 161, "type": "METRIC", "confidence": 0.42633000016212463}]}]}