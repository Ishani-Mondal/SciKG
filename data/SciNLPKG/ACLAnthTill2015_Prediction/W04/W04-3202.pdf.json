{"title": [{"text": "Active Learning and the Total Cost of Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "Active learning (AL) promises to reduce the cost of annotating labeled datasets for trainable human language technologies.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7455124974250793}]}, {"text": "Contrary to expectations, when creating labeled training material for HPSG parse selection and later reusing it with other models, gains from AL maybe negligible or even negative.", "labels": [], "entities": [{"text": "HPSG parse selection", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8048758308092753}]}, {"text": "This has serious implications for using AL, showing that additional cost-saving strategies may need to be adopted.", "labels": [], "entities": []}, {"text": "We explore one such strategy: using a model during annotation to automate some of the decisions.", "labels": [], "entities": []}, {"text": "Our best results show an 80% reduction in annotation cost compared with labeling randomly selected data with a single model.", "labels": [], "entities": []}], "introductionContent": [{"text": "AL methods such as uncertainty sampling or query by committee () have all been shown to dramatically reduce the cost of creating highly informative labeled sets for speech and language technologies.", "labels": [], "entities": [{"text": "uncertainty sampling", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7553988993167877}]}, {"text": "However, experiments using AL assume a model that is fixed ahead of time: the model used in AL is the same one we are currently developing training material for.", "labels": [], "entities": []}, {"text": "For many complex tasks, we are unlikely to have a clear idea how best to model the task at the time of annotation; thus, in practice, we will need to reuse the labeled training material with other models.", "labels": [], "entities": []}, {"text": "In this paper, we show that AL can be brittle: under a variety of natural reuse scenarios (for example, allowing the later model to improve in quality, or else reusing the labeled training material using a different machine learning algorithm) performance of later models can be significantly undermined when training upon material created using AL.", "labels": [], "entities": []}, {"text": "The key to knowing how well one model will be able to use material selected by another is their relatedness -yet there maybe no means to determine this prior to annotation, leading to a chicken-and-egg problem.", "labels": [], "entities": []}, {"text": "Our reusability results thus demonstrate that, additionally, other strategies must be adopted to ensure we reduce the total cost of annotation.", "labels": [], "entities": []}, {"text": "In Osborne and Baldridge (2004), we showed that ensemble models can increase model performance and also produce annotation savings when incorporated into the AL process.", "labels": [], "entities": []}, {"text": "An obvious next step is automating some decisions.", "labels": [], "entities": []}, {"text": "Here, we consider a simple automation strategy that reduces annotation costs independently of AL and examine its effect on reusability.", "labels": [], "entities": []}, {"text": "We find that using both semi-automation and AL with high-quality models can eliminate the performance gap found in many reuse scenarios.", "labels": [], "entities": []}, {"text": "However, for weak models, we show that semi-automation with random sampling is more effective for improving reusability than using it with AL -demonstrating further cause for caution with AL.", "labels": [], "entities": []}, {"text": "Finally, we show that under the standard assumption of reuse by the selecting model, using a strategy which combines AL, ensembling, and semiautomated annotation, we are able to achieve our highest annotation savings to date on the complex task of parse selection for HPSG: an 80% reduction in annotation cost compared with labeling randomly selected data with our best single model.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 248, "end_pos": 263, "type": "TASK", "confidence": 0.9528591930866241}]}], "datasetContent": [{"text": "For all experiments, we used a 20-fold crossvalidation strategy by randomly selecting 10% (roughly 500 sentences) for the test set and selecting samples from the remaining 90% (roughly 4500 sentences) as training material.", "labels": [], "entities": []}, {"text": "Each run of AL begins with a single randomly chosen annotated seed sentence.", "labels": [], "entities": []}, {"text": "At each round, new examples are selected for annotation from a randomly chosen, fixed sized 500 sentence subset according to random selection or uncertainty sampling until models reach certain desired accuracies.", "labels": [], "entities": []}, {"text": "We select 20 examples for annotation at each round, and exclude all examples that have more than 500 parses.", "labels": [], "entities": []}, {"text": "AL results are usually presented in terms of the amount of labeling necessary to achieve given performance levels.", "labels": [], "entities": []}, {"text": "We say that one method is better than another method if, fora given performance level, less annotation is required.", "labels": [], "entities": []}, {"text": "The performance metric used here is parse selection accuracy as described in section 2.3.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.928274005651474}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8977887034416199}]}], "tableCaptions": [{"text": " Table 2: Comparison of various selection and reuse conditions. Values are given for discriminant cost (DC)  and the percent increase (Incr) in cost over use of material selected by the reuser.", "labels": [], "entities": [{"text": "discriminant cost (DC)", "start_pos": 85, "end_pos": 107, "type": "METRIC", "confidence": 0.8488051295280457}, {"text": "Incr)", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.9646852016448975}]}, {"text": " Table 3: Cost for LL-PROD to reach given perfor- mance levels when using n-best automation (NB).", "labels": [], "entities": [{"text": "n-best automation (NB)", "start_pos": 74, "end_pos": 96, "type": "METRIC", "confidence": 0.725678825378418}]}, {"text": " Table 4: Cost for LL-PROD to reach given perfor- mance levels in reuse situations where n-best au- tomation (NB) was used with LL-MRS with uncer- tainty sampling (US) or random sampling (RAND).", "labels": [], "entities": [{"text": "n-best au- tomation (NB)", "start_pos": 89, "end_pos": 113, "type": "METRIC", "confidence": 0.7807169854640961}, {"text": "random sampling (RAND)", "start_pos": 171, "end_pos": 193, "type": "METRIC", "confidence": 0.7530271887779236}]}]}