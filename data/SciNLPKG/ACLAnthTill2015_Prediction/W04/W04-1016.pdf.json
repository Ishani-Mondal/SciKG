{"title": [{"text": "Generic Sentence Fusion is an Ill-Defined Summarization Task", "labels": [], "entities": [{"text": "Generic Sentence Fusion", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6944423119227091}]}], "abstractContent": [{"text": "We report on a series of human evaluations of the task of sentence fusion.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7658343613147736}]}, {"text": "In this task, a human is given two sentences and asked to produce a single coherent sentence that contains only the important information from the original two.", "labels": [], "entities": []}, {"text": "Thus, this is a highly constrained summarization task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9721026420593262}]}, {"text": "Our investigations show that even at this restricted level, there is no measurable agreement between humans regarding what information should be considered important.", "labels": [], "entities": []}, {"text": "We further investigate the ability of separate evaluators to assess summaries, and find similarly disturbing lack of agreement.", "labels": [], "entities": [{"text": "summaries", "start_pos": 68, "end_pos": 77, "type": "TASK", "confidence": 0.9032740592956543}]}], "introductionContent": [], "datasetContent": [{"text": "We perform three types of manual evaluation on the summaries from the previous section.", "labels": [], "entities": []}, {"text": "In the first, the ranked evaluation, we present evaluators with original two document sentences; they also see a list of hypothesis summaries and are asked to rank them relative to one another.", "labels": [], "entities": []}, {"text": "In the second evaluation, the absolute evaluation, evaluators are presented with the reference summary and a hypothesis and are asked to produce an absolute score for the hypothesis.", "labels": [], "entities": []}, {"text": "In the third, the factoid evaluation, we manually inspect the information content of each hypothesis.", "labels": [], "entities": []}, {"text": "In the ranked evaluation, human evaluators are presented with the original two document sentences.", "labels": [], "entities": []}, {"text": "They also see a list of 12 hypothesis summaries: the reference summary, the eight summaries elicited from human subjects, and the three baseline summaries.", "labels": [], "entities": []}, {"text": "They are asked to produce a ranking of the 12 summaries based both on their faithfulness to the original document sentences and on their grammaticality.", "labels": [], "entities": []}, {"text": "They were allowed to assign the same score to two systems if they felt neither was any better (or worse) than the other.", "labels": [], "entities": []}, {"text": "They ranked the systems from 1 (best) to 12 (worst), though typically enough systems performed \"equally well\" that a rank of 12 was not assigned.", "labels": [], "entities": []}, {"text": "Three humans performed this evaluation.", "labels": [], "entities": []}, {"text": "In the absolute evaluation, human evaluators are shown the reference summary and a single hypothesis summary.", "labels": [], "entities": []}, {"text": "In order to partially assuage the issue of humans doing little more than string matching, the reference and hypothesis were shown on separate pages and humans were asked not to go \"back\" during the evaluation.", "labels": [], "entities": [{"text": "string matching", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.7065769881010056}]}, {"text": "Due to time constraints, only three systems were evaluated in this manner, one of the humans (the human output was selected so that it was neither too cut-andpaste nor too generative), the LONGER and COMP systems.", "labels": [], "entities": [{"text": "LONGER", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.8982695937156677}]}, {"text": "Three humans performed this task (each shown a single different system output for each reference summary) and scored outputs on a scale from 1 (best) to 5 (worst).", "labels": [], "entities": []}, {"text": "They were told to deduct points for any information contained in the reference not contained in the hypothesis, any information contained in the hypothesis not contained in the reference, and ungrammaticality.", "labels": [], "entities": []}, {"text": "The third evaluation we perform ourselves, due to its difficulty.", "labels": [], "entities": []}, {"text": "This follows the general rubric described by Nenkova and Passonneau's (2004) pyramid scoring scheme, though it differs in the sense that we base our evaluation not on a reference summary, but on the original two document sentences.", "labels": [], "entities": []}, {"text": "Our methodology is described below.", "labels": [], "entities": []}, {"text": "We assume that we are given the original pair of sentences from the document and the hypothesis summaries for many systems (in our experiments, we used the original reference summary, the outputs of three representative humans, and the LONGER and COMP baselines).", "labels": [], "entities": []}, {"text": "Given this data, we first segment the original pair of sentences into \"factoids\" in the style of.", "labels": [], "entities": []}, {"text": "Then, for each hypothesis summary and each factoid, we indicate whether the summary contained that factoid.", "labels": [], "entities": []}, {"text": "Grammaticality of summary hypotheses enters into the calculation of the factoid agreement numbers.", "labels": [], "entities": []}, {"text": "A system only gets credit fora factoid if its summary contains that factoid in a sufficiently grammatical form that the following test could be passed: given any reasonable question one could pose about this factoid, and given the hypothesis summary, could one answer the question correctly.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "Based on this information, it is possible to select one or more of the outputs as the \"gold standard\" and compare the rest in the pyramid scoring scheme described by.", "labels": [], "entities": []}, {"text": "If only one output is used as the gold standard, then it is sufficient to compute precision and recall against that gold standard, and then use these numbers to compute an F-score, which essentially measures agreement between the chosen gold standard and another hypothesis.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9990284442901611}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9986482262611389}, {"text": "F-score", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.9967901110649109}]}, {"text": "In the remainder of this analysis, when we report an F-score over the factoid, this is calculated when the REF summary is taken as the standard.", "labels": [], "entities": [{"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.997359573841095}, {"text": "REF summary", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.6075418591499329}]}, {"text": "The fundamental question we would like to answer is whether humans agree in terms of what information should be preserved in a summary.", "labels": [], "entities": []}, {"text": "Given our data, there are two ways of looking at this.", "labels": [], "entities": []}, {"text": "Researchers have observed that kappa scores over 0.8 indicate strong agreement, while scores between 0.6 and 0.8 indicate reasonable agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9848677515983582}]}, {"text": "Kappa values below 0.6 indicate little to no agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9618908762931824}]}, {"text": "The kappa values for various combinations of columns are shown in.", "labels": [], "entities": []}, {"text": "As we can see from this table, there is essentially no agreement found anywhere.", "labels": [], "entities": []}, {"text": "The maximum agreement is between HUMAN 2 and HUMAN 3, but even a kappa value of 0.470 is regarded as virtually no agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9580419659614563}, {"text": "HUMAN", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8187122344970703}, {"text": "HUMAN", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.8695983290672302}]}, {"text": "Furthermore, the kappa values comparing the human outputs to the reference outputs is even lower, attaining a maximum of 0.251; again, no agreement.", "labels": [], "entities": []}, {"text": "One is forced to conclude that in the task of generic sentence fusion, people will not produce a summary containing the same information as the original reference sentence, and will not produce summaries that contain the same information as another person in the same situation.", "labels": [], "entities": [{"text": "generic sentence fusion", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.830929676691691}]}, {"text": "Despite: Factoid F-score, absolute score and relative ranking for 6 outputs maries, they will be able to distinguish one as somehow better than another.", "labels": [], "entities": [{"text": "Factoid", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9921541810035706}, {"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.7035948038101196}, {"text": "absolute score", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.9809502959251404}]}, {"text": "Answering this question is the aim of the other two evaluations.", "labels": [], "entities": []}, {"text": "First, we consider the absolute rankings.", "labels": [], "entities": []}, {"text": "Recall that in this evaluation, humans are presented with the reference summary as the gold standard summary.", "labels": [], "entities": []}, {"text": "Since, in addition to grammaticality, this is supposed to measure the correctness of information preservation, it is reasonable to compare these numbers to the F-scores that can be computed based on the factoid evaluation.", "labels": [], "entities": [{"text": "information preservation", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.7223305404186249}, {"text": "F-scores", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9763038754463196}]}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "For the first column (F-Score), higher numbers are better; for the second and third columns, lower scores are better.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9971103668212891}]}, {"text": "We can see that the evaluation prefers the human output to the outputs of either of the systems.", "labels": [], "entities": []}, {"text": "However, the factoid scoring prefers the COMP model to the LONGER model, though the Absolute scoring rates them in the opposite direction.", "labels": [], "entities": [{"text": "LONGER", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.7497137784957886}, {"text": "Absolute scoring", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.6467309445142746}]}, {"text": "As we can see from the Relative column in Table 4, human elicited summaries are consistently preferred to any of the others.", "labels": [], "entities": []}, {"text": "This is good news: even if people cannot agree on what information should go into a summary, they at least prefer human written summaries to others.", "labels": [], "entities": []}, {"text": "After the human elicited summaries, there is a relatively large jump to the LONGER baseline, which is unfortunately preferred to the REFERENCE summary.", "labels": [], "entities": [{"text": "LONGER baseline", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.9272542893886566}, {"text": "REFERENCE", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.987820029258728}]}, {"text": "After the reference summary, there are two large jumps, first to the document compression model and then to the DROPSTOP baseline.", "labels": [], "entities": [{"text": "DROPSTOP baseline", "start_pos": 112, "end_pos": 129, "type": "DATASET", "confidence": 0.7954084277153015}]}, {"text": "However, when comparing the relative scores to the F-Score, we see that, again, the factoid metric prefers the COMP model to the LONGER model, but this is not reflected in the relative scoring metric.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9611513614654541}]}], "tableCaptions": [{"text": " Table 2: Factoid-based evaluation scheme for the sentence pair \"Connecting Point has taken leadership by volume,", "labels": [], "entities": []}, {"text": " Table 3: Agreement (kappa) scores for different  combinations of systems and humans", "labels": [], "entities": [{"text": "Agreement (kappa) scores", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.9363248229026795}]}, {"text": " Table 4: Factoid F-score, absolute score and relative  ranking for 6 outputs", "labels": [], "entities": [{"text": "Factoid", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9009038209915161}, {"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.5795828700065613}, {"text": "absolute score", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.9746187329292297}]}]}