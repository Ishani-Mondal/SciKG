{"title": [{"text": "An Enhanced Model for Chinese Word Segmentation and Part-of-speech Tagging", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.5828214685122172}, {"text": "Part-of-speech Tagging", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.6990552842617035}]}], "abstractContent": [{"text": "This paper will present an enhanced probabilistic model for Chinese word segmentation and part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6074467500050863}, {"text": "part-of-speech (POS) tagging", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.6497517704963685}]}, {"text": "The model introduces the information of Chinese word length as one of its features to reach a more accurate result.", "labels": [], "entities": []}, {"text": "And in addition, the model also achieves the integration of segmentation and POS tagging.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.9632078409194946}, {"text": "POS tagging", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.737946093082428}]}, {"text": "After presenting the model, this paper will give a brief discussion on how to solve the problems in statistics and how to further integrate Chinese Named Entity Recognition into the model.", "labels": [], "entities": [{"text": "Chinese Named Entity Recognition", "start_pos": 140, "end_pos": 172, "type": "TASK", "confidence": 0.6852780431509018}]}, {"text": "Finally, some figures of experiments and comparisons will be reported, which shows that the accuracy of word segmentation is 97.09%, and the accuracy of POS tagging is 98.77%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9994673132896423}, {"text": "word segmentation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7176930010318756}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9992666840553284}, {"text": "POS tagging", "start_pos": 153, "end_pos": 164, "type": "TASK", "confidence": 0.7828479707241058}]}], "introductionContent": [{"text": "Generally, Chinese Lexical Analysis consists of two phases; one is word segmentation and the other is part-of-speech(POS) tagging.", "labels": [], "entities": [{"text": "Chinese Lexical Analysis", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.5943935513496399}, {"text": "word segmentation", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7408042401075363}, {"text": "part-of-speech(POS) tagging", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6853689789772034}]}, {"text": "Rule -based approach and statistic -based approach are two dominant ways in natural language processing, as well as Chinese Lexical Analysis.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.644345611333847}, {"text": "Chinese Lexical Analysis", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.7268926898638407}]}, {"text": "This paper will only focus on the later one.", "labels": [], "entities": []}, {"text": "Hence, our model is called a probabilistic model.", "labels": [], "entities": []}, {"text": "Scanning through the researches in this field before, we have just found two points at which the performance of a Chinese word segmentation and POS tagging system could get better.", "labels": [], "entities": [{"text": "Chinese word segmentation and POS tagging", "start_pos": 114, "end_pos": 155, "type": "TASK", "confidence": 0.6209345559279124}]}, {"text": "One is the on the system architecture, and the other is from the Machine Learning theory.", "labels": [], "entities": []}, {"text": "First, the traditional way of Chinese Lexical Analysis simply regards the word segmentation and POS tagging as two separated phases.", "labels": [], "entities": [{"text": "Chinese Lexical Analysis", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.5996661980946859}, {"text": "word segmentation", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.6973714232444763}, {"text": "POS tagging", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.772627055644989}]}, {"text": "Each one of them has its own algorithms and models.", "labels": [], "entities": []}, {"text": "Dividing the whole process into two independent parts can lower the complexity of the design of system, but decrease the performance as well, because the two are fully integrated when a human processing a sentence.", "labels": [], "entities": []}, {"text": "Fortunately, many researchers have already noticed it, and recent projects pay more attention on the integration of word segmentation and POS tagging, such as's pseudo trigram integrated model,'s analyzer which incorporates backward Dynamic Programming and A* algorithm,]'s 'Divide and Conquer integration',]'s hierarchical hidden Markov model and soon.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7342527210712433}, {"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.754006028175354}, {"text": "Divide and Conquer integration", "start_pos": 275, "end_pos": 305, "type": "TASK", "confidence": 0.8004379272460938}]}, {"text": "The experiments given by these papers also showed a great potential of the integrated models.", "labels": [], "entities": []}, {"text": "Besides the system architecture, another point should be noticed.", "labels": [], "entities": []}, {"text": "A probabilistic model of word segmentation and POS tagging can be regarded as an instance of Machine Learning.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7624143064022064}, {"text": "POS tagging", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.7563540637493134}]}, {"text": "In Machine Learning, the feature extraction is the most important aspect, and far more important than a learning algorithm.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7225160300731659}]}, {"text": "In the models nowadays, it seems that the features for Chinese Lexical Analysis area little too simple . Most of them take tag sequences, or word frequencies as the distinguishing features and ignore the other useful information that are provided by Chinese itself.", "labels": [], "entities": [{"text": "Chinese Lexical Analysis", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6339975694815317}]}, {"text": "In this paper, we will present an enhanced, not too complex, model for word segmentation and POS tagging, which will not only inherit the merit of an integrated model, but also take anew feature (word length) into account.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7931036949157715}, {"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.8488732576370239}]}, {"text": "The second part of this paper will describe the model, including the input, output, and some assumptions.", "labels": [], "entities": []}, {"text": "The third part will give some brief discussion about the model on some issues like data sparseness and Named Entity Recognition.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.6653504967689514}]}, {"text": "In the final part, the results of our experiments will be reported.", "labels": [], "entities": []}], "datasetContent": [{"text": "This paper has presented an enhanced probabilistic model of Chinese Lexical Analysis, which introduces word length as one of the features and achieves the integration of word segmentation, Named Entity Recognition and POS tagging.", "labels": [], "entities": [{"text": "Chinese Lexical Analysis", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7428735891977946}, {"text": "word segmentation", "start_pos": 170, "end_pos": 187, "type": "TASK", "confidence": 0.6866491436958313}, {"text": "Named Entity Recognition", "start_pos": 189, "end_pos": 213, "type": "TASK", "confidence": 0.5663418074448904}, {"text": "POS tagging", "start_pos": 218, "end_pos": 229, "type": "TASK", "confidence": 0.7749678790569305}]}, {"text": "At last, we will briefly give the results of our experiments.", "labels": [], "entities": []}, {"text": "In the previous experiments, we have compared many simple probabilistic models for Chinese word segmentation and POS tagging, and found that the system using maximum word frequency as segmentation strategy and forward tri-gram Markov model as POS tagging model (MWF + FTMM) reaches the best performance.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.5911030570665995}, {"text": "POS tagging", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.7868110537528992}, {"text": "POS tagging", "start_pos": 243, "end_pos": 254, "type": "TASK", "confidence": 0.7149584889411926}, {"text": "FTMM", "start_pos": 268, "end_pos": 272, "type": "METRIC", "confidence": 0.7697785496711731}]}, {"text": "Our comparisons will be done between the MWF+FTMM and the enhance model with trigram assumption.", "labels": [], "entities": [{"text": "FTMM", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.5677066445350647}]}, {"text": "PTA (by sentence): the number of correctly tagged sentences divided by the number of correctly segmented sentences in corpus.", "labels": [], "entities": [{"text": "PTA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8512018918991089}]}, {"text": "A correctly tagged sentence is a sentence whose words are all correctly segmented and tagged.", "labels": [], "entities": []}, {"text": "Total (by sentence): WSA * PTA.", "labels": [], "entities": [{"text": "WSA", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.5221030116081238}, {"text": "PTA", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.8803231120109558}]}, {"text": "Named entity considered or not: When named entity is not considered, all the unknown words in corpus are deleted before evaluation.", "labels": [], "entities": []}, {"text": "Otherwise, nothing is done on the corpus.", "labels": [], "entities": []}, {"text": "According to the results above.1,.2,.3,.4), the new enhanced model does better than the MWF + FTMM in every field.", "labels": [], "entities": [{"text": "FTMM", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.38055166602134705}]}, {"text": "Introducing the word length into a Chinese word segmentation and POS tagging system seems effective.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6133630176385244}, {"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7148108035326004}]}, {"text": "This paper just focuses on the pure probabilistic model for word segmetation and POS tagging.", "labels": [], "entities": [{"text": "word segmetation", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.786016196012497}, {"text": "POS tagging", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.8751955032348633}]}, {"text": "It can be predicted that, with more disambiguation strategies, such as some rule based approaches, being implemented into the new model to achieve a multi-engine system, the performance will be further improved.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4.1: The accuracy by word,  with named entity not considered", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995269775390625}]}, {"text": " Table 4.2: The accuracy by word,  with named entity considered", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9994630217552185}]}]}