{"title": [{"text": "Semantic Role Labeling by Tagging Syntactic Chunks *", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8296265800793966}, {"text": "Tagging Syntactic Chunks", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8145693937937418}]}], "abstractContent": [{"text": "In this paper, we present a semantic role la-beler (or chunker) that groups syntactic chunks (i.e. base phrases) into the arguments of a predicate.", "labels": [], "entities": []}, {"text": "This is accomplished by casting the semantic labeling as the classification of syntactic chunks (e.g. NP-chunk, PP-chunk) into one of several classes such as the beginning of an argument (B-ARG), inside an argument (I-ARG) and outside an argument (O).", "labels": [], "entities": []}, {"text": "This amounts to tagging syntactic chunks with semantic labels using the IOB representation.", "labels": [], "entities": []}, {"text": "The chunker is realized using support vector machines as one-versus-all classifiers.", "labels": [], "entities": []}, {"text": "We describe the representation of data and information used to accomplish the task.", "labels": [], "entities": []}, {"text": "We participate in the \"closed challenge\" of the CoNLL-2004 shared task and report results on both development and test sets.", "labels": [], "entities": [{"text": "CoNLL-2004 shared task", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.8003526528676351}]}], "introductionContent": [{"text": "In semantic role labeling the goal is to group sequences of words together and classify them by using semantic labels.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.6884024937947592}]}, {"text": "For meaning representation the predicate-argument structure that exists inmost languages is used.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.822265237569809}]}, {"text": "In this structure a word (most frequently a verb) is specified as a predicate, and a number of word groups are considered as arguments accompanying the word.", "labels": [], "entities": []}, {"text": "In this paper, we select support vector machines (SVMs)) to implement the semantic role classifiers, due to their ability to handle an extremely large number of (overlapping) features with quite strong generalization properties.", "labels": [], "entities": []}, {"text": "Support vector machines for semantic role chunking were first used in ( as word-by-word (W-by-W) classifiers.", "labels": [], "entities": [{"text": "semantic role chunking", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7152566711107889}]}, {"text": "The system was then applied to the constituent-by-constituent (C-by-C) classification in).", "labels": [], "entities": []}, {"text": "In (), several extensions to the basic system have been proposed, extensively studied and systematically compared to other systems.", "labels": [], "entities": []}, {"text": "In this paper, we implement a system that classifies syntactic chunks (i.e. base phrases) instead of words or the constituents derived from syntactic trees.", "labels": [], "entities": []}, {"text": "This system is referred to as the phraseby-phrase (P-by-P) semantic role classifier.", "labels": [], "entities": []}, {"text": "We participate in the \"closed challenge\" of the CoNLL-2004 shared task and report results on both development and test sets.", "labels": [], "entities": [{"text": "CoNLL-2004 shared task", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.8003526528676351}]}, {"text": "A detailed description of the task, data and related work can be found in).", "labels": [], "entities": []}], "datasetContent": [{"text": "The data provided for the shared task is apart of the February 2004 release of the PropBank corpus.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.9633566737174988}]}, {"text": "It consists of sections from the Wall Street Journal part of the Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal part of the Penn Treebank", "start_pos": 33, "end_pos": 78, "type": "DATASET", "confidence": 0.9495119675993919}]}, {"text": "All experiments were carried out using Sections 15-18 for training Section-20 for development and Section-21 for testing.", "labels": [], "entities": []}, {"text": "The results were evaluated for precision, recall and F \u03b2=1 numbers using the srl-eval.pl script provided by the shared task organizers.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9996647834777832}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9995872378349304}, {"text": "F \u03b2=1 numbers", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.9499244332313538}]}, {"text": "In these experiments we used only the base features to compare the two approaches.", "labels": [], "entities": []}, {"text": "illustrates the overall performance on the dev set.", "labels": [], "entities": []}, {"text": "Although both systems were trained using the same number of sentences, the actual number of training examples in each case were quite different.", "labels": [], "entities": []}, {"text": "Those numbers are presented in.", "labels": [], "entities": []}, {"text": "It is clear that P-by-P method uses much less data for the same number of sentences.", "labels": [], "entities": []}, {"text": "Despite this we particularly note a considerable improvement in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9993228912353516}]}, {"text": "Actually, the data reduction was not without a cost.", "labels": [], "entities": [{"text": "data reduction", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.6935984939336777}]}, {"text": "Some arguments have been missed as they do not align with the base phrase chunks due to inconsistencies in semantic annotation and due to errors in automatic base phrase chunking.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.6903562148412069}]}, {"text": "The percentage of this misalignment was around 2.5% (over the dev set).", "labels": [], "entities": []}, {"text": "We observed that nearly 45% of the mismatches were for the \"outside\" chunks.", "labels": [], "entities": []}, {"text": "Therefore, sequences of words with outside tags were not collapsed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of W-by-W and P-by-P methods.  Both systems use the base features provided (i.e. no fea- ture engineering is done). Results are on dev set.", "labels": [], "entities": []}, {"text": " Table 2: Number of sentences and unique training exam- ples in each method.  Method Sentences Training Examples  P-by-P  19K  347K  W-by-W  19K  534K", "labels": [], "entities": []}]}