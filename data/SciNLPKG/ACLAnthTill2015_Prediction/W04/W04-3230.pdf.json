{"title": [{"text": "Applying Conditional Random Fields to Japanese Morphological Analysis", "labels": [], "entities": [{"text": "Applying", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9035672545433044}, {"text": "Japanese Morphological Analysis", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6397466659545898}]}], "abstractContent": [{"text": "This paper presents Japanese morphological analysis based on conditional random fields (CRFs).", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.6390212575594584}]}, {"text": "Previous work in CRFs assumed that observation sequence (word) boundaries were fixed.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.9510608911514282}]}, {"text": "However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible.", "labels": [], "entities": []}, {"text": "We show how CRFs can be applied to situations where word boundary ambiguity exists.", "labels": [], "entities": []}, {"text": "CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis.", "labels": [], "entities": [{"text": "statistical Japanese morphological analysis", "start_pos": 71, "end_pos": 114, "type": "TASK", "confidence": 0.6103003174066544}]}, {"text": "First, flexible feature designs for hierarchical tagsets become possible.", "labels": [], "entities": []}, {"text": "Second, influences of label and length bias are minimized.", "labels": [], "entities": []}, {"text": "We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task.", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6639443834622701}]}, {"text": "Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conditional random fields (CRFs) () applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features.", "labels": [], "entities": []}, {"text": "They are considered to be the state-of-the-art framework to date.", "labels": [], "entities": []}, {"text": "Empirical successes with CRFs have been reported recently in part-of-speech tagging (), shallow parsing (, named entity recognition, Chinese word segmentation ( ), and Information Extraction ().", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.747616320848465}, {"text": "shallow parsing", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.8184081614017487}, {"text": "named entity recognition", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.614074726899465}, {"text": "Chinese word segmentation", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.6395388046900431}, {"text": "Information Extraction", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.8325969278812408}]}, {"text": "Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label * At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech).", "labels": [], "entities": [{"text": "NTT Communication Science Laboratories, 2-4", "start_pos": 154, "end_pos": 197, "type": "DATASET", "confidence": 0.8865557312965393}]}, {"text": "However, word boundaries are not clear in non-segmented languages.", "labels": [], "entities": []}, {"text": "One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7138575613498688}]}, {"text": "In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists.", "labels": [], "entities": []}, {"text": "CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g.,) or with maximum entropy Markov models (MEMMs) (e.g., ().", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6079874634742737}]}, {"text": "First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types.", "labels": [], "entities": []}, {"text": "These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing.", "labels": [], "entities": [{"text": "HMMs", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.8062618374824524}, {"text": "unknown word guessing", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.5975001056989034}]}, {"text": "Second, as mentioned in the literature, MEMMs could evade neither from label bias () nor from length bias (a bias occurring because of word boundary ambiguity).", "labels": [], "entities": []}, {"text": "Easy sequences with low entropy are likely to be selected during decoding in MEMMs.", "labels": [], "entities": []}, {"text": "The consequence is serious especially in Japanese morphological analysis due to hierarchical tagsets as well as word boundary ambiguity.", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6352873345216116}]}, {"text": "The key advantage of CRFs is their flexibility to include a variety of features while avoiding these bias.", "labels": [], "entities": []}, {"text": "In what follows, we describe our motivations of applying CRFs to Japanese morphological analysis (Section 2).", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.6118908822536469}]}, {"text": "Then, CRFs and their parameter estimation are provided (Section 3).", "labels": [], "entities": []}, {"text": "Finally, we discuss experimental results (Section 4) and give conclusions with possible future directions (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs.", "labels": [], "entities": [{"text": "Kyoto University Corpus ver 2.0 (KC)", "start_pos": 77, "end_pos": 113, "type": "DATASET", "confidence": 0.9362641796469688}, {"text": "RWCP Text Corpus (RWCP)", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.9079780677954356}]}, {"text": "Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in.", "labels": [], "entities": []}, {"text": "One of the advantages of CRFs is that they are flexible enough to capture many correlated features, including overlapping and non-independent features.", "labels": [], "entities": []}, {"text": "We thus use as many features as possible, which could not be used in HMMs.", "labels": [], "entities": []}, {"text": "summarizes the set of feature templates used in the KC data.", "labels": [], "entities": [{"text": "KC data", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.7210093587636948}]}, {"text": "The templates for RWCP are essentially the same as those of KC except for the maximum level of POS subcatgeories.", "labels": [], "entities": [{"text": "RWCP", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.5290955305099487}]}, {"text": "Word-level templates are employed when the words are lexicalized, i.e., those that belong to particle, auxiliary verb, or suffix . For an unknown word, length of the word, up to 2 suffixes/prefixes and character types are used as the features.", "labels": [], "entities": []}, {"text": "We use all features observed in the lattice without any cut-off thresholds.", "labels": [], "entities": []}, {"text": "also includes the number of features in both data sets.", "labels": [], "entities": []}, {"text": "We evaluate performance with the standard Fscore (F \u03b2=1 ) defined as follows: where Recall = # of correct tokens # of tokens in test corpus P recision = # of correct tokens # of tokens in system output . In the evaluations of F-scores, three criteria of correctness are used: seg: (only the word segmentation is evaluated), top: (word segmentation and the top level of POS are evaluated), and all: (all information is used for evaluation).", "labels": [], "entities": [{"text": "Fscore", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9163511395454407}, {"text": "Recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9937745332717896}, {"text": "word segmentation", "start_pos": 330, "end_pos": 347, "type": "TASK", "confidence": 0.6782822906970978}]}, {"text": "The hyperparameters C for L1-CRFs and L2-CRFs are selected by cross-validation.", "labels": [], "entities": []}, {"text": "Experiments are implemented in C++ and executed on Linux with XEON 2.8 GHz dual processors and 4.0 Gbyte of main memory.", "labels": [], "entities": []}, {"text": "show experimental results using KC and RWCP respectively.", "labels": [], "entities": [{"text": "RWCP", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9243561625480652}]}, {"text": "The three F-scores (seg/top/all) for our CRFs and a baseline bi-gram HMMs are listed.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9949980974197388}]}], "tableCaptions": [{"text": " Table 1: Details of Data Set", "labels": [], "entities": [{"text": "Details of Data Set", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.6462336704134941}]}]}