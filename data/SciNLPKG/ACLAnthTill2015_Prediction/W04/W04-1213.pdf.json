{"title": [{"text": "Introduction to the Bio-Entity Recognition Task at JNLPBA", "labels": [], "entities": [{"text": "Bio-Entity Recognition Task", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.9146259625752767}, {"text": "JNLPBA", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.8792296648025513}]}], "abstractContent": [{"text": "We describe here the JNLPBA shared task of bio-entity recognition using an extended version of the GENIA version 3 named entity corpus of MEDLINE abstracts.", "labels": [], "entities": [{"text": "bio-entity recognition", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6816675513982773}, {"text": "GENIA version 3 named entity corpus of MEDLINE abstracts", "start_pos": 99, "end_pos": 155, "type": "DATASET", "confidence": 0.8754387034310235}]}, {"text": "We provide background information on the task and present a general discussion of the approaches taken by participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bio-entity recognition aims to identify and classify technical terms in the domain of molecular biology that correspond to instances of concepts that are of interest to biologists.", "labels": [], "entities": [{"text": "Bio-entity recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8584800958633423}]}, {"text": "Examples of such entities include the names of proteins, genes and their locations of activity such as cells or organism names as shown in.", "labels": [], "entities": []}, {"text": "Entity recognition is a core component technology in several higher level information access tasks such as information extraction (template filling), summarization and question answering.", "labels": [], "entities": [{"text": "Entity recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8406502902507782}, {"text": "information extraction (template filling)", "start_pos": 107, "end_pos": 148, "type": "TASK", "confidence": 0.7652416030565897}, {"text": "summarization", "start_pos": 150, "end_pos": 163, "type": "TASK", "confidence": 0.9887556433677673}, {"text": "question answering", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8766875863075256}]}, {"text": "These tasks aim to help users find structure in unstructured text data and aid in finding relevant factual information.", "labels": [], "entities": []}, {"text": "This is becoming increasingly important with the massive increase in reported results due to high throughput experimental methods.", "labels": [], "entities": []}, {"text": "Bio-entity recognition by computers remains a significantly challenging task.", "labels": [], "entities": [{"text": "Bio-entity recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9585873484611511}]}, {"text": "Despite good progress in newswire entity recognition (e.g.) that has led to 'near human' levels of performance, measured in the high 90s for Fscore, similar methods have not performed so well in the bio-domain leaving an accuracy gap of some 30 points of Fscore.", "labels": [], "entities": [{"text": "newswire entity recognition", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.5884481072425842}, {"text": "Fscore", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.7732330560684204}, {"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9988045692443848}, {"text": "Fscore", "start_pos": 255, "end_pos": 261, "type": "METRIC", "confidence": 0.7544515132904053}]}, {"text": "ing consistently annotated human training data with a large number of classes, etc.", "labels": [], "entities": []}, {"text": "In order to make progress it is becoming clear that several points need to be considered: (1) extension of feature sets beyond the lexical level (part of speech, orthography etc.) and use of higher-levels of linguistic knowledge such as dependency relations, (2) potential for re-use of external domain knowledge resources such as gazetteers and ontologies, (3) improved quality control methods for building annotation collections, (4) fine grained error analysis beyond the F-score statistics.", "labels": [], "entities": [{"text": "F-score", "start_pos": 475, "end_pos": 482, "type": "METRIC", "confidence": 0.9594470858573914}]}, {"text": "The JNLPBA shared task 1 is an open challenge task and as such we allowed participants to use whatever methodology and knowledge sources they liked in the bio-entity task.", "labels": [], "entities": [{"text": "JNLPBA shared task 1", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8187325596809387}]}, {"text": "The systems were evaluated on a common benchmark data set using a common evaluation method.", "labels": [], "entities": []}, {"text": "Although it is not directly possible to compare systems due to the diversity of resources used the F-score results provide an approximate indication of how useful each method is.", "labels": [], "entities": [{"text": "F-score", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9862672090530396}]}], "datasetContent": [{"text": "The 2,000 abstracts of the GENIA corpus version 3.02 which had already been made publicly available were formatted for IOB2 notation and made available as training materials.", "labels": [], "entities": [{"text": "GENIA corpus version 3.02", "start_pos": 27, "end_pos": 52, "type": "DATASET", "confidence": 0.9564856886863708}]}, {"text": "For testing, additional 404 abstracts were randomly selected from an unpublished set of the GENIA corpus and the annotations were re-checked by a biologist.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.975304901599884}]}, {"text": "The training set consists of abstracts retrieved from the MEDLINE database with MeSH terms 'human', 'blood cells' and 'transcription factors', and their publication year ranges over 1990\u223c1999.", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9451781511306763}]}, {"text": "Most parts of the test set include abstracts retrieved with the same set of MeSH terms, and their publication year ranges over 1978\u223c2001.", "labels": [], "entities": [{"text": "MeSH terms", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.8461365401744843}]}, {"text": "To seethe effect of publication year, the test set was roughly divided into four set (which represents an old age from the viewpoint of the models that will be trained using the training set), 1990-1999 set (which represents the same age as the training set), 2000-2001 set (which represents anew age compared to the training set) and S/1998-2001 set (which represents roughly anew age in a super domain).", "labels": [], "entities": []}, {"text": "The last subset represents a super domain and the abstracts was retrieved with MeSH terms, 'blood cells' and 'transcription factors' (without 'human') 2 . illustrates the size of the data sets shows the number of entities annotated in each data set 3 . As seen in the table, the annotation density of proteins increases over the ages significantly, whereas the annotation density of DNAs and RNAs increases in the 1990-1999 set and slightly decreases in the 2000-2001 set.", "labels": [], "entities": []}, {"text": "This tendency roughly corresponds to the expansion in the subject area as a whole that can be estimated from statistics on the MeSH terms introduced in each age shown in.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.7243511080741882}]}, {"text": "This observation suggests that the density of mention of a class of entities in academic papers is affected by the amount of interest the entity receives in each age.", "labels": [], "entities": []}, {"text": "shows the ratio of annotated structures in each set.", "labels": [], "entities": []}, {"text": "In accordance with our expectation, the 1990-1999 set has the most similar annotation trait with the training set.", "labels": [], "entities": []}, {"text": "set is also similar to the training set, but the 1978-1989 set had quite a different distribution of entity classes.", "labels": [], "entities": []}, {"text": "The variation of domain does not seem to make any significant difference to the distribution of entities mentioned.", "labels": [], "entities": []}, {"text": "One reason maybe the large fraction of abstracts from the same domain in the super domain set.", "labels": [], "entities": []}, {"text": "In fact, among 206 abstracts in the super domain set, 140 abstracts (69%) are also from the same domain.", "labels": [], "entities": []}, {"text": "It also corresponds to the fraction in the whole MEDLINE database: among 9,362 abstracts that can be retrieved with MeSH terms, 'blood cells' and 'transcription factors', 6,297 abstracts (67%) can also be retrieved with MeSH terms 'human', 'blood cells' and 'transcription factors'.", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.9522611200809479}]}, {"text": "To  In the example, \"T-and B-lymphocyte\" is annotated as one structure but involves two entity names, \"T-lymphocyte\" and \"B-lymphocyte\", whereas \"lymphocytes\" is annotated as one and involves as many entity names.", "labels": [], "entities": []}, {"text": "Results are given as F-scores using a modified version of the CoNLL evaluation script and are defined as F = (2P R)/(P + R), where P denotes Precision and R Recall.", "labels": [], "entities": [{"text": "CoNLL evaluation script", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.8503118554751078}, {"text": "F = (2P R)/(P + R)", "start_pos": 105, "end_pos": 123, "type": "METRIC", "confidence": 0.7791324466466903}, {"text": "Precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9913111925125122}, {"text": "Recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.7278268337249756}]}, {"text": "P is the ratio of the number of correctly found NE chunks to the number of found NE chunks, and R is the ratio of the number of correctly found NE chunks to the number of true NE chunks.", "labels": [], "entities": [{"text": "R", "start_pos": 96, "end_pos": 97, "type": "METRIC", "confidence": 0.9664008617401123}]}, {"text": "The script outputs three sets of F-scores according to exact boundary match, right and left boundary matching.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9247984290122986}, {"text": "exact boundary match", "start_pos": 55, "end_pos": 75, "type": "METRIC", "confidence": 0.9141308466593424}]}, {"text": "In the right boundary matching only right boundaries of entities are considered without matching left boundaries and vice versa.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Basic statistics for the data sets", "labels": [], "entities": []}, {"text": " Table 3: MeSH terms in each age (#/year)", "labels": [], "entities": [{"text": "MeSH", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6805475354194641}]}, {"text": " Table 5: Performance of each participating system and a baseline model (BL) (recall / precision /  F-score)", "labels": [], "entities": [{"text": "baseline model (BL)", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.7014333724975585}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9966699481010437}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.8165666460990906}, {"text": "F-score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.8866543769836426}]}]}