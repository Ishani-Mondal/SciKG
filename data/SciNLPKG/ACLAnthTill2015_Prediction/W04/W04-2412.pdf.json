{"title": [{"text": "Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.6308885216712952}]}], "abstractContent": [{"text": "In this paper we describe the CoNLL-2004 shared task: semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.6953558524449667}]}, {"text": "We introduce the specification and goal of the task, describe the data sets and evaluation methods, and present a general overview of the systems that have contributed to the task, providing comparative description.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years there has been an increasing interest in semantic parsing of natural language, which is becoming a key issue in Information Extraction, Question Answering, Summarization, and, in general, in all NLP applications requiring some kind of semantic interpretation.", "labels": [], "entities": [{"text": "semantic parsing of natural language", "start_pos": 57, "end_pos": 93, "type": "TASK", "confidence": 0.8386946678161621}, {"text": "Information Extraction", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.8396390080451965}, {"text": "Question Answering", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.8430491089820862}, {"text": "Summarization", "start_pos": 172, "end_pos": 185, "type": "TASK", "confidence": 0.9828999638557434}]}, {"text": "The shared task of CoNLL-2004 1 concerns the recognition of semantic roles, for the English language.", "labels": [], "entities": [{"text": "recognition of semantic roles", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.884433850646019}]}, {"text": "We will refer to it as Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.7725723485151926}]}, {"text": "Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence.", "labels": [], "entities": []}, {"text": "In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be extracted (see fora detailed example).", "labels": [], "entities": []}, {"text": "Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.", "labels": [], "entities": [{"text": "Manner", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9256170392036438}]}, {"text": "Most existing systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels.", "labels": [], "entities": [{"text": "automatic semantic role labeling", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.5749979615211487}]}, {"text": "Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments.", "labels": [], "entities": [{"text": "recognition and labeling of arguments", "start_pos": 82, "end_pos": 119, "type": "TASK", "confidence": 0.6748826801776886}]}, {"text": "Regarding the learning component of the systems, we find pure probabilistic models;, Maximum Entropy (), generative models (, Decision Trees (, and Support Vector Machines ().", "labels": [], "entities": []}, {"text": "There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.", "labels": [], "entities": []}, {"text": "For instance, in; Hacioglu and, a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks.", "labels": [], "entities": [{"text": "IOB sequence tagging", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.6916707952817281}]}, {"text": "Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems:) and FrameNet ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9449833631515503}, {"text": "FrameNet", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.8188460469245911}]}, {"text": "In the CoNLL-2004 shared task we concentrate on the PropBank corpus, which is the Penn Treebank corpus enriched with predicate-argument structures.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9526824951171875}, {"text": "Penn Treebank corpus", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.9901047150293986}]}, {"text": "It addresses predicates expressed by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates.", "labels": [], "entities": []}, {"text": "A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations.", "labels": [], "entities": [{"text": "Treebank functional tags", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.8958670298258463}, {"text": "PropBank", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9039143323898315}]}, {"text": "To date, the best results reported on the PropBank correspond to a F 1 measure slightly over 83, when using the gold standard parse trees from Penn Treebank as the main source of information ().", "labels": [], "entities": [{"text": "PropBank", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9705881476402283}, {"text": "F 1 measure", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.990334709485372}, {"text": "Penn Treebank", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.811312735080719}]}, {"text": "This performance drops to 77 when areal parser is used instead.", "labels": [], "entities": []}, {"text": "Comparatively, the best SRL system based solely on shallow syntactic information () performs more than 15 points below.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9780067205429077}]}, {"text": "Although these results are not directly comparable to the ones obtained in the CoNLL-2004 shared task (different datasets, different version of PropBank, etc.) they give an idea about the state-of-the art results on the task.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 144, "end_pos": 152, "type": "DATASET", "confidence": 0.9422914981842041}]}, {"text": "The challenge for CoNLL-2004 shared task is to come up with machine learning strategies which address the SRL problem on the basis of only partial syntactic information, avoiding the use of full parsers and external lexico-semantic knowledge bases.", "labels": [], "entities": [{"text": "SRL problem", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9165450930595398}]}, {"text": "The annotations provided for the development of systems include, apart from the argument boundaries and role labels, the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the general setting of the task.", "labels": [], "entities": []}, {"text": "Section 3 provides a detailed description of training, development and test data.", "labels": [], "entities": []}, {"text": "Participant systems are described and compared in section 4.", "labels": [], "entities": []}, {"text": "In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets.", "labels": [], "entities": [{"text": "SRL", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9917640089988708}]}, {"text": "Finally, section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "A baseline rate was computed for the task.", "labels": [], "entities": []}, {"text": "It was produced by a system developed by Erik Tjong Kim Sang, from the University of Antwerp, Belgium.", "labels": [], "entities": []}, {"text": "The baseline processor finds semantic roles based on the following seven rules: \u2022 Tag target verb and successive particles as V.", "labels": [], "entities": []}, {"text": "\u2022 Tag modal verbs in target verb chunk as AM-MOD.", "labels": [], "entities": []}, {"text": "\u2022 Tag first NP before target verb as A0.", "labels": [], "entities": []}, {"text": "\u2022 Tag first NP after target verb as A1.", "labels": [], "entities": [{"text": "Tag", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9419323205947876}]}, {"text": "\u2022 Tag that, which and who before target verb as R-A0.", "labels": [], "entities": []}, {"text": "\u2022 Switch A0 and A1, and R-A0 and R-A1 if the target verb is part of a passive VP chunk.", "labels": [], "entities": []}, {"text": "A VP chunk is considered in passive voice if it contains a form of to be and the verb does not end in ing.", "labels": [], "entities": []}, {"text": "presents the overall results obtained by the ten participating systems, on the development and test sets.", "labels": [], "entities": []}, {"text": "The best performance was obtained by the SVMbased IOB tagger of (, which almost reached the performance of 70 in F 1 on the test.", "labels": [], "entities": [{"text": "SVMbased IOB tagger", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.6537323097387949}, {"text": "F 1", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9539504945278168}]}, {"text": "The seven best systems obtained F 1 scores in the range of 60-70, and only three systems scored below that.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9850736459096273}]}, {"text": "Comparing the results across development and test corpora, most systems experienced a decrease in performance between 1.5 and 3 points.", "labels": [], "entities": []}, {"text": "As in previous editions of the shared task, we attribute this behavior to a greater difficulty of the test set instead of an overfitting effect.", "labels": [], "entities": []}, {"text": "Interestingly, the three systems performing below 60 in the development set did not experienced this decrease.", "labels": [], "entities": []}, {"text": "In fact () and () even improved the results on the test set.", "labels": [], "entities": []}, {"text": "details the performance of systems for the A0-A4 arguments, on the test set.", "labels": [], "entities": []}, {"text": "Consistently, the best performing system of the task also outperforms all other systems on these semantic roles.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of the preprocessing modules on the de- velopment and test sets. Named Entity figures are based  on the CoNLL-2003 test set.", "labels": [], "entities": [{"text": "CoNLL-2003 test set", "start_pos": 122, "end_pos": 141, "type": "DATASET", "confidence": 0.9818331400553385}]}, {"text": " Table 5: Overall precision, recall and F 1 rates obtained by  the ten participating systems in the CoNLL-2004 shared  task on the development and test sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9997156262397766}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9997226595878601}, {"text": "F 1 rates", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9863336682319641}]}]}