{"title": [{"text": "Priors in Bayesian Learning of Phonological Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a Bayesian procedure for un-supervised learning of phonological rules from an unlabeled corpus of training data.", "labels": [], "entities": []}, {"text": "Like Goldsmith's Linguistica program (Goldsmith, 2004b), whose output is taken as the starting point of this procedure , our learner returns a grammar that consists of a set of signatures, each of which consists of a set of stems and a set of suffixes.", "labels": [], "entities": []}, {"text": "Our grammars differ from Linguistica's in that they also contain a set of phonological rules, specifically insertion, deletion and substitution rules, which permit our grammars to collapse far more words into a signature than Linguistica can.", "labels": [], "entities": []}, {"text": "Interestingly, the choice of Bayesian prior turns out to be crucial for obtaining a learner that makes linguistically appropriate generalizations through a range of different sized training corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised learning presents unusual challenges to the field of computational linguistics.", "labels": [], "entities": []}, {"text": "In supervised systems, the task of learning can often be restricted to finding the optimal values for the parameters of a pre-specified model.", "labels": [], "entities": []}, {"text": "In contrast, an unsupervised learning system must often propose the structure of the model itself, as well as the values for any parameters in that model.", "labels": [], "entities": []}, {"text": "In general, there is a trade-off between the structural complexity of a model and its ability to explain a set of data.", "labels": [], "entities": []}, {"text": "One way to deal with this trade-off is by using Bayesian learning techniques, where the objective function used to evaluate the overall goodness of a system takes the form Pr(H) where Pr(H) is the prior probability of the hypothesized model H, and Pr(D|H) is the likelihood of the data D given that model.", "labels": [], "entities": []}, {"text": "Ina Bayesian system, we want to find the hypothesis H for which Pr(H)Pr(D|H) is highest (or, equivalently, where \u2212 log Pr(H) \u2212 log Pr(D|H) is lowest).", "labels": [], "entities": []}, {"text": "While calculating the likelihood of the data given a particular hypothesis is generally straightforward, the more difficult question in Bayesian learning is how to determine the prior probabilities of various hypotheses.", "labels": [], "entities": []}, {"text": "In this paper, we compare the results of using two different prior distributions for an unsupervised learning task in the domain of morpho-phonology.", "labels": [], "entities": []}, {"text": "Our goal is to learn transformation rules of the form x \u2192 y / C, where x and y are individual characters (or the empty character \ud97b\udf59) and C is some representation of the context licensing the transformation.", "labels": [], "entities": []}, {"text": "Our input is an existing segmentation of words from the Penn Treebank () into stems and suffixes.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.994498610496521}]}, {"text": "This segmentation is provided by the Linguistica morphological analyzer, itself an unsupervised algorithm.", "labels": [], "entities": []}, {"text": "Using the transformation rules we learn, we are able to output anew segmentation that more closely matches our linguistic intuitions.", "labels": [], "entities": []}, {"text": "We are not the first to apply Bayesian learning techniques for unsupervised learning of morphology and phonology.", "labels": [], "entities": []}, {"text": "Several other researchers have also pursued these methods, usually within a Minimum Description Length (MDL) framework.", "labels": [], "entities": []}, {"text": "In MDL approaches, \u2212 log Pr(H) is taken to be proportional to the length of H in some standard encoding, and \u2212 log Pr(D|H) is the length of D using the encoding specified by H. MDL-based systems have been relatively successful for tasks including word segmentation (, morphological Since we use ordinary text, rather than phonological transcriptions, as input, the rules we learn are really spelling rules, not phonological rules.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 247, "end_pos": 264, "type": "TASK", "confidence": 0.8012977838516235}]}, {"text": "We believe that the work discussed here would be equally applicable, and possibly more successful, with phonological transcriptions.", "labels": [], "entities": []}, {"text": "However, since we wish to have an entirely unsupervised system and we require a morphological segmentation as input, we are currently limited by the capabilities of Linguistica, which requires standard textual input.", "labels": [], "entities": []}, {"text": "For the remainder of this paper, we use \"phonology\" and \"phonological rules\" in abroad sense to include orthography as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments with learning phonological rules, we used two different corpora obtained from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9959737062454224}]}, {"text": "The larger corpus contains the words from sections 2-21 of the treebank, filtered to remove most numbers, acronyms, and words containing puctuation.", "labels": [], "entities": []}, {"text": "This corpus consists of approximately 900,000 tokens.", "labels": [], "entities": []}, {"text": "The smaller corpus is simply the first 100,000 words from the larger corpus.", "labels": [], "entities": []}, {"text": "We ran each corpus through the Linguistica program to obtain an initial morphological segmentation.", "labels": [], "entities": []}, {"text": "Statistics on the results of this segmentation are shown in the left half of.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9658598899841309}]}, {"text": "\"Singleton signatures\" are those containing a single stem, and \"Non-\ud97b\udf59 stems\" refers to stems in a signature other than the \ud97b\udf59\ud97b\udf59\ud97b\udf59 signature, i.e. those stems that combine with at least one non-\ud97b\udf59 suffix.", "labels": [], "entities": []}, {"text": "The original function we used to evaluate the utility of our grammars was an MDL prior very similar to the one described by.", "labels": [], "entities": []}, {"text": "This prior is simply the number of bits required to describe the grammar using a fairly straightforward encoding.", "labels": [], "entities": []}, {"text": "The encoding essentially lists all the suffixes in the grammar along with pointers to each one; then lists the phonological rules with their pointers; then lists all the signatures.", "labels": [], "entities": []}, {"text": "Each signature is a list of stems and their pointers, and a list of pointers to suffixes.", "labels": [], "entities": []}, {"text": "Each exceptional stem also has  a pointer to a phonological rule.", "labels": [], "entities": []}, {"text": "Our algorithm considered a total of 11 possible transformations in the small corpus and 40 in the large corpus, but using this prior, only a single type of transformation appeared in any rule in the final grammar: e \u2192 \ud97b\udf59, with seven contexts in the small corpus and eight contexts in the large corpus.", "labels": [], "entities": []}, {"text": "In analyzing why our algorithm failed to accept any other types of rules, we realized that there were several problems with the MDL prior.", "labels": [], "entities": []}, {"text": "Consider what happens to the overall evaluation when two signatures are collapsed.", "labels": [], "entities": []}, {"text": "In general, the likelihood of the corpus will go down, because the stem and suffix probabilities in the combined signature will not fit the true probabilities of the words as well as two separate signatures could.", "labels": [], "entities": []}, {"text": "For large corpora like the ones we are using, this likelihood drop can be quite large.", "labels": [], "entities": []}, {"text": "In order to counterbalance it, there must be a large gain in the prior.", "labels": [], "entities": []}, {"text": "But now look at, which shows the effects of adding all they \u2192 i rules to the grammar for the large corpus under the MDL prior.", "labels": [], "entities": []}, {"text": "The first two lines give the number of signatures and stems in each grammar.", "labels": [], "entities": []}, {"text": "The next line shows the total length (in bits) of each grammar, and this value is then broken down into three different components: the overhead caused by listing the signatures and their suffixes, the length of the stem list (not including the length required to specify exceptions to rules), and the length of the phonological component (including both rules and exception specifications).", "labels": [], "entities": []}, {"text": "Finally, we have the negative log likelihood under each grammar and the total MDL cost (grammar plus likelihood).", "labels": [], "entities": []}, {"text": "As expected, the likelihood term for the grammar  with y \u2192 i rules has increased, indicating a drop in the probability of the corpus under this grammar.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9762520790100098}]}, {"text": "But notice that the total grammar size has also increased, leading to an overall evaluation that is worse than for the original grammar.", "labels": [], "entities": []}, {"text": "There are two main reasons for this increase in grammar size.", "labels": [], "entities": []}, {"text": "Initially, the more puzzling of the two is the fact that the number of bits required to list all the stems has increased, despite the fact that the number of stems has decreased due to reanalyzing some pairs of stems into single stems.", "labels": [], "entities": []}, {"text": "It turns out that this effect is due to the encoding used for stems, which is simply a bitwise encoding of each character in the stem.", "labels": [], "entities": []}, {"text": "This encoding means that longer stems require longer descriptions.", "labels": [], "entities": []}, {"text": "When reanalysis requires shifting a character from a suffix onto the entire set of stems in a signature (as in {certif, empt, hurr} \u00d7 {ied, y} \u2192 {certify, empty, hurry} \u00d7 {\ud97b\udf59, ed}), there can be a large gain in description length simply due to the extra characters in the stems.", "labels": [], "entities": []}, {"text": "If the number of stems eliminated through reanalysis is high enough (as it is for thee \u2192 \ud97b\udf59 rules), this stem length effect will be outweighed.", "labels": [], "entities": []}, {"text": "But when only a few stems are eliminated relative to the number that get longer, the overall length of the stem list increases.", "labels": [], "entities": []}, {"text": "However, even without the stem list, the grammar with y \u2192 i rules would still be slightly longer than the grammar without them.", "labels": [], "entities": []}, {"text": "In this case, the reason in that under our MDL prior, it is quite efficient to encode a signature and its suffixes.", "labels": [], "entities": []}, {"text": "Therefore the grammar reduction caused by removing a few signatures is not enough to outweigh the increase caused by adding a few phonological rules.", "labels": [], "entities": []}, {"text": "Using these observations as a guideline, we redesigned our prior by assigning a fixed cost to each stem and increasing the overhead cost for signatures.", "labels": [], "entities": []}, {"text": "The new overhead function is equal to the sum of the lengths of all the suffixes in the signature, times a constant factor.", "labels": [], "entities": []}, {"text": "This function means there is more incentive to collapse two signatures that share several suffixes, such as \ud97b\udf59e.ed.er.ing\ud97b\udf59/\ud97b\udf59\ud97b\udf59.ed.er.ing\ud97b\udf59, than to collapse signatures sharing only a single suffix, such as \ud97b\udf59ing.s\ud97b\udf59/\ud97b\udf59\ud97b\udf59.ing\ud97b\udf59.", "labels": [], "entities": []}, {"text": "This behavior is exactly what we want, since these shorter pairs are more likely to be accidental.", "labels": [], "entities": []}, {"text": "shows the effects of adding they \u2192 i rules under this new prior.", "labels": [], "entities": []}, {"text": "The starting grammar is somewhat different from the one in, because more rules have already been added by the time they \u2192 i rules are considered.", "labels": [], "entities": []}, {"text": "The important point, however, is that the cost of each component of the grammar changes in the direction we expect it to, and the total grammar cost is reduced enough to more than makeup for the loss in likelihood.", "labels": [], "entities": []}, {"text": "With this new prior, our algorithm was more successful, learning from the large corpus the three major transformations for English (e \u2192 \ud97b\udf59, \ud97b\udf59 \u2192 e, and y \u2192 i) with a total of 22 contexts.", "labels": [], "entities": []}, {"text": "Eight of these rules, such as \ud97b\udf59 \u2192 e / V xs# and y \u2192 i / CyeC, had no exceptions.", "labels": [], "entities": []}, {"text": "Of the remaining rules, the exceptions to six of the rules were correctly analyzed stems (for example, unhappy + ly \u2192 unhappily and necessary + ly \u2192 necessarily but sly + ly \u2192 slyly), while the remaining eight rules contained misanalyzed exceptions (such as overse + er \u2192 overseer, which was listed as an exception to the rule e \u2192 \ud97b\udf59 / CeeC, rather than being reanalyzed as oversee + er).", "labels": [], "entities": []}, {"text": "In the small corpus, no y \u2192 i rules were learned due to the fact that no similar signatures attesting to these rules were found.", "labels": [], "entities": []}, {"text": "Using these phonological rules, a total of 31 signatures in the small corpus and 57 signatures in the large corpus were collapsed, with subsequent reanalysis of 225 and 528 stems, respectively.", "labels": [], "entities": []}, {"text": "This represents 7-10% of all the non-\ud97b\udf59 stems.", "labels": [], "entities": []}, {"text": "The final grammars are summarized in the right half of Table 1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Grammatical Analysis of our Corpora", "labels": [], "entities": [{"text": "Grammatical Analysis", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8782102465629578}]}]}