{"title": [{"text": "A New Chinese Natural Language Understanding Architecture Based on Multilayer Search Mechanism", "labels": [], "entities": []}], "abstractContent": [{"text": "A classical Chinese Natural Language Understanding (NLU) architecture usually includes several NLU components which are executed with some mechanism.", "labels": [], "entities": [{"text": "Chinese Natural Language Understanding (NLU)", "start_pos": 12, "end_pos": 56, "type": "TASK", "confidence": 0.6478696508066994}]}, {"text": "A new Multilayer Search Mechanism (MSM) which integrates and quantifies these components into a uniform multi-layer treelike architecture is presented in this paper.", "labels": [], "entities": [{"text": "Multilayer Search Mechanism (MSM", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.726074755191803}]}, {"text": "The mechanism gets the optimal result with search algorithms.", "labels": [], "entities": []}, {"text": "The components in MSM affect each other.", "labels": [], "entities": [{"text": "MSM", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9020590782165527}]}, {"text": "At last, the performance of each component is enhanced.", "labels": [], "entities": []}, {"text": "We built a practical system-CUP (Chinese Understanding Platform) based on MSM with three layers.", "labels": [], "entities": []}, {"text": "By the experiments on Word Segmen-tation, a better performance was achieved.", "labels": [], "entities": [{"text": "Word Segmen-tation", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.7928186655044556}]}, {"text": "In theory the normal cascade and feedback mechanism are just some special cases of MSM.", "labels": [], "entities": [{"text": "MSM", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9547947645187378}]}], "introductionContent": [{"text": "At present a classical Chinese NLU architecture usually includes several components, such as Word Segmentation (Word-Seg), POS Tagging, Phrase Analysis, Parsing, Word Sense Disambiguation (WSD) and soon.", "labels": [], "entities": [{"text": "Word Segmentation (Word-Seg)", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.7208097219467163}, {"text": "POS Tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.7372673153877258}, {"text": "Phrase Analysis", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7973162829875946}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 162, "end_pos": 193, "type": "TASK", "confidence": 0.6572789897521337}]}, {"text": "These components are executed one by one from lower layers (such as Word-Seg, POS Tagging) to higher layers (such as Parsing, WSD) to form a kind of cascade mechanism.", "labels": [], "entities": [{"text": "Word-Seg", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9425793290138245}, {"text": "POS Tagging)", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.6934963365395864}]}, {"text": "But when people build a NLU system based on these complex language analysis, it is a very serious problem since the errors of each layer component are multiplied.", "labels": [], "entities": []}, {"text": "With more and more analysis components, the final result becomes too bad to be applicable.", "labels": [], "entities": []}, {"text": "Another problem is that the components in the system affect each other when people build a practical but toy NLU system.", "labels": [], "entities": []}, {"text": "Here the toy system means that each component is ideal enough with perfect input.", "labels": [], "entities": []}, {"text": "But in fact, on the one hand the lower layer components need the information of higher layer components; on the other hand the incorrect analysis of lower layers must reduce the accuracy of higher layers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9986172914505005}]}, {"text": "In Chinese Word-Seg component, many segmentation ambiguities which cannot be solved using only lexical information.", "labels": [], "entities": []}, {"text": "In order to improve the performance of Word-Seg, we have to use some syntax and even semantic information.", "labels": [], "entities": []}, {"text": "Without correct Word-Seg results, however the syntax and semantic parser cannot obtain a correct analysis.", "labels": [], "entities": []}, {"text": "It is a chain debts problem.", "labels": [], "entities": []}, {"text": "People have tried to solve the error-multiplied problem by integrating multi-layers into a uniform model (.", "labels": [], "entities": []}, {"text": "But with the increasing number of integrated layers, the model becomes too complex to build or solve.", "labels": [], "entities": []}, {"text": "The feedback mechanism () helps to use the information of high layers to control the final result.", "labels": [], "entities": []}, {"text": "If the analysis at feedback point cannot be passed, the whole analysis will be denied.", "labels": [], "entities": []}, {"text": "This mechanism places too much burden on the function of feedback point.", "labels": [], "entities": []}, {"text": "This leads to the problems that a correct lower layer result maybe rejected or an error result maybe accepted.", "labels": [], "entities": []}, {"text": "We propose anew Multilayer Search Mechanism (MSM) to solve the problems mentioned above.", "labels": [], "entities": [{"text": "Multilayer Search Mechanism (MSM", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.6709639847278595}]}, {"text": "Based on the mechanism, we build a practical Chinese NLU platform -CUP (Chinese Understanding Platform).", "labels": [], "entities": [{"text": "Chinese NLU platform -CUP (Chinese Understanding Platform)", "start_pos": 45, "end_pos": 103, "type": "DATASET", "confidence": 0.7983128070831299}]}, {"text": "Section 2 introduces the background and architecture of the new mechanism and how to build it up.", "labels": [], "entities": []}, {"text": "Experimental results with CUP is given in Section 3.", "labels": [], "entities": [{"text": "CUP", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8762257695198059}]}, {"text": "In Section 4, we discuss why the new mechanism gets better results than the old ones.", "labels": [], "entities": []}, {"text": "Conclusions and the some future work follow in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The five-best results of each compo- nent  1-best  5-best  Word-Seg 87.83% 94.45%  POS Tag 85.34% 93.28%  Parsing  80.25% 82.13%", "labels": [], "entities": [{"text": "POS Tag 85.34", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.7560335397720337}]}, {"text": " Table 2: SC and Minimal Error Rate  Definition Form of SC Minimal  Error Rate", "labels": [], "entities": []}, {"text": " Table 3: F-Score of Word-Seg  OWS  WLW  SC  FB  F-Score 86.99% 85.80% 88.13% 80.72%", "labels": [], "entities": [{"text": "F-Score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9951045513153076}, {"text": "Word-Seg  OWS  WLW  SC  FB  F-Score 86.99", "start_pos": 21, "end_pos": 62, "type": "DATASET", "confidence": 0.6508725583553314}]}]}