{"title": [{"text": "Towards Measuring Scalability in Natural Language Understanding Tasks", "labels": [], "entities": [{"text": "Natural Language Understanding Tasks", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.7144768983125687}]}], "abstractContent": [{"text": "In this paper we present a discussion of existing metrics for evaluation the performance of individual natural language understanding systems and components as well as the commonly employed metrics for measuring the specific task difficulties.", "labels": [], "entities": []}, {"text": "We extend and generalize the common majority class baseline metric and introduce an general entropy-based metric for measuring the task difficulty of arbitrary language understanding tasks.", "labels": [], "entities": []}, {"text": "Finally, we show an empirical study evaluating this metric followed by a discussion of its role in measuring the scal-ability of language understanding systems and components.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current evaluation frameworks for uni-or multi-modal dialogue systems () that allow for spoken language input do not include metrics for measuring the accuracy of the involved intention recognition systems, simply because such information is hard to extract automatically from log files.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9951930046081543}]}, {"text": "Furthermore no general computational method or framework for measuring the difficulty of natural language understanding tasks have been proposed so far.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 89, "end_pos": 125, "type": "TASK", "confidence": 0.7150793820619583}]}, {"text": "We are, therefore, faced with alack of methods for measuring the difficulties of the individual tasks involved in the language understanding process.", "labels": [], "entities": [{"text": "language understanding process", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.8275987108548483}]}, {"text": "Such generally applicable methods, however, are needed for measuring the scalability of natural language understanding systems and components.", "labels": [], "entities": []}, {"text": "In this paper we first discuss existing metrics for measuring task-specific performances and the corresponding baseline metrics in natural language understanding in Section 2.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.6733563343683878}]}, {"text": "We, then, propose a generalized baselinebased metric in Section 4.1 as well as a general entropybased metric in Section 4.2.", "labels": [], "entities": []}, {"text": "Both methods can be employed for measuring the difficulties of various understanding tasks and, consequently, for evaluating natural language understanding components involved in the intention recognition process.", "labels": [], "entities": [{"text": "intention recognition process", "start_pos": 183, "end_pos": 212, "type": "TASK", "confidence": 0.7861592471599579}]}, {"text": "Section 5 provides a case study evaluation of the proposed methods.", "labels": [], "entities": []}, {"text": "In Section 6 we discuss how an analysis of a specific system on tasks differing in their difficulty can yield a first approach for measuring the scalability of a natural language understanding systems and its components.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation of the overall performance of a dialogue system as a whole frameworks such as PAR-ADISE () for unimodal and PROMISE) for multimodal systems have set a de facto standard.", "labels": [], "entities": [{"text": "PAR-ADISE", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.956666886806488}]}, {"text": "Their importance, however, will remain undiminished, as ways of determining such global parameters are vital to determining the aggregate usefulness and felicity of a system as a whole.", "labels": [], "entities": []}, {"text": "At the same time individual components and ensembles thereof -such as the performance of the uni-or multi-modal input understanding system -need to be evaluated as well to determine bottlenecks and weak links in the discourse understanding processing chain.", "labels": [], "entities": [{"text": "discourse understanding processing chain", "start_pos": 216, "end_pos": 256, "type": "TASK", "confidence": 0.7822910398244858}]}, {"text": "Evaluation of the Automatic Speech Recognition Performance: The commonly used word error rate (WER) can be calculated by aligning any two sets word sequences and adding the number of substitutions S, deletions D and insertions I.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.5772383809089661}, {"text": "word error rate (WER)", "start_pos": 78, "end_pos": 99, "type": "METRIC", "confidence": 0.8588784088691076}]}, {"text": "The WER is then given by the following formula where N is the total number of words in the test set.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7703222036361694}]}, {"text": "Another measure of accuracy that is frequently used is the so called Out Of Vocabulary (OOV) measure, which represents the percentage of words that was not recognized despite their lexical coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9990473389625549}, {"text": "Out Of Vocabulary (OOV) measure", "start_pos": 69, "end_pos": 100, "type": "METRIC", "confidence": 0.9248678343636649}]}, {"text": "WER and OOV are commonly intertwined together with the combined acoustic-and language-model confidence scores, which are constituted by the posterior probabilities of the hidden Markov chains and n-gram frequencies.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6273170709609985}, {"text": "OOV", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9607009291648865}]}, {"text": "Together these scores enable evaluators to measure the absolute performance of a given speech recognition system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7159372717142105}]}, {"text": "In order to arrive at a measure that is relative to the given taskdifficulty, this difficulty must also be calculated, which can be done by means of measuring the perplexity of the task (see Section 3).", "labels": [], "entities": []}, {"text": "Performance: A measure for understanding ratescalled concept error rate has been proposed for example by and is designed in analogy to word error rates employed in automatic speech recognition that are combined with keyword spotting systems.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 164, "end_pos": 192, "type": "TASK", "confidence": 0.7266049186388651}, {"text": "keyword spotting", "start_pos": 216, "end_pos": 232, "type": "TASK", "confidence": 0.7409126460552216}]}, {"text": "propose to differentiate whether the erroneous concept occurs in a non-concept slot that contains information that is captured in the grammar but not considered relevant for selecting a system action (e.g., politeness markers, such as please), in a value-insensitive slot whose identity, suffices to produce a system action (e.g., affirmatives such as yes), or in a value-sensitive slot for which both the occurrence and the value of the slot are important (e.g., a goal object, such as Heidelberg).", "labels": [], "entities": [{"text": "Heidelberg", "start_pos": 487, "end_pos": 497, "type": "DATASET", "confidence": 0.9478819370269775}]}, {"text": "An alternative proposal for concept error rates is embedded into the speech recognition and intention spotting system by Lumenvox 1 , wherein two types of errors and two types of non-errors for concept transcriptions are proposed: \u2022 A match when the application returned the correct concept and an out of grammar match when the ap-1 www.lomunevox.com/support/tunerhelp/Tuning/Concept Transcription.htm plication returned no concepts, or discarded the returned concepts because the user failed to say any concept covered by the grammar.", "labels": [], "entities": [{"text": "speech recognition and intention spotting", "start_pos": 69, "end_pos": 110, "type": "TASK", "confidence": 0.7214414834976196}]}, {"text": "\u2022 A grammar mismatch when the application returned the incorrect concept, but the user said a concept covered by the grammar and an out of grammar mismatch when the application returned a concept, and chose that concept as a correct interpretation, but the user did not say a concept covered by the grammar.", "labels": [], "entities": []}, {"text": "Neither of these measures are suitable for our purposes as they are known to be feasible only for contextinsensitive applications that do not include discourse models, implicit domain-specific information and other contextual knowledge as discussed in ( ).", "labels": [], "entities": []}, {"text": "Therefore this measure has also been called keyword recognition rate for single utterance systems.", "labels": [], "entities": [{"text": "keyword recognition", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7900949120521545}]}, {"text": "In our minds another crucial shortcoming is the lack of comparability, as these measures do not take the general difficulty of the understanding tasks into account.", "labels": [], "entities": []}, {"text": "Again, this has been realized in the automatic speech recognition community and led to the so called perplexity measurements fora given speech recognition task.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6253723502159119}, {"text": "speech recognition task", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.7790554463863373}]}, {"text": "We will, therefore, sketch out the commonly employed perplexity measurements in Section 3.", "labels": [], "entities": []}, {"text": "The most detailed evaluation scheme for discourse comprehension, introduced by and also extended by, features the metrics given in These metrics are combined by means of combining the results of an m 5 multiple linear regression algorithm and a support vector regression approach.", "labels": [], "entities": []}, {"text": "The resulting weighted sum is compared to human intuitions and PARADISE-like metrics concerning task completion rates and -times.", "labels": [], "entities": [{"text": "PARADISE-like", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9523218274116516}]}, {"text": "While this promising approach manages to combine factors related to speech recognition, interpretation and discourse modeling, there are some shortcomings that stem from the fact that this schema was developed for single-domain systems that employ frame-based attribute value pairs for representing the user's intent.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7567625939846039}, {"text": "discourse modeling", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7109745442867279}]}, {"text": "Recent advances in dialogue management and multidomain systems enable approaches that are more flexible than slot-filling, e.g. using discourse pegs, dialogue games and overlay operations for handling multiple tasks and cross-modal references.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8510390818119049}]}, {"text": "More importantly -for the topic of this paper -no means of measuring the a priori discourse understanding difficulty is given.", "labels": [], "entities": []}], "tableCaptions": []}