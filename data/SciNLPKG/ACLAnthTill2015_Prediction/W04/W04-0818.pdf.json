{"title": [{"text": "The MITRE Logical Form Generation System", "labels": [], "entities": [{"text": "MITRE Logical Form Generation", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6927628815174103}]}], "abstractContent": [{"text": "In this paper, we describe MITRE's contribution to the logical form generation track of Senseval-3.", "labels": [], "entities": []}, {"text": "We begin with a description of the context of MITRE's work, followed by a description of the MITRE system and its results.", "labels": [], "entities": []}, {"text": "We conclude with a commentary on the form and structure of this evaluation track.", "labels": [], "entities": []}], "introductionContent": [{"text": "The logic form identification track of the 2004 Senseval evaluation requires its participants to produce aversion of each input sentence with each input word in citation form, annotated with both a scopefree Davidsonian logic and lexical category information for major categories.", "labels": [], "entities": []}, {"text": "The output ignores elements like determiners and negation, and features such as plurals and verb tenses.", "labels": [], "entities": []}, {"text": "This evaluation is of interest to the MITRE Corporation because it has a long-standing interest in text processing and understanding, in all its various dimensions.", "labels": [], "entities": [{"text": "text processing and understanding", "start_pos": 99, "end_pos": 132, "type": "TASK", "confidence": 0.8161545693874359}]}, {"text": "In our current internally funded Reading Comprehension (RC) project, we focus on the detailed understanding of individual stories, using the ability to answer comprehension questions associated with these stories as our evaluation metric.", "labels": [], "entities": [{"text": "Reading Comprehension (RC)", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6530777394771576}]}, {"text": "At the moment, we are interested in getting a sense of how much inference is routinely needed in order to answer RC questions; so generation of sentence meanings is not currently our research focus.", "labels": [], "entities": [{"text": "generation of sentence meanings", "start_pos": 130, "end_pos": 161, "type": "TASK", "confidence": 0.8548927009105682}]}, {"text": "However, in the context of our exploration, we continue to maintain an automated system for producing sentence meanings from text.", "labels": [], "entities": []}], "datasetContent": [{"text": "We found some problems in this evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Argument and predicate precision and re- call.", "labels": [], "entities": [{"text": "Argument", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.983272910118103}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9140965938568115}]}, {"text": " Table 2: Sentence-based accuracy of extracted logic  forms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9816049933433533}]}]}