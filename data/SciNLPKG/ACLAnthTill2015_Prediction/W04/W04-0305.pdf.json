{"title": [{"text": "Lookahead in Deterministic Left-Corner Parsing *", "labels": [], "entities": [{"text": "Deterministic Left-Corner Parsing", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.5738365749518076}]}], "abstractContent": [{"text": "To support incremental interpretation, any model of human sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence prefix.", "labels": [], "entities": [{"text": "incremental interpretation", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.7000047266483307}]}, {"text": "De-terministic parsing takes the extreme position that there can only be one analysis for any sentence prefix.", "labels": [], "entities": []}, {"text": "Experiments with an incremen-tal statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each prefix.", "labels": [], "entities": []}, {"text": "One method which has been extensively used to address the difficulty of deterministic parsing is lookahead, where information about a bounded number of subsequent words is used to decide which analyses to pursue.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.595313310623169}]}, {"text": "We simulate the effects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue.", "labels": [], "entities": []}, {"text": "We find that a large improvement is achieved with one word lookahead, but that more lookahead results in relatively small additional improvements.", "labels": [], "entities": []}, {"text": "This suggests that one word lookahead is sufficient, but that other modifications to our left-corner parsing model could make determin-istic parsing more effective.", "labels": [], "entities": [{"text": "determin-istic parsing", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.6113315373659134}]}], "introductionContent": [{"text": "Incremental interpretation is a fundamental property of the human parsing mechanism.", "labels": [], "entities": [{"text": "Incremental interpretation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8871609568595886}]}, {"text": "To support incremental interpretation, any model of sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence prefix.", "labels": [], "entities": [{"text": "incremental interpretation", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.7207216322422028}]}, {"text": "Otherwise the ambiguity of natural language would make the number of possible interpretations at any point in the parse completely overwhelming.", "labels": [], "entities": []}, {"text": "Deter-ministic parsing takes the extreme position that there can only be one analysis for any sentence prefix.", "labels": [], "entities": [{"text": "Deter-ministic parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6484524756669998}]}, {"text": "We investigate methods which make such a strong constraint feasible, in particular the use of lookahead.", "labels": [], "entities": []}, {"text": "In this paper we do not try to construct a single deterministic parser, but instead consider a family of deterministic parsers and empirically measure the optimal performance of a deterministic parser in this family.", "labels": [], "entities": []}, {"text": "As has been previously proposed by, we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser.", "labels": [], "entities": []}, {"text": "The statistical parser uses an incremental history-based probability model based on left-corner parsing, and the parameters of this model are estimated using a neural network.", "labels": [], "entities": [{"text": "statistical parser", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.5970699191093445}]}, {"text": "Performance of this basic model is state-of-the-art, making these results likely to generalize beyond this specific system.", "labels": [], "entities": []}, {"text": "We specify the family of deterministic parsers in terms of pruning the search for the most probable parse.", "labels": [], "entities": []}, {"text": "Both deterministic parsing and the use of k-word lookahead are characterized as constraints on pruning this search.", "labels": [], "entities": []}, {"text": "We then derive the optimal pruning strategy given these constraints and the probabilities provided by the statistical parser's left-corner probability model.", "labels": [], "entities": []}, {"text": "Empirical experiments on the accuracy of a parser which uses this pruning method indicate the best accuracy we could expect from a deterministic parser of this kind.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9989224672317505}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9979640245437622}]}, {"text": "This allows us to compare different deterministic parsing methods, in particular the use of different amounts of lookahead.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first discuss how the principles of deterministic parsing can be expressed in terms of constraints on the search strategy used by a statistical parser.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.5839274227619171}]}, {"text": "We then present the probability model used by the statistical parser, the way a neural network is used to estimate the parameters of this probability model, and the methods used to search for the most probable parse according these parameters.", "labels": [], "entities": [{"text": "statistical parser", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6940646022558212}]}, {"text": "Finally, we present the empirical experiments on deterministic parsing with lookahead, and discuss the implications of these results.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6886263489723206}]}], "datasetContent": [{"text": "To investigate the effects of lookahead on our family of deterministic parsers, we ran empirical experiments on the standard the Penn Treebank () datasets.", "labels": [], "entities": [{"text": "Penn Treebank () datasets", "start_pos": 129, "end_pos": 154, "type": "DATASET", "confidence": 0.9780227392911911}]}, {"text": "The input to the network is a sequence of tag-word pairs.", "labels": [], "entities": []}, {"text": "We report results fora vocabulary size of 508 tag-word pairs (a frequency threshold of 200).", "labels": [], "entities": []}, {"text": "We first trained a network to estimate the parameters of the basic probability model.", "labels": [], "entities": []}, {"text": "We determined appropriate training parameters and network size based on intermediate validation deterministic recall deterministic precision non-deterministic recall non-deterministic precision: Labeled constituent recall and precision as a function of the number of words of lookahead used by a deterministic parser.", "labels": [], "entities": [{"text": "precision", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.9961103796958923}]}, {"text": "Curves reach their non-deterministic performance with large lookahead.", "labels": [], "entities": []}, {"text": "results and our previous experience.", "labels": [], "entities": []}, {"text": "We trained several networks and chose the best ones based on their validation performance.", "labels": [], "entities": []}, {"text": "The best post-word search beam width for the nondeterministic parser was determined on the validation set, which was 100.", "labels": [], "entities": []}, {"text": "To avoid repeated testing on the standard testing set, we measured the performance of the different models on section 0 of the Penn Treebank (which is not included in either the training or validation sets).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.9869963526725769}]}, {"text": "Standard measures of accuracy for different lookahead lengths are plotted in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9993594288825989}]}, {"text": "First we should note that the non-deterministic parser has state-of-theart accuracy (89.0% F-measure), considering its vocabulary size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9482247829437256}, {"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9969019889831543}]}, {"text": "A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9995160102844238}]}, {"text": "As expected, the deterministic parsers do worse than the non-deterministic one, and this difference becomes less as the lookahead is lengthened.", "labels": [], "entities": []}, {"text": "What is surprising about the curves in is that there is a very large increase in performance from zero words of lookahead The best network had 80 hidden units for the history representation.", "labels": [], "entities": []}, {"text": "Weight decay regularization was applied at the beginning of training but reduced to near 0 by the end of training.", "labels": [], "entities": [{"text": "Weight decay regularization", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.8609933058420817}]}, {"text": "Training was stopped when maximum performance was reached on the validation set, using a post-word beam width of 5. 3 All our results are computed with the evalb program following the standard criteria in.", "labels": [], "entities": []}, {"text": "We used the standard training (sections 2-22, 39,832 sentences, 910,196 words) and validation (section 24, 1346 sentence, 31507 words) sets.", "labels": [], "entities": []}, {"text": "Results of the nondeterministic parser average 0.2% worse on the standard testing set, and average 0.8% better when a larger vocabulary (4215 tag-word pairs) is used.", "labels": [], "entities": []}, {"text": "(i.e. pruning the search to 1 alternative directly after every word) to one word of lookahead.", "labels": [], "entities": []}, {"text": "After one word of lookahead the curves show relatively moderate improvements with each additional word of lookahead, converging to the nondeterministic level, as would be expected.", "labels": [], "entities": []}, {"text": "But between zero words of lookahead and one word of lookahead there is a 5.6% absolute improvement in F-measure (versus a 0.9% absolute improvement between one and two words of lookahead).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9984530210494995}]}, {"text": "In other words, adding the first word of lookahead results in a 2/3 reduction in the difference between the deterministic and nondeterministic parser's F-measure, while adding subsequent words results in at most a 1/3 reduction per word.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9754968881607056}]}], "tableCaptions": []}