{"title": [{"text": "Evaluating information content by factoid analysis: human annotation and stability", "labels": [], "entities": [{"text": "factoid analysis", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8155606985092163}]}], "abstractContent": [{"text": "We present anew approach to intrinsic summary evaluation, based on initial experiments in van Halteren and Teufel (2003), which combines two novel aspects: comparison of information content (rather than string similarity) in gold standard and system summary, measured in shared atomic information units which we call factoids, and comparison to more than one gold standard summary (in our data: 20 and 50 summaries respectively).", "labels": [], "entities": [{"text": "intrinsic summary evaluation", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6098602116107941}]}, {"text": "In this paper, we show that factoid annotation is highly reproducible , introduce a weighted factoid score, estimate how many summaries are required for stable system rankings, and show that the fac-toid scores cannot be sufficiently approximated by unigrams and the DUC information overlap measure.", "labels": [], "entities": [{"text": "DUC information overlap measure", "start_pos": 267, "end_pos": 298, "type": "METRIC", "confidence": 0.6567761898040771}]}], "introductionContent": [{"text": "Many researchers in summarisation believe that the best way to evaluate a summary is extrinsic evaluation: to measure the quality of the summary on the basis of degree of success in executing a specific task with that summary.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9712340831756592}]}, {"text": "The summary evaluation performed in SUMMAC () followed that strategy.", "labels": [], "entities": []}, {"text": "However, extrinsic evaluations are time-consuming to setup and can thus not be used for the day-to-day evaluation needed during system development.", "labels": [], "entities": []}, {"text": "So in practice, a method for intrinsic evaluation is needed, where the properties of the summary itself are examined, independently of its application.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation of summary quality is undeniably hard, as there are two subtasks of summarisation which need to be evaluated, information selection and text production -in fact these two subtasks are often separated in evaluation).", "labels": [], "entities": [{"text": "information selection", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.7965054512023926}]}, {"text": "If we restrict our attention to information selection, systems are tested byway of comparison against a \"gold standard\", a manually produced result which is supposed to be the \"correct\", \"true\" or \"best\" result.", "labels": [], "entities": []}, {"text": "In summarisation there appears to be no \"one truth\", but rather various \"good\" results.", "labels": [], "entities": []}, {"text": "Human subjectivity in what counts as the most important information is high.", "labels": [], "entities": []}, {"text": "This is evidenced by low agreement on sentence selection tasks (, and low word overlap measures in the task of creating summaries by reformulation in the summarisers' own words (e.g. word overlap of the 542 single document summary pairs in DUC-02 averaged only 47%).", "labels": [], "entities": [{"text": "sentence selection tasks", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7915246685345968}, {"text": "DUC-02", "start_pos": 240, "end_pos": 246, "type": "DATASET", "confidence": 0.8430157899856567}]}, {"text": "But even though the non-existence of anyone gold standard is generally acknowledged in the summarisation community, actual practice nevertheless ignores this, mostly due to the expense of compiling summary gold standards and the lack of composite measures for comparison to more than one gold standard.", "labels": [], "entities": []}, {"text": "Other fields such as information retrieval (IR) also have to deal with human variability concerning the question of what \"relevant to a query\" means.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8628497958183289}]}, {"text": "This problem is circumvented by extensive sampling: many different queries are collected to level out the differences in query formulation and relevance judgements.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.6898990124464035}]}, {"text": "shows that the relative rankings of IR systems are stable across annotators even though relevance judgements differ significantly between humans.", "labels": [], "entities": [{"text": "IR", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9789117574691772}]}, {"text": "Similarly, in MT, the recent BLEU metric () also uses the idea that one gold standard is not enough.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9659770131111145}, {"text": "BLEU metric", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9718479812145233}]}, {"text": "Their ngram-based metric derived from four reference translations of 40 general news stories shows high correlation with human judgement.", "labels": [], "entities": []}, {"text": "examine the use of ngram-based multiple gold standards for summarisation evaluation, and conclude \"we need more than one model summary although we cannot estimate how many model summaries are required to achieve reliable automated summary evaluation\".", "labels": [], "entities": [{"text": "summarisation evaluation", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.9721551537513733}]}, {"text": "In this paper, we explore the differences and similarities between various human summaries in order to create a basis for such an estimate and examine the degree of difference between the use of a single summary gold standard and the use of a consensus gold standard for two sample texts, based on 20 and 50 summaries respectively.", "labels": [], "entities": []}, {"text": "The second aspect we examine is the similarity measure which compares system and gold standard summaries.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 36, "end_pos": 54, "type": "METRIC", "confidence": 0.9739056825637817}]}, {"text": "In principle, the comparison can be done via co-selection of extracted sentences, by string-based surface measures (), or by subjective judgements of the amount of information overlap).", "labels": [], "entities": []}, {"text": "String-based metrics are superior to sentence co-selection, as co-selection cannot take similar or even identical information into account if it does not occur in the sentences which were chosen.", "labels": [], "entities": []}, {"text": "The choice of information overlap judgements as the main metric in DUC reflects the intuition that human judgements of shared \"meaning\" of two texts should in principle be superior to surface-based similarity.", "labels": [], "entities": []}, {"text": "DUC assessors judge the informational overlap between \"model units\" (elementary discourse units (EDUs), i.e. clause-like units, taken from the gold standard summary) and \"peer units\" (sentences taken from the participating summaries) on the basis of the question: \"How much of the information in a model unit is contained in a peer unit: 100%, 80%, 60%, 40%, 20%, 0%?\"", "labels": [], "entities": [{"text": "DUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8226363658905029}]}, {"text": "Weighted recall measures report how much gold standard information is present in the summaries.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9947994947433472}]}, {"text": "However, information overlap judgement is not something humans seem to be good at, either.", "labels": [], "entities": [{"text": "information overlap judgement", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.8498743971188863}]}, {"text": "show the instability of the evaluation, expressed in system rankings.", "labels": [], "entities": []}, {"text": "They also examined those cases where annotators incidentially had to judge a given model-peer pair more than once (because different systems returned the same \"peer\" sentence).", "labels": [], "entities": []}, {"text": "In those cases, assessors agreed with their own prior judgement in only 82% of the cases.", "labels": [], "entities": []}, {"text": "We propose a novel gold standard comparison based on factoids.", "labels": [], "entities": []}, {"text": "Identifying factoids in text is a more objective task than judging information overlap\u00e0overlap`overlap\u00e0 la DUC.", "labels": [], "entities": [{"text": "Identifying factoids in text", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8807009756565094}]}, {"text": "Our annotation experiments show high human agreement on the factoid annotation task.", "labels": [], "entities": [{"text": "agreement", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9184558391571045}]}, {"text": "We believe this is due to the way how factoids are defined, and due to our precise guidelines.", "labels": [], "entities": []}, {"text": "The factoid measure also allows quantification of the specific elements of information overlap, rather than just giving a quantitative judgement expressed in percentages.", "labels": [], "entities": []}, {"text": "In an example from, a DUC assessor judged some content overlap between \"Thousands of people are feared dead\" and \"3,000 and perhaps ...", "labels": [], "entities": [{"text": "DUC assessor", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.875820517539978}]}, {"text": "5,000 people have been killed.\"", "labels": [], "entities": []}, {"text": "In our factoid representation, a distinction between \"killed\" and \"feared dead\" would be made, and different numbers of people mentioned would have been differentiated.", "labels": [], "entities": []}, {"text": "Thus, the factoid approach can capture much finer shades of meaning differentiation than DUC-style information overlap does.", "labels": [], "entities": [{"text": "meaning differentiation", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7213833928108215}, {"text": "DUC-style information overlap", "start_pos": 89, "end_pos": 118, "type": "TASK", "confidence": 0.5918721258640289}]}, {"text": "Futhermore, it can provide feedback to system builders on the exact information their systems fail to include or include superfluously.", "labels": [], "entities": []}, {"text": "We describe factoid analysis in section 2, a method for comparison of the information content of different summaries of the same texts, and describe our method for measuring agreement and present results in section 3.", "labels": [], "entities": [{"text": "factoid analysis", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.9193226099014282}]}, {"text": "We then investigate the distribution of factoids across the summaries in our data sets in section 4, and define a weighted factoid score in section 5.", "labels": [], "entities": []}, {"text": "In that section, we also perform stability experiments, to test whether rankings of system summaries remain stable if fewer than all summaries which we have available are used, and compare weighted factoid scores to other summary evaluation metrics.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}