{"title": [{"text": "A Powerful and General Approach to Context Exploitation in Natural Language Processing", "labels": [], "entities": [{"text": "Context Exploitation in Natural Language Processing", "start_pos": 35, "end_pos": 86, "type": "TASK", "confidence": 0.6689271430174509}]}], "abstractContent": [{"text": "In natural language, the meaning of a lexeme often varies due to the specific surrounding context.", "labels": [], "entities": []}, {"text": "Computational approaches to natural language processing can benefit from a reliable , long-range-context-dependent representation of the meaning of each lexeme that appears in a given sentence.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.7108007868131002}]}, {"text": "We have developed a general new technique that produces a context-dependent 'meaning' representation fora lexeme in a specific surrounding context.", "labels": [], "entities": []}, {"text": "The 'meaning' of a lexeme in a specific context is represented by a list of semantically replaceable elements the members of which are other lexemes from our experimental lexicon.", "labels": [], "entities": []}, {"text": "We have performed experiments with a lexicon composed of individual English words and also with a lexicon of individual words and selected phrases.", "labels": [], "entities": []}, {"text": "The resulting lists can be used to compare the 'meaning' of conceptual units (individual words or frequently-occurring phrases) in different contexts and also can serve as features for machine learning approaches to classify semantic roles and relationships .", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical natural language approaches build models based on annotated corpora as well as unlabeled corpora.", "labels": [], "entities": []}, {"text": "The latter, requiring unsupervised knowledge acquisition, has the advantage of larger training sets-it is possible to exploit corpora composed of billions of words.", "labels": [], "entities": []}, {"text": "A number of researchers have observed that such use of very large corpora improves the stability of statistical models (e.g.).", "labels": [], "entities": []}, {"text": "The mathematical procedures employed here are based upon Hecht-Nielsen's neuroscience theory of cognition.", "labels": [], "entities": []}, {"text": "Ina nutshell, this theory holds that cognition is based upon a procedure of ruling out all unreasonable conclusions and then deciding, of the remaining conclusions, which are the least worst ones.", "labels": [], "entities": []}, {"text": "This mathematical symbolic predictive technique is called confabulation.", "labels": [], "entities": [{"text": "mathematical symbolic predictive", "start_pos": 5, "end_pos": 37, "type": "TASK", "confidence": 0.5957343578338623}]}, {"text": "The knowledge employed by confabulation is vast quantities of conditional probabilities for pairs of symbols.", "labels": [], "entities": []}, {"text": "This knowledge, which is of no value for reasoning or probabilistic inference, is readily obtainable.", "labels": [], "entities": []}, {"text": "Hecht-Nielsen's discovery is that, given the proper coding of a problem into symbols, confabulation works essentially as well as reasoning would if we were in possession of the necessary 'omniscient' knowledge that reasoning requires.", "labels": [], "entities": []}, {"text": "Unfortunately, 'omniscient' knowledge is not practically obtainable, thereby making attempts to implement reasoning, in any form, futile.", "labels": [], "entities": []}, {"text": "Confabulation, on the other hand, although it does require storage and use of large volumes of knowledge, is simple and practical (e.g., see for the number of items of knowledge used in the experiments reported here).", "labels": [], "entities": []}, {"text": "Confabulation provides an explicit mechanism that can now be used to build artificial intelligence.", "labels": [], "entities": []}, {"text": "Our approach to 'meaning' representation for lexemes is to provide a set of similar elements that are grammatically and/or semantically interchangeable with a given lexeme.", "labels": [], "entities": []}, {"text": "Others have constructed lexical similarity clusters using order-dependent co-occurrence statistics, particularly with N-gram models-see for an example where words are sorted into exclusive classes based on bigram statistics.", "labels": [], "entities": []}, {"text": "The occurrence statistics of bigrams do stabilize for frequent words given a training corpus of hundreds of millions of words.", "labels": [], "entities": []}, {"text": "However, beyond tri-grams, the theoretical size of a training corpus required for completeness is unreasonable.", "labels": [], "entities": []}, {"text": "Our method uses only pairwise conditionals.", "labels": [], "entities": []}, {"text": "To analyze a given text stream, we use a hierarchy consisting of a word-level representation and a conceptual-unit-level representation to analyze arbitrary single-clause English sentences.", "labels": [], "entities": []}, {"text": "Each of these representations uses a lexicon of language element tokens to encode free text as described below.", "labels": [], "entities": []}, {"text": "The representation of a sentence with two levels of hierarchy at the word level and the phrase level is consistent with Late Assignment of Syntax Theory, an analysis by synthesis model advocated by.", "labels": [], "entities": [{"text": "Late Assignment of Syntax Theory", "start_pos": 120, "end_pos": 152, "type": "TASK", "confidence": 0.5627496719360352}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 5. Size of knowledge bases used for the  SRE expansion", "labels": [], "entities": [{"text": "SRE expansion", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.7887340486049652}]}]}