{"title": [{"text": "Bootstrapping Spoken Dialog Systems with Data Reuse", "labels": [], "entities": []}], "abstractContent": [{"text": "Building natural language spoken dialog systems requires large amounts of human transcribed and labeled speech utterances to reach useful operational service performances.", "labels": [], "entities": []}, {"text": "Furthermore , the design of such complex systems consists of several manual steps.", "labels": [], "entities": []}, {"text": "The User Experience (UE) expert analyzes and defines by hand the system core functionalities: the system semantic scope (call-types) and the dialog manager strategy which will drive the human-machine interaction.", "labels": [], "entities": []}, {"text": "This approach is extensive and error prone since it involves several non-trivial design decisions that can only be evaluated after the actual system deployment.", "labels": [], "entities": []}, {"text": "Moreover, scalability is compromised by time, costs and the high level of UE know-how needed to reach a consistent design.", "labels": [], "entities": []}, {"text": "We propose a novel approach for bootstrapping spoken dialog systems based on reuse of existing transcribed and labeled data, common reusable dialog templates and patterns, generic language and understanding models, and a consistent design process.", "labels": [], "entities": []}, {"text": "We demonstrate that our approach reduces design and development time while providing an effective system without any application specific data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialog systems aim to identify intents of humans, expressed in natural language, and take actions accordingly, to satisfy their requests ().", "labels": [], "entities": []}, {"text": "Ina natural spoken dialog system, typically, first the speaker's utterance is recognized using an automatic speech recognizer (ASR).", "labels": [], "entities": []}, {"text": "Then, the intent of the speaker is identified from the recognized sequence, using a spoken language understanding (SLU) component.", "labels": [], "entities": []}, {"text": "This step can be framed as a classification problem for goal-oriented call routing systems (, among others).", "labels": [], "entities": [{"text": "call routing", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.7752534747123718}]}, {"text": "Then, the user would be engaged in a dialog via clarification or confirmation prompts if necessary.", "labels": [], "entities": []}, {"text": "The role of the dialog manager (DM) is to interact in a natural way and help the user to achieve the task that the system is designed to support.", "labels": [], "entities": []}, {"text": "In our case we only consider automated call routing systems where the task is to reach the right route in a large call center, which could be either a live operator or an automated system.", "labels": [], "entities": [{"text": "call routing", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.73161780834198}]}, {"text": "An example dialog from a telephone-based customer care application is given in.", "labels": [], "entities": []}, {"text": "Typically the design of such complex systems consists of several manual steps, including analysis of existing IVR (Interactive Voice Response) systems, customer service representative (CSR) interviews, customers' testimonials, CSR training material, and, when available, human-machine unconstrained speech recordings.", "labels": [], "entities": []}, {"text": "Based on these heterogeneous requirements, the User Experience (UE) expert analyzes and defines by hand the system core functionalities: the system semantic scope (call-types) and the dialog manager strategy which will drive the human-machine interaction.", "labels": [], "entities": []}, {"text": "Once the UE expert designs the system, large amounts of transcribed and labeled speech utterances are needed for building the ASR and SLU models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.8599439859390259}]}, {"text": "In our previous work, we have presented active and unsupervised (or semi-supervised) learning algorithms in order to reduce the amount of labeling effort needed while building ASR and SLU systems ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 176, "end_pos": 179, "type": "TASK", "confidence": 0.9529488682746887}]}, {"text": "There we focus on a single application, and only the ASR and SLU components.", "labels": [], "entities": [{"text": "ASR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.91096031665802}]}, {"text": "In this study, we aim to exploit the labeled and transcribed data and common reusable dialog templates and patterns obtained from similar previous applications to bootstrap the whole spoken dialog system with ASR, SLU, and DM components.", "labels": [], "entities": []}, {"text": "tion 2 describes briefly the AT&T Spoken Dialog System, which we use in this study, and its main components, ASR, SLU, and DM.", "labels": [], "entities": [{"text": "AT&T Spoken Dialog System", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.9125191569328308}]}, {"text": "In Section 3 we present our method to bootstrap ASR, SLU, and DM fora new application.", "labels": [], "entities": [{"text": "bootstrap ASR", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.5734920799732208}]}, {"text": "Section 4 presents our experiments using real data from a customer care application.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we selected an application from the pharmaceutical domain to bootstrap.", "labels": [], "entities": []}, {"text": "We have evaluated the performances of the ASR language model, call classifier, and dialog manager as described below.", "labels": [], "entities": [{"text": "ASR language", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.8776137828826904}]}, {"text": "To bootstrap a statistical language model for ASR, we used human-machine spoken language data from two previous AT&T VoiceTone spoken dialog applications (App.", "labels": [], "entities": [{"text": "ASR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9943102598190308}, {"text": "AT&T VoiceTone spoken dialog", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.69215061267217}]}, {"text": "1 (telecommunication domain) and App.", "labels": [], "entities": [{"text": "App", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9152202606201172}]}, {"text": "2 (medical insurance domain)).", "labels": [], "entities": []}, {"text": "We also used some data from the application domain web pages (Web).", "labels": [], "entities": []}, {"text": "lists the sizes of these corpora.", "labels": [], "entities": []}, {"text": "Training Data\" and \"App.", "labels": [], "entities": [{"text": "Training Data", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9218505620956421}, {"text": "App", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.5125159025192261}]}, {"text": "Test Data\" correspond to the training and test data we have for the new application and are used for controlled experiments.", "labels": [], "entities": []}, {"text": "We also extended the available corpora with human-human dialog data from the Switchboard corpus (SWBD) (.", "labels": [], "entities": [{"text": "Switchboard corpus (SWBD)", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.8930910229682922}]}, {"text": "summarizes some style and content features of the available corpora.", "labels": [], "entities": []}, {"text": "For simplification, we only compared the percentage of pronouns and filled pauses to show style differences, and the domain test data outof-vocabulary word (OOV) rate for content variations.", "labels": [], "entities": [{"text": "domain test data outof-vocabulary word (OOV) rate", "start_pos": 117, "end_pos": 166, "type": "METRIC", "confidence": 0.7048668199115329}]}, {"text": "The human-machine spoken dialog corpora include much more pronouns than the web data.", "labels": [], "entities": []}, {"text": "There are even further differences between the individual pronoun distributions.", "labels": [], "entities": []}, {"text": "For example, out of all the pronouns in the web data, 35% is \"you\", and 0% is \"I\", whereas in all of the humanmachine dialog corpora, more than 50% of the pronouns are \"I\".", "labels": [], "entities": []}, {"text": "In terms of style, both spoken dialog corpora can be considered as similar.", "labels": [], "entities": []}, {"text": "In terms of content, the second application data is the most similar corpus, as it results in the lowest OOV rate for the domain test data.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.989978551864624}]}, {"text": "In, we show further reductions in the App.", "labels": [], "entities": [{"text": "App", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.8857243061065674}]}, {"text": "test set OOV rate, when we combine these corpora.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9849705696105957}]}, {"text": "shows the effect of using various corpora as training data for statistical language models used in the   recognition of the test data.", "labels": [], "entities": []}, {"text": "We also computed ASR runtime curves by varying the beam-width of the decoder, as the characteristics of the corpora effects the size of the language model.", "labels": [], "entities": [{"text": "ASR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9710296988487244}]}, {"text": "Content-wise the most similar corpus (App. 2) resulted in the best performing language model, when the corpora are considered separately.", "labels": [], "entities": [{"text": "App.", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.6303234696388245}]}, {"text": "We obtained the best recognition accuracy, when we augment App.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9285888671875}, {"text": "App", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.5623112320899963}]}, {"text": "1 and the web data.", "labels": [], "entities": [{"text": "web data", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.8326665461063385}]}, {"text": "Switchboard corpus also resulted in a reasonable performance, but the problem is that it resulted in a very big language model, slowing down the recognition.", "labels": [], "entities": []}, {"text": "In that figure, we also show the word accuracy curve, when we use in-domain transcribed data for training the language model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9363800883293152}]}, {"text": "Once some data from the domain is available, it is possible to weight the available out-of-domain data and the web-data while reusing, to achieve further improvements.", "labels": [], "entities": []}, {"text": "When we lack any in-domain data, we expect the UE expert to reuse the application data from the most similar sectors and/or combine all available data.", "labels": [], "entities": []}, {"text": "We have performed the SLU tests using the Boostexter tool).", "labels": [], "entities": [{"text": "SLU", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8301836252212524}, {"text": "Boostexter tool", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9083499312400818}]}, {"text": "For all experiments, we have used word -grams of transcriptions as features and iterated Boostexter 1,100 times.", "labels": [], "entities": []}, {"text": "In this study we have assumed that all candidate utterances are first recognized by the same automatic speech recognizer (ASR), so we deal with only text input of the same quality, which corresponds to the recognitions obtained using the language model trained from App.", "labels": [], "entities": [{"text": "App", "start_pos": 266, "end_pos": 269, "type": "DATASET", "confidence": 0.9747109413146973}]}, {"text": "2, and Web data.", "labels": [], "entities": [{"text": "Web data", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.8333634734153748}]}, {"text": "As the test set, we again used the 5,537 utterances collected from a pharmaceutical domain customer care application.", "labels": [], "entities": []}, {"text": "We used a very limited library of call-types from a telecommunications domain application.", "labels": [], "entities": []}, {"text": "We have made controlled experiments where we know the true call-types of the utterances.", "labels": [], "entities": []}, {"text": "In this application we have 97 call-types with a fairly high perplexity of 32.81.", "labels": [], "entities": []}, {"text": "If an utterance has the call-type which is covered by the bootstrapped model, we expect that call-type to get high confidence.", "labels": [], "entities": []}, {"text": "Otherwise, we expect the model to reject it by assigning the special call-type Not(Understood) meaning that the intent in the utterance is known to be not covered or some call-type with low confidence.", "labels": [], "entities": []}, {"text": "Then we compute the rejection accuracy (RA) of the bootstrap model:: SLU results using transcriptions and ASR output with the models trained with in-domain data, only generic call-types, and also with call-types from the library and rules.", "labels": [], "entities": [{"text": "rejection accuracy (RA)", "start_pos": 20, "end_pos": 43, "type": "METRIC", "confidence": 0.7972144961357117}, {"text": "SLU", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.7569888830184937}]}, {"text": "In order to evaluate the classifier performance for the utterances whose call-types are covered by the bootstrapped model, we have used classification accuracy (CA) which is the fraction of utterances in which the top scoring call-type is one of the true call-types assigned by a human-labeler and its confidence score is more than some threshold: These two measures are actually complementary to each other.", "labels": [], "entities": [{"text": "classification accuracy (CA)", "start_pos": 136, "end_pos": 164, "type": "METRIC", "confidence": 0.8333961844444275}]}, {"text": "For the complete model trained with all the training data, where all the intents are covered, these two metrics are the same.", "labels": [], "entities": []}, {"text": "In order to see our upper bound, we first trained a classifier using 30,000 labeled utterances from the same application.", "labels": [], "entities": []}, {"text": "First row of presents these results using both the transcriptions of the test set and using the ASR output with around 68% word accuracy.", "labels": [], "entities": [{"text": "ASR", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.7244433164596558}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.8045357465744019}]}, {"text": "As the confidence threshold we have chosen a hypothetical value of 0.3 for all experiments.", "labels": [], "entities": [{"text": "confidence", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9633612632751465}]}, {"text": "As seen, 78.27% classification (or rejection) accuracy is the performance using all training data.", "labels": [], "entities": [{"text": "rejection)", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.8311109244823456}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.6462118029594421}]}, {"text": "This reduces to 61.73% when we use the ASR output.", "labels": [], "entities": [{"text": "ASR", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7346834540367126}]}, {"text": "This is mostly because of the unrecognized words which are critical for the application.", "labels": [], "entities": []}, {"text": "This is intuitive since ASR language model has not been trained with domain data.", "labels": [], "entities": [{"text": "ASR language", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8744529783725739}]}, {"text": "Then we trained a generic model using only generic call-types.", "labels": [], "entities": []}, {"text": "This model has achieved better accuracies as seen in the second row, since we do not expect it to distinguish among the reusable or specific call-types.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9903687834739685}]}, {"text": "Furthermore, for classification accuracy we only use the portion of the test set whose call-types are covered by the model and the call-types in this model are definitely easier than the specific ones.", "labels": [], "entities": [{"text": "classification", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9641901850700378}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8728799223899841}]}, {"text": "The drawback is that we only cover about half of the utterances.", "labels": [], "entities": []}, {"text": "Using the ASR output, unlike the in-domain model case, did not hurt much, since the ASR already covers the utterances with generic calltypes with great accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9905907511711121}]}, {"text": "We then trained a bootstrapped model using 13 calltypes from the library and a few simple rules written manually for three frequent intents.", "labels": [], "entities": []}, {"text": "Since the library consists of an application from a fairly different domain, we could only exploit intents related to billing, such as Request(Account Balance).", "labels": [], "entities": [{"text": "Request(Account Balance)", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.6068207442760467}]}, {"text": "While determining the calltypes to write rules, we actually played the expert which has previous knowledge on the application.", "labels": [], "entities": []}, {"text": "This enabled us to increase the coverage to 70.34%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9963616728782654}]}, {"text": "The most impressive result of these experiments is that, we have got a call classifier which is trained without any in-domain data and can handle most utterances with almost same accuracy as the trained with extensive amounts of data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9967645406723022}]}, {"text": "Noting the weakness of our current calltype library we expect even better performances as we augment more call-types from on-going applications.", "labels": [], "entities": []}, {"text": "Evaluation of spoken dialog system performances is a complex task and depends on the purpose of the desired dialog metric.", "labels": [], "entities": []}, {"text": "While ASR and SLU can be fairly assessed off-line using utterances collected in previous runs of the baseline system, the dialog manager requires interaction with areal motivated user who will cooperate with the system to complete the task.", "labels": [], "entities": [{"text": "ASR", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.554754912853241}, {"text": "SLU", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.7574138641357422}]}, {"text": "Ideally, the bootstrap system has to be deployed in the field and the dialogs have to be manually labeled to provide accurate measure of task completion rate.", "labels": [], "entities": []}, {"text": "Usability metrics also require direct feedback from the caller to properly measure the user satisfaction (specifically, task success and dialog cost) ().", "labels": [], "entities": []}, {"text": "However, we are more interested in automatically comparing the bootstrap system performances with a reference system, working on the same domain and with identical dialog strategies.", "labels": [], "entities": []}, {"text": "As a first order of approximation, we reused the 3,082 baseline test dialogs (5,537 utterances) collected by the live reference system and applied the same dialog turn sequence to evaluate the bootstrap system.", "labels": [], "entities": []}, {"text": "According to the reference system call flow, the 97 call-types covered by the reference classifier are clustered into 32 DM categories (DMC).", "labels": [], "entities": []}, {"text": "A DMC is a generalization of more specific intents.", "labels": [], "entities": []}, {"text": "The bootstrap system only classifies 16 call-types and 16 DMC accordingly to the bootstrapping SLU design requirements described in 4.2.", "labels": [], "entities": []}, {"text": "This is only half of the reference system DMC coverage, but it actually addresses 70.34% of the total utterance classification task.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.8885263204574585}]}, {"text": "We simulate the execution of the dialog using data collected from a deployed system, with the following proce-  is the rejection threshold; 3.", "labels": [], "entities": []}, {"text": "if in any turn of the dialog, a mismatching concrete call-type is found, the dialog is considered as unsuccessful:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ASR Data Characteristics", "labels": [], "entities": [{"text": "ASR Data Characteristics", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7976673245429993}]}, {"text": " Table 2: Style and Content differences among various data sources.", "labels": [], "entities": [{"text": "Style", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9793685674667358}]}, {"text": " Table 3: Effect of training corpus combination on OOV  rate of the test data.", "labels": [], "entities": [{"text": "OOV  rate", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9838180840015411}]}, {"text": " Table 4: SLU results using transcriptions and ASR output with the models trained with in-domain data, only generic  call-types, and also with call-types from the library and rules.", "labels": [], "entities": [{"text": "ASR", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.7677184343338013}]}, {"text": " Table 5: DM Evaluation results.", "labels": [], "entities": [{"text": "DM Evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8097356855869293}]}]}