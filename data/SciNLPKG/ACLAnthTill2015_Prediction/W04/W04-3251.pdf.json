{"title": [{"text": "Instance-Based Question Answering: A Data-Driven Approach", "labels": [], "entities": [{"text": "Instance-Based Question Answering", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8706041971842448}]}], "abstractContent": [{"text": "Anticipating the availability of large question-answer datasets, we propose a principled, data-driven Instance-Based approach to Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.8552675843238831}]}, {"text": "Most question answering systems incorporate three major steps: classify questions according to answer types, formulate queries for document retrieval, and extract actual answers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.8248196840286255}, {"text": "document retrieval", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.6873053163290024}]}, {"text": "Under our approach , strategies for answering new questions are directly learned from training data.", "labels": [], "entities": []}, {"text": "We learn models of answer type, query content, and answer extraction from clusters of similar questions.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7526810467243195}]}, {"text": "We view the answer type as a distribution, rather than a class in an ontology.", "labels": [], "entities": []}, {"text": "In addition to query expansion, we learn general content features from training data and use them to enhance the queries.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.8740661144256592}]}, {"text": "Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as corrector incorrect answers.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.9139949381351471}]}, {"text": "We present a basic implementation of these concepts that achieves a good performance on TREC test data.", "labels": [], "entities": [{"text": "TREC test data", "start_pos": 88, "end_pos": 102, "type": "DATASET", "confidence": 0.7744433085123698}]}], "introductionContent": [{"text": "Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics -as reflected by the TREC QA track.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.9228130578994751}, {"text": "TREC QA track", "start_pos": 208, "end_pos": 221, "type": "DATASET", "confidence": 0.6957268516222636}]}, {"text": "Starting from successful pipeline architectures ( ), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (), handling additional types of questions, employing complex reasoning mechanisms (, tapping into external data sources such as the Web, encyclopedias, databases (;, and merging multiple agents and strategies into meta-systems ().", "labels": [], "entities": []}, {"text": "In recent years, learning components have started to permeate Question Answering (.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.6916968077421188}]}, {"text": "Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods.", "labels": [], "entities": [{"text": "question classification", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.8272107243537903}, {"text": "answer extraction", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.8893666863441467}, {"text": "answer verification", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.8017573952674866}]}, {"text": "At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7693314254283905}]}, {"text": "Moreover, Question Answering is expanding to different languages ( and domains other than news stories.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8263051211833954}]}, {"text": "These trends suggest the need for principled, statistically based, easily re-trainable, language independent QA systems that take full advantage of large amounts of training data.", "labels": [], "entities": []}, {"text": "We propose an instance-based, data-driven approach to Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8361960351467133}]}, {"text": "Instead of classifying questions according to limited, predefined ontologies, we allow training data to shape the strategies for answering new questions.", "labels": [], "entities": []}, {"text": "Answer models, query content models, and extraction models are also learned directly from training data.", "labels": [], "entities": []}, {"text": "We present a basic implementation of these concepts and evaluate the performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present a basic implementation of the instancebased approach.", "labels": [], "entities": []}, {"text": "The resulting QA system is fully automatically trained, without human intervention.", "labels": [], "entities": [{"text": "QA", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.8491705656051636}]}, {"text": "Instance-based approaches are known to require large, dense training datasets which are currently underdevelopment.", "labels": [], "entities": []}, {"text": "Although still sparse, the subset of all temporal questions from the TREC 9-12 datasets is relatively dense compared to the rest of the question space.", "labels": [], "entities": [{"text": "TREC 9-12 datasets", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.9199897845586141}]}, {"text": "This makes it a good candidate for evaluating our instance-based QA approach until larger and denser datasets become available.", "labels": [], "entities": []}, {"text": "It is also broad enough to include different question structures and varying degrees of difficulty and complexity such as: \u2022 \"When did Beethoven die?\"", "labels": [], "entities": [{"text": "\"When did Beethoven die?\"", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6448713328157153}]}, {"text": "\u2022 \"How long is a quarter in an NBA game?\"", "labels": [], "entities": []}, {"text": "\u2022 \"What year did General Montgomery lead the Allies to a victory over the Axis troops in North Africa?\"", "labels": [], "entities": []}, {"text": "The 296 temporal questions and their corresponding answer patterns provided by NIST were used in our experiments.", "labels": [], "entities": [{"text": "NIST", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9609124660491943}]}, {"text": "The questions were processed with apart of speech tagger and a parser.", "labels": [], "entities": []}, {"text": "The questions were clustered using templatestyle frames that incorporate lexical items, parser labels, and surface form flags.", "labels": [], "entities": []}, {"text": "Consider the following question and several of its corresponding frames: where <NNP>,<NP>,<VB>,<Q> denote: proper noun, noun phrase, verb, and generic question term sequence, respectively.", "labels": [], "entities": [{"text": "NNP>,<NP>,<VB>,<Q>", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.6663290858268738}]}, {"text": "Initially, frames are generated exhaustively for each question.", "labels": [], "entities": []}, {"text": "Each frame that applies to more than three questions is then selected to represent a specific cluster.", "labels": [], "entities": []}, {"text": "One hundred documents were retrieved for each query through the Google API (www.google.com/api).", "labels": [], "entities": []}, {"text": "Documents containing the full question, question number, references to TREC, NIST, AQUAINT, Question Answering and other similar problematic content were filtered out.", "labels": [], "entities": [{"text": "TREC", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.6420603394508362}, {"text": "NIST", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8618658781051636}, {"text": "AQUAINT", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.7220911979675293}, {"text": "Question Answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7118296772241592}]}, {"text": "When building the Query Content Model keyword-based queries were initially formulated and expanded.", "labels": [], "entities": []}, {"text": "From the retrieved documents a set of content features (n-grams and paraphrases) were selected through average mutual information.", "labels": [], "entities": []}, {"text": "The features were added to the simple queries and anew set of documents was retrieved.", "labels": [], "entities": []}, {"text": "The enhanced queries were scored and the corresponding top 10 ngrams/paraphrases were included in the Query Content Model.", "labels": [], "entities": []}, {"text": "The maximum n-gram and paraphrase size for these features was set to 6 words.", "labels": [], "entities": []}, {"text": "The Extraction Model uses a support vector machine (SVM) classifier) with a linear kernel.", "labels": [], "entities": []}, {"text": "The task of the classifier is to decide if text snippets contain a correct answer.", "labels": [], "entities": []}, {"text": "The SVM was trained on features extracted from one-sentence passages containing at least one keyword from the original question.", "labels": [], "entities": []}, {"text": "The features consist of: distance between keywords and potential answers, keyword density in a passage, simple statistics such as document and sentence length, query type, lexical ngrams (up to 6-grams), and paraphrases.", "labels": [], "entities": []}, {"text": "We performed experiments using leave-one-out cross validation.", "labels": [], "entities": []}, {"text": "The system was trained and tested without any question filtering or manual input.", "labels": [], "entities": []}, {"text": "Each cluster produced an answer set with corresponding scores.", "labels": [], "entities": []}, {"text": "Top 5 answers for each instance were considered by a mean reciprocal rank (MRR) metric overall N questions: , where rank i refers to the first correct occurrence in the top 5 answers for question i.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 53, "end_pos": 79, "type": "METRIC", "confidence": 0.880065788825353}]}, {"text": "While not the focus of this paper, answer clustering algorithms are likely to further improve performance.", "labels": [], "entities": [{"text": "answer clustering", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9300870299339294}]}], "tableCaptions": []}