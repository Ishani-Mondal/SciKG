{"title": [{"text": "Co-training and Self-training for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.772145410378774}]}], "abstractContent": [{"text": "This paper investigates the application of co-training and self-training to word sense disam-biguation.", "labels": [], "entities": [{"text": "word sense disam-biguation", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.7870946526527405}]}, {"text": "Optimal and empirical parameter selection methods for co-training and self-training are investigated, with various degrees of error reduction.", "labels": [], "entities": []}, {"text": "A new method that combines co-training with majority voting is introduced, with the effect of smoothing the bootstrapping learning curves, and improving the average performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of word sense disambiguation consists in assigning the most appropriate meaning to a polysemous word within a given context.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.770563801129659}]}, {"text": "Most of the efforts in solving this problem were concentrated so far towards supervised learning, where each sense tagged occurrence of a particular word is transformed into a feature vector, which is then used in an automatic learning process.", "labels": [], "entities": []}, {"text": "While these algorithms usually achieve the best performance, as compared to their unsupervised or knowledge-based alternatives, there is an important shortcoming associated with these methods: their applicability is limited only to those words for which sense tagged data is available, and their accuracy is strongly connected to the amount of labeled data available at hand.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 296, "end_pos": 304, "type": "METRIC", "confidence": 0.9987322688102722}]}, {"text": "In this paper, we investigate methods for building sense classifiers when only relatively few annotated examples are available.", "labels": [], "entities": []}, {"text": "We explore bootstrapping methods using co-training and self-training, and evaluate their performance on the SENSEVAL-2 nouns.", "labels": [], "entities": [{"text": "SENSEVAL-2 nouns", "start_pos": 108, "end_pos": 124, "type": "DATASET", "confidence": 0.6619280576705933}]}, {"text": "We show that classifiers built for different words have different behavior during the bootstrapping process.", "labels": [], "entities": []}, {"text": "If the right parameters for co-training and self-training can be identified (growth size, pool size, and number of iterations, as explained later in the paper), an average error reduction of 25.5% is achieved, with similar performance observed for both co-training and self-training.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 172, "end_pos": 187, "type": "METRIC", "confidence": 0.9752119183540344}]}, {"text": "However, with empirical settings, the error reduction is significantly smaller, with a 9.8% error rate reduction achieved fora new method that combines co-training with majority voting.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.9869246780872345}, {"text": "error rate reduction", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.9802087346712748}]}, {"text": "We first overview the general approach of bootstrapping for natural language learning using co-training and self-training.", "labels": [], "entities": []}, {"text": "We then introduce the problem of supervised word sense disambiguation, and define several local and topical basic classifiers.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.7101985514163971}]}, {"text": "We investigate the applicability of co-training and self-training to supervised word sense disambiguation, starting with these basic classifiers, and perform comparative evaluations of optimal and empirical bootstrapping parameter settings.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.681294173002243}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Size of training, test, and raw data, precision of basic classifiers, and maximum precision obtained for optimal  parameter settings for self-training and co-training. The optimal settings column lists the values for the three parameters  (growth size G / pool size P / iteration I) for which the maximum precision was observed.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9984071850776672}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9838413596153259}, {"text": "precision", "start_pos": 315, "end_pos": 324, "type": "METRIC", "confidence": 0.9479143619537354}]}, {"text": " Table 3: Precision obtained with co-training and self-training, for global and per-word parameter selection.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8976667523384094}]}, {"text": " Table 4: Basic and smoothed co-training, with global and  per-word parameter settings (same settings as listed in", "labels": [], "entities": []}]}