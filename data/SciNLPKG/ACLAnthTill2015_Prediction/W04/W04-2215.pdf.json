{"title": [{"text": "PolyphraZ : a tool for the management of parallel corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "The PolyphraZ tool is being developed in the framework of the TraCorpEx project", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to Internet grow, the number of available documents grows dramatically.", "labels": [], "entities": []}, {"text": "There is a strategic need for companies to produce and manage information written in more than 30 languages.", "labels": [], "entities": []}, {"text": "This requires powerful tools to manage multilingual documents.", "labels": [], "entities": []}, {"text": "Current techniques for handling multilingual documents use large-grained linking (at the level of HTML pages), but don't allow fine-grained synchronization (at paragraph or sentence level) and don't permit bilingual or multilingual editing through the Web.", "labels": [], "entities": []}, {"text": "The interest to synchronize at least at the level of sentences is double: \u00df make it possible to use Machine Aided Human Translation (MAHT) techniques, in particular translation memories, for translating and postediting multilingual documents.", "labels": [], "entities": [{"text": "Machine Aided Human Translation (MAHT)", "start_pos": 100, "end_pos": 138, "type": "TASK", "confidence": 0.801744784627642}, {"text": "translating and postediting multilingual documents", "start_pos": 191, "end_pos": 241, "type": "TASK", "confidence": 0.7561350047588349}]}, {"text": "\u00df add UNL tags at sentence level to store the translations as well as UNL hypergraphs (anglosemantic interlingual representations), from which raw (or rough!) translations into other languages can be obtained from distant \"deconversion\" servers.", "labels": [], "entities": []}, {"text": "Here, we are not concerned with the problem of aligning parallel monolingual documents, or realigning them after they have been modified, a frequent need in the case of leaflets and booklets.) proposed a tool to handle the noncentralized management of the evolution of multilingual parallel documents.", "labels": [], "entities": []}, {"text": "We consider the case, frequent in the industry, where documents are managed centrally, even if they are distributed on several sites.", "labels": [], "entities": []}, {"text": "What happens in general is that they are aligned at the level of large blocks, with one file per block and language (fileXXX.en.htm, fileXXX.fr.htm etc. for HTML pages).", "labels": [], "entities": []}, {"text": "What we propose is to align them at the level of sentences, but of course not to have one file per sentence.", "labels": [], "entities": []}, {"text": "Rather, if there are N languages, fora given \"block\" corresponding to some unit of processing (e.g. visualization), we will have either N monolingual sentence-aligned files, or 1 multilingual file.", "labels": [], "entities": []}, {"text": "In both cases, sentences or place holders for sentences will be linked to a MPM to manage translation and postedition.", "labels": [], "entities": [{"text": "translation", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.9540567398071289}]}, {"text": "We began to build PolyphraZ in the context of the TraCorpEx project (Translation of Corpora of Examples).", "labels": [], "entities": [{"text": "Translation of Corpora of Examples)", "start_pos": 69, "end_pos": 104, "type": "TASK", "confidence": 0.5448269794384638}]}, {"text": "A more recent motivation is to extend the BTEC corpus of CSTAR III (163000 sentences in tourism) to French and Arabic, and to evaluate various Chinese-English MT systems on it.", "labels": [], "entities": [{"text": "BTEC corpus of CSTAR III", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.946973466873169}, {"text": "MT", "start_pos": 159, "end_pos": 161, "type": "TASK", "confidence": 0.9038237929344177}]}, {"text": "We will first present the data we start with, and our goals in more detail.", "labels": [], "entities": []}, {"text": "Ina second part, we will describe the architecture of PolyphraZ, starting from scenarios of use and types of users.", "labels": [], "entities": [{"text": "PolyphraZ", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.9348077774047852}]}, {"text": "Lastly, we will describe the current status of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We also wish that the same platform makes it possible to evaluate automatic translators with automatic methods such as NIST, BLEU, PER, and to use this possibility in CSTAR, to evaluate the Chinese-English and Japanese-English translations.", "labels": [], "entities": [{"text": "NIST", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.830708384513855}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9974337220191956}, {"text": "PER", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9905748963356018}]}, {"text": "To evaluate the results of various MT systems will also enable us to determine \"the best\" (or less bad!) translation, proposable to a contributor as a starting point for revision.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9734963178634644}]}, {"text": "We also want to test a hypothesis by the second author: the quality of the translations could also be evaluated using calculations of distances between sentences and reverse translations.", "labels": [], "entities": []}, {"text": "We have programmed and integrad in PolyphraZ three evaluation methods (NIST, BLEU and distance calculation).", "labels": [], "entities": [{"text": "NIST", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.6086301207542419}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9979756474494934}, {"text": "distance calculation", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.8261078596115112}]}, {"text": "NIST and BLEU are well known.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9783537983894348}, {"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9869685769081116}]}, {"text": "Let us give more details about distance calculation between 2 sentences.", "labels": [], "entities": []}, {"text": "The distance we compute between two strings is a linear combination of two edit distances, one at the level of characters, the other at the level of words.", "labels": [], "entities": []}, {"text": "In general, the edit distance between two strings P1 and P2 of atoms (characters or words here) is the minimal number of suppressions, insertions or replacements of atoms necessary to transform P1 into P2 or, equivalently, P2 into P1.", "labels": [], "entities": []}, {"text": "To compute the edit distance between P1 and P2 at the level of words, one segments them into words, computes the character distances between words of P1 and words of P2, and then computes the word distance using words as \"large characters\".", "labels": [], "entities": []}, {"text": "We use the well-known dynamic programming algorithm of.", "labels": [], "entities": []}, {"text": "To combine the two levels (characters and words), we use the formula: D = (aD char +bD word )/(a+b) ; a +b=1 In certain cases, the representation at the level of the characters is more compact and readable that at the level of words, while it is the opposite in other cases.", "labels": [], "entities": []}, {"text": "In fact, this representation is not \"faithful\" to the trace, because a sequence of exchanges is transformed into a sequence of suppressions and a sequence of insertions.", "labels": [], "entities": []}, {"text": "One interesting and today unsolved problem is how to merge the 2 levels: given 2 sentences and their character and word edit distances, necessarily both minimal, how to produce a trace which would be \"the best\" or \"a best\" combination of the 2 traces?", "labels": [], "entities": []}], "tableCaptions": []}