{"title": [{"text": "Bilingual Parsing with Factored Estimation: Using English to Parse Korean", "labels": [], "entities": [{"text": "Bilingual Parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8834061920642853}]}], "abstractContent": [{"text": "We describe how simple, commonly understood statistical models, such as statistical dependency parsers, proba-bilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best En-glish parse, Korean parse, and word alignment, where these hidden structures all constrain each other.", "labels": [], "entities": [{"text": "statistical dependency parsers", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.6325375934441885}, {"text": "word-to-word translation", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.7103702574968338}, {"text": "word alignment", "start_pos": 308, "end_pos": 322, "type": "TASK", "confidence": 0.7628616988658905}]}, {"text": "The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9852936863899231}]}, {"text": "We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.", "labels": [], "entities": [{"text": "Penn Korean Treebank", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.9815157850583395}, {"text": "parsing Korean", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.8660701513290405}]}], "introductionContent": [{"text": "Consider the problem of parsing a language L for which annotated resources like treebanks are scarce.", "labels": [], "entities": [{"text": "parsing a language L", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8788540661334991}]}, {"text": "Suppose we have a small amount of text data with syntactic annotations and a fairly large corpus of parallel text, for which the other language (e.g., English) is not resourceimpoverished.", "labels": [], "entities": []}, {"text": "How might we exploit English parsers to improve syntactic analysis tools for this language?", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7375819981098175}]}, {"text": "One idea) is to project English analysis onto L data, \"through\" word-aligned parallel text.", "labels": [], "entities": []}, {"text": "To do this, we might use an English parser to analyze the English side of the parallel text and a word-alignment algorithm to induce word correspondences.", "labels": [], "entities": []}, {"text": "By positing a coupling of English syntax with L syntax, we can induce structure on the L side of the parallel text that is in some sense isomorphic to the English parse.", "labels": [], "entities": []}, {"text": "We might take the projection idea a step farther.", "labels": [], "entities": []}, {"text": "A statistical English parser can tell us much more than the hypothesized best parse.", "labels": [], "entities": []}, {"text": "It can be used to find every parse admitted by a grammar, and also scores of those parses.", "labels": [], "entities": []}, {"text": "Similarly, translation models, which yield word alignments, can be used in principle to score competing alignments and offer alternatives to a single-best alignment.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9630860686302185}, {"text": "word alignments", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.704001784324646}]}, {"text": "It might also be beneficial to include the predictions of an L parser, trained on any available annotated L data, however few.", "labels": [], "entities": []}, {"text": "This paper describes how simple, commonly understood statistical models-statistical dependency parsers, probabilistic context-free grammars (PCFGs), and word translation models (TMs)-can be effectively combined into a unified framework that jointly searches for the best English parse, L parse, and word alignment, where these hidden structures are all constrained to be consistent.", "labels": [], "entities": [{"text": "word translation models (TMs)-", "start_pos": 153, "end_pos": 183, "type": "TASK", "confidence": 0.7673692504564921}, {"text": "L parse", "start_pos": 286, "end_pos": 293, "type": "TASK", "confidence": 0.6466014683246613}, {"text": "word alignment", "start_pos": 299, "end_pos": 313, "type": "TASK", "confidence": 0.7684048116207123}]}, {"text": "This inference task is carried out by a bilingual parser.", "labels": [], "entities": []}, {"text": "At present, the model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9865619540214539}]}, {"text": "First, we discuss bilingual parsing ( \u00a72) and show how it can solve the problem of joint English-parse, L-parse, and word-alignment inference.", "labels": [], "entities": [{"text": "bilingual parsing", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7115166783332825}, {"text": "word-alignment inference", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7221486866474152}]}, {"text": "In \u00a73 we describe parameter estimation for each of the factored models, including novel applications of log-linear models to English dependency parsing and Korean morphological analysis.", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.5807058314482371}, {"text": "Korean morphological analysis", "start_pos": 156, "end_pos": 185, "type": "TASK", "confidence": 0.6910445888837179}]}, {"text": "\u00a74 presents Korean parsing results with various monolingual and bilingual algorithms, including our bilingual parsing algorithm.", "labels": [], "entities": [{"text": "Korean parsing", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.49686379730701447}]}, {"text": "We close by reviewing prior work in areas related to this paper ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "Having trained each part of the model, we bring them together in a unified dynamic program to perform inference on the bilingual text as described in \u00a72.", "labels": [], "entities": []}, {"text": "In order to experiment easily with different algorithms, we implemented all the morphological disambiguation and parsing models in this paper in Dyna, anew language for weighted dynamic programming).", "labels": [], "entities": []}, {"text": "For parameter estimation, we used the complementary DynaMITE tool.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.627181202173233}]}, {"text": "Just as CKY parsing starts with words in its chart, the dynamic program chart for the bilingual parser is seeded with the links given in the hypothesized word alignment.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.7175961136817932}]}, {"text": "All our current results are optimal under the model, but as we scale up to more complex data, we might introduce A * heuristics or, at the possible expense of optimality, abeam search or pruning techniques.", "labels": [], "entities": []}, {"text": "Our agenda discipline is uniform-cost search, which guarantees that the first full parse discovered will be optimal-if none of the weights are positive.", "labels": [], "entities": []}, {"text": "In our case we are maximizing sums of negative weights, as if working with log probabilities.", "labels": [], "entities": []}, {"text": "When evaluating our parsing output against the test data from the KTB, we do not claim credit for the single outermost bracketing or for unary productions.", "labels": [], "entities": [{"text": "KTB", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.8685135245323181}]}, {"text": "Since unary productions do not translate well from language to language), we collapse them to their lower nodes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Korean morphological analysis accuracy on ambiguous tokens in the training and test sets: means (and  standard deviations) are shown over five-fold cross-validation. Over 65% of word tokens are ambiguous. The accuracy  of the first tag in each word affects the PCFG and the accuracy of the first morpheme affects the translation model  (under our aggressive morphological lemmatization).", "labels": [], "entities": [{"text": "Korean morphological analysis", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6575730840365092}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9544972777366638}, {"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9877957105636597}, {"text": "PCFG", "start_pos": 271, "end_pos": 275, "type": "DATASET", "confidence": 0.8954229950904846}, {"text": "accuracy", "start_pos": 284, "end_pos": 292, "type": "METRIC", "confidence": 0.9896983504295349}]}, {"text": " Table 5: The gold-standard parse, PCFG parse, bilingual parse, and English translation for two selected test sentences.  GIZA-aligned words are coindexed with subscripts. The bilingual parser recovers from erroneous morphological  tagging in the first sentence and finds the proper NP bracketing in the second.", "labels": [], "entities": [{"text": "PCFG parse", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.6840154230594635}]}, {"text": " Table 7: Bilingual parsing performance on Korean: the table shows means (and standard deviations) for five-fold cross- validation. Bold-faced numbers in the bilingual parsers indicate significant improvements on the PCFG baseline using  the paired-sample t-test at the 0.01 level.", "labels": [], "entities": [{"text": "Bilingual parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7851145565509796}, {"text": "PCFG baseline", "start_pos": 217, "end_pos": 230, "type": "DATASET", "confidence": 0.9544868469238281}]}]}