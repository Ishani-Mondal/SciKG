{"title": [{"text": "Word Sense Discrimination by Clustering Contexts in Vector and Similarity Spaces", "labels": [], "entities": [{"text": "Word Sense Discrimination", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6170439223448435}]}], "abstractContent": [{"text": "This paper systematically compares unsuper-vised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.7373427748680115}]}, {"text": "The context of each instance is represented as a vector in a high dimensional feature space.", "labels": [], "entities": []}, {"text": "Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space.", "labels": [], "entities": []}, {"text": "We employ two different representations of the context in which a target word occurs.", "labels": [], "entities": []}, {"text": "First order context vectors represent the context of each instance of a target word as a vector of features that occur in that context.", "labels": [], "entities": []}, {"text": "Second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context.", "labels": [], "entities": []}, {"text": "We evaluate the discriminated clusters by carrying out experiments using sense-tagged instances of 24 SENSEVAL-2 words and the well known Line, Hard and Serve sense-tagged corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most words in natural language have multiple possible meanings that can only be determined by considering the context in which they occur.", "labels": [], "entities": []}, {"text": "Given a target word used in a number of different contexts, word sense discrimination is the process of grouping these instances of the target word together by determining which contexts are the most similar to each other.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.7650745312372843}]}, {"text": "This is motivated by, who hypothesize that words with similar meanings are often used in similar contexts.", "labels": [], "entities": []}, {"text": "Hence, word sense discrimination reduces to the problem of finding classes of similar contexts such that each class represents a single word sense.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.7700780034065247}]}, {"text": "Put another way, contexts that are grouped together in the same class represent a particular word sense.", "labels": [], "entities": []}, {"text": "While there has been some previous work in sense discrimination (e.g.,,,,,), by comparison it is much less than that devoted to word sense disambiguation, which is the process of assigning a meaning to a word from a predefined set of possibilities.", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7419231832027435}, {"text": "word sense disambiguation", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.7173728148142496}]}, {"text": "However, solutions to disambiguation usually require the availability of an external knowledge source or manually created sensetagged training data.", "labels": [], "entities": []}, {"text": "As such these are knowledge intensive methods that are difficult to adapt to new domains.", "labels": [], "entities": []}, {"text": "By contrast, word sense discrimination is an unsupervised clustering problem.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.774921178817749}]}, {"text": "This is an attractive methodology because it is a knowledge lean approach based on evidence found in simple raw text.", "labels": [], "entities": []}, {"text": "Manually sense tagged text is not required, nor are specific knowledge rich resources like dictionaries or ontologies.", "labels": [], "entities": []}, {"text": "Instances are clustered based on their mutual contextual similarities which can be completely computed from the text itself.", "labels": [], "entities": []}, {"text": "This paper presents a systematic comparison of discrimination techniques suggested by,) and by,).", "labels": [], "entities": []}, {"text": "This paper also proposes and evaluates several extensions to these techniques.", "labels": [], "entities": []}, {"text": "We begin with a summary of previous work, and then a discussion of features and two types of context vectors.", "labels": [], "entities": []}, {"text": "We summarize techniques for clustering in vector versus similarity spaces, and then present our experimental methodology, including a discussion of the data used in our experiments.", "labels": [], "entities": []}, {"text": "Then we describe our approach to the evaluation of unsupervised word sense discrimination.", "labels": [], "entities": [{"text": "evaluation of unsupervised word sense discrimination", "start_pos": 37, "end_pos": 89, "type": "TASK", "confidence": 0.70728832979997}]}, {"text": "Finally we present an analysis of our experimental results, and conclude with directions for future work.", "labels": [], "entities": []}, {"text": "( and) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.", "labels": [], "entities": []}, {"text": "This information is recorded in a (dis)similarity matrix whose rows/columns represent the instances of the target word that are to be discriminated.", "labels": [], "entities": []}, {"text": "The cell entries of the matrix show the degree to which the pair of instances represented by the corresponding row and column are (dis)similar.", "labels": [], "entities": []}, {"text": "The (dis)similarity is computed from the first order context vectors of the instances which show each instance as a vector of features that directly occur near the target word in that instance.", "labels": [], "entities": [{"text": "similarity", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.7028127312660217}]}, {"text": "( introduces second order context vectors that represent an instance by averaging the feature vectors of the content words that occur in the context of the target word in that instance.", "labels": [], "entities": []}, {"text": "These second order context vectors then become the input to the clustering algorithm which clusters the given contexts in vector space, instead of building the similarity matrix structure.", "labels": [], "entities": []}, {"text": "There are some significant differences in the approaches suggested by Pedersen and Bruce and by Sch\u00fctze.", "labels": [], "entities": []}, {"text": "As yet there has not been any systematic study to determine which set of techniques results in better sense discrimination.", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7244218587875366}]}, {"text": "In the sections that follow, we highlight some of the differences between these approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use 24 of the 73 words in the SENSEVAL-2 sensetagged corpus, and the Line, Hard and Serve sensetagged corpora.", "labels": [], "entities": [{"text": "SENSEVAL-2 sensetagged corpus", "start_pos": 33, "end_pos": 62, "type": "DATASET", "confidence": 0.683630625406901}]}, {"text": "Each of these corpora are made up of instances that consist of 2 or 3 sentences that include a single target word that has a manually assigned sense tag.", "labels": [], "entities": []}, {"text": "However, we ignore the sense tags at all times except during evaluation.", "labels": [], "entities": []}, {"text": "At no point do the sense tags enter into the clustering or feature selection processes.", "labels": [], "entities": []}, {"text": "To be clear, we do not believe that unsupervised word sense discrimination needs to be carried out relative to a pre-existing set of senses.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6579990386962891}]}, {"text": "In fact, one of the great advantages of unsupervised technique is that it doesn't need a manually annotated text.", "labels": [], "entities": []}, {"text": "However, here we employ sense-tagged text in order to evaluate the clusters that we discover.", "labels": [], "entities": []}, {"text": "The SENSEVAL-2 data is already divided into training and test sets, and those splits were retained for these experiments.", "labels": [], "entities": [{"text": "SENSEVAL-2 data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.785905122756958}]}, {"text": "The SENSEVAL-2 data is relatively small, in that each word has approximately 50-200 training and test instances.", "labels": [], "entities": [{"text": "SENSEVAL-2 data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7811019122600555}]}, {"text": "The data is particularly challenging for unsupervised algorithms due to the large number of fine grained senses, generally 8 to 12 per word.", "labels": [], "entities": []}, {"text": "The small volume of data combined with large number of possible senses leads to very small set of examples for most of the senses.", "labels": [], "entities": []}, {"text": "As a result, prior to clustering we filter the training and test data independently such that any instance that uses a sense that occurs in less than 10% of the available instances fora given word is removed.", "labels": [], "entities": []}, {"text": "We then eliminate any words that have less than 90 training instances after filtering.", "labels": [], "entities": []}, {"text": "This process leaves us with a set of 24 SENSEVAL-2 words, which includes the 14 nouns, 6 adjectives and 4 verbs that are shown in.", "labels": [], "entities": [{"text": "SENSEVAL-2", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.8756787180900574}]}, {"text": "In creating our evaluation standard, we assume that each instance will be assigned to at most a single cluster.", "labels": [], "entities": []}, {"text": "Therefore if an instance has multiple correct senses associated with it, we treat the most frequent of these as the desired tag, and ignore the others as possible correct answers in the test data.", "labels": [], "entities": []}, {"text": "The Line, Hard and Serve corpora do not have a standard training-test split, so these were randomly divided into 60-40 training-test splits.", "labels": [], "entities": []}, {"text": "Due to the large number of training and test instances for these words, we filtered out instances associated with any sense that occurred in less than 5% of the training or test instances.", "labels": [], "entities": []}, {"text": "We also randomly selected five pairs of words from the SENSEVAL-2 data and mixed their instances together (while retaining the training and test distinction that already existed in the data).", "labels": [], "entities": [{"text": "SENSEVAL-2 data", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.7120656371116638}]}, {"text": "After mixing, the data was filtered such that any sense that made up less than 10% in the training or test data of the new mixed sample was removed; this is why the total number of instances for the mixed pairs is not the same as the sum of those for the individual words.", "labels": [], "entities": []}, {"text": "These mix-words were created in order to provide data that included both fine grained and coarse grained distinctions.", "labels": [], "entities": []}, {"text": "shows all words that were used in our experiments along with their parts of speech.", "labels": [], "entities": []}, {"text": "Thereafter we show the number of training (TRN) and test instances (TST) that remain after filtering, and the number of senses found in the test data (S).", "labels": [], "entities": []}, {"text": "We also show the percentage of the majority sense in the test data (MAJ).", "labels": [], "entities": [{"text": "MAJ)", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.8860682547092438}]}, {"text": "This is particularly useful, since this is the accuracy that would be attained by a baseline clustering algorithm that puts all test instances into a single cluster.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9993046522140503}]}, {"text": "When we cluster test instances, we specify an upper limit on the number of clusters that can be discovered.", "labels": [], "entities": []}, {"text": "In these experiments that value is 7.", "labels": [], "entities": []}, {"text": "This reflects the fact that we do not know a-priori the number of possible senses a word will have.", "labels": [], "entities": []}, {"text": "This also allows us to verify the hypothesis that a good clustering approach will automatically discover approximately same number of clusters as senses for that word, and the extra clusters (7-#actual senses) will contain very few instances.", "labels": [], "entities": []}, {"text": "As can be seen from column S in, most of the words have 2 to 4 senses on an average.", "labels": [], "entities": []}, {"text": "Of the 7 clusters created by an algorithm, we detect the significant clusters by ignoring (throwing out) clusters that contain less than 2% of the total instances.", "labels": [], "entities": []}, {"text": "The instances in the discarded clusters are counted as unclustered instances and are subtracted from the total number of instances.", "labels": [], "entities": []}, {"text": "Our basic strategy for evaluation is to assign available sense tags to the discovered clusters such that the assignment leads to a maximally accurate mapping of senses to clusters.", "labels": [], "entities": []}, {"text": "The problem of assigning senses to clusters becomes one of reordering the columns of a confusion matrix that shows how senses and clusters align such that the diagonal sum is maximized.", "labels": [], "entities": []}, {"text": "This corresponds to several well known problems, among them the Assignment Problem in Operations Research, or determining the maximal matching of a bipartite graph in Graph Theory.", "labels": [], "entities": [{"text": "Assignment", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.8912752866744995}, {"text": "Operations Research", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8473521769046783}, {"text": "Graph Theory", "start_pos": 167, "end_pos": 179, "type": "TASK", "confidence": 0.8201663494110107}]}, {"text": "During evaluation we assign one sense to at most one cluster, and vice versa.", "labels": [], "entities": []}, {"text": "When the number of discovered clusters is the same as the number of senses, then there is a one to one mapping between them.", "labels": [], "entities": []}, {"text": "When the number of clusters is greater than the number of actual senses, then some clusters will be left unassigned.", "labels": [], "entities": []}, {"text": "And when the number of senses is greater than the number of clusters, some senses will not be assigned to any cluster.", "labels": [], "entities": []}, {"text": "The reason for not assigning a single sense to multiple clusters or multiple senses to one cluster is that, we are assuming one sense per instance and one sense per cluster.", "labels": [], "entities": []}, {"text": "We measure the precision and recall based on this maximally accurate assignment of sense tags to clusters.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996631145477295}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996269941329956}]}, {"text": "Precision is defined as the number of instances that are clustered correctly divided by the number of instances clustered, while recall is the number of instances clustered correctly over the total number of instances.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9907446503639221}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9993728995323181}]}, {"text": "From that we compute the F-measure, which is two times the precision and recall, divided by the sum of precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.997582197189331}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.999632716178894}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9974397420883179}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9988505840301514}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9921995997428894}]}, {"text": "We present the discrimination results for six configurations of features, context representations and clustering algorithms.", "labels": [], "entities": []}, {"text": "These were run on each of the 27 target words, and also on the five mixed words.", "labels": [], "entities": []}, {"text": "What follows is a concise description of each configuration.", "labels": [], "entities": []}, {"text": "\u2022 PB1 : First order context vectors, using cooccurrence features, are clustered in similarity space using the UPGMA technique.", "labels": [], "entities": []}, {"text": "\u2022 PB2 : Same as PB1, except that the first order context vectors are clustered in vector space using Repeated Bisections.", "labels": [], "entities": []}, {"text": "\u2022 PB3: Same as PB1, except the first order context vectors used bigram features instead of cooccurrences.", "labels": [], "entities": [{"text": "PB3", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.9299800395965576}]}, {"text": "All of the PB experiments use first order context representations that correspond to the approach suggested by Pedersen and Bruce.", "labels": [], "entities": []}, {"text": "\u2022 SC1: Second order context vectors of instances were clustered in vector space using the Repeated Bisections technique.", "labels": [], "entities": []}, {"text": "The context vectors were created from the word co-occurrence matrix whose dimensions were reduced using SVD.", "labels": [], "entities": []}, {"text": "\u2022 SC2: Same as SC1 except that the second order context vectors are converted to a similarity matrix and clustered using the UPGMA method.", "labels": [], "entities": [{"text": "UPGMA", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.8309958577156067}]}, {"text": "\u2022 SC3: Same as SC1, except the second order context vectors were created from the bigram matrix.", "labels": [], "entities": []}, {"text": "All of the SC experiments use second order context vectors and hence follow the approach suggested by Sch\u00fctze.", "labels": [], "entities": []}, {"text": "Experiment PB2 clusters the Pedersen and Bruce style (first order) context vectors using the Sch\u00fctze like clustering scheme, while SC2 tries to seethe effect of using the Pedersen and Bruce style clustering method on Sch\u00fctze style (second order) context vectors.", "labels": [], "entities": []}, {"text": "The motivation behind experiments PB3 and SC3 is to try bigram features in both PB and SC style context vectors.", "labels": [], "entities": []}, {"text": "The F-measure associated with the discrimination of each word is shown in.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9973256587982178}]}, {"text": "Any score that is significantly greater than the majority sense (according to a paired t-test) is shown in boldface.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Bigrams vs. Co-occurrences", "labels": [], "entities": []}, {"text": " Table 3: Repeated Bisections vs. UPGMA", "labels": [], "entities": [{"text": "Repeated Bisections", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.5158923268318176}, {"text": "UPGMA", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8459596037864685}]}]}