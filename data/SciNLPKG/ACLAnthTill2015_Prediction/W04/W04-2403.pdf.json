{"title": [{"text": "A Semantic Kernel for Predicate Argument Classification", "labels": [], "entities": [{"text": "Predicate Argument Classification", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.8865447044372559}]}], "abstractContent": [{"text": "Automatically deriving semantic structures from text is a challenging task for machine learning.", "labels": [], "entities": []}, {"text": "The flat feature representations, usually used in learning models, can only partially describe structured data.", "labels": [], "entities": []}, {"text": "This makes difficult the processing of the semantic information that is embedded into parse-trees.", "labels": [], "entities": []}, {"text": "In this paper anew kernel for automatic classification of predicate arguments has been designed and experimented.", "labels": [], "entities": [{"text": "automatic classification of predicate arguments", "start_pos": 30, "end_pos": 77, "type": "TASK", "confidence": 0.7810370683670044}]}, {"text": "It is based on sub-parse-trees annotated with predicate argument information from PropBank corpus.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.9791734516620636}]}, {"text": "This kernel , exploiting the convolution properties of the parse-tree kernel, enables us to learn which syntactic structures can be associated with the arguments defined in PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 173, "end_pos": 181, "type": "DATASET", "confidence": 0.9199937582015991}]}, {"text": "Support Vector Machines (SVMs) using such a kernel classify arguments with a better accuracy than SVMs based on linear kernel.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9967603087425232}]}], "introductionContent": [{"text": "Several linguistic theories, e.g., claim that semantic information in natural language texts is connected to syntactic structures.", "labels": [], "entities": []}, {"text": "Hence, to deal with natural language semantics, the learning algorithm should be able to represent and process structured data.", "labels": [], "entities": []}, {"text": "The classical solution adopted for such tasks is to convert syntax structures in a flat feature representation, which is suitable fora given learning model.", "labels": [], "entities": []}, {"text": "The main drawback is structures may not be properly represented by flat features as: (1) these latter may not be able to capture the required properties or (2) the feature designer may not know what structure properties enable the processing of semantic information.", "labels": [], "entities": []}, {"text": "In particular, these problems arise for semantic information represented via predicate argument structures defined on syntactic parse trees.", "labels": [], "entities": []}, {"text": "For example, shows the parse tree of the sentence: \"Paul gives a lecture in Rome\" along with the annotation of predicate arguments.", "labels": [], "entities": []}, {"text": "A predicate maybe a verb or a noun or an adjective whereas generally Arg 0 stands for agent, Arg 1 for direct objector theme or patient and ArgM may indicate locations, as in our example.", "labels": [], "entities": [{"text": "Arg 0", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9663306176662445}, {"text": "Arg", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9853515625}, {"text": "ArgM", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9896266460418701}]}, {"text": "A standard for predicate argument annotation is provided in the PropBank project.", "labels": [], "entities": [{"text": "predicate argument annotation", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.8296511769294739}, {"text": "PropBank project", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.9350378513336182}]}, {"text": "It has produced one million word corpus annotated with predicate-argument structures on top of the Penn Treebank 2 Wall Street Journal texts.", "labels": [], "entities": [{"text": "Penn Treebank 2 Wall Street Journal texts", "start_pos": 99, "end_pos": 140, "type": "DATASET", "confidence": 0.9689981171063015}]}, {"text": "In this way, fora large number of the Penn TreeBank parse-trees, there are available predicate annotations in a style similar to that shown in.", "labels": [], "entities": [{"text": "Penn TreeBank parse-trees", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.9751949707667033}]}, {"text": "In PropBank only verbs are considered to be predicates whereas arguments are labeled sequentially from Arg 0 to Arg 9 . In addition to these core arguments, adjunctive arguments are marked up.", "labels": [], "entities": [{"text": "Arg", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9445469975471497}]}, {"text": "They include functional tags, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative and ArgM-TMP stands fora temporal.", "labels": [], "entities": [{"text": "ArgM-DIR", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9930180311203003}, {"text": "ArgM-LOC", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.992662787437439}, {"text": "ArgM-TMP", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.985123336315155}]}, {"text": "An example of PropBank markup is: Automatically recognizing the boundaries and classifying the type of arguments allows Natural Language Processing systems (e.g. Information Extraction, Question Answering or Summarization) to answer questions such as \"Who\", \"When\", \"What\", \"Where\", \"Why\", and soon.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.7399342954158783}, {"text": "Question Answering or Summarization) to answer questions such as \"Who\", \"When\", \"What\", \"Where\", \"Why\",", "start_pos": 186, "end_pos": 289, "type": "Description", "confidence": 0.749284655849139}]}, {"text": "Given the importance of this task for Natural Language Processing applications, several machine learning approaches for argument identification and classification have been developed (.", "labels": [], "entities": [{"text": "argument identification and classification", "start_pos": 120, "end_pos": 162, "type": "TASK", "confidence": 0.7225786074995995}]}, {"text": "Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation.", "labels": [], "entities": []}, {"text": "The major problem of this choice is that there is no linguistic theory that supports the selection of syntactic features to recognize semantic structures.", "labels": [], "entities": []}, {"text": "As a consequence, researchers are still trying to extend the basic features with other ones, e.g. (, to improve the flat feature space.", "labels": [], "entities": []}, {"text": "Convolution kernels area viable alternative to flat feature representation that aims to capture the structural information in term of sub-structures.", "labels": [], "entities": []}, {"text": "The kernel functions can be used to measure similarities between two objects without explicitly evaluating the object features.", "labels": [], "entities": []}, {"text": "That is, we do not need to understand which syntactic feature maybe suited for representing semantic data.", "labels": [], "entities": []}, {"text": "We need only to define the similarity function between two semantic structures.", "labels": [], "entities": []}, {"text": "An example of convolution kernel on the parse-tree space is given in).", "labels": [], "entities": []}, {"text": "The aim was to design a novel syntactic parser by looking at the similarity between the testing parse-trees and the correct parse-trees available for training.", "labels": [], "entities": []}, {"text": "In this paper, we define a kernel in a semantic structure space to learn the classification function of predicate arguments.", "labels": [], "entities": []}, {"text": "The main idea is to select portions of syntactic/semantic trees that include the target <predicate, argument> pair and to define a kernel function between these objects.", "labels": [], "entities": []}, {"text": "If our similarity function is well defined the learning model will converge and provide an effective argument classification.", "labels": [], "entities": []}, {"text": "Experiments on PropBank data show not only that Support Vector Machines (SVMs) trained with the proposed semantic kernel converge but also that they have a higher accuracy than SVMs trained with a linear kernel on the standard features proposed in (.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.9215911030769348}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9973992109298706}]}, {"text": "This provides apiece of evidence that convolution kernel can be used to learn semantic linguistic structures.", "labels": [], "entities": []}, {"text": "Moreover, interesting research lines on the use of kernel for NLP are enabled, e.g. question classification in Question/Answering or automatic template designing in Information Extraction.", "labels": [], "entities": [{"text": "question classification", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.8254654705524445}, {"text": "Question/Answering", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.6549628973007202}, {"text": "template designing", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.6967594474554062}]}, {"text": "The remaining of this paper is organized as follows: Section 2 defines the Predicate Argument Extraction problem and the standard solution to solve it.", "labels": [], "entities": [{"text": "Predicate Argument Extraction problem", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7714627534151077}]}, {"text": "In Section 3 we present our approach based on the parse-tree kernel whereas in Section 4 we show our comparative results between SVMs using standard features and the proposed kernel.", "labels": [], "entities": []}, {"text": "Finally, Section 5 summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments, we used PropBank (www.cis.upenn.edu/\u223cace) along with PennTreeBank 5 2 (www.cis.upenn.edu/\u223ctreebank).", "labels": [], "entities": [{"text": "PropBank", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.8998580574989319}, {"text": "PennTreeBank 5 2", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9381391406059265}]}, {"text": "This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (.", "labels": [], "entities": []}, {"text": "In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.", "labels": [], "entities": []}, {"text": "We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing data to affect the global performance.", "labels": [], "entities": [{"text": "Arg9", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7077646255493164}, {"text": "ArgA", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.7186513543128967}, {"text": "ArgM", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.7224979996681213}]}, {"text": "In some characteristics of the corpus used in our experiments are reported.", "labels": [], "entities": []}, {"text": "The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at http://svmlight.joachims.org/ with the default linear kernel for the standard feature evaluations.", "labels": [], "entities": []}, {"text": "For processing our semantic structures, we implemented our own kernel and we used it inside SVM-light.", "labels": [], "entities": [{"text": "SVM-light", "start_pos": 92, "end_pos": 101, "type": "DATASET", "confidence": 0.8971070647239685}]}, {"text": "The classification performances were evaluated using the f 1 measure for single arguments as each of them has a different Precision and Recall and by using the accuracy for the final multi-class classifier as the global Precision = Recall = accuracy.", "labels": [], "entities": [{"text": "Precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9832192063331604}, {"text": "Recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9219643473625183}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9987868666648865}, {"text": "Recall", "start_pos": 232, "end_pos": 238, "type": "METRIC", "confidence": 0.5755975842475891}, {"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9855571389198303}]}, {"text": "The latter measure allows us to compare the results with previous literature works, e.g. (. To evaluate the effectiveness of our new kernel we divided the experiments in 3 steps: \u2022 The evaluation of SVMs trained with standard features in a linear kernel, for comparison purposes.", "labels": [], "entities": []}, {"text": "\u2022 The estimation of the \u03bb parameter (equations 4 and 5) for SK from the validation-set . \u2022 The performance measurement of SVMs, using SK along with \u03bb computed in the previous step.", "labels": [], "entities": []}, {"text": "Additionally, both Linear and SK kernels were evaluated using different percentages of training data to compare the gradients of their learning curves.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Characteristics of the corpus used in the experiments.", "labels": [], "entities": []}, {"text": " Table 3: f 1 of SVMs using linear and semantic kernel com-", "labels": [], "entities": []}]}