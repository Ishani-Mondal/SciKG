{"title": [{"text": "Audio Hot Spotting and Retrieval Using Multiple Features", "labels": [], "entities": [{"text": "Audio Hot Spotting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.4936401844024658}]}], "abstractContent": [{"text": "This paper reports our ongoing efforts to exploit multiple features derived from an audio stream using source material such as broadcast news, teleconferences, and meetings.", "labels": [], "entities": []}, {"text": "These features are derived from algorithms including automatic speech recognition, automatic speech indexing, speaker identification, prosodic and audio feature extraction.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7246691137552261}, {"text": "speech indexing", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.7194460332393646}, {"text": "speaker identification", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.8214269578456879}, {"text": "audio feature extraction", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.6796560883522034}]}, {"text": "We describe our research prototype-the Audio Hot Spotting System-that allows users to query and retrieve data from multimedia sources utilizing these multiple features.", "labels": [], "entities": []}, {"text": "The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event.", "labels": [], "entities": []}, {"text": "In addition to spoken keywords, the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate, and other non-lexical features, including applause and laughter.", "labels": [], "entities": []}, {"text": "Finally, we discuss our approach to semantic, morphological, phonetic query expansion to improve audio retrieval performance and to access cross-lingual data.", "labels": [], "entities": [{"text": "phonetic query expansion", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6620094875494639}, {"text": "audio retrieval", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.6912745982408524}]}], "introductionContent": [{"text": "Audio contains more information than is conveyed by the text transcript produced by an automatic speech recognizer.", "labels": [], "entities": []}, {"text": "Information such as: a) who is speaking, b) the vocal effort used by each speaker, and c) prosodic features and certain non-speech background sounds, are lost in a simple speech transcript.", "labels": [], "entities": []}, {"text": "In addition, due to the variability of acoustic channels and noise conditions, speaker variance, language models the recognizer is based on, and the limitations of automatic speech recognition (ASR), speech transcripts can be full of errors.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 164, "end_pos": 198, "type": "TASK", "confidence": 0.8180686334768931}]}, {"text": "Deletion errors can prevent the users from finding what they are looking for from audio or video data, while insertion and substitution errors can be misleading and confusing.", "labels": [], "entities": []}, {"text": "Our approach is to automatically detect, index, and retrieve multiple features from the audio stream to compensate for the weakness of using speech transcribed text alone.", "labels": [], "entities": []}, {"text": "The multiple time-stamped features from the audio include an automatically generated index derived from ASR speech transcripts, automatic speaker identification, and automatically identified prosodic and audio cues.", "labels": [], "entities": [{"text": "ASR speech transcripts", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.6802269617716471}, {"text": "speaker identification", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.6570626944303513}]}, {"text": "In this paper, we describe our indexing algorithm that automatically identifies potential search keywords that are information rich and provide a quick clue to the document content.", "labels": [], "entities": []}, {"text": "We also describe how our Audio Hot Spotting prototype system uses multiple features to automatically locate regions of interest in an audio or video file that meet a user's specified query criteria.", "labels": [], "entities": []}, {"text": "In the query, users may search for keywords or phrases, speakers, keywords and speakers together, non-verbal speech characteristics, or non-speech signals of interest.", "labels": [], "entities": []}, {"text": "The system also uses multiple features to refine query results.", "labels": [], "entities": []}, {"text": "Finally, we discuss our query expansion mechanism by using natural language processing techniques to improve retrieval performance and to access crosslingual data.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.8014581799507141}]}], "datasetContent": [], "tableCaptions": []}