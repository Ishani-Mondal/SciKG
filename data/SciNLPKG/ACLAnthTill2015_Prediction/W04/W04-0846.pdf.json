{"title": [{"text": "Context Clustering for Word Sense Disambiguation Based on Modeling Pairwise Context Similarities", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.8093430399894714}]}], "abstractContent": [{"text": "Traditionally, word sense disambiguation (WSD) involves a different context model for each individual word.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.8444115618864695}]}, {"text": "This paper presents anew approach to WSD using weakly supervised learning.", "labels": [], "entities": [{"text": "WSD", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9931060671806335}]}, {"text": "Statistical models are not trained for the contexts of each individual word, but for the similarities between context pairs at category level.", "labels": [], "entities": []}, {"text": "The insight is that the correlation regularity between the sense distinction and the context distinction can be captured at category level, independent of individual words.", "labels": [], "entities": []}, {"text": "This approach only requires a limited amount of existing annotated training corpus in order to disambiguate the entire vocabulary.", "labels": [], "entities": []}, {"text": "A context clustering scheme is developed within the Bayesian framework.", "labels": [], "entities": [{"text": "context clustering", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.7011802792549133}]}, {"text": "A maximum entropy model is then trained to represent the generative probability distribution of context similarities based on heterogeneous features, including trigger words and parsing structures.", "labels": [], "entities": []}, {"text": "Statistical annealing is applied to derive the final context clusters by globally fitting the pairwise context similarity distribution.", "labels": [], "entities": []}, {"text": "Benchmarking shows that this new approach significantly outperforms the existing WSD systems in the unsupervised category, and rivals supervised WSD systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is one of the central problems in Natural Language Processing.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7893424878517786}, {"text": "Natural Language Processing", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6363138655821482}]}, {"text": "The difficulty of this task lies in the fact that context features and the corresponding statistical distribution are different for each individual word.", "labels": [], "entities": []}, {"text": "Traditionally, WSD involves modeling the contexts for each word.] uses the Na\u00efve Bayes method for context modeling which requires a manually truthed corpus for each ambiguous word.", "labels": [], "entities": [{"text": "WSD", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9141958355903625}, {"text": "context modeling", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7108174860477448}]}, {"text": "This causes a serious Knowledge Bottleneck.", "labels": [], "entities": [{"text": "Knowledge Bottleneck", "start_pos": 22, "end_pos": 42, "type": "DATASET", "confidence": 0.8404617607593536}]}, {"text": "The situation is worse when considering the domain dependency of word senses.", "labels": [], "entities": []}, {"text": "To avoid the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed.", "labels": [], "entities": []}, {"text": "These include the bootstrapping approach and the context clustering approach].", "labels": [], "entities": [{"text": "context clustering", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7185773849487305}]}, {"text": "We previously at-tempted combining both types of evidence but only achieved limited improvement due to the lack of a proper modeling of information over-lapping . This paper presents anew algorithm that addresses these problems.", "labels": [], "entities": []}, {"text": "A novel context clustering scheme based on modeling the similarities between pairwise contexts at category level is presented in the Bayesian framework.", "labels": [], "entities": [{"text": "context clustering", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7467642724514008}]}, {"text": "A generative maximum entropy model is then trained to represent the generative probability distribution of pairwise context similarities based on heterogeneous features that cover both cooccurring words and parsing structures.", "labels": [], "entities": [{"text": "generative maximum entropy", "start_pos": 2, "end_pos": 28, "type": "TASK", "confidence": 0.9013004899024963}]}, {"text": "Statistical annealing is used to derive the final context clusters by globally fitting the pairwise context similarities.", "labels": [], "entities": []}, {"text": "This new algorithm only requires a limited amount of existing annotated corpus to train the generative maximum entropy model for the entire vocabulary.", "labels": [], "entities": []}, {"text": "This capability is based on the observation that a system does not necessarily require training data for word A in order to disambiguate A.", "labels": [], "entities": []}, {"text": "The insight is that the correlation regularity between the sense distinction and the context distinction can be captured at category level, independent of individual words.", "labels": [], "entities": []}, {"text": "In what follows, Section 2 formulates WSD as a context clustering task based on the pairwise context similarity model.", "labels": [], "entities": [{"text": "WSD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9179354310035706}, {"text": "context clustering task", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7736509442329407}]}, {"text": "The context clustering algorithm is described in Sections 3 and 4, corresponding to the two key aspects of the algorithm, i.e. the generative maximum entropy modeling and the annealing-based optimization.", "labels": [], "entities": [{"text": "context clustering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7350582182407379}, {"text": "generative maximum entropy modeling", "start_pos": 131, "end_pos": 166, "type": "TASK", "confidence": 0.8734319061040878}]}, {"text": "Section 5 describes benchmarks and conclusion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}