{"title": [{"text": "Hierarchical Recognition of Propositional Arguments with Perceptrons", "labels": [], "entities": [{"text": "Hierarchical Recognition of Propositional Arguments", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8104193568229675}]}], "abstractContent": [], "introductionContent": [{"text": "We describe a system for the CoNLL-2004 Shared Task on Semantic Role Labeling (Carreras and M` arquez, 2004a).", "labels": [], "entities": [{"text": "CoNLL-2004 Shared Task on Semantic Role Labeling", "start_pos": 29, "end_pos": 77, "type": "TASK", "confidence": 0.5106883347034454}]}, {"text": "The system implements a two-layer learning architecture to recognize arguments in a sentence and predict the role they play in the propositions.", "labels": [], "entities": []}, {"text": "The exploration strategy visits possible arguments bottom-up, navigating through the clause hierarchy.", "labels": [], "entities": []}, {"text": "The learning components in the architecture are implemented as Perceptrons, and are trained simultaneously online, adapting their behavior to the global target of the system.", "labels": [], "entities": []}, {"text": "The learning algorithm follows the global strategy introduced in) and adapted in) for partial parsing tasks.", "labels": [], "entities": [{"text": "partial parsing tasks", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.6681051452954611}]}], "datasetContent": [{"text": "We have build a system which implements the presented architecture for recognizing arguments and their semantic roles.", "labels": [], "entities": []}, {"text": "The configuration of learning functions, related to the roles in the CoNLL-2004 data, is set as follows : \u2022 Five score functions for the A0-A4 types, and two shared filtering functions F AN Sand F AN E . \u2022 For each of the 13 adjunct types (AM-*), a score function and a pair of filtering functions.", "labels": [], "entities": [{"text": "CoNLL-2004 data", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9736233651638031}]}, {"text": "\u2022 Three score functions for the R0-R2 types, and two filtering functions F R Sand F R E shared among them.", "labels": [], "entities": []}, {"text": "\u2022 For verbs, a score function and an end filter.", "labels": [], "entities": []}, {"text": "We ran the learning algorithm on the training set (with predicted input syntax) with a polynomial kernel of degree 2, for up to 8 epochs.", "labels": [], "entities": []}, {"text": "presents the obtained results on the development set, either artificial or real.", "labels": [], "entities": []}, {"text": "The second and third rows provide, respectively, the loss suffered because of errors in the filtering and scoring layer.", "labels": [], "entities": []}, {"text": "The filtering layer performs reasonably well, since 89.44% recall can be achieved on the top of it.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9967789053916931}]}, {"text": "However, the scoring functions clearly moderate the performance, since working with perfect start-end functions only achieve an F 1 at 75.60.", "labels": [], "entities": [{"text": "F 1", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9948046803474426}]}, {"text": "Finally, table 2 presents final detailed results on the test set.", "labels": [], "entities": []}], "tableCaptions": []}