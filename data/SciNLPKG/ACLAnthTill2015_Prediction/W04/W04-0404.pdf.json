{"title": [{"text": "Translation by Machine of Complex Nominals: Getting it Right", "labels": [], "entities": [{"text": "Getting it Right", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7878323793411255}]}], "abstractContent": [{"text": "We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.", "labels": [], "entities": [{"text": "compositionally translating noun-noun (NN) compounds", "start_pos": 24, "end_pos": 76, "type": "TASK", "confidence": 0.6797538484845843}, {"text": "candidate generation", "start_pos": 146, "end_pos": 166, "type": "TASK", "confidence": 0.7438304722309113}]}, {"text": "We propose a support vector learning-based method employing target language corpus and bilingual dictionary data, and evaluate it over a English Japanese machine translation task.", "labels": [], "entities": [{"text": "English Japanese machine translation task", "start_pos": 137, "end_pos": 178, "type": "TASK", "confidence": 0.6971385300159454}]}, {"text": "We show the proposed method to be superior to previous methods and also robust over low-frequency NN compounds.", "labels": [], "entities": []}], "introductionContent": [{"text": "No such segmentation boundary is indicated in the original Japanese.).", "labels": [], "entities": []}, {"text": "We identified NN compounds in each corpus using the method described in E 2.2 below, and from this, derived the statistics of occurrence presented in.", "labels": [], "entities": []}, {"text": "The token coverage of NN compounds in each corpus refers to the percentage of words which are contained in NN compounds; based on our corpora, we estimate this figure to be as high as 3-5%.", "labels": [], "entities": []}, {"text": "If we then look at the average token frequency of each distinct NN compound type, we see that it is a relatively modest figure given the size of each of the corpora, the reason for which is seen in the huge number of distinct NN compound types.", "labels": [], "entities": []}, {"text": "Combining these observations, we see that a translator or MT system attempting to translate one of these corpora will run across NN compounds with high frequency, but that each individual NN compound will occur only a few times (with around 45-60% occuring only once).", "labels": [], "entities": []}, {"text": "The upshot of this for MT systems and translators is that NN compounds are too varied to be able to pre-compile an exhaustive list of translated NN compounds, and must instead be able to deal with novel NN compounds on the fly.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9773109555244446}]}, {"text": "This claim is supported by, who found that static bilingual dictionaries had a type coverage of around 84% and 94% over the top-250 most frequent English and Japanese NN compounds, respectively, but only 27% and 60%, respectively, over a random sample of NN compounds occurring more than 10 times in the corpus.", "labels": [], "entities": [{"text": "type coverage", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9083664119243622}]}], "datasetContent": [{"text": "We evaluate the method over both JE and EJ translation selection, using the two sets of 750 NN compounds described in E 2.2.", "labels": [], "entities": [{"text": "EJ translation selection", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6807753245035807}]}, {"text": "In each case, we first evaluate system performance according to goldstandard accuracy, i.e. the proportion of inputs for which the (unique) gold-standard translation is ranked top amongst the translation candidates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.5908347964286804}]}, {"text": "For the method to have a chance at selecting the goldstandard translation, we clearly must be able to generate it.", "labels": [], "entities": []}, {"text": "The first step is thus to identify inputs which have translation-compositional goldstandard translations, and generate the translation candidates for each.", "labels": [], "entities": []}, {"text": "The translation-compositional data has the distribution given in.", "labels": [], "entities": []}, {"text": "The overall proportion of translation-compositional inputs is somewhat lower than suggested by, although this is conditional on the coverage of the particular dictionaries we use.", "labels": [], "entities": []}, {"text": "The degree of translation-compositionality appears to be relatively constant across the three frequency bands, a somewhat surprising finding as we had expected the lower frequency NN compounds to be less conventionalised and therefore have more straightforwardly compositional translations.", "labels": [], "entities": []}, {"text": "We use the translation-compositional test data to evaluate the proposed method (SVM5 ) against CTQ and a simple baseline derived from CTQ, which takes the most probable fully-specified translation   ).", "labels": [], "entities": [{"text": "CTQ", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.8188827037811279}, {"text": "CTQ", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.8879415988922119}]}, {"text": "We additionally tested the proposed method using just corpus-based features (SVM5 ) and bilingual dictionary-based features (SVM8 ) to get a better sense for the relative impact of each on overall performance.", "labels": [], "entities": []}, {"text": "In the case of the proposed method and its derivants, evaluation is according to 10-fold stratified cross-validation, with stratification taking place across the three frequency bands.", "labels": [], "entities": []}, {"text": "The average number of translations generated for the JE dataset was 205.6, and that for the EJ dataset was 847.5.", "labels": [], "entities": [{"text": "JE dataset", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.959801197052002}, {"text": "EJ dataset", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.9806033670902252}]}, {"text": "We were unable to generate any translations for 17 (2.3%) and 57 (7.6%) of the NN compounds in the JE and EJ datasets, respectively, due to there being no word-level translations for N\u00a9 and/or N in the combined ALTDIC/EDICT dictionaries.", "labels": [], "entities": [{"text": "JE and EJ datasets", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.7863476425409317}, {"text": "ALTDIC/EDICT dictionaries", "start_pos": 211, "end_pos": 236, "type": "DATASET", "confidence": 0.7292049527168274}]}, {"text": "The gold-standard accuracies are presented in Table 5, with figures in boldface indicating a statistically significant improvement over both CTQ and the baseline.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.8393364548683167}, {"text": "CTQ", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.9355071783065796}]}, {"text": "Except for SVM8 in the EJ task, all evaluated methods surpass the baseline, and all variants of SVM surpassed CTQ.", "labels": [], "entities": [{"text": "CTQ", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.8582289814949036}]}, {"text": "SVM5 appears to successfully consolidate on SVM5 and SVM8 , indicating that our modelling of target language corpus and crosslingual data is complementary.", "labels": [], "entities": [{"text": "SVM5", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9465017318725586}, {"text": "SVM8", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8988427519798279}]}, {"text": "Overall, the results for the EJ task are higher than those for the JE task.", "labels": [], "entities": [{"text": "JE task", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.6733797490596771}]}, {"text": "Part of the reason for this is that Japanese has less translation variability fora given pair of word translations, as discussed below.", "labels": [], "entities": []}, {"text": "In looking through the examples where a goldstandard translation was not returned by the different methods, we often find that the uniqueness of gold-standard translation has meant that equally good translations (e.g. dollar note vs. the gold-standard translation dollar bill for\u00a8\u00a9    translations are defined to be syntactically unmarked, capture the basic semantics of the source language expression and allow the source language expression to be recovered with reasonable confidence.", "labels": [], "entities": []}, {"text": "While evaluation of L1-recoverability is inevitably subjective, we minimise bias towards any given system by performing the L1-recoverability annotation for all methods in a single batch, without giving the annotator any indication of which method selected which translation.", "labels": [], "entities": []}, {"text": "The average number of English and Japanese L1-recoverable translations were 1.9 and 0.94, respectively.", "labels": [], "entities": []}, {"text": "The principle reason for the English data being more forgiving is the existence of possessive-and PP-based paraphrases of NN gold-standard translations (e.g. ammendment of rule(s) as an L1-recoverable paraphrase of rule ammendment).", "labels": [], "entities": []}, {"text": "We combine the gold-standard data and L1-recoverable translation data together into a single silver standard translation dataset, based upon which we calculate silver-standard translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.907680869102478}]}, {"text": "The results for the translation-compositional data are given in.", "labels": [], "entities": []}, {"text": "Once again, we find that the proposed method is superior to the baseline and CTQ, and that the combination of crosslingual and target language corpus data is superior to the individual data sources.", "labels": [], "entities": []}, {"text": "SVM8 fares particularly badly under silver-standard evaluation as it is unable to capture the target language lexical and constructional preferences as are needed to generate syntactically-unmarked, natural-sounding translations.", "labels": [], "entities": [{"text": "SVM8", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8550089001655579}]}, {"text": "Unsurprisingly, the increment between gold-standard accuracy and silver-standard accuracy is greater for English than Japanese.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9619622230529785}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9281885623931885}]}], "tableCaptions": [{"text": " Table 1: Corpus occurrence of NN compounds", "labels": [], "entities": []}, {"text": " Table 2. This al- lows us to analyse the robustness of our method over  data of different frequencies.", "labels": [], "entities": []}, {"text": " Table 4: Analysis of translation compositionality", "labels": [], "entities": [{"text": "translation compositionality", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8836403787136078}]}, {"text": " Table 5: Gold-standard translation accuracies", "labels": [], "entities": []}, {"text": " Table 6: Silver-standard translation accuracies", "labels": [], "entities": []}, {"text": " Table 7: JE translation accuracies across different  frequency bands", "labels": [], "entities": [{"text": "JE translation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6747230887413025}]}, {"text": " Table 8: EJ translation accuracies across different  frequency bands", "labels": [], "entities": [{"text": "EJ translation accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6528781056404114}]}, {"text": " Table 9: Silver-standard translation accuracies over  non-translation-compositional data", "labels": [], "entities": []}]}