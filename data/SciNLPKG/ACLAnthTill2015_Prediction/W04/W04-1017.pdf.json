{"title": [], "abstractContent": [{"text": "Most approaches to extractive summarization define a set of features upon which selection of sentences is based, using algorithms independent of the features themselves.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.755184680223465}]}, {"text": "We propose anew set of features based on low-level, atomic events that describe relationships between important actors in a document or set of documents.", "labels": [], "entities": []}, {"text": "We investigate the effect this new feature has on extractive summarization, compared with a baseline feature set consisting of the words in the input documents, and with state-of-the-art summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.6672441959381104}]}, {"text": "Our experimental results indicate that not only the event-based features offer an improvement in summary quality over words as features, but that this effect is more pronounced for more sophisticated summarization methods that avoid redundancy in the output.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main goal of extractive summarization can be concisely formulated as extracting from the input pieces of text which contain the information about the most important concepts mentioned in the input text or texts.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.5771786868572235}]}, {"text": "This definition conceals a lot of important issues that should betaken into consideration in the process of summary construction.", "labels": [], "entities": [{"text": "summary construction", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.8638353943824768}]}, {"text": "First, it is necessary to identify the important concepts which should be described in the summary.", "labels": [], "entities": []}, {"text": "When those important concepts are identified then the process of summarization can be presented as: 1.", "labels": [], "entities": [{"text": "summarization", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.9598278999328613}]}, {"text": "Break the input text into textual units (sentences, paragraphs, etc.).", "labels": [], "entities": []}, {"text": "2. See what concepts each textual unit covers.", "labels": [], "entities": []}, {"text": "3. Choose a particular textual unit for the output according to the concepts present in all textual units.", "labels": [], "entities": []}, {"text": "4. Continue choosing textual units until reaching the desired length of the summary.", "labels": [], "entities": []}, {"text": "Some current summarization systems add a clustering step, substituting the analysis of all the textual units by the analysis of representative units from each cluster.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9618672728538513}]}, {"text": "Clustering is helpful for avoiding repetitions in the summary.", "labels": [], "entities": []}, {"text": "In this paper we propose anew representation for concepts and correspondingly anew feature on which summarization can be based.", "labels": [], "entities": [{"text": "summarization", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.9609061479568481}]}, {"text": "We adapt the algorithm we proposed earlier) for assigning to each sentence a list of low-level, atomic events.", "labels": [], "entities": []}, {"text": "These events capture information about important named entities for the input text or texts, and the relationships between these named entities.", "labels": [], "entities": []}, {"text": "We also discuss a general model which treats summarization as a threecomponent problem, involving the identification of the textual units into which the input text should be broken and which are later used as the constituent parts of the final summary, the textual features which are associated with the important concepts described in the input text, and the appropriate algorithm for selecting the textual units to be included into the summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9879207611083984}]}, {"text": "We focus on the latter two of those steps and explore interdependencies between the choice of features (step 2) and selection algorithm (step 3).", "labels": [], "entities": []}, {"text": "We experimentally test our hypothesis that event-based features are helpful for summarization by comparing the performance of three sentence selection algorithms when we use such features versus the case where we use another, widely used set of textual features: the words in the input texts, weighted by their tf*idf scores.", "labels": [], "entities": [{"text": "summarization", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9877243041992188}]}, {"text": "The results establish that for the majority of document sets in our test collection, events outperform tf*idf for all algorithms considered.", "labels": [], "entities": []}, {"text": "Furthermore, we show that this benefit is more pronounced when the selection algorithm includes steps to address potential repetition of information in the output summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "We chose as our input data the document sets used in the evaluation of multidocument summarization during the first Document Understanding Conference (DUC), organized by NIST).", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6299428343772888}, {"text": "Document Understanding Conference (DUC)", "start_pos": 116, "end_pos": 155, "type": "TASK", "confidence": 0.7398426532745361}]}, {"text": "This collection contains 30 test document sets, each with approximately 10 news stories on different events; document sets vary significantly in their internal coherence.", "labels": [], "entities": []}, {"text": "For each document set three human-constructed summaries are provided for each of the target lengths of 50, 100, 200, and 400 words.", "labels": [], "entities": []}, {"text": "We selected DUC 2001 because ideal summaries are available for multiple lengths.", "labels": [], "entities": [{"text": "DUC 2001", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.9420005083084106}]}, {"text": "Concepts and Textual Units Our textual units are sentences, while the features representing concepts are either atomic events, as described in Section 4, or a fairly basic and widely used set of lexical features, namely the list of words present in each input text.", "labels": [], "entities": []}, {"text": "The algorithm for extracting event triplets assigns a weight to each such triplet, while for words we used as weights their tf*idf values, taking idf values from http://elib.cs. berkeley.edu/docfreq/.", "labels": [], "entities": []}, {"text": "Evaluation Metric Given the difficulties in coming up with a universally accepted evaluation measure for summarization, and the fact that obtaining judgments by humans is time-consuming and labor-intensive, we adopted an automated process for comparing system-produced summaries to \"ideal\" summaries written by humans.", "labels": [], "entities": [{"text": "Evaluation Metric", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6595579236745834}, {"text": "summarization", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9917802214622498}]}, {"text": "The method, ROUGE (, is based on n-gram overlap between the system-produced and ideal summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9937396049499512}]}, {"text": "As such, it is a recall-based measure, and it requires that the length of the summaries be controlled to allow meaningful comparisons.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.9985989928245544}]}, {"text": "ROUGE can be readily applied to compare the performance of different systems on the same set of documents, assuming that ideal summaries are available for those documents.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9426978230476379}]}, {"text": "At the same time, ROUGE evaluation has not yet been tested extensively, and ROUGE scores are difficult to interpret as they are not absolute and not comparable across source document sets.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8965932726860046}]}, {"text": "In our comparison, we used as reference summaries those created by NIST assessors for the DUC task of generic summarization.", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.897344559431076}, {"text": "DUC task", "start_pos": 90, "end_pos": 98, "type": "TASK", "confidence": 0.7670726180076599}, {"text": "generic summarization", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.8252900540828705}]}, {"text": "The human annotators may not have created the same models if asked for summaries describing the major events in the input texts instead of generic summaries.", "labels": [], "entities": []}, {"text": "Summary Length For a given set of features and selection algorithm we get a sorted list of sentences extracted according to that particular algorithm.", "labels": [], "entities": [{"text": "Summary Length", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8439851105213165}]}, {"text": "Then, for each DUC document set we create four summaries of length 50, 100, 200, and 400.", "labels": [], "entities": [{"text": "DUC document set", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.9423218766848246}]}, {"text": "In all the suggested methods a whole sentence is added at every step.", "labels": [], "entities": []}, {"text": "We extracted exactly 50, 100, 200, and 400 words out of the top sentences (truncating the last sentence if necessary).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Static greedy algorithm, events versus  tf*idf", "labels": [], "entities": []}, {"text": " Table 3: Adaptive greedy algorithm, events versus  tf*idf", "labels": [], "entities": []}, {"text": " Table 4: Adaptive greedy algorithm versus static  greedy algorithm, using events as features", "labels": [], "entities": [{"text": "Adaptive greedy", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7975749373435974}]}, {"text": " Table 5: Adaptive greedy algorithm versus static  greedy algorithm, using tf*idf as features", "labels": [], "entities": [{"text": "Adaptive greedy", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7650845050811768}]}, {"text": " Table 6: Modified adaptive greedy algorithm versus  static greedy algorithm, using events as features", "labels": [], "entities": [{"text": "Modified adaptive greedy", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6470129688580831}]}, {"text": " Table 7: Modified adaptive greedy algorithm versus  static greedy algorithm, using tf*idf as features", "labels": [], "entities": []}, {"text": " Table 8: Modified adaptive greedy algorithm, events  versus tf*idf", "labels": [], "entities": []}]}