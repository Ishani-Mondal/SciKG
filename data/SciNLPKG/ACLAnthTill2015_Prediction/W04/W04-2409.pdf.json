{"title": [{"text": "A Comparison of Manual and Automatic Constructions of Category Hierarchy for Classifying Large Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "We address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classification of large corpora.", "labels": [], "entities": []}, {"text": "We use two well-known techniques, partitioning clustering, \ud97b\udf59-means and a \u00d0\u00d3\u00d7\u00d7 \u00d9\u00d2\u00d2\u00d8\u00d8\u00d3\u00d2 to create category hierarchy.", "labels": [], "entities": []}, {"text": "\ud97b\udf59-means is to cluster the given categories in a hierarchy.", "labels": [], "entities": []}, {"text": "To select the proper number of \ud97b\udf59, we use a \u00d0\u00d3\u00d7\u00d7 \u00d9\u00d2\u00d2\u00d8\u00d8\u00d3\u00d2 which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction.", "labels": [], "entities": []}, {"text": "Once the optimal number of \ud97b\udf59 is selected, for each cluster , the procedure is repeated.", "labels": [], "entities": []}, {"text": "Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy.", "labels": [], "entities": [{"text": "1996 Reuters corpus which consists of 806,791 documents", "start_pos": 25, "end_pos": 80, "type": "DATASET", "confidence": 0.8605818003416061}, {"text": "classification", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.9519749283790588}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.8529417514801025}]}], "introductionContent": [{"text": "Text classification has an important role to play, especially with the recent explosion of readily available online documents.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8234262466430664}]}, {"text": "Much of the previous work on text classification use statistical and machine learning techniques.", "labels": [], "entities": [{"text": "text classification", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8428554832935333}]}, {"text": "However, the increasing number of documents and categories often hamper the development of practical classification systems, mainly by statistical, computational, and representational problems).", "labels": [], "entities": []}, {"text": "One strategy for solving these problems is to use category hierarchies.", "labels": [], "entities": []}, {"text": "The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are often employed to make the large collection of categories more manageable.", "labels": [], "entities": []}, {"text": "McCallum et. al. presented a method called 'shrinkage' to improve parameter estimates by taking advantage of the hierarchy).", "labels": [], "entities": []}, {"text": "They tested their method using three different real-world datasets: 20,000 articles from the UseNet, 6,440 web pages from the Industry Sector, and 14,831 pages from the Yahoo, and showed improved performance.", "labels": [], "entities": []}, {"text": "Dumais et. al. also described a method for hierarchical classification of Web content consisting of 50,078 Web pages for training, and 10,024 for testing, with promising results).", "labels": [], "entities": [{"text": "hierarchical classification of Web content", "start_pos": 43, "end_pos": 85, "type": "TASK", "confidence": 0.7445391297340394}]}, {"text": "Both of them use hierarchies which are manually constructed.", "labels": [], "entities": []}, {"text": "Such hierarchies are costly human intervention, since the number of categories and the size of the target corpora are usually very large.", "labels": [], "entities": []}, {"text": "Further, manually constructed hierarchies are very general in order to meet the needs of a large number of forthcoming accessible source of text data, and sometimes constructed by relying on human intuition.", "labels": [], "entities": []}, {"text": "Therefore, it is difficult to keep consistency, and thus, problematic for classifying text automatically.", "labels": [], "entities": [{"text": "consistency", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9924324750900269}]}, {"text": "In this paper, we address the problem dealing with a large collection of data, and propose a method to generate category hierarchy for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.8011063635349274}]}, {"text": "Our method uses two well-known techniques, partitioning clustering method called \ud97b\udf59-means and a \u00d0\u00d3\u00d7\u00d7 \u00d9\u00d2\u00d2\u00d8\u00d8\u00d3\u00d2 to create hierarchical structure.", "labels": [], "entities": []}, {"text": "\ud97b\udf59-means partitions a set of given categories into \ud97b\udf59 clusters, locally minimizing the average squared distance between the data points and the cluster centers.", "labels": [], "entities": []}, {"text": "The algorithm involves iterating through the data that the system is permitted to classify during each iteration and constructs category hierarchy.", "labels": [], "entities": []}, {"text": "To select the proper number of \ud97b\udf59 during each iteration, we use a \u00d0\u00d3\u00d7\u00d7 \u00d9\u00d2\u00d2\u00d8\u00d8\u00d3\u00d2 which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction.", "labels": [], "entities": []}, {"text": "Another focus of this paper is whether or not a large collection of data, the 1996 Reuters corpus helps to generate a category hierarchy which is used to classify documents.", "labels": [], "entities": [{"text": "1996 Reuters corpus", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.691162645816803}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section presents a brief review the earlier work.", "labels": [], "entities": []}, {"text": "We then explain the basic framework for constructing category hierarchy, and describe hierarchical classification.", "labels": [], "entities": [{"text": "hierarchical classification", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.737573117017746}]}, {"text": "Finally, we report some experiments using the 1996 Reuters corpus with a discussion of evaluation.", "labels": [], "entities": [{"text": "1996 Reuters corpus", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.8760881225268046}]}], "datasetContent": [{"text": "We compare automatically created hierarchy with flat and manually constructed hierarchy with respect to classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9250988960266113}]}, {"text": "We further evaluate the generated category hierarchy from two perspectives: we examine (i) whether or not the number of categories effects to construct a category hierarchy, and (ii) whether or not a large collection of data helps to generate a category hierarchy.", "labels": [], "entities": []}, {"text": "The data we used is the 1996 Reuters corpus which is available lately).", "labels": [], "entities": [{"text": "1996 Reuters corpus", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.8877135515213013}]}, {"text": "The corpus from 20th Aug., 1996 to 19th Aug., 1997 consists of 806,791 documents.", "labels": [], "entities": []}, {"text": "These documents are organized into 126 topical categories with a fifth level hierarchy.", "labels": [], "entities": []}, {"text": "After eliminating unlabeled documents, we divide these documents into four sets.", "labels": [], "entities": []}, {"text": "illustrates each data which we used in each model, i.e. a flat non-hierarchical model, manually constructed hierarchy, and automatically created hierarchy.", "labels": [], "entities": []}, {"text": "The same notation of (X) in denotes a pair of training and test data.", "labels": [], "entities": []}, {"text": "For example, '(F1) Training data' shows that 145,919 samples are used for training NB classifiers, and '(F1) Test data' illustrates that 290,665 samples are used for classification.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9931889772415161}, {"text": "F1) Test data", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.8495222628116608}]}, {"text": "We selected 102 categories which have at least one document in each data.", "labels": [], "entities": []}, {"text": "We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-of-speech tagger, and stop word removal.", "labels": [], "entities": [{"text": "stop word removal", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.6620272596677145}]}, {"text": "our original development training set, 300,000 documents, a different training set which consists of 200,000 documents is created by random sampling.", "labels": [], "entities": []}, {"text": "The learner then creates anew NB classifier from this sample.", "labels": [], "entities": []}, {"text": "This procedure is repeated 10 times, and the final class posterior for an instance is taken to be the average of the class posteriors for each of the classifiers.", "labels": [], "entities": []}, {"text": "For evaluating the effectiveness of category assignments, we use the standard recall, precision, and F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9993277788162231}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9992528557777405}, {"text": "F-score", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9983615279197693}]}, {"text": "Recall is defined to be the ratio of correct assignments by the system divided by the total number of correct assignments.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9722407460212708}]}, {"text": "Precision is the ratio of correct assignments by the system divided by the total number of the system's assignments.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9926647543907166}]}, {"text": "The F-score which combines recall (\u00d6) and precision (\u00d4) with an equal weight is F\u00b4\u00d6\ud97b\udf59 \u00d4 \u00b5 \u00b5 \u00be\u00d6\u00d4 \u00d6\u00b7\u00d4 . We use micro-averaging F score where it computes globally over \u00d2(all the number of categories) \u00a2 \u00d1 (the number of total test documents) binary decisions.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9872451424598694}, {"text": "recall (\u00d6)", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9323271960020065}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.989459753036499}, {"text": "F\u00b4\u00d6", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9973024129867554}, {"text": "F score", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9455858170986176}]}, {"text": "shows atop level of the hierarchy which is manually constructed.", "labels": [], "entities": []}, {"text": "shows a portion of the automatically generating hierarchy which is associated with the categories shown in.", "labels": [], "entities": []}, {"text": "'\u00dc-\u00dd-\u00a1 \u00a1 \u00a1 ' shows the ID number which is assigned to each node of the tree.", "labels": [], "entities": []}, {"text": "For example, 3-5-1 shows that the ID number of the top, second, and third level is 3, 5, and 1, respectively.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 shows a threshold value obtained by the training samples and development test samples.", "labels": [], "entities": []}, {"text": "indicate that the automatically constructed hierarchy has different properties from manually created hierarchies.", "labels": [], "entities": []}, {"text": "When the top level categories of hierarchical structure are equally \u00d7\u00d7\u00d6\u00d1\u00d1\u00d2\u00d2\u00d8\u00d8\u00d2\u00d2 properties, they are useful for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8847669363021851}]}, {"text": "In the manual construction of hierarchy), there are 25 categories in the top level, while the result of our method based on corpus statistics shows that the top 4 frequent categories are selected as a discriminative properties, and other categories are sub-categorised into 'Government/social' except for 'Labour issues' and 'Weather'.", "labels": [], "entities": []}, {"text": "In the automatically generating hierarchy, 'Economics' \ud97b\udf59 'Expenditure' \ud97b\udf59 'Welfare', and 'Economics' \ud97b\udf59 'Labour issues' are created, while 'Welfare' and 'Labour issues' belong to the top level in the manually constructed hierarchy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Manually constructed hierarchies  Level  \ud97b\udf59  Category node  Top  0.400 1 Corporate/industrial 2 Economics  3 Government/social  4 Markets  5 Crime  6 Defence  7 International relations 8 Disasters  9 Entertainment  10 Environment 11 Fashion  12 Health  13 Labour issues  14 Obituaries  15 Human interest  16 Domestic politics  17 Biographies/people 18 Religion  19 Science  20 Sports  21 Tourism  22 War  23 Elections  24 Weather  25 Welfare", "labels": [], "entities": []}, {"text": " Table 5: Classification accuracy  Method  miR  miP  miF  Flat  0.753 0.647 0.695  Manual  0.685 0.708 0.701  Automatic 0.807 0.675 0.734", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9687260985374451}, {"text": "miR  miP  miF  Flat  0.753 0.647 0.695  Manual  0.685 0.708 0.701  Automatic 0.807 0.675 0.734", "start_pos": 43, "end_pos": 137, "type": "METRIC", "confidence": 0.8584115068117778}]}, {"text": " Table 6: Accuracy by hierarchical level(Manual)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981458187103271}]}, {"text": " Table 7: Accuracy by hierarchical level(Automatic)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985554814338684}]}]}