{"title": [{"text": "The information-processing difficulty of incremental parsing", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.5682511925697327}]}], "abstractContent": [{"text": "When an incremental parser gets the next word, its expectations about upcoming grammatical structures can change.", "labels": [], "entities": []}, {"text": "When a word greatly constrains these grammatical expectations, uncertainty is reduced.", "labels": [], "entities": [{"text": "uncertainty", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9688848853111267}]}, {"text": "This elimination of possibilities constitutes information processing work.", "labels": [], "entities": [{"text": "information processing", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8981985151767731}]}, {"text": "Formalizing this notion of information processing work yields a complexity metric that predicts human repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of rela-tivizable grammatical relations.", "labels": [], "entities": [{"text": "repetition accuracy scores", "start_pos": 102, "end_pos": 128, "type": "METRIC", "confidence": 0.791003962357839}]}], "introductionContent": [{"text": "An attractive hypothesis in psycholinguistics, dating back at least to the 1950s, has been that the degree of predictability of words in sentences is somehow related to understandability, production difficulty or, more recently, eye-movements.", "labels": [], "entities": []}, {"text": "However, since the 1950s, integrating this hypothesis with realistic models of linguistic structure has remained a challenge.", "labels": [], "entities": []}, {"text": "appreciated the formal character of the problem.", "labels": [], "entities": []}, {"text": "He defined a finite, artificial language, endowed with a rudimentary phonology, morphology and syntax, and showed that a word's informational contribution could be formally defined as the entropy reduction brought about by its addition to the end of a sentence fragment.", "labels": [], "entities": []}, {"text": "He qualified the significance of his achievement, saying An entropy reduction analysis presupposes that the number of possible messages is finite, and that the probabilities of each of the messages is known....Thus it appears that the entropy reduction analysis could be applied only to limited classes of natural language messages since the number of messages in nearly all languages is indefinitely large A fuller presentation of this work can be found in Hale (forthcoming).", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7890550196170807}, {"text": "entropy reduction", "start_pos": 235, "end_pos": 252, "type": "TASK", "confidence": 0.7871496081352234}]}, {"text": "The present paper extends Lounsbury's original idea to infinite languages, by applying two classical ideas in (probabilistic) formal language theory: closed-form solution for the entropy of a nonterminal in a probabilistic context-free phrase structure grammar, and insight that an intermediate parser state is itself a specification of a grammar.", "labels": [], "entities": []}, {"text": "This extension permits the psycholinguistic hypothesis ERH to be examined.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}