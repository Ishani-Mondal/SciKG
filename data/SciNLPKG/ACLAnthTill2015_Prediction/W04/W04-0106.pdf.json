{"title": [{"text": "Induction of a Simple Morphology for Highly-Inflecting Languages", "labels": [], "entities": [{"text": "Induction", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9816092848777771}]}], "abstractContent": [{"text": "This paper presents an algorithm for the unsuper-vised learning of a simple morphology of a natural language from raw text.", "labels": [], "entities": [{"text": "learning of a simple morphology of a natural language from raw text", "start_pos": 55, "end_pos": 122, "type": "TASK", "confidence": 0.6866436650355657}]}, {"text": "A generative prob-abilistic model is applied to segment word forms into morphs.", "labels": [], "entities": []}, {"text": "The morphs are assumed to be generated by one of three categories, namely prefix, suffix , or stem, and we make use of some observed asymmetries between these categories.", "labels": [], "entities": []}, {"text": "The model learns a word structure, where words are allowed to consist of lengthy sequences of alternating stems and affixes, which makes the model suitable for highly-inflecting languages.", "labels": [], "entities": []}, {"text": "The ability of the algorithm to find real morpheme boundaries is evaluated against a gold standard for both Finnish and English.", "labels": [], "entities": []}, {"text": "In comparison with a state-of-the-art algorithm the new algorithm performs best on the Finnish data, and on roughly equal level on the En-glish data.", "labels": [], "entities": [{"text": "Finnish data", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.8930331170558929}, {"text": "En-glish data", "start_pos": 135, "end_pos": 148, "type": "DATASET", "confidence": 0.8338514268398285}]}], "introductionContent": [{"text": "We are intrigued by the endeavor of devising artificial systems that are capable of learning natural language in an unsupervised manner.", "labels": [], "entities": []}, {"text": "As untagged text data is available in large quantities fora large number of languages, unsupervised methods maybe applied much more widely, or with much lower cost, than supervised ones.", "labels": [], "entities": []}, {"text": "Some languages, such as Finnish, Turkish, and Swahili, are highly-inflecting.", "labels": [], "entities": []}, {"text": "We wish to use this term in a wide sense including many kinds of processes for word forming, e.g., compounding and derivation.", "labels": [], "entities": [{"text": "word forming", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.7836718559265137}]}, {"text": "Their essential challenge for natural language applications arises from the very large number of possible word forms, which causes problems of data sparsity.", "labels": [], "entities": []}, {"text": "For instance, creating extensive word lists is not a feasible strategy for obtaining good coverage on the vocabulary necessary fora general dictation task in automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.6016552646954855}]}, {"text": "Instead, a model of the language should incorporate regularities of how words are formed; cf. e.g.,).", "labels": [], "entities": []}, {"text": "We will now focus on methods that try to induce the morphology of a natural language from raw text, that is, on algorithms that learn in an unsupervised manner how words are formed.", "labels": [], "entities": []}, {"text": "If a human were to learn a language in an analogous way, this would correspond to being exposed to a stream of large amounts of language without observing or interacting with the world where this language is produced.", "labels": [], "entities": []}, {"text": "This is clearly not a realistic assumption about language learning in humans.", "labels": [], "entities": []}, {"text": "However, show that adults are capable of discovering word units rapidly in a stream of a nonsense language, where there is no connection to a meaning of the discovered word-like units.", "labels": [], "entities": []}, {"text": "This suggests that humans douse distributional cues, such as transition probabilities between sounds, in language learning.", "labels": [], "entities": []}, {"text": "And these kinds of statistical patterns in language data can be successfully exploited by appropriately designed algorithms.", "labels": [], "entities": []}, {"text": "Existing morphology learning algorithms are commonly based on the Item and Arrangement model, i.e., words are formed by a concatenation of morphemes, which are the smallest meaningbearing units in language.", "labels": [], "entities": [{"text": "Arrangement", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.7566221952438354}]}, {"text": "The methods segment words, and the resulting segments are supposed to be close to linguistic morphemes.", "labels": [], "entities": []}, {"text": "In addition to producing a segmentation of words the aim is often to discover structure, such as knowledge of which word forms belong to the same inflectional paradigm.", "labels": [], "entities": []}, {"text": "Typically, generative models are used, either formulated in a Bayesian framework, e.g.,; or applying the Minimum Description Length (MDL) principle, e.g., ().", "labels": [], "entities": [{"text": "Minimum Description Length (MDL)", "start_pos": 105, "end_pos": 137, "type": "METRIC", "confidence": 0.6677564134200414}]}, {"text": "There is another approach, inspired by the works of Zellig Harris, where a morpheme boundary is suggested at locations where the predictability of the next letter in a letter sequence is low, cf. e.g.,.", "labels": [], "entities": []}, {"text": "As it is necessary to learn both which segments are plausible morphemes and what sequences of morphemes are possible, the learning task is al- leviated by making simplifying assumptions about word structure.", "labels": [], "entities": []}, {"text": "Often words are assumed to consist of one stem followed by one, possibly empty, suffix as in, e.g.,).", "labels": [], "entities": []}, {"text": "In) a recursive structure is proposed, such that stems can consist of a sub-stem and a suffix.", "labels": [], "entities": []}, {"text": "Other algorithms ( have been developed for highly-inflecting languages, such as Finnish, where words can consist of lengthy sequences of alternating stems and affixes (see for an example).", "labels": [], "entities": []}, {"text": "These resemble algorithms that segment text without blanks (or transcribed speech) into words, e.g., (, in that they do not distinguish between stems and affixes, but split words into so called morphs, which carry no explicit category information.", "labels": [], "entities": []}, {"text": "Some algorithms do not rely on the Item and Arrangement (IA) model, but learn relationships between words by comparing the orthographic similarity of pairs of words.", "labels": [], "entities": []}, {"text": "In (), a morphological learner based on the theory of Whole Word Morphology is outlined.", "labels": [], "entities": [{"text": "Whole Word Morphology", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6615391373634338}]}, {"text": "Full words are related to other full words, and complex word forms are analyzed into a variable and non-variable component.", "labels": [], "entities": []}, {"text": "Conceivably, in this framework nonconcatenative morphological processes, such as umlaut in German, should not be as problematic as in the IA model.", "labels": [], "entities": []}, {"text": "Other algorithms combine information of both orthographic and semantic similarity of words).", "labels": [], "entities": []}, {"text": "Semantic similarity is measured in terms of similar word contexts.", "labels": [], "entities": []}, {"text": "If two orthographically similar words occur in the context of roughly the same set of other words they probably share the same base form, e.g. German 'Vertrag' vs..", "labels": [], "entities": []}, {"text": "Further cues for morphological learning are presented in and).", "labels": [], "entities": []}, {"text": "The latter utilizes frequency distributions over different inflectional forms (e.g., how often an English verb occurs in its past tense form in comparison to its base form).", "labels": [], "entities": []}, {"text": "The algorithm is not entirely unsupervised.", "labels": [], "entities": []}, {"text": "However, none of these non-IA models suits highly-inflecting languages as they assume only two or three constituents per word, analogous to stem and suffix.", "labels": [], "entities": []}, {"text": "In order to cope with a broader range of languages we would need the following: On the one hand, words should be allowed to consist of any number of alternating stems and affixes, making the model more flexible than, e.g., the model in).", "labels": [], "entities": []}, {"text": "On the other hand, in contrast with (, sequential dependencies between morphs, i.e., morphotactics, should betaken into account in order to reduce the error rate.", "labels": [], "entities": []}, {"text": "We present a model that incorporates both of these aspects.", "labels": [], "entities": []}, {"text": "Experiments show that the new algorithm is able to obtain considerable improvements over the segmentation produced by the algorithm described in.", "labels": [], "entities": []}, {"text": "Moreover, it performs better than a state-of-the-art morphologylearning algorithm, Linguistica, when evaluated on Finnish data.", "labels": [], "entities": []}, {"text": "In the evaluation, the ability of the algorithms to detect morpheme boundaries are measured against a gold standard for both Finnish and English, languages with rather different types of word structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have produced gold standard segmentations with marked morpheme boundaries for 1.4 million Finnish and 36 000 English word forms.", "labels": [], "entities": []}, {"text": "We evaluate the segmentations produced by our splitting algorithm against the gold standard, and compute precision and recall on discovered morpheme boundaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9994122982025146}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.999235987663269}]}, {"text": "Precision is the proportion of correct boundaries among all morph boundaries suggested by the algorithm.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9925922155380249}]}, {"text": "Recall is the proportion of correct boundaries discovered by the algorithm in relation to all morpheme boundaries in the gold standard.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9899197816848755}]}, {"text": "The gold standard was created semiautomatically, by first running all words through a morphological analyzer based on the two-level morphology of.", "labels": [], "entities": []}, {"text": "For each word form, the analyzer outputs the base form of the word together with grammatical tags indicating, e.g., the part-of-speech, case, or derivational type of the word form.", "labels": [], "entities": []}, {"text": "In addition, the boundaries between the constituents of compound words are often marked.", "labels": [], "entities": []}, {"text": "We thoroughly investigated the correspondence between the grammatical tags and the corresponding morphemes and created a rule-set for segmenting the original word forms with the help of the output of the analyzer.", "labels": [], "entities": []}, {"text": "As there can sometimes be many plausibly correct segmentation of a word we supplied several alternatives when needed, e.g., English 'evening' (time of day) vs. 'even+ing' (verb).", "labels": [], "entities": []}, {"text": "We also introduced so called \"fuzzy\" boundaries between stems and endings, allowing some letter to belong to either the stem or ending, when both alternatives are reasonable, e.g., English 'invite+s' vs. 'invit+es' (cf. 'invit+ing'), or Finnish 't\u00e4hde+n' vs. 't\u00e4hd+en' (\"of the star\"; the base form is 't\u00e4hti').", "labels": [], "entities": []}, {"text": "We report experiments on Finnish and English corpora.", "labels": [], "entities": []}, {"text": "The new category-learning algorithm is compared to two other algorithms, namely the baseline segmentation algorithm presented in, which was also utilized for initializing the segmentation in the category-learning algorithm, and the Linguistica algorithm).", "labels": [], "entities": []}], "tableCaptions": []}