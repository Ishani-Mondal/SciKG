{"title": [{"text": "Regularized Least-Squares Classification for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6934501131375631}]}], "abstractContent": [{"text": "The paper describes RLSC-LIN and RLSC-COMB systems which participated in the Senseval-3 English lexical sample task.", "labels": [], "entities": [{"text": "Senseval-3 English lexical sample task", "start_pos": 77, "end_pos": 115, "type": "TASK", "confidence": 0.6419803619384765}]}, {"text": "These systems are based on Regularized Least-Squares Classification (RLSC) learning method.", "labels": [], "entities": []}, {"text": "We describe the reasons of choosing this method, how we applied it to word sense disambigua-tion, what results we obtained on Senseval-1, Senseval-2 and Senseval-3 data and discuss some possible improvements.", "labels": [], "entities": [{"text": "word sense disambigua-tion", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.6699869434038798}]}], "introductionContent": [{"text": "Word sense disambiguation can be viewed as a classification problem and one way to obtain a classifier is by machine learning methods.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6616220275561014}]}, {"text": "Unfortunately, there is no single one universal good learning procedure.", "labels": [], "entities": []}, {"text": "The No Free Lunch Theorem assures us that we cannot design a good learning algorithm without any assumptions about the structure of the problem.", "labels": [], "entities": []}, {"text": "So, we start by trying to find out what are the particular characteristics of the learning problem posed by the word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.6475506822268168}]}, {"text": "In our opinion, one of the most important particularities of the word sense disambiguation learning problem, seems to be the dimensionality problem, more specifically the fact that the number of features is much greater than the number of training examples.", "labels": [], "entities": [{"text": "word sense disambiguation learning", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.7764156460762024}]}, {"text": "This is clearly true about data in Senseval-1, Senseval-2 and Senseval-3.", "labels": [], "entities": [{"text": "Senseval-3", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.8910884857177734}]}, {"text": "One can argue that this happens because of the small number of training examples in these data sets, but we think that this is an intrinsic propriety of learning task in the case of word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 182, "end_pos": 207, "type": "TASK", "confidence": 0.6793793241182963}]}, {"text": "In word sense disambiguation one important knowledge source is the words that co-occur (in local or broad context) with the word that had to be disambiguated, and every different word that appears in the training examples will become a feature.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.7208952605724335}]}, {"text": "Increasing the number of training examples will increase also the number of different words that appear in the training examples, and so will increase the number of features.", "labels": [], "entities": []}, {"text": "Obviously, the rate of growth will not be the same, but we consider that for any reasonable number of training examples (reasonable as the possibility of obtaining these training examples and as the capacity of processing, learning from these examples) the dimension of the feature space will be greater.", "labels": [], "entities": []}, {"text": "Actually, the high dimensionality of the feature space with respect to the number of examples is a general scenario of learning in the case of Natural Language Processing tasks and word sense disambiguation is one of these examples.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 181, "end_pos": 206, "type": "TASK", "confidence": 0.6965457598368326}]}, {"text": "In such situations, when the dimension of the feature space is greater than the number of training examples, the potential for overfitting is huge and some form of regularization is needed.", "labels": [], "entities": []}, {"text": "This is the reason why we chose to use Regularized Least-Squares Classification (RLSC), a method of learning based on kernels and Tikhonov regularization.", "labels": [], "entities": [{"text": "Regularized Least-Squares Classification (RLSC)", "start_pos": 39, "end_pos": 86, "type": "TASK", "confidence": 0.6776631772518158}]}, {"text": "In the next section we explain what source of information we used and how this information is transformed into features.", "labels": [], "entities": []}, {"text": "In section 3 we briefly describe the RLSC learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what results we have obtained.", "labels": [], "entities": [{"text": "RLSC learning", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.8635780215263367}, {"text": "word sense disambiguation", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.7065955003102621}]}, {"text": "Finally, in section 5, we discuss some possible improvements.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Fine-grained score for SVM-LIN and  SVM-COMB on Senseval data sets", "labels": [], "entities": [{"text": "Senseval data sets", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.7427665690581003}]}]}