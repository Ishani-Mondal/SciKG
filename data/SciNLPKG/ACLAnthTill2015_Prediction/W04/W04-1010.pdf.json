{"title": [], "abstractContent": [{"text": "Headline summarization is a difficult task because it requires maximizing text content in short summary length while maintaining gram-maticality.", "labels": [], "entities": [{"text": "Headline summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8708509802818298}]}, {"text": "This paper describes our first attempt toward solving this problem with a system that generates key headline clusters and fine-tunes them using templates.", "labels": [], "entities": []}], "introductionContent": [{"text": "Producing headline-length summaries is a challenging summarization problem.", "labels": [], "entities": [{"text": "headline-length summaries", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.4917760342359543}, {"text": "summarization", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9890338182449341}]}, {"text": "But the need for grammaticality-or at least intelligibility-sometimes requires the inclusion of non-content words.", "labels": [], "entities": []}, {"text": "Forgoing grammaticality, one might compose a \"headline\" summary by simply listing the most important noun phrases one after another.", "labels": [], "entities": []}, {"text": "At the other extreme, one might pick just one fairly indicative sentence of appropriate length, ignoring all other material.", "labels": [], "entities": []}, {"text": "Ideally, we want to find a balance between including raw information and supporting intelligibility.", "labels": [], "entities": []}, {"text": "We experimented with methods that integrate content-based and form-based criteria.", "labels": [], "entities": []}, {"text": "The process consists two phases.", "labels": [], "entities": []}, {"text": "The keyword-clustering component finds headline phrases in the beginning of the text using a list of globally selected keywords.", "labels": [], "entities": []}, {"text": "The template filter then uses a collection of pre-specified headline templates and subsequently populates them with headline phrases to produce the resulting headline.", "labels": [], "entities": []}, {"text": "In this paper, we describe in Section 2 previous work.", "labels": [], "entities": []}, {"text": "Section 3 describes a study on the use of headline templates.", "labels": [], "entities": [{"text": "headline templates", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6951084733009338}]}, {"text": "A discussion on the process of selecting and expanding key headline phrases is in Section 4.", "labels": [], "entities": []}, {"text": "And Section 5 goes back to the idea of templates but with the help of headline phrases.", "labels": [], "entities": []}, {"text": "Future work is discussed in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Ideally, the evaluation should show the system's performance on both content selection and grammaticality.", "labels": [], "entities": [{"text": "content selection", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.6993558257818222}]}, {"text": "However, it is hard to measure the level of grammaticality achieved by a system computationally.", "labels": [], "entities": []}, {"text": "Similar to), we restricted the evaluation to a quantitative analysis on content only.", "labels": [], "entities": []}, {"text": "Our system was evaluated on previously unseen DUC2003 test data of 615 files.", "labels": [], "entities": [{"text": "DUC2003 test data of 615 files", "start_pos": 46, "end_pos": 76, "type": "DATASET", "confidence": 0.9399645328521729}]}, {"text": "For each file, headlines generated at various lengths were compared against i) the original headline, and ii) headlines written by four DUC2003 human assessors.", "labels": [], "entities": [{"text": "DUC2003 human assessors", "start_pos": 136, "end_pos": 159, "type": "DATASET", "confidence": 0.8644091486930847}]}, {"text": "The performance metric was to count term overlaps between the generated headlines and the test standards.", "labels": [], "entities": []}, {"text": "shows the human agreement and the performance of the system comparing with the two test standards.", "labels": [], "entities": []}, {"text": "P and R are the precision and recall scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9997616410255432}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9974798560142517}]}, {"text": "The system-generated headlines were also evaluated using the automatic summarization evaluation tool ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin and Hovy,: Results evaluated using unigram overlap 2003).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9593034386634827}]}, {"text": "The ROUGE score is a measure of n-gram recall between candidate headlines and a set of reference headlines.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9846909046173096}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9854457378387451}]}, {"text": "Its simplicity and reliability are gaining audience and becoming a standard for performing automatic comparative summarization evaluation.", "labels": [], "entities": [{"text": "comparative summarization evaluation", "start_pos": 101, "end_pos": 137, "type": "TASK", "confidence": 0.727253258228302}]}, {"text": "shows the ROUGE performance results for generate d headlines with length 12 against headlines written by human assessors.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9401325583457947}, {"text": "length", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9593421816825867}]}], "tableCaptions": [{"text": " Table 1: Study on sequential template matching  of a headline against its text, on training data", "labels": [], "entities": [{"text": "sequential template matching  of a headline", "start_pos": 19, "end_pos": 62, "type": "TASK", "confidence": 0.7682166397571564}]}, {"text": " Table 2: Results on model combinations", "labels": [], "entities": []}, {"text": " Table 4: Results evaluated using unigram over- lap", "labels": [], "entities": []}]}