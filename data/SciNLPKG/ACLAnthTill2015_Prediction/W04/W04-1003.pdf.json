{"title": [{"text": "The Effects of Human Variation in DUC Summarization Evaluation", "labels": [], "entities": [{"text": "DUC Summarization", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.76835897564888}]}], "abstractContent": [{"text": "There is along history of research in automatic text summarization systems by both the text retrieval and the natural language processing communities, but evaluation of such systems' output has always presented problems.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.5886150995890299}]}, {"text": "One critical problem remains how to handle the unavoidable variability inhuman judgments at the core of all the evaluations.", "labels": [], "entities": []}, {"text": "Sponsored by the DARPA TIDES project, NIST launched anew text summarization evaluation effort , called DUC, in 2001 with follow-on workshops in 2002 and 2003.", "labels": [], "entities": [{"text": "NIST", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8505535125732422}, {"text": "text summarization evaluation", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.7237965067227682}]}, {"text": "Human judgments provided the foundation for all three evaluations and this paper examines how the variation in those judgments does and does not affect the results and their interpretation .", "labels": [], "entities": []}], "introductionContent": [{"text": "Research in summarization was one of the first efforts to use computers to \"understand\" language.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9932637214660645}]}, {"text": "Work was done back in the 1950s by many groups, including commercial services, to automatically produce abstracts or lists of pertinent keywords for documents.", "labels": [], "entities": []}, {"text": "The interest in automatic summarization of text has continued, and currently is enjoying increased emphasis as demonstrated by the numerous summarization workshops held during the last five years.", "labels": [], "entities": [{"text": "summarization of text", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8771775960922241}, {"text": "summarization", "start_pos": 140, "end_pos": 153, "type": "TASK", "confidence": 0.9689916372299194}]}, {"text": "The DUC summarization evaluations)(http://duc.nist.gov) sponsored by the DARPA TIDES project (Translingual Information Detection, Extraction, and Summarization) are prominent examples.", "labels": [], "entities": [{"text": "Translingual Information Detection, Extraction, and Summarization)", "start_pos": 94, "end_pos": 160, "type": "TASK", "confidence": 0.7469247480233511}]}, {"text": "DUC has been guided by a roadmap developed by members of the summarization research community.", "labels": [], "entities": [{"text": "DUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9083305597305298}, {"text": "summarization research", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.9030839502811432}]}, {"text": "Along with the research has come efforts to evaluate automatic summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.8438194990158081}]}, {"text": "Two major types of evaluation have been used: extrinsic evaluation, where one measures indirectly how well the summary performs by measuring performance in a task putatively dependent on the quality of the summary, and intrinsic evaluation, where one measures the quality of the created summary directly.", "labels": [], "entities": []}, {"text": "Extrinsic evaluation requires the selection of a task that could use summarization and measurement of the effect of using automatic summaries instead of the original text.", "labels": [], "entities": [{"text": "summarization", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.9774252772331238}]}, {"text": "Critical issues here are the selection of areal task and the metrics that will be sensitive to differences in the quality of the summaries.", "labels": [], "entities": []}, {"text": "This paper concerns itself with intrinsic evaluations.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation requires some standard or model against which to judge summarization quality and usually this standard is operationalized by finding an existing abstract/text data set or by having humans create model summaries (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9386888742446899}]}, {"text": "Intrinsic evaluations have taken two main forms: manual, in which one or more people evaluate the system-produced summary and automatic, in which the summary is evaluated without the human in the loop.", "labels": [], "entities": [{"text": "Intrinsic evaluations", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.820570170879364}]}, {"text": "But both types involve human judgments of some sort and with them their inherent variability.", "labels": [], "entities": []}, {"text": "Humans vary in what material they choose to include in a summary and in how they express the content.", "labels": [], "entities": []}, {"text": "Humans judgments of summary quality vary from one person to another and across time for one person.", "labels": [], "entities": []}, {"text": "In human judgments have formed the foundation of the evaluations and information has been collected each year on one or more sorts of variation in those judgments.", "labels": [], "entities": []}, {"text": "The following sections examine this information and how the variation inhuman input affected or did not affect the results of those evaluations.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Analysis of Variance for DUC-2003", "labels": [], "entities": [{"text": "DUC-2003", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.798379123210907}]}]}