{"title": [{"text": "Adaptive Compression-based Approach for Chinese Pinyin Input", "labels": [], "entities": []}], "abstractContent": [{"text": "This article presents a compression-based adap-tive algorithm for Chinese Pinyin input.", "labels": [], "entities": []}, {"text": "There are many different input methods for Chinese character text and the phonetic Pinyin input method is the one most commonly used.", "labels": [], "entities": []}, {"text": "Compression by Partial Match (PPM) is an adaptive statistical modelling technique that is widely used in the field of text compression.", "labels": [], "entities": [{"text": "Compression by Partial Match (PPM)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5787193690027509}, {"text": "text compression", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7850227355957031}]}, {"text": "Compression-based approaches are able to build models very efficiently and incrementally.", "labels": [], "entities": []}, {"text": "Experiments show that adaptive compression-based approach for Pinyin input outperforms modified Kneser-Ney smoothing method implemented by SRILM language tools (Stolcke, 2002).", "labels": [], "entities": []}], "introductionContent": [{"text": "Chinese words comprise ideographic and pictographic characters.", "labels": [], "entities": []}, {"text": "Unlike English, these characters can't be entered by keyboard directly.", "labels": [], "entities": []}, {"text": "They have to be transliterated from keyboard input based on different input methods.", "labels": [], "entities": []}, {"text": "There are two main approaches: phonetic-based input methods such as Pinyin input and structurebased input methods such as WBZX.", "labels": [], "entities": [{"text": "WBZX", "start_pos": 122, "end_pos": 126, "type": "DATASET", "confidence": 0.981551468372345}]}, {"text": "Pinyin input is the easiest to learn and most widely used.", "labels": [], "entities": []}, {"text": "WBZX is more difficult as the user has to remember all the radical parts of each character, but it is faster.", "labels": [], "entities": [{"text": "WBZX", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8501058220863342}, {"text": "remember", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9780670404434204}]}, {"text": "Early products using Pinyin input methods are very slow because of the large number of homonyms in the Chinese language.", "labels": [], "entities": []}, {"text": "The user has to choose the correct character after each Pinyin has been entered.", "labels": [], "entities": []}, {"text": "The situation in current products such as Microsoft IME for Chinese and Chinese Star has been improved with the progress in language modelling) but users are still not satisfied.", "labels": [], "entities": [{"text": "Chinese Star", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.9372620582580566}]}], "datasetContent": [{"text": "We use 220MB People Daily (91-95) as the training corpus and 58M People Daily (96) and stories download from Internet (400K) as the test corpus.", "labels": [], "entities": [{"text": "People Daily", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.8063113391399384}]}, {"text": "We used SRILM language tools) to collect trigram counts and applied modified Kneser-Ney smoothing method to build the language model.", "labels": [], "entities": []}, {"text": "Then we used disambig to translate Pinyin to Chinese characters.", "labels": [], "entities": []}, {"text": "In PPM model we used the same count data collected by SRILM tools.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.8229603171348572}]}, {"text": "We chose a trie structure to store the symbol and count.", "labels": [], "entities": [{"text": "count", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9862791299819946}]}, {"text": "Adaptive PPM model updates the counts during Pinyin input.", "labels": [], "entities": []}, {"text": "It is similar to a cache model ().", "labels": [], "entities": []}, {"text": "We tested both static and adaptive PPM models on test corpus.", "labels": [], "entities": []}, {"text": "PPM models run twice faster than SRILM tool disambig.", "labels": [], "entities": [{"text": "SRILM tool disambig", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.656943162282308}]}, {"text": "It took 20 hours to translate Pinyin (People Daily 96) to character on a Sparc with two CPUs(900Mhz) using SRILM tools.", "labels": [], "entities": [{"text": "People Daily 96)", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.6679148823022842}]}, {"text": "The following shows the results in terms of character error rate.", "labels": [], "entities": [{"text": "character error rate", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.6867911914984385}]}], "tableCaptions": [{"text": " Table 1: PPM model after processing the string  dealornodeal", "labels": [], "entities": []}, {"text": " Table 2: Compression results for different com- pression methods", "labels": [], "entities": []}, {"text": " Table 3: Character Error Rates for Kneser-Ney,  Static and Adaptive PPM", "labels": [], "entities": [{"text": "Character Error Rates", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7182262142499288}, {"text": "PPM", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.5317075848579407}]}]}