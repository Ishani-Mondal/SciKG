{"title": [{"text": "An Efficient Algorithm to Induce Minimum Average Lookahead Grammars for Incremental LR Parsing", "labels": [], "entities": [{"text": "Incremental LR Parsing", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.5847833851973215}]}], "abstractContent": [{"text": "We define anew learning task, minimum average lookahead grammar induction, with strong potential implications for incremental parsing in NLP and cognitive models.", "labels": [], "entities": [{"text": "lookahead grammar induction", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.5708581209182739}]}, {"text": "Our thesis is that a suitable learning bias for grammar induction is to minimize the degree of lookahead required, on the underlying tenet that language evolution drove grammars to be efficiently parsable in incremental fashion.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7236524671316147}]}, {"text": "The input to the task is an unannotated corpus, plus a non-deterministic constraining grammar that serves as an abstract model of environmental constraints confirming or rejecting potential parses.", "labels": [], "entities": []}, {"text": "The constraining grammar typically allows ambiguity and is itself poorly suited for an incremental parsing model, since it gives rise to a high degree of nondetermin-ism in parsing.", "labels": [], "entities": []}, {"text": "The learning task, then, is to induce a deterministic LR (k) grammar under which it is possible to incrementally construct one of the correct parses for each sentence in the corpus, such that the average degree of lookahead needed to do so is minimized.", "labels": [], "entities": []}, {"text": "This is a significantly more difficult optimization problem than merely compiling LR (k) grammars, since k is not specified in advance.", "labels": [], "entities": []}, {"text": "Clearly, na\u00a8\u0131vena\u00a8\u0131ve approaches to this optimization can easily be computationally infeasible.", "labels": [], "entities": []}, {"text": "However , by making combined use of GLR ancestor tables and incremental LR table construction methods , we obtain an O(n 3 + 2 m) greedy approximation algorithm for this task that is quite efficient in practice.", "labels": [], "entities": [{"text": "LR table construction", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.5680704116821289}]}], "introductionContent": [{"text": "Marcus' (1980) Determinism Hypothesis proposed that natural language can be parsed by a mechanism that operates \"strictly deterministically\" in that it does not simulate a nondeterministic machine.", "labels": [], "entities": []}, {"text": "Although the structural details of the deterministic LR The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.", "labels": [], "entities": [{"text": "Hong Kong Research Grants Council (RGC)", "start_pos": 91, "end_pos": 130, "type": "DATASET", "confidence": 0.902629166841507}]}, {"text": "A key issue is that, to give the Determinism Hypothesis teeth, it is necessary to limit the size of the decision window.", "labels": [], "entities": []}, {"text": "Otherwise, it is always possible to circumvent the constraints simply by increasing the degree of lookahead or, equivalently, increasing the buffer size (which we might call the degree of \"look-behind\"); either way, increasing the decision window essentially delays decisions until enough disambiguating information is seen.", "labels": [], "entities": []}, {"text": "In the limit, a decision window equal to the sentence length renders the claim of incremental parsing meaningless.", "labels": [], "entities": []}, {"text": "Marcus simply postulated that a maximum buffer size of three was sufficient.", "labels": [], "entities": []}, {"text": "In contrast, our approach permits greater flexibility and finer gradations, where the average degree of lookahead required can be minimized with the aim of assisting grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7350365519523621}]}, {"text": "Since Marcus' work, a significant body of work on incremental parsing has developed in the sentence processing community, but much of this work has actually suggested models with an increased amount of nondeterminism, often with probabilistic weights (e.g.,;).", "labels": [], "entities": [{"text": "incremental parsing", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.6867943555116653}]}, {"text": "Meanwhile, in the way of formal methods, Tomita (1986) introduced Generalized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic.", "labels": [], "entities": [{"text": "Generalized LR parsing", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7395405371983846}]}, {"text": "Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g.,).", "labels": [], "entities": []}, {"text": "However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints.", "labels": [], "entities": []}, {"text": "Unfortunately, there has still been relatively little work on automatic learning of grammars for deterministic parsers to date.", "labels": [], "entities": []}, {"text": "describe a semi-automatic procedure for learning a deterministic parser from a treebank, which requires the intervention of a human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions such as \"merge\" and \"add-into\", and to identify specific features for disambiguating actions.", "labels": [], "entities": []}, {"text": "In our earlier work we described a deterministic parser with a fully automatically learned decision algorithm).", "labels": [], "entities": []}, {"text": "But unlike our present work, the decision algorithms in both and are procedural; there is no explicit representation of the grammar that can be meaningfully inspected.", "labels": [], "entities": []}, {"text": "Finally, we observe that there are also trainable stochastic shift-reduce parser models, which are theoretically related to shift-reduce parsing, but operate in a highly nondeterministic fashion during parsing.", "labels": [], "entities": []}, {"text": "We believe the shortage of learning models for deterministic parsing is in no small part due to the difficulty of overcoming computational complexity barriers in the optimization problems this would involve.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.5725858062505722}]}, {"text": "Many types of factors need to be optimized in learning, because deterministic parsing is much more sensitive to incorrect choice of structural features (e.g., categories, rules) than nondeterministic parsing that employ robustness mechanisms such as weighted charts.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.6218294501304626}]}, {"text": "Consequently, we suggest shifting attention to the development of new methods that directly address the problem of optimizing criteria associated with deterministic parsing, in computationally feasible ways.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.557441771030426}]}, {"text": "In particular, we aim in this paper to develop a method that efficiently searches fora parser under a minimum average lookahead cost function.", "labels": [], "entities": []}, {"text": "It should be emphasized that we view the role of a deterministic parser as one component in a larger model.", "labels": [], "entities": []}, {"text": "A deterministic parsing stage can be expected to handle most input sentences, but not all.", "labels": [], "entities": []}, {"text": "Other nondeterministic mechanisms will clearly be needed to handle a minority of cases, the most obvious being garden-path sentences.", "labels": [], "entities": []}, {"text": "In the sections that follow, we first formalize the learning problem.", "labels": [], "entities": []}, {"text": "We then describe an efficient approximate algorithm for this task.", "labels": [], "entities": []}, {"text": "The operation of this algorithm is illustrated with an example.", "labels": [], "entities": []}, {"text": "Finally, we give an analysis of the complexity characteristics of the algorithm.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}