{"title": [{"text": "Empirical Acquisition of Differentiating Relations from Definitions", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes anew automatic approach for extracting conceptual distinctions from dictionary definitions.", "labels": [], "entities": [{"text": "extracting conceptual distinctions from dictionary definitions", "start_pos": 49, "end_pos": 111, "type": "TASK", "confidence": 0.8910495837529501}]}, {"text": "A broad-coverage dependency parser is first used to extract the lexical relations from the definitions.", "labels": [], "entities": [{"text": "broad-coverage dependency parser", "start_pos": 2, "end_pos": 34, "type": "TASK", "confidence": 0.6498345931371053}]}, {"text": "Then the relations are disambiguated using associations learned from tagged corpora.", "labels": [], "entities": []}, {"text": "This contrasts with earlier approaches using manually developed rules for disambiguation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large-scale lexicons for computational semantics often lack sufficient distinguishing information for the concepts serving to define words.", "labels": [], "entities": []}, {"text": "For example, WordNet recently introduced new relations for domain category and location in Version 2.0, along with 6,000+ instances; however, about 38% of the noun synsets are still not explicitly distinguished from sibling synsets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9516776204109192}, {"text": "domain category and location", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.7552855014801025}]}, {"text": "Work on the Extended WordNet project () is achieving substantial progress in making the information in WordNet more explicit.", "labels": [], "entities": [{"text": "Extended WordNet project", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.6588031550248464}]}, {"text": "The main goal is to transform the definitions into a logical form representation suitable for drawing inferences; in addition, the content words in the definitions are being disambiguated.", "labels": [], "entities": []}, {"text": "In the logical form representation, separate predicates are used for each preposition, as well as for some other functional words (e.g., conjunctions); thus, ambiguity in the underlying relations implicit in the definitions is not being resolved.", "labels": [], "entities": []}, {"text": "The work described here automates the process of relation disambiguation.", "labels": [], "entities": [{"text": "relation disambiguation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7990309298038483}]}, {"text": "This can be used to further the transformation of WordNet into an explicit lexical knowledge base.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9240956902503967}]}, {"text": "Earlier approaches to differentia extraction have predominantly relied upon manually constructed pattern matching rules for extracting relations from dictionary definitions).", "labels": [], "entities": [{"text": "differentia extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.952845573425293}]}, {"text": "These rules can be very precise, but achieving broad-coverage can be difficult.", "labels": [], "entities": []}, {"text": "Here abroad coverage dependency parser is first used to determine the syntactic relations that are present among the constituents in the sentence.", "labels": [], "entities": [{"text": "abroad coverage dependency parser", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.5938993021845818}]}, {"text": "Then the syntactic relations between sentential constituents are converted into semantic relations between the underlying concepts using statistical classification.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.6915336847305298}]}, {"text": "Isolating the disambiguation step from the extraction step in this manner allows for greater flexibility over earlier approaches.", "labels": [], "entities": []}, {"text": "For example, different parsers can be incorporated without having to rework the disambiguation process.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 details the steps in extracting the initial relations from the definition parse.", "labels": [], "entities": []}, {"text": "Section 3 illustrates the disambiguation process, the crucial part of this approach.", "labels": [], "entities": [{"text": "disambiguation process", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9104040861129761}]}, {"text": "Section 4 presents an evaluation of the relations that are extracted from the WordNet definitions.", "labels": [], "entities": [{"text": "WordNet definitions", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.9574510455131531}]}, {"text": "Lastly, Section 5 compares the approach to previous approaches that have been tried.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation discussed here assesses the quality of the information that would be added to the lexicons with respect to relation disambiguation, which is the focus of the research.", "labels": [], "entities": [{"text": "relation disambiguation", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.7850997447967529}]}, {"text": "An application-oriented evaluation is discussed in (O'Hara, forthcoming), showing how using the extracted information improves wordsense disambiguation.", "labels": [], "entities": [{"text": "O'Hara, forthcoming)", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.9420427232980728}, {"text": "wordsense disambiguation", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.7076922953128815}]}, {"text": "All the definitions from WordNet 1.7.1 were run through the differentia-extraction process.", "labels": [], "entities": [{"text": "WordNet 1.7.1", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9405112266540527}]}, {"text": "This involved 111,223 synsets, of which 10,810 had preprocessing or parse-related errors leading to no relations being extracted.", "labels": [], "entities": []}, {"text": "shows the frequency of the relations in the output from the differentia extraction process.", "labels": [], "entities": [{"text": "differentia extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8197648525238037}]}, {"text": "The most common relation used is Theme, which occurs four times as much compared to the annotations.", "labels": [], "entities": [{"text": "Theme", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9952881336212158}]}, {"text": "It is usually annotated as the sense for 'of,' which also occurs with roles Source, Category, Ground , Agent , Characteristic, and Experiencer . Some of these represent subtle distinctions, so it is likely that the difference in the text genre is causing the classifier to use the default more often.", "labels": [], "entities": []}, {"text": "Four human judges were recruited to evaluate random samples of the relations that were extracted.", "labels": [], "entities": []}, {"text": "To allow for inter-coder reliability analysis, each evaluator evaluated some samples that were also evaluated by the others, half as part of a training phase and half after training.", "labels": [], "entities": [{"text": "inter-coder reliability analysis", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.718740443388621}]}, {"text": "In addition, they also evaluated a few samples that were manually corrected beforehand.", "labels": [], "entities": []}, {"text": "This provides a baseline against which the uncorrected results can be measured against.", "labels": [], "entities": []}, {"text": "Because the research only addresses relations indicated by prepositional phrases, the evaluation is restricted to these cases.", "labels": [], "entities": []}, {"text": "Specifically, the judges rate the assignment of relations to the prepositional phrases on a scale from 1 to 5, with 5 being an exact match.", "labels": [], "entities": []}, {"text": "The evaluation is based on averaging the assess-: Mean assessment score for all extracted relationships.", "labels": [], "entities": [{"text": "Mean assessment score", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.9491563042004904}]}, {"text": "25 relationships were each evaluated by 4 judges.", "labels": [], "entities": []}, {"text": "Mean gives the mean of the assessment ratings (from 1 to 5).", "labels": [], "entities": []}, {"text": "Score gives ratings relative to scale from 0 to 1.", "labels": [], "entities": []}, {"text": "ment scores over the relationships.", "labels": [], "entities": [{"text": "ment scores", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9669302105903625}]}, {"text": "shows the results from this evaluation, including the manually corrected as well as the uncorrected subsets of the relationships.", "labels": [], "entities": []}, {"text": "For the corrected output, the mean assessment value was 3.225, which translates into an overall score of 0.60.", "labels": [], "entities": [{"text": "mean assessment value", "start_pos": 30, "end_pos": 51, "type": "METRIC", "confidence": 0.9137299060821533}]}, {"text": "For the uncorrected system output, the mean assessment value was 3.033, which translates into an overall score of 0.58.", "labels": [], "entities": []}, {"text": "Although the absolute score is not high, the system's output is generally acceptable, as the score for the uncorrected set of relationships is close to that of the manually corrected set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frequency of relations extracted.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9593666791915894}]}, {"text": " Table 2: Mean assessment score for all ex- tracted relationships. 25 relationships were each  evaluated by 4 judges. Mean gives the mean of the  assessment ratings (from 1 to 5). Score gives ratings  relative to scale from 0 to 1.", "labels": [], "entities": [{"text": "Mean assessment score", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9513384302457174}, {"text": "Mean", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9685533046722412}, {"text": "Score", "start_pos": 180, "end_pos": 185, "type": "METRIC", "confidence": 0.9760297536849976}]}]}