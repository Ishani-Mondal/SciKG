{"title": [{"text": "The Entropy Rate Principle as a Predictor of Processing Effort: An Evaluation against Eye-tracking Data", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper provides evidence for Genzel and Char-niak's (2002) entropy rate principle, which predicts that the entropy of a sentence increases with its position in the text.", "labels": [], "entities": []}, {"text": "We show that this principle holds for individual sentences (not just for averages), but we also find that the entropy rate effect is partly an artifact of sentence length, which also correlates with sentence position.", "labels": [], "entities": [{"text": "entropy rate effect", "start_pos": 110, "end_pos": 129, "type": "METRIC", "confidence": 0.829766571521759}]}, {"text": "Secondly, we evaluate a set of predictions that the entropy rate principle makes for human language processing; using a corpus of eye-tracking data, we show that entropy and processing effort are correlated, and that processing effort is constant throughout a text.", "labels": [], "entities": [{"text": "human language processing", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6827907860279083}]}], "introductionContent": [{"text": "Genzel and Charniak introduce the entropy rate principle, which states that speakers produce language whose entropy rate is on average constant.", "labels": [], "entities": []}, {"text": "The motivation for this comes from information theory: the most efficient way of transmitting information through a noisy channel is at a constant rate.", "labels": [], "entities": []}, {"text": "If human communication has evolved to be optimal in this sense, then we would expect humans to produce text and speech with approximately constant entropy.", "labels": [], "entities": []}, {"text": "There is some evidence that this is true for speech.", "labels": [], "entities": []}, {"text": "For text, the entropy rate principle predicts that the entropy of an individual sentence increases with its position in the text, if entropy is measured out of context.", "labels": [], "entities": []}, {"text": "show that this prediction is true for the Wall Street Journal corpus, for both function words and for content words.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 42, "end_pos": 68, "type": "DATASET", "confidence": 0.970785915851593}]}, {"text": "They estimate entropy either using a language model or using a probabilistic parser; the effect can be observed in both cases.", "labels": [], "entities": []}, {"text": "extend this results in several ways: they show that the effect holds for different genres (but the effect size varies across genres), and also applies within paragraphs, not only within whole texts.", "labels": [], "entities": []}, {"text": "Furthermore, they show that the effect can also be obtained for language other than English (Russian and Spanish).", "labels": [], "entities": []}, {"text": "The entropy rate principle also predicts that a language model that takes context into account should yield lower entropy estimates compared to an out of context language model.", "labels": [], "entities": []}, {"text": "show that this prediction holds for caching language models such as the ones proposed by.", "labels": [], "entities": []}, {"text": "The aim of the present paper is to shed further light on the entropy rate effect discovered by Genzel and Charniak) (henceforth G&C) by providing new evidence in two areas.", "labels": [], "entities": []}, {"text": "In Experiment 1, we replicate G&C's entropy rate effect and investigate the source of the effect.", "labels": [], "entities": []}, {"text": "The results show that the correlation coefficients that G&C report are inflated by averaging over sentences with the same position, and by restricting the range of the sentence position considered.", "labels": [], "entities": []}, {"text": "Once these restrictions are removed the effect is smaller, but still significant.", "labels": [], "entities": []}, {"text": "We also show that the effect is to a large extend due to a confound with sentence length: longer sentences tend to occur later in the text.", "labels": [], "entities": []}, {"text": "However, we are able to demonstrate that the entropy rate effect still holds once this confound has been removed.", "labels": [], "entities": [{"text": "entropy rate effect", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.9423308173815409}]}, {"text": "In Experiment 2, we test the psycholinguistic predictions of the entropy rate principle.", "labels": [], "entities": []}, {"text": "This experiment uses a subset of the British National Corpus as training data and tests on the Embra corpus, a set of newspaper articles annotated with eye-movement data.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.9327953259150187}, {"text": "Embra corpus", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9932686686515808}]}, {"text": "We find that there is a correlation between the entropy of a sentence and the processing effort it causes, as measured by reading times in eyetracking data.", "labels": [], "entities": []}, {"text": "We also show that there is no correlation between processing effort and sentence position, which indicates that processing effort in context is constant through a text, which is one of the assumptions underlying the entropy rate principle.", "labels": [], "entities": []}, {"text": "context predictions and out-of-context predictions.", "labels": [], "entities": [{"text": "context predictions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.717600867152214}]}, {"text": "The principle states that the entropy rate in a text is constant, i.e., that speakers produce sentences so that on average, all sentences in a text have the same entropy.", "labels": [], "entities": []}, {"text": "In other words, communication is optimal in the sense that all sentences in the text are equally easy to understand, as they all have the same entropy.", "labels": [], "entities": []}, {"text": "This constancy principle is claimed to hold for connected text: all sentences in a text should be equally easy to process if they are presented in context.", "labels": [], "entities": []}, {"text": "If we take reading time as a measure of processing effort, then the principle predicts that there should be no significant correlation between sentence position and reading time in context.", "labels": [], "entities": []}, {"text": "We will test this prediction in Experiment 2 using an eyetracking corpus consisting of connected text.", "labels": [], "entities": []}, {"text": "The entropy rate principle also makes the following prediction: if the entropy of a sentence is measured out of context (i.e., without taking the preceding sentences into account), then entropy will increase with sentence position.", "labels": [], "entities": []}, {"text": "This prediction was tested extensively by G&C, whose results will be replicated in Experiment 1.", "labels": [], "entities": [{"text": "G&C", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8137811024983724}]}, {"text": "With respect to processing difficulty, the entropy rate principle also predicts that processing difficulty out of context (i.e., if isolated sentences are presented to experimental subjects) should increase with sentence position.", "labels": [], "entities": []}, {"text": "We could not test this prediction, as we only had incontext reading time data available for the present study.", "labels": [], "entities": []}, {"text": "However, there is another important prediction that can be derived from the entropy rate principle: sentences with a higher entropy should have higher reading times.", "labels": [], "entities": []}, {"text": "This is an important precondition for the entropy rate principle, whose claims about the relationship between entropy and sentence position are only meaningful if entropy and processing effort are correlated.", "labels": [], "entities": []}, {"text": "If there was no such correlation, then there would be no reason to assume that the outof-context entropy of a sentence increases with sentence position.", "labels": [], "entities": []}, {"text": "G&C explicitly refer to this relationship i.e., they assume that a sentence that is more informative is harder to process.", "labels": [], "entities": []}, {"text": "Experiment 1 will try to demonstrate the validity of this important prerequisite of the entropy rate principle.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main aim of this experiment was to replicate G&C's entropy rate effect.", "labels": [], "entities": [{"text": "replicate G&C", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.7263321876525879}]}, {"text": "A second aim was to test the generality of their result by determining if the relationship between sentence position and entropy also holds for individual sentences (rather than for averages over sentences of a given position, as tested by G&C).", "labels": [], "entities": []}, {"text": "We also investigated the effect of two parameters that G&C did not explore: the cutoff for article position (G&C only deal with sentences up to position 25), and the size of the n-gram used for estimating sentence probability.", "labels": [], "entities": []}, {"text": "Finally, we include sentence length as a baseline that entropybased models should be evaluated against.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of Experiment 1: correlation of sentence entropy and sentence position, on binned data; c:  cut-off, r: correlation coefficient, p: significance level,  * : correlation significantly different from baseline  (sentence length)", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 122, "end_pos": 145, "type": "METRIC", "confidence": 0.9526367783546448}]}, {"text": " Table 2: Results of Experiment 1: correlation of entropy and sentence length with sentence position, with  the other factor partialled out", "labels": [], "entities": []}, {"text": " Table 3: Results of Experiment 2: correlation of sentence entropy and sentence position on the BNC", "labels": [], "entities": [{"text": "BNC", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.6755948662757874}]}, {"text": " Table 5: Results of Experiment 2: correlation of  reading times with sentence entropy and sentence  position", "labels": [], "entities": []}]}