{"title": [{"text": "Reference Resolution over a Restricted Domain: References to Documents", "labels": [], "entities": [{"text": "Reference Resolution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9028536379337311}]}], "abstractContent": [{"text": "This article studies the resolution of references made by speakers to documents discussed during a meeting.", "labels": [], "entities": [{"text": "resolution of references made by speakers to documents discussed during a meeting", "start_pos": 25, "end_pos": 106, "type": "TASK", "confidence": 0.8466998289028803}]}, {"text": "The focus is on transcribed recordings of press review meetings, in French.", "labels": [], "entities": [{"text": "transcribed recordings of press review meetings", "start_pos": 16, "end_pos": 63, "type": "TASK", "confidence": 0.6896796524524689}]}, {"text": "After an overview of the required framework for reference resolution-specification of the task, data annotation , and evaluation procedure-we propose, analyze and evaluate an algorithm for the resolution of references to documents (ref2doc) based on anaphora tracking and context matching.", "labels": [], "entities": [{"text": "reference resolution-specification", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.7290199398994446}, {"text": "context matching", "start_pos": 272, "end_pos": 288, "type": "TASK", "confidence": 0.6937640011310577}]}, {"text": "Applications to speech-to-document alignment and more generally to meeting processing and retrieval are finally discussed.", "labels": [], "entities": [{"text": "speech-to-document alignment", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7154417634010315}, {"text": "meeting processing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8846682608127594}]}], "introductionContent": [{"text": "The references made by the speakers to the entities that they talk about are one of the keys to the understanding of human dialogs.", "labels": [], "entities": []}, {"text": "When speakers discuss one or more documents, as in a press review meeting, the references to these documents constitute a significant proportion of all the occurring references.", "labels": [], "entities": []}, {"text": "A computer representation of the referents is available in this case, unlike references to more abstract objects, since here the documents can be stored in electronic format.", "labels": [], "entities": []}, {"text": "Reference resolution amounts thus to the construction of links between each referring expression (RE) and the corresponding document element.", "labels": [], "entities": [{"text": "Reference resolution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9195654690265656}]}, {"text": "For example, if someone says: \"I do not agree with the title of our latest report\", then 'our latest report' refers to a document available as a computer file, and 'the title of our latest report' refers precisely to its title, an element that can be retrieved from the file.", "labels": [], "entities": []}, {"text": "We propose here an algorithm for the resolution of references to documents, or ref2doc.", "labels": [], "entities": [{"text": "resolution of references to documents", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.8887313723564148}]}, {"text": "Its implementation and evaluation require a computational framework that includes several types of datadocuments, transcriptions, and links-and an evaluation measure.", "labels": [], "entities": []}, {"text": "We summarize our view of reference resolution over a restricted domain in Section 2.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8718602657318115}]}, {"text": "Then, we situate the present task in the overall speech-todocument alignment process (Section 3).", "labels": [], "entities": [{"text": "speech-todocument alignment", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6769856214523315}]}, {"text": "The annotated data and the evaluation metric are described in Section 4, along with empirical results regarding the patterns of the REs.", "labels": [], "entities": []}, {"text": "The resolution algorithm is presented in Section 5, and the results obtained in various configurations are analyzed in Section 6, with conclusions about their relevance.", "labels": [], "entities": []}, {"text": "Section 7 outlines the applications of the ref2doc algorithm to the exploitation of documents in meeting processing and retrieval applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two important elements for testing are the available data (4.2), which must be specifically annotated (4.1), and a scoring procedure (4.3), which is quite straightforward, and provides several scores.", "labels": [], "entities": []}, {"text": "Unlike intra-document coreference resolution, for which evaluation is a complex task (, the evaluation of reference resolution over a specific domain is quite straightforward.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7864184677600861}]}, {"text": "One must compare for each RE the referent found by the system with the correct one selected by the annotators.", "labels": [], "entities": [{"text": "RE", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9473734498023987}]}, {"text": "If the two are the same, the system scores 1, otherwise it scores 0.", "labels": [], "entities": []}, {"text": "The total score is the number of correctly solved REs out of the total number of REs (100% means perfect).", "labels": [], "entities": [{"text": "REs", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9773110747337341}]}, {"text": "The automatic evaluation measure we implemented using the XML annotation described above provides in fact three scores: 1.", "labels": [], "entities": []}, {"text": "The number of times the document an RE refers to is correctly identified.", "labels": [], "entities": []}, {"text": "This is informative only when a dialog deals with more than one document.", "labels": [], "entities": []}, {"text": "2. The number of times the document element, characterized by its ID attribute, is correctly identified.", "labels": [], "entities": []}, {"text": "Here, the possible types of document elements are article: MasterArticle, JournalArticle, Article or Highlight.", "labels": [], "entities": [{"text": "MasterArticle", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.8826673030853271}, {"text": "Article", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.8311688899993896}]}, {"text": "3. The number of times the specific part of an article is correctly identified (e.g., content, title, author, image, as indicated by the XPath annotation in the XML output format).", "labels": [], "entities": []}, {"text": "The third score is necessarily lower than the second one, and the second one is necessarily lower than the first one.", "labels": [], "entities": []}, {"text": "The third score is not used for the moment, since our ref2doc algorithms do not target sub-article elements.", "labels": [], "entities": []}, {"text": "To help adjust the resolution algorithm, the scoring program also outputs a detailed evaluation report for each meeting, so that a human scorer can compare the system's output and the correct answer explicitly.", "labels": [], "entities": [{"text": "resolution", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.9391557574272156}]}], "tableCaptions": []}