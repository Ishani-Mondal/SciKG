{"title": [{"text": "Semantic Role Labelling With Chunk Sequences", "labels": [], "entities": [{"text": "Semantic Role Labelling", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8180718024571737}]}], "abstractContent": [{"text": "We describe a statistical approach to semantic role labelling that employs only shallow information.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7021897037823995}]}, {"text": "We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument candidate.", "labels": [], "entities": []}, {"text": "The instances to be classified are sequences of chunks that occur frequently as arguments in the training corpus.", "labels": [], "entities": []}, {"text": "Our best model obtains an F score of 51.70 on the test set.", "labels": [], "entities": [{"text": "F score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9925328195095062}]}], "introductionContent": [{"text": "This paper describes a statistical approach to semantic role labelling addressing the CoNLL shared task 2004, which is based on the the current release of the English PropBank data).", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.6125557124614716}, {"text": "CoNLL shared task 2004", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.6685881242156029}, {"text": "English PropBank data", "start_pos": 159, "end_pos": 180, "type": "DATASET", "confidence": 0.9411373337109884}]}, {"text": "For further details of the task, see.", "labels": [], "entities": []}, {"text": "We address the main challenge of the task, the absence of deep syntactic information, with three main ideas: Proper constituents being unavailable, we use chunk sequences as instances for classification.", "labels": [], "entities": []}, {"text": "The classification is performed by a maximum entropy model, which can integrate features from heterogeneous data sources.", "labels": [], "entities": []}, {"text": "We model the fit between verb and argument candidate by clusters induced with EM on the training data, which we use as features during classification.", "labels": [], "entities": []}, {"text": "Sections 2 through 4 describe the systems' architecture.", "labels": [], "entities": []}, {"text": "First, we compute chunk sequences for all sentences.", "labels": [], "entities": []}, {"text": "Then, we classify these sequences with maximum entropy models.", "labels": [], "entities": []}, {"text": "Finally, we determine the most probable chain of sequences covering the whole sentence (Sec. 4).", "labels": [], "entities": []}, {"text": "Section 5 discusses the impact of different parameters and gives final results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Different models for argument identification  (evaluation scores category-specific for LABEL)", "labels": [], "entities": [{"text": "argument identification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7887205481529236}, {"text": "LABEL", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.7688347697257996}]}, {"text": " Table 2: Different models for argument labelling  (based on the best argument identification model)", "labels": [], "entities": [{"text": "argument labelling", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7100661247968674}]}]}