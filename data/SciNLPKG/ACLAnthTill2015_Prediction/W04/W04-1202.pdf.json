{"title": [{"text": "Using Argumentation to Retrieve Articles with Similar Citations from MEDLINE", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.7366300225257874}]}], "abstractContent": [{"text": "The aim of this study is to investigate the relationships between citations and the scientific argumentation found in the abstract.", "labels": [], "entities": []}, {"text": "We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain.", "labels": [], "entities": []}, {"text": "In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.7853531837463379}]}, {"text": "Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS, and CONCLUSION.", "labels": [], "entities": [{"text": "PURPOSE", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9903625845909119}, {"text": "METHODS", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9915871620178223}, {"text": "RESULTS", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9947197437286377}, {"text": "CONCLUSION", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.8754546046257019}]}, {"text": "A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories.", "labels": [], "entities": []}, {"text": "The categories are used to generate four different argumentative indexes.", "labels": [], "entities": []}, {"text": "A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms.", "labels": [], "entities": [{"text": "Medical Subject Headings (MeSH) terms", "start_pos": 86, "end_pos": 123, "type": "TASK", "confidence": 0.7040954317365374}]}, {"text": "To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark.", "labels": [], "entities": []}, {"text": "Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9982276558876038}, {"text": "PURPOSE", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9831463694572449}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9990647435188293}, {"text": "RESULTS", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9648996591567993}, {"text": "METHODS", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.8891486525535583}]}, {"text": "A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Numerous techniques help researchers locate relevant documents in an ever-growing mountain of scientific publications.", "labels": [], "entities": []}, {"text": "Among these techniques is the analysis of bibliographic information, which can identify conceptual connections between large numbers of articles.", "labels": [], "entities": []}, {"text": "Although helpful, most of these systems deliver masses of documents to the researcher for analysis, which contain various degrees of similarity.", "labels": [], "entities": []}, {"text": "This paper introduces a method to determine the similarity of a bibliographic co-citation list, that is the list of citations that are shared between articles, and the argumentative moves of an abstract in an effort to define novel similarity searches.", "labels": [], "entities": []}, {"text": "Authors of biological papers develop arguments and present the justification for their experiments based on previously documented results.", "labels": [], "entities": []}, {"text": "These results are represented as citations to earlier scientific literature and establish the links between old and new findings.", "labels": [], "entities": []}, {"text": "The assumption is that the majority of scientific papers employing the same citations depict related viewpoints.", "labels": [], "entities": []}, {"text": "The method described here is applied to improve retrieval of similar articles based on co-citations, but other applications are foreseen.", "labels": [], "entities": []}, {"text": "Documents that should be conceptually correlated due to bibliographic relatedness but which propose different or novel arguments are often not easily located in the majority of bibliographically correlated articles.", "labels": [], "entities": []}, {"text": "Our system can be tuned to identify these documents.", "labels": [], "entities": []}, {"text": "Conversely, such a system could also be used as a platform to aid authors by means of automatic assembly or refinement of their bibliographies through the suggestion of citations coming from documents containing similar arguments.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: section 2 describes the background related to experiments using citations or argumentation that compare aspects connected to the logical content of publications.", "labels": [], "entities": []}, {"text": "Section 3 details the method and the generation of the different indexes used in our analyses, e.g. the citation index, the four argumentative indexes and the abstract index (abstract, title and keywords).", "labels": [], "entities": []}, {"text": "Section 4 presents the results of the evaluations we performed.", "labels": [], "entities": []}, {"text": "Section 5 closes with a summary of the contribution of this work, limitations and future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Confusion matrices for each argumentative class.", "labels": [], "entities": []}, {"text": " Table 3. Mean average precision (MAP) for each query set", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9584344724814097}]}, {"text": " Table 4. MAP results from querying the collection using  only the argumentative move.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7557535767555237}]}]}