{"title": [], "abstractContent": [{"text": "We present a model and an experimental platform of a bootstrapping approach to statistical induction of natural language properties that is constraint based with voting components.", "labels": [], "entities": [{"text": "statistical induction of natural language properties", "start_pos": 79, "end_pos": 131, "type": "TASK", "confidence": 0.8639346361160278}]}, {"text": "The system is incremental and unsupervised.", "labels": [], "entities": []}, {"text": "In the following discussion we focus on the components for morphological induction.", "labels": [], "entities": [{"text": "morphological induction", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8326486945152283}]}, {"text": "We show that the much harder problem of incremental unsupervised morphological induction can outperform comparable all-at-once algorithms with respect to precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9952746629714966}]}, {"text": "We discuss how we use such systems to identify cues for induction in a cross-level architecture.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years there has been a growing amount of work focusing on the computational modeling of language processing and acquisition, implying a cognitive and theoretical relevance both of the models as such, as well as of the language properties extracted from raw linguistic data.", "labels": [], "entities": [{"text": "language processing and acquisition", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.6987292692065239}]}, {"text": "In the computational linguistic literature several attempts to induce grammar or linguistic knowledge from such data have shown that at different levels a high amount of information can be extracted, even with no or minimal supervision.", "labels": [], "entities": []}, {"text": "Different approaches tried to show how various puzzles of language induction could be solved.", "labels": [], "entities": [{"text": "language induction", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7718341648578644}]}, {"text": "From this perspective, language acquisition is the process of segmentation of non-discrete acoustic input, mapping of segments to symbolic representations, mapping representations on higher-level representations such as phonology, morphology and syntax, and even induction of semantic properties.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7199474573135376}]}, {"text": "Due to space restrictions, we cannot discuss all these approaches in detail.", "labels": [], "entities": []}, {"text": "We will focus on the close domain of morphology.", "labels": [], "entities": []}, {"text": "Approaches to the induction of morphology as presented in e.g. or show that the morphological properties of a small subset of languages can be induced with high accuracy, most of the existing approaches are motivated by applied or engineering concerns, and thus make assumptions that are less cognitively plausible: a.", "labels": [], "entities": [{"text": "induction of morphology", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8498823046684265}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.972805380821228}]}, {"text": "Large corpora are processed all at once, though unsupervised incremental induction of grammars is rather the approach that would be relevant from a psycholinguistic perspective; b.", "labels": [], "entities": []}, {"text": "Arbitrary decisions about selections of sets of elements are made, based on frequency or frequency profile rank, 2 though such decisions should rather be derived or avoided in general.", "labels": [], "entities": []}, {"text": "However, the most important aspects missing in these approaches, however, are the link to different linguistic levels and the support of a general learning model that makes predictions about how knowledge is induced on different linguistic levels and what the dependencies between information at these levels are.", "labels": [], "entities": []}, {"text": "Further, there is no study focusing on the type of supervision that might be necessary for the guidance of different algorithm types towards grammars that resemble theoretical and empirical facts about language acquisition, and processing and the final knowledge of language.", "labels": [], "entities": []}, {"text": "While many theoretical models of language acquisition use innateness as a crutch to avoid outstanding difficulties, both on the general and abstract level of I-language as well as the more detailed level of E-language, (see, among others, and, there is also significant research being done which shows that children take advantage of statistical regularities in the input for use in the languagelearning task (see and related references within).", "labels": [], "entities": []}, {"text": "In language acquisition theories the dominant view is that knowledge of one linguistic level is bootstrapped from knowledge of one, or even several different levels.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7027619630098343}]}, {"text": "Just to mention such approaches:, and 2 Just to mention some of the arbitrary decisions made in various approaches, e.g. Mintz (1996) selects a small set of all words, the most frequent words, to induce word types via clustering ; select words with frequency higher than 5 to induce morphological segmentation.", "labels": [], "entities": []}, {"text": "assume that semantic properties are used to bootstrap syntactic knowledge, and suggested that prosodic properties of language establish a bias for specific syntactic properties, e.g. headedness or branching direction of constituents.", "labels": [], "entities": []}, {"text": "However, these approaches are based on conceptual considerations and psycholinguistc empirical grounds, the formal models and computational experiments are missing.", "labels": [], "entities": []}, {"text": "It is unclear how the induction processes across linguistic domains might work algorithmically, and the quantitative experiments on large scale data are missing.", "labels": [], "entities": []}, {"text": "As for algorithmic approaches to cross-level induction, the best example of an initial attempt to exploit cues from one level to induce properties of another is presented in, where morphological cues are identified for induction of syntactic structure.", "labels": [], "entities": [{"text": "cross-level induction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7634460031986237}]}, {"text": "Along these lines, we will argue fora model of statistical cue-based learning, introducing a view on bootstrapping as proposed in, and, that relies on identification of elementary cues in the language input and incremental induction and further cue identification across all linguistic levels.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following we discuss the experimental setting.", "labels": [], "entities": []}, {"text": "We used the Brown corpus, 7 the child- oriented speech portion of the CHILDES Peter corpus, 8 and Caesar's \"De Bello Gallico\" in Latin.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.9672832489013672}, {"text": "CHILDES Peter corpus", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.9405184189478556}]}, {"text": "From the Brown corpus we used the files ck01 -ck09, with an average number of 2000 words per chapter.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.9725778698921204}]}, {"text": "The total number of words in these files is 18071.", "labels": [], "entities": []}, {"text": "The randomly selected portion of \"De Bello Gallico\" contained 8300 words.", "labels": [], "entities": []}, {"text": "The randomly selected portion of the Peter corpus contains 58057 words.", "labels": [], "entities": [{"text": "Peter corpus", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8600453436374664}]}, {"text": "The system reads in each file and dumps log information during runtime that contains the information for online and offline evaluation, as described below in detail.", "labels": [], "entities": []}, {"text": "The gold standard for evaluation is based on human segmentation of the words in the respective corpora.", "labels": [], "entities": []}, {"text": "We create for every word a manual segmentation for the given corpora, used for online evaluation of the system for accuracy of hypothesis generation during runtime.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9945999383926392}, {"text": "hypothesis generation", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.6703034192323685}]}, {"text": "Due to complicated cases, where linguist are undecided about the accurate morphological segmentation, a team of 5 linguists was cooperating with this task.", "labels": [], "entities": []}, {"text": "The offline evaluation is based on the grammar that is generated and dumped during runtime after each input file is processed.", "labels": [], "entities": []}, {"text": "The grammar is manually annotated by a team of linguists, indicating for each construction whether it was segmented correctly and exhaustively.", "labels": [], "entities": []}, {"text": "An additional evaluation criterion was to mark undecided cases, where even linguists do not agree.", "labels": [], "entities": []}, {"text": "This information was however not used in the final evaluation.", "labels": [], "entities": []}, {"text": "We used two methods to evaluate the performance of the algorithm.", "labels": [], "entities": []}, {"text": "The first analyzes the accuracy of the morphological rules produced by the algorithm after an increment of n words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9990450739860535}]}, {"text": "The second looks at how accurately the algorithm parsed each word that it encountered as it progressed through the corpus.", "labels": [], "entities": []}, {"text": "The morphological rule analysis looks at each grammar rule generated by the algorithm and judges it on the correctness of the rule and the resulting parse.", "labels": [], "entities": []}, {"text": "A grammar rule consists of a stem and the suffixes and prefixes that can be attached to it, similar to the signatures used in Goldsmith (2001).", "labels": [], "entities": []}, {"text": "The grammar rule was then marked as to whether it consisted of legitimate suffixes and prefixes for that stem, and also as to whether the stem of the rule was a true stem, as opposed to a stem plus another morpheme that wasn't identified by the algorithm.", "labels": [], "entities": []}, {"text": "The number of rules that were correct in these two categories were then summed, and precision and recall figures were calculated for the trial.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9995588660240173}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9990894794464111}]}, {"text": "The trials described in the graph below were run on three increasingly large portions of the general fiction section of the Brown Corpus.", "labels": [], "entities": [{"text": "general fiction section of the Brown Corpus", "start_pos": 93, "end_pos": 136, "type": "DATASET", "confidence": 0.6884566673210689}]}, {"text": "The first trial was run on one randomly chosen chapter, the second trial on two chapters, and the third on three chapters.", "labels": [], "entities": []}, {"text": "The graph shows the harmonic average (F-score) of precision and recall.", "labels": [], "entities": [{"text": "harmonic average (F-score)", "start_pos": 20, "end_pos": 46, "type": "METRIC", "confidence": 0.8247693300247192}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9984052777290344}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9972284436225891}]}, {"text": "The second analysis is conducted as the algorithm is running and examines each parse the system produces.", "labels": [], "entities": []}, {"text": "The algorithm's parses are compared with the \"correct\" morphological parse of the word using the following method to derive a numerical score fora particular parse.", "labels": [], "entities": []}, {"text": "The first part of the score is the distance in characters between each morphological boundary in the two parses, with a score of one point for each character space.", "labels": [], "entities": []}, {"text": "The second part is a penalty of two points for each morphological boundary that occurs in one parse and not the other.", "labels": [], "entities": []}, {"text": "These scores were examined within a moving window of words that progressed through the corpus as the algorithm ran.", "labels": [], "entities": []}, {"text": "The average scores of words in each such window were calculated as the window advanced.", "labels": [], "entities": []}, {"text": "The purpose of this method was to allow the performance of the algorithm to be judged at a given point without prior performance in the corpus affecting the analysis of the current window.", "labels": [], "entities": []}, {"text": "The following graph shows how the average performance of the windows of analyzed words as the algorithm progresses through five randomly chosen chapters of general fiction in the Brown Corpus amounting to around 10,000 words.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 179, "end_pos": 191, "type": "DATASET", "confidence": 0.9662317335605621}]}, {"text": "The window size for the following graph was set to 40 words.", "labels": [], "entities": []}, {"text": "The evaluations on Latin were based on the initial 4000 words of \"De Bello Gallico\" in a pretest.", "labels": [], "entities": []}, {"text": "In the very initial phase we reached a precision of 99.5% and a recall of 13.2%.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9996139407157898}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9996843338012695}]}, {"text": "This is however the preliminary result for the initial phase only.", "labels": [], "entities": []}, {"text": "We expect that fora larger corpus the recall will increase much higher, given the rich morphology of Latin, potentially with negative consequences for precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9996469020843506}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9921939373016357}]}, {"text": "The results on the Peter corpus are shown in the following We notice a more or less stable precision value with decreasing recall, due to a higher number of words.", "labels": [], "entities": [{"text": "Peter corpus", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9404744803905487}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9984534978866577}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9986201524734497}]}, {"text": "The Peter corpus contains also many very specific transcriptions and tokens that are indeed unique, thus it is rather surprising to get such results at all.", "labels": [], "entities": [{"text": "Peter corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9555008113384247}]}, {"text": "The following graphics shows the Fscore for the Peter corpus:", "labels": [], "entities": [{"text": "Fscore", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.5795913338661194}, {"text": "Peter corpus", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9482226073741913}]}], "tableCaptions": []}