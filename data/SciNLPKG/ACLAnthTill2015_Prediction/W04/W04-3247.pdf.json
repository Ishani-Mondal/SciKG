{"title": [{"text": "LexPageRank: Prestige in Multi-Document Text Summarization", "labels": [], "entities": [{"text": "LexPageRank", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9301460981369019}, {"text": "Multi-Document Text Summarization", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6088741620381674}]}], "abstractContent": [{"text": "Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document.", "labels": [], "entities": [{"text": "Multidocument extractive summarization", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8649141788482666}]}, {"text": "Central-ity is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence.", "labels": [], "entities": []}, {"text": "We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.", "labels": [], "entities": [{"text": "computing sentence importance", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7302699486414591}, {"text": "LexPageRank", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.9763596653938293}]}, {"text": "In this model, a sentence connectivity matrix is constructed based on cosine similarity.", "labels": [], "entities": [{"text": "sentence connectivity", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6942225992679596}]}, {"text": "If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the con-nectivity matrix.", "labels": [], "entities": []}, {"text": "We provide an evaluation of our method on DUC 2004 data.", "labels": [], "entities": [{"text": "DUC 2004 data", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9852228164672852}]}, {"text": "The results show that our approach outperforms centroid-based summa-rization and is quite successful compared to other summarization systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7586644291877747}]}, {"text": "In this paper, we focus on multi-document generic text summarization, where the goal is to produce a summary of multiple documents about the same, but unspecified topic.", "labels": [], "entities": [{"text": "multi-document generic text summarization", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.7038489431142807}]}, {"text": "Our summarization approach is to assess the centrality of each sentence in a cluster and include the most important ones in the summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9706296920776367}]}, {"text": "In Section 2, we present centroid-based summarization, a wellknown method for judging sentence centrality.", "labels": [], "entities": [{"text": "judging sentence centrality", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.6120449701944987}]}, {"text": "Then we introduce two new measures for centrality, Degree and LexPageRank, inspired from the \"prestige\" concept in social networks and based on our new approach.", "labels": [], "entities": [{"text": "Degree", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9931939840316772}]}, {"text": "We compare our new methods and centroidbased summarization using a feature-based generic summarization toolkit, MEAD, and show that new features outperform Centroid inmost of the cases.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9311829209327698}]}, {"text": "Test data for our experiments is taken from Document Understanding Conferences (DUC) 2004 summarization evaluation to compare our system also with other state-of-the-art summarization systems.", "labels": [], "entities": [{"text": "Document Understanding Conferences (DUC) 2004 summarization evaluation", "start_pos": 44, "end_pos": 114, "type": "DATASET", "confidence": 0.5799813303682539}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results for Task 2", "labels": [], "entities": []}, {"text": " Table 3: Results for Task 4", "labels": [], "entities": []}, {"text": " Table 4: Summary of official ROUGE scores for DUC 2004 Task 2. Peer codes: baseline(2), manual[A-H],  and system submissions .  .  .  .  .  .  .  .  .  .  .  .  .  .  Task 4b  23.  .  .  .  .  .  .  .  .  .  .  .  .  .", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.6900860667228699}, {"text": "DUC 2004 Task", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.8498821258544922}]}]}