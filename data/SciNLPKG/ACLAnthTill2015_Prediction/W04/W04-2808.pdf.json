{"title": [{"text": "Making Relative Sense: From Word-graphs to Semantic Frames", "labels": [], "entities": []}], "abstractContent": [{"text": "Scaling up from controlled single domain spoken dialogue systems towards conversational, multi-domain and multimodal dialogue systems poses new challenges for the reliable processing of less restricted user utterances.", "labels": [], "entities": []}, {"text": "In this paper we explore the feasibility to employ a general purpose ontology for various tasks involved in processing the user's utterances.", "labels": [], "entities": []}], "introductionContent": [{"text": "We differentiate between controlled single-domain and more conversational multi-domain spoken dialogue systems ().", "labels": [], "entities": []}, {"text": "The transition from the former to the later can be regarded as a scaling process, since virtually every processing technique applicable for restricted single domain user utterances has to be adopted to new challenges, i.e., varying context-dependencies ( ) increasing levels of ambiguity () and less predictable input ().", "labels": [], "entities": []}, {"text": "Additionally, for conversational multi-domain spoken dialogue systems tasks have to be tackled that were by and large unnecessary in restricted single-domain systems.", "labels": [], "entities": []}, {"text": "In this exploration, we will focus on a subset of these tasks, namely: \u00a2 hypotheses verification (HV) -i.e. finding the best hypothesis out of a set of possible speech recognition hypotheses (SRH); \u00a2 sense disambiguation (SD) -i.e. determining the best mapping of the lexically ambiguous linguistic forms contained therein to their sense-specific semantic representations; \u00a2 relation tagging (RT) -i.e. determining adequate semantic relations between the relevant sense-tagged entities.", "labels": [], "entities": [{"text": "speech recognition hypotheses (SRH)", "start_pos": 161, "end_pos": 196, "type": "TASK", "confidence": 0.7612804770469666}, {"text": "relation tagging (RT)", "start_pos": 375, "end_pos": 396, "type": "TASK", "confidence": 0.7805092334747314}]}, {"text": "Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation, sense disambiguation in speech synthesis, and relation tagging in information retrieval).", "labels": [], "entities": [{"text": "hypothesis verification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.806288331747055}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7611198425292969}, {"text": "sense disambiguation in speech synthesis", "start_pos": 131, "end_pos": 171, "type": "TASK", "confidence": 0.7316247940063476}, {"text": "relation tagging", "start_pos": 177, "end_pos": 193, "type": "TASK", "confidence": 0.7574079632759094}]}, {"text": "These challenges also apply for spoken dialogue systems and arise when they are scaled up towards multidomain and more conversational settings.", "labels": [], "entities": []}, {"text": "In this paper we will address the utility of using ontologically modeled knowledge to assist in solving these tasks in spoken dialogue systems.", "labels": [], "entities": []}, {"text": "Following an overview of the state of the art in Section 2 and the ontology-based coherence scoring system in Section 3, we describe its employment in the task of hypotheses verification in Section 4.", "labels": [], "entities": [{"text": "hypotheses verification", "start_pos": 163, "end_pos": 186, "type": "TASK", "confidence": 0.7373693883419037}]}, {"text": "In Section 5 we describe the system's employment for the task of sense disambiguation and in Section 6 we present first results of a study examining the performance of the system for the task of relation tagging.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7046940326690674}, {"text": "relation tagging", "start_pos": 195, "end_pos": 211, "type": "TASK", "confidence": 0.8580915629863739}]}, {"text": "An analysis of the evaluation results and concluding remarks are given in Section 7.", "labels": [], "entities": [{"text": "Section 7", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.8802291750907898}]}], "datasetContent": [{"text": "For evaluating the performance of the ONTOSCORE system we defined an accurate match, if the correct semantic relation (role) was chosen by the system for the corresponding concepts contained therein 1 . As inaccurate we counted in analogy to the word error rates in speech recognition: An example of a substitution in this task is given the SRH shown in Example 2.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 266, "end_pos": 284, "type": "TASK", "confidence": 0.7268214523792267}, {"text": "SRH", "start_pos": 341, "end_pos": 344, "type": "DATASET", "confidence": 0.4963324964046478}]}, {"text": "In this case the sense disambiguation was accurate, so that the two ambiguous entities, i.e. kommen and Schloss, were correctly mapped onto a MotionDirectedTransliterated (MDT) process and Sight object -the concept Person resulted from an unambiguous word-to-concept mapping from the form I.", "labels": [], "entities": []}, {"text": "As an insertion we counted the opposite case, i.e. where any relations, e.g. between and in Example (2) were tagged by the system.", "labels": [], "entities": []}, {"text": "As compared to the human gold standard we obtained an accuracy of 76.31\u00a2 and an inaccuracy of substitutions of 15.32\u00a2 , deletions of 7.11\u00a2 and insertions of 1.26\u00a2 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9996138215065002}, {"text": "insertions", "start_pos": 143, "end_pos": 153, "type": "METRIC", "confidence": 0.9965804219245911}]}], "tableCaptions": []}