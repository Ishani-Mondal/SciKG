{"title": [{"text": "The \"Meaning\" System on the English Allwords Task", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "The \"Meaning\" system has been developed within the framework of the Meaning European research project . It is a combined system, which integrates several supervised machine learning word sense disambiguation modules, and several knowledgebased (unsupervised) modules.", "labels": [], "entities": [{"text": "Meaning European research project", "start_pos": 68, "end_pos": 101, "type": "DATASET", "confidence": 0.7710368633270264}, {"text": "machine learning word sense disambiguation", "start_pos": 165, "end_pos": 207, "type": "TASK", "confidence": 0.6821129381656647}]}, {"text": "See section 2 for details.", "labels": [], "entities": []}, {"text": "The supervised modules have been trained exclusively on the SemCor corpus, while the unsupervised modules use WordNet-based lexico-semantic resources integrated in the Multilingual Central Repository (MCR) of the Meaning project ().", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.6948843449354172}]}, {"text": "The architecture of the system is quite simple.", "labels": [], "entities": []}, {"text": "Raw text is passed through a pipeline of linguistic processors (tokenizers, POS tagging, named entity extraction, and parsing) and then a Feature Extraction module codifies examples with features extracted from the linguistic annotation and MCR.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.752419114112854}, {"text": "named entity extraction", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.6531950831413269}]}, {"text": "The supervised modules have priority over the unsupervised and they are combined using a weighted voting scheme.", "labels": [], "entities": []}, {"text": "For the words lacking training examples, the unsupervised modules are applied in a cascade sorted by decreasing precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.997007429599762}]}, {"text": "The tuning of the combination setting has been performed on the Senseval-2 allwords corpus.", "labels": [], "entities": [{"text": "Senseval-2 allwords corpus", "start_pos": 64, "end_pos": 90, "type": "DATASET", "confidence": 0.8597463766733805}]}, {"text": "Several research groups have been providers of resources and tools, namely: IXA group from the University of the Basque Country, ITC-irst (\"Istituto per la Ricerca Scientifica e Tecnologica\"), University of Sussex (UoS), University of Alicante (UoA), and TALP research center at the Technical University of Catalonia.", "labels": [], "entities": [{"text": "IXA", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.821403980255127}]}, {"text": "The integration was carried out by the TALP group.", "labels": [], "entities": [{"text": "integration", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.975243091583252}, {"text": "TALP group", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7980766892433167}]}], "datasetContent": [{"text": "For simplicity, and also due to time constraints, the supervised modules were trained exclusively on the SemCor-1.6 corpus, intentionally avoiding the use of other sources of potential training examples, e.g, other corpora, WordNet examples and glosses, similar/substitutable examples extracted from the same Semcor-1.6, etc.", "labels": [], "entities": [{"text": "SemCor-1.6 corpus", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.6987461596727371}]}, {"text": "An independent training set was generated for each polysemous word (of a certain part-of-speech) with 10 or more examples in the SemCor-1.6 corpus.", "labels": [], "entities": [{"text": "SemCor-1.6 corpus", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.8196353018283844}]}, {"text": "This makes a total of 2,440 independent learning problems, on which all supervised WSD systems were trained.", "labels": [], "entities": [{"text": "WSD", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9375346302986145}]}, {"text": "The feature representation of the training examples was shared between all learning modules.", "labels": [], "entities": []}, {"text": "It consists of a rich feature representation obtained using the Feature Extraction module of the TALP team in the Senseval-3 English lexical sample task.", "labels": [], "entities": [{"text": "Senseval-3 English lexical sample task", "start_pos": 114, "end_pos": 152, "type": "DATASET", "confidence": 0.633276641368866}]}, {"text": "The feature set includes the classic window-based pattern features extracted from a local context and the \"bag-of-words\" type of features taken from a broader context.", "labels": [], "entities": []}, {"text": "It also contains a set of features representing the syntactic relations involving the target word, and semantic features of the surrounding words extracted from the MCR of the Meaning project.", "labels": [], "entities": [{"text": "MCR of the Meaning project", "start_pos": 165, "end_pos": 191, "type": "DATASET", "confidence": 0.7373068332672119}]}, {"text": "See () for more details on the set of features used.", "labels": [], "entities": []}, {"text": "The validation corpus for these classifiers was the Senseval-2 allwords dataset, which contains 2,473 target word occurrences.", "labels": [], "entities": [{"text": "Senseval-2 allwords dataset", "start_pos": 52, "end_pos": 79, "type": "DATASET", "confidence": 0.8146919210751852}]}, {"text": "From those, 2,239 occurrences correspond to polysemous words.", "labels": [], "entities": []}, {"text": "We will refer to this subcorpus as S2-pol.", "labels": [], "entities": []}, {"text": "Only 1,254 words from S2-pol were actually covered by the classifiers trained on the SemCor-1.6 corpus.", "labels": [], "entities": [{"text": "SemCor-1.6 corpus", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.8515616953372955}]}, {"text": "We will refer to this subset of words as the S2-pol-sup corpus.", "labels": [], "entities": []}, {"text": "The conversion between WordNet-1.6 synsets (SemCor-1.6) and WordNet-1.7 (Senseval-2) was performed on the output of the classifiers by applying an automatically derived mapping provided by TALP 2 . shows the results (precision and coverage) obtained by the individual supervised modules on the S2-pol-sup subcorpus, and by the unsupervised modules on the S2-pol subcorpus (i.e., we exclude from evaluation the monosemous words).", "labels": [], "entities": [{"text": "WordNet-1.7", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9635803699493408}, {"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.998748779296875}]}, {"text": "Support Vector Machines and AdaBoost are the best performing methods, though all of them perform in a small accuracy range from 53.4% to 59.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9986709356307983}]}, {"text": "Regarding the unsupervised methods, DDD is clearly the best performing method, achieving a remarkable precision of 61.9% with the DDD variant, at a cost of a lower coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9988258481025696}, {"text": "coverage", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9674079418182373}]}, {"text": "The DDD\u00a1 \u00a3 \u00a2 appears to be the best system for augmenting the coverage of the former.", "labels": [], "entities": [{"text": "DDD\u00a1 \u00a3 \u00a2", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8792671263217926}]}, {"text": "Note that the autoPS heuristic for ranking senses is a more precise estimator than the WordNet most-frequent-sense (MFS).", "labels": [], "entities": [{"text": "WordNet most-frequent-sense (MFS)", "start_pos": 87, "end_pos": 120, "type": "DATASET", "confidence": 0.714760422706604}]}, {"text": "The Senseval-3 test set contains 2,081 target words, 1,851 of them polysemous.", "labels": [], "entities": [{"text": "Senseval-3 test set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8428529898325602}]}, {"text": "The subset covered by the SemCor-1.6 training contains 1,211 target words (65.42%, compared to the 56.0% of the Senseval-2 corpus).", "labels": [], "entities": [{"text": "Senseval-2 corpus", "start_pos": 112, "end_pos": 129, "type": "DATASET", "confidence": 0.7125935554504395}]}, {"text": "We submitted the outputs of two different configurations of the Meaning system: Meaningc and Meaning-wv.", "labels": [], "entities": []}, {"text": "These systems correspond to Base-3 and W-Vot (in the best configuration) from table 3, respectively.", "labels": [], "entities": []}, {"text": "The results from the official evaluation are given in table 4.", "labels": [], "entities": []}, {"text": "Again, we applied an automatic mapping from WordNet-1.6 to WordNet-1.7.1 synset labels.", "labels": [], "entities": [{"text": "WordNet-1.6", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9794272184371948}, {"text": "WordNet-1.7.1", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9331702589988708}]}, {"text": "However, there are senses in 1.7.1 that do not exist in 1.6, thus our system simply cannot assign them.", "labels": [], "entities": []}, {"text": "It can be observed that, even though on the tuning corpus both variants obtained very similar precision (67.4 and 67.5), on the test set the weighted voting scheme is clearly better than the baseline system, probably due to the robustness achieved by the ensemble.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9943630695343018}]}, {"text": "The performance decrease observed on the test set with respect to the Senseval-2 corpus is very significant ( 5 points).", "labels": [], "entities": [{"text": "Senseval-2 corpus", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.8743916153907776}]}, {"text": "Given that the baseline system performs worse than the voted approach, it seems unlikely that there is overfitting during the ensemble tuning.", "labels": [], "entities": []}, {"text": "However, we plan to repeat the tuning experiments directly on the Senseval-3 corpus to see if the same behavior and conclusions are observed.", "labels": [], "entities": [{"text": "Senseval-3 corpus", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9354290664196014}]}, {"text": "Probably, the decrease in performance is due to the differences between the training and test corpora.", "labels": [], "entities": []}, {"text": "We intend to investigate the differences between SemCor-1.6, Senseval-2, and Senseval-3 corpora at different levels of linguistic information in order to check the appropriateness of using SemCor-1.6 as the main information source.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of individual supervised and unsupervised WSD modules", "labels": [], "entities": [{"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.8642317652702332}]}, {"text": " Table 2: Results on frequency-based word sets", "labels": [], "entities": []}, {"text": " Table 3: Results of the combination of systems", "labels": [], "entities": []}, {"text": " Table 4: Results on the Senseval-3 test corpus", "labels": [], "entities": [{"text": "Senseval-3 test", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.7058482766151428}]}]}