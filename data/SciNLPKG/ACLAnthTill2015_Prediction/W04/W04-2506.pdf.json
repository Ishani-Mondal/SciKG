{"title": [{"text": "A Novel Approach to Focus Identification in Question/Answering Systems", "labels": [], "entities": [{"text": "Focus Identification in Question/Answering", "start_pos": 20, "end_pos": 62, "type": "TASK", "confidence": 0.8873495856920878}]}], "abstractContent": [{"text": "Modern Question/Answering systems rely on expected answer types for processing questions.", "labels": [], "entities": [{"text": "Question/Answering", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.783786416053772}]}, {"text": "The answer type is a semantic category provided by Named Entity recognizer or by semantic hierarchies.", "labels": [], "entities": []}, {"text": "We argue in this paper that Q/A systems should take advantage of the topic information by exploiting several models of question and answer categorization.", "labels": [], "entities": []}, {"text": "The matching of the question category with the answer category allows the system to filter out many incorrect answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "One method of retrieving information from vast document collections is by using textual Question/Answering.", "labels": [], "entities": [{"text": "Question/Answering", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.6321099599202474}]}, {"text": "Q/A is an Information Retrieval (IR) paradigm that returns a shortlist of answers, extracted from relevant documents, to a question formulated in natural language.", "labels": [], "entities": [{"text": "Information Retrieval (IR) paradigm", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8578791519006094}]}, {"text": "Another, different method to find the desired information is by navigating along subject categories assigned hierarchically to groups of documents, in a style made popular by Yahoo.com among others.", "labels": [], "entities": []}, {"text": "When the defined category is reached, documents are inspected and the information is eventually retrieved.", "labels": [], "entities": []}, {"text": "Q/A systems incorporate a paragraph retrieval engine, to find paragraphs that contain candidate answers, as reported in).", "labels": [], "entities": []}, {"text": "To our knowledge no information on the text categories of these paragraphs is currently employed in any of the Q/A systems.", "labels": [], "entities": []}, {"text": "Instead, another semantic information, such as the semantic classes of the expected answers, derived from the question processing, is used to retrieve paragraphs and later to extract answers.", "labels": [], "entities": []}, {"text": "Typically, the semantic classes of answers are organized in hierarchical ontologies and do not relate in anyway to the categories associated with documents.", "labels": [], "entities": []}, {"text": "The ontology of expected answer classes contains concepts like PERSON, LOCATION or PRODUCT, whereas categories associated with documents are more similar to topics than concepts, e.g., acquisitions, trading or earnings.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9808760285377502}, {"text": "LOCATION", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9199277758598328}]}, {"text": "Given that text categories indicate different semantic information than the classes of the expected answers, we argue in this paper that text categories can be used to improve the quality of textual Q/A.", "labels": [], "entities": []}, {"text": "In fact, by assigning text categories to both questions and answers, we have additional information on their similarity, which allows systems to perform a first level of word disambiguation.", "labels": [], "entities": [{"text": "word disambiguation", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.6966336071491241}]}, {"text": "For example, if a user asks about the Apple characteristics, two type of answers maybe retrieved: (a) about the apple company and (b) related to the agricultural domain.", "labels": [], "entities": []}, {"text": "Instead, if the computer subject is selected, only the answers involving the Apple company will be considered.", "labels": [], "entities": []}, {"text": "Thus, topic categories allows Q/A systems to detect the correct focus and consequently filter out many incorrect answers.", "labels": [], "entities": []}, {"text": "In order to assign categories to questions and answers, the set of documents, on which the Q/A systems operate, has to be pre-categorized.", "labels": [], "entities": []}, {"text": "For our experiments we trained our basic Q/A system on the well-known text categorization benchmark, Reuters-21578.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.9024242162704468}]}, {"text": "This allows us to assume as categories of an answer the categories of the documents, which contain such answer.", "labels": [], "entities": []}, {"text": "More difficult, instead, is assigning categories to questions as: (a) they are not known in advance and (b) their reduced size (in term of number of words) often prevents the detection of their categories.", "labels": [], "entities": []}, {"text": "The article is organized as follows: Section 2 describes our Q/A system whereas Section 3 shows the question categorization problem and the solutions adopted.", "labels": [], "entities": []}, {"text": "Section 4 presents the filtering and the re-ranking methods that combine the basic Q/A with the question classification models.", "labels": [], "entities": [{"text": "question classification", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.6908325105905533}]}, {"text": "Section 5 reports the experiments on question categorization, basic Question Answering and Question Answering based on Text Categorization (TC).", "labels": [], "entities": [{"text": "question categorization", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8236356377601624}, {"text": "Question Answering", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7168834209442139}, {"text": "Question Answering based on Text Categorization (TC)", "start_pos": 91, "end_pos": 143, "type": "TASK", "confidence": 0.6879297031296624}]}, {"text": "Finally, Section 6 derives the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of the experiments is to prove that category information used, as described in the previous section, is useful for Q/A systems.", "labels": [], "entities": []}, {"text": "For this purpose we have to show that the performance of a basic Q/A system is improved when the question classification is adopted.", "labels": [], "entities": [{"text": "question classification", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.6809805631637573}]}, {"text": "To implement our Q/A and filtering system we used: (1) A state of the art Q/A system: improving low accurate systems is not enough to prove that TC is useful for Q/A.", "labels": [], "entities": []}, {"text": "The basic Q/A system that we employed is based on the architecture described in , which is the current state-of-the-art.", "labels": [], "entities": []}, {"text": "(2) The Reuters collection of categorized documents on which training our basic Q/A system.", "labels": [], "entities": [{"text": "Reuters collection of categorized documents", "start_pos": 8, "end_pos": 51, "type": "DATASET", "confidence": 0.9180940866470337}]}, {"text": "(3) A set of questions categorized according to the Reuters categories.", "labels": [], "entities": [{"text": "Reuters categories", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.9000674188137054}]}, {"text": "A portion of this set is used to train PRTC and QSVM models, the other disjoint portion is used to measure the performance of the Q/A systems.", "labels": [], "entities": [{"text": "PRTC", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7131730318069458}]}, {"text": "Next section, describes the technique used to produce the question corpus.", "labels": [], "entities": []}, {"text": "To evaluate the impact of our filtering methods on Q/A we first scored the answers of a basic Q/A system for the test set, by using both the MRAR and the SRAR measures.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9090006351470947}, {"text": "SRAR", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.870827317237854}]}, {"text": "Additionally, we evaluated (1) the MRAR when answers were re-ranked based on question and answer category information; and (2) the SRAR in the case when answers extracted from documents with different categories were eliminated.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9939035773277283}, {"text": "SRAR", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9909947514533997}]}, {"text": "Rows 1 and 2 of report the MRAR and SRAR performances of the basic Q/A.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.6268570423126221}, {"text": "SRAR", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6660624146461487}]}, {"text": "Column 2,3,4,5 and 6 show the MRAR and SRAR accuracies (rows 4 and 5) of Q/A systems that eliminate or re-rank the answer by using the RTC0, SVM0, PRTC, QSVM and QATC question categorization models.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.7832493782043457}, {"text": "SRAR accuracies", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.9344252347946167}]}, {"text": "The basic Q/A results show that answering the Reuters based questions is a quite difficult task 9 as the MRAR is .662, about 15 percent points under the best system result obtained in the 2003 TREC competition.", "labels": [], "entities": [{"text": "Q/A", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8188225626945496}, {"text": "Reuters based questions", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.8503254055976868}, {"text": "MRAR", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9559372067451477}]}, {"text": "Note that the basic Q/A system, employed in these experiments, uses the same techniques adopted by the best figure Q/A system of TREC 2003.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.8538865745067596}]}, {"text": "The quality of the Q/A results is strongly affected by the question classification accuracy.", "labels": [], "entities": [{"text": "Q/A", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7717844645182291}, {"text": "question classification", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.681956022977829}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.7813171744346619}]}, {"text": "In fact, RTC0 and QATC that have the lowest classification f 1 (see) produce very low MRAR (i.e. .622% and .607%) and SRAR (i.e. -..", "labels": [], "entities": [{"text": "RTC0", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.7157392501831055}, {"text": "QATC", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.7612786889076233}, {"text": "MRAR", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9976921081542969}, {"text": "SRAR", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9954259991645813}]}, {"text": "When the best question classification model QSVM is used, the basic Q/A performance improves with respect to both the MRAR (66.35% vs 66.19%) and the SRAR (-.077% vs -.372%) scores.", "labels": [], "entities": [{"text": "Q/A", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.6845477024714152}, {"text": "MRAR", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9887591600418091}, {"text": "SRAR", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.8449907898902893}]}, {"text": "In order to study how the number of answers impacts the accuracy of the proposed models, we have evaluated the MRAR and the SRAR score varying the maximum number of answers, provided by the basic Q/A system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9982485771179199}, {"text": "MRAR", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9957838654518127}, {"text": "SRAR score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9652893245220184}]}, {"text": "We adopted as filtering policy the answer re-ranking.", "labels": [], "entities": [{"text": "answer re-ranking", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7708076238632202}]}, {"text": "shows that as the number of answers increases the MRAR score for QSVM, PRTC and the basic Q/A in-   creases, for the first four answers and it reaches a plateau afterwards.", "labels": [], "entities": [{"text": "MRAR score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9856681525707245}, {"text": "QSVM", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.6093944907188416}, {"text": "PRTC", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.7436781525611877}, {"text": "Q/A in-   creases", "start_pos": 90, "end_pos": 107, "type": "METRIC", "confidence": 0.9001183807849884}]}, {"text": "We also notice that the QSVM outperforms both PRTC and the basic Q/A.", "labels": [], "entities": [{"text": "PRTC", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.843086838722229}]}, {"text": "This figure also shows that question categorization per se does not greatly impact the MRAR score of Q/A. illustrates the SRAR curves by considering the answer elimination policy.", "labels": [], "entities": [{"text": "question categorization", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.780678391456604}, {"text": "MRAR score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9717618525028229}, {"text": "Q/A.", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.8549793163935343}, {"text": "SRAR", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.8110140562057495}, {"text": "answer elimination", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.8284282088279724}]}, {"text": "The figure clearly shows that the QSVM and PRTC models for question categorization determine a higher SRAR score, thus indicating that fewer irrelevant answers are left.", "labels": [], "entities": [{"text": "QSVM", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8567246198654175}, {"text": "PRTC", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8047832250595093}, {"text": "question categorization", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.798004150390625}, {"text": "SRAR score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.976098358631134}]}, {"text": "shows that question categorization can greatly improve the quality of Q/A when irrelevant answers are considered.", "labels": [], "entities": []}, {"text": "It also shows that perhaps, when evaluating Q/A systems with the MRAR scoring method, the \"optimistic\" view of Q/A is taken, in which erroneous results are ignored for the sake of emphasizing that an answer was obtained after all, even if it was ranked below several incorrect answers.", "labels": [], "entities": [{"text": "MRAR scoring", "start_pos": 65, "end_pos": 77, "type": "METRIC", "confidence": 0.7837546169757843}]}, {"text": "In contrast, the SRAR score that we have described in Section 5.2 produce a \"harsher\" score, in which errors are given the same weight as the correct results, but affect negatively the overall score.", "labels": [], "entities": [{"text": "SRAR score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.8221073746681213}, {"text": "Section 5.2", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.8953275084495544}, {"text": "harsher\" score", "start_pos": 77, "end_pos": 91, "type": "METRIC", "confidence": 0.8382832805315653}]}, {"text": "This explains why, even fora baseline Q/A, we obtained a negative score, as illustrated in.", "labels": [], "entities": []}, {"text": "This shows that the Q/A system generates more erroneous answers then correct answers.", "labels": [], "entities": []}, {"text": "If only the MRAR scores would be considered we may assess that TC does not bring significant information to Q/A for precision enhancement by re-ranking answers.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9747402667999268}, {"text": "TC", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.6354784369468689}, {"text": "Q/A", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.8885095715522766}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9961850047111511}]}, {"text": "However, the results obtained with the SRAR scoring scheme, indicate that text categorization impacts on Q/A results, by eliminating incorrect answers.", "labels": [], "entities": [{"text": "SRAR", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.5415180325508118}, {"text": "Q/A", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.727082371711731}]}, {"text": "We plan to further study the question categorization methods and empirically find which weighting scheme is ideal.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: f1 performances of question categorization.", "labels": [], "entities": []}, {"text": " Table 5: Performance comparisons between basic Q/A and", "labels": [], "entities": []}]}