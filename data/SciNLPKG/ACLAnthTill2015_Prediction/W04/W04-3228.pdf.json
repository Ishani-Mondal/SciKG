{"title": [{"text": "Dependencies vs. Constituents for Tree-Based Alignment", "labels": [], "entities": []}], "abstractContent": [{"text": "Given a parallel parsed corpus, statistical tree-to-tree alignment attempts to match nodes in the syntactic trees fora given sentence in two languages.", "labels": [], "entities": [{"text": "statistical tree-to-tree alignment", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.6219713787237803}]}, {"text": "We train a probabilistic tree transduction model on a large automatically parsed Chinese-English corpus, and evaluate results against human-annotated word level alignments.", "labels": [], "entities": []}, {"text": "We find that a constituent-based model performs better than a similar probability model trained on the same trees converted to a dependency representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical approaches to machine translation, pioneered by, estimate parameters fora probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7396603226661682}]}, {"text": "In recent years, a number of syntactically motivated approaches to statistical machine translation have been proposed.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7569789389769236}]}, {"text": "These approaches assign a parallel tree structure to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure.", "labels": [], "entities": []}, {"text": "The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.", "labels": [], "entities": []}, {"text": "Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given anew sentence.", "labels": [], "entities": []}, {"text": "modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.", "labels": [], "entities": []}, {"text": "The trees of Wu's Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure.", "labels": [], "entities": [{"text": "Inversion Transduction Grammar", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.7785034775733948}]}, {"text": "While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees.", "labels": [], "entities": []}, {"text": "present an algorithm for estimating probabilistic parameters fora similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.", "labels": [], "entities": []}, {"text": "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in, but the specific bracketing of the parse tree provided.", "labels": [], "entities": []}, {"text": "Recent models of alignment have attempted to exploit syntactic information from both languages by aligning a pair of parse trees for the same sentence in either language node by node.", "labels": [], "entities": []}, {"text": "Eisner (2003) presented such a system for transforming semantic-level dependecy trees into syntactic-level dependency trees for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.7616311609745026}]}, {"text": "Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments.", "labels": [], "entities": [{"text": "Korean-English Treebank", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.7518341839313507}]}, {"text": "align parallel dependency trees with a divide and conquer strategy, choosing a highly likely word-pair as a splitting point in each tree.", "labels": [], "entities": []}, {"text": "In addition to providing a deeper level of representation for the transformations of the translation model to work with, tree-to-tree models have the advantage that they are much less computationally costly to train than models which must induce tree structure on one or both sides of the translation pair.", "labels": [], "entities": []}, {"text": "Because Expectation Maximization for tree-to-tree models iterates over pairs of nodes in the two trees, it is O(n 2 ) in the sentence length, rather than O(n 6 ) for Wu's Inversion Transduction Grammar or O(n 4 ) for the Yamada and Knight tree-to-string model.", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.7491131126880646}, {"text": "O", "start_pos": 110, "end_pos": 111, "type": "METRIC", "confidence": 0.984377920627594}, {"text": "O", "start_pos": 154, "end_pos": 155, "type": "METRIC", "confidence": 0.9787559509277344}]}, {"text": "In this paper, we make a comparison of two treeto-tree models, one trained on the trees produced by automatic parsers for both our English and Chinese corpora, and one trained on the same parser output converted to a dependency representation.", "labels": [], "entities": []}, {"text": "The trees are converted using a set of deterministic head rules for each language.", "labels": [], "entities": []}, {"text": "The dependency representation equalizes some differences in the annotation style between the English and Chinese treebanks.", "labels": [], "entities": []}, {"text": "However, the dependency representation makes the assumption that not only the bracketing structure, but also the headword choices, will correspond in the two trees.", "labels": [], "entities": []}, {"text": "Our evaluation is in terms of agreement with word-level alignments created by bilingual human annotators.", "labels": [], "entities": []}, {"text": "Our model of alignment is that of, reviewed in Section 2 and extended to dependency trees in Section 3.", "labels": [], "entities": [{"text": "alignment", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.9532864689826965}]}, {"text": "We describe our data and experiments in Section 4, and discuss results in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained our translation models on a parallel corpus of Chinese-English newswire text.", "labels": [], "entities": []}, {"text": "We re-, and English data was parsed using Collins (1999).", "labels": [], "entities": [{"text": "Collins (1999)", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9101875722408295}]}, {"text": "Our hand-aligned test data were those used in, and consisted of 48 sentence pairs also with less than 25 words in either language, fora total of 788 English words and 580 Chinese words.", "labels": [], "entities": []}, {"text": "The hand aligned data consisted of 745 individual aligned word pairs.", "labels": [], "entities": []}, {"text": "Words could be aligned one-to-many in either direction.", "labels": [], "entities": []}, {"text": "This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used.", "labels": [], "entities": []}, {"text": "A separate set of 49 hand-aligned sentence pairs was used to control overfitting in training our models.", "labels": [], "entities": []}, {"text": "We evaluate our translation models in terms of agreement with human-annotated word-level alignments between the sentence pairs.", "labels": [], "entities": []}, {"text": "For scoring the viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of, which measures agreement at the level of pairs of words: 1 where A is the set of word pairs aligned by the automatic system, and G the set aligned in the gold standard.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 104, "end_pos": 130, "type": "METRIC", "confidence": 0.9152400294939677}]}, {"text": "For a better understanding of how the models 1 While Och and Ney (2000) differentiate between sure and possible hand-annotated alignments, our gold standard alignments come in only one variety.", "labels": [], "entities": []}, {"text": "differ, we break this figure down into precision: and recall: Since none of the systems presented in this comparison make use of hand-aligned data, they may differ in the overall proportion of words that are aligned, rather than inserted or deleted.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9993699193000793}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9996417760848999}]}, {"text": "This affects the precision/recall tradeoff; better results with respect to human alignments maybe possible by adjusting an overall insertion probability in order to optimize AER.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9994056224822998}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9777848720550537}, {"text": "insertion probability", "start_pos": 131, "end_pos": 152, "type": "METRIC", "confidence": 0.9674377739429474}, {"text": "AER", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.9835467338562012}]}, {"text": "provides a comparison of results using the tree-based models with the word-level IBM models.", "labels": [], "entities": []}, {"text": "IBM Models 1 and 4 refer to.", "labels": [], "entities": [{"text": "IBM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9115791916847229}]}, {"text": "We used the GIZA++ package, including the HMM model of.", "labels": [], "entities": []}, {"text": "We trained each model until AER began to increase on our held-out cross validation data, resulting in running Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations (the optimal number of iterations for Models 2 and 3 was zero).", "labels": [], "entities": [{"text": "AER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9665993452072144}]}, {"text": "\"Constituent Tree-to-Tree\" indicates the model of Section 2 trained and tested directly on the trees output by the parser, while \"Dependency Tree-to-Tree\" makes the modifications to the model described in Section 3.", "labels": [], "entities": []}, {"text": "For reasons of computational efficiency, our constituent-based training procedure skipped sentences for which either tree had anode with more than five children, and the dependency-based training skipped trees with more than six children.", "labels": [], "entities": []}, {"text": "Thus, the tree-based models were effectively trained on less data than IBM Model 4: 11,422 out of 18,773 sentence pairs for the constituent model and 10,662 sentence pairs for the dependency model.", "labels": [], "entities": []}, {"text": "Our tree-based models were initialized with lexical translation probabilities trained using IBM Model 1, and uniform probabilities for the tree reordering operations.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9142986337343851}]}, {"text": "The models were trained until AER began to rise on our held-out cross-validation data, though in practice AER was nearly constant for both tree-based models after the first iteration.", "labels": [], "entities": [{"text": "AER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9986925721168518}, {"text": "AER", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9996117949485779}]}], "tableCaptions": [{"text": " Table 2: Alignment results on Chinese-English corpus. Higher precision and recall correspond to lower  alignment error rate.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.999235987663269}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.999489426612854}, {"text": "alignment error rate", "start_pos": 104, "end_pos": 124, "type": "METRIC", "confidence": 0.9035873214403788}]}]}