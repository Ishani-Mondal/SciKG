{"title": [], "abstractContent": [{"text": "Answer validation is a component of question answering system, which selects reliable answer from answer candidates extracted by certain methods.", "labels": [], "entities": [{"text": "Answer validation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9161639511585236}, {"text": "question answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8293288052082062}]}, {"text": "In this paper, we propose an approach of answer validation based on the strengths of lexical association between the keywords extracted from a question sentence and each answer candidate.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.8804503083229065}]}, {"text": "The proposed answer validation process is decomposed into two steps: the first is to extract appropriate keywords from a question sentence using word features and the strength of lexical association, while the second is to estimate the strength of the association between the keywords and an answer candidate based on the hits of search engines.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8153274655342102}]}, {"text": "In the result of experimental evaluation, we show that a good proportion (79%) of a multiple-choice quiz \"Who wants to be a millionaire\" can be solved by the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "The technology of searching for the answer of a question written in natural language is called \"Question Answering\"(QA), and has gotten a lot of attention recently.", "labels": [], "entities": [{"text": "searching for the answer of a question written in natural language", "start_pos": 18, "end_pos": 84, "type": "TASK", "confidence": 0.7790808027440851}, {"text": "Question Answering\"(QA)", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.8626384973526001}]}, {"text": "Research activities of QA have been promoted through competitions such as TREC QA Track) and NTCIR QAC ().", "labels": [], "entities": [{"text": "TREC QA Track)", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.6892013847827911}, {"text": "NTCIR QAC", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.8272959887981415}]}, {"text": "Question answering systems can be decomposed into two steps: first step is to collect answer candidates, while the second is to validate each of those candidates.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8913445770740509}]}, {"text": "The first step of collecting answer candidates has been well studied so far.", "labels": [], "entities": []}, {"text": "Its standard technology is as follows: first, the answer type of a question, such as LOCATION or PERSON, is identified.", "labels": [], "entities": [{"text": "LOCATION", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9359936118125916}, {"text": "PERSON", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.8579626679420471}]}, {"text": "Then, the documents which may contain answer candidates are retrieved by querying available document set with queries generated from the question sentence.", "labels": [], "entities": []}, {"text": "Finally, named entities which match the answer type of the question sentence are collected from the retrieved documents as answer candidates.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the second step of how to validate an answer candidate.", "labels": [], "entities": []}, {"text": "Several answer validation methods have been proposed.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8601833879947662}]}, {"text": "One of the well-known approaches is that based on deep understanding of text (e.g.).", "labels": [], "entities": []}, {"text": "In the approach of answer validation based on deep understanding, first a question and the paragraph including an answer candidate are parsed and transformed into logical forms.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.806187629699707}]}, {"text": "Second, the validity of the answer candidate is examined through logical inference.", "labels": [], "entities": []}, {"text": "One drawback of this approach is that it requires a rich set of lexical knowledge such as WordNet and world knowledge such as the inference rule set.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9744207262992859}]}, {"text": "Consequently, this approach is computationally expensive.", "labels": [], "entities": []}, {"text": "In contrast, in this paper, we propose another approach of answer validation, which is purely based on the estimation of the strengths of lexical association between the keywords extracted from a question sentence and each answer candidate.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8723970949649811}]}, {"text": "One underlying motivation of this paper is to examine the effectiveness of quite low level semantic operation such as measuring lexical association against knowledge rich NLP tasks such as answer validation of question answering.", "labels": [], "entities": [{"text": "answer validation of question answering", "start_pos": 189, "end_pos": 228, "type": "TASK", "confidence": 0.8664554953575134}]}, {"text": "Surprisingly, as we show later, given multiple-choices as answer candidates of a question, a good proportion of a certain set of questions can be solved by our method based on lexical association.", "labels": [], "entities": []}, {"text": "In our framework of answer validation by keyword association (in the remaining of this paper, we call the notion of the lexical association introduced above as \"keyword association\"), the answer validation process is decomposed into two steps: the first step is to extract appropriate keywords from a question sentence, while the second step is to estimate the strength of the association between the keywords and an answer candidate.", "labels": [], "entities": [{"text": "answer validation by keyword association", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.8090081214904785}, {"text": "answer validation", "start_pos": 188, "end_pos": 205, "type": "TASK", "confidence": 0.777660459280014}]}, {"text": "We propose two methods for the keyword selection step: one is by a small number of hand-crafted rules for determining word weights based on word features, while the other is based on search engine hits.", "labels": [], "entities": [{"text": "keyword selection step", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.8446847597757975}]}, {"text": "In the second step of how to validate an answer candidate, the web is used as a knowledge base for estimating the strength of the association between the extracted keywords and an answer candidate.", "labels": [], "entities": []}, {"text": "Its basic idea is as follows: the stronger the association between the keywords and an answer candidate, the more frequently they co-occur on the web.", "labels": [], "entities": []}, {"text": "In this paper, we introduce several measures for estimating the strength of the association, and show their effectiveness through experimental evaluation.", "labels": [], "entities": []}, {"text": "In this paper, in order to concentrate on the issue of answer validation, but not the whole QA processes, we use an existing multiple-choice quiz as the material for our study.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8514043986797333}]}, {"text": "The multiplechoice quiz we used is taken from \"Who wants to be a millionaire\".", "labels": [], "entities": []}, {"text": "\"Who wants to be a millionaire\" is a famous TV show, which originated in the United Kingdom and has been localized in more than fifty countries.", "labels": [], "entities": []}, {"text": "We used the Japanese version, which is produced by Fuji Television Network, Inc..", "labels": [], "entities": [{"text": "Fuji Television Network", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.9667380253473917}]}, {"text": "In the experimental evaluation, about 80% of the questions of this quiz can be solved by the proposed method of answer validation by keyword association.", "labels": [], "entities": []}, {"text": "Section 2 introduces the idea of question answering by keyword association.", "labels": [], "entities": [{"text": "question answering by keyword association", "start_pos": 33, "end_pos": 74, "type": "TASK", "confidence": 0.7629551291465759}]}, {"text": "Section 3 describes how to select keywords from a question sentence.", "labels": [], "entities": []}, {"text": "Section 4 describes how to select the answer of multiple-choice questions.", "labels": [], "entities": []}, {"text": "Section 5 describes how to integrate the procedures of keyword selection and answer selection.", "labels": [], "entities": [{"text": "keyword selection", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8395446240901947}, {"text": "answer selection", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.9084897041320801}]}, {"text": "Section 6 presents the results of experimental evaluations.", "labels": [], "entities": []}, {"text": "Section 7 compares our work with several related works Section 8 presents our conclusion and future works.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Hits of Keywords and the Choices for the Question Q3", "labels": [], "entities": []}, {"text": " Table 4: Rules for Word Weights", "labels": [], "entities": [{"text": "Word Weights", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.8187417984008789}]}, {"text": " Table 5: Evaluation of Keyword Association  Ratios (precision/coverage)(%)", "labels": [], "entities": [{"text": "precision/coverage)(%)", "start_pos": 53, "end_pos": 75, "type": "METRIC", "confidence": 0.8114708364009857}]}, {"text": " Table 6: Evaluation of Keyword Association  Ratio: BA ratio of F A max and second-max", "labels": [], "entities": [{"text": "BA ratio", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9874337911605835}, {"text": "F A max", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9374993244806925}]}, {"text": " Table 7: Precision of Answer Selection (with  keyword selection by word weights)", "labels": [], "entities": [{"text": "Precision of Answer Selection", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7322845011949539}]}, {"text": " Table 8: Evaluation of Each Answer Selection  Rule (with keyword selection by word weights)", "labels": [], "entities": [{"text": "Evaluation of Each Answer Selection", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.5127676427364349}]}, {"text": " Table 10: Total Evaluation Results (preci- sion/coverage)(%)", "labels": [], "entities": [{"text": "preci- sion/coverage)(%)", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.6267059048016866}]}]}