{"title": [{"text": "OntoSem Methods for Processing Semantic Ellipsis", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes various types of semantic ellipsis and underspecification in natural language , and the ways in which the meaning of semantically elided elements is reconstructed in the Ontological Semantics (OntoSem) text processing environment.", "labels": [], "entities": []}, {"text": "The description covers phenomena whose treatment in OntoSem has reached various levels of advancement: fully implemented, partially implemented, and described algorithmically outside of implementation.", "labels": [], "entities": []}, {"text": "We present these research results at this point-prior to full implementation and extensive evaluation-for two reasons: first, new descriptive material is being reported; second, some subclasses of the phenomena in question will require a truly long-term effort whose results are best reported in installments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic ellipsis -the non-expression of syntactically obligatory elements -has been widely studied in computational (not to mention other branches of) linguistics, largely because accounting for missing syntactic elements is a crucial aspect of achieving a full parse, and parsing is required for many approaches to NLP.", "labels": [], "entities": [{"text": "Syntactic ellipsis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8647516965866089}]}, {"text": "Much less attention has been devoted to what we will call semantic ellipsis, or the non-expression of elements that, while not syntactically obligatory, are required fora full semantic interpretation of a text.", "labels": [], "entities": []}, {"text": "Naturally, semantic ellipsis is important only in truly knowledge-rich ap-1 Examples of NLP efforts to resolve syntactic ellipsis include and Lappin 1992, among many others.", "labels": [], "entities": []}, {"text": "Some of the types of semantic underspecification treated here are described in the literature (e.g., in theoretical terms, not as heuristic algorithms.", "labels": [], "entities": []}, {"text": "This is due, in large part, to alack of knowledge sources for semantic reasoning in those contributions.", "labels": [], "entities": [{"text": "semantic reasoning", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8676451444625854}]}, {"text": "proaches to NLP, which few current non-toy systems pursue.", "labels": [], "entities": []}, {"text": "All definitions of ellipsis derive from a stated or implied notion of completeness.", "labels": [], "entities": []}, {"text": "Taking, again, the example of syntactic ellipsis, this means that obligatory verbal arguments must be overt, auxiliary verbs must have complements, etc.", "labels": [], "entities": []}, {"text": "-all of which is defined in lexico-grammatical terms.", "labels": [], "entities": []}, {"text": "But even if a text is devoid of syntactic gaps, much remains below the surface, easily interpretable by people but not directly observable.", "labels": [], "entities": []}, {"text": "Typical examples of semantically underspecified elements are pronouns and indexicals (e.g., here, now, yesterday), whose real-world anchors must be clarified in a fully developed semantic representation (i.e., yesterday has a concrete meaning only if one knows when today is).", "labels": [], "entities": []}, {"text": "Pronouns and indexicals, though often difficult to resolve, have one advantage over the cases to be discussed here: the trigger that further semantic specification need be carried out is the word itself, and the inventory of such words is well known.", "labels": [], "entities": []}, {"text": "By contrast, the semantically underspecified cases in the following examples are more subtle: (1) After boosting employment the past few years, Aluminum Co. of America won't be doing any hiring this fall beyond replacing those who leave.", "labels": [], "entities": []}, {"text": "(2) Mitchell said he planned to work late tonight to complete the legislation.", "labels": [], "entities": []}, {"text": "(3) Civilians invited into the prison by the administration to help keep the peace were unable to stanch the bloodshed.", "labels": [], "entities": []}, {"text": "The categories of semantic ellipsis illustrated by these examples can be described as follows.", "labels": [], "entities": []}, {"text": "shows reference resolution that relies on the reconstruction of a semantically elided category: i.e., to understand who those refers to, one must understand that the implicit object of hire is 'employees', and that the elided head of the NP with those as its determiner also refers to employees (albeit a different real-world set of employees).", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.782494068145752}]}, {"text": "(2) illustrates semantic event ellipsis in configurations containing modal/aspectual + OBJECT: i.e., the meaning of complete the legislation is actually complete writing the legislation.", "labels": [], "entities": [{"text": "OBJECT", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.970325231552124}]}, {"text": "(3) illustrates lexical patterns with predictable event ellipsis: e.g., invite <person> to <loca-tion> means 'invite someone to come/go to the location.'", "labels": [], "entities": []}, {"text": "These examples, which illustrate the types of semantic ellipsis to be discussed below, require special treatment in our ontological semantic (OntoSem) text processing system, since its goal is to automatically produce fully specified semantic representations of unrestricted text that can then be used in a wide variety of applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "In response to the current evaluation standards in NLP (which are more suited to and informative for stochastic-based systems than knowledge-based ones), we have recently developed a novel evaluation methodology that assigns scores as well as blame for errors to various aspects of the TMRs generated during OntoSem text processing.", "labels": [], "entities": [{"text": "OntoSem text processing", "start_pos": 308, "end_pos": 331, "type": "TASK", "confidence": 0.6794743537902832}]}, {"text": "While percentage scores for correct vs. incorrect results can provide a general evaluation of system function, it is blame assignment that drives development.", "labels": [], "entities": []}, {"text": "Blame assignment is determined by processing each sentence multiple times: first without manual intervention, then with the correction of preprocessor errors, then with the correction of syntax errors.", "labels": [], "entities": [{"text": "Blame assignment", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8782196938991547}]}, {"text": "The rationale behind these loops of correction and reevaluation is that \"low level\" mistakes like preprocessor errors or lack of coverage of some syntactic construction require different development action than more weighty (from our point of view) errors in semantic interpretation that might result from gaps in knowledge, insufficient reasoning engines, etc.", "labels": [], "entities": []}, {"text": "The first experiment with our new evaluation regime produced the following results (reported on in detail in ): the analyzer was shown to carryout word sense disambiguation at over 90% and semantic dependency determination at 87% on the basis of correct syntactic analysis and on sentences of an average length of over 25 words with 1.33 unknown words on average per input sentence.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 147, "end_pos": 172, "type": "TASK", "confidence": 0.669743001461029}, {"text": "semantic dependency determination", "start_pos": 189, "end_pos": 222, "type": "TASK", "confidence": 0.6094129880269369}]}, {"text": "Outstanding errors in semantic analysis were due, inmost cases, to non-literal use of language (which is one of our topics of ongoing investigation).", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9193613529205322}]}, {"text": "Although this first formal experiment was limited to WSD and semantic dependencies, testing of other modules -like those for reference resolution and ellipsis -will soon be added to the formal evaluation regime.", "labels": [], "entities": [{"text": "WSD", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.7604053616523743}, {"text": "reference resolution", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.7696593701839447}]}, {"text": "At this stage, evaluation work is slow, but we are well into the development of an evaluation and correction environment that promises to significantly speedup both evaluation and system enhancement.", "labels": [], "entities": [{"text": "evaluation and correction", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6710955202579498}]}], "tableCaptions": []}