{"title": [{"text": "Statistical Significance Tests for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Statistical Significance", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6466806977987289}, {"text": "Machine Translation Evaluation", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.8993760148684183}]}], "abstractContent": [{"text": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality?", "labels": [], "entities": []}, {"text": "To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9587207734584808}]}, {"text": "Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, the field of machine translation has been changed by the emergence both of effective statistical methods to automatically train machine translation systems from translated text sources (so-called parallel corpora) and of reliable automatic evaluation methods.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7958759963512421}]}, {"text": "Machine translation systems can now be built and evaluated from black box tools and parallel corpora, with no human involvement at all.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7948642373085022}]}, {"text": "The evaluation of machine translation systems has changed dramatically in the last few years.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7757570743560791}]}, {"text": "Instead of reporting human judgment of translation quality, researchers now rely on automatic measures, most notably the BLEU score, which measures n-gram overlap with reference translations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9862363934516907}]}, {"text": "Since it has been shown that the BLEU score correlates with human judgment, an improvement in BLEU is taken as evidence for improvement in translation quality.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9809080362319946}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9987547397613525}]}, {"text": "Building the tools for any translation system involves many iterations of changes and performance testing.", "labels": [], "entities": []}, {"text": "It is important to have a method at hand that gives us assurances that the observed increase in the test score on a test set reflects true improvement in system quality.", "labels": [], "entities": []}, {"text": "In other words, we need to be able to gauge, if the increase in score is statistically significant.", "labels": [], "entities": []}, {"text": "Since complex metrics such as BLEU do not lend themselves to an analytical technique for assessing statistical significance, we propose bootstrap resampling methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.996532678604126}, {"text": "bootstrap resampling", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.6690373122692108}]}, {"text": "We also provide empirical evidence that the estimated significance levels are accurate by comparing different systems on a large number of test sets of various sizes.", "labels": [], "entities": []}, {"text": "In this paper, after providing some background, we will examine some properties of the widely used BLEU metric, discuss experimental design, introduce bootstrap resampling methods for statistical significance estimation and report on experimental results that demonstrate the accuracy of the methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9912458062171936}, {"text": "statistical significance estimation", "start_pos": 184, "end_pos": 219, "type": "TASK", "confidence": 0.7226883172988892}, {"text": "accuracy", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.9964035749435425}]}], "datasetContent": [{"text": "To adequately evaluate the quality of any translation is difficult, since it is not entirely clear what the focus of the evaluation should be.", "labels": [], "entities": []}, {"text": "Surely, a good translation has to adequately capture the meaning of the foreign original.", "labels": [], "entities": []}, {"text": "However, pinning down all the nuances is hard, and often differences in emphasis are introduced based on the interpretation of the translator.", "labels": [], "entities": []}, {"text": "At the same time, it is desirable to have fluent output that can be read easily.", "labels": [], "entities": []}, {"text": "These two goals, adequacy and fluency, are the main criteria in machine translation evaluation.: Translation quality of three systems, measured by the BLEU score and n-gram precision Human judges maybe asked to evaluate the adequacy and fluency of translation output, but this is a laborious and expensive task.", "labels": [], "entities": [{"text": "machine translation evaluation.", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.8807210127512614}, {"text": "Translation", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.9534772634506226}, {"text": "BLEU score", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9773095846176147}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.7928207516670227}]}, {"text": "Papineni et al. addressed the evaluation problem by introducing an automatic scoring metric, called BLEU, which allowed the automatic calculation of translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9983310103416443}]}, {"text": "The system output is compared against a reference translation of the same source text.", "labels": [], "entities": []}, {"text": "In this section, we describe the experimental framework of our work.", "labels": [], "entities": []}, {"text": "We also report on a number of  We introduced two methods using bootstrap resampling.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.726603239774704}]}, {"text": "One method estimates bounds for the true performance level of a system.", "labels": [], "entities": []}, {"text": "The other method, paired bootstrap resampling, estimates how confidently we can draw the conclusion from a test result that one system outperforms another.", "labels": [], "entities": []}, {"text": "We would now like to provide experimental evidence that these estimates are indeed correct at the specified level of statistical significance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Translation quality of three systems, mea- sured by the BLEU score and n-gram precision", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9699178338050842}, {"text": "mea- sured", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9123981992403666}, {"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9814566075801849}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.8246460556983948}]}, {"text": " Table 1. Six different sys- tems are compared here (we will get later into the  nature of these systems). While the unigram pre- cision of the three systems hovers around 60%, the  difference in 4-gram precision is much larger. The  Finnish system has only roughly half (7.8%) of the  4-gram precision of the Spanish system (14.7%).  This is the cause for the relative large distance in  overall BLEU (28.9% vs. 20.2%)", "labels": [], "entities": [{"text": "precision", "start_pos": 203, "end_pos": 212, "type": "METRIC", "confidence": 0.9288075566291809}, {"text": "BLEU", "start_pos": 397, "end_pos": 401, "type": "METRIC", "confidence": 0.9994094371795654}]}, {"text": " Table 4: Validation of the statistical significance es- timations: Number of conclusions drawn at a certain  level and accuracy of the conclusions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9994858503341675}]}, {"text": " Table 3: The table displays how often a conclusion with 95% statistical significance is made for different  system comparisons and different sample sizes. 12%/1% means 12% correct and 1% wrong conclusions.  30,000 test sentences are divided into 300, 100, 50, and 10 samples, each the size of 100, 300, 600, and  3000 sentences respectively.", "labels": [], "entities": []}]}