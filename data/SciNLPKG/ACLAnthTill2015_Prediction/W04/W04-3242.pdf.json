{"title": [], "abstractContent": [{"text": "In this paper, we explore the use of Random Forests (RFs) (Amit and Geman, 1997; Breiman, 2001) in language modeling, the problem of predicting the next word based on words already seen before.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7709933817386627}, {"text": "predicting the next word", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.8631144613027573}]}, {"text": "The goal in this work is to develop anew language mod-eling approach based on randomly grown Decision Trees (DTs) and apply it to automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 130, "end_pos": 158, "type": "TASK", "confidence": 0.5936163564523061}]}, {"text": "We study our RF approach in the context of \u00a2-gram type language modeling.", "labels": [], "entities": [{"text": "RF", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9614203572273254}, {"text": "\u00a2-gram type language modeling", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.5422104239463806}]}, {"text": "Unlike regular \u00a2-gram language models, RF language models have the potential to generalize well to unseen data, even when a complicated history is used.", "labels": [], "entities": []}, {"text": "We show that our RF language models are superior to regular \u00a2-gram language models in reducing both the per-plexity (PPL) and word error rate (WER) in a large vocabulary speech recognition system.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 126, "end_pos": 147, "type": "METRIC", "confidence": 0.8873671690622965}]}], "introductionContent": [{"text": "In many systems dealing with natural speech or language, such as Automatic Speech Recognition and Statistical Machine Translation, a language model is a crucial component for searching in the often prohibitively large hypothesis space.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.6218076447645823}, {"text": "Statistical Machine Translation", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.680908590555191}]}, {"text": "Most state-ofthe-art systems use \u00a2 -gram language models, which are simple and effective most of the time.", "labels": [], "entities": []}, {"text": "Many smoothing techniques that improve language model probability estimation have been proposed and studied in the \u00a2 -gram literature.", "labels": [], "entities": [{"text": "language model probability estimation", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.6817195564508438}]}, {"text": "There has also been work in exploring Decision Tree (DT) language models (, which attempt to cluster similar histories together to achieve better probability estimation.", "labels": [], "entities": []}, {"text": "However, the results were not promising ( The aim of DT language models is to alleviate the data sparseness problem encountered in \u00a2 -gram language models.", "labels": [], "entities": []}, {"text": "However, the cause of the negative results is exactly the same: data sparseness, coupled with the fact that the DT construction algorithms decide on tree splits solely on the basis of seen data.", "labels": [], "entities": [{"text": "DT construction", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.9184174239635468}]}, {"text": "Although various smoothing techniques were studied in the context of DT language models, none of them resulted in significant improvements over \u00a2 -gram models.", "labels": [], "entities": []}, {"text": "Recently, a neural network based language modeling approach has been applied to trigram language models to deal with the curse of dimensionality ().", "labels": [], "entities": []}, {"text": "Significant improvements in both perplexity (PPL) and word error rate (WER) over backoff smoothing were reported after interpolating the neural network models with the baseline backoff models.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 54, "end_pos": 75, "type": "METRIC", "confidence": 0.915013829867045}]}, {"text": "However, the neural network models rely on interpolation with \u00a2 -gram models, and use \u00a2 -gram models exclusively for low frequency words.", "labels": [], "entities": []}, {"text": "We believe improvements in \u00a2 -gram models should also improve the performance of neural network models.", "labels": [], "entities": []}, {"text": "We propose anew Random Forest (RF) approach for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8128733038902283}]}, {"text": "The idea of using RFs for language modeling comes from the recent success of RFs in classification and regression.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.744711697101593}]}, {"text": "By definition, RFs are collections of Decision Trees (DTs) that have been constructed randomly.", "labels": [], "entities": []}, {"text": "Therefore, we also propose anew DT language model which can be randomized to construct RFs efficiently.", "labels": [], "entities": []}, {"text": "Once constructed, the RFs function as a randomized history clustering which can help in dealing with the data sparseness problem.", "labels": [], "entities": []}, {"text": "Although they do not perform well on unseen test data individually, the collective contribution of all DTs makes the RFs generalize well to unseen data.", "labels": [], "entities": []}, {"text": "We show that our RF approach for \u00a2 -gram language modeling can result in a significant improvement in both PPL and WER in a large vocabulary speech recognition system.", "labels": [], "entities": [{"text": "\u00a2 -gram language modeling", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6161303699016571}, {"text": "WER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9934245347976685}, {"text": "large vocabulary speech recognition", "start_pos": 124, "end_pos": 159, "type": "TASK", "confidence": 0.675002709031105}]}, {"text": "The paper is organized as follows: In Section 2, we review the basics about language modeling and smoothing.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7685364484786987}]}, {"text": "In Section 3, we briefly review DT based language models and describe our new DT and RF approach for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7257652133703232}]}, {"text": "In Section 4, we show the performance of our RF based language models as measured by both PPL and WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.7304198741912842}]}, {"text": "After some discussion and analysis, we finally summarize our work and propose some future directions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will first show the performance of our RF language models as measured by PPL.", "labels": [], "entities": []}, {"text": "After analyzing these results, we will present the performance when the RF language models are used in a large vocabulary speech recognition system.", "labels": [], "entities": [{"text": "large vocabulary speech recognition", "start_pos": 105, "end_pos": 140, "type": "TASK", "confidence": 0.7343400716781616}]}], "tableCaptions": [{"text": " Table 2: Interpolating KN-trigram with", "labels": [], "entities": []}, {"text": " Table 3: PPL of seen and unseen test events", "labels": [], "entities": [{"text": "PPL", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7445317506790161}]}, {"text": " Table 4: Test events analyzed by number of times  seen in 100 DTs", "labels": [], "entities": [{"text": "DTs", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.642628014087677}]}]}