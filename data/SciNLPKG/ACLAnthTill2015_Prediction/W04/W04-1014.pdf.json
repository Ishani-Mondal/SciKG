{"title": [{"text": "Evaluation Measures Considering Sentence Concatenation for Automatic Summarization by Sentence or Word Extraction", "labels": [], "entities": [{"text": "Automatic Summarization by Sentence or Word Extraction", "start_pos": 59, "end_pos": 113, "type": "TASK", "confidence": 0.7148293341909137}]}], "abstractContent": [{"text": "Automatic summaries of text generated through sentence or word extraction has been evaluated by comparing them with manual summaries generated by humans by using numerical evaluation measures based on precision or accuracy.", "labels": [], "entities": [{"text": "summaries of text generated through sentence or word extraction", "start_pos": 10, "end_pos": 73, "type": "TASK", "confidence": 0.7914775146378411}, {"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9971803426742554}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.8426644206047058}]}, {"text": "Although sentence extraction has previously been evaluated based only on precision of a single sentence, sentence concate-nations in the summaries should be evaluated as well.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8141284883022308}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9976019263267517}]}, {"text": "We have evaluated the appropriateness of sentence concatenations in summaries by using evaluation measures used for evaluating word concate-nations in summaries through word extraction.", "labels": [], "entities": [{"text": "word extraction", "start_pos": 169, "end_pos": 184, "type": "TASK", "confidence": 0.7460487186908722}]}, {"text": "We determined that measures considering sentence con-catenation much better reflect the human judgment rather than those based only on the precision of a single sentence.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9943795800209045}]}], "introductionContent": [], "datasetContent": [{"text": "Metrics that can be used to accurately evaluate the various appropriateness to summarization are needed.The simplest and probably the ideal way of evaluating automatic summarization is to have human subjects read the summaries and evaluate them in terms of the appropriateness of summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.981209933757782}]}, {"text": "However, this type of evaluation is too expensive for comparing the efficiencies of many different approaches precisely and repeatedly.", "labels": [], "entities": []}, {"text": "We thus need automatic evaluation metrics to numerically validate the efficiency of various approaches repeatedly and consistently.", "labels": [], "entities": []}, {"text": "Automatic summaries can be evaluated by comparing them with manual summaries generated by humans.", "labels": [], "entities": [{"text": "summaries", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.8533211946487427}]}, {"text": "The similarities between the targets and the automatically processed results provide metrics indicating the extent to which the task was accomplished.", "labels": [], "entities": []}, {"text": "The similarity that can better reflect subjective judgments is a better metric.", "labels": [], "entities": []}, {"text": "To create correct answers for automatic summarization, humans generate manual summaries through sentence or word extraction.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9127867817878723}, {"text": "word extraction", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.6816898286342621}]}, {"text": "However, references consisting of manual summaries vary among humans.", "labels": [], "entities": []}, {"text": "The problems in validating automatic summaries by comparing them with various references are as follows: \u2022 correct answers for automatic results cannot be unified because of subjective variation, \u2022 the coverage of correct answers in the collected manual summaries is unknown, and \u2022 the reliability of references in the collected manual summaries is not always guaranteed.", "labels": [], "entities": [{"text": "validating automatic summaries", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.7095107436180115}, {"text": "reliability", "start_pos": 286, "end_pos": 297, "type": "METRIC", "confidence": 0.979965090751648}]}, {"text": "When the similarity between automatic results and references is used for the evaluation metrics, the similarity determination function counts overlapping of each component or sequence of components in the automatic results.", "labels": [], "entities": [{"text": "similarity determination function", "start_pos": 101, "end_pos": 134, "type": "METRIC", "confidence": 0.9338905811309814}]}, {"text": "If concatenations between components in a summary had no meaning, the overlap of a single component between the automatic results and the references can represent the extent of summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.9640107154846191}]}, {"text": "However, concatenations between sentences or words have meanings, so some concatenations of sentences or words in the automatic summaries sometimes generate meanings different from the original.", "labels": [], "entities": []}, {"text": "The evaluation metrics for summarization should thus consider each concatenation between components in the automatic results.", "labels": [], "entities": [{"text": "summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9846451878547668}]}, {"text": "To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU () for machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.7000194191932678}, {"text": "word string precision", "start_pos": 213, "end_pos": 234, "type": "METRIC", "confidence": 0.4929945270220439}, {"text": "summarization", "start_pos": 240, "end_pos": 253, "type": "TASK", "confidence": 0.9848623275756836}, {"text": "word extraction", "start_pos": 262, "end_pos": 277, "type": "TASK", "confidence": 0.7327836453914642}, {"text": "ROUGE", "start_pos": 279, "end_pos": 284, "type": "METRIC", "confidence": 0.9931172132492065}, {"text": "BLEU", "start_pos": 325, "end_pos": 329, "type": "METRIC", "confidence": 0.9988390803337097}, {"text": "machine translation", "start_pos": 337, "end_pos": 356, "type": "TASK", "confidence": 0.7595954239368439}]}, {"text": "Evaluation metrics based on word accuracy, summarization accuracy (SumACCY), using a word network made by merging manual summaries has been proposed).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8062864542007446}, {"text": "summarization accuracy (SumACCY)", "start_pos": 43, "end_pos": 75, "type": "METRIC", "confidence": 0.6961272835731507}]}, {"text": "In addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (WSumACCY) in which SumACCY is weighted by the majority of the humans' selections, has been proposed).", "labels": [], "entities": [{"text": "coverage of correct answers", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.8383393883705139}, {"text": "reliability", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.961132287979126}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.8756169080734253}]}, {"text": "In contrast, summarization through sentence extraction has been evaluated using only single sentence precision.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9931879043579102}, {"text": "sentence extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7310127913951874}]}, {"text": "Sentence extraction should also be evaluated using measures that take into account sentence concatenations, the coverage of correct answers, and the reliability of manual summaries.", "labels": [], "entities": [{"text": "Sentence extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9695420265197754}, {"text": "reliability", "start_pos": 149, "end_pos": 160, "type": "METRIC", "confidence": 0.9752124547958374}]}, {"text": "This paper presents evaluation results of automatic summarization through sentence or word extraction using the above mentioned metrics based on n-gram precision and sentence/word accuracy and examines how well these measures reflect the judgments of humans as well.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8634637594223022}, {"text": "sentence or word extraction", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.6089745461940765}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.783805251121521}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9275627136230469}]}, {"text": "In summarization through sentence or word extraction under a specific summarization ratio, the order of the sentences or words and the length of the summaries are restricted by the original documents or sentences.", "labels": [], "entities": [{"text": "summarization", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9887324571609497}, {"text": "sentence or word extraction", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.5892610549926758}]}, {"text": "Metrics based on the accuracy of the components in the summary is a straight-forward approach to measuring similarities between the target and automatic summaries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9979991316795349}]}, {"text": "Newspaper articles and broadcast news speech were automatically summarized through sentence extraction and word extraction respectively under the given summarization ratio, which is the ratio of the numbers of sentences or words in the summary to that in the original.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7047479599714279}, {"text": "word extraction", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.72837895154953}]}, {"text": "The automatic summarization results were subjectively evaluated by ten human subjects.", "labels": [], "entities": [{"text": "summarization", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.9208321571350098}]}, {"text": "The subjects read these summaries and rated each one from 1 (incorrect) to 5 (perfect).", "labels": [], "entities": []}, {"text": "The automatic summaries were also evaluated by using the numerical metrics SumACCY, WSumACCY, NrstACCY, and n-gram precision (1 \u2264 n \u2264 5) in comparison with reference summaries generated by humans.", "labels": [], "entities": [{"text": "summaries", "start_pos": 14, "end_pos": 23, "type": "TASK", "confidence": 0.9517483115196228}, {"text": "WSumACCY", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.7408379912376404}, {"text": "NrstACCY", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9746252298355103}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.7795625925064087}]}, {"text": "The precisions of 1-gram, . .", "labels": [], "entities": [{"text": "precisions", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9987284541130066}]}, {"text": "., 5-gram are denoted PREC1, . .", "labels": [], "entities": [{"text": "PREC1", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.597831666469574}]}, {"text": "The numerical evaluation results were averaged over the number of automatic summaries.", "labels": [], "entities": []}, {"text": "Note that the subjects who judged the automatic summaries did not include anyone who generated the references.", "labels": [], "entities": []}, {"text": "To examine the similarity of the human judgments and that of the manual summaries, the kappa statistics, \u03ba, was calculated using eq.", "labels": [], "entities": []}, {"text": "(A-1) in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.7232462763786316}]}, {"text": "Finally, to examine how much the evaluation measures reflected the human judgment, the correlation coefficients between the human judgments and the numerical evaluation results were calculated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Kappa statistics for manual summaries and  human judgments for sentence extraction.", "labels": [], "entities": [{"text": "summaries", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.810260534286499}, {"text": "sentence extraction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7718518376350403}]}, {"text": " Table 3: Kappa statistics for manual summaries and  human judgments for word extraction", "labels": [], "entities": [{"text": "summaries", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.7942937016487122}, {"text": "word extraction", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.7847903370857239}]}]}