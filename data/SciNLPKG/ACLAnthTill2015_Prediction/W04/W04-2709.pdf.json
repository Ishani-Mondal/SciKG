{"title": [{"text": "Interlingual Annotation of Multilingual Text Corpora", "labels": [], "entities": [{"text": "Interlingual Annotation of Multilingual Text Corpora", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.69246906042099}]}], "abstractContent": [{"text": "This paper describes a multi-site project to annotate six sizable bilingual parallel corpora for interlingual content.", "labels": [], "entities": []}, {"text": "After presenting the background and objectives of the effort, we describe the data set that is being annotated, the interlingua representation language used, an interface environment that supports the annotation task and the annotation process itself.", "labels": [], "entities": []}, {"text": "We will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issues which have arisen.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes a multi-site National Science Foundation project focusing on the annotation of six sizable bilingual parallel corpora for interlingual content with the goal of providing a significant data set for improving knowledge-based approaches to machine translation (MT) and a range of other Natural Language Processing (NLP) applications.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 258, "end_pos": 282, "type": "TASK", "confidence": 0.8715690612792969}]}, {"text": "The project participants include the Computing Research Laboratory at NMSU, the Language Technologies Institute at CMU, the Information Science Institute at USC, UMIACS at the University of Maryland, the MITRE Corporation and Columbia University.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we first present the background and objectives of the project.", "labels": [], "entities": []}, {"text": "We then describe the data set that is being annotated, the interlingual representation language being used, an interface environment that is designed to support the annotation task, and the process of annotation itself.", "labels": [], "entities": []}, {"text": "We will then outline a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a set of issues that have arisen since the project began.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation criteria and metrics continue to evolve and are in the early stages of formation and implementation.", "labels": [], "entities": []}, {"text": "Several possible courses for evaluating the annotations and resulting structures exist.", "labels": [], "entities": []}, {"text": "In the first of these, the annotations are measured according to interannotator agreement.", "labels": [], "entities": []}, {"text": "For this purpose, data is collected reflecting the annotations selected, the Omega nodes selected and the theta roles assigned.", "labels": [], "entities": []}, {"text": "Then, inter-coder agreement is measured by a straightforward match, with agreement calculated by a Kappa measure and a Wood standard similarity ).", "labels": [], "entities": [{"text": "agreement", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9924933314323425}]}, {"text": "This is done for three agreement points: annotations, Omega selection and theta role selection.", "labels": [], "entities": [{"text": "theta role selection", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.5647400319576263}]}, {"text": "At this time, the Kappa statistic's expected agreement is defined as 1/(N+1) where N is the number of choices at a given data point.", "labels": [], "entities": []}, {"text": "In the case of Omega nodes, this means the number of matched Omega nodes (by string match) plus one for the possibility of the annotator traversing up or down the hierarchy.", "labels": [], "entities": []}, {"text": "Multiple measures are used because it is important to have a mechanism for evaluating inter-coder consistency in the use of the IL representation language which does not depend on the assumption that there is a single correct annotation of a given text.", "labels": [], "entities": []}, {"text": "The tools for evaluation have been modified from pervious use ).", "labels": [], "entities": []}, {"text": "Second, the accuracy of the annotation is measured.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995982050895691}]}, {"text": "Here accuracy is defined as correspondence to a predefined baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9993557333946228}]}, {"text": "In the initial development phase, all sites annotated the same texts and many of the variations were discussed at that time, permitting the development of a baseline annotation.", "labels": [], "entities": []}, {"text": "While not a useful long-term strategy, this produced a consensus baseline for the purpose of measuring the annotators' task and the solidity of the annotation standard.", "labels": [], "entities": []}, {"text": "The final measurement technique derives from the ultimate goal of using the IL representation for MT, therefore, we are measuring the ability to generate accurate surface texts from the IL representation as annotated.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9671815037727356}]}, {"text": "At this stage, we are using an available generator, Halogen.", "labels": [], "entities": []}, {"text": "A tool to convert the representation to meet Halogen requirements is being built.", "labels": [], "entities": []}, {"text": "Following the conversion, surface forms will be generated and then compared with the originals through a variety of standard MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9709983468055725}]}], "tableCaptions": []}