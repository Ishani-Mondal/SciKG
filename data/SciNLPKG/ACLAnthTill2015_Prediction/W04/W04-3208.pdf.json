{"title": [{"text": "Mining Very-Non-Parallel Corpora: Parallel Sentence and Lexicon Extraction via Bootstrapping and EM", "labels": [], "entities": [{"text": "Lexicon Extraction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7002334296703339}, {"text": "EM", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.3644629418849945}]}], "abstractContent": [{"text": "We present a method capable of extracting parallel sentences from far more disparate \"very-non-parallel corpora\" than previous \"comparable corpora\" methods, by exploiting bootstrapping on top of IBM Model 4 EM.", "labels": [], "entities": [{"text": "IBM Model 4 EM", "start_pos": 195, "end_pos": 209, "type": "DATASET", "confidence": 0.8727173805236816}]}, {"text": "Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents.", "labels": [], "entities": []}, {"text": "But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of \"find-one-get-more\", which claims that documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity.", "labels": [], "entities": []}, {"text": "We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence.", "labels": [], "entities": []}, {"text": "This novel \"find-one-get-more\" principle allows us to add more parallel sentences from dissimilar documents, to the baseline set.", "labels": [], "entities": []}, {"text": "Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration.", "labels": [], "entities": []}, {"text": "We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.", "labels": [], "entities": [{"text": "IBM Model 4 EM lexical learner", "start_pos": 77, "end_pos": 107, "type": "DATASET", "confidence": 0.8466532031695048}]}, {"text": "Parallel sentence and lexicon extraction via Bootstrapping and EM The most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data.", "labels": [], "entities": [{"text": "Parallel sentence and lexicon extraction", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5862140893936157}]}, {"text": "Recent work (Munteanu et al., 2004, Zhao and Vogel, 2002) on extracting parallel sentences from comparable data, and others on extracting paraphrasing sentences from monolingual corpora (Barzilay and Elhadad 2003) are based on the \"find-topic-extract-sentence\" principle which claims that parallel sentences only exist in document pairs with high similarity.", "labels": [], "entities": []}, {"text": "They all use lexical information (e.g. word overlap, cosine similarity) to match documents first, before extracting sentences from these documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel sentences are important resources for training and improving statistical machine translation and cross-lingual information retrieval systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7253892918427786}, {"text": "cross-lingual information retrieval", "start_pos": 106, "end_pos": 141, "type": "TASK", "confidence": 0.6243138015270233}]}, {"text": "Various methods have been previously proposed to extract parallel sentences from multilingual corpora.", "labels": [], "entities": []}, {"text": "Some of them are described in detail in.", "labels": [], "entities": []}, {"text": "The challenge of these tasks varies by the degree of parallel-ness of the input multilingual documents.", "labels": [], "entities": []}, {"text": "However, the non-parallel corpora used so far in the previous work tend to be quite comparable.", "labels": [], "entities": []}, {"text": "used a corpus of Chinese and English versions of news stories from the Xinhua News agency, with \"roughly similar sentence order of content\".", "labels": [], "entities": [{"text": "Xinhua News agency", "start_pos": 71, "end_pos": 89, "type": "DATASET", "confidence": 0.9109845161437988}]}, {"text": "This corpus can be more accurately described as noisy parallel corpus.", "labels": [], "entities": []}, {"text": "mined paraphrasing sentences from weather reports.", "labels": [], "entities": []}, {"text": "used news articles published within the same 5-day window.", "labels": [], "entities": []}, {"text": "All these corpora have documents in the same, matching topics.", "labels": [], "entities": []}, {"text": "They can be described as on-topic documents.", "labels": [], "entities": []}, {"text": "In fact, both and assume similar sentence orders and applied dynamic programming in their work.", "labels": [], "entities": []}, {"text": "In our work, we try to find parallel sentences from far more disparate, very-non-parallel corpora than in any previous work.", "labels": [], "entities": []}, {"text": "Since many more multilingual texts available today contain documents that do not have matching documents in the other language, we propose finding more parallel sentences from off-topic documents, as well as on-topic documents.", "labels": [], "entities": []}, {"text": "An example is the TDT corpus, which is an aggregation of multiple news sources from different time periods.", "labels": [], "entities": [{"text": "TDT corpus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.8142227828502655}]}, {"text": "We suggest the \"find-one-get-more\" principle, which claims that as long as two documents are found to contain one pair of parallel sentence, they must contain others as well.", "labels": [], "entities": []}, {"text": "Based on this principle, we propose an effective Bootstrapping method to accomplish our task ().", "labels": [], "entities": []}, {"text": "We also apply the IBM Model 4 EM lexical learning to find unknown word translations from the extracted parallel sentences from our system.", "labels": [], "entities": [{"text": "IBM Model 4 EM lexical learning", "start_pos": 18, "end_pos": 49, "type": "DATASET", "confidence": 0.7790458897749583}]}, {"text": "The IBM models are commonly used for word alignment in statistical MT systems.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.8274831771850586}, {"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.7308167815208435}]}, {"text": "This EM method differs from some previous work, which used a seed-word lexicon to extract new word translations or word senses from comparable corpora ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our algorithm on a very-non-parallel corpus of TDT3 data, which contains various news stories transcription of radio broadcasting or TV news report from 1998-2000 in English and Chinese Channels.", "labels": [], "entities": [{"text": "TDT3 data", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.9741775095462799}]}, {"text": "We compare the results of our proposed method against a baseline method that is based on the conventional, \"find-topic-extract-sentence\" principle only.", "labels": [], "entities": []}, {"text": "We investigate the performance of the IBM Model 4 EM lexical learner on data from very-non-parallel corpus, and evaluate how our method can boost its performance.", "labels": [], "entities": [{"text": "IBM Model 4 EM lexical learner", "start_pos": 38, "end_pos": 68, "type": "DATASET", "confidence": 0.7174306611220042}]}, {"text": "The results are described in the following sub-sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Bilingual lexical matching scores of  different corpora", "labels": [], "entities": [{"text": "Bilingual lexical matching", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8101774056752523}]}]}