{"title": [{"text": "Automatic Evaluation of Summaries Using Document Graphs", "labels": [], "entities": [{"text": "Automatic Evaluation of Summaries", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6451984345912933}]}], "abstractContent": [{"text": "Summarization evaluation has been always a challenge to researchers in the document summariza-tion field.", "labels": [], "entities": [{"text": "Summarization evaluation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9755918979644775}]}, {"text": "Usually, human involvement is necessary to evaluate the quality of a summary.", "labels": [], "entities": []}, {"text": "Here we present anew method for automatic evaluation of text summaries by using document graphs.", "labels": [], "entities": [{"text": "automatic evaluation of text summaries", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.6244249224662781}]}, {"text": "Data from Document Understanding Conference 2002 (DUC-2002) has been used in the experiment.", "labels": [], "entities": [{"text": "Document Understanding Conference 2002 (DUC-2002)", "start_pos": 10, "end_pos": 59, "type": "TASK", "confidence": 0.742507917540414}]}, {"text": "We propose measuring the similarity between two summaries or between a summary and a document based on the concepts/entities and relations between them in the text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document summarization has been the focus of many researchers for the last decade, due to the increase in on-line information and the need to find the most important information in a (set of) document(s).", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9041363894939423}]}, {"text": "One of the biggest challenges in text summarization research is how to evaluate the quality of a summary or the performance of a summarization tool.", "labels": [], "entities": [{"text": "text summarization research", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.8197282751401266}]}, {"text": "There are different approaches to evaluate overall quality of a summarization system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9598692059516907}]}, {"text": "In general, there are two types of evaluation categories: intrinsic and extrinsic.", "labels": [], "entities": []}, {"text": "Extrinsic approaches measure the quality of a summary based on how it affects certain tasks.", "labels": [], "entities": []}, {"text": "In intrinsic approaches, the quality of the summarization is evaluated based on analysis of the content of a summary itself.", "labels": [], "entities": []}, {"text": "In both categories human involvement is used to judge the summarization outputs.", "labels": [], "entities": [{"text": "summarization", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9788484573364258}]}, {"text": "The problem with having humans involved in evaluating summaries is that we cannot hire human judges every time we want to evaluate summaries (.", "labels": [], "entities": [{"text": "summaries", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.7474984526634216}]}, {"text": "In this paper, we discuss anew automated way to evaluate machinegenerated summaries without the need to have human judges being involved which decreases the cost of determining which summarization system is best.", "labels": [], "entities": []}, {"text": "In our experiment, we used data from Document Understanding Conference 2002.", "labels": [], "entities": [{"text": "Document Understanding Conference 2002", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.6689181476831436}]}], "datasetContent": [{"text": "A total of 10 different automatic summarization systems submitted their summaries to DUC.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9552318453788757}, {"text": "DUC", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9635024666786194}]}, {"text": "We obtained a ranking order of these 10 systems based on sentence precision/recall by comparing the machine generated extracts to the human generated model summaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.8788893818855286}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9529423117637634}]}, {"text": "The F-factor is calculated from the following equation ( ) ( where P is the precision and R is the recall.", "labels": [], "entities": [{"text": "F-factor", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9990684390068054}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9995044469833374}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9995450377464294}]}, {"text": "We think this ranking order gives us some idea on how human judges think about the performance of different systems.", "labels": [], "entities": []}, {"text": "For our evaluation based on DGs, we also calculated F-factors based on precision and recall, where P = Sim(DG 1 , DG 2 ) and R = Sim(DG 2 , DG 1 ).", "labels": [], "entities": [{"text": "F-factors", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9973854422569275}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9994712471961975}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9991043210029602}]}, {"text": "In the first experiment, we ranked the 10 automatic summarization systems by comparing DGs generated from their outputs to the DGs generated from model summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8813680410385132}]}, {"text": "In this case, DG 1 is the machine generated extract and DG 2 is the human generated extract.", "labels": [], "entities": []}, {"text": "In the second experiment, we ranked the systems by comparing machine generated extracts to the original documents.", "labels": [], "entities": []}, {"text": "In this case, DG 1 is an extract and DG 2 is the corresponding original document.", "labels": [], "entities": []}, {"text": "Since the extracts were generated from multi-document sets, we used the average of the F-factors for ranking purposes.", "labels": [], "entities": [{"text": "F-factors", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.951708197593689}]}], "tableCaptions": [{"text": " Table 1: Model Summaries vs. machine- generated summaries. Ranking results for 200  words extracts", "labels": [], "entities": []}, {"text": " Table 5: Average F-factors for the model sum- maries and machine-generated summaries.", "labels": [], "entities": [{"text": "F-factors", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8312975168228149}]}, {"text": " Table 4: Machine-generated summaries vs.  source documents. Ranking results for 400  words extracts", "labels": [], "entities": []}]}