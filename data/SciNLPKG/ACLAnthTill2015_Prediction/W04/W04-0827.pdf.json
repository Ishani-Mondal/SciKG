{"title": [{"text": "GAMBL, Genetic Algorithm Optimization of Memory-Based WSD", "labels": [], "entities": [{"text": "GAMBL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8878921866416931}, {"text": "WSD", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.5197662115097046}]}], "abstractContent": [{"text": "GAMBL is a word expert approach to WSD in which each word expert is trained using memory-based learning.", "labels": [], "entities": [{"text": "GAMBL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8237298130989075}, {"text": "WSD", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9868844747543335}]}, {"text": "Joint feature selection and algorithm parameter optimization are achieved with a genetic algorithm (GA).", "labels": [], "entities": [{"text": "feature selection", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7755439281463623}, {"text": "algorithm parameter optimization", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.6304927865664164}]}, {"text": "We use a cascaded classi-fier approach in which the GA optimizes local context features and the output of a separate keyword classifier (rather than also optimizing the keyword features together with the local context features).", "labels": [], "entities": []}, {"text": "A further innovation on earlier versions of memory-based WSD is the use of grammatical relation and chunk features.", "labels": [], "entities": []}, {"text": "This paper presents the architecture of the system briefly, and discusses its performance on the English lexical sample and all words tasks in SENSEVAL-3. 1 Memory-Based WSD We interpret WSD as a classification task distributed over word experts: given an ambiguous word and its context as input features, a classifier specialized on that word assigns the contextually appropriate sense to it.", "labels": [], "entities": []}, {"text": "For each word-lemma-POS-tag combination , a separate classifier is trained.", "labels": [], "entities": []}, {"text": "Information about the words immediately surrounding the ambiguous word (the local context), as well as information about sense-related words in a wider context (keywords) are provided as information sources, coded in a feature vector.", "labels": [], "entities": []}, {"text": "To train the word experts , memory-based learning (MBL) is used, an instance of the lazy learning paradigm: all contexts in which an ambiguous word occurs in the training text are kept in memory and abstraction only occurs at classification time by extrapolating a class from the most similar item(s) in memory to the new test item.", "labels": [], "entities": []}, {"text": "This contrasts with eager learning methods such as decision lists which abstract from the training data at training time and forget about the examples themselves.", "labels": [], "entities": []}, {"text": "For our experiments, we use the MBL algorithms implemented in TIMBL 1.", "labels": [], "entities": [{"text": "TIMBL 1", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.8764799237251282}]}, {"text": "This software 1 We used TIMBL version 5.0.0, which is available from http://ilk.kub.nl Figure 1: An overview of our architecture for word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.7584397792816162}]}, {"text": "YES NO is sent as a feature to the second classifier CLASSIFIER 2 based on binary keywords local context and prediction CLASSIFIER 1 binary representation of context TEXT WORD EXPERT MODULE ASSIGN SENSE linguistic preprocessing lookup lexicon sense above threshold?", "labels": [], "entities": [{"text": "YES", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9785127639770508}, {"text": "NO", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.593047022819519}, {"text": "ASSIGN SENSE linguistic preprocessing lookup lexicon", "start_pos": 190, "end_pos": 242, "type": "TASK", "confidence": 0.7935305138429006}]}, {"text": "YES NO prediction of the first classifier keywords above threshold parameter optimization and feature selection with genetic algorithm heuristic optimization parameter than one sense?", "labels": [], "entities": [{"text": "YES", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9787599444389343}, {"text": "feature selection", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.70457723736763}]}, {"text": "more allows a choice between different statistical and information-theoretic feature and value weighting methods, different neighborhood size and weighting parameters, etc., that should be optimized for each word expert independently.", "labels": [], "entities": []}, {"text": "See (Daelemans et al., 2003b) for more information.", "labels": [], "entities": []}, {"text": "It has been claimed, e.g. in (Daelemans et al., 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods.", "labels": [], "entities": []}, {"text": "Previous work on memory-based WSD includes work from Ng and Lee (1996), Veen-stra et al.", "labels": [], "entities": [{"text": "WSD", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8149096965789795}]}, {"text": "(2000), Hoste et al.", "labels": [], "entities": []}, {"text": "(2002) and Mihalcea (2002).", "labels": [], "entities": [{"text": "Mihalcea (2002)", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.6971985101699829}]}, {"text": "The current design of our WSD system is largely based on Hoste et al.", "labels": [], "entities": [{"text": "WSD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9062950611114502}]}, {"text": "Figure 1 gives an overview of the design of our WSD system: the training text is first linguistically analyzed.", "labels": [], "entities": [{"text": "WSD", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9559288024902344}]}, {"text": "For each word-lemma-POS-tag combination , we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has a frequency in the training text above a certain threshold.", "labels": [], "entities": []}, {"text": "For all combinations matching these three conditions, we train a word expert module.", "labels": [], "entities": []}, {"text": "To all combinations with only one sense, or with more senses and a frequency below the threshold, we assign the default sense, which is respectively the only or most frequent sense in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 184, "end_pos": 191, "type": "DATASET", "confidence": 0.9773383140563965}]}, {"text": "The word expert module consists of two cascaded memory-based classifiers: the sense predicted by", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "presents the results of our WSD system for each word in the LS task, and our overall score (the opt column).", "labels": [], "entities": [{"text": "WSD", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.7907349467277527}]}, {"text": "We included the results of TIMBL with default settings (the def column) and the score of a statistical baseline (the maj column), which assigns the sense  with the highest frequency in the training set to the test instances.", "labels": [], "entities": [{"text": "TIMBL", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.44139793515205383}]}, {"text": "For comparison, we also list ten-fold cross-validation results (with default and optimized settings) of the second classifier on the training set.", "labels": [], "entities": []}, {"text": "Looking at the overall score, we see that TIMBL with default settings already outperforms the baseline with 5%, and that the TIMBL classifier optimized with the GA, improves our score even more with another 7%.", "labels": [], "entities": []}, {"text": "For most words, the improvement after optimization with the genetic algorithm on the training set, also holds on the test set, though for 15 words, the optimal setting from the GA does not result in a better score than the default score.", "labels": [], "entities": []}, {"text": "For four words, TIMBL and the GA cannot outperform the majority sense baseline.", "labels": [], "entities": [{"text": "TIMBL", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9773243069648743}]}, {"text": "We do not yet know what causes TIMBL and the GA to perform badly, but a difference between the sense distributions in the training and test set might be a factor.", "labels": [], "entities": [{"text": "TIMBL", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.8907746076583862}]}, {"text": "The distribution of the majority sense in the training set of source is 48.4%, while in the test set this distribution increases to 62.6%.", "labels": [], "entities": []}, {"text": "For important there is a similar increase: from 38.9% to 47.4%.", "labels": [], "entities": []}, {"text": "However, sense distribution differences in training and test set cannot be the only cause, because for activate and lose there is no such difference between the sense distributions.", "labels": [], "entities": []}, {"text": "Finally, depicts the fine-grained classification accuracies of our system per POS in the LS task, again compared with the accuracies of the majority sense baseline and TIMBL with default settings.", "labels": [], "entities": [{"text": "TIMBL", "start_pos": 168, "end_pos": 173, "type": "METRIC", "confidence": 0.7093698382377625}]}, {"text": "The classification accuracy for nouns and verbs is more or less the same as the overall score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9503198266029358}]}, {"text": "Adjectives, however, seem to be the harder to classify for our system: the classification accuracy is 13% lower than the overall score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9757248163223267}]}, {"text": "This could be related to the on average higher number of senses for the adjectives.", "labels": [], "entities": []}, {"text": "The last column of Table 3 presents our results on the AW test set: the results of the classifier optimized with the GA are compared with the results of TIMBL with default settings, and with a majority sense baseline, which  predicts for each word to be sense-tagged the sense that is listed in WordNet as the most frequent one.", "labels": [], "entities": [{"text": "AW test set", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9061814745267233}, {"text": "WordNet", "start_pos": 295, "end_pos": 302, "type": "DATASET", "confidence": 0.9541082978248596}]}, {"text": "The first half of the table lists the results when we only take into account words for which a word expert is built.", "labels": [], "entities": []}, {"text": "TIMBL with default settings cannot outperform the already strong baseline, but after optimization with the GA, we see a 4% improvement.", "labels": [], "entities": [{"text": "TIMBL", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7794169187545776}]}, {"text": "Unfortunately, this increase is not as high as the performance boost we see in the ten-fold crossvalidation results on the training set, listed in the first column of: there is a large increase of 12% after the optimization step.", "labels": [], "entities": []}, {"text": "Words for which no word expert is built are tagged with their majority sense from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9813909530639648}]}, {"text": "When we also take these words into account, we see similar results: again, default TIMBL cannot outperform the baseline, but GA optimization gives a 3% increase.", "labels": [], "entities": [{"text": "TIMBL", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.968461275100708}]}], "tableCaptions": [{"text": " Table 2: Classification accuracy per POS in the En- glish lexical sample task.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8991701006889343}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9312165379524231}]}, {"text": " Table 3: Classification accuracy in the English all  words task.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9617898464202881}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.972751796245575}]}, {"text": " Table 1: Classification accuracies for all lemmas in the English lexical sample task.", "labels": [], "entities": []}, {"text": " Table 4: The GA's selection of the different types of  features in percentages.", "labels": [], "entities": [{"text": "GA", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.6780487895011902}]}]}