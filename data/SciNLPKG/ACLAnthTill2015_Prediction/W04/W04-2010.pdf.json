{"title": [{"text": "Robust Ending Guessing Rules with Application to Slavonic Languages", "labels": [], "entities": [{"text": "Robust Ending Guessing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8780651092529297}]}], "abstractContent": [{"text": "The paper studies the automatic extraction of diagnostic word endings for Slavonic languages aimed to determine some grammatical, morphological and semantic properties of the underlying word.", "labels": [], "entities": [{"text": "automatic extraction of diagnostic word endings", "start_pos": 22, "end_pos": 69, "type": "TASK", "confidence": 0.8342569271723429}]}, {"text": "In particular, ending guessing rules are being learned from a large morphological dictionary of Bulgarian in order to predict POS, gender, number, article and semantics.", "labels": [], "entities": [{"text": "ending guessing", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7606419920921326}, {"text": "POS", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.7981246709823608}]}, {"text": "A simple exact high accuracy algorithm is developed and compared to an approximate one, which uses a scoring function previously proposed by Mikheev for POS guessing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.7649590969085693}, {"text": "POS guessing", "start_pos": 153, "end_pos": 165, "type": "TASK", "confidence": 0.9105754494667053}]}, {"text": "It is shown how the number of rules of the latter can be reduced by a factor of up to 35, without sacrificing performance.", "labels": [], "entities": []}, {"text": "The evaluation demonstrates coverage close to 100%, and precision of 97-99% for the approximate algorithm.", "labels": [], "entities": [{"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9968432188034058}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9997509121894836}]}], "introductionContent": [{"text": "An important property of the Slavonic languages is the rich morphology, which determines the specifics of their representation and processing in NLP applications.", "labels": [], "entities": []}, {"text": "This variety is arranged not only linearly along the paradigmatic axe, i.e. abundance of wordforms fora given lemma (up to 52 forms for the Bulgarian verb), but also in the derivational tree (up to 30 members per word formation).", "labels": [], "entities": []}, {"text": "The grammatical system of the Slavonic languages and their descriptions differentiate these two mechanisms as word formation and word derivation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.7346600443124771}, {"text": "word derivation", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.7029253244400024}]}, {"text": "The word formation building blocks define the so called inflectional classes, which represent sequential letter strings associated with word classes as well as with individual words, also known as isuffixes in Porter-like stemmers.", "labels": [], "entities": []}, {"text": "The derivational building blocks represent derivational suffixes listed in grammars (d-suffixes in Porterlike stemmers).", "labels": [], "entities": []}, {"text": "A considerable part of the Slavonic d-suffixes change not only the part of speech (POS) but also the semantics of the newly formed word.", "labels": [], "entities": []}, {"text": "When multiple d-suffixes are concatenated, the word formation chain yields also a semantic derivation.", "labels": [], "entities": [{"text": "word formation chain", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.760564128557841}]}, {"text": "For example, the chain (observe ; observer ; observing ; observability): !\"#$%&-(=>=?)", "labels": [], "entities": []}, {"text": "; @=ABCD-\"'($ ; @=ABCD=EFB-(! ; @=ABCD=EFB@-)*' represents the derivation: verb ; noun ; adjective ; noun but also the following semantic transformation: action ; actor ; feature ; abstract feature The combination of grammatical and semantic functions of the Slavonic d-suffixes, together with their frequent usage (at least for some of them) and the high productivity, make very attractive the idea to study the regularities and the predictive power of ending letter combinations in a large text set.", "labels": [], "entities": []}, {"text": "We believe the results obtained over a representative collection can be used in a variety of robust analysis applications.", "labels": [], "entities": []}, {"text": "Linguistically, we interpret the last term as operations over a large text set with insufficient linguistic support, typically given by a lexical database, grammatical rules, parsing rules etc.", "labels": [], "entities": []}, {"text": "We target applications like POS tagging, text categorisation, information extraction, word sense disambiguation, question answering etc.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8575363159179688}, {"text": "text categorisation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7655678689479828}, {"text": "information extraction", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8481069505214691}, {"text": "word sense disambiguation", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.7356580495834351}, {"text": "question answering", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8877234756946564}]}, {"text": "Below we concentrate on the automatic extraction of a set of diagnostic word endings for Bulgarian that can determine the POS as well as some grammatical, morphological and semantic properties of the underlying word.", "labels": [], "entities": [{"text": "POS", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.8473579287528992}]}, {"text": "This is a two-step process including endings identification & learning and application & evaluation.", "labels": [], "entities": [{"text": "endings identification", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8932593762874603}]}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the related work on POS guessing and general morphology.", "labels": [], "entities": [{"text": "POS guessing", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9751390218734741}, {"text": "general morphology", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.9391586780548096}]}, {"text": "Section 3 introduces our basic resource: the Large Grammatical Dictionary of Bulgarian.", "labels": [], "entities": [{"text": "Large Grammatical Dictionary of Bulgarian", "start_pos": 45, "end_pos": 86, "type": "DATASET", "confidence": 0.6095314681529999}]}, {"text": "Section 4 describes two algorithms for ending guessing rules induction (an exact and an approximate one) and how to reduce the number of rules by a factor of up to 35.", "labels": [], "entities": [{"text": "ending guessing rules induction", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.8045850098133087}]}, {"text": "Section 5 contains the experimental setup and evaluation trying to predict POS, gender, number, article and semantics.", "labels": [], "entities": []}, {"text": "Section 6 discusses the results and Section 7 points to direction for future work.", "labels": [], "entities": []}, {"text": "uses pre-specified suffixes and performs statistical learning for POS guessing.", "labels": [], "entities": [{"text": "POS guessing", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.8861463069915771}]}, {"text": "The XEROX tagger comes with a list of built-in ending guessing rules.", "labels": [], "entities": [{"text": "XEROX tagger", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.6629806756973267}]}, {"text": "In addition to the ending, exploit capitalisation.", "labels": [], "entities": []}, {"text": "consider contextual information, word endings, entropy and open-class smoothing.", "labels": [], "entities": [{"text": "word endings", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.738934725522995}]}, {"text": "A similar approach is presented in. combine POS guessing, contextual rules and Markov models to build a POS tagger for biomedical text.", "labels": [], "entities": []}, {"text": "Avery influential is the work of, who induces more linguistically motivated rules exploiting both a tagged corpus and a lexicon.", "labels": [], "entities": []}, {"text": "He does not look at the affixes only, but also checks their POS class in a lexicon.", "labels": [], "entities": []}, {"text": "proposes a similar approach, but learns the rules from raw as opposed to tagged text.", "labels": [], "entities": []}, {"text": "Daciuk (1999) speeds up the process by means of finite state transducers.", "labels": [], "entities": []}, {"text": "use ending guessing rules to predict the morphological class of unknown German nouns.", "labels": [], "entities": []}, {"text": "apply latent semantic analysis fora knowledge-free morphology induction.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.6160208980242411}, {"text": "knowledge-free morphology induction", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.6684690316518148}]}, {"text": "DeJean (1998), follow a successor variety approach: the word is cut, if the number of distinct letters after a pre-specified sequence surpasses a threshold.", "labels": [], "entities": []}, {"text": "Goldsmith (2001) performs a minimum description length analysis of the morphology of several European languages using corpora.", "labels": [], "entities": []}, {"text": "induces derivational morphology from a lexicon by means of p-similarity based splitting.", "labels": [], "entities": []}, {"text": "Jacquemin (1997) focuses on the morphological processes.", "labels": [], "entities": []}, {"text": "Van den Bosch and Daelemans (1999) propose a memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes.", "labels": [], "entities": []}, {"text": "Yarowsky and Wicentowski (2000) present a corpus-based approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities.", "labels": [], "entities": []}, {"text": "Another interesting work, exploiting capitalisation and fixed/variable suffixes, is presented in Cucerzan and Yarowsky (2000).", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran two different general types of experiments: using the dictionary only and using additional raw text to estimate the frequencies of the dictionary words.", "labels": [], "entities": []}, {"text": "We split the dictionary into two parts at random: 894,915 wordforms for training (about 90%) and the remaining 99,624 wordforms for testing.", "labels": [], "entities": []}, {"text": "In the dictionary-only experiments we extracted the ending guessing rules by observing the endings of all wordforms from the training part of the dictionary . We then applied the rules thus learned (each time preferring the longest one that is compatible with the target word) to the testing part of the dictionary and we measured the precision P (what % of the cases the predicted class matched the hypothesised one) and the coverage C (what % of the cases there was at least one rule that was compatible with the target wordform).", "labels": [], "entities": [{"text": "precision P", "start_pos": 335, "end_pos": 346, "type": "METRIC", "confidence": 0.9405071437358856}]}, {"text": "We also calculated a kind of F-measure, which is normally defined as 2PR/(P+R), where R is the recall (proportion of proposed instances out of all that have to be found).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9953794479370117}, {"text": "2PR", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9794435501098633}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9993888139724731}]}, {"text": "Precision, recall and F-measure are defined in the information retrieval community in terms of positive and negative documents fora given query, i.e. with respect to a single class, but here we have multiple of them.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9812522530555725}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9976358413696289}, {"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9967967867851257}]}, {"text": "While we can define both an overall and a class-specific precision, it makes sense to talk about recall with respect to a particular class, but about coverage, when this is a measure for all classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9517536163330078}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9978109002113342}, {"text": "coverage", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.969254732131958}]}, {"text": "So, we redefined the F-measure as 2PC/(P+C).", "labels": [], "entities": []}, {"text": "In the dictionary+text experiments, we use the same training and testing parts of the dictionary, and in addition, the frequencies for the corresponding words in the training and testing text sets, accordingly.", "labels": [], "entities": []}, {"text": "I.e. a wordform in the text that is not in the dictionary is ignored and the rest are treated as if they have been repeated in its training/testing part the same number of times as they were met in the training/testing raw text.", "labels": [], "entities": []}, {"text": "We used a collection of 23.5 MB of various genres of Bulgarian texts as follows: \u2022 legal: 742 KB \u2022 poetry: 236 KB \u2022 prose: 1,032 KB \u2022 religion: 393 KB \u2022 newspapers: 21,118 KB We used 2,211 KB of the newspaper texts for testing (about 10% of the collection) and the rest for training.", "labels": [], "entities": []}, {"text": "As we already mentioned above, the same graphemic wordform can be met in the dictionary multiple times with different annotations.", "labels": [], "entities": []}, {"text": "In such cases, we treated them as equally likely both on training and testing.", "labels": [], "entities": []}, {"text": "This resulted in 1,751,963 wordforms tokens on training and 18,832 on testing.", "labels": [], "entities": []}, {"text": "The huge difference is due to the fact that on testing we have both 10 times smaller dictionary and 10 times smaller text set to estimate the wordform frequencies from, which multiply and result in 100 fold drop.", "labels": [], "entities": []}, {"text": "For all experiments, we excluded the wordforms from a stoplist composed of the closed class words, i.e. the ones with the following POS (counts in parentheses): auxiliary verbs (91), conjunctions (31), interjections (28), particles (41), prepositions (69) and pronouns (286).", "labels": [], "entities": [{"text": "POS", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9930604696273804}]}, {"text": "We have been hesitating also about the numerals but there were 582 of them in the dictionary and one can produce more, so they do not represent a closed class and we did not include them.", "labels": [], "entities": []}, {"text": "A potential problem with the stopwords removal is that many of them can also be non-stop ones depending on their POS, e.g.: while g\\D/preposition (under), EFhX/pronoun (these) and AXB/auxiliary (has been) are stop-words, g\\D/noun (floor), EFhX/noun (theses) and iXB/person (Bill) are not.", "labels": [], "entities": []}, {"text": "We did not try to address this problem (which would have required POS tagging and possibly morphological analysis, which is unacceptable, given our task) and we simply removed all homographs that matched a stoplist wordform.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.6958101093769073}]}, {"text": "We performed several experiments trying to assess the performance of the ending guessing rules as predictors for POS, article, gender, number and semantics.", "labels": [], "entities": [{"text": "POS", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.6149609088897705}]}], "tableCaptions": [{"text": " Table 1: Dictionary ambiguity with respect to POS.", "labels": [], "entities": [{"text": "Dictionary ambiguity", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7267346382141113}, {"text": "POS", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.5117120146751404}]}, {"text": " Table 2: Mikheev-like rules for POS guessing co- unt: original and cleaned (100% correct and all).", "labels": [], "entities": [{"text": "POS guessing", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9152722954750061}]}, {"text": " Table 3: Some redundant selection for \"-dEF\".", "labels": [], "entities": []}, {"text": " Table 4: Prior (training) distribution of POS.", "labels": [], "entities": []}, {"text": " Table 5: Prior (training) distribution for article  (no POS).", "labels": [], "entities": [{"text": "Prior", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9114692807197571}, {"text": "POS", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9780174493789673}]}, {"text": " Table 6: Prior (training) distribution of article  (with POS).", "labels": [], "entities": [{"text": "POS", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.7885172963142395}]}, {"text": " Table 7: Prior (training) distribution of gender  (no POS).", "labels": [], "entities": [{"text": "Prior", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9653056263923645}, {"text": "POS", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9761044979095459}]}, {"text": " Table 8: Prior (training) distribution of gender  (with POS).", "labels": [], "entities": [{"text": "Prior", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.949263334274292}, {"text": "POS", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9583449363708496}]}, {"text": " Table 9: Prior (training) distribution of number  (no POS).", "labels": [], "entities": [{"text": "POS", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9885733127593994}]}, {"text": " Table 10: Prior (training) distribution of number  (with POS).", "labels": [], "entities": [{"text": "POS", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9724617600440979}]}, {"text": " Table 11: Training (prior) distribution of  semantics.", "labels": [], "entities": []}, {"text": " Table 12: Experiments summary (dictionary).", "labels": [], "entities": []}, {"text": " Table 13: Testing performance per class for  gender+POS approximate rules 0.50 (dictionary).", "labels": [], "entities": [{"text": "POS approximate", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9226635992527008}]}, {"text": " Table 14: Testing performance per class for  gender+POS exact rules (dictionary).", "labels": [], "entities": [{"text": "POS exact rules", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.831844170888265}]}, {"text": " Table 15: Training performance per class for  gender+POS approximate rules 0.50 (dictionary).", "labels": [], "entities": [{"text": "POS approximate", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.9244217872619629}]}]}