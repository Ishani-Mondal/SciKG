{"title": [{"text": "A Little Goes a Long Way: Quick Authoring of Semantic Knowledge Sources for Interpretation", "labels": [], "entities": [{"text": "Interpretation", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.5475676655769348}]}], "abstractContent": [{"text": "In this paper we present an evaluation of Carmel-Tools, a novel behavior oriented approach to authoring and maintaining domain specific knowledge sources for robust sentence-level language understanding.", "labels": [], "entities": [{"text": "sentence-level language understanding", "start_pos": 165, "end_pos": 202, "type": "TASK", "confidence": 0.6317168573538462}]}, {"text": "Carmel-Tools provides a layer of abstraction between the author and the knowledge sources, freeing up the author to focus on the desired language processing behavior that is desired in the target system rather than the linguistic details of the knowledge sources that would make this behavior possible.", "labels": [], "entities": []}, {"text": "Furthermore, Carmel-Tools offers greater flexibility in output representation than the context-free rewrite rules produced by previous semantic authoring tools, allowing authors to design their own predicate language representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the major obstacles that currently makes it impractical for language technology applications to make use of sophisticated approaches to natural language understanding, such as deep semantic analysis and domain level reasoning, is the tremendous expense involved in authoring and maintaining domain specific knowledge sources.", "labels": [], "entities": [{"text": "deep semantic analysis", "start_pos": 183, "end_pos": 205, "type": "TASK", "confidence": 0.6891440351804098}]}, {"text": "In this paper we describe an evaluation of Carmel-Tools as a proof of concept fora novel behavior oriented approach to authoring and maintaining domain specific knowledge sources for robust sentence-level language understanding.", "labels": [], "entities": [{"text": "sentence-level language understanding", "start_pos": 190, "end_pos": 227, "type": "TASK", "confidence": 0.6328133145968119}]}, {"text": "What we mean by behavior oriented is that CarmelTools provides a layer of abstraction between the author and the knowledge sources, freeing up the author to focus on the desired language processing behavior that is desired in the target system rather than the linguistic details of the knowledge sources that would make this behavior possible.", "labels": [], "entities": []}, {"text": "Thus, Carmel-Tools is meant to make the knowledge source engineering process accessible to a broader audience.", "labels": [], "entities": []}, {"text": "Carmel-Tools is used to author domain specific semantic knowledge sources for the Carmel Workbench) that contains broad coverage domain general syntactic and lexical knowledge sources for robust language understanding in English.", "labels": [], "entities": [{"text": "Carmel Workbench)", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9354674220085144}, {"text": "robust language understanding", "start_pos": 188, "end_pos": 217, "type": "TASK", "confidence": 0.7374127705891927}]}, {"text": "Our evaluation demonstrates how Carmel-Tools can be used to interpret sentences in the physics domain as part of a content-based approach to automatic essay grading.", "labels": [], "entities": []}, {"text": "Sentence: The man is moving horizontally at a constant velocity with the pumpkin.", "labels": [], "entities": []}], "datasetContent": [{"text": "A preliminary evaluation was run for the physics domain.", "labels": [], "entities": []}, {"text": "We used for our evaluation a corpus of essays written by students in response to 5 simple qualitative physics questions such as \"If a man is standing in an elevator holding his keys in front of his face, and if the cable holding the elevator snaps and the man then lets goof the keys, what will be the relationship between the position of the keys and that of the man as the elevator falls to the ground?", "labels": [], "entities": []}, {"text": "A predicate language definition was designed consisting of 40 predicates, 31 predicate types, 160 tokens, 37 token types, and 15 abstract types.", "labels": [], "entities": []}, {"text": "The language was meant to be able to represent physical objects mentioned in our set of physics problems, body states (e.g., freefall, contact, non-contact), quantities that can be measured (e.g., force, velocity, acceleration, speed, etc.), features of these quantities (e.g., direction, magnitude, etc.), comparisons between quantities (equivalence, non-equivalence, relative size, relative time, relative location), physics laws, and dependency relations.", "labels": [], "entities": []}, {"text": "An initial set of 250 example sentences was then annotated, including sentences from each of a set of 5 physics problems.", "labels": [], "entities": []}, {"text": "Next a set of 202 novel test sentences, each between 4 and 64 words long, was extracted from the corpus.", "labels": [], "entities": []}, {"text": "Since comparisons, such as between the accelerations of objects in freefall together, are important for the reasoning in all of the questions used for corpus collection, we focused the coverage evaluation specifically on sentences pertaining to comparisons, such as in.", "labels": [], "entities": [{"text": "corpus collection", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7433649599552155}]}, {"text": "The goal of the evaluation was to test the extent to which knowledge generated from annotated examples generalizes to novel examples.", "labels": [], "entities": []}, {"text": "Since obtaining the correct predicate language representation requires obtaining a correct syntactic parse, we first evaluated CARMEL's syntactic coverage over the corpus of test sentences to obtain an upper bound for expected performance.", "labels": [], "entities": []}, {"text": "We assigned the syntactic interpretation of each sentence a score of None, Bad, Partial, or Acceptable.", "labels": [], "entities": [{"text": "None", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9692927002906799}, {"text": "Acceptable", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9848618507385254}]}, {"text": "A grade of None indicates that no interpretation was built by the grammar.", "labels": [], "entities": [{"text": "None", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.998896598815918}]}, {"text": "Bad indicates that parses were generated, but they contained errorfull functional relationships between constituents.", "labels": [], "entities": []}, {"text": "Partial indicates that no parse was generated that covered the entire sentence, ut the portions that were completely correct for at least one interpretation of the sentence.", "labels": [], "entities": []}, {"text": "Acceptable indicates that a complete parse was built that contained no incorrect functional relationships.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9946950078010559}]}, {"text": "If any word of the sentence was not covered, it was one that would not change the meaning of the sentence.", "labels": [], "entities": []}, {"text": "For example, \"he had the same velocity as you had\" is the same as \"he had the same velocity as you\", so if \"did\" was not part of the final parse but other than that, the parse was fine, it was counted as Acceptable.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 204, "end_pos": 214, "type": "METRIC", "confidence": 0.9977599382400513}]}, {"text": "Overall the coverage of the grammar was very good.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9864491820335388}]}, {"text": "166 sentences were graded Acceptable, which is about 83% of the corpus.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9973378777503967}]}, {"text": "8 received a grade of Partial, 26 Bad, and 1 None.", "labels": [], "entities": [{"text": "Partial", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9878189563751221}, {"text": "Bad", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9751571416854858}, {"text": "None", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9970092177391052}]}, {"text": "We then applied the same set of grades to the quality of the predicate language output.", "labels": [], "entities": []}, {"text": "Note that that the grade assigned to an analysis represents the correctness and completeness of the predicate representation the system obtained for that sentence.", "labels": [], "entities": []}, {"text": "In this case, a grade of Acceptable meant that all aspects of intended meaning were accounted for, and no misleading information was encoded.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9753457903862}]}, {"text": "Partial indicated that some non-trivial part of the intended meaning was communicated.", "labels": [], "entities": []}, {"text": "Any interpretation containing any misleading information was counted as Bad.", "labels": [], "entities": [{"text": "Bad", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9901279807090759}]}, {"text": "If no predicate language representation was returned, the sentence was graded as None.", "labels": [], "entities": []}, {"text": "As expected, grades for semantic interpretation were not as high as for syntactic analysis.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.8731947243213654}, {"text": "syntactic analysis", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7223638892173767}]}, {"text": "In particular, 107 were assigned a grade of Acceptable, 45 were assigned a grade of Partial, 36 were assigned a grade of Bad, and 14 received a nil interpretation.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.998293936252594}, {"text": "Bad", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9906893968582153}]}, {"text": "Our evaluation demonstrates that knowledge generated from annotated examples can be used to interpret novel sentences, however, there are still gaps in the coverage of the automatically generated knowledge sources that need to be filled in with new annotated examples.", "labels": [], "entities": []}, {"text": "Furthermore, the small but noticeable percentage of bad interpretations indicates that some previously annotated examples need to be modified in order to prevent these bad interpretations from being generated.", "labels": [], "entities": []}], "tableCaptions": []}