{"title": [{"text": "Evaluating the Performance of the OntoSem Semantic Analyzer", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an innovative evaluation regimen developed for the text meaning representations (TMRs) produced by the Ontological Semantic (OntoSem) general purpose syntactic-semantic analyzer.", "labels": [], "entities": [{"text": "text meaning representations (TMRs)", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.7651983002821604}]}, {"text": "The goal of evaluation is not only to determine the quality of TMRs forgiven texts, but also to assign blame for various classes of errors, thus suggesting directions for continued work on both knowledge resources and processors.", "labels": [], "entities": [{"text": "TMRs forgiven texts", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.822977880636851}]}, {"text": "The paper includes descriptions of the OntoSem processing environment, the evaluation regime itself and results from our first evaluation effort.", "labels": [], "entities": []}, {"text": "The overall architecture of the OntoSem semantic analyzer.", "labels": [], "entities": [{"text": "OntoSem semantic analyzer", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6734013358751932}]}, {"text": "The evaluation regimen described in this paper evaluates the production of basic TMRs.", "labels": [], "entities": []}, {"text": "Other processing will be evaluated in follow-up work.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we describe the evaluation regimen fora general-purpose syntacticsemantic analyzer, OntoSem, under continuous development at the Institute for Language and Information Technologies (ILIT) of the University of Maryland Baltimore County.", "labels": [], "entities": []}, {"text": "Its top-level architecture is illustrated in.", "labels": [], "entities": []}, {"text": "The knowledge in the fact repository and the ontology serves not only OntoSem itself but also provides a knowledge substrate to be used in a variety of reasoning applications.", "labels": [], "entities": []}, {"text": "At present, the acquisition of the ontology and the semantic lexicon is carried out by human acquirers using interactive tools.", "labels": [], "entities": []}, {"text": "The acquisition of the fact repository is mixed, with some of it carried out manually and The approach to semantic analysis in OntoSem is described in some detail in, e.g.,, Nire some of it resulting from the operation of the fact extractor on the results of semantic analysis.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7719118297100067}, {"text": "OntoSem", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.8909251093864441}]}], "datasetContent": [{"text": "Once the gold standard TMRs are produced, the evaluation of OntoSem proceeds fully matically.", "labels": [], "entities": []}, {"text": "each run, we produce four output files: rocessor results; performed by automatically comparing the actual preprocessor, syntax or semantic results to the corresponding gold standard outputs.", "labels": [], "entities": []}, {"text": "The evaluation produces statistics and/or measurements as follows.", "labels": [], "entities": []}, {"text": "For this evaluation, the lexicon provided almost complete lexical coverage of the input texts (in fact only one word was missing).", "labels": [], "entities": []}, {"text": "We will use the are ollected for each evaluation run.", "labels": [], "entities": []}, {"text": "ches between an ctual run and the gold standard, n is the number of of hrases, and phrase attachment.", "labels": [], "entities": [{"text": "phrase attachment", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7300212532281876}]}, {"text": "Each phrase in the gold standard syntax output is compared to its closest match in the output under consideration.The output phrase that has the same label (NP, CL, etc.), the same headword, and the closest matching starting and ending points is used for the comparison.", "labels": [], "entities": []}, {"text": "Each phrase is given the score: whereat the start of the phrase and start is the word num evaluated.", "labels": [], "entities": []}, {"text": "Thus, if the gold standard phrase began at word 10 and ended at word 16, and the closest matching phrase in the output being evaluated began at word 9 and ended at word 17, then the score for this phrase would be 1 -(|10 -9| + |16 -17|) / (16 -10) = (1 -(2 / 6)) = 2/3.", "labels": [], "entities": []}, {"text": "If no matching phrase could be found (i.e. no overlapping phrase could be found with the same phrase label and head word), then a score of 0.0 is assigned.", "labels": [], "entities": []}, {"text": "The score for the whole sentence under evaluation is the average of the scores for each of the phrases.", "labels": [], "entities": []}, {"text": "For phrase head determination, the standard (m/n) measure is used.", "labels": [], "entities": [{"text": "phrase head determination", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.8943876425425211}, {"text": "standard (m/n) measure", "start_pos": 35, "end_pos": 57, "type": "METRIC", "confidence": 0.9282609735216413}]}, {"text": "For e and headword that overlaps with the gold standard phrase.", "labels": [], "entities": []}, {"text": "Attachment is also measured as (m/n).", "labels": [], "entities": [{"text": "Attachment", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.987865149974823}]}, {"text": "For each phrase in the gold standard syntax, the evaluation proced speech, the same headword and the same constituents.", "labels": [], "entities": []}, {"text": "For example, if the gold standard output has a PP attached to a NP, it will be shown to be a constituent of that NP.", "labels": [], "entities": []}, {"text": "If the output being evaluated attaches the PP at a different constituent, then a mismatch will be identified.", "labels": [], "entities": []}, {"text": "core between 0.0 and 1.0 is assigned for band c follows: Score = m/(m+n).", "labels": [], "entities": [{"text": "Score", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9547298550605774}]}, {"text": "The Syntactic lysis Overa a Semantic analysis statistics measure the quality of word sense disambiguation (WSD) and semantic dependency A) First, the standard match/mismatch (m/n) is used.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.5669697821140289}, {"text": "match/mismatch (m/n)", "start_pos": 159, "end_pos": 179, "type": "METRIC", "confidence": 0.9175691455602646}]}, {"text": "Each TMR element in the gold standard semantic representati arose.", "labels": [], "entities": []}, {"text": "The TMR element in the semantic representation being evaluated that corresponds to that same word number is then compared with it.", "labels": [], "entities": []}, {"text": "Second, the evaluation system produces a weighted score for WSD complexity.", "labels": [], "entities": [{"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.8749447464942932}]}, {"text": "An overall score betwe penalized less than a mismatch of a word with fewer senses.", "labels": [], "entities": []}, {"text": "The score for each mismatch is 1 -(2 / number-of-senses), if the word has more than 2 senses, and 0.0 if it has less than or equal to 2 senses.", "labels": [], "entities": []}, {"text": "An exact match is given a score of 1.0.", "labels": [], "entities": []}, {"text": "The overall score for the sentence is the average score for each TMR element.", "labels": [], "entities": []}, {"text": "The system also computes a weighted score for WSD \"distance.\"", "labels": [], "entities": [{"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.44442832469940186}, {"text": "distance", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.5103431940078735}]}, {"text": "An overall score between 0.0 and 1.0 is returned.", "labels": [], "entities": []}, {"text": "A mismatch that penalized less than a mismatch that is ontologically \"far\" from the correct semantics.", "labels": [], "entities": []}, {"text": "The ontological distance is computed using the Ontosearch algorithm that returns a score between 0.0 and 1.0 reflecting how close the two concepts are in the ontology, with a score of 1.0 indicating a Example Semantic Evaluation perfect match.", "labels": [], "entities": []}, {"text": "The overall score for the sentence is the average score of each TMR element.", "labels": [], "entities": []}, {"text": "The quality of semantic dependency determination is computed using the standard (m/n) measur We will now exemplify the evaluation of the semantic analysis of the sample sentence in 1: D) e.", "labels": [], "entities": [{"text": "semantic dependency determination", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.6541487574577332}]}, {"text": "Each TMR element in the gold standard is compared to the corresponding 1.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9333971738815308}]}, {"text": "Hall is scheduled to embark on the 12 hour overland trip to the Iraqi capital, Baghdad.", "labels": [], "entities": []}, {"text": "TMR element in the semantics being evaluated.", "labels": [], "entities": [{"text": "TMR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5420611500740051}]}, {"text": "Each property modifying the gold standard TMR element that is also in the evaluation TMR element increments them count, each property in the gold standard TMR element that is not in the evaluation TMR element increments then count.", "labels": [], "entities": []}, {"text": "The fillers of matching properties are also compared.", "labels": [], "entities": []}, {"text": "If the filler of the gold standard property is another TMR element (as opposed to being a literal), then the filler is also matched against the corresponding filler in the semantic representation being evaluated, incrementing them and n counters as appropriate.", "labels": [], "entities": []}, {"text": "The relations between TMR elements is one of the central aspects of Ontological Semantics which goes beyond simple word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6752880215644836}]}, {"text": "This score reflects how well the dependency determination was performed.", "labels": [], "entities": []}, {"text": "n the evaluation TMR element increments them count, each property in the gold standard TMR element that is not in the evaluation TMR element increments then count.", "labels": [], "entities": []}, {"text": "The fillers of matching properties are also compared.", "labels": [], "entities": []}, {"text": "If the filler of the gold standard property is another TMR element (as opposed to being a literal), then the filler is also matched against the corresponding filler in the semantic representation being evaluated, incrementing them and n counters as appropriate.", "labels": [], "entities": []}, {"text": "The relations between TMR elements is one of the central aspects of Ontological Semantics which goes beyond simple word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6752880215644836}]}, {"text": "This score reflects how well the dependency determination was performed.", "labels": [], "entities": []}, {"text": "The analyzer produces the syntactic analysis shown in.", "labels": [], "entities": []}, {"text": "This analysis contains many spurious parses (along with the correct ones).", "labels": [], "entities": []}, {"text": "The gold standard parse of this sentence is shown in.", "labels": [], "entities": []}, {"text": "The illustrations are difficult to read but the number of edges can be visually compared.", "labels": [], "entities": []}, {"text": "In order to make an interesting evaluation example, we forced the semantic analyzer to misinterpret capital.", "labels": [], "entities": []}, {"text": "The analyzer actually chose the correct sense, CAPITAL-CITY, but here we will force it to select the monetary sense, CAPITAL.", "labels": [], "entities": []}, {"text": "We will now demonstrate the calculation and significance of the semantic evaluation parameters.", "labels": [], "entities": []}, {"text": "This increments the mismatch counter by one.", "labels": [], "entities": [{"text": "mismatch counter", "start_pos": 20, "end_pos": 36, "type": "METRIC", "confidence": 0.9243172407150269}]}, {"text": "The other five fillers match with the gold standard, thus the match counter is incremented by 5.", "labels": [], "entities": []}, {"text": "For the whole sentence, the dependency matches will be 11 and the mismatches will be 1.", "labels": [], "entities": []}, {"text": "In this case, the mismatched dependency was caused by the misana between syntactic and semantic structures. of 11/12 = 0.92 is calculated for use in the overall score.", "labels": [], "entities": []}, {"text": "the different statistics and runs Section 3.", "labels": [], "entities": [{"text": "Section 3", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.9000441431999207}]}], "tableCaptions": [{"text": " Table 1. The general statistics for the  first evaluation run of OntoSem", "labels": [], "entities": [{"text": "OntoSem", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.739629864692688}]}, {"text": " Table 2. Results of the initial evaluation of the OntoSem semantic analyzer.", "labels": [], "entities": [{"text": "OntoSem semantic analyzer", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6285766164461771}]}]}