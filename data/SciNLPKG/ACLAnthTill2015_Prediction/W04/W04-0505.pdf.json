{"title": [{"text": "BioGrapher: Biography Questions as a Restricted Domain Question Answering Task", "labels": [], "entities": [{"text": "Restricted Domain Question Answering", "start_pos": 37, "end_pos": 73, "type": "TASK", "confidence": 0.5954406559467316}]}], "abstractContent": [{"text": "We address Question Answering (QA) for biographical questions, i.e., questions asking for biographical facts about persons.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8346686005592346}]}, {"text": "The domain of biographical documents differs from other restricted domains in that the available collections of biographies are inherently incomplete: a major challenge is to answer questions about persons for whom biographical information is not present in biography collections.", "labels": [], "entities": []}, {"text": "We present BioGrapher, a biographical QA system that addresses this problem by machine learning algorithms for biography classification.", "labels": [], "entities": [{"text": "biography classification", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.8311619460582733}]}, {"text": "BioGrapher first attempts to answer a question by searching in a given collection of biographies, using techniques tailored for the restricted nature of the domain.", "labels": [], "entities": []}, {"text": "If a biography is not found, BioGrapher attempts to find an answer on the web: it retrieves documents using a web search engine, filters these using the biography classifier, and then extracts answers from documents classified as biographies.", "labels": [], "entities": []}, {"text": "Our empirical results show that biographical classification, prior to answer extraction, improves the results.", "labels": [], "entities": [{"text": "biographical classification", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.8258443772792816}, {"text": "answer extraction", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8679546415805817}]}], "introductionContent": [{"text": "Although most current research in question answering (QA) is oriented towards open domains, as witnessed by evaluation exercises such as TREC, CLEF, and NTCIR, various significant applications concern restricted domains, e.g., software manuals.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.90762300491333}, {"text": "NTCIR", "start_pos": 153, "end_pos": 158, "type": "DATASET", "confidence": 0.8601553440093994}]}, {"text": "In restricted domains, a QA system faces questions and documents that exhibit less variation in language use (e.g., words and fixed phrases, more specific terminology) than in an open domain, and it could access high-quality knowledge sources that cover the entire domain.", "labels": [], "entities": []}, {"text": "Open domain QA as it is assessed at TREC, CLEF, and NTCIR concerns abroad variety of fairly restricted question types, such as location questions, monetary questions, biography questions, questions that ask for concept definitions, etc.", "labels": [], "entities": [{"text": "Open domain QA", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.5655620694160461}, {"text": "TREC", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8502134680747986}, {"text": "CLEF", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.8907719850540161}, {"text": "NTCIR", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9467626810073853}]}, {"text": "How useful or effective is it to adopt a restricted domain approach to some of these question types?", "labels": [], "entities": []}, {"text": "In this paper we explore so-called biographical questions, e.g., Who was Algar Hiss? or Who is Sir John Hale?, i.e., questions that demand answers consisting of biograpical key facts that are typically found in biographies and that typically involve fixed phrases about, e.g., birthdates, education, societal roles.", "labels": [], "entities": [{"text": "Who was Algar Hiss? or Who is Sir John Hale?", "start_pos": 65, "end_pos": 109, "type": "TASK", "confidence": 0.6068756158153216}]}, {"text": "This type of questions was found to be quite frequent in search engine logs.", "labels": [], "entities": []}, {"text": "We believe that biographical questions can be usefully viewed as defining a restricted domain for QA: the domain of biographical information as represented by biographies.", "labels": [], "entities": []}, {"text": "Ideally, biographical questions are answered by retrieving a biography from some existing collection of biographies (such as biography.com) and extracting snippets from it.", "labels": [], "entities": []}, {"text": "Such resources, however, have a limited coverage.", "labels": [], "entities": []}, {"text": "There will always be people whose biographical information is not contained in any of the existing collections.", "labels": [], "entities": []}, {"text": "This necessitates retrieval of \"biography-like\" documents, i.e., documents with biographical information.", "labels": [], "entities": []}, {"text": "The problem of identifying biography-like documents by machine learning algorithms turns out to be a challenging but rewarding task as we will see below.", "labels": [], "entities": []}, {"text": "In this paper we address the problem of question answering within the biographical domain.", "labels": [], "entities": [{"text": "question answering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9046028256416321}]}, {"text": "We describe BioGrapher, a restricted domain QA system for answering bibliographical questions in which a baseline approach, that exploits biography collections, is extended with a trainable biography classifier operating on the web in order to enhance coverage.", "labels": [], "entities": []}, {"text": "The baseline system helps us understand the usefulness of existing high quality biography collections within a QA system.", "labels": [], "entities": []}, {"text": "The extension of our baseline approach concerns the problem of identifying biography-like documents, and the extraction from such documents of answers for questions that could not be answered using biography collections.", "labels": [], "entities": []}, {"text": "A main challenge lies in constructing an algorithm for identifying documents containing usefull biographical information that may provide an answer fora given question.", "labels": [], "entities": []}, {"text": "To addresss this challenge, we explore two machine learning algorithms: Ripper and Support-Vector Machines.", "labels": [], "entities": []}, {"text": "Section 2 provides some background, and in Section 3 we briefly describe our baseline QA system, based on external knowledge sources complemented with a naive approach to retrieving biography snippets using a web search engine.", "labels": [], "entities": []}, {"text": "In Section 4 we prepare the ground for our text classification experiments.", "labels": [], "entities": [{"text": "text classification", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8611190319061279}]}, {"text": "In Section 5 we present two classifiers: one loosely based on the Ripper algorithm and the other based on an SVM classifier.", "labels": [], "entities": []}, {"text": "In Section 6 we compare the performanc of the baseline against versions of the system integrated with the two classifiers.", "labels": [], "entities": []}, {"text": "Section 7 discusses the results and considers the possibility of applying our approach to other restricted domains.", "labels": [], "entities": []}, {"text": "We conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of individual person definition questions was done using the F-measure: where P is precision (to be defined shortly), R is recall (to be defined shortly), and \u03b2 was set to 5, indicating that precision was more important than recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9958335161209106}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9985995888710022}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9812713265419006}, {"text": "precision", "start_pos": 202, "end_pos": 211, "type": "METRIC", "confidence": 0.9984989166259766}, {"text": "recall", "start_pos": 236, "end_pos": 242, "type": "METRIC", "confidence": 0.9950030446052551}]}, {"text": "Length was used as a crude approximation to precision; it gives a system an allowance of 100 (non-white-space) characters for each correct snippet it retrieved.", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9881529211997986}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9991440773010254}]}, {"text": "The precision score is set to one if the response is no longer than this allowance, otherwise it is downgraded using the function P = 1 \u2212 ((length \u2212 allowance)/length).", "labels": [], "entities": [{"text": "precision score", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9889905750751495}, {"text": "length \u2212 allowance)/length", "start_pos": 140, "end_pos": 166, "type": "METRIC", "confidence": 0.7382970333099366}]}, {"text": "As to recall, for each question, the TREC assessors marked some snippets as vital and the remainder as non-vital.", "labels": [], "entities": [{"text": "recall", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9781408309936523}, {"text": "TREC", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.6186535954475403}]}, {"text": "The non-vital snippets act as \"don't care\" condition.", "labels": [], "entities": []}, {"text": "That is, systems should be penalized for not retrieving vital nuggets, and for retrieving snippets that are not in the assessors' snippet lists at all, but should be neither penalized nor rewarded for returning a non-vital snippet.", "labels": [], "entities": []}, {"text": "To implement the \"don't care\" condition, snippet recall is computed only over vital snippets.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9886232614517212}]}, {"text": "In total, 30 person definition questions were evaluated at the TREC 2003 QA track.", "labels": [], "entities": [{"text": "TREC 2003 QA track", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.9023168534040451}]}, {"text": "The overall F score of a run was obtained by averaging overall the individual questions.", "labels": [], "entities": [{"text": "F score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9917905926704407}]}], "tableCaptions": [{"text": " Table 4: Distribution of documents retrieved (in- cluding false-positive)", "labels": [], "entities": []}]}