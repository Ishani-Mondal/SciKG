{"title": [], "abstractContent": [{"text": "This paper describes our method based on Support Vector Machines for automatically assigning semantic roles to constituents of English sentences.", "labels": [], "entities": [{"text": "automatically assigning semantic roles to constituents of English sentences", "start_pos": 69, "end_pos": 144, "type": "TASK", "confidence": 0.775520834657881}]}, {"text": "This method employs four different feature sets, one of which being first reported herein.", "labels": [], "entities": []}, {"text": "The combination of features as well as the extended training data we considered have produced in the Senseval-3 experiments an F1-score of 92.5% for the unrestricted case and of 76.3% for the restricted case.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9997363686561584}]}], "introductionContent": [{"text": "The evaluation of the Senseval-3 task for Automatic Labeling of Semantic Roles is based on the annotations made available by the FrameNet Project ().", "labels": [], "entities": [{"text": "Automatic Labeling of Semantic Roles", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.7655512154102325}]}, {"text": "The idea of automatically identifying and labeling frame-specific roles, as defined by the semantic frames, was first introduced by).", "labels": [], "entities": []}, {"text": "Each semantic frame is characterized by a set of target words which can be nouns, verbs or adjectives.", "labels": [], "entities": []}, {"text": "This helps abstracting the thematic roles and adding semantics to the given frame, highlighting the characteristic semantic features.", "labels": [], "entities": []}, {"text": "Frames are characterized by (1) target words or lexical predicates whose meaning includes aspects of the frame; (2) frame elements (FEs) which represent the semantic roles of the frame and (3) examples of annotations performed on the British National Corpus (BNC) for instances of each target word.", "labels": [], "entities": [{"text": "frame elements (FEs", "start_pos": 116, "end_pos": 135, "type": "METRIC", "confidence": 0.5770806521177292}, {"text": "British National Corpus (BNC)", "start_pos": 234, "end_pos": 263, "type": "DATASET", "confidence": 0.9677446981271108}]}, {"text": "Thus FrameNet frames are schematic representations of situations lexicalized by the target words (predicates) in which various participants and conceptual roles are related (the frame elements), exemplified by sentences from the BNC in which the target words and the frame elements are annotated.", "labels": [], "entities": [{"text": "BNC", "start_pos": 229, "end_pos": 232, "type": "DATASET", "confidence": 0.8952519297599792}]}, {"text": "In Senseval-3 two different cases of automatic labeling of the semantic roles were considered.", "labels": [], "entities": []}, {"text": "The Unrestricted Case requires systems to assign FE labels to the test sentences for which (a) the boundaries of each frame element were given and the target words identified.", "labels": [], "entities": [{"text": "FE", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9307025074958801}]}, {"text": "The Restricted Case requires systems to (i) recognize the boundaries of the FEs for each evaluated frame as well as to (ii) assign a label to it.", "labels": [], "entities": []}, {"text": "Both cases can be cast as two different classifications: (1) a classification of the role when its boundaries are known and (2) a classification of the sentence words as either belonging to a role or not 1 . A similar approach was used for automatically identifying predicate-argument structures in English sentences.", "labels": [], "entities": [{"text": "automatically identifying predicate-argument structures in English sentences", "start_pos": 240, "end_pos": 316, "type": "TASK", "confidence": 0.764341299022947}]}, {"text": "The PropBank annotations (www.cis.upenn.edu/\u223cace) enable training for two distinct learning techniques: (1) decision trees (Surdeanu et al., 2003) and (2) Support Vector Machines (SVMs) ().", "labels": [], "entities": []}, {"text": "The SVMs produced the best results, therefore we decided to use the same learning framework for the Senseval-3 task for Automatic Labeling of Semantic Roles.", "labels": [], "entities": [{"text": "Automatic Labeling of Semantic Roles", "start_pos": 120, "end_pos": 156, "type": "TASK", "confidence": 0.7778706431388855}]}, {"text": "Additionally, we have performed the following enhancements: \u2022 we created a multi-class classifier for each frame, thus achieving improved accuracy and efficiency; \u2022 we combined some new features with features from (); \u2022 we resolved the data sparsity problem generated by limited training data for each frame, when using the examples associated with any other frame from FrameNet that had at least one FE shared with each frame that was evaluated; \u2022 we crafted heuristics that improved mappings from the syntactic constituents to the semantic roles.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9977868795394897}, {"text": "FE", "start_pos": 401, "end_pos": 403, "type": "METRIC", "confidence": 0.9221000671386719}]}, {"text": "We believe that the combination of these four extensions are responsible for our results in Senseval-3.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our methods of classifying semantic roles whereas Section 3 describes our method of identifying role boundaries.", "labels": [], "entities": []}, {"text": "Section 4 details our heuristics and Section 5 details the experimental results.", "labels": [], "entities": []}, {"text": "Section 6 summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the Senseval-3 task for Automatic Labeling of Semantic Roles 24,558 sentences from FrameNet were assigned for training while 8,002 for testing.", "labels": [], "entities": [{"text": "Automatic Labeling of Semantic Roles", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.8302536249160767}]}, {"text": "We used 30% of the training set (7367 sentences) as a validation-set for selecting SVM parameters that optimize accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9917945265769958}]}, {"text": "The number of FEs for which labels had to be assigned were: 51,010 for the training set; 15,924 for the validation set and 16,279 for the test set.", "labels": [], "entities": [{"text": "FEs", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9848564267158508}]}, {"text": "We used an additional set of 66,687 sentences (hereafter extended data) as extended data produced when using the examples associated with any other frame from FrameNet that had at least one FE shared with any of the 40 frames evaluated in Senseval-3.", "labels": [], "entities": [{"text": "FE", "start_pos": 190, "end_pos": 192, "type": "METRIC", "confidence": 0.9481422305107117}]}, {"text": "These sentences were parsed with the Collins' parser).", "labels": [], "entities": [{"text": "Collins'", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9808330535888672}]}, {"text": "The classifier experiments were carried out using the SVM-light software (Joachims, 1999) available at http://svmlight.joachims.org/ with a polynomial kernel 2 (degree=3).", "labels": [], "entities": []}, {"text": "For this task we devised four different experiments that used four different combination of features: (1) FS1 indicates using only Feature Set 1; (2) +H indicates that we added the heuristics; (3) +FS2+FS3 indicates that we add the feature Set 2 and 3; and (4) +E indicates that the extended data has also been used.", "labels": [], "entities": [{"text": "FS1", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9972261786460876}, {"text": "FS2", "start_pos": 198, "end_pos": 201, "type": "METRIC", "confidence": 0.9501246809959412}, {"text": "FS3", "start_pos": 202, "end_pos": 205, "type": "METRIC", "confidence": 0.5140859484672546}]}, {"text": "For each of the four experiments we trained 40 multi-class classifiers, (one for each frame) fora total of 385 binary role classifiers.", "labels": [], "entities": []}, {"text": "The following  In order to find the best feature combination for this task we carried out some preliminary experiments over five frames.", "labels": [], "entities": []}, {"text": "In, the row labeled B lists the F 1-score of boundary detection over 4 different feature sets: FS1, +H, +FS4 and +E, the extended data.", "labels": [], "entities": [{"text": "FS1", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.8291602730751038}, {"text": "FS4", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.8314867615699768}]}, {"text": "The row labeled R lists the same results for the whole Restricted Case. were obtained by comparing the FE boundaries identified by our parser with those annotated in FrameNet.", "labels": [], "entities": [{"text": "FE", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9904444813728333}, {"text": "FrameNet", "start_pos": 166, "end_pos": 174, "type": "DATASET", "confidence": 0.9391297101974487}]}, {"text": "We believe that these results are more indicative of the performance of our systems than those obtained when using the scorer provided by Senseval-3.", "labels": [], "entities": []}, {"text": "When using this scorer, our results have a precision of 89.9%, recall of 77.2% and an F 1-score of 83.07% for the Restricted Case.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9997099041938782}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.999893069267273}, {"text": "F 1-score", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.99077507853508}]}, {"text": "To generate the final Senseval-3 submissions we selected the most accurate models (for unrestricted and restricted tasks) of the validation experiments.", "labels": [], "entities": []}, {"text": "Then we re-trained such models with all training data (i.e. our training plus validation data) and the setting (parameters, heuristics and extended data) derived over the validation-set.", "labels": [], "entities": []}, {"text": "Finally, we run all classifiers on the test-set of the task.", "labels": [], "entities": []}, {"text": "illustrates the final results for both sub-tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Restrictive experiments on validation-set.", "labels": [], "entities": []}, {"text": " Table 2: Results on the test-set.", "labels": [], "entities": []}]}