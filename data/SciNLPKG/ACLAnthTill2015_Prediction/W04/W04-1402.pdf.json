{"title": [{"text": "Work-in-Progress project report : CESTA -Machine Translation Evaluation Campaign", "labels": [], "entities": [{"text": "CESTA -Machine Translation Evaluation", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.7774806618690491}]}], "abstractContent": [{"text": "CESTA, the first European Campaign dedicated to MT Evaluation, is a project labelled by the French Technolangue action.", "labels": [], "entities": [{"text": "CESTA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9189277291297913}, {"text": "MT Evaluation", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9885410070419312}, {"text": "French Technolangue action", "start_pos": 92, "end_pos": 118, "type": "DATASET", "confidence": 0.8757225076357523}]}, {"text": "CESTA provides an evaluation of six commercial and academic MT systems using a protocol set by an international panel of experts.", "labels": [], "entities": [{"text": "CESTA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9614353179931641}, {"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9776216745376587}]}, {"text": "CESTA aims at producing reusable resources and information about reliability of the metrics.", "labels": [], "entities": [{"text": "CESTA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9280997514724731}, {"text": "reliability", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9659658670425415}]}, {"text": "Two runs will be carried out: one using the system's basic dictionary, another after terminological adaptation.", "labels": [], "entities": []}, {"text": "Evaluation task, test material, resources, evaluation measures, metrics, will be detailed in the full paper.", "labels": [], "entities": []}, {"text": "The protocol is the combination of a contrastive reference to: IBM \"BLEU\" protocol (Papineni, K., S. Roukos, T. Ward", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9420844316482544}]}], "introductionContent": [], "datasetContent": [{"text": "An emerging evaluation methodology in NLP technology focuses on quality requirements analysis.", "labels": [], "entities": [{"text": "quality requirements analysis", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.6661134958267212}]}, {"text": "The needs and consequently the satisfaction of end-users, and this will depend on the tasks and expected results requirement domains, which we have identified as diagnostic quality dimensions.", "labels": [], "entities": []}, {"text": "One of the most suitable methods in this type of evaluation is the adequacy evaluation that aims at finding out whether a system or product is adequate to someone's needs (see among many others fora more detailed discussion of these issues).", "labels": [], "entities": []}, {"text": "This approach encourages communication between users and developers.", "labels": [], "entities": []}, {"text": "The definition of the CESTA evaluation protocol took into account the Framework for MT Evaluation in ISLE (FEMTI), available online.", "labels": [], "entities": [{"text": "MT Evaluation in ISLE (FEMTI)", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.7367171560014997}]}, {"text": "FEMTI offers the possibility to define evaluation requirements, then to select relevant 'qualities', and the metrics commonly used to score them (cf. ISO/IEC 9126, 14598).", "labels": [], "entities": [{"text": "FEMTI", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8392904996871948}, {"text": "ISO/IEC 9126, 14598", "start_pos": 150, "end_pos": 169, "type": "DATASET", "confidence": 0.7487591902414957}]}, {"text": "The CESTA evaluation methodology is founded on a black box approach.", "labels": [], "entities": []}, {"text": "CESTA evaluators considered a generic user, which is interested in general-purpose, readyto-use translations, preferably using an off-theshelf system.", "labels": [], "entities": [{"text": "CESTA evaluators", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9717589020729065}]}, {"text": "In addition, CESTA aims at producing reusable resources, and providing information about the reliability of the metrics (validation), while being cost-effective and fast.", "labels": [], "entities": []}, {"text": "With these evaluation requirements in mind (FEMTI-1), it appears that the relevant qualities (FEMTI-2) are 'suitability', 'accuracy' and 'well-formedness'.", "labels": [], "entities": [{"text": "FEMTI-1", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9755744934082031}, {"text": "FEMTI-2", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9883227944374084}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9981866478919983}]}, {"text": "Automated metrics best meet the CESTA needs for reusability, among which BLEU, X-score and D-score (chosen for internal reasons).", "labels": [], "entities": [{"text": "CESTA", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8637519478797913}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9990200996398926}, {"text": "X-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9413222074508667}, {"text": "D-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9812055826187134}]}, {"text": "Their validation requires the comparison of their scores with recognised human scores for the same qualities (e.g., human assessment of fidelity or fluency).", "labels": [], "entities": []}, {"text": "'Efficiency', measured through post-editing time, was also discussed.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 1, "end_pos": 11, "type": "METRIC", "confidence": 0.9990718364715576}]}, {"text": "For the evaluation, first a general-purpose dictionary could be used, then a domain-specific one.", "labels": [], "entities": []}, {"text": "One of the particularities of the CESTA protocol is to provide a Meta evaluation of the automated metrics used for the campaign -a kind of state of the art of evaluation metrics.", "labels": [], "entities": []}, {"text": "The robustness of the metrics will be tested on minor language pairs through a contrastive evaluation against human judgement.", "labels": [], "entities": []}, {"text": "The scientific committee has decided to use Arabic\ud97b\udf59French as a minor language pair.", "labels": [], "entities": []}, {"text": "Evaluation on the minor language pair will be carried directly on two of the participating systems and using English as a pivotal language on the other systems.", "labels": [], "entities": []}, {"text": "Translation through a pivotal language will then be the following : Arabic\ud97b\udf59English\ud97b\udf59French.", "labels": [], "entities": []}, {"text": "Organiser are, of course, perfectly aware of the potential loss of quality provoked by the use of a pivotal language but recall however that, contrarily to the major language pair, evaluation carried out on the minor language pair through a pivotal system will not be used to evaluate these systems themselves, but metric robustness.", "labels": [], "entities": []}, {"text": "Results of metric evaluation and systems evaluation will, of course, be obtained and disseminated separately.", "labels": [], "entities": [{"text": "metric evaluation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8404363095760345}]}, {"text": "During the tests of the first campaign, the French\ud97b\udf59English system obtaining the best ranking will be selected to be used as a pivotal system for metrics robustness Meta evaluation.", "labels": [], "entities": [{"text": "French\ud97b\udf59English system", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.9161982387304306}]}], "tableCaptions": []}