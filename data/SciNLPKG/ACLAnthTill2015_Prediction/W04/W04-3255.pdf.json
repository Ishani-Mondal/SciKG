{"title": [{"text": "Efficient Decoding for Statistical Machine Translation with a Fully Expanded WFST Model", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.8504104216893514}, {"text": "WFST", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.7469829320907593}]}], "abstractContent": [{"text": "This paper proposes a novel method to compile statistical models for machine translation to achieve efficient decoding.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7955906391143799}]}, {"text": "In our method, each statistical submodel is represented by a weighted finite-state transducer (WFST), and all of the submodels are expanded into a composition model beforehand.", "labels": [], "entities": []}, {"text": "Furthermore , the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding.", "labels": [], "entities": []}, {"text": "The experimental results show that the proposed model representation drastically improves the efficiency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, research on statistical machine translation has grown along with the increase in computational power as well as the amount of bilingual corpora.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7410410741964976}]}, {"text": "The basic idea of modeling machine translation was proposed by, who assumed that machine translation can be modeled on noisy channels.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7658531367778778}, {"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7628569900989532}]}, {"text": "The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language.", "labels": [], "entities": []}, {"text": "showed that the translation problem defined by is NPcomplete.", "labels": [], "entities": [{"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9742108583450317}]}, {"text": "Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process.", "labels": [], "entities": []}, {"text": "Several studies have proposed methods for searching suboptimal solutions. and proposed such depth-first search methods as stack decoders. and proposed breadth-first search methods, i.e. beam search. and proposed greedy type decoding methods.", "labels": [], "entities": [{"text": "beam search.", "start_pos": 186, "end_pos": 198, "type": "TASK", "confidence": 0.7075442373752594}]}, {"text": "In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency.", "labels": [], "entities": []}, {"text": "For model representation, a search method based on weighted finite-state transducer (WFST) ( has achieved great success in the speech recognition field.", "labels": [], "entities": [{"text": "model representation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7595870494842529}, {"text": "speech recognition", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7795042097568512}]}, {"text": "The basic idea is that each statistical model is represented by a WFST and they are composed beforehand; the composed model is optimized by WFST operations such as determinization and minimization.", "labels": [], "entities": []}, {"text": "This fully expanded model permits efficient searches.", "labels": [], "entities": []}, {"text": "Our motivation is to apply this approach to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8089956343173981}]}, {"text": "However, WFST optimization operations such as determinization are nearly impossible to apply to WFSTs in machine translation because they are much more ambiguous than speech recognition.", "labels": [], "entities": [{"text": "WFST optimization", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8997783660888672}, {"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7174809128046036}, {"text": "speech recognition", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7079988420009613}]}, {"text": "To reduce the ambiguity, we propose a WFST optimization method that considers the statistics of hypotheses while decoding.", "labels": [], "entities": [{"text": "WFST optimization", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6974494010210037}]}, {"text": "Some approaches have applied WFST to statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6976921757062277}]}, {"text": "proposed the representation of IBM model 3 with WFSTs; Bangalore and Riccardi (2001) studied WFST models in call-routing tasks, and modeled phrase-based translation by WFSTs.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 140, "end_pos": 164, "type": "TASK", "confidence": 0.7055390924215317}]}, {"text": "All of these studies mainly focused on the representation of each submodel used in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7934893071651459}]}, {"text": "However, few studies have focued on the integration of each WFST submodel to improve the decoding efficiency of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7552862465381622}]}, {"text": "To this end, we propose a method that expands all of the submodels into a composition model, reducing the ambiguity of the expanded model by the statistics of hypotheses while decoding.", "labels": [], "entities": []}, {"text": "First, we explain the translation model () that we used as abase for our decoding research.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9628015756607056}]}, {"text": "Second, our proposed method is introduced.", "labels": [], "entities": []}, {"text": "Finally, experimental results show that our proposed method drastically improves decoding efficiency. is referred to as a language model and is referred to as a translation model.", "labels": [], "entities": []}, {"text": "In this paper, we use word trigram fora language model and IBM model 3 fora translation model.", "labels": [], "entities": []}, {"text": "The translation model is represented as follows considering all possible word alignments.", "labels": [], "entities": []}, {"text": "The IBM model only assumes a one-to-many word alignment, where a Japanese word The IBM model 3 uses the following is translated to Japanese word 0 and called translation probability. is conditional probability where the English word in the is called distortion probability.", "labels": [], "entities": []}, {"text": "In our experiment, we used the IBM model 3 while assuming constant distortion probability for simplicity.", "labels": [], "entities": [{"text": "IBM model 3", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9140647649765015}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Submodel/Full-Expansion Model Size", "labels": [], "entities": [{"text": "Size", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.6391809582710266}]}, {"text": " Table 2: Static / Dynamic Composition", "labels": [], "entities": [{"text": "Static / Dynamic Composition", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8994210213422775}]}]}