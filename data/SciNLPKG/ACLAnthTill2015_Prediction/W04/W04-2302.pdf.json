{"title": [{"text": "Stochastic Language Generation in a Dialogue System: Toward a Domain Independent Generator", "labels": [], "entities": [{"text": "Stochastic Language Generation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7804513772328695}]}], "abstractContent": [{"text": "Until recently, surface generation in dialogue systems has served the purpose of simply providing a backend to other areas of research.", "labels": [], "entities": [{"text": "surface generation", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7544346749782562}]}, {"text": "The generation component of such systems usually consists of templates and canned text, providing inflexible, unnatural output.", "labels": [], "entities": []}, {"text": "To make matters worse, the resources are typically specific to the domain in question and not portable to new tasks.", "labels": [], "entities": []}, {"text": "In contrast, domain-independent generation systems typically require large grammars, full lexicons, complex collocational information, and much more.", "labels": [], "entities": []}, {"text": "Furthermore, these frameworks have primarily been applied to text applications and it is not clear that the same systems could perform well in a dialogue application.", "labels": [], "entities": []}, {"text": "This paper explores the feasibility of adapting such systems to create a domain-independent generation component useful for dialogue systems.", "labels": [], "entities": []}, {"text": "It utilizes the domain independent semantic form of The Rochester Interactive Planning System (TRIPS) with a domain independent stochas-tic surface generation module.", "labels": [], "entities": [{"text": "Rochester Interactive Planning System (TRIPS)", "start_pos": 56, "end_pos": 101, "type": "DATASET", "confidence": 0.9004913823945182}]}, {"text": "We show that a written text language model can be used to predict dialogue utterances from an over-generated word forest.", "labels": [], "entities": []}, {"text": "We also present results from a human oriented evaluation in an emergency planning domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper takes steps toward three surface generation goals in dialogue systems; to create a domainindependent surface generator, to create a surface generator that reduces dependence on large and/or domainspecific resources by using out of domain language models, and to create an effective human-like surface generator.", "labels": [], "entities": []}, {"text": "Natural Language systems are relatively young and most of today's architectures are designed and tested on specific domains.", "labels": [], "entities": []}, {"text": "It is becoming increasingly desirable to build components that are domain-independent and require a small amount of time to instantiate.", "labels": [], "entities": []}, {"text": "Unfortunately, when components are tailored to a specific domain, it requires a complete overhaul to use the architecture in anew domain.", "labels": [], "entities": []}, {"text": "While dialogue systems have found success in many areas, the backend of these systems, Natural Language Generation (NLG), has largely been ignored and used solely to show the progress of other components.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 87, "end_pos": 120, "type": "TASK", "confidence": 0.8004011313120524}]}, {"text": "However, it is now important to generate not just content-rich utterances, but also natural utterances that do not interfere with the dialogue.", "labels": [], "entities": []}, {"text": "Easy to build template-based NLG components can usually satisfy the content requirement, but their static, inflexible forms rarely facilitate an effective human oriented dialogue system.", "labels": [], "entities": []}, {"text": "Natural surface generation requires hand-crafted lexicons, grammars, ontologies, and much more to be successful.", "labels": [], "entities": [{"text": "Natural surface generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6478818655014038}]}, {"text": "The time required to create a simple surface generation component is small, but the time required to create even a mildly natural component is very large.", "labels": [], "entities": []}, {"text": "Language modeling offers hope that the information encoded in these grammars and lexicons is implicitly present in spoken and written text.", "labels": [], "entities": []}, {"text": "There have been many advances with stochastic approaches in areas that have taken advantage of the large corpora of available newswire, such as Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.8495782732963562}]}, {"text": "If newswire text (which makes up much of the available English corpora) can be applied to dialogue, we could depend lesson handcrafted grammars and domain-specific resources.", "labels": [], "entities": []}, {"text": "This paper describes an approach to surface generation in dialogue systems that uses out of domain language models; a model based on newswire text and a model based on spoken dialogue transcripts.", "labels": [], "entities": [{"text": "surface generation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7813481688499451}]}, {"text": "We also describe how this approach fits with a domain independent logical form being used for interpretation in TRIPS.", "labels": [], "entities": []}, {"text": "Our analysis of this approach shows that newswire corpora can generate not only the semantic content in its output, but also shows that it can be integrated successfully into a dialogue system, resulting in only a slight decrease in naturalness as judged by human evaluators.", "labels": [], "entities": []}, {"text": "This paper begins with a description of previous surface generation work.", "labels": [], "entities": []}, {"text": "Section 3 describes the stochastic algorithm used from the Machine Translation (MT) system, HALogen, including differences in dialogue versus newswire text.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8508700847625732}]}, {"text": "Section 4 describes the domain independence of the logical form in TRIPS and how independence is preserved in translating into the stochastic component.", "labels": [], "entities": []}, {"text": "Section 5 describes our evaluation including the language models and the domain we used for evaluation.", "labels": [], "entities": []}, {"text": "Finally, we present the results and discussion in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "This paper is evaluating two surface generation design decisions: the effectiveness of stochastic (word forest based) surface generation with domain independent language models, and the benefits of using dialogue vs. newswire models.", "labels": [], "entities": [{"text": "word forest based) surface generation", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.6381040414174398}]}, {"text": "Evaluating any natural language generation system involves many factors, but we focused on two of the most important aspects to evaluate, the content and clarity (naturalness) of the output (English utterances).", "labels": [], "entities": [{"text": "clarity", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9875208139419556}]}, {"text": "This section briefly describes previous automatic evaluation approaches that we are avoiding, followed by the human evaluation we have performed on our system.", "labels": [], "entities": []}, {"text": "Evaluating generation is particularly difficult due to the diverse amount of correct output that can be generated.", "labels": [], "entities": [{"text": "Evaluating generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9569419324398041}]}, {"text": "There are many ways to present a given semantic representation in English and what determines quality of content and form are often subjective measures.", "labels": [], "entities": []}, {"text": "There are two general approaches to a surface generation evaluation.", "labels": [], "entities": [{"text": "surface generation evaluation", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.8111181656519572}]}, {"text": "The first uses human evaluators to score the output with some pre-defined ranking measure.", "labels": [], "entities": []}, {"text": "The second uses a quantitative automatic approach usually based on n-gram presence and word ordering.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.7072390615940094}]}, {"text": "Bangalore et al. describe some of the quantitative measures that have been used in ( ).", "labels": [], "entities": []}, {"text": "Callaway recently used quantitative measures in an evaluation between symbolic and stochastic surface generators in.", "labels": [], "entities": []}, {"text": "The most common quantitative measure is Simple String Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.6270542740821838}]}, {"text": "This metric uses an ideal output string and compares it to a generated string using a metric that combines three word error counts; insertion, deletion, and substitution.", "labels": [], "entities": []}, {"text": "One variation on this approach is tree-based metrics.", "labels": [], "entities": []}, {"text": "These attempt to better represent how bad a bad result is.", "labels": [], "entities": []}, {"text": "The tree-based accuracy metrics do not compare two strings directly, but instead build a dependency tree for the ideal string and attempt to create the same dependency tree from the generated string.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.983992874622345}]}, {"text": "The score is dependent not only on word choice, but on positioning at the phrasal level.", "labels": [], "entities": []}, {"text": "Finally, the most recent evaluation metric is the Bleu Metric from IBM().", "labels": [], "entities": [{"text": "Bleu", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.8517573475837708}]}, {"text": "Designed for Machine Translation, it scores generated sentences based on the n-gram appearance from multiple ideal sentences.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8038790822029114}]}, {"text": "This approach provides more than one possible realization of an LF and compares the generated sentence to all possibilities.", "labels": [], "entities": []}, {"text": "Unfortunately, the above automatic metrics are very limited in mimicking human scores.", "labels": [], "entities": []}, {"text": "The Bleu metric can give reasonable scores, but the results are not as good when only one human translation is available.", "labels": [], "entities": [{"text": "Bleu metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.885328084230423}]}, {"text": "These automatic metrics all compare the desired output with the actual output.", "labels": [], "entities": []}, {"text": "We decided to ignore this evaluation because it is too dependent on syntactic likeness.", "labels": [], "entities": []}, {"text": "The following two sentences represent the same semantic meaning yet appear very different in structure: The injured person is still waiting at the hospital.", "labels": [], "entities": []}, {"text": "The person with the injury at the hospital is still waiting.", "labels": [], "entities": []}, {"text": "The scoring metrics would judge very harshly, yet a human evaluator should see little difference in semantic content.", "labels": [], "entities": []}, {"text": "Clearly, the first is indeed better in naturalness (closeness to human English dialogue), but both content and naturalness cannot be measured with the current quantitative (and many human study) approaches.", "labels": [], "entities": []}, {"text": "Although it is very time consuming, human evaluation continues to be the gold standard for generation evaluation.", "labels": [], "entities": [{"text": "generation evaluation", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7520307302474976}]}, {"text": "Our evaluation does not compare an ideal utterance with a generated one.", "labels": [], "entities": []}, {"text": "We use areal human-human dialogue transcript and replace every utterance of one of the participants with our generated output.", "labels": [], "entities": []}, {"text": "The evaluators are thereby reading a dialogue between a human and a computer generated human, yet it is based on the original human-human dialogue.", "labels": [], "entities": []}, {"text": "Through this approach, we can present the evaluators with both our generated and the original transcripts (as the control group).", "labels": [], "entities": []}, {"text": "However, they do not know which is artificial, or even that any of them are not human to human.", "labels": [], "entities": []}, {"text": "The results will give an accurate portrayal of how well the system generates dialogue.", "labels": [], "entities": []}, {"text": "The two aspects of dialogue that the evaluators were asked to measure for each utterance were understandability (semantically within context) and naturalness.", "labels": [], "entities": []}, {"text": "There have been many metrics used in the past.", "labels": [], "entities": []}, {"text": "Metrics range from scoring each utterance with a subjective score (Good,Bad) to using a numeric scale.", "labels": [], "entities": []}, {"text": "Our evaluators use a numeric scale from 0 to 5.", "labels": [], "entities": []}, {"text": "The main motivation for this is so we can establish averages and performance results more easily.", "labels": [], "entities": []}, {"text": "The final step is to obtain a suitable domain of study outside the typical air travel domain.", "labels": [], "entities": []}], "tableCaptions": []}