{"title": [{"text": "Spelling correction as an iterative process that exploits the collective knowledge of web users", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9911420941352844}]}], "abstractContent": [{"text": "Logs of user queries to an internet search engine provide a large amount of implicit and explicit information about language.", "labels": [], "entities": []}, {"text": "In this paper, we investigate their use in spelling correction of search queries, a task which poses many additional challenges beyond the traditional spelling correction problem.", "labels": [], "entities": [{"text": "spelling correction of search queries", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.8808477282524109}, {"text": "spelling correction", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.7182277143001556}]}, {"text": "We present an approach that uses an iterative transformation of the input query strings into other strings that correspond to more and more likely queries according to statistics extracted from internet search query logs.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of general purpose spelling correction has along history (e.g., traditionally focusing on resolving typographical errors such as insertions, deletions, substitutions, and transpositions of letters that result in unknown words (i.e. words not found in a trusted lexicon of the language).", "labels": [], "entities": [{"text": "general purpose spelling correction", "start_pos": 12, "end_pos": 47, "type": "TASK", "confidence": 0.683815136551857}, {"text": "resolving typographical errors such as insertions, deletions, substitutions, and transpositions of letters that result in unknown words (i.e. words not found in a trusted lexicon", "start_pos": 99, "end_pos": 277, "type": "Description", "confidence": 0.7633878465356498}]}, {"text": "Typical word processing spell checkers compute for each unknown word a small set of in-lexicon alternatives to be proposed as possible corrections, relying on information about in-lexicon-word frequencies and about the most common keyboard mistakes (such as typing m instead of n) and phonetic/cognitive mistakes, both at word level (e.g. the use of acceptible instead of acceptable) and at character level (e.g. the misuse off instead of ph).", "labels": [], "entities": [{"text": "word processing spell checkers", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.8152638971805573}]}, {"text": "Very few spell checkers attempt to detect and correct word substitution errors, which refer to the use of in-lexicon words in inappropriate contexts and can also be the result of both typographical mistakes (such as typing coed instead of cord) and cognitive mistakes (e.g. principal and principle).", "labels": [], "entities": []}, {"text": "Some research efforts to tackle this problem have been made; for example and developed systems that rely on syntactic patterns to detect substitution errors, while employed word co-occurrence evidence from a large corpus to detect and correct such errors.", "labels": [], "entities": []}, {"text": "The former approaches were based on the impractical assumption that all possible syntactic uses of all words (i.e. part-of-speech) are known, and presented both recall and precision problems because many of the substitution errors are not syntactically anomalous and many unusual syntactic constructions do not contain errors.", "labels": [], "entities": [{"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9981592297554016}, {"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9980594515800476}]}, {"text": "The latter approach had very limited success under the assumptions that each sentence contains at most one misspelled word, each misspelling is the result of a single point change (insertion, deletion, substitution, or transposition), and the defect rate (the relative number of errors in the text) is known.", "labels": [], "entities": [{"text": "defect rate", "start_pos": 243, "end_pos": 254, "type": "METRIC", "confidence": 0.9456493258476257}]}, {"text": "A different body of work (e.g.) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", "labels": [], "entities": [{"text": "context sensitive spelling correction (CSSC)", "start_pos": 124, "end_pos": 168, "type": "TASK", "confidence": 0.7453766890934536}]}, {"text": "Although promising results were obtained (92-95% accuracy), the scope of this work was very limited as it only addressed known sets of commonly confused words, such as {peace, piece}).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.998514711856842}]}], "datasetContent": [{"text": "For this work, we are concerned primarily with recall because providing good suggestions for misspelled queries can be viewed as more important than abstaining to provide alternative query suggestions for valid queries as long as these suggestions are reasonable (for example, suggesting cowboy ropes for cowboy robes may not have major cost to a user).", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9954397678375244}]}, {"text": "A real system would have a component that decides whether to surface a spelling suggestion based on where we want to be on the ROC curve, thus negotiating between precision and recall.", "labels": [], "entities": [{"text": "ROC curve", "start_pos": 127, "end_pos": 136, "type": "DATASET", "confidence": 0.6579155623912811}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9990948438644409}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9949000477790833}]}, {"text": "One problem with evaluating a spellchecker designed to correct search queries is that evaluation data is hard to get.", "labels": [], "entities": []}, {"text": "Even if the system were used by a search engine and click-through information were available, such information would provide only a crude measure of precision and would not allow us to measure recall, by capturing only cases in which the corrections proposed by that particular speller are clicked on by the users.", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9992351531982422}, {"text": "recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9979749321937561}]}, {"text": "We performed two different evaluations of the proposed system.", "labels": [], "entities": []}, {"text": "The first evaluation was done on a test set comprising 1044 unique randomly sampled queries from a daily query log, which were annotated by two annotators.", "labels": [], "entities": []}, {"text": "Their interagreement rate was 91.3%.", "labels": [], "entities": [{"text": "interagreement", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7776645421981812}]}, {"text": "864 of these queries were considered valid by both annotators; for the other 180, the annotators provided spelling corrections.", "labels": [], "entities": []}, {"text": "The overall agreement of our system with the annotators was 81.8%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.990752100944519}]}, {"text": "The system suggested 131 alternative queries for the valid set, counted as false positives, and 156 alternative queries for the misspelled set.", "labels": [], "entities": []}, {"text": "shows the accuracy obtained by the proposed system and results from an ablation study where we disabled various components of the system, to measure their influence on performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995468258857727}]}, {"text": "By completely removing the trusted lexicon, the accuracy of the system on misspelled queries (61.1%) was higher than in the case of only using a trusted lexicon and no query log data (52.8%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9997373223304749}]}, {"text": "It can also be observed that the language model built using query logs is by far more important than the channel model employed: using a poorer character error model by setting all edit weights equal did not have a major impact on performance (66.1% recall), while using a poorer language model that only employs unigram statistics from the query logs crippled the system (41.7% recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 250, "end_pos": 256, "type": "METRIC", "confidence": 0.9973410964012146}, {"text": "recall", "start_pos": 379, "end_pos": 385, "type": "METRIC", "confidence": 0.9935261011123657}]}, {"text": "Another interesting aspect is related to the number of iterations.", "labels": [], "entities": []}, {"text": "Because the first iteration is more conservative than the following iterations, using only one iteration led to fewer false positives but also to a much lower recall (47.2%).", "labels": [], "entities": [{"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9996988773345947}]}, {"text": "Two iterations were sufficient to correct most of the misspelled queries that the full system could correct.", "labels": [], "entities": []}, {"text": "While fringes did not have a major impact on recall, they helped avoid false positives (and had a major impact on speed).", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9981667995452881}, {"text": "speed", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9991835951805115}]}, {"text": "Accuracy and recall as functions of the number of monthly query logs used to train the language model shows the performance of the full system as a function of the number of monthly query logs employed.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9927207827568054}, {"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9990720748901367}]}, {"text": "While both the total accuracy and the recall increased when using 2 months of data instead of 1 month, by using more query log data (3 and 4 month), the recall (or accuracy on misspelled queries) still improves but at the expense of having more false positives for valid queries, which leads to an overall slightly smaller accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9842422604560852}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9995511174201965}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9992009997367859}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9966912269592285}, {"text": "accuracy", "start_pos": 323, "end_pos": 331, "type": "METRIC", "confidence": 0.9960357546806335}]}, {"text": "A post-analysis of the results showed that the system suggested in many cases reasonable corrections but different from the gold standard ones.", "labels": [], "entities": [{"text": "corrections", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.9531663060188293}]}, {"text": "Many false positives could be considered reasonable suggestions, although it is not clear whether they would have been helpful to the users (e.g. 2002 kawasaki ninja zx6e \ud97b\udf59 2002 kawasaki ninja zx6r was counted as an error, although the suggestion represents a more popular motorcycle model).", "labels": [], "entities": []}, {"text": "In the case of misspelled queries in which the user's intent was not clear, the suggestion made by the system could be considered valid despite the fact that it disagreed with the annotators' choice (e.g. gogle \ud97b\udf59 google instead of the gold standard correction goggle).", "labels": [], "entities": []}, {"text": "To address the problems generated by the fact that the annotators could only guess the user intent, we performed a second evaluation, on a set of queries randomly extracted from query log data, by sampling pairs of successive queries ) , ( 2 1 q q sent by the same users in which the queries differ from one another by an un-weighted edit distance of at most 1+(len( 1 q )+len( 2 q ))/10 (i.e. allow a point change for every 5 letters).", "labels": [], "entities": []}, {"text": "Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated The main system disagreed 99 times with the gold standard, in 80 of these cases suggesting a different correction.", "labels": [], "entities": []}, {"text": "40 of the corrections were not appropriate (e.g. porat was corrected by our system to pirate instead of port in chinese porat also called xiamen), 15 were functionally equivalent corrections given our target search engine (e.g. audio flie \ud97b\udf59 audio files instead of audio file), 17 were different valid suggestions (e.g. bellsouth lphone isting \ud97b\udf59 bellsouth phone listings instead of bellsouth telephone listing), while 8 represented gold standard errors (e.g. the speller correctly suggested brandy sniffters \ud97b\udf59 brandy snifters instead of brandy sniffers).", "labels": [], "entities": []}, {"text": "Out of 19 cases in which the system did not make a suggestion, 13 were genuine errors (e.g. paul waskiewiscz with the correct spelling paul waskiewicz), 4 were cases in which the original input was correct, although different from the user's intent (e.g. cooed instead of coed) and 2 were gold standard errors (e.g. commandos 3 walkthrough had the wrong correction commando 3 walkthrough, as this query refers to a popular videogame called \"commandos 3\").", "labels": [], "entities": []}, {"text": "Differences Gold std errors Format Diff.", "labels": [], "entities": [{"text": "Differences Gold std errors Format Diff.", "start_pos": 0, "end_pos": 40, "type": "METRIC", "confidence": 0.8176029324531555}]}, {"text": "valid Real Errors 80+19 8+2 15+0 17+4 40+13 The above table shows a synthesis of this error analysis on the second evaluation set.", "labels": [], "entities": [{"text": "Real Errors", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.8908846080303192}]}, {"text": "The first number in each column refers to a precision error (i.e. the speller suggested something different than the gold standard), while the second refers to a recall error (i.e. no suggestion).", "labels": [], "entities": [{"text": "precision error", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.977226048707962}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.996491014957428}]}, {"text": "As a result of this error analysis, we could arguably consider that while the agreement with the gold standard experiments are useful for measuring the relative importance of components, they do not give us an absolute measure of system usefulness/accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9933499693870544}]}], "tableCaptions": [{"text": " Table 2. Accuracy of various instantiations of the system", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984849095344543}]}, {"text": " Table 3. Accuracy of the proposed system on a set which  contains misspelled queries that the users had reformulated", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9893297553062439}]}]}