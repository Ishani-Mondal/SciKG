{"title": [{"text": "Using Higher-level Linguistic Knowledge for Speech Recognition Error Correction in a Spoken Q/A Dialog", "labels": [], "entities": [{"text": "Speech Recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7265870422124863}]}], "abstractContent": [{"text": "Speech interface is often required in many application environments such as telephone-based information retrieval, car navigation systems , and user-friendly interfaces, but the low speech recognition rate makes it difficult to extend its application to new fields.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7179297655820847}]}, {"text": "Several approaches to increase the accuracy of the recognition rate have been researched by error correction of the recognition results, but previous approaches were mainly lexical-oriented ones in post error correction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9988049268722534}]}, {"text": "We suggest an improved syllable-based model and anew semantic-oriented approach to correct both semantic and lexical errors, which is also more accurate for especially domain-specific speech error correction.", "labels": [], "entities": [{"text": "domain-specific speech error correction", "start_pos": 168, "end_pos": 207, "type": "TASK", "confidence": 0.5954643189907074}]}, {"text": "Through extensive experiments using a speech-driven in-vehicle telem-atics information retrieval, we demonstrate the superior performance of our approach and some advantages over previous lexical-oriented approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "New application environments such as telephone-based retrieval, car navigation systems, and mobile information retrieval, often require speech interface to conveniently process user queries.", "labels": [], "entities": [{"text": "car navigation", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7300118207931519}, {"text": "mobile information retrieval", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.6285742521286011}]}, {"text": "In these environments, keyboard input is inconvenient or sometimes impossible because of spatial limitation on mobile devices and instability in manipulating the devices.", "labels": [], "entities": []}, {"text": "However, because of the low recognition rate in current speech recognition systems, the performance of speech applications such as speech-driven information retrieval (IR) and question answering (QA), and speech dialogue systems is very low.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7412518262863159}, {"text": "speech-driven information retrieval (IR)", "start_pos": 131, "end_pos": 171, "type": "TASK", "confidence": 0.7638519704341888}, {"text": "question answering (QA)", "start_pos": 176, "end_pos": 199, "type": "TASK", "confidence": 0.8450544059276581}]}, {"text": "The performance of the serially connected spoken QA system, based on the QA system from text input which has 76% performance and the output of the ASR which operated at a 30% WER, was only 7% ().", "labels": [], "entities": [{"text": "WER", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9937018156051636}]}, {"text": "() exposes several fundamental flaws of this simple combination of an automatic speech recognition (ASR) and QA system, including the importance of named entity information, and the inadequacies of current speech recognition technology based on n-gram language models.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.7942335108915964}, {"text": "speech recognition", "start_pos": 206, "end_pos": 224, "type": "TASK", "confidence": 0.7496463358402252}]}, {"text": "The major problem of speech-driven IR and QA is the decreasing of the performance due to the recognition errors in ASR systems.", "labels": [], "entities": [{"text": "IR", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.883788526058197}, {"text": "ASR", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9668247103691101}]}, {"text": "Erroneously recognized spoken queries drop the precision and recall of IR and QA system.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9995651841163635}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9991042017936707}]}, {"text": "Some authors investigated the relation of ASR errors and precision of IR ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9409908652305603}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9994370341300964}, {"text": "IR", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.5302610993385315}]}, {"text": "They evaluated the effectiveness of the IR systems through various error rates using 35 queries of TREC.", "labels": [], "entities": [{"text": "IR", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9809780120849609}, {"text": "TREC", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.5297314524650574}]}, {"text": "Their researches show that the increasing word error rate (WER) quickly decreases the precision of IR.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.8900586267312368}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9995641112327576}, {"text": "IR", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9612502455711365}]}, {"text": "Another group investigated the performance of spoken queries in NTCIR collections (.", "labels": [], "entities": [{"text": "NTCIR collections", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.801780104637146}]}, {"text": "They evaluated a variety of speakers, and calculated the error rate with respect to a query term, which is a keyword used for the retrieval.", "labels": [], "entities": [{"text": "error rate", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9640520811080933}]}, {"text": "They showed that the WER of the query terms was generally higher than that of the general words irrespective of the speakers.", "labels": [], "entities": [{"text": "WER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9490572810173035}]}, {"text": "In other words, recognition of content words related to the IR and QA performance was more difficult than that of normal words.", "labels": [], "entities": [{"text": "recognition of content words", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.8553362786769867}, {"text": "IR and QA", "start_pos": 60, "end_pos": 69, "type": "TASK", "confidence": 0.5754043857256571}]}, {"text": "So, they introduced a method to improve the precision of speechdriven IR by suggesting anew type of IR system tightlyintegrated with a speech input interface ().", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9988741278648376}]}, {"text": "In their system, document collection provides an adaptation of the language model of the ASR, which results in a drop of the word error rate.", "labels": [], "entities": [{"text": "document collection", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7042173892259598}, {"text": "ASR", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8454283475875854}, {"text": "word error rate", "start_pos": 125, "end_pos": 140, "type": "METRIC", "confidence": 0.7098287145296732}]}, {"text": "For this reason, some appropriate adaptation techniques are required for overcoming speech recognition errors such as post error correction.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.690120130777359}]}, {"text": "ASR error correction can be one of the domain adaptation techniques to improve the recognition accuracy, and the primary advan-: Adaptation via Post Error Correction tage of the error correction approach is its independence of the specific speech recognizer.", "labels": [], "entities": [{"text": "ASR error correction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9153337876001993}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9706897735595703}]}, {"text": "If the speech recognizer can be regarded as a black-box, we can perform robust and flexible domain adaptation through the post error correction process.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.6933080554008484}]}, {"text": "shows the paradigm of this post error correction approach.", "labels": [], "entities": [{"text": "post error correction", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.5730314056078593}]}, {"text": "One approach in post error correction, which is a straightforward and intuitive method to robustly handle many kinds of recognition errors, was rule-based approach () collected many lexical error patterns that occurred in a speech translation system in Japanese.", "labels": [], "entities": [{"text": "post error correction", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.6793453296025594}]}, {"text": "They could correct any type of errors by matching the strings in the transcription with lexical error patterns in the database.", "labels": [], "entities": []}, {"text": "However, their approach has a disadvantage in that the correction is only feasible to the trained (or collected) lexical error patterns.", "labels": [], "entities": []}, {"text": "Another approach has been based on a statistical method utilizing the probabilistic information of words in a spoken dialogue situation and the language models adapted to the application domain ( . ( ) applied the noisy channel model to the correction of the errors in speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 269, "end_pos": 287, "type": "TASK", "confidence": 0.7084721773862839}]}, {"text": "They simplified a statistical machine translation (MT) model called an IBM model (, and tried to construct a general post-processor that can correct errors generated by any speech recognizer.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7812720934549967}]}, {"text": "The model consists of two parts: a channel model, which accounts for errors made by the ASR, and the language model, which accounts for the likelihood of a sequence of words being uttered.", "labels": [], "entities": [{"text": "ASR", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.8844764232635498}]}, {"text": "They trained the channel model and the language model both using some transcriptions from TRAINS-95 dialogue system which is a train traveling planning system . Here, the channel model has the distribution that an original word maybe recognized as an erroneous word.", "labels": [], "entities": [{"text": "TRAINS-95 dialogue system", "start_pos": 90, "end_pos": 115, "type": "DATASET", "confidence": 0.8769607941309611}]}, {"text": "They use the probability of mistakenly recognized words, the co-occurrence information extracted from the words and their neighboring words, and the tagged word bi-grams, which are all lexical clues in error strings.", "labels": [], "entities": []}, {"text": "Such approaches based on lexical information of words have shown some successful results, but they still have major drawbacks; The performance of such systems depends on the size and the quality of speech recognition result, or on the database of collected error strings since they are directly dependent on lexical items.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.7124879658222198}]}, {"text": "The error patterns constructed are available but not enough, because it is expensive to collect them; so in many cases, they fail to recover the original strings from the lexical specific error patterns.", "labels": [], "entities": []}, {"text": "Also, since they are sensitive to the error patterns, they occasionally mis-identify a correct word as an error word.", "labels": [], "entities": []}, {"text": "We suggest a more improved and robust semanticoriented error correction approach, which can be integrated into previous fragile lexical-based approaches.", "labels": [], "entities": [{"text": "semanticoriented error correction", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.5787316858768463}]}, {"text": "In our approach, in addition to lexical information, we use high level syntactic and semantic information of the words in a speech transcription.", "labels": [], "entities": []}, {"text": "We obtain semantic information from a knowledge base such as general thesauri and a special domain dictionary that we construct by ourselves to contain some domain specific knowledge to the target application.", "labels": [], "entities": []}, {"text": "In the next section, we first describe a general noisy channel model for ASR error correction and discuss some problems with them.", "labels": [], "entities": [{"text": "ASR error correction", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8695770303408304}]}, {"text": "We then introduce our improved channel model especially for Korean language in section 3.", "labels": [], "entities": []}, {"text": "We also propose anew high-level error correction model using syntactic and semantic knowledge in section 4.", "labels": [], "entities": [{"text": "high-level error correction", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6369496484597524}]}, {"text": "We prove the feasibility of our approach through some experiments in section 5, and draw some conclusions in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed several experiments on the domain of invehicle telematics IR related to navigation question answering services.", "labels": [], "entities": [{"text": "invehicle telematics IR", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.5685372749964396}, {"text": "navigation question answering", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.8766346573829651}]}, {"text": "The speech transcripts used in the experiments were composed of 462 queries, which were collected by 1 male speaker in areal application.", "labels": [], "entities": []}, {"text": "We also used two Korean speech recognizers: a speech recognizer made by LG-Elite (LG Electronics Institute of Technology) and a Korean commercial speech recognizer, ByVoice (refer to http://www.voicetech.co.kr).", "labels": [], "entities": [{"text": "LG-Elite (LG Electronics Institute of Technology)", "start_pos": 72, "end_pos": 121, "type": "DATASET", "confidence": 0.8523922860622406}, {"text": "ByVoice", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9254705905914307}]}, {"text": "For: Example of Semantic-oriented Error Correction our semantic-oriented error correction, we constructed a domain knowledge for our target domain.", "labels": [], "entities": [{"text": "semantic-oriented error correction", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.6032267312208811}]}, {"text": "We constructed 3,195 entries of domain dictionary, 13,154 entries of ontology dictionary, and 436 semantic templates generated automatically using domain dictionary and ontology dictionary.", "labels": [], "entities": []}, {"text": "We implemented both word-based and syllable-based model for comparison, and combined the system of syllable-based lexical correction with the LSP-based semantic error correction.", "labels": [], "entities": [{"text": "syllable-based lexical correction", "start_pos": 99, "end_pos": 132, "type": "TASK", "confidence": 0.6560779313246409}, {"text": "LSP-based semantic error correction", "start_pos": 142, "end_pos": 177, "type": "TASK", "confidence": 0.4760253578424454}]}, {"text": "For experiments, we use trigrams language model generated by SRILM toolkit, and a training program for channel model made by ourselves.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9153532981872559}]}, {"text": "And, we divided the 462 queries into 6 different sets, and evaluated the results of 6-fold cross validation for each model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Result of LG-Elite Recognizer", "labels": [], "entities": []}, {"text": " Table 4: Result of Term Error Rate", "labels": [], "entities": [{"text": "Result of Term Error Rate", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6640895366668701}]}]}