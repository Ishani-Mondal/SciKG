{"title": [{"text": "Efficient incremental beam-search parsing with generative and discriminative models", "labels": [], "entities": [{"text": "beam-search parsing", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6870106756687164}]}], "abstractContent": [{"text": "Extended Abstract: This talk will present several issues related to incre-mental (left-to-right) beam-search parsing of natural language using generative or discriminative models , either individually or in combination.", "labels": [], "entities": [{"text": "beam-search parsing of natural language", "start_pos": 97, "end_pos": 136, "type": "TASK", "confidence": 0.8179563283920288}]}, {"text": "The first part of the talk will provide background in incre-mental top-down and (selective) left-corner beam-search parsing algorithms, and in stochastic models for such derivation strategies.", "labels": [], "entities": [{"text": "beam-search parsing", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.6665278971195221}]}, {"text": "Next, the relative benefits and drawbacks of generative and discriminative models with respect to heuristic pruning and search will be discussed.", "labels": [], "entities": []}, {"text": "A range of methods for using multiple models during incremental parsing will be detailed.", "labels": [], "entities": []}, {"text": "Finally, we will discuss the potential for effective use of fast, finite-state processing, e.g. part-of-speech tagging, to reduce the parsing search space without accuracy loss.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7129992097616196}, {"text": "parsing search", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.8845621347427368}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9902819395065308}]}, {"text": "POS-tagging is shown to improve efficiency by as much as 20-25 percent with the same accuracy, largely due to the treatment of unknown words.", "labels": [], "entities": [{"text": "efficiency", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9728278517723083}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9986427426338196}]}, {"text": "In contrast, an 'islands-of-certainty' approach, which quickly annotates labeled bracketing over low-ambiguity word sequences, is shown to provide little or no efficiency gain over the existing beam-search.", "labels": [], "entities": []}, {"text": "The basic parsing approach that will be described in this talk is stochastic incremental top-down parsing , using a beam-search to prune the search space.", "labels": [], "entities": []}, {"text": "Grammar induction occurs from an annotated tree-bank, and non-local features are extracted from each derivation to enrich the stochastic model.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7399637699127197}]}, {"text": "Left-corner grammar and tree transforms can be applied to the treebank or the induced grammar, either fully or selectively , to change the derivation order while retaining the same underlying parsing algorithm.", "labels": [], "entities": []}, {"text": "This approach has been shown to be accurate, relatively efficient , and robust using both generative and discrim-inative models (Roark, 2001; Roark, 2004; Collins and Roark, 2004).", "labels": [], "entities": []}, {"text": "The key to effective beam-search parsing is comparability of analyses when the pruning is done.", "labels": [], "entities": [{"text": "beam-search parsing", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.9088955223560333}]}, {"text": "If two competing parses are at different points in their respective derivations, e.g. one is near the end of the derivation and another is near the beginning, then it will be difficult to evaluate which of the two is likely to result in a better parse.", "labels": [], "entities": []}, {"text": "With a generative model, comparability can be accomplished by the use of a look-ahead statistic, which estimates the amount of probability mass required to extend a given derivation to include the word(s) in the look-ahead.", "labels": [], "entities": []}, {"text": "Every step in the derivation decreases the probability of the derivation, but also takes the derivation one step closer to attaching to the look-ahead.", "labels": [], "entities": []}, {"text": "For good parses, the look-ahead statistic should increase with each step of the derivation, ensuring a certain degree of comparability among competing parses with the same look-ahead.", "labels": [], "entities": []}, {"text": "Beam-search parsing using an unnormalized dis-criminative model, as in Collins and Roark (2004), requires a slightly different search strategy than the original generative model described in Roark (2001; 2004).", "labels": [], "entities": [{"text": "Beam-search parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6774869859218597}]}, {"text": "This alternate search strategy is closer to the approach taken in Costa et al.", "labels": [], "entities": []}, {"text": "(2001; 2003), in that it enumerates a set of possible ways of attaching the next word before evaluating with the model.", "labels": [], "entities": []}, {"text": "This ensures comparability for models that do not have the sort of behavior described above for the generative models, rendering look-ahead statistics difficult to estimate.", "labels": [], "entities": []}, {"text": "This approach is effective, although somewhat less so than when a look-ahead statistic is used.", "labels": [], "entities": []}, {"text": "A generative parsing model can be used on its own, and it was shown in Collins and Roark (2004) that a discriminative parsing model can be used on its own.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.95674067735672}]}, {"text": "Most discriminative parsing approaches, e.g. (Johnson et al., 1999; Collins, 2000; Collins and Duffy, 2002), are re-ranking approaches, in which another model (typically a generative model) presents a relatively small set of candidates, which are then re-scored using a second, discriminatively trained model.", "labels": [], "entities": [{"text": "discriminative parsing", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.660698652267456}]}, {"text": "There are other ways to combine a generative and discriminative model apart from waiting for the former to provide a set of completed candidates to the latter.", "labels": [], "entities": []}, {"text": "For example, the scores can be used simultaneously; or the generative model can present candidates to the discriminative model at intermediate points in the string, rather than simply at the end.", "labels": [], "entities": []}, {"text": "We discuss these options and their potential benefits.", "labels": [], "entities": []}, {"text": "Finally, we discuss and present a preliminary evaluation of the use of rapid finite-state tagging to reduce the parsing search space, as was done in (Rat-naparkhi, 1997; Ratnaparkhi, 1999).", "labels": [], "entities": [{"text": "parsing search", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.9045593738555908}]}, {"text": "When the parsing algorithm is integrated with model training, such efficiency improvements can be particularly important.", "labels": [], "entities": [{"text": "parsing", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.9725043773651123}]}, {"text": "POS-tagging using a simple bi-tag model improved parsing efficiency by nearly 25 percent without a loss inaccuracy, when 1.2 tags per word were produced on average by the tagger.", "labels": [], "entities": []}, {"text": "Producing a single tag sequence for each string resulted in further speedups, but at the loss of 1-2 points of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9980655312538147}]}, {"text": "We show that much, but not all, of the speedup from POS-tagging is due to more constrained tagging of unknown words.", "labels": [], "entities": []}, {"text": "Ina second set of trials, we make use of what we are calling 'syntactic collocations', i.e. collo-cations that are (nearly) unambiguously associated with a particular syntactic configuration.", "labels": [], "entities": []}, {"text": "For example , a chain of auxiliaries in English will always combine in a particular syntactic configuration, mod-ulo noise in the annotation.", "labels": [], "entities": []}, {"text": "In our approach, the labeled bracketing spanning the sub-string is treated as a tag for the sequence.", "labels": [], "entities": []}, {"text": "A simple, finite-state method for finding such collocations, and an efficient longest match algorithm for labeling strings will be presented.", "labels": [], "entities": []}, {"text": "The labeled-bracketing 'tags' are integrated with the parse search as follows: when a derivation reaches the first word of such a colloca-tion, the remaining words are attached in the given configuration.", "labels": [], "entities": []}, {"text": "This has the effect of extending the look-ahead beyond the collocation, as well as potentially reducing the amount of search required to extend the derivations to include the words in the collocation.", "labels": [], "entities": []}, {"text": "However, while POS-tagging improved efficiency, we find that using syntactic collocations does not, indicating that 'islands-of-certainty' approaches are not what is needed from shallow processing ; rather genuine dis-ambiguation of the sort provided by the POS-tagger.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}