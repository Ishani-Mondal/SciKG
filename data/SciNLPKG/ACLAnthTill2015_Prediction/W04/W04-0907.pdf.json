{"title": [{"text": "Making Sense of Japanese Relative Clause Constructions", "labels": [], "entities": [{"text": "Making Sense of Japanese Relative Clause Constructions", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7212026417255402}]}], "abstractContent": [{"text": "We apply the C4.5 decision tree learner in interpreting Japanese relative clause constructions, based around shallow syntactic and semantic processing.", "labels": [], "entities": [{"text": "interpreting Japanese relative clause constructions", "start_pos": 43, "end_pos": 94, "type": "TASK", "confidence": 0.9006067395210267}]}, {"text": "In parameterising data for use with C4.5, we propose and test various means of reducing intra-clausal interpretational ambiguity, and cross indexing the overall analysis of cosubordinated relative clause constructions.", "labels": [], "entities": []}, {"text": "We additionally investigate the disambiguating effect of the different parameter types used, and establish upper bounds for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Japanese relative clause constructions have the general structure[NP]], and constitute a noun phrase.", "labels": [], "entities": []}, {"text": "We will term the modifying S the \"relative clause\", the modified NP the \"head NP\", and the overall NP a \"relative clause construction\" or RCC.", "labels": [], "entities": []}, {"text": "Example RCCs are: (1) kin\u00af o yesterday katta bought b\u00af osi hat \"the hat which ( ) bought yesterday\" Different claims have been made as to the roles of syntax, semantics and pragmatics (or frame semantics) in the construal of Japanese RCCs (e.g.,,).", "labels": [], "entities": []}, {"text": "We consider two basic syntacticosemantic selection processes to govern RCC construal: selection of the relative clause by the head NP and selection of the head NP by the relative The following abbreviations are used in glosses: NOM = nominative, ACC = accusative, PRES = non-past and POT = potential.", "labels": [], "entities": [{"text": "NOM", "start_pos": 228, "end_pos": 231, "type": "METRIC", "confidence": 0.9098722338676453}, {"text": "PRES", "start_pos": 264, "end_pos": 268, "type": "METRIC", "confidence": 0.9723692536354065}, {"text": "POT", "start_pos": 284, "end_pos": 287, "type": "METRIC", "confidence": 0.9862171411514282}]}, {"text": "( ) is used to indicate zero (anaphoric) arguments. clause.", "labels": [], "entities": []}, {"text": "There is a close relationship between syntax and semantics here, in that syntax provides the basic argument and modifier positions for the head verb of the relative clause, which semantics fleshes out byway of selectional restrictions.", "labels": [], "entities": []}, {"text": "Pragmatics also has a role to play in rating the plausibility of different interpretations), although we ignore its effects, and indeed the impact of context, in this research.", "labels": [], "entities": []}, {"text": "Our objective in this paper is, given a taxonomy of Japanese RCC semantic types and a gold-standard set of Japanese RCC instances, to investigate the success of various parameter configurations in interpreting RCCs.", "labels": [], "entities": [{"text": "interpreting RCCs", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.8875367641448975}]}, {"text": "One feature of the proposed method is that it is based on shallow analysis, centring principally around a basic case frame and verb class description.", "labels": [], "entities": []}, {"text": "That is, we attempt to make maximum use of surface information in performing a deep semantic task, in the same vein, e.g., as for English verb classification and Lapata in disambiguating nominalisations.", "labels": [], "entities": [{"text": "English verb classification", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.6022598743438721}]}, {"text": "Relative clause interpretation is a core component of text understanding, as demonstrated in the context of the MUC conference series.", "labels": [], "entities": [{"text": "Relative clause interpretation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9072995781898499}, {"text": "text understanding", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.890924870967865}, {"text": "MUC conference series", "start_pos": 112, "end_pos": 133, "type": "DATASET", "confidence": 0.8887892365455627}]}, {"text": "It also has immediate applications in, e.g., Japanese-English machine translation: for case-slot gapping RCCs such as (1), we extrapose the head NP from the appropriate argument position in the English relative clause (producing, e.g., \"the hat [\u00a1 bought yesterday]\"), and for attributive RCCs such as (2), we generate the English relative clause without extraposition and select the relative pronoun according to the head NP (producing, e.g., \"the reason that the hat was bought\").", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6910559982061386}]}, {"text": "RCC interpretation is dogged by analytical ambiguity, in particular for phrase boundary, phrase head/attachment and word sense ambiguity.", "labels": [], "entities": [{"text": "RCC interpretation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7940155863761902}, {"text": "phrase head/attachment", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.6232310086488724}]}, {"text": "The first two of these concerns can be dealt with by a parser such as KNP ( or CaboCha (), or alternatively a tag sequence-based technique such as that proposed by for English.", "labels": [], "entities": []}, {"text": "Word sense ambiguity is an issue if we wish to determine the valence of the verb and make use of selectional restrictions.", "labels": [], "entities": [{"text": "Word sense ambiguity", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6442587971687317}]}, {"text": "We sidestep full-on verb sense disambiguation by associating a unique case frame with each verb stem type and encoding common alternations in the verb class.", "labels": [], "entities": [{"text": "full-on verb sense disambiguation", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.6346093863248825}]}, {"text": "Even here, however, we must have some means of dealing with verb homonymy and integrating analyses for cosubordinated relative clauses.", "labels": [], "entities": []}, {"text": "We investigate various techniques to resolve such ambiguity and combine the analysis of multiple component clauses.", "labels": [], "entities": []}, {"text": "In the following, we define the RCC semantic types (", "labels": [], "entities": []}], "datasetContent": [{"text": "In evaluation, we compare different clausal interpretation selection techniques.", "labels": [], "entities": [{"text": "clausal interpretation selection", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.6780779957771301}]}, {"text": "We further goon to investigate the efficacy of different parameter partitions on disambiguation, and generate a learning curve.", "labels": [], "entities": []}, {"text": "Evaluation was carried out byway of stratified 10-fold cross validation throughout, using the C4.5 decision tree learner.", "labels": [], "entities": [{"text": "C4.5 decision tree learner", "start_pos": 94, "end_pos": 120, "type": "DATASET", "confidence": 0.915516585111618}]}, {"text": "As C4.5 induces a unique decision tree from the training data and then applies this to the test data, we are able to evaluate both training and test classification accuracy, i.e. the relative success of the decision tree in classifying the training data and test data, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9653280377388}]}, {"text": "The data used in evaluation is a set of 5143 RCC instances from the EDR corpus, of which 4.7% included cosubordinated relative clauses (i.e. the total number of unit relative clauses is 5408).", "labels": [], "entities": [{"text": "EDR corpus", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9775364100933075}]}, {"text": "Each RCC instance was manually annotated for default interpretation independent of sentential context.", "labels": [], "entities": []}, {"text": "The 10 most-frequent interpretations (out of 27) in this test set are presented below: Based on this, we can derive a baseline accuracy of 64.0%, obtained by allocating the SUBJECT interpretation to every RCC input.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9915881752967834}]}, {"text": "First, we evaluate analytical disambiguation by decomposing each RCC into its component cosubordinated RCCs and selecting most plausible interpretation for each unit clause (UC).", "labels": [], "entities": []}, {"text": "We compare: (a) a random selection baseline method (Random UC ); (b) a method where all feature vectors for the unit relative clause are logically AND'ed together (AND UC ); (c) a method where all feature vectors for the unit clause are logically OR'ed together (OR UC ); and (d) the cascaded-heuristic method from \u00a2 4.2 above (Heuristic UC ).", "labels": [], "entities": []}, {"text": "The results for the various methods are presented in.", "labels": [], "entities": []}, {"text": "Note that 28.8% of clauses occurring in the data are associated with analytical ambiguity, and for the remainder, there is only one verb entry in the case frame dictionary.", "labels": [], "entities": []}, {"text": "Heuristic UC outperforms the Random UC baseline to a level of statistical significance, 4 in both training and testing.", "labels": [], "entities": []}, {"text": "OR UC lags behind Heuristic UC in testing in particular, but is vastly superior to AND UC , which All statistical significance judgements are based on the paired is marginally worse than Random UC in both training and testing.", "labels": [], "entities": [{"text": "OR UC", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9672450125217438}, {"text": "AND UC", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9698899984359741}]}, {"text": "Based on these results, we conclude that our system of cascaded heuristics (Heuristic UC ) is the best of the tested methods and use this as our intra-clause disambiguation method in subsequent evaluation.", "labels": [], "entities": []}, {"text": "We further partitioned up the parameter space and ran C4.5 over the different combinations thereof, using AND CI . The particular parameter partitions we target are case slot instantiation flags (C: 11 features), head noun semantics (N: 14 features) and verb classes (V: 27 features).", "labels": [], "entities": []}, {"text": "The system results over the individual parameter partitions, and the various combinations of case slot instantiation, head noun semantics and verb classes (e.g. N+V = head noun semantics and verb classes), are presented in.", "labels": [], "entities": []}, {"text": "The value of head noun semantics is borne out by the high test accuracy for N of 76.0%.", "labels": [], "entities": [{"text": "head noun semantics", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.5977271894613901}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9786607027053833}, {"text": "N", "start_pos": 76, "end_pos": 77, "type": "METRIC", "confidence": 0.953058660030365}]}, {"text": "We can additionally see that case slot instantiation and verb class features provide approximately equivalent discriminatory power, both well above the absolute baseline of 64.0%.", "labels": [], "entities": [{"text": "case slot instantiation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6496924459934235}]}, {"text": "This is despite case slot instantiation flags being less than half the number of verb classes, largely due to the direct correlation between case slot instantiation judgements and case-slot gapping analyses, which account for around 80% of all RCCs.", "labels": [], "entities": []}, {"text": "The affinity between case slot instantiation judgements and the semantics of the head noun is evidenced in the strong performance of C+N, although even here, verb classes gain us an additional 5% of performance.", "labels": [], "entities": [{"text": "case slot instantiation judgements", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.7168069332838058}]}, {"text": "Essentially what is occurring here is that selectional preferences between particular head noun semantics and certain case-slot/analysis types are incrementally enhanced as we add in the extra dimensions of case slot instantiation and verb classes.", "labels": [], "entities": []}, {"text": "The orthogonality of the three dimensions is demonstrated by the incremental performance improvement as we add in extra parameter types.", "labels": [], "entities": []}, {"text": "This finding provides evidence for our earlier claims about selection in RCCs being based on the combination of head noun semantics, verb classes and information about what case slots are vacant in the relative clause.", "labels": [], "entities": []}, {"text": "To determine if the 90.2% upper bound on classification accuracy for the given experimental setup is due to limitations in the particular resources we are using or an inherent bound on the RCC interpretation task as defined herein, we performed a manual annotation task involving 4 annotators and 100 randomly-selected RCCs, taken from the 5143 RCCs used in this research.", "labels": [], "entities": [{"text": "classification", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9340898394584656}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.8879603147506714}, {"text": "RCC interpretation", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.860875129699707}]}, {"text": "The mean agreement between the annotators was 90.0%, coinciding remarkably well with the 90.2% This provides extra evidence for the success of the proposed method, and suggests that there is little room for improvement given the current task definition.", "labels": [], "entities": [{"text": "agreement", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.628657877445221}]}], "tableCaptions": []}