{"title": [{"text": "Unifying Annotated Discourse Hierarchies to Create a Gold Standard", "labels": [], "entities": []}], "abstractContent": [{"text": "Human annotation of discourse corpora typically results in segmentation hierarchies that vary in their degree of agreement.", "labels": [], "entities": []}, {"text": "This paper presents several techniques for unifying multiple discourse annotations into a single hierarchy , deemed a \"gold standard\"-the segmen-tation that best captures the underlying linguistic structure of the discourse.", "labels": [], "entities": []}, {"text": "It proposes and analyzes methods that consider the level of em-beddedness of a segmentation as well as methods that do not.", "labels": [], "entities": []}, {"text": "A corpus containing annotated hierarchical discourses, the Boston Directions Corpus, was used to evaluate the \"goodness\" of each technique, by comparing the similarity of the segmentation it derives to the original annotations in the corpus.", "labels": [], "entities": [{"text": "Boston Directions Corpus", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.9858330488204956}]}, {"text": "Several metrics of similarity between hierarchical segmentations are computed: precision/recall of matching utterances , pairwise inter-reliability scores (), and non-crossing-brackets.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9991812109947205}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.808967649936676}]}, {"text": "A novel method for unification that minimizes conflicts among an-notators outperforms methods that require consensus among a majority for the and precision metrics, while capturing much of the structure of the discourse.", "labels": [], "entities": [{"text": "unification", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.969913899898529}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9954805374145508}]}, {"text": "When high recall is preferred, methods requiring a majority are preferable to those that demand full consensus among anno-tators.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9991067051887512}]}], "introductionContent": [{"text": "The linguistic structure of a discourse is composed of utterances that exhibit meaningful hierarchical relationships (.", "labels": [], "entities": []}, {"text": "Automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarization to anaphora resolution.", "labels": [], "entities": [{"text": "Automatic segmentation of discourse", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7780948281288147}, {"text": "information retrieval", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.8116978406906128}, {"text": "text summarization", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7517850995063782}, {"text": "anaphora resolution", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7344418466091156}]}, {"text": "These automatic methods, usually based on supervised machine learning techniques, require a manually annotated corpus of data for training.", "labels": [], "entities": []}, {"text": "The creation of these corpora often involves multiple judges annotating the same discourses, so as to avoid bias from using a single judge's annotations as ground truth.", "labels": [], "entities": []}, {"text": "Usually, fora particular discourse, these multiple annotations are unified into a single annotation, either manually by the annotators' discussions or automatically.", "labels": [], "entities": []}, {"text": "However, annotation unification approaches have not been formally evaluated, and although manual unification might be the best approach, it can be time-consuming.", "labels": [], "entities": [{"text": "annotation unification", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.715644508600235}, {"text": "manual unification", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7535982728004456}]}, {"text": "Indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation, because of the difficulties of obtaining consistent hierarchical annotations.", "labels": [], "entities": [{"text": "automatic recognition of discourse structure", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.786032748222351}]}, {"text": "In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods).", "labels": [], "entities": []}, {"text": "There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations.", "labels": [], "entities": [{"text": "unifying hierarchical discourse annotations", "start_pos": 121, "end_pos": 164, "type": "TASK", "confidence": 0.6559794992208481}]}, {"text": "First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties.", "labels": [], "entities": []}, {"text": "reported that \"hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.\" conducted a pilot study in which subjects found it \"difficult and time-consuming\" to identify hierarchical relations in discourse.", "labels": [], "entities": []}, {"text": "Other attempts have had more success using improved annotation tools and more precise instructions (.", "labels": [], "entities": []}, {"text": "Second, hierarchical segmentation of discourse is subjective.", "labels": [], "entities": []}, {"text": "While agreement among annotators regarding linear segmentation has been found to be higher than 80%, with respect to hierarchical segmentation it has been observed to be as low as 60% (.", "labels": [], "entities": []}, {"text": "Moreover, the precise definition of \"agreement\" with respect to hierarchical segmentation is unclear, complicating evaluation.", "labels": [], "entities": []}, {"text": "It is natural to consider two segments in separate annotations to agree if they both span precisely the same utterances and agree on the level of embeddedness.", "labels": [], "entities": []}, {"text": "However, it is less clear how to handle segments that share the same utterances but differ with respect to the level of embeddedness.", "labels": [], "entities": []}, {"text": "In this paper, we show that despite these difficulties it is possible to automatically combine a set of multi-level discourse annotations together into a single gold standard, a segmentation that best captures the underlying linguistic structure of the discourse.", "labels": [], "entities": []}, {"text": "We aspire to create corpora analogous to the Penn Treebank in which a unique parse tree exists for each sentence that is agreed upon by all to convey the \"correct\" parse of the sentence.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9948907196521759}]}, {"text": "However, whereas the Penn Treebank parses are determined through a time-consuming negotiation between labelers, we aim to derive gold standard segmentations automatically.", "labels": [], "entities": [{"text": "Penn Treebank parses", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.9469410181045532}]}, {"text": "There are several potential benefits for having a unifying standard for discourse corpora.", "labels": [], "entities": []}, {"text": "First, it can constitute a unique segmentation of the discourse that is deemed the nearest approximation of the true objective structure, assuming one exists.", "labels": [], "entities": []}, {"text": "Second, it can be used as a single unified version with which to train and evaluate algorithms for automatic discourse segmentation.", "labels": [], "entities": [{"text": "automatic discourse segmentation", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.6224144796530405}]}, {"text": "Third, it can be used as a preprocessing step for computational tasks that require discourse structure, such as anaphora resolution and summarization.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7181365340948105}, {"text": "summarization", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.9618322253227234}]}, {"text": "In this work, we describe and evaluate several approaches for unifying multiple hierarchical discourse segmentations into one gold standard.", "labels": [], "entities": []}, {"text": "Some of our approaches measure the agreement between annotations by taking into account the level of embeddedness and others ignore the hierarchy.", "labels": [], "entities": []}, {"text": "We also introduce a novel method, called the Conflict-Free Union, that minimizes the number of conflicts between annotations.", "labels": [], "entities": []}, {"text": "For our experiments, we used the Boston Directions Corpus (BDC).", "labels": [], "entities": [{"text": "Boston Directions Corpus (BDC)", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.972824364900589}]}, {"text": "Ideally, each technique would be evaluated with respect to a single unified segmentation of the BDC that was deemed \"true\" by annotators who are experts in discourse theory, but we did not have the resources to attempt this task.", "labels": [], "entities": [{"text": "BDC", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.784538209438324}]}, {"text": "Instead, we measure each technique by comparing the average similarity between its gold standard and the original annotations used to create it.", "labels": [], "entities": []}, {"text": "Our similarity metrics measure both hierarchical and linear segment agreement using precision/recall metrics, interreliability similarities among annotations using the ( ) metric, and percentage of non-crossing-brackets.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9981973767280579}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.8003526329994202}]}, {"text": "We found that there is no single approach that does well with respect to all of the similarity metrics.", "labels": [], "entities": []}, {"text": "However, the Conflict-Free Union approach outperforms the other methods for the and precision metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9953250885009766}]}, {"text": "Also, techniques that include majority agreements of annotators have better recall than techniques which demanded full consensus among annotators.", "labels": [], "entities": [{"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9994710087776184}]}, {"text": "We also uncovered some flaws in each technique; for example, we found that gold standards that include dense structure are over-penalized by some of the metrics.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are several ways of evaluating an algorithm for creating a gold standard, just as there are several ways of evaluating any segmentation algorithm.", "labels": [], "entities": []}, {"text": "Ideally, we would like to compare to some objectively true gold standard, but it is impossible to determine if there are one or more true standards, or even if one exists.", "labels": [], "entities": []}, {"text": "Instead, we can compare a gold standard against each annotator's individual structuring, or against that of several human annotators collectively.", "labels": [], "entities": []}, {"text": "Also, we could compare gold standards with each other in terms of how they affect the out-2 1: The metric would consider these two segmentations in agreement.", "labels": [], "entities": []}, {"text": "come of some computational task which considers discourse structure, such as anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.718348890542984}]}, {"text": "This last approach is probably the best when the purpose of the gold standard is known in advance, but in this paper we consider only task-independent metrics.", "labels": [], "entities": []}, {"text": "For the sake of scientific validity, we did not compare a gold standard with a segmentation of our own.", "labels": [], "entities": []}, {"text": "Instead, we chose to evaluate gold standards by averaging their similarity to the original segmentations made by human annotators.", "labels": [], "entities": []}, {"text": "For each approach we presented earlier, we report an average similarity score overall original segmentations and the gold standard, based on several different quantitative measures of inter-reliability.", "labels": [], "entities": [{"text": "similarity", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.961329996585846}]}, {"text": "Our experiments were run on 12 discourses in the spontaneous speech component of the BDC.", "labels": [], "entities": [{"text": "BDC", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9100731015205383}]}, {"text": "The lengths of the discourses ranged from 15 to 150 intonational phrases.", "labels": [], "entities": []}, {"text": "Each discourse was segmented by three different annotators, resulting in 36 separate annotations.", "labels": [], "entities": []}, {"text": "For each discourse, we combined the three annotations into a gold standard according to each technique described in Section 2.", "labels": [], "entities": []}, {"text": "We then proceeded to compute the similarity between the gold standard and each of the original annotations by using the pairwise evaluation metrics described in Section 3.", "labels": [], "entities": [{"text": "similarity", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9781060218811035}, {"text": "gold standard", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.8989432156085968}]}], "tableCaptions": []}