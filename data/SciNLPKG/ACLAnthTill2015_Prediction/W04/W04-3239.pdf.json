{"title": [{"text": "A Boosting Algorithm for Classification of Semi-Structured Text", "labels": [], "entities": [{"text": "Classification of Semi-Structured Text", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.8887338936328888}]}], "abstractContent": [{"text": "The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7943400144577026}, {"text": "topic identification", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7340222895145416}, {"text": "opinion/modality identification", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.6355085670948029}]}, {"text": "Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required.", "labels": [], "entities": []}, {"text": "Accordingly, learning algorithms must be created that can handle the structures observed in texts.", "labels": [], "entities": []}, {"text": "In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.", "labels": [], "entities": []}, {"text": "The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.", "labels": [], "entities": []}, {"text": "We also discuss the relation between our algorithm and SVMs with tree kernel.", "labels": [], "entities": []}, {"text": "Two experiments on opinion/modality classification confirm that subtree features are important.", "labels": [], "entities": [{"text": "opinion/modality classification", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6498517170548439}]}], "introductionContent": [{"text": "Text classification plays an important role in organizing the online texts available on the World Wide Web, Internet news, and E-mails.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.845085620880127}]}, {"text": "Until recently, a number of machine learning algorithms have been applied to this problem and have been proven successful in many domains).", "labels": [], "entities": []}, {"text": "In the traditional text classification tasks, one has to identify predefined text \"topics\", such as politics, finance, sports or entertainment.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.8029687603314718}]}, {"text": "For learning algorithms to identify these topics, a text is usually represented as a bag-of-words, where a text is regarded as a multi-set (i.e., a bag) of words and the word order or syntactic relations appearing in the original text is ignored.", "labels": [], "entities": []}, {"text": "Even though the bag-of-words representation is naive and does not convey the meaning of the original text, reasonable accuracy can be obtained.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9988678693771362}]}, {"text": "This is because each word occurring in the text is highly relevant to the predefined \"topics\" to be identified.", "labels": [], "entities": []}, {"text": "Given that a number of successes have been reported in the field of traditional text classification, the focus of recent research has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.6870484054088593}, {"text": "topic identification", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.7165917754173279}, {"text": "opinion/modality identification", "start_pos": 210, "end_pos": 241, "type": "TASK", "confidence": 0.645942248404026}]}, {"text": "Example includes categorization of customer E-mails and reviews by types of claims, modalities or subjectivities.", "labels": [], "entities": []}, {"text": "For the latter, the traditional bag-of-words representation is not sufficient, and a richer, structural representation is required.", "labels": [], "entities": []}, {"text": "A straightforward way to extend the traditional bag-of-words representation is to heuristically add new types of features to the original bag-of-words features, such as fixed-length n-grams (e.g., word bi-gram or tri-gram) or fixedlength syntactic relations (e.g., modifier-head relations).", "labels": [], "entities": []}, {"text": "These ad-hoc solutions might give us reasonable performance, however, they are highly taskdependent and require careful design to create the \"optimal\" feature set for each task.", "labels": [], "entities": []}, {"text": "Generally speaking, by using text processing systems, a text can be converted into a semi-structured text annotated with parts-of-speech, base-phrase information or syntactic relations.", "labels": [], "entities": []}, {"text": "This information is useful in identifying opinions or modalities contained in the text.", "labels": [], "entities": []}, {"text": "We think that it is more useful to propose a learning algorithm that can automatically capture relevant structural information observed in text, rather than to heuristically add this information as new features.", "labels": [], "entities": []}, {"text": "From these points of view, this paper proposes a classification algorithm that captures sub-structures embedded in text.", "labels": [], "entities": []}, {"text": "To simplify the problem, we first assume that a text to be classified is represented as a labeled ordered tree, which is a general data structure and a simple abstraction of text.", "labels": [], "entities": []}, {"text": "Note that word sequence, base-phrase annotation, dependency tree and an XML document can be modeled as a labeled ordered tree.", "labels": [], "entities": []}, {"text": "The algorithm proposed here has the following characteristics: i) It performs learning and classification using structural information of text.", "labels": [], "entities": []}, {"text": "ii) It uses a set of all subtrees (bag-of-subtrees) for the feature set without any constraints.", "labels": [], "entities": []}, {"text": "iii) Even though the size of the candidate feature set becomes quite large, it automatically selects a compact and relevant feature set based on Boosting.", "labels": [], "entities": [{"text": "Boosting", "start_pos": 145, "end_pos": 153, "type": "DATASET", "confidence": 0.8290087580680847}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we describe the details of our Boosting algorithm in which the subtree-based decision stumps are applied as weak learners.", "labels": [], "entities": []}, {"text": "Second, we show an implementation issue related to constructing an efficient learning algorithm.", "labels": [], "entities": []}, {"text": "We also discuss the relation between our algorithm and SVMs) with tree kernel.", "labels": [], "entities": []}, {"text": "Two experiments on the opinion and modality classification tasks are employed to confirm that subtree features are important.", "labels": [], "entities": [{"text": "modality classification tasks", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.8221416076024374}]}], "datasetContent": [{"text": "We conducted two experiments in sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.791539877653122}]}, {"text": "\u2022 PHS review classification (PHS) The goal of this task is to classify reviews (in Japanese) for PHS 2 as positive reviews or negative reviews.", "labels": [], "entities": [{"text": "PHS review classification (PHS)", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.7509255508581797}]}, {"text": "A total of 5,741 sentences were collected from a Web-based discussion BBS on PHS, in which users are directed to submit positive reviews separately from negative reviews.", "labels": [], "entities": [{"text": "PHS", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9324331879615784}]}, {"text": "The unit of classification is a sentence.", "labels": [], "entities": []}, {"text": "The categories to be identified are \"positive\" or \"negative\" with the numbers 2,679 and 3,062 respectively.", "labels": [], "entities": []}, {"text": "\u2022 Modality identification (MOD) This task is to classify sentences (in Japanese) by modality.", "labels": [], "entities": [{"text": "Modality identification (MOD)", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.8247905969619751}]}, {"text": "A total of 1,710 sentences from a Japanese newspaper were manually annotated according to Tamura's taxonomy.", "labels": [], "entities": []}, {"text": "The unit of classification is a sentence.", "labels": [], "entities": []}, {"text": "The categories to be identified are \"opinion\", \"assertion\" or \"description\" with the numbers 159, 540, and 1,011 respectively.", "labels": [], "entities": []}, {"text": "To employ learning and classification, we have to represent a given sentence as a labeled ordered tree.", "labels": [], "entities": []}, {"text": "In this paper, we use the following three representation forms.", "labels": [], "entities": []}, {"text": "\u2022 bag-of-words (bow), baseline Ignoring structural information embedded in text, we simply represent a text as a set of words.", "labels": [], "entities": []}, {"text": "This is exactly the same setting as Boostexter.", "labels": [], "entities": [{"text": "Boostexter", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.8686346411705017}]}, {"text": "Word boundaries are identified using a Japanese morphological analyzer, ChaSen 3 . \u2022 Dependency (dep) We represent a text in a word-based dependency tree.", "labels": [], "entities": []}, {"text": "We first use CaboCha 4 to obtain a chunk-based dependency tree of the text.", "labels": [], "entities": []}, {"text": "The chunk approximately corresponds to the basephrase in English.", "labels": [], "entities": []}, {"text": "By identifying the headword in the chunk, a chunk-based dependency tree is converted into a word-based dependency tree.", "labels": [], "entities": []}, {"text": "\u2022 N-gram (ngram) It is the word-based dependency tree that assumes that each word simply modifies the next word.", "labels": [], "entities": []}, {"text": "Any subtree of this structure becomes a word n-gram.", "labels": [], "entities": []}, {"text": "We compared the performance of our Boosting algorithm and support vector machines (SVMs) with bag-of-words kernel and tree kernel according to their F-measure in 5-fold cross validation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9941316246986389}]}, {"text": "Although there exist some extensions for tree kernel), we use the original tree kernel by), where all subtrees of a tree are used as distinct features.", "labels": [], "entities": []}, {"text": "This setting yields a fair comparison in terms of feature space.", "labels": [], "entities": []}, {"text": "To extend a binary classifier to a multi-class classifier, we use the one-vs-rest method.", "labels": [], "entities": []}, {"text": "Hyperparameters, such as number of iterations K in Boosting and soft-margin parameter C in SVMs were selected by using cross-validation.", "labels": [], "entities": []}, {"text": "We implemented SVMs with tree kernel based on TinySVM 5 with custom kernels incorporated therein.", "labels": [], "entities": [{"text": "TinySVM 5", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.9308821558952332}]}, {"text": "summarizes the results of PHS and MOD tasks.", "labels": [], "entities": [{"text": "MOD tasks", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.7154441475868225}]}, {"text": "To examine the statistical significance of the results, we employed a McNemar's paired test, a variant of the sign test, on the labeling disagreements.", "labels": [], "entities": [{"text": "McNemar's paired test", "start_pos": 70, "end_pos": 91, "type": "METRIC", "confidence": 0.6913259103894234}]}, {"text": "This table also includes the results of significance tests.", "labels": [], "entities": [{"text": "significance", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9216355681419373}]}], "tableCaptions": []}