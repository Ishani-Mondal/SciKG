{"title": [{"text": "Using word similarity lists for resolving indirect anaphora", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work we test the use of word similarity lists for anaphora resolution in Portuguese corpora.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7578293085098267}]}, {"text": "We applied an automatic lexical acquisition technique over parsed texts to identify semantically similar words.", "labels": [], "entities": []}, {"text": "After that, we made use of this lexical knowledge to resolve coreferent definite descriptions where the head-noun of the anaphor is different from the head-noun of its antecedent, which we call indirect anaphora.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this work we investigate the use of word similarity list for treating coreference, especially the cases where the coreferent expressions have semantically related head nouns (instead of same head nouns), which we call indirect anaphora.", "labels": [], "entities": []}, {"text": "We applied a lexical acquisition technique ) over Portuguese parsed corpora to automatically identify semantically similar words.", "labels": [], "entities": []}, {"text": "After that, we made use of this lexical knowledge to resolve the coreferent definite descriptions where the head-noun of the anaphor is different from the head-noun of its antecedent.", "labels": [], "entities": []}, {"text": "Previous work on anaphoric resolution of English texts has used acquired lexical knowledge in different ways, examples are (; Schulte im.", "labels": [], "entities": [{"text": "anaphoric resolution of English texts", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.8491751670837402}]}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "The next section explain our notion of indirect anaphora.", "labels": [], "entities": []}, {"text": "Section 3 details the tools and techniques used to the construction of our lexical resource.", "labels": [], "entities": []}, {"text": "Section 4 presents our heuristic for solving the indirect anaphors on the basis of such resource.", "labels": [], "entities": []}, {"text": "Section 5 details the corpus we are using for evaluating the proposed heuristics.", "labels": [], "entities": []}, {"text": "Section 6 reports the implementation of the heuristic and in Section 7 we present our experiments over Portuguese annotated corpora.", "labels": [], "entities": []}, {"text": "In Section 8 we discuss our results and compare them to previous works.", "labels": [], "entities": []}, {"text": "Finally, Section 9 presents our concluding comments.", "labels": [], "entities": []}], "datasetContent": [{"text": "As result of previous work , we have a Portuguese corpus manually annotated with coreference information.", "labels": [], "entities": [{"text": "Portuguese corpus", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.8477161824703217}]}, {"text": "This corpus is considered our gold-standard to evaluate the performance of the heuristic presented in the previous section.", "labels": [], "entities": []}, {"text": "The study aimed to verify if we could get a similar distribution of types of definite descriptions for Portuguese and English, which would serve as an indication that the same heuristics tested for English () could apply for Portuguese.", "labels": [], "entities": []}, {"text": "The main annotation task in this experiment was identifying antecedents and classifying each definite description according to the four classes presented in section 2.", "labels": [], "entities": []}, {"text": "For the annotation task, we adopted the MMAX annotation tool, that requires all data to be encoded in XML format.", "labels": [], "entities": [{"text": "MMAX", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8693476915359497}]}, {"text": "The corpus is encoded by <word> elements with sequential identifiers, and the output -the anaphors and its antecedents -are enconded as <markable> elements, with the anaphor markable pointing to the antecedent markable by a 'pointer' attribute.", "labels": [], "entities": []}, {"text": "The annotation process was split in 4 steps: selecting coreferent terms; identifying the antecedent of coreferent terms; classifying coreferent terms (direct or indirect); classifying non-coreferent terms (discourse new or other anaphora).", "labels": [], "entities": []}, {"text": "About half of the anaphoras were classified as discourse new descriptions, which account for about 70% of noncoreferent cases.", "labels": [], "entities": []}, {"text": "Among the coreferent cases the number of direct coreference is twice the number of indirect coreference.", "labels": [], "entities": []}, {"text": "This confirms previous work done for English.", "labels": [], "entities": []}, {"text": "For the present work, we took then the 95 cases classified as indirect coreference to serve as our evaluation set.", "labels": [], "entities": []}, {"text": "In 14 of this cases, the relation between anaphor and antecedent is synonymy, in 43 of the cases the relation is hyponymy, and in 38, the antecedent or the anaphor area proper name.", "labels": [], "entities": []}, {"text": "We run two experiments: one using the similarity lists with proper names and another with the lists containing just common nouns.", "labels": [], "entities": []}, {"text": "With these experiments we verify the values for precision, recall and false positives on the task of choosing an semantically similar antecedent for each indirect anaphor.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9995272159576416}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9993413090705872}]}, {"text": "Our annotated corpus has 95 indirect anaphors with nominal antecedents, where 57 of them do not include proper names (as anaphor or as antecedent).", "labels": [], "entities": []}, {"text": "We use anon annotated version of this corpus for the experiments.", "labels": [], "entities": []}, {"text": "It contains around 6000 words, from 24 news texts of 6 different newspaper sections.", "labels": [], "entities": []}, {"text": "Firstly, we reduced both sets of similarity lists to contain just the list for the words present in this portion of the corpus (660 lists without proper names and 742 including proper names).", "labels": [], "entities": []}, {"text": "Considering the 57 indirect anaphoras to be solved (the ones that do not include any proper name), we could solve 19 of them.", "labels": [], "entities": []}, {"text": "It leads to a precision of 52.7% and a a recall of 33.3%.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9985113739967346}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9781132340431213}]}, {"text": "shows the result of our study considering the set of common noun lists.", "labels": [], "entities": []}, {"text": "Most of the cases could be resolved by 'right direction', that represents the more intuitive way.", "labels": [], "entities": []}, {"text": "21 of the cases didn't get any antecedent.", "labels": [], "entities": []}, {"text": "We got 17 false positives, with different causes: 1.", "labels": [], "entities": []}, {"text": "the right antecedent was not in the lists, therefore it could not be found but other wrong antecedents were retrieved.", "labels": [], "entities": []}, {"text": "For example, in meu amigo Ives Gandra da Silva Martins escreveu para esse jornal ...", "labels": [], "entities": []}, {"text": "o conselheiro Ives (my friend Ives_Gandra_da_Silva_Martins wrote to this newspaper ...", "labels": [], "entities": []}, {"text": "the councillor Ives), two more candidates head-nouns are similar words to \"conselheiro\" (councillor): \"arquiteto\" (architect) and \"consultor\" (consultant), but not \"amigo\" (friend); 2.", "labels": [], "entities": []}, {"text": "the right antecedent was in the lists but another wrong antecedent was given higher weights, because of proximity to the anaphora, as in the example a rodovia Comandante Jo\u00e3o Ribeiro de Barros ...", "labels": [], "entities": []}, {"text": "ao tentar atravessar a estrada (the highway Comandante Joao Ribeiro de Barros ...", "labels": [], "entities": []}, {"text": "while trying to cross the road).", "labels": [], "entities": []}, {"text": "Here, the correct antecedent to \"a estrada\" (the road) is \"rodovia\" (the highway) and it is present in \"estrada\"'s similarity list (right direction), but also is \"ponte\" (the bridge) and it is closer to the anaphor in the text.", "labels": [], "entities": []}, {"text": "As expected, most of the false positives (11 cases) were 'resolved' by \"indirect way\".", "labels": [], "entities": []}, {"text": "Considering all similar words found among the candidates, not just the one with highest weight, we could find the correct antecedent in 24 cases (42%).", "labels": [], "entities": []}, {"text": "The average number of similar words among the candidates was 2.8, taking into account again the positive and false positive cases.", "labels": [], "entities": []}, {"text": "These numbers report how much the similarity lists encode the semantic relations present in the corpus.", "labels": [], "entities": []}, {"text": "64% of the synonymy cases and 28% of the hyponymy cases could be resolved.", "labels": [], "entities": []}, {"text": "35% of the hyponymy cases resulted in false positives, the same happened with just 14% of the synonymy cases.", "labels": [], "entities": []}, {"text": "We replicated the previous experiment now using the similarity lists that include proper names.", "labels": [], "entities": []}, {"text": "shows the results considering the set of lists for nouns and proper names.", "labels": [], "entities": []}, {"text": "Considering the 95 indirect anaphoras to be solved, we could solve 21 of them.", "labels": [], "entities": []}, {"text": "It leads to a precision of 36.8% and a a recall of 22.1%.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9988340735435486}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9874085187911987}]}, {"text": "There was no antecedent found for 38 anaphors, and 36 anaphors got wrong antecedents (half of them by \"inderect way\").", "labels": [], "entities": []}, {"text": "We observed the same causes for false positives as the two presented for experiment 1.", "labels": [], "entities": []}, {"text": "Considering all cases resolved (correct and false ones), we could find the correct antecedent among the similar words of the anaphor in 31 cases (32.6%).", "labels": [], "entities": []}, {"text": "The average number of similar words among the candidates was 2.75.", "labels": [], "entities": []}, {"text": "The numbers for synonymy and hyponymy cases were the same as in experiment 1 -64% and 28% respectively.", "labels": [], "entities": []}, {"text": "The numbers for proper names were 50% of false positives and 50% of unresolved cases.", "labels": [], "entities": [{"text": "proper names", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9638814032077789}]}, {"text": "It means none of the cases that include proper names could be resolved, but do not means they hadn't any influence in other nouns similarity lists.", "labels": [], "entities": []}, {"text": "In 26% of the false positive cases, the correct antecedent (a proper name) was in the anaphor similarity list (but was not selected due to the weighting strategy).", "labels": [], "entities": []}, {"text": "The experiment with the similarity lists that include proper names was able to solve more cases, but experiment 1 got better precision and recall values.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9996533393859863}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9994495511054993}]}], "tableCaptions": [{"text": " Table 2: Results considering just nouns", "labels": [], "entities": []}, {"text": " Table 3: Results considering nouns and proper  names", "labels": [], "entities": []}]}