{"title": [], "abstractContent": [{"text": "Probabilistic models have been effective in resolving prepositional phrase attachment ambiguity , but sparse data remains a significant problem.", "labels": [], "entities": [{"text": "resolving prepositional phrase attachment ambiguity", "start_pos": 44, "end_pos": 95, "type": "TASK", "confidence": 0.6500554382801056}]}, {"text": "We propose a solution based on similarity-based smoothing, where the probability of new PPs is estimated with information from similar examples generated using a thesaurus.", "labels": [], "entities": []}, {"text": "Three thesauruses are compared on this task: two existing generic thesauruses and anew specialist PP thesaurus tailored for this problem.", "labels": [], "entities": []}, {"text": "We also compare three smoothing techniques for prepositional phrases.", "labels": [], "entities": []}, {"text": "We find that the similarity scores provided by the thesaurus tend to weight distant neighbours too highly, and describe a better score based on the rank of a word in the list of similar words.", "labels": [], "entities": []}, {"text": "Our smoothing methods are applied to an existing PP attachment model and we obtain significant improvements over the baseline.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.7929432094097137}]}], "introductionContent": [{"text": "Prepositional phrases are an interesting example of syntactic ambiguity and a challenge for automatic parsers.", "labels": [], "entities": []}, {"text": "The ambiguity arises whenever a prepositional phrase can modify a preceding verb or noun, as in the canonical example I saw the man with the telescope.", "labels": [], "entities": []}, {"text": "In syntactic terms, the prepositional phrase attaches either to the noun phrase or the verb phrase.", "labels": [], "entities": []}, {"text": "Many kinds of syntactic ambiguity can be resolved using structural information alone), but in this case both candidate structures are perfectly grammatical and roughly equally likely.", "labels": [], "entities": []}, {"text": "Therefore ambiguous prepositional phrases require some kind of additional context to disambiguate correctly.", "labels": [], "entities": []}, {"text": "In some cases a small amount of lexical knowledge is sufficient: for example of almost always modifies the noun.", "labels": [], "entities": []}, {"text": "Other cases, such as the telescope example, are potentially much harder since discourse or world knowledge might be required.", "labels": [], "entities": [{"text": "telescope", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.8393691778182983}]}, {"text": "Fortunately it is possible to do well at this task just by considering the lexical preferences of the words making up the PP.", "labels": [], "entities": []}, {"text": "Lexical preferences describe the tendency for certain words to occur together or only in specific constructions.", "labels": [], "entities": []}, {"text": "For example, saw and telescope are more likely to occur together than man and telescope, so we can infer that the correct attachment is likely to be verbal.", "labels": [], "entities": []}, {"text": "The most useful lexical preferences are captured by the quadruple (v, n 1 , p, n 2 ) where v is the verb, n 1 is the head of the direct object, p is the preposition and n 2 is the head of the prepositional phrase.", "labels": [], "entities": []}, {"text": "A benchmark dataset of 27,937 such quadruples was extracted from the Wall Street Journal corpus by and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 69, "end_pos": 95, "type": "DATASET", "confidence": 0.9646138399839401}]}, {"text": "This paper examines the effect of particular smoothing algorithms on the performance of an existing statistical PP model.", "labels": [], "entities": []}, {"text": "A major problem faced by any statistical attachment algorithm is sparse data, which occurs when plausible PPs are not well-represented in the training data.", "labels": [], "entities": []}, {"text": "For example, if the observed frequency of a PP in the training is zero then the maximum likelihood estimate is also zero.", "labels": [], "entities": [{"text": "maximum likelihood estimate", "start_pos": 80, "end_pos": 107, "type": "METRIC", "confidence": 0.8145904541015625}]}, {"text": "Since the training corpus only represents a fraction of all possible PPs, this is probably an underestimate of the true probability.", "labels": [], "entities": []}, {"text": "An appealing course of action when faced with an unknown PP is to consider similar known examples instead.", "labels": [], "entities": []}, {"text": "For example, we may not have any data for eat pizza with fork, but if we have seen eat pasta with fork or even drink beer with straw then it seems reasonable to base our decision on these instead.", "labels": [], "entities": []}, {"text": "Similarity is a rather nebulous concept but for our purposes we can define it to be distributional similarity, where two words are considered similar if they occur in similar contexts.", "labels": [], "entities": []}, {"text": "For example, pizza and pasta are similar since they both often occur as the direct object of eat.", "labels": [], "entities": []}, {"text": "A thesaurus collects together lists of such similar words.", "labels": [], "entities": []}, {"text": "The first step in constructing a thesaurus is to collect co-occurrence statistics from some large corpus of text.", "labels": [], "entities": []}, {"text": "Each word is assigned a probability distribution describing the probability of it occurring with all other words, and by comparing distributions we can arrive at a similarity score.", "labels": [], "entities": []}, {"text": "The corpus, co-occurrence relationships and distributional similarity metric all affect the nature of the final thesaurus.", "labels": [], "entities": []}, {"text": "There has been a considerable amount of research comparing corpora, co-occurrence relations and similarity measures for general-purpose thesauruses, and these thesauruses are often compared against wide-coverage and general purpose semantic resources such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 259, "end_pos": 266, "type": "DATASET", "confidence": 0.9513397812843323}]}, {"text": "In this paper we examine whether it is useful to tailor the thesaurus to the task.", "labels": [], "entities": []}, {"text": "General purpose thesauruses list words that tend to occur together in free text; we want to find words that behave in similar ways specifically within prepositional phrases.", "labels": [], "entities": []}, {"text": "To this end we create a PP thesaurus using existing similarity metrics but using a corpus consisting of automatically extracted prepositional phrases.", "labels": [], "entities": []}, {"text": "A thesaurus alone is not sufficient to solve the PP attachment problem; we also need a model of the lexical preferences of prepositional phrases.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.871883362531662}]}, {"text": "Here we use the back-off model described in) but with maximum likelihood estimates smoothed using similar PPs discovered using a thesaurus.", "labels": [], "entities": []}, {"text": "Such similarity-based smoothing methods have been successfully used in other NLP applications but our use of them here is novel.", "labels": [], "entities": []}, {"text": "A key difference is that smoothing is not done over individual words but over entire prepositional phrases.", "labels": [], "entities": []}, {"text": "Similar PPs are generated by replacing each component word with a distributionally similar word, and we define a similarity functions for comparing PPs.", "labels": [], "entities": []}, {"text": "We find that using a score based on the rank of a word in the similarity list is more accurate than the actual similarity scores provided by the thesaurus, which tend to weightless similar words too highly.", "labels": [], "entities": []}, {"text": "In Section 2 we cover related work in PP attachment and smoothing techniques, with a brief comparison between similarity-based smoothing and the more common (for PP attachment) class-based smoothing.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8216051161289215}]}, {"text": "Section 3 describes Collins' PP attachment model and our thesaurusbased smoothing extensions.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.6515377759933472}]}, {"text": "Section 4 discusses the thesauruses used in our experiment and describes how the specialist thesaurus is constructed.", "labels": [], "entities": []}, {"text": "Experimental results are given in Section 5 and we show statistically significant improvements over the baseline model using generic thesauruses.", "labels": [], "entities": []}, {"text": "Contrary to our hypothesis the specialist thesaurus does not lead to significant improvements and we discuss possible reasons why it underperforms on this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we use the Wall Street Journal dataset created by.", "labels": [], "entities": [{"text": "Wall Street Journal dataset", "start_pos": 31, "end_pos": 58, "type": "DATASET", "confidence": 0.9775393903255463}]}, {"text": "This is divided into a training set of 20,801 words, a development set of 4,039 words and a test set of 3,097 words.", "labels": [], "entities": []}, {"text": "Each word was reduced to its morphological root using the morphological analyser described in ().", "labels": [], "entities": []}, {"text": "Strings of four digits beginning with a 1 or 2 are replaced with YEAR and all other digit strings including those including commas and full stops were replaced with NUM.", "labels": [], "entities": [{"text": "YEAR", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9918699264526367}]}, {"text": "Our implementation of Collins' algorithm only achieves 84.3% on the test data, with the shortfall of 0.2% primarily due to the different morphological analysers used", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy and coverage of the first two backing  off stages on the development data. The smoothed model  uses WASPS with \u03b2 = 0.5 and k = 5.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9939882755279541}, {"text": "coverage", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9902693033218384}, {"text": "WASPS", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.454174280166626}]}, {"text": " Table 2: Accuracy on the test data using \u03b2 = 0.05 and  k = 5; the size of the noun section of each thesaurus, and  coverage of smoothed 4-and 3-tuples", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9951035976409912}]}, {"text": " Table 3: Accuracy of various attachment models using  WordNet or automatic clustering algorithms", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.975419282913208}, {"text": "WordNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9670977592468262}]}]}