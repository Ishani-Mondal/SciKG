{"title": [{"text": "Semantic Role Labeling with Boosting, SVMs, Maximum Entropy, SNOW, and Decision Lists Grace NGAI \u20201 , Dekai WU \u20212", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6078311602274576}, {"text": "NGAI \u20201", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.8307592272758484}, {"text": "Dekai WU \u2021", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.8213030695915222}]}], "abstractContent": [{"text": "This paper describes the HKPolyU-HKUST systems which were entered into the Semantic Role Labeling task in Senseval-3.", "labels": [], "entities": [{"text": "HKPolyU-HKUST", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.7988125085830688}, {"text": "Semantic Role Labeling task", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.8352710455656052}]}, {"text": "Results show that these systems, which are based upon common machine learning algorithms, all manage to achieve good performances on the non-restricted Semantic Role Labeling task.", "labels": [], "entities": [{"text": "Semantic Role Labeling task", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.7641699761152267}]}], "introductionContent": [{"text": "This paper describes the HKPolyU-HKUST systems which participated in the Senseval-3 Semantic Role Labeling task.", "labels": [], "entities": [{"text": "HKPolyU-HKUST", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8508527874946594}, {"text": "Senseval-3 Semantic Role Labeling task", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.7418353319168091}]}, {"text": "The systems represent a diverse array of machine learning algorithms, from decision lists to SVMs to Winnow-type networks.", "labels": [], "entities": []}, {"text": "Semantic Role Labeling (SRL) is a task that has recently received a lot of attention in the NLP community.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8245293498039246}]}, {"text": "The SRL task in Senseval-3 used the Framenet () corpus: given a sentence instance from the corpus, a system's job would be to identify the phrase constituents and their corresponding role.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.896034836769104}, {"text": "Framenet () corpus", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.7923654119173685}]}, {"text": "The Senseval-3 task was divided into restricted and non-restricted subtasks.", "labels": [], "entities": []}, {"text": "In the non-restricted subtask, any and all of the gold standard annotations contained in the FrameNet corpus could be used.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9476231038570404}]}, {"text": "Since this includes information on the boundaries of the parse constituents which correspond to some frame element, this effectively maps the SRL task to that of a role-labeling classification task: given a constituent parse, identify the frame element that it belongs to.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 142, "end_pos": 150, "type": "TASK", "confidence": 0.8873806595802307}]}, {"text": "Due to the lack of time and resources, we chose to participate only in the non-restricted subtask.", "labels": [], "entities": []}, {"text": "This enabled our systems to take the classification approach mentioned in the previous paragraph.", "labels": [], "entities": []}, {"text": "The author would like to thank the Hong Kong Polytechnic University for supporting this research in part through research grants A-PE37 and 4-Z03S.", "labels": [], "entities": [{"text": "Hong Kong Polytechnic University", "start_pos": 35, "end_pos": 67, "type": "DATASET", "confidence": 0.8886310011148453}, {"text": "A-PE37", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.7898377180099487}]}, {"text": "The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.", "labels": [], "entities": [{"text": "Hong Kong Research Grants Council (RGC)", "start_pos": 35, "end_pos": 74, "type": "DATASET", "confidence": 0.9221507534384727}]}], "datasetContent": [{"text": "This section describes the features that were used for the SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.9515025913715363}]}, {"text": "Since the non-restricted SRL task is essentially a classification task, each parse constituent that was known to correspond to a frame element was considered to be a sample.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.9168643951416016}]}, {"text": "The features that we used for each sample have been previously shown to be helpful for the SRL task ().", "labels": [], "entities": [{"text": "SRL task", "start_pos": 91, "end_pos": 99, "type": "TASK", "confidence": 0.8957138657569885}]}, {"text": "Some of these features can be obtained directly from the Framenet annotations: \u2022 The name of the frame.", "labels": [], "entities": []}, {"text": "\u2022 The lexical unit of the sentence -i.e. the lexical identity of the target word in the sentence.", "labels": [], "entities": []}, {"text": "\u2022 The general part-of-speech tag of the target word.", "labels": [], "entities": []}, {"text": "\u2022 The \"phrase type\" of the constituent -i.e. the syntactic category (e.g. NP, VP) that the constituent falls into.", "labels": [], "entities": []}, {"text": "\u2022 The \"grammatical function\" (e.g. subject, object, modifier, etc) of the constituent, with respect to the target word.", "labels": [], "entities": []}, {"text": "\u2022 The position (e.g. before, after) of the constituent, with respect to the target word.", "labels": [], "entities": []}, {"text": "In addition to the above features, we also extracted a set of features which required the use of some statistical NLP tools: \u2022 Transitivity and voice of the target wordThe sentence was first part-of-speech tagged and chunked with the fnTBL transformationbased learning tools.", "labels": [], "entities": []}, {"text": "Simple heuristics were then used to deduce the transitivity voice of the target word.", "labels": [], "entities": []}, {"text": "\u2022 Head word (and its part-of-speech tag) of the constituent -After POS tagging, a syntactic parser was then used to obtain the parse tree for the sentence.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7212058901786804}]}, {"text": "The headword (and the POS tag of the head word) of the syntactic parse constituent whose span corresponded most closely to the candidate constituent was then assumed to be the headword of the candidate constituent.", "labels": [], "entities": [{"text": "POS", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9020397663116455}]}, {"text": "The resulting training data set consisted of 51,366 constituent samples with a total of 151 frame element types.", "labels": [], "entities": []}, {"text": "These ranged from \"Descriptor\" (3520 constituents) to \"Baggage\" and \"Carrier\" (1 constituent each).", "labels": [], "entities": []}, {"text": "This training data was randomly partitioned into a 80/20 \"development training\" and \"validation\" set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Boosting Models: Validation Set Results", "labels": [], "entities": []}, {"text": " Table 2: SVM Models: Validation Set Results", "labels": [], "entities": []}, {"text": " Table 3: Maximum Entropy Models: Validation Set  Results", "labels": [], "entities": []}, {"text": " Table 4: SNOW Models: Validation Set Results", "labels": [], "entities": [{"text": "SNOW", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9300982356071472}]}, {"text": " Table 5: Decision List Models: Validation Set Re- sults", "labels": [], "entities": [{"text": "Validation Set Re- sults", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6039445996284485}]}, {"text": " Table 7: Test set results for all our official systems, as well as the base models used in the ensemble system.", "labels": [], "entities": []}, {"text": " Table 6: Combined Models: Validation Set Results", "labels": [], "entities": []}]}