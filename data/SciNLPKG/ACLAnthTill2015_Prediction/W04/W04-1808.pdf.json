{"title": [{"text": "Discovering Synonyms and Other Related Words", "labels": [], "entities": [{"text": "Discovering Synonyms", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9112626612186432}]}], "abstractContent": [{"text": "Discovering synonyms and other related words among the words in a document collection can be seen as a clustering problem, where we expect the words in a cluster to be closely related to one another.", "labels": [], "entities": []}, {"text": "The intuition is that words occurring in similar contexts tend to convey similar meaning.", "labels": [], "entities": []}, {"text": "We introduce away to use translation dictionaries for several languages to evaluate the rate of synonymy found in the word clusters.", "labels": [], "entities": []}, {"text": "We also apply the information radius to calculating similarities between words using a full dependency syntactic feature space, and introduce a method for similarity recalculation during clustering as a fast approximation of the high-dimensional feature space.", "labels": [], "entities": []}, {"text": "Finally, we show that 69-79 % of the words in the clusters we discover are useful for thesaurus construction.", "labels": [], "entities": [{"text": "thesaurus construction", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7251646965742111}]}], "introductionContent": [{"text": "Finding related words among the words in a document collection can be seen as a clustering problem, where we expect the words in a cluster to be closely related to the same sense or to be distributional substitutes or proxies for one another.", "labels": [], "entities": []}, {"text": "A number of language-technology tasks can benefit from such word clusters, e.g. document classification applications, language modelling, resolving prepositional phrase attachment, conjunction scope identification, word sense disambiguation, word sense separation, automatic thesaurus generation, information retrieval, anaphor resolution, text simplification, topic identification, spelling correction.", "labels": [], "entities": [{"text": "document classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7117962837219238}, {"text": "language modelling", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7682471871376038}, {"text": "resolving prepositional phrase attachment", "start_pos": 138, "end_pos": 179, "type": "TASK", "confidence": 0.5780784860253334}, {"text": "conjunction scope identification", "start_pos": 181, "end_pos": 213, "type": "TASK", "confidence": 0.6320697764555613}, {"text": "word sense disambiguation", "start_pos": 215, "end_pos": 240, "type": "TASK", "confidence": 0.6868866682052612}, {"text": "word sense separation", "start_pos": 242, "end_pos": 263, "type": "TASK", "confidence": 0.7027385830879211}, {"text": "automatic thesaurus generation", "start_pos": 265, "end_pos": 295, "type": "TASK", "confidence": 0.6969501773516337}, {"text": "information retrieval", "start_pos": 297, "end_pos": 318, "type": "TASK", "confidence": 0.821674644947052}, {"text": "anaphor resolution", "start_pos": 320, "end_pos": 338, "type": "TASK", "confidence": 0.7316037118434906}, {"text": "topic identification", "start_pos": 361, "end_pos": 381, "type": "TASK", "confidence": 0.8644731938838959}, {"text": "spelling correction", "start_pos": 383, "end_pos": 402, "type": "TASK", "confidence": 0.8744260668754578}]}, {"text": "At present, synonyms and other related words are available in manually constructed ontologies, such as synonym dictionaries, thesauri, translation dictionaries and terminologies.", "labels": [], "entities": []}, {"text": "Manually constructing ontologies is timeconsuming even fora single domain.", "labels": [], "entities": []}, {"text": "On the world-wide web there are documents on many topics in different languages that could benefit from having an ontology.", "labels": [], "entities": []}, {"text": "For many of them some degree of automation is eventually needed.", "labels": [], "entities": [{"text": "automation", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9801123142242432}]}, {"text": "Humans often infer the meaning of an unknown word from its context.", "labels": [], "entities": []}, {"text": "Lets look at a less well-known word like blopping.", "labels": [], "entities": []}, {"text": "We look it upon the Web.", "labels": [], "entities": []}, {"text": "Some of the hits are: Blopping through some of my faves, i.e. leafing through favourite web links, A blop module emits strange electronic blopping noises, i.e. an electronic sound, The volcano looked like something off the cover of a Tolkien novel -perfectly conical, billowing smoke and blopping out chunks of bright orange lava, i.e. spluttering liquid.", "labels": [], "entities": [{"text": "Blopping", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9536412358283997}]}, {"text": "At first we find all of them different and perhaps equally important.", "labels": [], "entities": []}, {"text": "When looking at further links, we get an intuition that the first instance is perhaps a spurious creative metonym, whereas the two others can be regarded as more or less conventional and represent two distinct senses of blopping.", "labels": [], "entities": []}, {"text": "However, the meaning of all three seems to be related to a sound, which is either clicking or spluttering in nature.", "labels": [], "entities": []}, {"text": "The intuition is that words occurring in the same or similar contexts tend to convey similar meaning.", "labels": [], "entities": []}, {"text": "This is known as the Distributional Hypothesis.", "labels": [], "entities": [{"text": "Distributional Hypothesis", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.9021136164665222}]}, {"text": "There are many approaches to computing semantic similarity between words based on their distribution in a corpus.", "labels": [], "entities": [{"text": "computing semantic similarity between words", "start_pos": 29, "end_pos": 72, "type": "TASK", "confidence": 0.7964355826377869}]}, {"text": "For a general overview of similarity measures, see, and for some recent and extensive overviews and evaluations of similarity measures for i.a. automatic thesaurus construction, see).", "labels": [], "entities": [{"text": "automatic thesaurus construction", "start_pos": 144, "end_pos": 176, "type": "TASK", "confidence": 0.637117604414622}]}, {"text": "They show that the information radius and the \u03b1-skew distance are among the best for finding distributional proxies for words.", "labels": [], "entities": []}, {"text": "If we assume that a word w is represented as a sum of its contexts and that we can calculate the similarities between such word representations, we get a list L w of words with quantifications of how similar they are tow.", "labels": [], "entities": []}, {"text": "Each similarity list L w contains a mix of words related to the senses of the word w.", "labels": [], "entities": []}, {"text": "If we wish to identify groups of synonyms and other related words in a list of similarityrated words, we need to find clusters of similar words that are more similar to one another than they are to other words.", "labels": [], "entities": []}, {"text": "For a review of general clustering algorithms, see) and fora recent evaluation of clustering algorithms for finding word categories, see.", "labels": [], "entities": []}, {"text": "shows that among the standard algorithms the average-link and the kmeans clustering perform the best when trying to discover meaningful word groups.", "labels": [], "entities": []}, {"text": "In order to evaluate the quality of the discovered clusters three methods can be used, i.e. measuring the internal coherence of clusters, embedding the clusters in an application, or evaluating against a manually generated answer key.", "labels": [], "entities": []}, {"text": "The first method is generally used by the clustering algorithms themselves.", "labels": [], "entities": []}, {"text": "The second method is especially relevant for applications that can deal with noisy clusters and avoids the need to generate answer keys specific to the word clustering task.", "labels": [], "entities": [{"text": "word clustering task", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.7851088444391886}]}, {"text": "The third method requires a gold standard such as WordNet or some other ontological resource.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9614067077636719}]}, {"text": "For an overview of evaluation methodologies for word clustering, see.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.758230060338974}]}, {"text": "The contribution of this article is four-fold.", "labels": [], "entities": []}, {"text": "The first contribution is to apply the information radius in a full dependency syntactic feature space when calculating the similarities between words.", "labels": [], "entities": []}, {"text": "Previously, only a restricted set of dependency relations has been applied.", "labels": [], "entities": []}, {"text": "The second contribution is a similarity recalculation during clustering, which we introduce as a fast approximation of high-dimensional feature space and study its effect on some standard clustering algorithms.", "labels": [], "entities": [{"text": "clustering", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.964374303817749}]}, {"text": "The third contribution is a simple but efficient way to evaluate the synonym content of clusters by using translation dictionaries for several languages.", "labels": [], "entities": []}, {"text": "Finally we show that 69-79 % of the words in the discovered clusters are useful for thesaurus construction.", "labels": [], "entities": [{"text": "thesaurus construction", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.708767294883728}]}, {"text": "The rest of this article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the corpus data and the the feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7408521473407745}]}, {"text": "Section 3 introduces the discovery methodology.", "labels": [], "entities": []}, {"text": "Section 4 presents the evaluation methodology.", "labels": [], "entities": []}, {"text": "In Section 5 we present the experiments and evaluate the results and their significance.", "labels": [], "entities": []}, {"text": "Sections 6 and 7 contain the discussion and conclusion, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the quality of the clusters we need a gold standard.", "labels": [], "entities": []}, {"text": "English and a number of other languages have resources such as WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9810081720352173}]}, {"text": "For Finnish there is no WordNet and there are no large on-line synonym dictionaries available.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9486066102981567}]}, {"text": "In fact, our experiment can be seen as a feasibility study for automatically extracting information that could be used for building a WordNet for Finnish.", "labels": [], "entities": []}, {"text": "We note that when translating a word from the source language the meaning of the word is rendered in a target language.", "labels": [], "entities": []}, {"text": "Such meaning preserving relations are available in translation dictionaries.", "labels": [], "entities": []}, {"text": "If we translate into the target language and back we end up i.a. with the synonyms of the original source language word.", "labels": [], "entities": []}, {"text": "In addition, we may also get some spurious words that are related to other meanings of the target language words.", "labels": [], "entities": []}, {"text": "If we assume that the other words represent spurious cases of polysemy or homonymy in the target language, we can reduce the impact of these spurious words by considering several target languages and for each source word we use only the back-translated source words that are common to all the target languages.", "labels": [], "entities": []}, {"text": "We call such a group of words a source word synonym set.", "labels": [], "entities": []}, {"text": "For an example, see.", "labels": [], "entities": []}, {"text": "In addition to the mechanical rating of the synonym content we also manually classified the words of some cluster samples into synonymy, antonymy, hyperonymy, hyponymy, complementarity and other relations.", "labels": [], "entities": []}, {"text": "In order to evaluate the clusters we picked a random sample of 1759 nouns from the corpus data, which represented approximately 10 % of the words we had clustered.", "labels": [], "entities": []}, {"text": "For these words we extracted the translations in the FinnishEnglish, Finnish-German and Finnish-French MOT dictionaries) available in electronic form.", "labels": [], "entities": []}, {"text": "We then translated each target language word back into Finnish using the same resources.", "labels": [], "entities": []}, {"text": "The dictionaries are based on extensive hand-made dictionaries.", "labels": [], "entities": []}, {"text": "The choice of words maybe slightly different in each of them, which means that the words in common for all the dictionaries after the back translation tend to be only the core synonyms.", "labels": [], "entities": []}, {"text": "For evaluation purposes it would be unfair to demand that the clustering generate words into the clusters that are not in the corpus data, so we also removed those back translations from the source word synonym sets.", "labels": [], "entities": []}, {"text": "Finally, only synonym sets that had more than one word remaining were interesting, i.e. they contained more than the original source word.", "labels": [], "entities": []}, {"text": "There were 453 of the 1759 test words that met the qualifications.", "labels": [], "entities": []}, {"text": "The average number of synonyms or back translations for these test words was 3.53 including the source word itself.", "labels": [], "entities": []}, {"text": "For manual classification we used a sample of 50 key clusters from the whole set of clusters and an additional sample of 50 key clusters from the words qualifying for the mechanical evaluation.", "labels": [], "entities": [{"text": "manual classification", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6917858123779297}]}, {"text": "The mechanical evaluation was performed by picking the key cluster produced by a clustering algorithm for each of the test words.", "labels": [], "entities": []}, {"text": "The key cluster was the cluster which contained the original source word.", "labels": [], "entities": []}, {"text": "The evaluation was a simple overlap calculation with the gold standard generated from the translation dictionaries.", "labels": [], "entities": []}, {"text": "By counting the number of cluster words in a source word synonym set and dividing by the synonym set size, we get the recall R.", "labels": [], "entities": [{"text": "recall R", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.961787611246109}]}, {"text": "By counting the number of of source word synonyms in a cluster and dividing by the cluster size, we get the precision P . The manual evaluation was performed independently by the two authors and an external linguist.", "labels": [], "entities": [{"text": "precision P", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.8852528929710388}]}, {"text": "We then discussed the result in order to arrive at a common view.", "labels": [], "entities": []}, {"text": "After evaluating against the translation dictionary gold standard, the result of the experiment with complete-link, average-link, k-means and SOM clustering using different similarity measures is shown in.", "labels": [], "entities": [{"text": "SOM clustering", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7807674407958984}]}, {"text": "The best recall with the best precision was achieved with the averagelink clustering using the information radius on the original feature space with 47 \u00b1 2 % recall and and 42 \u00b1 2 % precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9991869330406189}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9990624785423279}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9986502528190613}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9913933277130127}]}, {"text": "This produced clusters with an average size of 6.05 words.", "labels": [], "entities": []}, {"text": "The difference between complete-link and average-link clustering is not statistically significant even if the average-link is slightly better.", "labels": [], "entities": []}, {"text": "The recall is statistically significantly better in the original feature space than in the lowdimensional space at the risk level p = 0.05, whereas the precision remains roughly the same.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9996737241744995}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9991216063499451}]}, {"text": "The average-link and complete-link clustering have a statistically significantly better precision than k-means and SOM, respectively, at the risk level p < 0.05.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9977201819419861}]}, {"text": "We can also see that there is hardly any difference in practice between the Euclidean distance on normalized word vectors and the cosine distance despite the fact that the centroids were not normalized when using the squared Euclidean distance with k-means.", "labels": [], "entities": []}, {"text": "As can be seen from   clusters become less noisy.", "labels": [], "entities": []}, {"text": "We then performed a manual evaluation of the output of the best clustering algorithm.", "labels": [], "entities": []}, {"text": "We used one cluster sample from the 453 clusters qualifying for mechanical evaluation and one sample from the whole set of 1753 clusters.", "labels": [], "entities": []}, {"text": "The results of the manual evaluation is shown in Table 6.", "labels": [], "entities": []}, {"text": "The evaluation shows that 69-79 % of the material in the clusters is relevant for constructing a thesaurus.", "labels": [], "entities": []}, {"text": "The manual evaluation agrees with the mechanical evaluation, when the manual evaluation found a synonym content of 52 %, compared to the minimum synonym content of 42 % found by the mechanical evaluation.", "labels": [], "entities": []}, {"text": "This means that the clusters actually contain a few more synonyms than those conservatively agreed on by the three translation dictionaries.", "labels": [], "entities": []}, {"text": "If we evaluate the sample of key clusters drawn from all the words in the test sample, we get a synonym content of 38 %.", "labels": [], "entities": []}, {"text": "This figure is rather low, but can be explained by the fact that many of the words were compound nouns that had no synonyms, which is why the translation dictionaries either did not have them listed or contained no additional source word synonyms for them.", "labels": [], "entities": []}, {"text": "In, we see a few sample clusters whose words we rated during manual evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sample output from the FDG parser for Finnish (with an English gloss added).", "labels": [], "entities": [{"text": "FDG", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8714093565940857}]}, {"text": " Table 3: Cluster synonym content as average recall (R) and precision (P) in per cent (%) with a  standard deviation of 2 % using different clustering methods and similarity measures.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9245830923318863}, {"text": "precision (P)", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9797315150499344}]}, {"text": " Table 4: Cluster synonym content as average recall (R) and precision (P) in per cent (%) with a  standard deviation of 2 % using a denoised and a noisy low-dimensional feature space.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9197484999895096}, {"text": "precision (P)", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9787365943193436}]}]}