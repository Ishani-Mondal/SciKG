{"title": [{"text": "Resolution of Lexical Ambiguities in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Resolution of Lexical Ambiguities", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9194312542676926}]}], "abstractContent": [{"text": "The development of conversational multi-domain spoken dialogue systems poses new challenges for the reliable processing of less restricted user utterances.", "labels": [], "entities": []}, {"text": "Unlike in controlled and restricted dialogue systems a simple one-to-one mapping from words to meanings is no longer feasible here.", "labels": [], "entities": []}, {"text": "In this paper two different approaches to the resolution of lexical ambiguities are applied to a multi-domain corpus of speech recognition output produced from spontaneous utterances in a spoken dialogue system.", "labels": [], "entities": [{"text": "resolution of lexical ambiguities", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.8720951974391937}, {"text": "speech recognition output produced from spontaneous utterances", "start_pos": 120, "end_pos": 182, "type": "TASK", "confidence": 0.844862333365849}]}, {"text": "The resulting evaluations show that all approaches yield significant gains over the majority class baseline performance of .68, i.e. f-measures of .79 for the knowledge-driven approach and .86 for the supervised learning approach .", "labels": [], "entities": []}], "introductionContent": [{"text": "Following we can distinguish between data-and knowledge-driven word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "knowledge-driven word sense disambiguation (WSD)", "start_pos": 46, "end_pos": 94, "type": "TASK", "confidence": 0.7159762723105294}]}, {"text": "Given the basic distinction between written text and spoken utterances, we follow and differentiate further between controlled and conversational spoken dialogue systems.", "labels": [], "entities": []}, {"text": "Neither data-nor knowledge-driven word sense disambiguation has been performed on speech data stemming from human interactions with dialogue systems, since multidomain conversational spoken dialogue systems for human computer interaction (HCI) have not existed in the past.", "labels": [], "entities": [{"text": "knowledge-driven word sense disambiguation", "start_pos": 17, "end_pos": 59, "type": "TASK", "confidence": 0.6676853597164154}]}, {"text": "Now that speech data from multi-domain systems have become available, corresponding experiments and evaluations have become feasible.", "labels": [], "entities": []}, {"text": "In this paper we present the results of first word sense disambiguation annotation experiments on data from spoken interactions with multi-domain dialogue systems.", "labels": [], "entities": [{"text": "first word sense disambiguation annotation", "start_pos": 40, "end_pos": 82, "type": "TASK", "confidence": 0.7371925473213196}]}, {"text": "Additionally, we describe the results of a corresponding evaluation of a data-and a knowledge-driven word sense disambiguation system on that data.", "labels": [], "entities": []}, {"text": "For knowledge-driven disambiguation we examined whether the ontology-based method for computing semantic coherence introduced by can be employed to disambiguate between alternative interpretations, i.e. concept representations, of a given speech recognition hypothesis (SRH) at hand.", "labels": [], "entities": [{"text": "speech recognition hypothesis (SRH)", "start_pos": 239, "end_pos": 274, "type": "TASK", "confidence": 0.8535618185997009}]}, {"text": "We will show the results of its evaluation in the semantic interpretation task of WSD.", "labels": [], "entities": [{"text": "semantic interpretation task", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8083206613858541}, {"text": "WSD", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.46614980697631836}]}, {"text": "For example, in speech recognition hypotheses containing forms of the German verb kommen, i.e. (to) come, a decision had to be made whether its meaning corresponds to the motion sense or to the showing sense, i.e. becoming mapped onto either a MotionDirectedTransliteratedProcess or a WatchPerceptualProcess in the terminology of our spoken language understanding system.", "labels": [], "entities": [{"text": "speech recognition hypotheses containing forms of the German verb kommen", "start_pos": 16, "end_pos": 88, "type": "TASK", "confidence": 0.8087932527065277}]}, {"text": "For a datadriven approach we employed a highly supervised learning algorithm introduced by and trained it on a corpus of annotated data.", "labels": [], "entities": []}, {"text": "A second set of semantically annotated speech recognition hypotheses was employed as a gold-standard for evaluating both the ontology-based and supervised learning method.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6814799457788467}]}, {"text": "Both data sets were annotated by separate human annotators.", "labels": [], "entities": []}, {"text": "All annotated data stems from log files of an automatic speech recognition system that was implemented in the SMARTKOM system (.", "labels": [], "entities": [{"text": "SMARTKOM", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.7425994277000427}]}, {"text": "It is important to point out that there are at least two essential differences between spontaneous speech WSD and textual WSD, i.e., Existing spoken language understanding systems, that are not shallow and thusly produce deep syntactic and semantic representations for multiple domains, e.g. the production system approach described by or unification-based approaches described by, have shown to be more suitable for well-formed input but less robust in case of imperfect input.", "labels": [], "entities": [{"text": "Existing spoken language understanding", "start_pos": 133, "end_pos": 171, "type": "TASK", "confidence": 0.6255540549755096}]}, {"text": "For conversational and reliable dialogue systems that achieve satisfactory scores in evaluation frameworks such as proposed by for multi-modal dialogue systems, we need robust knowledge-or data-driven methods for disambiguating the sometimes less than ideal output of the large vocabulary spontaneous speech recognizers.", "labels": [], "entities": []}, {"text": "In the long run, we would also like to avoid expensive preprocessing work, which is necessary for both ontologydriven and supervised learning methods, i.e. labor intensive ontology engineering and data annotation respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the data collection and annotation experiments performed in order to obtain independent data sets for training and evaluation.", "labels": [], "entities": []}, {"text": "The percentage of correctly disambiguated lexemes from both systems is calculated by the following formula: Where \u00a7 is he result in percent, \u00a2 the number of lexemes that match with the goldstandard, \u00a4 the number of not-decidable ones and the number of total lexemes.", "labels": [], "entities": [{"text": "goldstandard", "start_pos": 185, "end_pos": 197, "type": "METRIC", "confidence": 0.9394655227661133}]}, {"text": "As opposed to the human annotators, both systems always select a specific reading and never assign the value not-decidable.", "labels": [], "entities": []}, {"text": "For this evaluation, therefore, we treat any concept occurring in a not-decidable slot as correct.", "labels": [], "entities": []}, {"text": "7 Such SRHs usually score below the consistency thresholds described by and are not passed on.", "labels": [], "entities": [{"text": "consistency thresholds", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.9676675498485565}]}, {"text": "For this evaluation, ONTOSCORE transformed the SRH from our corpus into concept representations as described above.", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.6918548345565796}]}, {"text": "To perform the WSD task, ONTOSCORE calculates a coherence score for each of these concept sets in . The concepts in the highest ranked set are considered to be the ones representing the correct word meaning in this context.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9188379049301147}, {"text": "ONTOSCORE", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9036464691162109}]}, {"text": "OntoScore has two variations: Using the first variation, the relations between two concepts are weighted for taxonomic relations and for all others.", "labels": [], "entities": []}, {"text": "The second mode allows each relation being assigned an individual weight as described in Section 4.1.", "labels": [], "entities": []}, {"text": "For this purpose, the relations have been weighted according to their level of generalization.", "labels": [], "entities": []}, {"text": "More specific relations should indicate a higher degree of semantic coherence and are therefore weighted cheaper, which means that they -more likely -assign the correct meaning.", "labels": [], "entities": []}, {"text": "Compared to the gold-standard, the original method of reached a precision of 63.76 (f-measure = .78) 8 as compared to 64.75 (f-measure = .79) for the new method described herein (baseline 52.48 ).", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9985201954841614}, {"text": "f-measure", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9554650783538818}]}, {"text": "For the purpose of evaluating a supervised learning approach on our data we used the efficient and general statistical TnT tagger, the short form for Trigrams'n'Tags.", "labels": [], "entities": [{"text": "TnT tagger", "start_pos": 119, "end_pos": 129, "type": "TASK", "confidence": 0.6341340988874435}]}, {"text": "With this tagger it is possible to train anew statistical model with any tagset.", "labels": [], "entities": []}, {"text": "In our case the tagset consisted of part-of-speech specific concepts of the SmartKom ontology.", "labels": [], "entities": []}, {"text": "The data we used for preparing the model consisted of a gold-standard annotation of the training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.7960326274236044}]}, {"text": "Compared to the gold-standard made for the test corpus the method achieved a precision of 75.07 (baseline 52.48 ).", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9989551305770874}]}, {"text": "For a direct comparison we computed f-measures for the human reliability, the majority class baseline method as well as for the knowledge-based and data-driven methods in   In this paper we presented two methods for disambiguating speech recognition hypotheses.", "labels": [], "entities": [{"text": "disambiguating speech recognition hypotheses", "start_pos": 216, "end_pos": 260, "type": "TASK", "confidence": 0.7284926325082779}]}, {"text": "Both methods showed significant gains over the majority class baseline.", "labels": [], "entities": []}, {"text": "The results also show that the statistical method outperforms the ontology-based method.", "labels": [], "entities": []}, {"text": "This is congruent to findings from textual WSD methods, where the results from data-based approaches frequently yield better scores.", "labels": [], "entities": [{"text": "WSD", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8212270736694336}]}, {"text": "However, labeling and training times for these methods are high and costly and they take up a significant amount of memory space.", "labels": [], "entities": [{"text": "labeling", "start_pos": 9, "end_pos": 17, "type": "TASK", "confidence": 0.9590007662773132}]}, {"text": "Furthermore, if new domains -featuring lexical ambiguites hitherto unseen by the statistical model -are integrated into the system, new models must consequently be trained in order to keep performance up to par.", "labels": [], "entities": []}, {"text": "In such cases, new annotated data has to be made available.", "labels": [], "entities": []}, {"text": "The results of the knowledge-based approach show that ontologies can be employed for such tasks even if they have not been constructed specifically for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.8021888732910156}]}, {"text": "Since ontology engineering is at least as costly as annotation and training of statistical models, alternative means for ontology construction and learning need to be pursed.", "labels": [], "entities": [{"text": "ontology engineering", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.9020178020000458}, {"text": "ontology construction", "start_pos": 121, "end_pos": 142, "type": "TASK", "confidence": 0.9028857350349426}]}, {"text": "Nonetheless, projects related the semantic web efforts) continue to increase their coverage and will become dynamically combinable so that new domains can be integrated in less time without the need of manually processed data.", "labels": [], "entities": []}, {"text": "Our future work will involve the testing of an unsupervised method as well as the improvement of the presented approaches.", "labels": [], "entities": []}, {"text": "This will include a compression of the databased model and experiments concerning the scalability of the knowledge-based approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Part of the trigram for GreetingProcess", "labels": [], "entities": [{"text": "GreetingProcess", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6424703598022461}]}, {"text": " Table 2: Part of the lexicon file: model.lex", "labels": [], "entities": []}, {"text": " Table 3: F-measures and gains on the test data", "labels": [], "entities": [{"text": "F-measures", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9979606866836548}]}]}