{"title": [{"text": "Learning Nonstructural Distance Metric by Minimum Cluster Distortions", "labels": [], "entities": [{"text": "Minimum Cluster Distortions", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.5989816784858704}]}], "abstractContent": [{"text": "Much natural language processing still depends on the Euclidean (cosine) distance function between two feature vectors, but this has severe problems with regard to feature weightings and feature correlations.", "labels": [], "entities": []}, {"text": "To answer these problems, we propose an optimal metric distance that can be used as an alternative to the cosine distance, thus accommodating the two problems at the same time.", "labels": [], "entities": []}, {"text": "This metric is optimal in the sense of global quadratic minimization , and can be obtained from the clusters in the training data in a supervised fashion.", "labels": [], "entities": []}, {"text": "We confirmed the effect of the proposed metric distance by a synonymous sentence retrieval task, document retrieval task and the K-means clustering of general vectorial data.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7239595055580139}]}, {"text": "The results showed constant improvement over the baseline method of Eu-clid and tf.idf, and were especially prominent for the sentence retrieval task, showing a 33% increase in the 11-point average precision.", "labels": [], "entities": [{"text": "sentence retrieval task", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.8315659562746683}, {"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.6816968321800232}]}], "introductionContent": [{"text": "Natural language processing involves many kinds of linguistic expressions, such as sentences, phrases, documents and the collection of documents.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6797033747037252}]}, {"text": "Comparing these expressions based on semantic proximity is a fundamental task and has many applications.", "labels": [], "entities": []}, {"text": "Generally, two basic approaches exist to compare two expressions: (a) structural and (b) nonstructural.", "labels": [], "entities": []}, {"text": "Structural approaches make use of syntactic parsing or dependency analysis to make a rigorous comparison; nonstructural approaches use vector representation and provide a rough but fast comparison that is required for search/retrieval from avast amount of corpora.", "labels": [], "entities": []}, {"text": "While structural approaches have recently become available in a kernel-based sophisticated treatment), here we concentrate on nonstructural comparison.", "labels": [], "entities": []}, {"text": "This is not only because nonstructural comparison constitutes an integral part in structural methods (that is, even in hierarchical methods the leaf comparison is still atomic), but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive.", "labels": [], "entities": []}, {"text": "For example, information retrieval has long used the 'bag of words' approach () mainly due to alack of scalable segmentation algorithms and the huge amount of data involved.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.8406438827514648}]}, {"text": "While segmentation algorithms, such as TEXTTILING and its recent successors using the inter-paragraph similarity matrix), all themselves use nonstructural cosine similarity as a measure of semantic proximity between paragraphs.", "labels": [], "entities": []}, {"text": "However, the distance function so far has been largely defined and used ad hoc, usually by a tf.idf weighting scheme) and a simple cosine similarity, equivalently, an Euclidean dot product.", "labels": [], "entities": []}, {"text": "In this paper, we propose an optimal distance function that is parameterized by a global metric matrix.", "labels": [], "entities": []}, {"text": "This metric is optimal in the sense of global quadratic minimization, and can be learned from the given clusters in the training data.", "labels": [], "entities": []}, {"text": "These clusters are often attributable with many forms, such as paragraphs, documents or document collections, as long as the items in the training data are not completely independent.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we describe the issue of traditional Euclidean distances, and section 3 places it into general perspective with related works in machine learning.", "labels": [], "entities": []}, {"text": "Section 4 introduces the proposed metric, and section 5 validates its effect on the task of sentence retrieval, document retrieval and the K-means clustering.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.757905900478363}, {"text": "document retrieval", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7779726982116699}, {"text": "K-means clustering", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.628126710653305}]}, {"text": "Sections 6 and 7 present discussions and the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For this purpose, we used the 20-Newsgroup dataset.", "labels": [], "entities": [{"text": "20-Newsgroup dataset", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.8731585443019867}]}, {"text": "This is a standard text classification dataset that has a relatively large number of classes, 20.", "labels": [], "entities": [{"text": "text classification", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7693338990211487}]}, {"text": "Among the 20 newsgroups, we selected 16 clusters of training data and 4 clusters of test data, and performed 5-fold cross validation.", "labels": [], "entities": []}, {"text": "The maximum number of documents per cluster is 100, and when it exceeds this limit, we made a random sampling of 100 documents as the sentence retrieval experiment.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.8040024638175964}]}, {"text": "Because our proposed metric is calculated from the distribution of vectors in high-dimensional feature space, it becomes inappropriate if the norm of the vectors (largely proportional to document length) differs much from document to document.", "labels": [], "entities": []}, {"text": "3 Therefore, we used subsampling/oversampling to form a median length (130 words) on training documents.", "labels": [], "entities": []}, {"text": "Further, we preprocessed them with tf.idf as a baseline method.", "labels": [], "entities": []}, {"text": "shows R-precision and 11-point average precision.", "labels": [], "entities": [{"text": "R-precision", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9899119734764099}, {"text": "11-point average precision", "start_pos": 22, "end_pos": 48, "type": "METRIC", "confidence": 0.7078821261723837}]}, {"text": "Since the test data contains 4 clusters, the baselines of precision are 0.25.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9990053772926331}]}, {"text": "We can see from both results that metric distance produces a better retrieval over the tf.idf and dot product.", "labels": [], "entities": []}, {"text": "However, refinements in precision are certain (average p = 0.0243) but subtle.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.999466598033905}]}], "tableCaptions": [{"text": " Table 1: Newsgroup text retrieval results.", "labels": [], "entities": [{"text": "Newsgroup text retrieval", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7957354386647543}]}]}