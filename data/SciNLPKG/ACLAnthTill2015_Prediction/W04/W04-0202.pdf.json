{"title": [{"text": "COOPML: Towards Annotating Cooperative Discourse", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we present a preliminary version of COOPML, a language designed for annotating cooperative discourse.", "labels": [], "entities": []}, {"text": "We investigate the different linguistic marks that identify and characterize the different forms of cooperativity found in written texts from FAQs, Forums and emails.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "At this stage, it is necessary to have evaluated by human annotators how clear, well-delimited and easy to use this classification is.", "labels": [], "entities": []}, {"text": "We do not have yet precise results, but it is clear that judgments may vary from one annotator to another.", "labels": [], "entities": []}, {"text": "This is not only due to the generic character of our definitions, but also to the existence of continuums between categories, and to the interpretation of responses that may vary depending on context, profile and culture of annotators.", "labels": [], "entities": []}, {"text": "An experiment carried out on three independent subjects (annotation task followed by a discussion of the results) reveals that there is a clear consensus of 80% on the annotations we did ourselves.", "labels": [], "entities": []}, {"text": "The other 20% reflect interpretation variations, in general highly contextual.", "labels": [], "entities": []}, {"text": "These 20% are almost the same cases for the three subjects.", "labels": [], "entities": []}, {"text": "In particular, at the level of additional information (CR), we observed some differences in judgement in particular between restrictions (AR) and warnings (AA), and a few others between CSFH and CSFC whose differences may sometimes be only superficial (presentation of the arguments of the response).", "labels": [], "entities": [{"text": "restrictions (AR) and warnings (AA)", "start_pos": 124, "end_pos": 159, "type": "METRIC", "confidence": 0.6897076368331909}]}, {"text": "We can now evaluate the accuracy of the linguistic marks given above.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995042085647583}]}, {"text": "For that purpose, we designed a programme in Prolog (for fast prototyping) that uses: (1) the domain lexicon and ontology, to have access e.g. to term lexicalizations and morphology, and (2) a set of 'local' grammars that implement the different marks.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9671192765235901}]}, {"text": "Since these marks involve lexical and morphological variations, negation, and some long-distance dependencies, grammars area good solution.", "labels": [], "entities": []}, {"text": "Tests were carried out on anew corpus, essentially from airlines FAQ.", "labels": [], "entities": [{"text": "FAQ", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.4866076111793518}]}, {"text": "134 QA pairs have been selected from this corpus containing some form of cooperativity.", "labels": [], "entities": []}, {"text": "The annotation of this corpus is automatic, while the evaluation of the results is manual and is carried out in parallel by both ourselves and by an external professional evaluator.", "labels": [], "entities": []}, {"text": "These 134 QA pairs contain a total of 237 MU, therefore an average of 1.76 MU per response.", "labels": [], "entities": []}, {"text": "Most responses have 2 MU, the maximum observed being 4.", "labels": [], "entities": [{"text": "MU", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9912489652633667}]}, {"text": "Surprisingly, out of the 134 pairs, only 108 contain direct responses followed by various CSF, the other 16 only contain cooperative know-how responses (CSF), without any direct response part.", "labels": [], "entities": []}, {"text": "Evaluation results, although carried out on a relatively small set of QA pairs, give good indications on the accuracy of the linguistic marks, and also on the typology of the different MU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9991047978401184}]}, {"text": "We consider here the MU: DS, AJ, AR, AA, as characterized above: Unit AB C Total correct annotation DS 102  A: number of MU annotated correctly for that category, B: MU not annotated (no decision made), C: incorrect annotation.", "labels": [], "entities": [{"text": "AA", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9754183888435364}, {"text": "Unit AB C Total correct annotation DS 102  A", "start_pos": 65, "end_pos": 109, "type": "METRIC", "confidence": 0.6830984718269772}]}, {"text": "MU boundaries have been correctly identified in 88% of the cases, they are mostly related to punctuation marks.", "labels": [], "entities": [{"text": "MU boundaries", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8154349029064178}]}, {"text": "There are obviously a few delicate cases where annotation is difficult if not impossible.", "labels": [], "entities": []}, {"text": "First, we observed a few discontinuities: an MU can be fragmented.", "labels": [], "entities": [{"text": "MU", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.73807692527771}]}, {"text": "In that case, it is necessary to add an index to the tag so that the different fragments can be unambiguously related, as in: Q: What is the deadline for an internet reservation?", "labels": [], "entities": []}, {"text": "R: < DR index = 1 > In the case of an electronic ticket, you can reserve up to 24h prior to departure < /DR > . < B > You just need to show up at the registration desk < /B > . < DR index = 1 > In the case of a traditional ticket ...", "labels": [], "entities": [{"text": "DR index", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9432116448879242}, {"text": "B", "start_pos": 114, "end_pos": 115, "type": "METRIC", "confidence": 0.9561731219291687}]}, {"text": "The index=1 allows to tie the two fragments of the enumeration.", "labels": [], "entities": []}, {"text": "Ina number of cases the direct response part is rather indirect, making its identification via the means presented above quite delicate: Q: I forgot to note my reservation number, how can I get it?", "labels": [], "entities": []}, {"text": "R: A confirmation email has been sent to you as soon as the reservation has been finalized....", "labels": [], "entities": []}, {"text": "To identify this portion of the response as a DR, it is necessary to infer that the email is a potential container fora reservation number.", "labels": [], "entities": []}], "tableCaptions": []}