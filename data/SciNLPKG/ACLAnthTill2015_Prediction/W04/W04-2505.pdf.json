{"title": [{"text": "Intentions, Implicatures and Processing of Complex Questions", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we introduce two methods for deriving the intentional structure of complex questions.", "labels": [], "entities": [{"text": "deriving the intentional structure of complex questions", "start_pos": 43, "end_pos": 98, "type": "TASK", "confidence": 0.7179262723241534}]}, {"text": "Techniques that enable the derivation of implied information are also presented.", "labels": [], "entities": []}, {"text": "We show that both the intentional structure and the implicatures enabled by it are essential components of Q/A systems capable of successfully processing complex questions.", "labels": [], "entities": []}, {"text": "The results of our evaluation support the claim that there are multiple interactions between the process of answer finding and the coercion of intentions and implicatures.", "labels": [], "entities": [{"text": "answer finding", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.8890767097473145}]}], "introductionContent": [{"text": "The Problem of Question Intentions.", "labels": [], "entities": [{"text": "Question Intentions", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.6973835974931717}]}, {"text": "When using a Question Answering system to find information, the user cannot separate the intentions and beliefs from the formulation of the question.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7231647074222565}]}, {"text": "A direct consequence of this phenomenon is that the user incorporates his or her intentions and beliefs into the interrogation.", "labels": [], "entities": []}, {"text": "For example, when asking the question: Q1: What kind of assistance has North Korea received from the USSR/Russia for its missile program?", "labels": [], "entities": []}, {"text": "the user associate with the question a number of intentions, that maybe expressed a set of intended questions.", "labels": [], "entities": []}, {"text": "Each intended question, in turn generates implied information, that maybe expresses as implied questions.", "labels": [], "entities": []}, {"text": "For question Q1, a list of intended questions and implied questions is detailed in Table1.", "labels": [], "entities": []}, {"text": "Most of the intended questions are similar with the questions evaluated in TREC . For example questions The Text REtrieval Conferences (TREC) are evaluation workshops in which Information Retrieval tasks are annually tested.", "labels": [], "entities": [{"text": "TREC", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.7297853827476501}, {"text": "Text REtrieval Conferences (TREC)", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.7835505505402883}, {"text": "Information Retrieval tasks", "start_pos": 176, "end_pos": 203, "type": "TASK", "confidence": 0.8791472514470419}]}, {"text": "Since 1999 the performance of question answering systems are measured in the TREC QA track.", "labels": [], "entities": [{"text": "question answering", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8955410122871399}, {"text": "TREC QA track", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.7167980670928955}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Question decomposition associated with question Q1", "labels": [], "entities": [{"text": "Question decomposition", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8741922378540039}]}]}