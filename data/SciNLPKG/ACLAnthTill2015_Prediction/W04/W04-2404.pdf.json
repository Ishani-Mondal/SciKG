{"title": [{"text": "Combining Lexical and Syntactic Features for Supervised Word Sense Disambiguation", "labels": [], "entities": [{"text": "Supervised Word Sense Disambiguation", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6253378093242645}]}], "abstractContent": [{"text": "The success of supervised learning approaches to word sense disambiguation is largely dependent on the features used to represent the context in which an ambiguous word occurs.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.7720261414845785}]}, {"text": "Previous work has reached mixed conclusions; some suggest that combinations of syntactic and lexical features will perform most effectively.", "labels": [], "entities": []}, {"text": "However, others have shown that simple lexical features perform well on their own.", "labels": [], "entities": []}, {"text": "This paper evaluates the effect of using different lexical and syntactic features both individually and in combination.", "labels": [], "entities": []}, {"text": "We show that it is possible fora very simple ensemble that utilizes a single lexical feature and a sequence of part of speech features to result in disambiguation accuracy that is near state of the art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9732067584991455}]}], "introductionContent": [{"text": "Most words in natural language exhibit polysemy, that is, they have multiple possible meanings.", "labels": [], "entities": []}, {"text": "Each of these meanings is referred to as a sense, and word sense disambiguation is the process of identifying the intended sense of a target word based on the context in which it is used.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.6670802334944407}]}, {"text": "The context of the target word consists of the sentence in which it occurs, and possibly one or two surrounding sentences.", "labels": [], "entities": []}, {"text": "Consider the following sentence: Harry cast a bewitching spell The target word spell has many possible senses, such as, a charm or incantation, to readout letter by letter, and a period of time.", "labels": [], "entities": []}, {"text": "The intended sense, a charm or incantation, can be identified based on the context, which in this case includes bewitching and a reference to a famous young wizard.", "labels": [], "entities": []}, {"text": "Word sense disambiguation is often approached by supervised learning techniques.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.752724160750707}]}, {"text": "The training data consists of sentences which have potential target words tagged by a human expert with their intended sense.", "labels": [], "entities": []}, {"text": "Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation.", "labels": [], "entities": []}, {"text": "However, both) and ( suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9623836874961853}]}, {"text": "Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g.,,,,).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.7527405222256979}]}, {"text": "However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9571318626403809}]}, {"text": "In this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8974965810775757}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.997199535369873}]}, {"text": "We find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of experiments using the SENSEVAL-1, SENSEVAL-2, line, hard, serve and interest data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9875268340110779}]}, {"text": "Together, this consists of more than 50,000 sense-tagged instances.", "labels": [], "entities": []}, {"text": "This paper also introduces a technique to quantify the optimum gain that is theoretically possible when two feature sets are combined in an ensemble.", "labels": [], "entities": []}, {"text": "In the process, we identify some of the most useful part of speech and parse features.", "labels": [], "entities": [{"text": "parse", "start_pos": 71, "end_pos": 76, "type": "TASK", "confidence": 0.957443118095398}]}], "datasetContent": [{"text": "We conducted experiments using part of speech tagged and parsed versions of the SENSEVAL-2, SENSEVAL-1, line, hard, serve and interest data.", "labels": [], "entities": [{"text": "SENSEVAL-2", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.6982139945030212}]}, {"text": "The packages posSenseval and parseSenseval part of speech tagged and parsed the data, respectively.", "labels": [], "entities": []}, {"text": "posSenseval uses the Brill Tagger while parseSenseval employs the Collins Parser.", "labels": [], "entities": [{"text": "Brill Tagger", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.7811275124549866}, {"text": "Collins Parser", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9383634328842163}]}, {"text": "We used the training and test data divisions that already exist in the SENSEVAL-2 and SENSEVAL-1 data.", "labels": [], "entities": [{"text": "SENSEVAL-2", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.6919968724250793}, {"text": "SENSEVAL-1 data", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.8951370120048523}]}, {"text": "However, the line, hard, serve and interest data do not have a standard division, so we randomly split the instances into test (20%) and training (80%) portions.", "labels": [], "entities": []}, {"text": "The SENSEVAL-2 and SENSEVAL-1 data were created for comparative word sense disambiguation exercises held in the summers of 2001 and 1998, respectively.", "labels": [], "entities": [{"text": "SENSEVAL-1 data", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.6857218742370605}, {"text": "comparative word sense disambiguation", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.6850409507751465}]}, {"text": "The SENSEVAL-2 data consists of 4,328 test instances and 8,611 training instances and include a total of 73 nouns, verbs and adjectives.", "labels": [], "entities": [{"text": "SENSEVAL-2 data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.6970173120498657}]}, {"text": "The training data has the target words annotated with senses from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9893303513526917}]}, {"text": "The target words have a varied number of senses ranging from two for collaborate, graceful and solemn to 43 for turn.", "labels": [], "entities": []}, {"text": "The SENSEVAL-1 data has 8,512 test and 13,276 training instances, respectively.", "labels": [], "entities": [{"text": "SENSEVAL-1 data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7207872569561005}]}, {"text": "The number of possible senses for these words range from 2 to 15, and are tagged with senses from the dictionary Hector.", "labels": [], "entities": [{"text": "Hector", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.6975577473640442}]}, {"text": "The line data consists of 4,149 instances where the noun line is used in one of six possible WordNet senses.", "labels": [], "entities": []}, {"text": "This data was extracted from the 1987-1989 Wall Street Journal (WSJ) corpus, and the American Printing House for the Blind (APHB) corpus.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9413102184023175}, {"text": "American Printing House for the Blind (APHB) corpus", "start_pos": 85, "end_pos": 136, "type": "DATASET", "confidence": 0.8508438289165496}]}, {"text": "The distribution of senses is somewhat skewed with more than 50% of the instances used in the product sense while all the other instances more or less equally distributed among the other five senses.", "labels": [], "entities": []}, {"text": "The hard data consists of 4,337 instances taken from the San Jose Mercury News Corpus (SJM) and are annotated with one of three senses of the adjective hard, from WordNet.", "labels": [], "entities": [{"text": "San Jose Mercury News Corpus (SJM)", "start_pos": 57, "end_pos": 91, "type": "DATASET", "confidence": 0.8070520684123039}, {"text": "WordNet", "start_pos": 163, "end_pos": 170, "type": "DATASET", "confidence": 0.9768575429916382}]}, {"text": "The distribution of instances is skewed with almost 80% of the instances used in the not easy -difficult sense.", "labels": [], "entities": []}, {"text": "The serve data consists of 5,131 instances with the verb serve as the target word.", "labels": [], "entities": []}, {"text": "They are annotated with one of four senses from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9862768650054932}]}, {"text": "Like line it was created from the WSJ and APHB corpora.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.9478967189788818}, {"text": "APHB corpora", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8417962491512299}]}, {"text": "The interest data consists of 2,368 instances where the noun interest is used in one of six senses taken from the Longman Dictionary of Contemporary English (LDOCE).", "labels": [], "entities": [{"text": "Longman Dictionary of Contemporary English (LDOCE)", "start_pos": 114, "end_pos": 164, "type": "DATASET", "confidence": 0.9331153035163879}]}, {"text": "The instances are extracted from the part of speech tagged subset of the Penn Treebank Wall Street Journal Corpus (ACL/DCI version).", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal Corpus (ACL/DCI version", "start_pos": 73, "end_pos": 130, "type": "DATASET", "confidence": 0.9623611948706887}]}, {"text": "The SyntaLex word sense disambiguation package was used to carryout our experiments.", "labels": [], "entities": []}, {"text": "It uses the C4.5 algorithm, as implemented by the J48 program in the Waikato Environment for Knowledge Analysis) to learn a decision tree for each word to be disambiguated.", "labels": [], "entities": []}, {"text": "We use the majority classifier as a baseline point of comparison.", "labels": [], "entities": []}, {"text": "This is a classifier that assigns all instances to the most frequent sense in the training data.", "labels": [], "entities": []}, {"text": "Our system defaults to the majority classifier if it lacks any other recourse, and therefore it disambiguates all instances.", "labels": [], "entities": []}, {"text": "We thus, report our results in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993557333946228}]}, {"text": "shows our overall experimental results, which will be discussed in the sections that follow.", "labels": [], "entities": []}, {"text": "Note that the results of the majority classifier appear at the bottom of that table, and that the most accurate result for each set of of data is shown in boldface.", "labels": [], "entities": [{"text": "accurate", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9692775011062622}]}], "tableCaptions": [{"text": " Table 1: Supervised WSD Accuracy by Feature Type", "labels": [], "entities": [{"text": "WSD", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9222692847251892}, {"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9132118821144104}]}, {"text": " Table 2: The Best Combinations of Syntactic and Lexical Features", "labels": [], "entities": []}]}