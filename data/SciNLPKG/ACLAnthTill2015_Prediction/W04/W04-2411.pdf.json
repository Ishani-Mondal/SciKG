{"title": [{"text": "Calculating Semantic Distance between Word Sense Probability Distributions", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic similarity measures have focused on individual word senses.", "labels": [], "entities": []}, {"text": "However, in many applications , it maybe informative to compare the overall sense distributions for two different contexts.", "labels": [], "entities": []}, {"text": "We propose anew method for comparing two probability distributions over WordNet, which captures in a single measure the aggregate semantic distance of the component nodes, weighted by their probability.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9641389846801758}]}, {"text": "Previous such measures compute only the dis-tributional distance, and do not take into account the semantic similarity between Word-Net senses across the distributions.", "labels": [], "entities": []}, {"text": "To incorporate semantic similarity, we calculate the (dis)similarity between two probability distributions as a weighted distance \"travelled\" from one to the other through the WordNet hierarchy.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 176, "end_pos": 193, "type": "DATASET", "confidence": 0.9366030693054199}]}, {"text": "We evaluate the measure by applying it to the acquisition of verb argument alternation knowledge, and find that overall it outperforms existing distance measures.", "labels": [], "entities": [{"text": "acquisition of verb argument alternation knowledge", "start_pos": 46, "end_pos": 96, "type": "TASK", "confidence": 0.6440149694681168}]}], "introductionContent": [{"text": "Much attention has recently been given to calculating the similarity of word senses, in support of various natural language learning and processing tasks.", "labels": [], "entities": []}, {"text": "Such techniques apply within a semantic hierarchy, or ontology, such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9572738409042358}]}, {"text": "Typical methods comprise an edgedistance measurement over the two sense nodes being compared within the hierarchy (.", "labels": [], "entities": []}, {"text": "Other approaches instead assume a probability distribution over the entire sense hierarchy; similarity is captured between individual senses by a formula over the information content (negative log probabilities) of relevant nodes (e.g.,).", "labels": [], "entities": []}, {"text": "The latter case assumes that there is a single WordNet probability distribution of interest, which is estimated by populating the hierarchy with word frequencies from an appropriate corpus (e.g,.", "labels": [], "entities": []}, {"text": "But some problems more naturally give rise to multiple conditional probability distributions estimated from counts that are conditioned on various contexts, such as different corpora or differing word usage within a single corpus.", "labels": [], "entities": []}, {"text": "Each of these contexts would yield a distinct WordNet probability distribution, or what we will calla sense profile.", "labels": [], "entities": [{"text": "WordNet probability distribution", "start_pos": 46, "end_pos": 78, "type": "DATASET", "confidence": 0.7766289114952087}]}, {"text": "In this situation, instead of asking how similar are two senses within a single sense profile, one may want to know how similar are two sense profiles-i.e., two (conditional) distributions across the entire set of nodes.", "labels": [], "entities": []}, {"text": "This question could be important to a number of applications.", "labels": [], "entities": []}, {"text": "When two sets of WordNet frequency counts are conditioned on differing contexts, a comparison of the resulting probability distributions can give us a measure of the degree of semantic similarity of the conditioning contexts themselves.", "labels": [], "entities": []}, {"text": "These conditioning contexts maybe any relevant ones defined by the application, such as differing sets of documents (to support asking how similar various document collections are), or differing usages of words within or across document collections (to support asking questions about the similarity of various words in their usages).", "labels": [], "entities": []}, {"text": "For example, we foresee comparing the sense profile of the objects of some verb in a particular set of documents to that of its objects in another set of documents, as an indicator of differing senses of the verb across the collections.", "labels": [], "entities": []}, {"text": "We have developed a general method for answering such questions, formulating a measure of the distance between probability distributions defined over an ontological hierarchy, which we call \"sense profile distance,\" or SPD.", "labels": [], "entities": []}, {"text": "SPD is calculated as a tree distance that aggregates the individual semantic distances between nodes in the hierarchy, weighted by their probability in the two sense profiles.", "labels": [], "entities": []}, {"text": "SPD can be calculated between two probability distributions over any hierarchy that supports a user-supplied semantic distance function.", "labels": [], "entities": []}, {"text": "(In fact, the two sense profiles need not strictly be probability distributions-the measure is well-defined as long as the sum of the values of the two sense profiles is equal.)", "labels": [], "entities": []}, {"text": "We demonstrate our method on a problem that arises in lexical acquisition, of determining whether two different argument positions across syntactic usages of a verb are assigned the same semantic role.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7453957796096802}]}, {"text": "For example, even though the truck shows up in two different syntactic positions, it is the Destination of the action in both of the sentences I loaded the truck with hay and I loaded hay onto the truck.", "labels": [], "entities": []}, {"text": "Automatic detection of such argument alternations is important to acquisition of verb lexical semantics (; Schulte im), and moreover, may play a role in automatic processing of language for applied tasks, such as question-answering (), information extraction (), detection of text relations, and determination of verbparticle constructions).", "labels": [], "entities": [{"text": "Automatic detection of such argument alternations", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7330833474795023}, {"text": "acquisition of verb lexical semantics", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.7690962433815003}, {"text": "information extraction", "start_pos": 236, "end_pos": 258, "type": "TASK", "confidence": 0.836911678314209}, {"text": "detection of text relations", "start_pos": 263, "end_pos": 290, "type": "TASK", "confidence": 0.8392113298177719}, {"text": "determination of verbparticle constructions", "start_pos": 296, "end_pos": 339, "type": "TASK", "confidence": 0.834201768040657}]}, {"text": "We focus on this problem to illustrate how our general method works, and how it aids in a particular natural language learning task.", "labels": [], "entities": []}, {"text": "As in McCarthy (2000), we cast argument alternation detection as a comparison of sense profiles across two different argument positions of a verb.", "labels": [], "entities": [{"text": "argument alternation detection", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.7596293290456136}]}, {"text": "Our method differs, however, in two important respects.", "labels": [], "entities": []}, {"text": "First, our measure can be used on any probability distribution, while McCarthy's approach applies only to a very narrow form of sense profile known as a tree cut.", "labels": [], "entities": []}, {"text": "The dependence on tree cuts greatly limits the applicability of her measure in both this and other problems, since only a particular method can be used for populating the WordNet hierarchy with probability estimates.", "labels": [], "entities": []}, {"text": "Second, our approach provides a much finer-grained measure of the distance between the two profiles.", "labels": [], "entities": []}, {"text": "McCarthy's method rewards probability mass that occurs in the same subtree across two distributions, but does not take into account the distance between the classes that carry the probability mass.", "labels": [], "entities": []}, {"text": "Our new SPD method integrates a comparison of probability distributions over WordNet with anode distance measure.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9755957126617432}]}, {"text": "SPD thus enables us to calculate a more detailed comparison over the probability patterns of WordNet classes.", "labels": [], "entities": [{"text": "WordNet classes", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9189914762973785}]}, {"text": "As our results indicate, this has advantages for argument alternation detection, but more importantly, we think it is crucial for generalizing the method to a wider range of problems.", "labels": [], "entities": [{"text": "argument alternation detection", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.899596651395162}]}, {"text": "1 A tree cut for tree T is a set of nodes C in T such that every leaf node of T has exactly one member of C on a path between it and the root (.", "labels": [], "entities": []}, {"text": "As a sense profile, a tree cut will have a non-zero probability associated with every node in C, and a zero probability for all other nodes in T. in Section 3 has examples of two tree cuts.", "labels": [], "entities": []}, {"text": "In the next section, we present background work on comparing sense profiles, and on using them to detect alternations.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our new SPD measure, and show how it captures both the general differences between WordNet probability distributions, as well as the fine-grained semantic distances between the nodes that comprise them.", "labels": [], "entities": [{"text": "WordNet probability distributions", "start_pos": 109, "end_pos": 142, "type": "DATASET", "confidence": 0.9076223572095236}]}, {"text": "Section 4 presents our corpus methodology and experimental set-up.", "labels": [], "entities": []}, {"text": "In Section 5, we evaluate SPD against other distance measures, and evaluate the different effects of our experimental factors, such as the precise distance functions we use in SPD and the division of our verbs into frequency bands.", "labels": [], "entities": []}, {"text": "By classifying the frequency bands separately, our method achieves a combined accuracy of 70% overall on unseen test verbs, in a task with a baseline of 50%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995645880699158}]}, {"text": "We summarize our findings in Section 6 and point to directions in our on-going work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using (verb,slot,noun) tuples from the corpus, we experimented with several ways of building sense profiles of each verb's target argument slots.", "labels": [], "entities": []}, {"text": "In both our pilot experiment and current development work, we found that the method of overall gave better performance, and so we limit our discussion hereto the results on their model.", "labels": [], "entities": []}, {"text": "Briefly, populate the WordNet hierarchy based on corpus frequencies (of all nouns fora verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using $ f to determine whether to generalize an estimate to a parent node in the hierarchy.", "labels": [], "entities": []}, {"text": "We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew), as well as the general vector distance given by cosine (cos).", "labels": [], "entities": []}, {"text": "These are the measures (aside from SPD) that performed best in our pilot experiments.", "labels": [], "entities": [{"text": "SPD", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9948522448539734}]}, {"text": "It is worth noting that the method of does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.919715404510498}]}, {"text": "This means that the kind of straightforward propagation method used by is not applicable to sense profiles of this type.", "labels": [], "entities": []}, {"text": "To determine whether a verb participates in the causative alternation, we adopt McCarthy's method of using a threshold over the calculated distance measures, testing both the mean and median distances as possible thresholds.", "labels": [], "entities": []}, {"text": "In our case, verbs with slot-distances . \"all\", \"high\", \"high-med\", \"med\", and \"low\" refer to the different frequency bands.", "labels": [], "entities": []}, {"text": "below the threshold (smaller distances) are classified as causative, and those above the threshold as non-causative.", "labels": [], "entities": []}, {"text": "In both our pilot and development work, median thresholds consistently fare better than average thresholds, hence we narrow our discussion hereto using median only.", "labels": [], "entities": []}, {"text": "Using the median also has the advantage of yielding a consistent 50% baseline.", "labels": [], "entities": []}, {"text": "Accuracy is used as the performance measure.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9938997030258179}]}, {"text": "We evaluate the SPD method on sense profiles created using the method of, with comparison to the other distance measures (skew and cos) as explained above.", "labels": [], "entities": []}, {"text": "In the calculation of SPD, we compare the two node distance measures, ), as described in Section 3.", "labels": [], "entities": [{"text": "SPD", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.8198111057281494}]}, {"text": "These settings are mentioned when relevant to distinguishing the results.", "labels": [], "entities": []}, {"text": "Recall that in all experiments the random baseline is 50%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The best development accuracy along with  the measure(s) that produce that result, using a median  threshold. SPD refers to SPD without entropy, using ei- ther", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9756313562393188}]}, {"text": " Table 2: The best accuracy achieved in testing, along  with the measure(s) that produced the result, using a me- dian threshold. SPD refers to SPD without entropy, us- ing the indicated node distance measure. \"all\", \"high\",  \"med\", and \"low\" refer to the different frequency bands.  \"avg(h,m,l)\" refers to the average accuracy of the three  frequency bands.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9987837672233582}, {"text": "avg", "start_pos": 285, "end_pos": 288, "type": "METRIC", "confidence": 0.9554929733276367}, {"text": "accuracy", "start_pos": 319, "end_pos": 327, "type": "METRIC", "confidence": 0.9892789721488953}]}]}