{"title": [{"text": "Adaptive Language and Translation Models for Interactive Machine Translation", "labels": [], "entities": [{"text": "Interactive Machine Translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6473240653673807}]}], "abstractContent": [{"text": "We describe experiments carried outwith adaptive language and translation models in the context of an interactive computer-assisted translation program.", "labels": [], "entities": []}, {"text": "We developed cache-based language models which were then extended to the bilingual case fora cache-based translation model.", "labels": [], "entities": []}, {"text": "We present the improvements we obtained in two contexts: in a theoretical setting, we achieved a drop in perplexity for the new models and, in a more practical situation simulating a user working with the system, we showed that fewer keystrokes would be needed to enter a translation .", "labels": [], "entities": []}], "introductionContent": [{"text": "Cache-based language models were introduced by for the dynamic adaptation of speech language models.", "labels": [], "entities": []}, {"text": "These models, inspired by the memory caches on modern computer architectures, are motivated by the principle of locality which states that a program tends to repeatedly use memory cells that are physically close.", "labels": [], "entities": []}, {"text": "Similarly, when speaking or writing, humans tend to use the same words and phrase constructs from paragraph to paragraph and from sentence to sentence.", "labels": [], "entities": []}, {"text": "This leads us to believe that, when processing a document, the part of a document that is already processed (e.g. for speech recognition, translation or text prediction) gives us very useful information for future processing in the same document or in other related documents.", "labels": [], "entities": [{"text": "speech recognition, translation or text prediction)", "start_pos": 118, "end_pos": 169, "type": "TASK", "confidence": 0.694050308316946}]}, {"text": "A cache-based language model is a language model to which is added a smaller model trained only on the history of the document being processed.", "labels": [], "entities": []}, {"text": "The history is usually the last N words or sentences seen in the document.", "labels": [], "entities": []}, {"text": "obtained a drop in perplexity of nearly 68% when adding an unigram POS (part-of-speech) cache on a 3g-gram model.", "labels": [], "entities": []}, {"text": "obtained a drop of nearly 21% when adding a bigram cache to a trigram model.", "labels": [], "entities": []}, {"text": "also obtained similar results with an exponentially decaying unigram cache.", "labels": [], "entities": []}, {"text": "The major problem with these theoretical results is that they assume the correctness of the material entering the cache.", "labels": [], "entities": []}, {"text": "In practice, this assumption does not always hold, and so a cache can sometimes do more harm than good.", "labels": [], "entities": []}], "datasetContent": [{"text": "As stated earlier, drops in perplexity are theoretical results that have been obtained previously in the case of unilingual dynamic adaptation but for which a corresponding level of practical success was rarely attained because of the cache correctness problem.", "labels": [], "entities": []}, {"text": "To show that the interactive nature of our assistedtranslation application can really benefit from dynamic adaptation, we tested our models in a more realistic translation context.", "labels": [], "entities": [{"text": "dynamic adaptation", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.6993147730827332}]}, {"text": "This test consists of simulating a translator using the IMT system as it proposes words and phrases and accepting, correcting or rejecting the proposals by trying to reproduce a given target translation ().", "labels": [], "entities": []}, {"text": "The metric used is the percentage of keystrokes saved by the use of the system instead of having to type directly all the target text.", "labels": [], "entities": []}, {"text": "For these simulations, we used only a 10K word split of the hansard and of the sniper corpus.", "labels": [], "entities": [{"text": "hansard", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9643313884735107}, {"text": "sniper corpus", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.7152328342199326}]}, {"text": "The reason is that the IMT application poten-   tially proposes new completions after every character typed by the user.", "labels": [], "entities": []}, {"text": "For a 10K word document, it needs to search about 1 million times for high probability words and phrases.", "labels": [], "entities": []}, {"text": "This leads to relatively long simulation times, even though predictions are made at real time speeds.", "labels": [], "entities": []}, {"text": "shows the results obtained with the MDI2B model to which we added a cache component for the reference interpolated trigram distribution.", "labels": [], "entities": []}, {"text": "We can see that the saved keystroke percentages are proportional to the perplexity drops reported in section 3.", "labels": [], "entities": []}, {"text": "The use of our models raises the saved keystrokes by nearly 1.5% in the case of well known documents and by nearly 17% in the case of very different documents.", "labels": [], "entities": []}, {"text": "These are very interesting results fora potential professional use of TransType.", "labels": [], "entities": []}, {"text": "shows an increase in the number of saved keystrokes: 0.44% on the hansard and 3.5% on the sniper corpora.", "labels": [], "entities": [{"text": "hansard", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9540221691131592}]}, {"text": "Once again, the results are not as impressive as the ones obtained for the monolingual dynamic adaptation case.", "labels": [], "entities": []}, {"text": "The results presented in section 3 on language model adaptation confirmed what had been reported in the literature: adding a cache component to a language model leads to a drop in perplexity.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7012370626131693}]}, {"text": "Moreover, we were able to demonstrate that using a cache-based language model inside a translation model leads to better performance for the whole translation model.", "labels": [], "entities": []}, {"text": "We obtained drops in perplexity of 5% on a corpus of the same type as the training corpus and of 50% on a different one.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9611740708351135}]}, {"text": "These theoretical results lead to very good practical results.", "labels": [], "entities": []}, {"text": "We were able to increase the saved keystroke percentage by 1.5% on the similar corpus as the training and by nearly 17% on the different corpus.", "labels": [], "entities": []}, {"text": "These results confirm our hypothesis that dynamic adaptation with cache-based language model can be useful in the context of IMT, particularly for new types of texts.", "labels": [], "entities": [{"text": "IMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.952282726764679}]}, {"text": "Results presented in section 4 on translation model adaptation show that our approach has led to drops in perplexity although not as high as we would have hoped.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.9502522746721903}]}, {"text": "To understand these disappointing results, we analyzed the content of the cache for different configurations of our MDI2BCache model.", "labels": [], "entities": []}, {"text": "shows the results of our sampling.", "labels": [], "entities": []}, {"text": "We tested three model configurations.", "labels": [], "entities": []}, {"text": "The first one, in the first column, was the base MDI2BCache model which adds all active pairs to the cache.", "labels": [], "entities": []}, {"text": "The second configuration, in the second column, was a threshold value of 0.3 that brings about 75% of the pairs being added to the cache.", "labels": [], "entities": []}, {"text": "The last configuration was a model with threshold value of 0.3 and a Viterbi alignment made prior to the addition of pairs in the cache.", "labels": [], "entities": []}, {"text": "The three model configuration were with only one feature weight.", "labels": [], "entities": []}, {"text": "For all three configurations, we took a sample of 10 pairs) and a sample of 100 pairs.", "labels": [], "entities": []}, {"text": "With the second sample, we manually analyzed each pair and counted the number of pairs (shown in the last row of the table) we believed were useful for the model (words that are occasionally translations of one another).", "labels": [], "entities": []}, {"text": "The results obtained in section 4 seem to agree with the current analysis.", "labels": [], "entities": []}, {"text": "From left to right in the table, the pairs seem to contain more information and to be more appropriate additions to the cache.", "labels": [], "entities": []}, {"text": "The configuration with Viterbi alignment which contains 86 good pairs clearly seems to be the configuration with the most interesting pairs.", "labels": [], "entities": []}, {"text": "The problem with such a cache-based translation model seem to be similar to the balance between precision and recall in information retrieval.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9989743232727051}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.992932915687561}]}, {"text": "On one hand, we want to add in the cache every word pair in which the two words are in translation relation in the text.", "labels": [], "entities": []}, {"text": "We further want to add only the pairs in which the two words are really in translation relation in the text.", "labels": [], "entities": []}, {"text": "It seems that with our base model, we add most of the good pairs, but also a lot of bad ones.", "labels": [], "entities": []}, {"text": "With the Viterbi alignment and a threshold value of 0.3, most of the pairs added are good ones, but we are probably missing a number of other appropriate ones.", "labels": [], "entities": [{"text": "Viterbi alignment", "start_pos": 9, "end_pos": 26, "type": "DATASET", "confidence": 0.9125088453292847}]}, {"text": "This comes back to the task of word alignment, which is a very difficult task for computers (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7967874705791473}]}, {"text": "Moreover, we would want to add in the cache only those words for which more than one translation is possible.", "labels": [], "entities": []}, {"text": "For example, the pair (today, aujourd'hui), though it is a very useful pair for the base model, is unlikely to help when added to the cache.", "labels": [], "entities": []}, {"text": "The reason is simple: they are two words that are always translations of one another, so the model will have no problem predicting them.", "labels": [], "entities": []}, {"text": "This ideal of precision and recall and of useful pairs in the cache is obtained by our model with threshold of 0.3, a Viterbi alignment and a cache size of 1000.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9995325803756714}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9994719624519348}]}, {"text": "One disadvantage of our bilingual adaptive model is the way it handles unknown words.", "labels": [], "entities": []}, {"text": "In the cachebased language model, the unknown words were dealt with normally, i.e. they were added to the cache and given a certain probability afterwards.", "labels": [], "entities": []}, {"text": "So, if an unknown word was seen in a certain sentence and then later on, it would receive a probability mass of its own but not the one given to any unknown word.", "labels": [], "entities": []}, {"text": "By having its own probability mass due to its presence in the cache, such previously unknown word can be predicted by the model.", "labels": [], "entities": []}, {"text": "In the case of our MDI2BCache model, because we have not yet implemented an algorithm for guessing the translations of unknown words, they are simply represented within the model as UNK words, which means that the model never learns them.", "labels": [], "entities": []}, {"text": "The results obtained with the sniper corpus shows us that dynamic adaptation is also more helpful for documents that are little known to the model in the bilingual context.", "labels": [], "entities": []}, {"text": "The results are four times better on the sniper corpus than on the Hansard testing corpus.", "labels": [], "entities": [{"text": "sniper corpus", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.7621660828590393}, {"text": "Hansard testing corpus", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.9779582818349203}]}, {"text": "Once again for the bilingual case, the practical test results in the number of saved keystrokes agree with the theoretical results of drops in perplexity.", "labels": [], "entities": []}, {"text": "This result shows that bilingual dynamic adaptation also can be implemented in a practical context and obtain results similar to the theoretical results.", "labels": [], "entities": [{"text": "bilingual dynamic adaptation", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.76967720190684}]}, {"text": "All things considered, we believe that a cachebased translation model shows a great potential for bilingual adaptation and that greater perplexity drops and keystroke savings could be obtained by either reengineering the model or by improving the MDI2BCache model.", "labels": [], "entities": [{"text": "bilingual adaptation", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.8074887096881866}]}], "tableCaptions": [{"text": " Table 1: Perplexities of the MDI2B model with a  cache component included in the reference distribu- tion on the hansard and sniper corpora.", "labels": [], "entities": []}, {"text": " Table 3: MDI2BCache test perplexities. One fea- ture weight, Viterbi alignment version. Sniper test", "labels": [], "entities": []}, {"text": " Table 4: Saved keystrokes raises for the MDI2B  model with cache component in the reference dis- tribution on the hansard and sniper corpora.", "labels": [], "entities": []}, {"text": " Table 5:  Saved keystrokes raises for the  MDI2BCache model with only one feature  weight and Viterbi alignment on the hansard and  sniper corpora.", "labels": [], "entities": [{"text": "Viterbi alignment", "start_pos": 95, "end_pos": 112, "type": "METRIC", "confidence": 0.8711321651935577}]}, {"text": " Table 6: Cache sampling of different configurations  of MDI2BCache model.", "labels": [], "entities": []}]}