{"title": [{"text": "Comparing and Combining Generative and Posterior Probability Models: Some Advances in Sentence Boundary Detection in Speech", "labels": [], "entities": [{"text": "Sentence Boundary Detection", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.8198971152305603}]}], "abstractContent": [{"text": "We compare and contrast two different models for detecting sentence-like units in continuous speech.", "labels": [], "entities": []}, {"text": "The first approach uses hidden Markov sequence models based on N-grams and maximum likelihood estimation, and employs model interpolation to combine different representations of the data.", "labels": [], "entities": []}, {"text": "The second approach models the posterior probabilities of the target classes; it is discriminative and integrates multiple knowledge sources in the maximum entropy (maxent) framework.", "labels": [], "entities": []}, {"text": "Both models combine lexical, syntactic, and prosodic information.", "labels": [], "entities": []}, {"text": "We develop a technique for integrating pre-trained probability models into the maxent framework , and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task.", "labels": [], "entities": [{"text": "sentence-boundary detection task", "start_pos": 186, "end_pos": 218, "type": "TASK", "confidence": 0.8085086544354757}]}, {"text": "An even more substantial improvement is obtained by combining the posterior probabilities of the two systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology to improve automatic speech transcription technology.", "labels": [], "entities": [{"text": "Sentence boundary detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9118517438570658}, {"text": "automatic speech transcription", "start_pos": 280, "end_pos": 310, "type": "TASK", "confidence": 0.6470107535521189}]}, {"text": "Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9633816480636597}]}, {"text": "The task is thus fundamentally based on both acoustic and textual (via automatic word recognition) information.", "labels": [], "entities": []}, {"text": "From a computational linguistics point of view, sentence units are crucial and assumed inmost of the further processing steps that one would want to apply to such output: tagging and parsing, information extraction, and summarization, among others.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.8734509746233622}, {"text": "information extraction", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.8686544001102448}, {"text": "summarization", "start_pos": 220, "end_pos": 233, "type": "TASK", "confidence": 0.9927915334701538}]}, {"text": "Sentence segmentation from speech is a difficult problem.", "labels": [], "entities": [{"text": "Sentence segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9725031852722168}]}, {"text": "The best systems benchmarked in a recent government-administered evaluation yield error rates between 30% and 50%, depending on the genre of speech processed (measured as the number of missed and inserted sentence boundaries as a percentage of true sentence boundaries).", "labels": [], "entities": [{"text": "error", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9684450626373291}]}, {"text": "Because of the difficulty of the task, which leaves plenty of room for improvement, its relevance to real-world applications, and the range of potential knowledge sources to be modeled (acoustics and text-based, lower-and higher-level), this is an interesting challenge problem for statistical and computational approaches.", "labels": [], "entities": []}, {"text": "All of the systems participating in the recent DARPA RT-03F Metadata Extraction evaluation (National Institute of were based on a hidden Markov model framework, in which word/tag sequences are modeled by N-gram language models (LMs).", "labels": [], "entities": [{"text": "DARPA RT-03F Metadata Extraction evaluation", "start_pos": 47, "end_pos": 90, "type": "TASK", "confidence": 0.6679397642612457}, {"text": "National Institute", "start_pos": 92, "end_pos": 110, "type": "DATASET", "confidence": 0.9279326796531677}]}, {"text": "Additional features (mostly reflecting speech prosody) are modeled as observation likelihoods attached to the Ngram states of the HMM ().", "labels": [], "entities": []}, {"text": "The HMM is a generative modeling approach, since it describes a stochastic process with hidden variables (the locations of sentence boundaries) that produces the observable data.", "labels": [], "entities": [{"text": "generative modeling", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9728482067584991}]}, {"text": "The segmentation is inferred by comparing the likelihoods of different boundary hypotheses.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9713901877403259}]}, {"text": "While the HMM approach is computationally efficient and (as described later) provides a convenient way for modularizing the knowledge sources, it has two main drawbacks: First, the standard training methods for HMMs maximize the joint probability of observed and hidden events, as opposed to the posterior probability of the correct hidden variable assignment given the observations.", "labels": [], "entities": []}, {"text": "The latter is a criterion more closely related to classification error.", "labels": [], "entities": [{"text": "classification error", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7098113596439362}]}, {"text": "Both techniques have been used previously for traditional NLP tasks, but they are not straightforward to apply in our case because of the diverse nature of the knowledge sources used in sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 186, "end_pos": 207, "type": "TASK", "confidence": 0.7195661067962646}]}, {"text": "We describe the techniques we developed to workaround these difficulties, and compare classification accuracy of the old and new approach on different genres of speech.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9303323030471802}]}, {"text": "We also investigate how word recognition error affects that comparison.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7309712171554565}]}, {"text": "Finally, we show that a simple combination of the two approaches turns out to be highly effective in improving the best previous results obtained on a benchmark task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments comparing the two modeling approaches were conducted on two corpora: broadcast news (BN) and conversational telephone speech (CTS).", "labels": [], "entities": []}, {"text": "BN and CTS differ in genre and speaking style.", "labels": [], "entities": [{"text": "BN", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.770173192024231}]}, {"text": "These differences are reflected in the frequency of SU boundaries: about 14% of inter-word boundaries are SUs in CTS, compared to roughly 8% in BN.", "labels": [], "entities": [{"text": "SU boundaries", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8784335255622864}]}, {"text": "The corpora are annotated by LDC according to the guidelines of of broadcast news shows in the training set and 3 hours (6 shows) in the test set.", "labels": [], "entities": [{"text": "LDC", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8383538722991943}]}, {"text": "The SU detection task is evaluated on both the reference transcriptions (REF) and speech recognition outputs (STT).", "labels": [], "entities": [{"text": "SU detection", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9890356957912445}]}, {"text": "The speech recognition output is obtained from the SRI recognizer (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7449840903282166}, {"text": "SRI recognizer", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.5876741111278534}]}, {"text": "System performance is evaluated using the official NIST evaluation tools, which implement the metric described earlier.", "labels": [], "entities": [{"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8663921356201172}]}, {"text": "In our experiments, we compare how the two approaches perform individually and in combination.", "labels": [], "entities": []}, {"text": "The combined classifier is obtained by simply averaging the posterior estimates from the two models, and then picking the event type with the highest probability at each position.", "labels": [], "entities": []}, {"text": "We also investigate other experimental factors, such as the impact of the speech recognition errors, the impact of genre, and the contribution of text versus prosodic information in each model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.6771836280822754}]}, {"text": "shows SU detection results for BN and CTS, using both reference transcriptions and speech recognition output, using the HMM and the maxent approach individually and in combination.", "labels": [], "entities": [{"text": "SU detection", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9670417904853821}, {"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7136470973491669}]}, {"text": "The maxent approach slightly outperforms the HMM approach when evaluating on the reference transcripts, and the combination of the two approaches achieves the best performance for all tasks (significant at p < 0:05 using the sign test on the reference transcription condition, mixed results on using recognition output).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SU detection results (error rate in %) using", "labels": [], "entities": [{"text": "SU detection", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9828785061836243}, {"text": "error rate", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9765004515647888}]}, {"text": " Table 2: Error rates for the two approaches on reference", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.997397780418396}]}, {"text": " Table 3: SU detection error rate (%) using different", "labels": [], "entities": [{"text": "SU", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9301822781562805}, {"text": "detection error rate", "start_pos": 13, "end_pos": 33, "type": "METRIC", "confidence": 0.7320618033409119}]}]}