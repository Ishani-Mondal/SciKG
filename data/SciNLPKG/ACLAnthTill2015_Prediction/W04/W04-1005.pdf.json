{"title": [], "abstractContent": [{"text": "Analysis of 9000 manually written summaries of newswire stories used in four Document Understanding Conferences indicates that approximately 40% of their lexical items do not occur in the source document.", "labels": [], "entities": [{"text": "Document Understanding Conferences", "start_pos": 77, "end_pos": 111, "type": "TASK", "confidence": 0.8044441143671671}]}, {"text": "A further comparison of different summaries of the same document shows agreement on 28% of their vocabulary.", "labels": [], "entities": []}, {"text": "It can be argued that these relationships establish a performance ceiling for automated summarization systems which do not perform syntactic and semantic analysis on the source document.", "labels": [], "entities": [{"text": "summarization", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.8771634697914124}]}], "introductionContent": [{"text": "Automatic summarization systems rely on manually prepared summaries for training data, heuristics and evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8395587205886841}]}, {"text": "Generic summaries are notoriously hard to standardize; biased summaries, even in a most restricted task or application, also tend to vary between authors.", "labels": [], "entities": [{"text": "Generic summaries", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.4952891021966934}]}, {"text": "It is unrealistic to expect one perfect model summary, and the presence of many, potentially quite diverse, models introduces considerable uncertainty into the summarization process.", "labels": [], "entities": []}, {"text": "In addition, many summarization systems tacitly assume that model summaries are somehow close to the source documents.", "labels": [], "entities": [{"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9829905033111572}]}, {"text": "We investigate this assumption, and study the variability of manually produced summaries.", "labels": [], "entities": []}, {"text": "We first describe the collection of documents with summaries which has been accumulated over several years of participation in the Document Understanding Conference (DUC) evaluation exercises sponsored by the National Institute of Science and Technology (NIST).", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC) evaluation", "start_pos": 131, "end_pos": 181, "type": "TASK", "confidence": 0.726330463375364}]}, {"text": "We then present our methodology, discuss the rather pessimistic results, and finally draw a few simple conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Phrases were extracted by applying a 987-item stop list developed by the authors to the test documents.", "labels": [], "entities": []}, {"text": "Each collocation separated by stop words is taken as a phrase . Test documents were tokenized by breaking the text on white space and trimming off punctuation external to the token.", "labels": [], "entities": []}, {"text": "Instances of each sort of item were recorded in a hash table and written to file.", "labels": [], "entities": []}, {"text": "Tokens are an obvious and unambiguous baseline for lexical agreement, one used by such summary evaluation systems as ROUGE (.", "labels": [], "entities": []}, {"text": "On the other hand, it is important to explain what we mean by units we call phrases; they should not be confused with syntactically correct constituents such as noun phrases or verb phrases.", "labels": [], "entities": []}, {"text": "Our units often are not syntactically well-formed.", "labels": [], "entities": []}, {"text": "Adjacent constituents not separated by a stop word are unified, single constituents are divided on any embedded stop word, and those composed entirely of stop words are simply missed.", "labels": [], "entities": []}, {"text": "Our phrases, however, are not n -grams.", "labels": [], "entities": []}, {"text": "A 10-word summary has precisely 9 bigrams but, in this study, only 3.4 phrases on average.", "labels": [], "entities": []}, {"text": "On the continuum of grammatic ality these units can thus be seen as lying somewhere between generated blindly n-grams and syntactically well-formed phrasal constituents.", "labels": [], "entities": []}, {"text": "We judge them to be weakly syntactically motivated 2 and only roughly analogous to the factoids identified by van in the sense that they also express semantic constructs.", "labels": [], "entities": []}, {"text": "Where van Halteren and Teufel identified factoids in 50 summaries, we sacrificed accuracy for automation in order to process 9000.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9986968636512756}]}, {"text": "We then assessed the degree to which a pair of documents for comparison shared vocabulary in terms of these units.", "labels": [], "entities": []}, {"text": "This was done by counting matches between the phrases.", "labels": [], "entities": []}, {"text": "Six different kinds of match were identified and are listed herein what we deem to be decreasing order of stringency.", "labels": [], "entities": []}, {"text": "While the match types are labelled and described in terms of summary and source document for clarity, they apply equally to summary pairs.", "labels": [], "entities": []}, {"text": "Candidate phrases are underlined and matching elements tinted in the examples; headings used in the results table) appear in SMALL CAPS.", "labels": [], "entities": [{"text": "SMALL CAPS", "start_pos": 125, "end_pos": 135, "type": "TASK", "confidence": 0.6455809772014618}]}, {"text": "1 When analysis of a summary indicated that it was a list of comma-or semicolon-delimited phrases, the phrasing provided by the summary author was adopted, including any stopwords present.", "labels": [], "entities": []}, {"text": "Turkey attacks Kurds in Iraq, warns Syria, accusations fuel tensions, Mubarak intercedes is thus split into four phrases with the first retaining the stopword in.", "labels": [], "entities": []}, {"text": "There are 453 such summaries.", "labels": [], "entities": []}, {"text": "While the lexical units in question might be more accurately labelled syntactically motivated ngrams, for simplicity we use phrase in the discussion.", "labels": [], "entities": []}, {"text": "The most demanding, requires candidates agree in all respects.", "labels": [], "entities": []}, {"text": "EXACT after Mayo Clinic stay \u2194 Mayo Clinic group \u2022 Case-insensitive exact match relaxes the requirement for agreement in case.", "labels": [], "entities": [{"text": "EXACT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9831472039222717}, {"text": "Mayo Clinic stay", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.9030624826749166}, {"text": "Mayo Clinic group", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9386081496874491}]}], "tableCaptions": [{"text": " Table 1: Number of documents and summaries by size and by year, and ratios", "labels": [], "entities": []}, {"text": " Table 2: Counts and percentages of vocabulary agreement, by size and total", "labels": [], "entities": []}]}