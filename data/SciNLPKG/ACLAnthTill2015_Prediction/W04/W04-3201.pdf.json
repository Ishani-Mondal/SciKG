{"title": [], "abstractContent": [{"text": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9723097085952759}]}, {"text": "Our formulation uses a factor-ization analogous to the standard dynamic programs for parsing.", "labels": [], "entities": []}, {"text": "In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates.", "labels": [], "entities": []}, {"text": "Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.", "labels": [], "entities": []}, {"text": "We provide an efficient algorithm for learning such models and show experimental evidence of the model's improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work has shown that discriminative techniques frequently achieve classification accuracy that is superior to generative techniques, over a wide range of tasks.", "labels": [], "entities": [{"text": "classification", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.945120632648468}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9198877215385437}, {"text": "generative", "start_pos": 116, "end_pos": 126, "type": "TASK", "confidence": 0.9722281098365784}]}, {"text": "The empirical utility of models such as logistic regression and support vector machines (SVMs) in flat classification tasks like text categorization, word-sense disambiguation, and relevance routing has been repeatedly demonstrated.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.732375830411911}, {"text": "word-sense disambiguation", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.7599507570266724}, {"text": "relevance routing", "start_pos": 181, "end_pos": 198, "type": "TASK", "confidence": 0.8340025246143341}]}, {"text": "For sequence tasks like part-of-speech tagging or named-entity extraction, recent top-performing systems have also generally been based on discriminative sequence models, like conditional Markov models ( or conditional random fields (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7050534933805466}, {"text": "named-entity extraction", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7135866284370422}]}, {"text": "A number of recent papers have considered discriminative approaches for natural language parsing;).", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.6367981433868408}]}, {"text": "Broadly speaking, these approaches fall into two categories, reranking and dynamic programming approaches.", "labels": [], "entities": []}, {"text": "In reranking methods, an initial parser is used to generate a number of candidate parses.", "labels": [], "entities": []}, {"text": "A discriminative model is then used to choose between these candidates.", "labels": [], "entities": []}, {"text": "In dynamic programming methods, a large number of candidate parse trees are represented compactly in a parse tree forest or chart.", "labels": [], "entities": []}, {"text": "Given sufficiently \"local\" features, the decoding and parameter estimation problems can be solved using dynamic programming algorithms.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.6853017657995224}]}, {"text": "For example,) describe approaches based on conditional log-linear (maximum entropy) models, where variants of the inside-outside algorithm can be used to efficiently calculate gradients of the log-likelihood function, despite the exponential number of trees represented by the parse forest.", "labels": [], "entities": []}, {"text": "In this paper, we describe a dynamic programming approach to discriminative parsing that is an alternative to maximum entropy estimation.", "labels": [], "entities": [{"text": "discriminative parsing", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6018940806388855}, {"text": "maximum entropy estimation", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.6595210830370585}]}, {"text": "Our method extends the maxmargin approach of to the case of context-free grammars.", "labels": [], "entities": []}, {"text": "The present method has several compelling advantages.", "labels": [], "entities": []}, {"text": "Unlike reranking methods, which consider only a pre-pruned selection of \"good\" parses, our method is an end-to-end discriminative model over the full space of parses.", "labels": [], "entities": []}, {"text": "This distinction can be very significant, as the set of n-best parses often does not contain the true parse.", "labels": [], "entities": []}, {"text": "For example, in the work of, 41% of the correct parses were not in the candidate pool of \u223c30-best parses.", "labels": [], "entities": []}, {"text": "Unlike previous dynamic programming approaches, which were based on maximum entropy estimation, our method incorporates an articulated loss function which penalizes larger tree discrepancies more severely than smaller ones.", "labels": [], "entities": []}, {"text": "Moreover, like perceptron-based learning, it requires only the calculation of Viterbi trees, rather than expectations overall trees (for example using the inside-outside algorithm).", "labels": [], "entities": []}, {"text": "In practice, it converges in many fewer iterations than CRF-like approaches.", "labels": [], "entities": []}, {"text": "For example, while our approach generally converged in 20-30 iterations, report experiments involving 479 iterations of training for one model, and 1550 iterations for another.", "labels": [], "entities": []}, {"text": "The primary contribution of this paper is the extension of the max-margin approach of to context free grammars.", "labels": [], "entities": []}, {"text": "We show that this framework allows high-accuracy parsing in cubic time by exploiting novel kinds of lexical information.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}