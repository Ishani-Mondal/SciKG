{"title": [{"text": "TINE: A Metric to Assess MT Adequacy", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.6837882995605469}]}], "abstractContent": [{"text": "We describe TINE, anew automatic evaluation metric for Machine Translation that aims at assessing segment-level adequacy.", "labels": [], "entities": [{"text": "TINE", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.8733687400817871}, {"text": "Machine Translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8147279024124146}]}, {"text": "Lexical similarity and shallow-semantics are used as indicators of adequacy between machine and reference translations.", "labels": [], "entities": []}, {"text": "The metric is based on the combination of a lexical matching component and an adequacy component.", "labels": [], "entities": []}, {"text": "Lexical matching is performed comparing bags-of-words without any linguistic annotation.", "labels": [], "entities": [{"text": "Lexical matching", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8982095122337341}]}, {"text": "The adequacy component consists in: i) using ontologies to align predicates (verbs), ii) using semantic roles to align predicate arguments (core arguments and modifiers), and iii) matching predicate arguments using dis-tributional semantics.", "labels": [], "entities": []}, {"text": "TINE's performance is comparable to that of previous metrics at segment level for several language pairs, with average Kendall's tau correlation from 0.26 to 0.29.", "labels": [], "entities": [{"text": "Kendall's tau correlation", "start_pos": 119, "end_pos": 144, "type": "METRIC", "confidence": 0.685435101389885}]}, {"text": "We show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9620867967605591}]}], "introductionContent": [{"text": "The automatic evaluation of Machine Translation (MT) is a long-standing problem.", "labels": [], "entities": [{"text": "evaluation of Machine Translation (MT)", "start_pos": 14, "end_pos": 52, "type": "TASK", "confidence": 0.7976486682891846}]}, {"text": "A number of metrics have been proposed in the last two decades, mostly measuring some form of matching between the MT output (hypothesis) and one or more human (reference) translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9765324592590332}]}, {"text": "However, most of these metrics focus on fluency aspects, as opposed to adequacy.", "labels": [], "entities": []}, {"text": "Therefore, measuring whether the meaning of the hypothesis and reference translation are the same or similar is still an understudied problem.", "labels": [], "entities": []}, {"text": "The most commonly used metrics, BLEU) and alike, perform simple exact matching of n-grams between hypothesis and reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9971987009048462}]}, {"text": "Such a simple matching procedure has well known limitations, including that the matching of non-content words counts as much as the matching of content words, that variations of words with the same meaning are disregarded, and that a perfect matching can happen even if the order of sequences of n-grams in the hypothesis and reference translation are very different, changing completely the meaning of the translation.", "labels": [], "entities": []}, {"text": "A number of other metrics have been proposed to address these limitations, for example, by allowing for the matching of synonyms or paraphrases of content words, such as in METEOR.", "labels": [], "entities": [{"text": "matching of synonyms or paraphrases of content words", "start_pos": 108, "end_pos": 160, "type": "TASK", "confidence": 0.7751983776688576}, {"text": "METEOR", "start_pos": 173, "end_pos": 179, "type": "DATASET", "confidence": 0.803440272808075}]}, {"text": "Other attempts have been made to capture whether the reference translation and hypothesis translations share the same meaning using shallow semantics, i.e., Semantic Role Labeling (.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.7421388228734335}]}, {"text": "However, these are limited to the exact matching of semantic roles and their fillers.", "labels": [], "entities": []}, {"text": "We propose TINE, anew metric that complements lexical matching with a shallow semantic component to better address adequacy.", "labels": [], "entities": [{"text": "TINE", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.960287868976593}]}, {"text": "The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations that considers both the semantic structure of the sentence and the content of the semantic elements.", "labels": [], "entities": []}, {"text": "The metric uses SRLs such as in (.", "labels": [], "entities": [{"text": "SRLs", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9589431285858154}]}, {"text": "However, it analyses the content of predicates and arguments seeking for either exact or \"similar\" matches.", "labels": [], "entities": []}, {"text": "The inexact matching is based on the use of ontologies such as VerbNet () and distributional semantics similarity metrics, such as Dekang Lin's thesaurus . In the remainder of this paper we describe some related work (Section 2), present our metric -TINE -(Section 3) and its performance compared to previous work (Section 4) as well as some further improvements.", "labels": [], "entities": [{"text": "metric -TINE -(", "start_pos": 242, "end_pos": 257, "type": "METRIC", "confidence": 0.6683851480484009}]}, {"text": "We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Kendall's tau segment-level correlation of the  lexical component with human judgments", "labels": [], "entities": []}, {"text": " Table 2: Comparison with previous semantically- oriented metrics using segment-level Kendall's tau cor- relation with human judgments", "labels": [], "entities": []}, {"text": " Table 3: TINE-B: Combination of BLEU and the  shallow-semantic component", "labels": [], "entities": [{"text": "TINE-B", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9713771343231201}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9912875294685364}]}, {"text": " Table 4: Optimized values of the parameters using a ge- netic algorithm and Kendall's tau and final correlation of  the metric on the test sets", "labels": [], "entities": [{"text": "correlation", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.5505335927009583}]}]}