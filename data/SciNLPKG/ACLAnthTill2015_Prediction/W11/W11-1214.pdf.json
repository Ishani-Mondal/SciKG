{"title": [{"text": "Unsupervised Alignment of Comparable Data and Text Resources", "labels": [], "entities": [{"text": "Unsupervised Alignment of Comparable Data and Text", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.7982695272990635}]}], "abstractContent": [{"text": "In this paper we investigate automatic data-text alignment, i.e. the task of automatically aligning data records with textual descriptions , such that data tokens are aligned with the word strings that describe them.", "labels": [], "entities": [{"text": "automatic data-text alignment", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.6519632935523987}]}, {"text": "Our methods make use of log likelihood ratios to estimate the strength of association between data tokens and text tokens.", "labels": [], "entities": []}, {"text": "We investigate data-text alignment at the document level and at the sentence level, reporting results for several methodological variants as well as base-lines.", "labels": [], "entities": [{"text": "data-text alignment", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7285027801990509}]}, {"text": "We find that log likelihood ratios provide a strong basis for predicting data-text alignment.", "labels": [], "entities": [{"text": "predicting data-text alignment", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.8635904391606649}]}], "introductionContent": [{"text": "Much of NLP system building currently uses aligned parallel resources that provide examples of the inputs to a system and the outputs it is intended to produce.", "labels": [], "entities": [{"text": "NLP system building", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7533875505129496}]}, {"text": "In Machine Translation (MT), such resources take the form of sentence-aligned parallel corpora of source-language and target-language texts; in parsing and surface realisation, parse-annotated corpora of naturally occurring texts are used, wherein parsing, the inputs are the sentences in the texts and the outputs are the parses represented by the annotations on the sentences, and in surface realisation, the roles of inputs and outputs are reversed.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8613159000873566}]}, {"text": "In MT parallel resources exist, and in fact are produced in large quantities daily, and in some cases (e.g. multilingual parliamentary proceedings) are publicly available.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9869256019592285}]}, {"text": "Moreover, even if resources are created specifically for system building (e.g. OpenMT evaluations) the cost is offset by the fact that the resulting translation system can be expected to generalise to new domains to some extent.", "labels": [], "entities": []}, {"text": "While parse-annotated corpora are in the first instance created by hand, here too, parsers and surface realisers built on the basis of such corpora are expected to generalise beyond the immediate corpus domain.", "labels": [], "entities": []}, {"text": "In data-to-text generation, as in parsing, parallel resources do not occur naturally and have to be created manually.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7725763618946075}]}, {"text": "The associated cost is, however, incurred for every new task, as systems trained on a given parallel data-text resource cannot be expected to generalise beyond task and domain.", "labels": [], "entities": []}, {"text": "Automatic data-text alignment methods, i.e. automatic methods for creating parallel data-text resources, would be extremely useful for system building in this situation, but no such methods currently exist.", "labels": [], "entities": [{"text": "data-text alignment", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7230929434299469}]}, {"text": "In MT there have been recent efforts (reviewed in the following section) to automatically produce aligned parallel corpora from comparable resources where texts in two different languages are about similar topics, but are not translations of each other).", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9668136835098267}]}, {"text": "Taking our inspiration from this work in MT, in this paper we investigate the feasibility of automatically creating aligned parallel data-text resources from comparable data and text resources available on the web.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9400598406791687}]}, {"text": "This task of automatic data-text alignment, previously unexplored as far as we are aware, is the task of automatically aligning data records with textual descriptions, such that data tokens are aligned with the word strings that describe them.", "labels": [], "entities": [{"text": "data-text alignment", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7191954404115677}]}, {"text": "For example, the data tokens height metres=250 might be aligned with the word string with an altitude of 250 metres above sea level.", "labels": [], "entities": []}, {"text": "We start in Section 2 with an overview of datato-text generation and of related work in MT.", "labels": [], "entities": [{"text": "datato-text generation", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7394586056470871}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9266988635063171}]}, {"text": "In Section 3 we describe our comparable data and text resources and the pre-processing methods we apply to them.", "labels": [], "entities": []}, {"text": "In Section 4 we provide an overview of our unsupervised learning task and of the methodology we have developed for it.", "labels": [], "entities": []}, {"text": "We then describe our methods and results for sentence selection (Section 5) and sentence-level data selection (Section 6) in more detail.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7474463582038879}, {"text": "sentence-level data selection", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6397645572821299}]}, {"text": "We finish with a discussion of our results and some conclusions (Section 7).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Sentence selection results in terms of Precision, Recall and F 1 Score.", "labels": [], "entities": [{"text": "Sentence selection", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.925749808549881}, {"text": "Precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9992183446884155}, {"text": "Recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9929064512252808}, {"text": "F 1 Score", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9884194334348043}]}, {"text": " Table 2: Data selection results in terms of Dice coefficient. Results shown for data selection methods preceded by  different sentence selection methods.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7685137093067169}, {"text": "Dice coefficient", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.8733634948730469}]}]}