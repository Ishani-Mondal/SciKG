{"title": [{"text": "Statistical Dependency Parsing in Korean: From Corpus Generation To Automatic Parsing", "labels": [], "entities": [{"text": "Statistical Dependency Parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8264737923940023}]}], "abstractContent": [{"text": "This paper gives two contributions to dependency parsing in Korean.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9130433797836304}]}, {"text": "First, we build a Korean dependency Treebank from an existing constituent Treebank.", "labels": [], "entities": [{"text": "Korean dependency Treebank", "start_pos": 18, "end_pos": 44, "type": "DATASET", "confidence": 0.7856276432673136}]}, {"text": "For a morphologically rich language like Korean, dependency parsing shows some advantages over constituent parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8314674198627472}, {"text": "constituent parsing", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7265633046627045}]}, {"text": "Since there is not much training data available, we automatically generate dependency trees by applying head-percolation rules and heuristics to the constituent trees.", "labels": [], "entities": []}, {"text": "Second, we show how to extract useful features for dependency parsing from rich morphology in Korean.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8638721704483032}]}, {"text": "Once we build the dependency Tree-bank, any statistical parsing approach can be applied.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.6137878000736237}]}, {"text": "The challenging part is how to extract features from tokens consisting of multiple morphemes.", "labels": [], "entities": []}, {"text": "We suggest away of selecting important morphemes and use only these as features to avoid sparsity.", "labels": [], "entities": []}, {"text": "Our parsing approach is evaluated on three different genres using both gold-standard and automatic morphological analysis.", "labels": [], "entities": []}, {"text": "We also test the impact of fine vs. coarse-grained morphologies on dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8512290120124817}]}, {"text": "With automatic morphological analysis, we achieve labeled attachment scores of 80% +.", "labels": [], "entities": [{"text": "labeled attachment scores", "start_pos": 50, "end_pos": 75, "type": "METRIC", "confidence": 0.6441877583662668}]}, {"text": "To the best of our knowledge, this is the first time that Korean dependency parsing has been evaluated on labeled edges with such a large variety of data.", "labels": [], "entities": [{"text": "Korean dependency parsing", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.5618380010128021}]}], "introductionContent": [{"text": "Statistical parsing has recently been popular in the NLP community; most state-of-the-art parsers take various statistical approaches to achieve their performance).", "labels": [], "entities": [{"text": "Statistical parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8368997275829315}]}, {"text": "The biggest advantage of statistical parsing over rule-based parsing is that it can automatically adapt to new domains, genres, or languages as long as it is provided with enough and proper training data.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7262108325958252}, {"text": "rule-based parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.671621710062027}]}, {"text": "On the other hand, this can also be the biggest drawback for statistical parsing because annotating such training data is manually intensive work that maybe costly and time consuming.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8205121457576752}]}, {"text": "For a morphologically rich language like Korean, dependency parsing shows some advantages over constituent parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8314674198627472}, {"text": "constituent parsing", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7265633046627045}]}, {"text": "Unlike phrase structure that is somewhat restricted by word-order (e.g., an object needs to be followed by a verb), dependency structure does not enforce such restrictions (e.g., an object is a dependent of a verb in any position), which makes it more suitable for representing flexible word-order languages.", "labels": [], "entities": []}, {"text": "Korean is known to be a flexible word-order language in that although it generally follows the SOV (subject-object-verb) construction, it still accepts sentences following different orders.", "labels": [], "entities": []}, {"text": "This is because subjects and objects are usually attached to case particles, so locating them in different positions does not create too much ambiguity.", "labels": [], "entities": []}, {"text": "Furthermore, these morphemes (case particles along with many others) often give important clues about dependency relations to their heads, which become very helpful for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8430803418159485}]}, {"text": "To perform statistical dependency parsing, we need sufficiently large training data.", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.7354187369346619}]}, {"text": "There is not much training data available for dependency structure in Korean.", "labels": [], "entities": [{"text": "dependency structure", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8944111168384552}]}, {"text": "However, there is a Treebank, called the Sejong Treebank 1 , containing a large number of constituent trees in Korean (about 60K sentences), formated similarly to the Penn Treebank ().", "labels": [], "entities": [{"text": "Sejong Treebank 1", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9203020532925924}, {"text": "Penn Treebank", "start_pos": 167, "end_pos": 180, "type": "DATASET", "confidence": 0.9933274686336517}]}, {"text": "The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics.", "labels": [], "entities": [{"text": "Penn Treebank style constituent trees", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.9637814879417419}]}, {"text": "By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing.", "labels": [], "entities": [{"text": "Sejong Treebank", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9409628808498383}, {"text": "dependency parsing", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7206545323133469}]}, {"text": "Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied ( In, a verb talk in Korean consists of three morphemes, talk as a noun, do as a verb-derivational suffix, and a final ending marker, such that although it appears to be a single token, it is really a sequence of three individual morphemes where each morpheme has its own POS tag.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7499800026416779}]}, {"text": "It is not clear which combination of these morphemes yields the best representation of the token for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.871919572353363}]}, {"text": "Moreover, deriving joined features from multiple tokens (e.g., a joined feature of POS tags between two tokens) can be problematic; considering all combinations of morphemes within multiple tokens can be cumbersome and generate very sparse features.", "labels": [], "entities": []}, {"text": "Obviously, having a good morphological analysis is very important for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9890030026435852}]}, {"text": "There are many automatic morphological analyzers available in Korean).", "labels": [], "entities": [{"text": "morphological analyzers", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6926099956035614}]}, {"text": "Some of them use different kinds of morphologies better suited for their purposes.", "labels": [], "entities": []}, {"text": "It is useful to have a fine-grained morphology; however, a more fine-grained morphology does not necessarily mean a better morphology for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 138, "end_pos": 145, "type": "TASK", "confidence": 0.9760162830352783}]}, {"text": "For instance, breaking a token into too many morphemes may cause a loss in the overall semantics of the token.", "labels": [], "entities": []}, {"text": "Thus, it is worth comparing outputs from different morphological analyzers and seeing how much impact each morphology has on dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.8136529624462128}]}, {"text": "In this paper, we present head-percolation rules and heuristics to convert constituent trees in the Sejong Treebank to dependency trees.", "labels": [], "entities": [{"text": "Sejong Treebank", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.963859349489212}]}, {"text": "We then suggest away of selecting important morphemes for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8605181276798248}]}, {"text": "To get automatically generated morphemes, we use two existing morphological analyzers.", "labels": [], "entities": []}, {"text": "All parsing models are built by a transition-based parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9669612050056458}]}, {"text": "We evaluate our models on test sets in three different genres.", "labels": [], "entities": []}, {"text": "Each test set is evaluated by using both gold-standard and automatic morphological analysis.", "labels": [], "entities": []}, {"text": "We also compare the impact of fine-grained vs. coarse-grained morphologies on dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7962718605995178}]}, {"text": "To the best of our knowledge, this is the first time that Korean dependency parsing has been evaluated on labeled edges with such a large variety of data.", "labels": [], "entities": [{"text": "Korean dependency parsing", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.5618380010128021}]}], "datasetContent": [{"text": "To set an upper bound, we first build a parsing model based on gold-standard morphology from the Sejong Treebank, which is considered fine-grained morphology.", "labels": [], "entities": [{"text": "Sejong Treebank", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9113158881664276}]}, {"text": "To compare the impact of fine-grained vs. coarse-grained morphologies, we train two other  gives an F1-score of 89.59% for morpheme segmentation, and a POS tagging accuracy of 94.66% on correctly segmented morphemes; our parsing model is expected to perform better as the automatic morphological analysis improves.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9994714856147766}, {"text": "morpheme segmentation", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.7933839857578278}, {"text": "POS tagging accuracy", "start_pos": 152, "end_pos": 172, "type": "METRIC", "confidence": 0.6967174013455709}]}, {"text": "On the other hand, the differences between the [auto, fine-grained] and [auto, coarse-grained] models are small.", "labels": [], "entities": []}, {"text": "More specifically, the difference between the average LAS achieved by these two models is statistically significant (McNemar, p = 0.01); however, the difference in the average UAS is not statistically significant, and the average LS is actually higher for the model.", "labels": [], "entities": [{"text": "LAS", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9633112549781799}, {"text": "UAS", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.7756417989730835}, {"text": "LS", "start_pos": 230, "end_pos": 232, "type": "METRIC", "confidence": 0.9558819532394409}]}, {"text": "These results seem to confirm our hypothesis, \"a more fine-grained morphology is not necessarily a better morphology for dependency parsing\"; however, more careful studies need to be done to verify this.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.8937217891216278}]}, {"text": "Furthermore, it is only fair to mention that the [auto, fine-grained] model uses a smaller set of features than the [auto, coarse-grained] model because many lexical features can be replaced with POS tag features without compromising accuracy for the [auto, fine-grained] model, but not for the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 234, "end_pos": 242, "type": "METRIC", "confidence": 0.997082531452179}]}, {"text": "It is interesting to see that the numbers are usually high for LS, which shows that our models successfully learn labeling information from morphemes such as case particles or ending markers.", "labels": [], "entities": []}, {"text": "All three models show robust results across different genres although the accuracies for NP are significantly lower than the others.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9911608695983887}, {"text": "NP", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.5417170524597168}]}, {"text": "We are currently working on the error analysis of why our models perform worse on NP and how to improve accuracy for this genre while keeping the same robustness across the others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9985712766647339}]}], "tableCaptions": [{"text": " Table 10: Number of sentences in training (T), develop- ment (D), and evaluation (E) sets for each genre.", "labels": [], "entities": [{"text": "develop- ment (D)", "start_pos": 48, "end_pos": 65, "type": "METRIC", "confidence": 0.8907634019851685}, {"text": "evaluation (E) sets", "start_pos": 71, "end_pos": 90, "type": "METRIC", "confidence": 0.6933478236198425}]}, {"text": " Table 11: Parsing accuracies achieved by three models (in %). LAS -labeled attachment score, UAS -unlabeled  attachment score, LS -label accuracy score", "labels": [], "entities": [{"text": "LAS -labeled attachment score", "start_pos": 63, "end_pos": 92, "type": "METRIC", "confidence": 0.946553111076355}, {"text": "UAS -unlabeled  attachment score", "start_pos": 94, "end_pos": 126, "type": "METRIC", "confidence": 0.9371571063995361}, {"text": "LS -label accuracy score", "start_pos": 128, "end_pos": 152, "type": "METRIC", "confidence": 0.9190721035003662}]}]}