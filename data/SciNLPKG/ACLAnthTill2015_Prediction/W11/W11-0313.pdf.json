{"title": [{"text": "Assessing Benefit from Feature Feedback in Active Learning for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7381000518798828}]}], "abstractContent": [{"text": "Feature feedback is an alternative to instance labeling when seeking supervision from human experts.", "labels": [], "entities": [{"text": "instance labeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7905052006244659}]}, {"text": "Combination of instance and feature feedback has been shown to reduce the total annotation cost for supervised learning.", "labels": [], "entities": []}, {"text": "However, learning problems may not benefit equally from feature feedback.", "labels": [], "entities": []}, {"text": "It is well understood that the benefit from feature feedback reduces as the amount of training data increases.", "labels": [], "entities": []}, {"text": "We show that other characteristics such as domain, instance granularity, feature space, instance selection strategy and proportion of relevant text, have a significant effect on benefit from feature feedback.", "labels": [], "entities": []}, {"text": "We estimate the maximum benefit feature feedback may provide; our estimate does not depend on how the feedback is solicited and incorporated into the model.", "labels": [], "entities": []}, {"text": "We extend the complexity measures proposed in the literature and propose some new ones to categorize learning problems , and find that they are strong indicators of the benefit from feature feedback.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linear classifiers model the response as a weighted linear combination of the features in input instances.", "labels": [], "entities": []}, {"text": "A supervised approach to learning a linear classifier involves learning the weights for the features from labeled data.", "labels": [], "entities": []}, {"text": "A large number of labeled instances maybe needed to determine the class association of the features and learn accurate weights for them.", "labels": [], "entities": []}, {"text": "Alternatively, the user may directly label the features.", "labels": [], "entities": []}, {"text": "For example, fora sentiment classification task, the user may label features, such as words or phrases, as expressing positive or negative sentiment.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.9066370924313863}]}, {"text": "Prior work ( has demonstrated that users are able to reliably provide useful feedback on features.", "labels": [], "entities": []}, {"text": "Direct feedback on a list of features () is limited to simple features like unigrams.", "labels": [], "entities": []}, {"text": "However, unigrams are limited in the linguistic phenomena they can capture.", "labels": [], "entities": []}, {"text": "Structured features such as dependency relations, paths in syntactic parse trees, etc., are often needed for learning the target concept ().", "labels": [], "entities": []}, {"text": "It is not clear how direct feature feedback can be extended straightforwardly to structured features, as they are difficult to present visually for feedback and may require special expertise to comprehend.", "labels": [], "entities": []}, {"text": "An alternative approach is to seek indirect feedback on structured features) by asking the user to highlight spans of text, called rationales, that support the instance label (.", "labels": [], "entities": []}, {"text": "For example, when classifying the sentiment of a movie review, rationales are spans of text in the review that support the sentiment label for the review.", "labels": [], "entities": [{"text": "classifying the sentiment of a movie review", "start_pos": 18, "end_pos": 61, "type": "TASK", "confidence": 0.8790239351136344}]}, {"text": "Assuming a fixed cost per unit of work, it might be cheaper to ask the user to label a few features, i.e. identify relevant features and their class association, than to label several instances.", "labels": [], "entities": []}, {"text": "Prior work has shown that a combination of instance and feature labeling can be used to reduce the total annotation cost required to learn the target concept.", "labels": [], "entities": []}, {"text": "However, the benefit from feature feedback may vary across learning problems.", "labels": [], "entities": []}, {"text": "If we can estimate the benefit from feature feedback fora given problem, we can minimize the total annotation cost for achieving the desired performance by selecting the optimal annotation strategy (feature feedback or not) at every stage in learning.", "labels": [], "entities": []}, {"text": "In this paper, we present the groundwork for this research problem by analyzing how benefit from feature feedback varies across different learning problems and what characteristics of a learning problem have a significant effect on benefit from feature feedback.", "labels": [], "entities": []}, {"text": "We define a learning problem (P = {D, G, F , L, I, S}) as a tuple of the domain (D), instance granularity (G), feature representation (F ), labeled data units (L), amount of irrelevant text (I) and instance selection strategy (S).", "labels": [], "entities": []}, {"text": "With enough labeled data, we may not benefit from feature feedback.", "labels": [], "entities": []}, {"text": "Benefit from feature feedback also depends on the features used to represent the instances.", "labels": [], "entities": []}, {"text": "If the feature space is large, we may need several labeled instances to identify the relevant features, while relatively fewer labeled features may help us quickly find these relevant features.", "labels": [], "entities": []}, {"text": "Apart from the feature space size, it also matters what types of features are used.", "labels": [], "entities": []}, {"text": "When hand crafted features from a domain expert are used) we expect to gain less from feature feedback as most of the features will be relevant.", "labels": [], "entities": []}, {"text": "On the other hand, when features are extracted automatically as patterns in annotation graphs ( feature feedback can help to identify relevant features from the large feature space.", "labels": [], "entities": []}, {"text": "In active learning, instances to be labeled are selectively sampled in each iteration.", "labels": [], "entities": []}, {"text": "Benefit from feature feedback will depend on the instances that were used to train the model in each iteration.", "labels": [], "entities": []}, {"text": "In the case of indirect feature feedback through rationales or direct feature feedback in context, instances selected will also determine what features receive feedback.", "labels": [], "entities": []}, {"text": "Hence, instance selection strategy should affect the benefit from feature feedback.", "labels": [], "entities": [{"text": "instance selection", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7538897097110748}]}, {"text": "In text classification, an instance may contain a large amount of text, and even a simple unigram representation will generate a lot of features.", "labels": [], "entities": [{"text": "text classification", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7708412110805511}]}, {"text": "Often only apart of the text is relevant for the classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8911703526973724}]}, {"text": "For example, in movie reviews, often the reviewers talk about the plot and characters in addition to providing their opinion about the movie.", "labels": [], "entities": []}, {"text": "Often this extra information is not relevant to the classification task and bloats the feature space without adding many useful features.", "labels": [], "entities": [{"text": "classification task", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8971310257911682}]}, {"text": "With feature feedback, we hope to filter out some of this noise and improve the model.", "labels": [], "entities": []}, {"text": "Thus, the amount of irrelevant information in the instance should play an important role in determining the benefit from feature feedback.", "labels": [], "entities": []}, {"text": "We expect to see less of such noise when the text instance is more concise.", "labels": [], "entities": []}, {"text": "For example, a movie review snippet (about a sentence length) tends to have less irrelevant text than a full movie review (several sentences).", "labels": [], "entities": []}, {"text": "In addition to analyzing document instances with varying amount of noise, we also compare the benefit from feature feedback for problems with different granularity.", "labels": [], "entities": []}, {"text": "Granularity fora learning problem is defined based on the average amount of text in its instances.", "labels": [], "entities": []}, {"text": "Benefit from feature feedback will also depend on how feedback is solicited from the user and how it is incorporated back into the model.", "labels": [], "entities": []}, {"text": "Independently from these factors, we estimate the maximum possible benefit and analyze how it varies across problems.", "labels": [], "entities": []}, {"text": "Next we describe measures proposed in the literature and propose some new ones for categorizing learning problems.", "labels": [], "entities": []}, {"text": "We then discuss our experimental setup and analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the details of our experimental setup followed by the results.", "labels": [], "entities": []}, {"text": "Rationales by definition are spans of text in a review that convey the sentiment of the reviewer and hence are the part of the document most relevant for the classification task.", "labels": [], "entities": []}, {"text": "In order to vary the amount of irrelevant text, we vary the amount of text (measured in terms of the number of characters) around the rationales that is included in the instance representation.", "labels": [], "entities": []}, {"text": "We call this the slack around rationales.", "labels": [], "entities": []}, {"text": "When using the rationales with or without the slack, only features that overlap with the rationales (and the slack, if used) are used to represent the instance.", "labels": [], "entities": []}, {"text": "Since we only have rationales for the movie review documents, we only studied the effect of varying the amount of irrelevant text on this dataset.", "labels": [], "entities": [{"text": "movie review documents", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.7064477602640787}]}, {"text": "{deterministic (deter), uncertainty (uncert)}: Experiment space for analysis of learning problems (P = {D, G, F, L, I, S}) For all our experiments, we used Support Vector Machines (SVMs) with linear kernel for learning (libSVM) in Minorthird).", "labels": [], "entities": [{"text": "Minorthird", "start_pos": 231, "end_pos": 241, "type": "DATASET", "confidence": 0.9675825834274292}]}, {"text": "For identifying the discriminative features we used the information gain score.", "labels": [], "entities": [{"text": "information gain score", "start_pos": 56, "end_pos": 78, "type": "METRIC", "confidence": 0.8644623160362244}]}, {"text": "For all datasets we used 1800 total examples with equal number of positive and negative examples.", "labels": [], "entities": []}, {"text": "We held out 10% of the data for estimating model's uncertainty as explained in Section 3.2.", "labels": [], "entities": [{"text": "estimating model", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.8548988103866577}]}, {"text": "The results we present are averaged over 10 cross validation folds on the remaining 90% of the data (1620 instances).", "labels": [], "entities": []}, {"text": "Ina cross validation fold, 10% data is used for testing (162 instances) and all of the remaining 1458 instances are used as the 'oracle' for calculating the feature complexity measures and estimating the maximum benefit from feature feedback as discussed in Sections 2 and 3.1 respectively.", "labels": [], "entities": []}, {"text": "The training data size is varied from 64 to 1024 instances (from the total of 1458 instances for training in a fold), based on the annotation cost budget.", "labels": [], "entities": []}, {"text": "Instances with their label are added to the training set either in the original order they existed in the dataset, i.e. no selective sampling (deterministic), or in the decreasing order of current model's uncertainty on them.", "labels": [], "entities": []}, {"text": "Uncertainty sampling in SVMs) selects the instances closest to the decision boundary since the model is expected to be most uncertain about these instances.", "labels": [], "entities": []}, {"text": "In each slice of the data, we ensured that there is equal distribution of the positive and negative class.", "labels": [], "entities": []}, {"text": "SVMs do not yield probabilistic output but a decision boundary, a common practice is to fit the decision values from SVMs to a sigmoid curve to estimate the probability of the predicted class (Platt, 1999).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Effect of variables defined in Table 1 on benefit  from feature feedback. Avg IP is the average increase in  performance (F 1) and Avg ILR is the average increase in  the learning rate. Different letters in Grp IP and Grp ILR  indicate significantly different results.", "labels": [], "entities": [{"text": "Avg IP", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9308305978775024}, {"text": "F 1)", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9679500659306844}, {"text": "Avg ILR", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9448298513889313}]}]}