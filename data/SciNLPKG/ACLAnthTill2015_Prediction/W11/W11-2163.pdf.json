{"title": [{"text": "Hierarchical Phrase-Based MT at the Charles University for the WMT 2011 Shared Task", "labels": [], "entities": [{"text": "Hierarchical Phrase-Based MT", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5681410630544027}, {"text": "Charles University", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.9402495622634888}, {"text": "WMT 2011 Shared Task", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.6980616897344589}]}], "abstractContent": [{"text": "We describe our experiments with hierarchical phrase-based machine translation for the WMT 2011 Shared Task.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6500254074732462}, {"text": "WMT 2011 Shared Task", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.63398177921772}]}, {"text": "We trained a system for all 8 translation directions between English on one side and Czech, German, Spanish or French on the other side, though we focused slightly more on the English-to-Czech direction.", "labels": [], "entities": []}, {"text": "We provide a detailed description of our configuration and data so the results are replicable.", "labels": [], "entities": []}], "introductionContent": [{"text": "With so many official languages, Europe is a paradise for machine translation research.", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.8873075842857361}]}, {"text": "One of the largest bodies of electronically available parallel texts is being nowadays generated by the European Union and its institutions.", "labels": [], "entities": []}, {"text": "At the same time, the EU also provides motivation and boosts potential market for machine translation outcomes.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8439559042453766}]}, {"text": "Most of the major European languages belong to one of the following three branches of the Indo-European language family: Germanic, Romance or Slavic.", "labels": [], "entities": []}, {"text": "Such relatedness is responsible for many structural similarities in European languages, although significant differences still exist.", "labels": [], "entities": []}, {"text": "Within the language portfolio selected for the WMT shared task, English, French and Spanish seem to be closer to each other than to the rest.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8591707348823547}]}, {"text": "German, despite being genetically related to English, differs in many properties.", "labels": [], "entities": []}, {"text": "Its word order rules, shifting verbs from one end of the sentence to the other, easily create long-distance dependencies.", "labels": [], "entities": []}, {"text": "Long German compound words are notorious for increasing out-of-vocabulary rate, which has led many researchers to devising unsupervised compound-splitting techniques.", "labels": [], "entities": []}, {"text": "Also, uppercase/lowercase distinction is more important because all German nouns start with an uppercase letter by the rule.", "labels": [], "entities": []}, {"text": "Czech is a language with rich morphology (both inflectional and derivational) and relatively free word order.", "labels": [], "entities": []}, {"text": "In fact, the predicate-argument structure, often encoded by fixed word order in English, is usually captured by inflection (especially the system of 7 grammatical cases) in Czech.", "labels": [], "entities": []}, {"text": "While the free word order of Czech is a problem when translating to English (the text should be parsed first in order to determine the syntactic functions and the English word order), generating correct inflectional affixes is indeed a challenge for Englishto-Czech systems.", "labels": [], "entities": []}, {"text": "Furthermore, the multitude of possible Czech word forms (at least order of magnitude higher than in English) makes the data sparseness problem really severe, hindering both directions.", "labels": [], "entities": []}, {"text": "There are numerous ways how these issues could be addressed.", "labels": [], "entities": []}, {"text": "For instance, parsing and syntax-aware reordering of the source-language sentences can help with the word order differences (same goal could be achieved by a reordering model or asynchronous context-free grammar in a hierarchical system).", "labels": [], "entities": []}, {"text": "Factored translation, a secondary language model of morphological tags or even a morphological generator are some of the possible solutions to the poor-to-rich translation issues.", "labels": [], "entities": [{"text": "Factored translation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6785369366407394}]}, {"text": "Our goal is to run one system under as similar conditions as possible to all eight translation directions, to compare their translation accuracies and see why some directions are easier than others.", "labels": [], "entities": []}, {"text": "Future work will benefit from knowing what are the special processing needs fora given language pair.", "labels": [], "entities": []}, {"text": "The current version of the system does not include really language-specific techniques: we neither split German compounds, nor do we address the peculiarities of Czech mentioned above.", "labels": [], "entities": []}, {"text": "Still, comparability of the results is limited, as the quality and quantity of English-Czech data differs from that of the other pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "All BLEU scores were computed directly by Joshua on the News Test 2011 set.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9622412323951721}, {"text": "News Test 2011 set", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.9941926300525665}]}, {"text": "Note that they differ from what the official evaluation script would report, due to different tokenization.", "labels": [], "entities": []}, {"text": "The set of baseline experiments with all translation directions involved running the system on lowercased News Commentary corpora.", "labels": [], "entities": [{"text": "News Commentary corpora", "start_pos": 106, "end_pos": 129, "type": "DATASET", "confidence": 0.8589117527008057}]}, {"text": "Word alignments were computed on lowercased 4-character stems.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6506767123937607}]}, {"text": "A hexagram language model was trained on the target side of the parallel corpus.", "labels": [], "entities": []}, {"text": "In the en-cs case, word alignments were computed on lemmatized version of the parallel cor-pus.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7354022860527039}]}, {"text": "Hexagram language model was trained on the monolingual data.", "labels": [], "entities": []}, {"text": "Truecased data were used for training, as described above; the BLEU score of this experiment in   An interesting perspective on the models is provided by the feature weights optimized during MERT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9996808767318726}, {"text": "MERT", "start_pos": 191, "end_pos": 195, "type": "TASK", "confidence": 0.6187023520469666}]}, {"text": "We can see in that translation models are trusted significantly more than language models for the en-de, de-en and es-en directions.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9616218209266663}]}, {"text": "In fact, the language model has a low relative weight in all language pairs but en-cs, which was the only pair where we used a significant amount of extra monolingual data.", "labels": [], "entities": []}, {"text": "In the future, we should probably use the Gigaword corpus for the to-English directions.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9645353555679321}]}, {"text": "Setup LM Pt 0 Pt 1 Pt 2 WP en-cs: Feature weights are relative to the weight of LM , the score by the language model.", "labels": [], "entities": []}, {"text": "Then there are the three translation features: Pt 0 = P (e|f ), Pt 1 = P lex (f |e) and Pt 2 = P lex (e|f ).", "labels": [], "entities": []}, {"text": "WP is the word penalty.", "labels": [], "entities": [{"text": "WP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8548571467399597}]}], "tableCaptions": [{"text": " Table 1: Number of sentence pairs and tokens for  every language pair in the parallel training cor- pus. Languages are identified by their ISO 639  codes: cs = Czech, de = German, en = English, es =  Spanish, fr = French. The en-cs line describes the  CzEng + EMEA combined corpus, all other lines  correspond to the respective versions of EuroParl  + News Commentary.", "labels": [], "entities": [{"text": "CzEng + EMEA combined corpus", "start_pos": 253, "end_pos": 281, "type": "DATASET", "confidence": 0.6777318894863129}, {"text": "EuroParl  + News Commentary", "start_pos": 341, "end_pos": 368, "type": "DATASET", "confidence": 0.9676708430051804}]}, {"text": " Table 2: Lowercased BLEU scores of the baseline  experiments on News Test 2011 data: BLEU J is  computed by the system, BLEU l is the official  evaluation by matrix.statmt.org (it differs be- cause of different tokenization). BLEU t is offi- cial truecased evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9917614459991455}, {"text": "News Test 2011 data", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.9800536185503006}, {"text": "BLEU J", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9395501613616943}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9969815611839294}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9968634843826294}]}, {"text": " Table 3: Feature weights are relative to the weight  of LM , the score by the language model. Then  there are the three translation features: P t 0 =  P (e|f ), P t 1 = P lex (f |e) and P t 2 = P lex (e|f ).  W P is the word penalty.", "labels": [], "entities": []}, {"text": " Table 4: Results of experiments with supervised  truecasing. Note that training on truecased corpus  slightly influenced even the lowercased BLEU (cf.  with", "labels": [], "entities": [{"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9932827949523926}]}, {"text": " Table 5: Results of experiments with word align- ment computed on different factors. BLEU J l4 is  the score computed by Joshua on lowercased test  data for the original experiments (alignment based  on lowercased 4-character prefixes). BLEU J lm  is the corresponding score for alignment based on  lemmas.", "labels": [], "entities": [{"text": "BLEU J l4", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9684488375981649}, {"text": "BLEU J lm", "start_pos": 238, "end_pos": 247, "type": "METRIC", "confidence": 0.9626866777737936}]}]}