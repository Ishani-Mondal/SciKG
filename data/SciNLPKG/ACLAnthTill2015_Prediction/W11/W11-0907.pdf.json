{"title": [{"text": "Using Grammar Rule Clusters for Semantic Relation Classification", "labels": [], "entities": [{"text": "Semantic Relation Classification", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.86186816294988}]}], "abstractContent": [{"text": "Automatically-derived grammars, such as the split-and-merge model, have proven helpful in parsing (Petrov et al., 2006).", "labels": [], "entities": [{"text": "parsing", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.9667953848838806}]}, {"text": "As such grammars are refined, latent information is recovered which maybe usable for linguistic tasks besides parsing.", "labels": [], "entities": []}, {"text": "In this paper, we present and examine anew method of semantic relation classification: using automatically-derived grammar rule clusters as a robust knowledge source for semantic relation classification.", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7907312512397766}, {"text": "semantic relation classification", "start_pos": 170, "end_pos": 202, "type": "TASK", "confidence": 0.7035630146662394}]}, {"text": "We examine performance of this feature group on the SemEval 2010 Relation Classification corpus, and find that it improves performance over both more coarse-grained and more fine-grained syntactic and colloca-tional features in semantic relation classification .", "labels": [], "entities": [{"text": "SemEval 2010 Relation Classification corpus", "start_pos": 52, "end_pos": 95, "type": "DATASET", "confidence": 0.6394606173038483}, {"text": "semantic relation classification", "start_pos": 228, "end_pos": 260, "type": "TASK", "confidence": 0.7083678642908732}]}], "introductionContent": [{"text": "In the process of discovering a refined grammar starting from rules in the original treebank grammar, latent-variable grammars recover latent information.", "labels": [], "entities": []}, {"text": "Intuitively, the new split grammar states should reflect linguistic information that has been generalized from the lexical level but is not so general as the original syntactic level.", "labels": [], "entities": []}, {"text": "While the intended use of this information is to improve syntactic parsing, the lexically-derived nature of the split grammar states suggests it may contain semantic information as well.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7247083336114883}]}, {"text": "note that while some of these split grammar states reflect true linguistic information, such as the clustering of verbs with similar dependencies, other grammar states may reflect useless information, such as a split between rules that each terminate in a comma.", "labels": [], "entities": []}, {"text": "However, it is the automatic nature of grammar splitting which shows potential for deriving semantic knowledge; such split grammar states may reflect statistical and linguistic observations not noticed by humans.", "labels": [], "entities": [{"text": "grammar splitting", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7655317783355713}]}, {"text": "In this paper, we use this recovered latent information for the classification of semantic relations.", "labels": [], "entities": [{"text": "classification of semantic relations", "start_pos": 64, "end_pos": 100, "type": "TASK", "confidence": 0.7494141310453415}]}, {"text": "Our goal is to determine whether recovered latent grammatical information is capable of contributing to the real-world linguistic task of relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 138, "end_pos": 161, "type": "TASK", "confidence": 0.9489406049251556}]}, {"text": "We will compare the feature performance of recovered latent information with that of other syntactic and collocational features to determine whether or not the recovered latent information is helpful in semantic relation classification.", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 203, "end_pos": 235, "type": "TASK", "confidence": 0.8221868475278219}]}], "datasetContent": [{"text": "In our first experiment, we compared two systems of Surrounding Words, Open-Class Words, and In-Between Terminal Tags, with and without InBetween Terminal Cluster ID's.", "labels": [], "entities": []}, {"text": "The results are shown in. shows the results of adding more specific Cluster ID features to the more general POStag, Open-Class, and Surrounding-Words features.", "labels": [], "entities": [{"text": "POStag", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.8845483660697937}]}, {"text": "While this could have led to an over-fitted model, apparently it did not.", "labels": [], "entities": []}, {"text": "Overall precision increased from 66.60% to 68.62%, an increase of 2.02%, yet recall also increased, from 64.26% to 65.33%, an increase of 1.07%.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9997914433479309}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9997344613075256}]}, {"text": "The more precise, more closely-fitted features did not harm performance, but actually enhanced it.", "labels": [], "entities": []}, {"text": "The Maximum Entropy learner itself preferred the Cluster ID features: shows perclass POS-tag and Cluster-ID features with a lambda value over 0.25, comparing when both POS-tag features and Cluster ID tags are available, versus just POS-tags (all among other features used in Experiment 1).", "labels": [], "entities": []}, {"text": "When given the opportunity, the MaxEnt learner considered the Cluster ID features more important than the POS-tag features.", "labels": [], "entities": []}, {"text": "As shown in, we can see that adding the Cluster ID's did mildly increase F-measure (by 1.41 %, from 65.01% to 66.42% 3 . However, when viewed on a class-by-class basis, some classes show great improvement with the addition of Cluster ID's while others remain unchanged.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9983208775520325}]}, {"text": "The classes Cause-Effect, Component-Whole, ContentContainer, Instrument-Agency, and Message-Topic all gained significantly with the addition of cluster ID features.", "labels": [], "entities": []}, {"text": "We investigated important features of these classes more carefully.", "labels": [], "entities": []}, {"text": "In our second experiment, using the same experimental set-up but different features, we compared In-Between Words (IBW), IBW Plus POS-tags, and IBW Plus POS-tags Plus Cluster ID features.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The goal of this experiment is to compare Cluster ID features to an even more fine-grained feature, the words themselves.", "labels": [], "entities": []}, {"text": "The words, POS-tags, and Cluster ID tags all concern the same nodes in the sentence.", "labels": [], "entities": []}, {"text": "line, IBW features, and adding coarser grained features, POS-tags and Cluster IDs, we might expect to see a simultaneous decrease in precision and increase in recall from the baseline IBW to the enhanced, POS-tag and Cluster ID versions.", "labels": [], "entities": [{"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9993280172348022}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9993258714675903}]}, {"text": "As can be seen in, this is exactly what happens.", "labels": [], "entities": []}, {"text": "However, Cluster ID features are found to be helpful to the overall goal of semantic relation classification, because they increase recall by much more (4.44%) than they decrease precision (-0.44%).", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.8310026923815409}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9984525442123413}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9987469911575317}]}], "tableCaptions": [{"text": " Table 1: Class distribution in the training section of Se- mEval 2010 Task 8 Semantic Relations Corpus.", "labels": [], "entities": [{"text": "Se- mEval 2010 Task 8 Semantic Relations Corpus", "start_pos": 56, "end_pos": 103, "type": "DATASET", "confidence": 0.693604995807012}]}, {"text": " Table 3: Comparison of Open-Class Words, Surrounding Words, and POS-tags, with and without Cluster ID features.  Per SemEval2010 task standards, total does not include 'Other'. Directionality is evaluated, but results are combined  for viewability. Bold-font differences are most notable.", "labels": [], "entities": []}, {"text": " Table 4: Number of POS-tag and Cluster ID features  with a lambda value over 0.25, with and without Clus- ter ID features being available. High lambda values are  assigned when a classifier finds the features has a high  positive correlation with correct examples in the training  data.", "labels": [], "entities": []}, {"text": " Table 9: F-measure comparison of In-Between Words, IBW plus POS-tags, and IBW plus POS-tags plus Cluster  ID features. Per SemEval2010 task standards, total does not include Other. Bold-font differences are the highest  improvements (or baseline, whichever is higher).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9867716431617737}]}, {"text": " Table 10: Comparison of IBW, IBW plus POS-tags,  and IBW plus POS-tags plus Cluster ID features. Per  SemEval2010 Task 8 standards, total does not include  Other. Bold-font differences are the highest improve- ments (or baseline).", "labels": [], "entities": [{"text": "improve- ments", "start_pos": 202, "end_pos": 216, "type": "METRIC", "confidence": 0.8640956282615662}]}]}