{"title": [{"text": "Parser Evaluation Using Elementary Dependency Matching", "labels": [], "entities": [{"text": "Parser Evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8167904615402222}, {"text": "Elementary Dependency Matching", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.5555688043435415}]}], "abstractContent": [{"text": "We present a perspective on parser evaluation in a context where the goal of parsing is to extract meaning from a sentence.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.9325816929340363}]}, {"text": "Using this perspective, we show why current parser evaluation metrics are not suitable for evaluating parsers that produce logical-form semantics and present an evaluation metric that is suitable, analysing some of the characteristics of this new metric.", "labels": [], "entities": []}], "introductionContent": [{"text": "A plethora of parser evaluation metrics exist, which evaluate different types of information, at different levels of granularity, using different methods of calculation.", "labels": [], "entities": []}, {"text": "All attempt to measure the syntactic quality of parser output, but that is not the only goal of parsing.", "labels": [], "entities": []}, {"text": "The DELPH-IN consortium 1 has produced many grammars of different languages, as well as a number of parsers, all with the aim of extracting meaning from text.", "labels": [], "entities": [{"text": "DELPH-IN consortium 1", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7887316743532816}]}, {"text": "In order to drive development, we need a parser evaluation metric that evaluates against that goal.", "labels": [], "entities": []}, {"text": "That is, we require a metric that measures semantic rather than syntactic output.", "labels": [], "entities": []}, {"text": "In the following section, we reflect on and categorize the semantic information we wish to evaluate, and discuss how current metrics partially overlap with this framework.", "labels": [], "entities": []}, {"text": "We then, after describing some of the specifics of the tools we work with, present an evaluation metric that fits within the given framework, and show, using a couple of case studies, some characteristics of the metric.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparing unknown word handling configu- rations.", "labels": [], "entities": [{"text": "Comparing unknown word handling configu", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.7865301847457886}]}, {"text": " Table 2: Comparing between the SRG and ERG gram- mars over a parallel test suite. PROP type triples are  excluded for compatibility.", "labels": [], "entities": [{"text": "SRG", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.6432859301567078}, {"text": "ERG gram- mars", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.630286768078804}]}]}