{"title": [{"text": "Unsupervised Alignment for Segmental-based Language Understanding", "labels": [], "entities": [{"text": "Unsupervised Alignment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6197318732738495}, {"text": "Segmental-based Language Understanding", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.8815275629361471}]}], "abstractContent": [{"text": "Recent years' most efficient approaches for language understanding are statistical.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.8470901250839233}]}, {"text": "These approaches benefit from a segmental semantic annotation of corpora.", "labels": [], "entities": []}, {"text": "To reduce the production cost of such corpora, this paper proposes a method that is able to match first identified concepts with word sequences in an unsuper-vised way.", "labels": [], "entities": []}, {"text": "This method based on automatic alignment is used by an understanding system based on conditional random fields and is evaluated on a spoken dialogue task using either manual or automatic transcripts.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the very first step to build a spoken language understanding (SLU) module for dialogue systems is the extraction of literal concepts from word sequences hypothesised by a speech recogniser.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.7620862921079}, {"text": "extraction of literal concepts from word sequences hypothesised by a speech recogniser", "start_pos": 109, "end_pos": 195, "type": "TASK", "confidence": 0.7696254104375839}]}, {"text": "To address this issue of concept tagging, several techniques are available.", "labels": [], "entities": [{"text": "concept tagging", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7313007414340973}]}, {"text": "These techniques rely on models, now classic, that can be either discriminant or generative.", "labels": [], "entities": []}, {"text": "Among these, we can cite: hidden Markov models, finite state transducers, maximal entropy Markov models, support vector machines, dynamic Bayesian networks (DBNs) or conditional Markov random fields (CRFs) ().", "labels": [], "entities": []}, {"text": "In (), it is shown that CRFs obtain the best performance on a reference task (MEDIA) in French (), but also on two other comparable corpora in Italian and Polish.", "labels": [], "entities": [{"text": "MEDIA", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9210098385810852}]}, {"text": "Besides, the comparison of the understanding results of manually vs automatically transcribed utterances has shown the robustness of CRFs.", "labels": [], "entities": []}, {"text": "Among the approaches evaluated in) was a method using log-linear models comparable to those used in stochastic machine translation, which turned out to have lower performance than CRF.", "labels": [], "entities": [{"text": "stochastic machine translation", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.6667460103829702}]}, {"text": "In this paper, we further exploit the idea of applying automatic translation techniques to language understanding but limiting ourselves to the objective of obtaining a segmental annotation of training data.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7733558118343353}]}, {"text": "In many former approaches literal interpretation was limited to list lexical-concept relations; for instance this is the case of the PHOENIX system based on the detection of keywords.", "labels": [], "entities": [{"text": "literal interpretation", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9429024159908295}]}, {"text": "The segmental approach allows a finer-grained analysis considering sentences as segment sequences during interpretation.", "labels": [], "entities": []}, {"text": "This characteristic enables the approach to correctly connect the various levels of sentence analysis (lexical, syntactic and semantic).", "labels": [], "entities": [{"text": "sentence analysis", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.717390850186348}]}, {"text": "However, in order to simplify its practical application, segments have been designed specifically for semantic annotation and do not integrate any constraint in their relation with the syntactic units (chunks, phrasal groups, etc.).", "labels": [], "entities": []}, {"text": "Not only it simplifies the annotation process itself but as the overall objective is to use the interpretation module inside a spoken dialogue system, transcribed speech data are noisy and generally bound the performance of syntactic analysers (due to highly spontaneous and ungrammatical utterances from the users, combined with errors from the speech recognizer).", "labels": [], "entities": []}, {"text": "Among other interesting proprieties, segmental approaches offer a convenient way to dissociate the detection of a conceptual unit from the estimation of its associated value.", "labels": [], "entities": []}, {"text": "The value corresponds to the normalisation of the surface form.", "labels": [], "entities": []}, {"text": "For instance, if 97 the segment \"no later than eleven\" is associated with the concept departure-time, its value is \"morning\"; the same value is associated with the segments \"between 8 and noon\" or \"in the morning\".", "labels": [], "entities": []}, {"text": "The value estimation requires a link between concepts and sentence words.", "labels": [], "entities": [{"text": "value estimation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7007512301206589}]}, {"text": "Then it becomes possible to treat the normalisation problem by means of regular expressions or concept-dependent language models (allowing an integrated approach such as described in).", "labels": [], "entities": []}, {"text": "In the case of global approaches (not segmental), value detection must be directly incorporated in the conceptual units to identify, as in ().", "labels": [], "entities": [{"text": "value detection", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.760369747877121}]}, {"text": "The additional level is areal burden and is only affordable when the number of authorised values is low.", "labels": [], "entities": [{"text": "areal burden", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.9447292685508728}]}, {"text": "Obviously a major drawback of the approach is its cost: associating concept tags with a dialogue transcription is already a tedious task and its complexity is largely increased by the requirement fora precise delimitation of the support (lexical segment) corresponding to each tag.", "labels": [], "entities": []}, {"text": "The SLU evaluation campaign MEDIA has been the first opportunity to collect and distribute a reasonably-sized corpus endowed with segmental annotations.", "labels": [], "entities": [{"text": "SLU evaluation campaign MEDIA", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.6491595357656479}]}, {"text": "Anyhow the difficulty remains unchanged each time a corpus has to be collected fora new task.", "labels": [], "entities": []}, {"text": "We propose in this study anew method that reduces the effort required to build training data for segmental annotation models.", "labels": [], "entities": []}, {"text": "Making the assumption that the concepts evoked in a sentence are automatically detected beforehand or provided by an expert, we study how to associate them with their lexical supports without prior knowledge.", "labels": [], "entities": []}, {"text": "A conceptual segmental annotation is obtained using alignment techniques designed to align multilingual parallel corpora in the machine translation domain.", "labels": [], "entities": [{"text": "machine translation domain", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.7524904906749725}]}, {"text": "This annotation can be considered as unsupervised since it is done without a training corpus with links between word sequences and concepts.", "labels": [], "entities": []}, {"text": "We present in the paper the necessary adaptations for the application of the alignment techniques in this new context.", "labels": [], "entities": []}, {"text": "They have been kept to their minimal so as to maintain the highest level of generality, which in return benefits from the availability of existing software tools.", "labels": [], "entities": []}, {"text": "Using a reference annotation, we evaluate the alignment quality from the unsupervised approach in two interesting situations depending on whether the correct order of the concepts is known or not.", "labels": [], "entities": []}, {"text": "Finally, the end-to-end evaluation of the approach is made by measuring the impact of the alignments on the CRF-based understanding system.", "labels": [], "entities": []}, {"text": "After a brief recall of the conceptual decoding principles in Section 2, the principles of automatic alignment of parallel corpora are described in Section 3 along with the specificities due to the alignment of semantic concepts.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiments and comments on the results, while Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of the introduced methods was carried out on the MEDIA corpus.", "labels": [], "entities": [{"text": "MEDIA corpus", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9540666043758392}]}, {"text": "This corpus consists of human-machine dialogues collected with a wizard of Oz procedure in the domain of negotiation of tourist services.", "labels": [], "entities": [{"text": "negotiation of tourist services", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.8983915597200394}]}, {"text": "Produced fora realistic task, it is annotated with 145 semantic concepts and their values (more than 2k in total for the enumerable cases).", "labels": [], "entities": []}, {"text": "The audio data are distributed with their manual transcripts and automatic speech recognition (ASR) hypotheses.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.74268106619517}]}, {"text": "The corpus is divided into three parts: a training set (approximatively 12k utterances), a development set (1.2k) and a test set (3k).", "labels": [], "entities": []}, {"text": "The experiments led on the alignment methods were evaluated on the development corpus using MGIZA++ (, a multi-thread version of GIZA++ () which also allows previously trained IBM alignments models to be applied on the development and test corpora.", "labels": [], "entities": []}, {"text": "The conceptual tagging process was evaluated on the test corpus, using WAPITI ( to train the CRF models.", "labels": [], "entities": [{"text": "WAPITI", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9040692448616028}]}, {"text": "Several setups have been tested: \u2022 manual vs ASR transcriptions, \u2022 inclusion (or not) of values during the error computation.", "labels": [], "entities": [{"text": "ASR transcriptions", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.6706027686595917}]}, {"text": "Several concept orderings (before automatic alignment) have also been considered: \u2022 a first ideal one, which takes reference concept sequences as they are, aka sequential order; \u2022 two more realistic variants that sort concepts either alphabetically or randomly, in order to simulate bag-of-concepts.", "labels": [], "entities": []}, {"text": "Alphabetical order is introduced solely to show that a particular order (which is not related to the natural order) might misled the alignment process by introducing undue regularities.", "labels": [], "entities": []}, {"text": "To give a rough idea, these experiments required a few minutes of computing time to train alignment models of 12k utterances, a few hours to train CRF models (using 8 CPUs on our cluster of Xeon CPUs) and a few seconds to apply alignment and CRF models in order to decode the test corpus.", "labels": [], "entities": []}, {"text": "Alignment quality is estimated using the alignment error rate (AER), a metric often employed in machine translation).", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 41, "end_pos": 67, "type": "METRIC", "confidence": 0.9438750545183817}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7433865070343018}]}, {"text": "If H stands for hypothesis alignments and R for reference alignments, AER is computed by the following relation: 2 In our context, this metrics is evaluated by representing a link between source and target identities by (w i , c j ), instead of the usual indices (i, j).", "labels": [], "entities": [{"text": "AER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9994372725486755}]}, {"text": "Indeed, alignments are then used to tag words.", "labels": [], "entities": []}, {"text": "Besides, concepts to align have positions that differ from the ones in the reference when they are reordered to simulate bags-of-concepts.", "labels": [], "entities": []}, {"text": "As mentioned in the introduction, we resort to widely used tools for alignment in order to be as general as possible in our approach.", "labels": [], "entities": [{"text": "alignment", "start_pos": 69, "end_pos": 78, "type": "TASK", "confidence": 0.9767091274261475}]}, {"text": "We do not modify the algorithms and rely on their generality to deal with specificities of the studied domain.", "labels": [], "entities": []}, {"text": "To train iteratively the alignment models, we use the same pipeline as in MOSES, a widely used machine translation system ( To measure the quality of the built models, the model obtained at the last iteration of this chain is applied on the development corpus.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8249709010124207}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7145068943500519}]}, {"text": "All the words of an utterance should normally be associated with one concept, which makes the IBM models' NULL word useless.", "labels": [], "entities": []}, {"text": "However, in the MEDIA corpus, a null semantic concept is associated with words that do not correspond to a concept relevant for the tourist domain and maybe omitted by counting on the probability with the NULL word included in the IBM models.", "labels": [], "entities": [{"text": "MEDIA corpus", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.8510669767856598}]}, {"text": "Two versions were specifically created to test this hypothesis: one with all the reference concept sequences and another without the null tags.", "labels": [], "entities": []}, {"text": "The results measured when taking into account these tags (AER of 14.2 %) are far better than the ones obtained when they are discarded (AER of 27.4 %), in the word \u2192 concept alignment direction.", "labels": [], "entities": [{"text": "AER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9994910955429077}, {"text": "AER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9990596175193787}]}, {"text": "We decided therefore to keep the null in all the experiments.", "labels": [], "entities": []}, {"text": "presents the alignment results measured on the development corpus according to the way concepts are reordered with respect to the reference and according to the considered alignment direction.", "labels": [], "entities": []}, {"text": "For a fair comparison between both setups, the null concept was ignored in H and R for this series of experiments.", "labels": [], "entities": []}, {"text": "The three first lines exhibit the results obtained with the last IBM4 iteration.", "labels": [], "entities": []}, {"text": "As expected, the AER measured with this model in the concept \u2192 word direction (second line), which can only associate at most one word per concept, is clearly higher than the one obtained in the opposite direction (first line).", "labels": [], "entities": [{"text": "AER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.992996871471405}]}, {"text": "Quite surprisingly, an improvement in terms of AER (third line) over the best direction (first line) is observed using the default MOSES heuristics (called growdiag-final) that symmetrizes alignments obtained in both directions.", "labels": [], "entities": [{"text": "AER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9992885589599609}]}, {"text": "IBM1 models, contrary to other models, do not take into account word index inside source and target sentences, which makes them relevant to deal with bag-of-concepts.", "labels": [], "entities": []}, {"text": "Therefore, we measured how AER varies when using models previously builtin the training chain.", "labels": [], "entities": [{"text": "AER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9966140389442444}]}, {"text": "The results obtained by applying IBM1 and by symmetrizing alignments (last line), show finally that these simple models lead to lower performance than the one measured with IBM4 or even HMM (last line), the concepts being ordered alphabetically or randomly (two last columns).", "labels": [], "entities": [{"text": "IBM1", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8542503118515015}]}, {"text": "The previous experiments have shown that alignment is clearly of lower quality when algorithms are faced with bags-of-concepts instead of well-ordered sequences.", "labels": [], "entities": []}, {"text": "In order to reduce this phenomenon, sequences are reordered after a first alignment A 1 generated by the symmetrized IBM4 model.", "labels": [], "entities": [{"text": "IBM4 model", "start_pos": 117, "end_pos": 127, "type": "DATASET", "confidence": 0.9058132171630859}]}, {"text": "Two strategies have been considered to fix the new position of each concept c i . The first one averages the indices of the words w i that are aligned with c i according to A 1 : The second one weights each word index with their transfer probabilities determined by IBM4: where and \u03bb is a coefficient fixed on the development corpus.", "labels": [], "entities": [{"text": "A", "start_pos": 173, "end_pos": 174, "type": "METRIC", "confidence": 0.9641841053962708}, {"text": "IBM4", "start_pos": 266, "end_pos": 270, "type": "DATASET", "confidence": 0.8529148697853088}]}, {"text": "Training alignment models on the corpus reordered according to pos   or pos 2 (third column) leads to a significant improvement of the AER.", "labels": [], "entities": [{"text": "AER", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.870566189289093}]}, {"text": "This reordering step can be repeated as long as performance goes on improving.", "labels": [], "entities": []}, {"text": "By proceeding like this until step 3 for the alphabetic order and until step 7 for the random order, values of AER below 20 % (last column) are finally obtained.", "labels": [], "entities": [{"text": "AER", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9990391731262207}]}, {"text": "It is noteworthy that random reordering has better results than alphabetic reordering.", "labels": [], "entities": []}, {"text": "Indeed, HMM, IBM3 and IBM4 models have probabilities that are more biased in this latter case, where the same sequences occur more often although many are not in the reference.", "labels": [], "entities": []}, {"text": "In order to measure how spoken language understanding is disturbed by erroneous alignments, CRFs parameters are trained under two conditions: one where concept tagging is performed by an expert and one where corpora are obtained using automatic alignment.", "labels": [], "entities": [{"text": "concept tagging", "start_pos": 152, "end_pos": 167, "type": "TASK", "confidence": 0.7380803525447845}]}, {"text": "The performance criterion used to evaluate the understanding task is the concept error rate (CER).", "labels": [], "entities": [{"text": "concept error rate (CER)", "start_pos": 73, "end_pos": 97, "type": "METRIC", "confidence": 0.9423781236012777}]}, {"text": "CER is computed in a similar way as word error rate (WER) used in speech recognition; it is obtained from the Levenshtein alignment between both hypothesized and reference sequences as the ratio of the sum of the concepts in the hypothesis substituted, inserted or omitted on the total number of concepts in the manual reference annotation.", "labels": [], "entities": [{"text": "CER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.860140860080719}, {"text": "word error rate (WER)", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.8644716640313467}, {"text": "speech recognition", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7308729737997055}]}, {"text": "The null concept is not considered during the score computation.", "labels": [], "entities": []}, {"text": "The CER can also take into account the normalized values in addition to the concept tags.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.56954425573349}]}, {"text": "Starting from a state-of-the-art system (Manual column), degradations due to various alignment conditions are reported in.", "labels": [], "entities": []}, {"text": "It can be noted that the absolute increase in CER is at most 8.0 % (from 17.6 to 25.6 with values) when models are trained on the corpus aligned with IBM models; the ordering information brings it back to 3.7 % (17.6 to 21.3), and finally with automatic transcription the impact of the automatic alignments is smaller (resp. 5.8 % and 2.0 %).", "labels": [], "entities": [{"text": "absolute", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9779614210128784}, {"text": "CER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.5189884901046753}]}, {"text": "As expected random order is preferable to alphabetic order (slight gain of 1 %).", "labels": [], "entities": []}, {"text": "In, the random order alignments are used but this time the n-best lists of alignments are considered and not only the 1-best hypotheses.", "labels": [], "entities": []}, {"text": "Instead of training CRFs with only one version of the alignment fora concept-word sequence pair, we filter out from the n-best lists the alignments having a probability above a given threshold.", "labels": [], "entities": []}, {"text": "It can be observed that varying this confidence threshold allows an improvement of the SLU performance (CER can be reduced by 0.8 % for manual transcription and 0.4 % for automatic transcription).", "labels": [], "entities": [{"text": "CER", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9954375624656677}]}, {"text": "However, this improvement is not propagated to scores with values (CER was reduced at best by 0.1 for manual transcription and was increased for automatic tran-102  scription).", "labels": [], "entities": [{"text": "CER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9964699745178223}]}, {"text": "After closer inspection of the scoring alignments, an explanation for this setback is that the manually-designed rules used for value extraction are perturbed by loose segmentation.", "labels": [], "entities": [{"text": "value extraction", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.8170604109764099}]}, {"text": "This is particularly the case for the concept used to annotate co-references, which has confusions between the values singular and plural (e.g. \"this\" is singular and \"those\" plural).", "labels": [], "entities": []}, {"text": "This issue can be solved by an ad hoc adaptation of the rules.", "labels": [], "entities": []}, {"text": "However, it would infringe our objective of relying upon unsupervised approaches and minimizing human expertise.", "labels": [], "entities": []}, {"text": "Therefore, a better answer would be to resort to a probabilistic scheme also for value extraction (as proposed in).", "labels": [], "entities": [{"text": "value extraction", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.8193246126174927}]}, {"text": "The optimal configuration (confidence threshold of 0.3, 4th row of) is close to the baseline 1-best system in terms of the number of training utterances.", "labels": [], "entities": []}, {"text": "We also tried a slightly different setup which adds the filtered alignments to the former corpus before CRF parameter training (i.e. the 1-best is not filtered in the n-best list).", "labels": [], "entities": []}, {"text": "In that case performance remains pretty stable with respect to the filtering process (CER is around 21.4 % for concepts and 25.2 % for concept+value for thresholds between 0.1 and 0.7).", "labels": [], "entities": [{"text": "CER", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9943859577178955}]}], "tableCaptions": [{"text": " Table 1: AER (%) measured on the MEDIA development corpus with respect to the alignment model used and its  direction.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9996836185455322}, {"text": "MEDIA development corpus", "start_pos": 34, "end_pos": 58, "type": "DATASET", "confidence": 0.9156812032063802}]}, {"text": " Table 2: AER (%) measured on the MEDIA development corpus according to the strategy used to reorder concepts.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9996659755706787}, {"text": "MEDIA development corpus", "start_pos": 34, "end_pos": 58, "type": "DATASET", "confidence": 0.8789457480112711}]}]}