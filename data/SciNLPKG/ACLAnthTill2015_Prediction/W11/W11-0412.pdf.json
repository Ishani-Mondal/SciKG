{"title": [{"text": "Assessing the Practical Usability of an Automatically Annotated Corpus", "labels": [], "entities": [{"text": "Assessing", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9689103960990906}]}], "abstractContent": [{"text": "The creation of a gold standard corpus (GSC) is a very laborious and costly process.", "labels": [], "entities": [{"text": "gold standard corpus (GSC)", "start_pos": 18, "end_pos": 44, "type": "DATASET", "confidence": 0.7898747225602468}]}, {"text": "Silver standard corpus (SSC) annotation is a very recent direction of corpus development which relies on multiple systems instead of human annotators.", "labels": [], "entities": [{"text": "Silver standard corpus (SSC) annotation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6730501907212394}]}, {"text": "In this paper, we investigate the practical usability of an SSC when a machine learning system is trained on it and tested on an unseen benchmark GSC.", "labels": [], "entities": [{"text": "SSC", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9583044648170471}, {"text": "GSC", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.8439767360687256}]}, {"text": "The main focus of this paper is how an SSC can be maximally exploited.", "labels": [], "entities": [{"text": "SSC", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9629166126251221}]}, {"text": "In this process, we inspect several hypotheses which might have influenced the idea of SSC creation.", "labels": [], "entities": [{"text": "SSC creation", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.991978257894516}]}, {"text": "Empirical results suggest that some of the hypotheses (e.g. a positive impact of a large SSC despite of having wrong and missing annotations) are not fully correct.", "labels": [], "entities": []}, {"text": "We show that it is possible to automatically improve the quality and the quantity of the SSC annotations.", "labels": [], "entities": []}, {"text": "We also observe that considering only those sentences of SSC which contain annotations rather than the full SSC results in a performance boost.", "labels": [], "entities": []}], "introductionContent": [{"text": "The creation of a gold standard corpus (GSC) is not only a very laborious task due to the manual effort involved but also a costly and time consuming process.", "labels": [], "entities": [{"text": "gold standard corpus (GSC)", "start_pos": 18, "end_pos": 44, "type": "DATASET", "confidence": 0.781521295507749}]}, {"text": "However, the importance of the GSC to effectively train machine learning (ML) systems cannot be underestimated.", "labels": [], "entities": []}, {"text": "Researchers have been trying for years to find alternatives or at least some compromise.", "labels": [], "entities": []}, {"text": "As a result, self-training, co-training and unsupervised approaches targeted for specific tasks (such as word sense disambiguation, syntactic parsing, etc) have emerged.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.6625744303067526}, {"text": "syntactic parsing", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7048374712467194}]}, {"text": "In the process of these researches, it became clear that the size of the (manually annotated) training corpus has an impact on the final outcome.", "labels": [], "entities": []}, {"text": "Recently an initiative is ongoing in the context of the European project CALBC 1 which aims to create a large, so called silver standard corpus (SSC) using harmonized annotations automatically produced by multiple systems (Rebholz-).", "labels": [], "entities": []}, {"text": "The basic idea is that independent biomedical named entity recognition (BNER) systems annotate a large corpus of biomedical articles without any restriction on the methodology or external resources to be exploited.", "labels": [], "entities": [{"text": "biomedical named entity recognition (BNER)", "start_pos": 35, "end_pos": 77, "type": "TASK", "confidence": 0.7342459431716374}]}, {"text": "The different annotations are automatically harmonized using some criteria (e.g. minimum number of systems to agree on a certain annotation) to yield a consensus based corpus.", "labels": [], "entities": []}, {"text": "This consensus based corpus is called silver standard corpus because, differently from a GSC, it is not created exclusively by human annotators.", "labels": [], "entities": []}, {"text": "Several factors can influence the quantity and quality of the annotations during SSC development.", "labels": [], "entities": [{"text": "SSC development", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.963047206401825}]}, {"text": "These include varying performance, methodology, annotation guidelines and resources of the SSC annotation systems (henceforth annotation systems).", "labels": [], "entities": []}, {"text": "The annotation of SSC in the framework of the CALBC project is focused on (bio) entity mentions (a specific application of the named entity recognition (NER) 2 task).", "labels": [], "entities": [{"text": "SSC", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8894370198249817}, {"text": "named entity recognition (NER) 2 task", "start_pos": 127, "end_pos": 164, "type": "TASK", "confidence": 0.809392087161541}]}, {"text": "However, the idea of SSC creation might also be applied to other types of annotations, e.g. annotation of relations among entities, annotation of treebanks and soon.", "labels": [], "entities": [{"text": "SSC creation", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9810479879379272}]}, {"text": "Hence, if it can be shown that an SSC is a useful resource for the NER task, similar resources can be developed for annotation of information other than entities and utilized for the relevant natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "NER task", "start_pos": 67, "end_pos": 75, "type": "TASK", "confidence": 0.8963814079761505}]}, {"text": "The primary objective of SSC annotation is to compensate the cost, time and manual effort required fora GSC.", "labels": [], "entities": [{"text": "SSC annotation", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.9721197187900543}, {"text": "GSC", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.7239225506782532}]}, {"text": "The procedure of SSC development is inexpensive, fast and yet capable of yielding huge amount of annotated data.", "labels": [], "entities": [{"text": "SSC development", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.9730441570281982}]}, {"text": "These advantages trigger several hypotheses.", "labels": [], "entities": []}, {"text": "For example: \u2022 The size of annotated training corpus always plays a crucial role in the performance of ML systems.", "labels": [], "entities": [{"text": "ML", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9812585711479187}]}, {"text": "If the annotation systems have very high precision and somewhat moderate recall, they would be also able to annotate automatically a huge SSC which would have a good quality of annotations.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.998365581035614}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9994633793830872}]}, {"text": "So, one might assume that, even if such an SSC may contain wrong and missing annotations, a relatively 15 or 20 times bigger SSC than a smaller GSC should allow an ML based system to ameliorate the adverse effects of the erroneous annotations.", "labels": [], "entities": []}, {"text": "\u2022 Rebholz- hypothesized that an SSC might serve as an approximation of a GSC.", "labels": [], "entities": [{"text": "GSC", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.7936607003211975}]}, {"text": "\u2022 In the absence of a GSC, it is expected that ML systems would be able to exploit the harmonised annotations of an SSC to annotate unseen text with reasonable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9885327816009521}]}, {"text": "\u2022 An SSC could be used to semi-automate the annotations of a GSC.", "labels": [], "entities": []}, {"text": "However, in that case, it is expected that the annotation systems would have very high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9990938901901245}]}, {"text": "One can assume that converting an SSC into a GSC would be less time consuming and less costly than developing a GSC from scratch.", "labels": [], "entities": []}, {"text": "All these hypotheses are yet to be verified.", "labels": [], "entities": []}, {"text": "Nevertheless, once we have an SSC annotated with certain type of information, the main question would be how this corpus can be maximally exploited given the fact that it might be created by annotation systems that used different resources and possibly not the same annotation guidelines.", "labels": [], "entities": []}, {"text": "This question is directly related to the practical usability of an SSC, which is the focus of this paper.", "labels": [], "entities": [{"text": "SSC", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9581975936889648}]}, {"text": "Taking the aforementioned hypotheses into account, our goal is to investigate the following research questions which are fundamental to the maximum exploitation of an SSC: 1.", "labels": [], "entities": [{"text": "SSC", "start_pos": 167, "end_pos": 170, "type": "TASK", "confidence": 0.9356055855751038}]}, {"text": "How can the annotation quality of an SSC be improved automatically?", "labels": [], "entities": [{"text": "SSC", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9182183742523193}]}, {"text": "2. How would a system trained on an SSC perform if tested on an unseen benchmark GSC?", "labels": [], "entities": []}, {"text": "3. Can an SSC combined with a GSC produce a better trained system?", "labels": [], "entities": [{"text": "SSC", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.94822758436203}, {"text": "GSC", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.7643250226974487}]}, {"text": "4. What would be the impact on system performance if unannotated sentences 3 are removed from an SSC?", "labels": [], "entities": []}, {"text": "5. What would be the effects of the variation in the size of an SSC on precision and recall?", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9992679953575134}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.998150646686554}]}, {"text": "Our goal is not to judge the procedure of SSC creation, rather our objective is to examine how an SSC can be exploited automatically and maximally fora specific task.", "labels": [], "entities": [{"text": "SSC creation", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9914367496967316}]}, {"text": "Perhaps this would provide useful insights to re-evaluate the approach of SSC creation.", "labels": [], "entities": [{"text": "SSC creation", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.9904837310314178}]}, {"text": "For our experiments, we use a benchmark GSC called the BioCreAtIvE II GM corpus () and the CALBC SSC-I corpus (Rebholz-).", "labels": [], "entities": [{"text": "BioCreAtIvE II GM corpus", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.6740505993366241}, {"text": "CALBC SSC-I corpus", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.8777812321980795}]}, {"text": "Both of these corpora are annotated with genes.", "labels": [], "entities": []}, {"text": "Our motivation behind the choice of a gene annotated GSC for the SSC evaluation is that ML based BNER for genes has already achieved a sufficient level of maturity.", "labels": [], "entities": [{"text": "SSC evaluation", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.8917523622512817}, {"text": "BNER", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.683797299861908}]}, {"text": "This is not the case for other important bio-entity types, primarily due to the absence of training GSC of adequate size.", "labels": [], "entities": []}, {"text": "In fact, for many bio-entity types there exist no GSC.", "labels": [], "entities": []}, {"text": "If we can achieve a reasonably good baseline for gene mention identification by maximizing the exploitation of SSC, we might be able to apply almost similar strategies to exploit SSC for other bioentity types, too.", "labels": [], "entities": [{"text": "gene mention identification", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.8060366113980612}]}, {"text": "The remaining of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 includes brief discussion of the related work.", "labels": [], "entities": []}, {"text": "Apart from mentioning the related literature, this section also underlines the difference of SSC development with respect to approaches such as self-training and co-training.", "labels": [], "entities": [{"text": "SSC development", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.9555598795413971}]}, {"text": "Then in Section 3, we describe the data used in our experiments and the experimental settings.", "labels": [], "entities": []}, {"text": "Following that, in Section 4, empirical results are presented and discussed.", "labels": [], "entities": []}, {"text": "Finally, we conclude with a description of what we learned from this work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the BioCreAtIvE II GM corpus (henceforth, only the GSC) for evaluation of an SSC.", "labels": [], "entities": [{"text": "BioCreAtIvE II GM corpus", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.8107627332210541}]}, {"text": "The training corpus in the GSC has in total 18,265 gene annotations in 15,000 sentences.", "labels": [], "entities": [{"text": "GSC", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7330802083015442}]}, {"text": "The GSC test data has 6,331 annotations in 5,000 sentences.", "labels": [], "entities": [{"text": "GSC test data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.943064272403717}]}, {"text": "Some of the CALBC challenge participants have used the BioCreAtIvE II GM corpus for training to annotate gene/protein in the CALBC SSC-II corpus.", "labels": [], "entities": [{"text": "BioCreAtIvE II GM corpus", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.7631676197052002}, {"text": "CALBC SSC-II corpus", "start_pos": 125, "end_pos": 144, "type": "DATASET", "confidence": 0.6758918960889181}]}, {"text": "We wanted our benchmark corpus and benchmark corpus annotation to be totally unseen by the systems that annotated the SSC to be used in our experiments so that there is no bias in our empirical results.", "labels": [], "entities": []}, {"text": "So, we use the SSC-I (henceforth, we would refer the CALBC SSC-I as simply the SSC) in our experiments despite the fact that it is almost 3 times smaller than the SSC-II.", "labels": [], "entities": [{"text": "CALBC SSC-I", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9051505923271179}]}, {"text": "The SSC has in total 137,610 gene annotations in 316,869 sentences of 50,000 abstracts.", "labels": [], "entities": []}, {"text": "Generally, using a customized dictionary of entity names along with annotated corpus boosts NER performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9603869318962097}]}, {"text": "However, since our objective is to observe to what extent a ML system can learn from SSC, we avoid the use of any dictionary.", "labels": [], "entities": []}, {"text": "We use an open source ML based BNER system named BioEnEx 8 (Chowdhury and Lavelli, 2010).", "labels": [], "entities": []}, {"text": "The system uses conditional random fields (CRFs), and achieves comparable results (F 1 score of 86.22% on the BioCreAtIvE II GM test corpus) to that of the other state-of-the-art systems without using any dictionary or lexicon.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9927822947502136}, {"text": "BioCreAtIvE II GM test corpus", "start_pos": 110, "end_pos": 139, "type": "DATASET", "confidence": 0.8235700130462646}]}, {"text": "One of the complex issues in NER is to come to an agreement regarding the boundaries of entity mentions.", "labels": [], "entities": [{"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9714512825012207}]}, {"text": "Different annotation guidelines have different preferences.", "labels": [], "entities": []}, {"text": "There maybe tasks where a longer entity mention such as \"human IL-7 protein\" maybe appropriate, while for another task a short one such as \"IL-7\" is adequate (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of experiments when trained with  different versions of the SSC and tested on the GSC test  data.", "labels": [], "entities": [{"text": "GSC test  data", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.9698516130447388}]}, {"text": " Table 2: The results of SSC experiments with varying size of the CSSC = condensed SSC (i.e. sentences containing  at least one annotation). SSC size = 316,869 sentences. CSSC size = 77,117.", "labels": [], "entities": [{"text": "SSC", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9636410474777222}]}, {"text": " Table 3: The results of experiments by training on the  GSC training data merged with the PSSC and the CSSC.", "labels": [], "entities": [{"text": "GSC training data", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.8535035252571106}, {"text": "CSSC", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9092874526977539}]}]}