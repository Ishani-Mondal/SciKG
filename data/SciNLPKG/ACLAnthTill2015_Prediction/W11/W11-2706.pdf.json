{"title": [{"text": "Linguistically Motivated Complementizer Choice in Surface Realization", "labels": [], "entities": [{"text": "Surface Realization", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7206915616989136}]}], "abstractContent": [{"text": "This paper shows that using linguistically motivated features for English that-complementizer choice in an averaged perceptron model for classification can improve upon the prediction accuracy of a state-of-the-art realization ranking model.", "labels": [], "entities": []}, {"text": "We report results on a binary classification task for predicting the presence/absence of a that-complementizer using features adapted from Jaeger's (2010) investigation of the uniform information density principle in the context of that-mentioning.", "labels": [], "entities": []}, {"text": "Our experiments confirm the efficacy of the features based on Jaeger's work, including information density-based features.", "labels": [], "entities": []}, {"text": "The experiments also show that the improvements in prediction accuracy apply to cases in which the presence of a that-complementizer arguably makes a substantial difference to fluency or intelli-giblity.", "labels": [], "entities": [{"text": "prediction", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.9247453212738037}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9071252346038818}]}, {"text": "Our ultimate goal is to improve the performance of a ranking model for surface realization, and to this end we conclude with a discussion of how we plan to combine the local complementizer-choice features with those in the global ranking model.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.75058913230896}]}], "introductionContent": [{"text": "Johnson (2009) observes that in developing statistical parsing models, \"shotgun\" features -that is, myriad scattershot features that pay attention to superficial aspects of structure -tend to be remarkably useful, while features based on linguistic theory seem to be of more questionable utility, with the most basic linguistic insights tending to have the greatest impact.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6503982841968536}]}, {"text": "Johnson also notes that feature design is perhaps the most important but least understood aspect of statistical parsing, and thus the disappointing impact of linguistic theory on parsing models is of real consequence.", "labels": [], "entities": [{"text": "feature design", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7408897280693054}, {"text": "statistical parsing", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.771928995847702}]}, {"text": "In this paper, by contrast, we show that in the context of surface realization, using linguistically motivated features for English that-complementizer choice can improve upon the prediction accuracy of a state-ofthe-art realization ranking model, arguably in ways that make a substantial difference to fluency and intelligiblity.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7747995555400848}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9465263485908508}]}, {"text": "In particular, we report results on a binary classification task for predicting the presence or absence of a that-complementizer using features adapted from Jaeger's (2010) investigation of the uniform information density principle in the context of that-mentioning.", "labels": [], "entities": []}, {"text": "This information-theoretic principle predicts that language production is affected by a preference to distribute information uniformly across the linguistic signal.", "labels": [], "entities": []}, {"text": "In Jaeger's study, uniform information density emerges as an important predictor of speakers' syntactic reduction preferences even when taking a sizeable variety of controls based on competing hypotheses into account.", "labels": [], "entities": []}, {"text": "Our experiments confirm the efficacy of the features based on Jaeger's work, including information density-based features.", "labels": [], "entities": []}, {"text": "The term \"shotgun\" feature appears in the slides for Johnson's talk (http://www.cog.brown.edu/ \u02dc mj/ papers/johnson-eacl09-workshop.pdf), rather than in the paper itself.", "labels": [], "entities": []}, {"text": "For German surface realization, show that incorporating information status features based on the linguistics literature improves performance on realization ranking.", "labels": [], "entities": [{"text": "German surface realization", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.5955424308776855}]}, {"text": "That-complementizers are optional words that introduce sentential complements in English.", "labels": [], "entities": []}, {"text": "In the Penn Treebank, they are left out roughly two-thirds of the time, thereby enhancing conciseness.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9933460354804993}]}, {"text": "This follows the low complementizer rates reported in previous work.", "labels": [], "entities": []}, {"text": "While some surface realizers, such as FUF/SURGE, have made use of input features to control the choice of whether to include a that-complementizer, for many applications the decision seems best left to the realizer, since multiple surface syntactic factors appear to govern the choice, rather than semantic ones.", "labels": [], "entities": []}, {"text": "In our experiments, we use the OpenCCG 3 surface realizer with logical form inputs underspecified for the presence of that in complement clauses.", "labels": [], "entities": [{"text": "OpenCCG 3 surface realizer", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.7573179304599762}]}, {"text": "While in many cases, adding or removing that results in an acceptable paraphrase, in the following example, the absence of that in (2) introduces a local ambiguity, which the original Penn Treebank sentence avoids by including the complementizer.", "labels": [], "entities": [{"text": "Penn Treebank sentence", "start_pos": 184, "end_pos": 206, "type": "DATASET", "confidence": 0.9777478774388632}]}, {"text": "(1) He said that for the second month in a row, food processors reported a shortage of nonfat dry milk.", "labels": [], "entities": []}, {"text": "(WSJ0036.61) (2) ? He said for the second month in a row, food processors reported a shortage of nonfat dry milk.", "labels": [], "entities": [{"text": "WSJ0036.61", "start_pos": 1, "end_pos": 11, "type": "METRIC", "confidence": 0.5540301203727722}]}, {"text": "The starting point for this paper is White and Rajkumar's (2009) realization ranking model, a stateof-the-art model employing shotgun features galore.", "labels": [], "entities": []}, {"text": "An error analysis of this model, performed by comparing CCGbank Section 00 realized derivations with their corresponding gold standard derivations, revealed that out of a total of 543 thatcomplementizer cases, the realized output did not match the gold standard choice 82 times (see in Section 5 for details).", "labels": [], "entities": [{"text": "CCGbank Section 00 realized derivations", "start_pos": 56, "end_pos": 95, "type": "DATASET", "confidence": 0.9405398011207581}]}, {"text": "Most of these mismatches involved cases where a clause originally containing a that-complementizer was realized in reduced form, with no that.", "labels": [], "entities": []}, {"text": "This under-prediction of that-inclusion is not surprising, since the realization ranking model makes use of baseline n-gram model features, and n-gram models are known to have a built-in bias for strings with fewer words.", "labels": [], "entities": []}, {"text": "3 openccg.sf.net We report hereon experiments comparing this global model to ones that employ local features specifically designed for that-choice in complement clauses.", "labels": [], "entities": []}, {"text": "As a prelude to incorporating these features into a model for realization ranking, we study the efficacy of these features in isolation by means of a binary classification task to predict the presence/absence of that in complement clauses.", "labels": [], "entities": [{"text": "realization ranking", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.9505421221256256}]}, {"text": "Ina global realization ranking setting, the impact of these phenomenon-specific features might be less evident, as they would interact with other features for lexical selection and ordering choices that the ranker makes.", "labels": [], "entities": []}, {"text": "Note that a comprehensive ranking model is desirable, since linear ordering and thatcomplementizer choices may interact.", "labels": [], "entities": []}, {"text": "For example, reports examples where explicitly marked phrases can occur either close to or far from their heads as in and, whereas zeromarked phrases are only rarely attested at some distance from their heads and prefer adjacency, as and show.", "labels": [], "entities": []}], "datasetContent": [{"text": "To train a local classification model to predict the presence of that in complement clauses, we used an averaged perceptron ranking model with the complementizer-specific features listed in to rank alternate with-that vs. that-less CC choices.", "labels": [], "entities": []}, {"text": "For each CC classification instance in CCGbank Sections 02-21, the derivation of the competing alternate choice was created; i.e., in the case of a that-CC, the corresponding that-less CC was created and vice versa.", "labels": [], "entities": [{"text": "CCGbank Sections 02-21", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.9536699851353964}]}, {"text": "illustrates classification results on Sections 00 (development) using models containing different feature sets & Section 23 (final test) for the best-performing classification and ranking models.", "labels": [], "entities": []}, {"text": "For both the development as well as test sections, the local classification model performed significantly better than the global realization ranking model according to McNemar's \u03c7 2 test (p = 0.005, two-tailed).", "labels": [], "entities": []}, {"text": "Feature ablation tests on the development data (Section 00) revealed that removing the information density features resulted in a loss of accuracy of around 1.8%.", "labels": [], "entities": [{"text": "development data (Section 00)", "start_pos": 30, "end_pos": 59, "type": "DATASET", "confidence": 0.7306520740191141}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9993182420730591}]}], "tableCaptions": [{"text": " Table 1: New features introduced (the prefix of each feature encodes the type of CC; subsequent parts supply the  feature name)", "labels": [], "entities": []}, {"text": " Table 3: Section 00 construction-wise that-CC propor- tions and model accuracies (total CC counts given in  brackets alongside labels); gold standard obviously has  100% accuracy; models are local that-classification and  White and Rajkumar's (2009) global realization ranking  model", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9986371397972107}]}]}