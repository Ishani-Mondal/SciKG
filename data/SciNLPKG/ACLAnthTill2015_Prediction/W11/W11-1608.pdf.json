{"title": [{"text": "Framework for Abstractive Summarization using Text-to-Text Generation", "labels": [], "entities": [{"text": "Abstractive Summarization", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6516517400741577}]}], "abstractContent": [{"text": "We propose anew, ambitious framework for abstractive summarization, which aims at selecting the content of a summary not from sentences , but from an abstract representation of the source documents.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.5228317081928253}]}, {"text": "This abstract representation relies on the concept of Information Items (INIT), which we define as the smallest element of coherent information in a text or a sentence.", "labels": [], "entities": []}, {"text": "Our framework differs from previous abstractive summarization models in requiring a semantic analysis of the text.", "labels": [], "entities": []}, {"text": "We present a first attempt made at developing a system from this framework, along with evaluation results for it from TAC 2010.", "labels": [], "entities": [{"text": "TAC 2010", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9151460528373718}]}, {"text": "We also present related work, both from within and outside of the automatic summarization domain .", "labels": [], "entities": [{"text": "summarization domain", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.8468070924282074}]}], "introductionContent": [{"text": "Summarization approaches can generally be categorized as extractive or abstractive).", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9537373185157776}]}, {"text": "Most systems developped for the main international conference on text summarization, the Text Analysis Conference (TAC), predominantly use sentence extraction, including all the top-ranked systems, which make only minor post-editing of extracted sentences () ()).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6886307746171951}, {"text": "Text Analysis Conference (TAC)", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.7927606105804443}, {"text": "sentence extraction", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7169088423252106}]}, {"text": "Abstractive methods require a deeper analysis of the text and the ability to generate new sentences, which provide an obvious advantage in improving the focus of a summary, reducing its redundancy and keeping a good compression rate.", "labels": [], "entities": []}, {"text": "According to a recent study), there is an empirical limit intrinsic to pure extraction, as compared to abstraction.", "labels": [], "entities": []}, {"text": "For these reasons, as well as for the technical and theoretical challenges involved, we were motivated to come up with an abstractive summarization model.", "labels": [], "entities": []}, {"text": "Recent abstractive approaches, such as sentence compression)) and sentence fusion () or revision ( have focused on rewriting techniques, without consideration fora complete model which would include a transition to an abstract representation for content selection.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7521305084228516}, {"text": "sentence fusion", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7865921556949615}]}, {"text": "We believe that a \"fully abstractive\" approach requires a separate process for the analysis of the text that serves as an intermediate step before the generation of sentences.", "labels": [], "entities": []}, {"text": "This way, content selection can be applied to an abstract representation rather than to original sentences or generated sentences.", "labels": [], "entities": [{"text": "content selection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7016492784023285}]}, {"text": "We propose the concept of Information Items (INIT) to help define the abstract representation.", "labels": [], "entities": []}, {"text": "An INIT is the smallest element of coherent information in a text or a sentence.", "labels": [], "entities": [{"text": "INIT", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9505681395530701}]}, {"text": "It can be something as simple as some entity's property or as complex as a whole description of an event or action.", "labels": [], "entities": []}, {"text": "We believe that such a representation could eventually allow for directly answering queries or guided topic aspects, by generating sentences targeted to address specific information needs.", "labels": [], "entities": []}, {"text": "source documents and generating a summary from them.", "labels": [], "entities": []}, {"text": "Sentence compression first compresses the sentences and chooses from those and the source documents' sentences to form a summary; it may also be completed in the reverse order, which is to select sentences from the source documents and then compress them for the summary.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9250397980213165}]}, {"text": "Sentence fusion first identifies themes (clusters of similar sentences) from the source documents and selects which themes are important for the summary (a process similar to the sentence selection of centroidbased extractive summarization methods () and then generates a representative sentence for each theme by sentence fusion.", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9144193828105927}]}, {"text": "Our proposed abstractive summarization approach is fundamentally different because the selection of content is on Information Items rather than on sentences.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.5089617818593979}]}, {"text": "The text-to-text generation aspect is also changed.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7766367197036743}]}, {"text": "Instead of purely going from whole sentences to generated sentences directly, there is now a text planning phase that occurs at the conceptual level, like in Natural Language Generation (NLG).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 158, "end_pos": 191, "type": "TASK", "confidence": 0.806255449851354}]}, {"text": "This approach has the advantage of generating typically short, information-focused sentences to produce a coherent, information rich, and less redundant summary.", "labels": [], "entities": []}, {"text": "However, the difficulties are great: it is difficult fora machine to properly extract information from sentences at an abstract level, and text generated from noisy data will often be flawed.", "labels": [], "entities": []}, {"text": "Generating sentences that do not all sound similar and generic is an additional challenge that we have for now circumvented by re-using the original sen-tence structure to a large extent, which is a type of text-to-text generation.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 207, "end_pos": 230, "type": "TASK", "confidence": 0.7635431587696075}]}, {"text": "Even considering those difficulties, we believe that efforts in abstractive summarization constitute the future of summarization research, and thus that it is worthwhile to work towards that end.", "labels": [], "entities": [{"text": "summarization", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.9806119799613953}]}, {"text": "In this paper, we present our new abstractive summarization framework in section 2.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.5380745530128479}]}, {"text": "Section 3 describes and analyses our first attempt at using this framework, for the TAC 2010 multi-document news summarization task, followed by the competition's results in section 4.", "labels": [], "entities": [{"text": "TAC 2010 multi-document news summarization task", "start_pos": 84, "end_pos": 131, "type": "TASK", "confidence": 0.7581581473350525}]}, {"text": "In this first attempt, we simplified the framework of section 2 to obtain early results which can help us as we move forward in this project.", "labels": [], "entities": []}, {"text": "Related work is discussed in section 5, and we conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Scores of pyramid, linguistic quality and overall  responsiveness for our Abstractive Summarization (AS)  system, the average of automatic systems (Avg), the best  score of any automatic system (Best), and the average  of the human-written models (Models). The rank is com- puted from amongst the 43 automatic summarization sys- tems that participated in TAC 2010.", "labels": [], "entities": [{"text": "automatic systems (Avg)", "start_pos": 139, "end_pos": 162, "type": "METRIC", "confidence": 0.7596962213516235}, {"text": "TAC 2010", "start_pos": 365, "end_pos": 373, "type": "DATASET", "confidence": 0.8168639838695526}]}]}