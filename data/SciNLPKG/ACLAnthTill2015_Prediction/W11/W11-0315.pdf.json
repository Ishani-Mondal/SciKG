{"title": [{"text": "Language Models as Representations for Weakly-Supervised NLP Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "Finding the right representation for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce.", "labels": [], "entities": []}, {"text": "This paper investigates language model representations, in which language models trained on unlabeled corpora are used to generate real-valued feature vectors for words.", "labels": [], "entities": [{"text": "language model representations", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.6595245202382406}]}, {"text": "We investigate ngram models and probabilistic graphical models, including a novel lattice-structured Markov Random Field.", "labels": [], "entities": []}, {"text": "Experiments indicate that language model representations outperform traditional representations, and that graphical model representations outperform ngram models, especially on sparse and polysemous words.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve strong performance.", "labels": [], "entities": []}, {"text": "Thus, a part-of-speech (POS) tagger would traditionally use a feature like, \"the previous token is the\" to help classify a given token as a noun or adjective.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagger", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.6320857107639313}]}, {"text": "For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results.", "labels": [], "entities": []}, {"text": "However, NLP systems are increasingly being applied to texts like the Web, scientific domains, and personal communications like emails, all of which have very different characteristics from traditional training corpora.", "labels": [], "entities": []}, {"text": "Collecting labeled training data for each new target domain is typically prohibitively expensive.", "labels": [], "entities": []}, {"text": "We investigate representations that can be applied when domain-specific labeled training data is scarce.", "labels": [], "entities": []}, {"text": "An increasing body of theoretical and empirical evidence suggests that traditional, manually-crafted features limit systems' performance in this setting for two reasons.", "labels": [], "entities": []}, {"text": "First, feature sparsity prevents systems from generalizing accurately to words and features not seen during training.", "labels": [], "entities": []}, {"text": "Because word frequencies are Zipf distributed, this often means that there is little relevant training data fora substantial fraction of parameters, especially in new domains).", "labels": [], "entities": []}, {"text": "For example, word-type features form the backbone of most POS-tagging systems, but types like \"gene\" and \"pathway\" show up frequently in biomedical literature, and rarely in newswire text.", "labels": [], "entities": []}, {"text": "Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features \"gene\" and \"pathway\").", "labels": [], "entities": []}, {"text": "Further, because words are polysemous, wordtype features prevent systems from generalizing to situations in which words have different meanings.", "labels": [], "entities": []}, {"text": "For instance, the word type \"signaling\" appears primarily as a present participle (VBG) in Wall Street Journal (WSJ) text, as in, \"Interest rates rose, signaling that . .", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) text", "start_pos": 91, "end_pos": 121, "type": "DATASET", "confidence": 0.8962397064481463}]}, {"text": "\". In biomedical text, however, \"signaling\" appears primarily in the phrase \"signaling pathway,\" where it is considered a noun (NN); this phrase never appears in the WSJ portion of the Penn Treebank (.", "labels": [], "entities": [{"text": "WSJ portion of the Penn Treebank", "start_pos": 166, "end_pos": 198, "type": "DATASET", "confidence": 0.9405019084612528}]}, {"text": "Our response to these problems with traditional NLP representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples.", "labels": [], "entities": []}, {"text": "Our approach depends on the well-known distributional hypothesis, which states that a word's meaning is identified with the contexts in which it appears).", "labels": [], "entities": []}, {"text": "Our goal is to develop probabilistic lan-guage models that describe the contexts of individual words accurately.", "labels": [], "entities": []}, {"text": "We then construct representations, or mappings from word tokens and types to real-valued vectors, from these language models.", "labels": [], "entities": []}, {"text": "Since the language models are designed to model words' contexts, the features they produce can be used to combat problems with polysemy.", "labels": [], "entities": []}, {"text": "And by careful design of the language models, we can limit the number of features that they produce, controlling how sparse those features are in training data.", "labels": [], "entities": []}, {"text": "In this paper, we analyze the performance of language-model-based representations on tasks where domain-specific training data is scarce.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We introduce a novel factorial graphical model representation, a Partial-Lattice Markov Random Field (PL-MRF), which is a tractable variation of a Factorial Hidden Markov Model (HMM) for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.6998023092746735}]}, {"text": "2. In experiments on POS tagging in a domain adaptation setting and on weakly-supervised information extraction (IE), we quantify the performance of representations derived from language models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.937924325466156}, {"text": "information extraction (IE)", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.8621697068214417}]}, {"text": "We show that graphical models outperform ngram representations.", "labels": [], "entities": []}, {"text": "The PL-MRF representation achieves a state-of-the-art 93.8% accuracy on the POS tagging task, while the HMM representation improves over the ngram model by 10% on the IE task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9994552731513977}, {"text": "POS tagging task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7991989850997925}]}, {"text": "3. We analyze how the performance of the different representations varies due to the fundamental challenges of sparsity and polysemy.", "labels": [], "entities": []}, {"text": "The next section discusses previous work.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 present the existing representations we investigate and the new PL-MRF, respectively.", "labels": [], "entities": [{"text": "PL-MRF", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.7280094623565674}]}, {"text": "Sections 5 and 6 describe our two tasks and the results of using our representations on each of them.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the same experimental setup as in HY10: the Penn Treebank () Wall Street Journal portion for our labeled training data; 561 MEDLINE sentences (9576 types, 14554 tokens, 23% OOV tokens) from the Penn BioIE project) for our labeled test set; and all of the unlabeled text from the Penn Treebank WSJ portion plus a MEDLINE corpus of 71,306 unlabeled sentences to train our language models.", "labels": [], "entities": [{"text": "HY10", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9576041102409363}, {"text": "Penn Treebank () Wall Street Journal", "start_pos": 51, "end_pos": 87, "type": "DATASET", "confidence": 0.937150756518046}, {"text": "Penn BioIE project", "start_pos": 201, "end_pos": 219, "type": "DATASET", "confidence": 0.8738911946614584}, {"text": "Penn Treebank WSJ portion", "start_pos": 286, "end_pos": 311, "type": "DATASET", "confidence": 0.979330763220787}]}, {"text": "The two texts come from two very different domains, making this data a tough test for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7731796205043793}]}, {"text": "We use an open source Conditional Random Field (CRF) () software package 4 designed by Sunita Sarawagi and William W. Cohen to implement our supervised models.", "labels": [], "entities": []}, {"text": "Let X be a training corpus, Z the corresponding labels, and Ra representation function.", "labels": [], "entities": []}, {"text": "For each token xi in X, we include a parameter in our CRF model for all features R(x i ) and all possible labels in Z.", "labels": [], "entities": []}, {"text": "Furthermore, we include transition parameters for pairs of consecutive labels z i , z i+1 . For representations, we tested TRAD-R, NGRAM-R, HMM-TOKEN-R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE-TOKEN-R.", "labels": [], "entities": [{"text": "TRAD-R", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9537632465362549}, {"text": "NGRAM-R", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.8256525993347168}, {"text": "LATTICE-TOKEN-R", "start_pos": 197, "end_pos": 212, "type": "METRIC", "confidence": 0.8159018158912659}]}, {"text": "Following HY10, each latent node in the I-HMMs have 80 possible values, creating 80 8 \u2248 10 15 possible configurations of the 8-layer I-HMM fora single word.", "labels": [], "entities": [{"text": "HY10", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.9620202779769897}]}, {"text": "Each node in the PL-MRF is binary, creating a much smaller number (2 20 \u2248 10 6 ) of possible configurations for each word in a 20-layer representation.", "labels": [], "entities": []}, {"text": "NGRAM-R was trained using an unsmoothed trigram model on the Web 1Tgram corpus.", "labels": [], "entities": [{"text": "NGRAM-R", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9190036058425903}, {"text": "Web 1Tgram corpus", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.8334374030431112}]}, {"text": "To keep the feature set manageable, we included the top 500 most common ngrams for each word type, and then used mutual information on the training data to select the top 10,000 most relevant ngram features for all word types.", "labels": [], "entities": []}, {"text": "We incorporated ngram features as binary values indicating whether xi appeared with the ngram or not.", "labels": [], "entities": []}, {"text": "We also report on the performance of Brown clusters and Blitzer et al.'s Structural Correspondence Learning (SCL) (2006) technique, which uses manually-selected \"pivot\" words (like \"of\", \"the\") to learn domain-independent features.", "labels": [], "entities": [{"text": "Structural Correspondence Learning (SCL) (2006)", "start_pos": 73, "end_pos": 120, "type": "TASK", "confidence": 0.7352209919028811}]}, {"text": "Finally, we compare against the self-training CRF technique from HY10.", "labels": [], "entities": [{"text": "HY10", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9830454587936401}]}, {"text": "In this section, we evaluate our learned representations on a different task that investigates the ability of each representation to capture semantic, rather than syntactic, information.", "labels": [], "entities": []}, {"text": "Specifically, we inves-  tigate a set-expansion task in which we're given a corpus and a few \"seed\" noun phrases from a semantic category (e.g. Superheroes), and our goal is to identify other examples of the category in the corpus.", "labels": [], "entities": []}, {"text": "This is a weakly-supervised task because we are given only a handful of examples of the category, rather than a large sample of positively and negatively labeled training examples.", "labels": [], "entities": []}, {"text": "Existing set-expansion techniques utilize the distributional hypothesis: candidate noun phrases fora given semantic class are ranked based on how similar their contextual distributions are to those of the seeds.", "labels": [], "entities": []}, {"text": "Here, we measure how performance on the set-expansion task varies when we employ different representations for the contextual distributions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Graphical models consistently outperform ngram models by a larger margin on sparse words than not-sparse  words. On polysemous words, the difference between graphical model performance and ngram performance grows  for POS tagging, where the context surrounding polysemous words is available to the language model, but not for  information extraction. For tagging, we show number of tokens and accuracies. For IE, we show number of types,  categories, and AUCs.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 228, "end_pos": 239, "type": "TASK", "confidence": 0.7784395217895508}, {"text": "information extraction", "start_pos": 337, "end_pos": 359, "type": "TASK", "confidence": 0.7666744887828827}]}]}