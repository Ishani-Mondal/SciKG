{"title": [{"text": "Comparison of the Baseline Knowledge-, Corpus-, and Web-based Similarity Measures for Semantic Relations Extraction", "labels": [], "entities": [{"text": "Semantic Relations Extraction", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.6552313069502512}]}], "abstractContent": [{"text": "Unsupervised methods of semantic relations extraction rely on a similarity measure between lexical units.", "labels": [], "entities": [{"text": "semantic relations extraction", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.7489871780077616}]}, {"text": "Similarity measures differ both in kinds of information they use and in the ways how this information is transformed into a similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 124, "end_pos": 140, "type": "METRIC", "confidence": 0.9349197745323181}]}, {"text": "This paper is making a step further in the evaluation of the available similarity measures within the context of semantic relation extraction.", "labels": [], "entities": [{"text": "semantic relation extraction", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.7968679269154867}]}, {"text": "We compare 21 baseline measures-8 knowledge-based, 4 corpus-based, and 9 web-based metrics with the BLESS dataset.", "labels": [], "entities": [{"text": "BLESS dataset", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.896463543176651}]}, {"text": "Our results show that existing similarity measures provide significantly different results, both in general performances and in relation distributions.", "labels": [], "entities": []}, {"text": "We conclude that the results suggest developing a combined similarity measure.", "labels": [], "entities": [{"text": "similarity", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9660029411315918}]}], "introductionContent": [{"text": "Semantic relations extraction aims to discover meaningful lexico-semantic relations such as synonyms and hyponyms between a given set of lexically expressed concepts.", "labels": [], "entities": [{"text": "Semantic relations extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8114116191864014}]}, {"text": "Automatic relations discovery is a subtask of automatic thesaurus construction (see, and Panchenko (2010)).", "labels": [], "entities": [{"text": "Automatic relations discovery", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7024502952893575}, {"text": "automatic thesaurus construction", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.7138993938763937}]}, {"text": "A set of semantic relations R between a set of concepts C is a binary relation R \u2286 C \u00d7 T \u00d7 C, where T is a set of semantic relation types.", "labels": [], "entities": []}, {"text": "A relation r \u2208 R is a triple c i , t, c j linking two concepts c i , c j \u2208 C with a semantic relation of type t \u2208 T . We are dealing with six types of semantic relations: hyperonymy, co-hyponymy, meronymy, event (associative), attributes, and random: T = {hyper, coord, mero, event, attri, random}.", "labels": [], "entities": []}, {"text": "We describe analytically and compare experimentally methods, which discover set of semantic relations\u02c6R relations\u02c6 relations\u02c6R fora given set of concepts C.", "labels": [], "entities": []}, {"text": "A semantic relation extraction algorithm aims to discover\u02c6Rdiscover\u02c6 discover\u02c6R \u223c R.", "labels": [], "entities": [{"text": "semantic relation extraction", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.7203445037206014}]}, {"text": "One approach for semantic relations extraction is based on the lexico-syntactic patterns which are constructed either manually or semiautomatically ().", "labels": [], "entities": [{"text": "semantic relations extraction", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.7670803864796957}]}, {"text": "The alternative approach, adopted in this paper, is unsupervised (see e.g. or).", "labels": [], "entities": []}, {"text": "It relies on a similarity measure between lexical units.", "labels": [], "entities": []}, {"text": "We compare 21 baseline measures: 8 knowledge-based, 4 corpus-based, and 9 web-based.", "labels": [], "entities": []}, {"text": "We would like to answer on two questions: \"What metric is most suitable for the unsupervised relation extraction?\", and \"Does various metrics capture the same semantic relations?\".", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7230591624975204}]}, {"text": "The second question is particularly interesting for developing of a meta-measure combining several metrics.", "labels": [], "entities": []}, {"text": "This information may also help us choose a measure well-suited fora concrete application.", "labels": [], "entities": []}, {"text": "We extend existing surveys in three ways.", "labels": [], "entities": []}, {"text": "First, we ground our comparison on the BLESS dataset , which is open, general, and was never used before for comparing all the considered metrics.", "labels": [], "entities": [{"text": "BLESS dataset", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9382942616939545}]}, {"text": "Secondly, we face corpus-, knowledge-, and web-based, which was never done before.", "labels": [], "entities": []}, {"text": "Thirdly, we go further than most of the comparisons and thoroughly compare the metrics with respect to relation types they provide.", "labels": [], "entities": []}, {"text": "We report empirical relation distributions for each measure and check if they are significantly different.", "labels": [], "entities": []}, {"text": "Next, we propose away to find the measures with the most and the least similar relation distributions.", "labels": [], "entities": []}, {"text": "Finally, we report information about redundant measures in an original way -in a form of an undirected graph.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with the knowledge-based measures implemented in the WORDNET::SIMILARITY package ().", "labels": [], "entities": [{"text": "WORDNET::SIMILARITY package", "start_pos": 67, "end_pos": 94, "type": "DATASET", "confidence": 0.7839308142662048}]}, {"text": "Our own implementation is used in the experiments with the corpusbased measures and the web-based measures relying on the YAHOO BOSS search engine API.", "labels": [], "entities": [{"text": "YAHOO BOSS search engine API", "start_pos": 122, "end_pos": 150, "type": "DATASET", "confidence": 0.8012817144393921}]}, {"text": "We use the MEASURES OF SEMANTIC RELATEDNESS web service 4 to assess the other web measures.", "labels": [], "entities": [{"text": "MEASURES OF SEMANTIC RELATEDNESS", "start_pos": 11, "end_pos": 43, "type": "METRIC", "confidence": 0.6557938158512115}]}, {"text": "The evaluation was done with the BLESS set of semantic relations.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9917452931404114}]}, {"text": "It relates 200 target concepts to some 8625 relatum concepts with 26554 semantic relations (14440 are correct and 12154 are random).", "labels": [], "entities": []}, {"text": "Every relation has one of the following six types: hyponymy, co-hyponymy, meronymy, attribute, event, and random.", "labels": [], "entities": []}, {"text": "The distribution of relations among those types is given in table 1.", "labels": [], "entities": []}, {"text": "Each concept is a single English word.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Columns 2-4: Recall and F-measure when Precision= 0.8 (correct relations of all types vs random relations).  Columns 5-9: percent of extracted relations of a certain type with respect to all correctly extracted relations, when  threshold k equal 10% or 40%. The best measure are sorted by F-measure; the best measures are in bold.", "labels": [], "entities": [{"text": "Recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.98943030834198}, {"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9988787770271301}, {"text": "Precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9967553019523621}, {"text": "F-measure", "start_pos": 299, "end_pos": 308, "type": "METRIC", "confidence": 0.972605288028717}]}]}