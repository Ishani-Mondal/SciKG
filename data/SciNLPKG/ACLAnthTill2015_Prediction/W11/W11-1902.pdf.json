{"title": [{"text": "Stanford's Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task", "labels": [], "entities": [{"text": "Sieve Coreference Resolution", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.669079065322876}, {"text": "CoNLL-2011 Shared Task", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.8978523015975952}]}], "abstractContent": [{"text": "This paper details the coreference resolution system submitted by Stanford at the CoNLL-2011 shared task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9561983346939087}, {"text": "CoNLL-2011 shared task", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.7565625309944153}]}, {"text": "Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic , and discourse information.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.8012897968292236}]}, {"text": "All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster.", "labels": [], "entities": []}, {"text": "We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.", "labels": [], "entities": []}, {"text": "Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9345442950725555}, {"text": "CoNLL-2011 shared task", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.8092344403266907}]}, {"text": "Our system extends the multi-pass sieve system of, which applies tiers of deterministic coreference models one at a time from highest to lowest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9721552133560181}]}, {"text": "Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones.", "labels": [], "entities": []}, {"text": "Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster.", "labels": [], "entities": []}, {"text": "We made three considerable extensions to the model.", "labels": [], "entities": []}, {"text": "First, we added five additional sieves, the majority of which address the semantic similarity between mentions, e.g., using WordNet distance, and shallow discourse understanding, e.g., linking speakers to compatible pronouns.", "labels": [], "entities": []}, {"text": "Second, we incorporated a mention detection sieve at the beginning of the processing flow.", "labels": [], "entities": [{"text": "mention detection sieve", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7852165301640829}]}, {"text": "This sieve filters our syntactic constituents unlikely to be mentions using a simple set of rules on top of the syntactic analysis of text.", "labels": [], "entities": []}, {"text": "And lastly, we added a post-processing step, which guarantees that the output of our system is compatible with the shared task and OntoNotes specifications ().", "labels": [], "entities": []}, {"text": "Using this system, we participated in both the closed and open 2 tracks, using both predicted and gold mentions.", "labels": [], "entities": []}, {"text": "Using predicted mentions, our system had an overall score of 57.8 in the closed track and 58.3 in the open track.", "labels": [], "entities": []}, {"text": "These were the top scores in both tracks.", "labels": [], "entities": []}, {"text": "Using gold mentions, our system scored 60.7 in the closed track in in the open track.", "labels": [], "entities": []}, {"text": "We describe the architecture of our entire system in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3 we show the results of several experiments, which compare the impact of the various features in our system, and analyze the performance drop as we switch from gold mentions and annotations (named entity mentions and parse trees) to predicted information.", "labels": [], "entities": []}, {"text": "We also report in this section our official results in the testing partition.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of the mention detection compo- nent, before and after coreference resolution, with both  gold and actual linguistic annotations.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.8733123242855072}]}, {"text": " Table 3: Comparison between various configurations of our system. ER, D, S stand for External Resources, Discourse,  and Semantics sieves. GA and GM stand for Gold Annotations, and Gold Mentions. The top part of the table shows  results using only predicted annotations and mentions, whereas the bottom part shows results of experiments with gold  information. Avg F1 is the arithmetic mean of MUC, B 3 , and CEAFE. We used the development partition for these  experiments.", "labels": [], "entities": [{"text": "GM", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9613272547721863}, {"text": "Avg F1", "start_pos": 362, "end_pos": 368, "type": "METRIC", "confidence": 0.9564694762229919}, {"text": "MUC", "start_pos": 395, "end_pos": 398, "type": "DATASET", "confidence": 0.7943875789642334}, {"text": "CEAFE", "start_pos": 410, "end_pos": 415, "type": "DATASET", "confidence": 0.8851054310798645}]}, {"text": " Table 4: Results on the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.7808914581934611}]}]}