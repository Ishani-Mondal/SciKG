{"title": [{"text": "Evaluating Answers to Reading Comprehension Questions in Context: Results for German and the Role of Information Structure", "labels": [], "entities": [{"text": "Evaluating Answers to Reading Comprehension Questions in Context", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.872538611292839}]}], "abstractContent": [{"text": "Reading comprehension activities are an authentic task including a rich, language-based context, which makes them an interesting real-life challenge for research into automatic content analysis.", "labels": [], "entities": [{"text": "automatic content analysis", "start_pos": 167, "end_pos": 193, "type": "TASK", "confidence": 0.628852109114329}]}, {"text": "For textual entailment research, content assessment of reading comprehension exercises provides an interesting opportunity for extrinsic, real-purpose evaluation, which also supports the integration of context and task information into the analysis.", "labels": [], "entities": []}, {"text": "In this paper, we discuss the first results for content assessment of reading comprehension activities for German and present results which are competitive with the current state of the art for English.", "labels": [], "entities": []}, {"text": "Diving deeper into the results, we provide an analysis in terms of the different question types and the ways in which the information asked for is encoded in the text.", "labels": [], "entities": []}, {"text": "We then turn to analyzing the role of the question and argue that the surface-based account of information that is given in the question should be replaced with a more sophisticated, linguistically informed analysis of the information structuring of the answer in the context of the question that it is a response to.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading comprehension exercises offer a real-life challenge for the automatic analysis of meaning.", "labels": [], "entities": [{"text": "automatic analysis of meaning", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.7594556361436844}]}, {"text": "Given a text and a question, the content assessment task is to determine whether the answer given to a reading comprehension question actually answers the question or not.", "labels": [], "entities": []}, {"text": "Such reading comprehension exercises area common activity in foreign language teaching, making it possible to use activities which are authentic and for which the language teachers provide the gold standard judgements.", "labels": [], "entities": [{"text": "foreign language teaching", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6793508132298788}]}, {"text": "Apart from the availability of authentic exercises and independently motivated gold standard judgements, there are two further reasons for putting reading comprehension tasks into the spotlight for automatic meaning analysis.", "labels": [], "entities": [{"text": "automatic meaning analysis", "start_pos": 198, "end_pos": 224, "type": "TASK", "confidence": 0.6788349350293478}]}, {"text": "Firstly, such activities include a text as an explicit context on the basis of which the questions are asked.", "labels": [], "entities": []}, {"text": "Secondly, answers to reading comprehension questions in foreign language teaching typically are between a couple of words and several sentences in length -too short to rely purely on the distribution of lexical material (as, e.g., in LSA,).", "labels": [], "entities": []}, {"text": "The answers also exhibit a significant variation inform, including a high number of form errors, which makes it necessary to develop an approach which is robust enough to determine meaning correspondences in the presence of errors yet flexible enough to support the rich variation inform which language offers for expressing related meanings.", "labels": [], "entities": []}, {"text": "There is relatively little research on content assessment for reading comprehension tasks and it so far has focused exclusively on English, including both reading comprehension questions answered by native speakers () and by language learners.", "labels": [], "entities": []}, {"text": "The task is related to the increasingly popular strand of research on Recognizing Textual Entailment (RTE,) and the Answer Validation Exercise (AVE,, which both have also generally targeted English.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE", "start_pos": 70, "end_pos": 105, "type": "TASK", "confidence": 0.7010120987892151}, {"text": "Answer Validation Exercise (AVE", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.721664959192276}]}, {"text": "The RTE challenge abstracts away from concrete tasks to emphasize the generic semantic inference component and it has significantly advanced the field under this perspective.", "labels": [], "entities": [{"text": "RTE challenge", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8930688798427582}]}, {"text": "At the same time, an investigation of the role of the context under which an inference holds requires concrete tasks, for which content assessment of reading comprehension tasks seems particularly well-suited.", "labels": [], "entities": []}, {"text": "Borrowing the terminology Sp\u00e4rck Jones (2007) coined in the context of evaluating automatic summarization systems, one can say that we pursue an extrinsic, full-purpose evaluation of aspects of textual inference.", "labels": [], "entities": []}, {"text": "The content assessment task provides two distinct opportunities to investigate textual entailment: On the one hand, one can conceptualize it as a textual inference task of deciding whether a given text T supports a particular student answer H.", "labels": [], "entities": []}, {"text": "On the other hand, if target answers are provided by the teachers, the task can be seen as a special bi-directional case of textual entailment, namely a paraphrase recognition task comparing the student answers to the teacher target answers.", "labels": [], "entities": [{"text": "paraphrase recognition task", "start_pos": 153, "end_pos": 180, "type": "TASK", "confidence": 0.7735398610432943}]}, {"text": "In this paper, we focus on this second approach.", "labels": [], "entities": []}, {"text": "The aim of this paper is twofold.", "labels": [], "entities": []}, {"text": "On the one hand, we want to present the first content assessment approach for reading comprehension activities focusing on German.", "labels": [], "entities": []}, {"text": "In the discussion of the results, we will highlight the impact of the question types and the way in which the information asked for is encoded in the text.", "labels": [], "entities": []}, {"text": "On the other hand, we want to discuss the importance of the explicit language-based context and how an analysis of the question and the way a text encodes the information being asked for can help advance research on automatic content assessment.", "labels": [], "entities": [{"text": "automatic content assessment", "start_pos": 216, "end_pos": 244, "type": "TASK", "confidence": 0.7482360402743021}]}, {"text": "Overall, the paper can be understood as a step in the long-term agenda of exploring the role and impact of the task and the context on the automatic analysis and interpretation of natural language.", "labels": [], "entities": [{"text": "automatic analysis and interpretation of natural language", "start_pos": 139, "end_pos": 196, "type": "TASK", "confidence": 0.7542898058891296}]}], "datasetContent": [{"text": "The overall results include many different question types which pose different kinds of challenges to our system.", "labels": [], "entities": []}, {"text": "To develop an understanding of those challenges, we performed a more fine-grained evaluation by question types.", "labels": [], "entities": []}, {"text": "To distinguish relevant subcases, we applied the question classification scheme introduced by.", "labels": [], "entities": [{"text": "question classification", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.8088197112083435}]}, {"text": "This scheme is more suitable here than other common answer-typing schemata such as the one in, which tend to focus on questions asking for factual knowledge.", "labels": [], "entities": []}, {"text": "distinguish five different question forms: yes/no (question to be answered with either yes or no), alternative (two or more yes/no questions connected with or), true or false (a statement to be classified as true or false), who/what/when/where/how/why (wh-question containing the respective question word), and multiple choice (choice between several answers presented with a question, of any other question type).", "labels": [], "entities": []}, {"text": "Out of the five different forms of question, our data contains questions of all forms except for the multiple choice category and the true or false category given that we are explicitly targeting free text responses.", "labels": [], "entities": []}, {"text": "To obtain a more detailed picture of the wh-question category, we decided to split that category into its respective wh-words and added one more category to it, for which.", "labels": [], "entities": []}, {"text": "Also, we added the type \"several\" for questions which contain more than one question presented to the student at a time.", "labels": [], "entities": []}, {"text": "Of the six comprehension types, our data contained literal, reorganization and inference questions.", "labels": [], "entities": []}, {"text": "reports the accuracy results by question forms and comprehension types for the combined OSU and KU data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9994811415672302}, {"text": "OSU and KU data set", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.8056143879890442}]}, {"text": "The counts encode the number of student answers for which accuracy is reported (micro-averages).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9991121888160706}]}, {"text": "The numbers in brackets specify the number of distinct questions and the corresponding accuracy measures are computed by grouping answers by their question (macro-averages).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9987416863441467}]}, {"text": "Comparing answer-based (micro-average) accuracy with question-based (macro-average) accuracy allows us to see whether the results for questions with a high number of answers outweigh questions with a small number of answers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9706921577453613}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9426023960113525}]}, {"text": "In general the micro-and macroaverages reported are very similar and the overall accuracy is the same (84.6%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9997753500938416}]}, {"text": "Overall, the results thus do not seem to be biased towards a specific, frequently answered question instance.", "labels": [], "entities": []}, {"text": "Where larger differences between micro-and macro-averages do arise, as for alternative, when, and where questions, these are cases with few overall instances in the data set, cautioning us against overinterpreting results for such small subsets.", "labels": [], "entities": []}, {"text": "The 4.2% gap for the relatively frequent \"several\" question type underlines the heterogeneous nature of this class, which may warrant more specific subclasses in the future.", "labels": [], "entities": []}, {"text": "Overall, the accuracy of content assessment for wh-questions that can be answered with a concrete piece of information from the text are highest, with 92.6% for \"which\" questions, and results in the upper 80s for five other wh-questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9995476603507996}]}, {"text": "Interestingly, \"who\" questions fare comparatively badly, pointing to a relatively high variability in the expression of subjects, which would warrant the integration of a dedicated approach to coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 193, "end_pos": 215, "type": "TASK", "confidence": 0.9636094570159912}]}, {"text": "Such a direct solution is not available for \"why\" questions, which at 79.3% is the worst wh-question type.", "labels": [], "entities": []}, {"text": "The high variability of those answers is rooted in the fact that they ask fora cause or reason, which can be expressed in a multitude of ways, especially for comprehension types involving inferences or reorganization of the information given in the text.", "labels": [], "entities": []}, {"text": "This drop between comprehension types, from literal (86.0%) to inference (81.5%) and reorganization (78.0%), can also be observed throughout and is expected given that the CoMiC-DE system makes use of surface-based alignments where it can find them.", "labels": [], "entities": []}, {"text": "For the system to improve on the non-literal comprehension types, features encoding a richer set of abstractions (e.g., to capture distributional similarity at the chunk level or global linguistic phenomena such as negation) need to be introduced.", "labels": [], "entities": []}, {"text": "Just as in the discussion of the micro-and macroaverages above, the \"several\" question type again rears its ugly heads in terms of a low overall accuracy (77.7%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9972537159919739}]}, {"text": "This supports the conclusion that it requires a dedicated approach.", "labels": [], "entities": []}, {"text": "Based on an analysis of the nature and sequence of the component questions, in future work we plan to determine how such combinations constrain the space of variation in acceptable answers.", "labels": [], "entities": []}, {"text": "Finally, while there are few instances for the \"alternative\" question type, the fact that it resulted in the lowest accuracy (57.1%) warrants some attention.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9988864064216614}]}, {"text": "The analysis indeed revealed a general issue, which is discussed in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 lists how many questions,  target answers and student answers each of the two  data sets contains. The data used for this paper is  made freely available upon request under a standard  Creative Commons by-nc-sa licence. 1", "labels": [], "entities": []}, {"text": " Table 4. We report accuracy and the total  number of answers for each data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9996484518051147}]}, {"text": " Table 4: Classification accuracy for the two data sets", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9539289474487305}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9773704409599304}]}, {"text": " Table 5: Accuracy by question form and comprehension types following", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9952580332756042}]}]}