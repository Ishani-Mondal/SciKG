{"title": [{"text": "Noisy SMS Machine Translation in Low-Density Languages", "labels": [], "entities": [{"text": "Noisy SMS Machine Translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7626603841781616}]}], "abstractContent": [{"text": "This paper presents the system we developed for the 2011 WMT Haitian Creole-English SMS featured translation task.", "labels": [], "entities": [{"text": "WMT Haitian Creole-English SMS featured translation task", "start_pos": 57, "end_pos": 113, "type": "TASK", "confidence": 0.7224757926804679}]}, {"text": "Applying standard statistical machine translation methods to noisy real-world SMS data in a low-density language setting such as Haitian Creole poses a unique set of challenges, which we attempt to address in this work.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6475132604440054}]}, {"text": "Along with techniques to better exploit the limited available training data, we explore the benefits of several methods for alleviating the additional noise inherent in the SMS and transforming it to better suite the assumptions of our hierarchical phrase-based model system.", "labels": [], "entities": []}, {"text": "We show that these methods lead to significant improvements in BLEU score over the baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9700051546096802}]}], "introductionContent": [{"text": "For the featured translation task of the Sixth Workshop on Statistical Machine Translation, we developed a system for translating Haitian Creole Emergency SMS messages.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.7258468667666117}, {"text": "translating Haitian Creole Emergency SMS messages", "start_pos": 118, "end_pos": 167, "type": "TASK", "confidence": 0.8687271078427633}]}, {"text": "Given the nature of the task, translating text messages that were sent during the January 2010 earthquake in Haiti to an emergency response service called Mission 4636, we were not only faced with the problem of dealing with a lowdensity language, but additionally, with noisy, realworld data in a domain which has thus far received relatively little attention in statistical machine translation.", "labels": [], "entities": [{"text": "translating text messages that were sent during the January 2010 earthquake", "start_pos": 30, "end_pos": 105, "type": "TASK", "confidence": 0.7870597405867144}, {"text": "statistical machine translation", "start_pos": 364, "end_pos": 395, "type": "TASK", "confidence": 0.6801669100920359}]}, {"text": "We were especially interested in this task because of the unique set of challenges that it poses for existing translation systems.", "labels": [], "entities": []}, {"text": "We focused our research effort on techniques to better utilize the limited available training resources, as well as ways in which we could automatically alleviate and transform the noisy data to our advantage through the use of automatic punctuation prediction, finite-state raw-to-clean transduction, and grammar extraction.", "labels": [], "entities": [{"text": "automatic punctuation prediction", "start_pos": 228, "end_pos": 260, "type": "TASK", "confidence": 0.6699448823928833}, {"text": "grammar extraction", "start_pos": 306, "end_pos": 324, "type": "TASK", "confidence": 0.8059830665588379}]}, {"text": "All these techniques contributed to improving translation quality as measured by BLEU score over our baseline system.", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9671684503555298}, {"text": "BLEU score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.97547847032547}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "First, we provide a brief overview of our baseline system in Section 2, followed by an examination of issues posed by this task and the steps we have taken to address them in Section 3, and finally we conclude with experimental results and additional analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results produced by the baseline systems are presented in.", "labels": [], "entities": []}, {"text": "As can be seen, the clean version performs on par with the French-English trans- lation quality in the 2011 WMT shared translation task, 2 and significantly outperforms the raw version, despite the content of the messages being identical.", "labels": [], "entities": [{"text": "WMT shared translation task", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.7095794826745987}]}, {"text": "This serves to underscore the importance of proper post-processing of the raw data in order to attempt to close the performance gap between the two versions.", "labels": [], "entities": []}, {"text": "Through analysis of the raw and clean data we identified several factors which we believe greatly contribute to the difference in translation output.", "labels": [], "entities": []}, {"text": "We examine punctuation in Section 3.2, grammar postprocessing in Section 3.3, and morphological differences in Sections 3.4 and 3.5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline system BLEU and TER scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9982621073722839}, {"text": "TER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9963209629058838}]}, {"text": " Table 2: Confidence weighting versus x10 duplication", "labels": [], "entities": [{"text": "Confidence weighting", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8977343440055847}]}, {"text": " Table 3: Automatic punctuation prediction results", "labels": [], "entities": [{"text": "Automatic punctuation prediction", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6481657524903616}]}, {"text": " Table 4: Results of filtering the grammar in a post- processing step before decoding", "labels": [], "entities": []}, {"text": " Table 5: Raw-Clean segmentation lattice tuning results", "labels": [], "entities": [{"text": "Raw-Clean segmentation lattice tuning", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6759935542941093}]}, {"text": " Table 6: Raw-to-clean transformation lattice results", "labels": [], "entities": []}, {"text": " Table 7: Comparison of all systems' performance on  devtest set", "labels": [], "entities": []}]}