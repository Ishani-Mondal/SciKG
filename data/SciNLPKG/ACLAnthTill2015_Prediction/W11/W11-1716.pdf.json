{"title": [{"text": "Automatic Expansion of Feature-Level Opinion Lexicons", "labels": [], "entities": []}], "abstractContent": [{"text": "In most tasks related to opinion mining and sentiment analysis, it is necessary to compute the semantic orientation (i.e., positive or negative evaluative implications) of certain opinion expressions.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.8608600497245789}, {"text": "sentiment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9428868293762207}]}, {"text": "Recent works suggest that semantic orientation depends on application domains.", "labels": [], "entities": [{"text": "semantic orientation", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8813144564628601}]}, {"text": "Moreover, we think that semantic orientation depends on the specific targets (fea-tures) that an opinion is applied to.", "labels": [], "entities": [{"text": "semantic orientation", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.871864527463913}]}, {"text": "In this paper , we introduce a technique to build domain-specific, feature-level opinion lexicons in a semi-supervised manner: we first induce a lexicon starting from a small set of annotated documents; then, we expand it automatically from a larger set of unannotated documents, using anew graph-based ranking algorithm.", "labels": [], "entities": []}, {"text": "Our method was evaluated in three different domains (headphones, hotels and cars), using a corpus of product reviews which opinions were annotated at the feature level.", "labels": [], "entities": []}, {"text": "We conclude that our method produces feature-level opinion lexicons with better accuracy and recall that domain-independent opinion lexicons using only a few annotated documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9988617897033691}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9703623652458191}]}], "introductionContent": [{"text": "Sentiment analysis is a modern subdiscipline of natural language processing which deals with subjectivity, affects and opinions in texts (a good survey on this subject can be found in ().", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9070744812488556}]}, {"text": "This discipline is also known as opinion mining, mainly in the context of text mining and information extraction.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8731539845466614}, {"text": "text mining", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.8354865908622742}, {"text": "information extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.857648640871048}]}, {"text": "Many classification and extraction problems have been defined, with different levels of granularity depending on applications requirements: e.g. classification of text documents or smaller pieces of text into objective and subjective, classification of opinionated documents or individual sentences regarding the overall opinion (into \"positive\" and \"negative\" classes, or into a multi-point scale) or extraction of individual opinions from apiece of text (may include opinion target, holder, polarity or intensity of the opinions, among others).", "labels": [], "entities": [{"text": "classification and extraction", "start_pos": 5, "end_pos": 34, "type": "TASK", "confidence": 0.7999924222628275}]}, {"text": "As a key in solving most of these problems, the semantic orientation of some opinion expressions should be computed: a numeric value, usually between \u22121 and 1, referring to the negative or positive affective implications of a given word or prhase.", "labels": [], "entities": []}, {"text": "These values can be collected in an opinion lexicon, so this resource can be accessed when needed.", "labels": [], "entities": []}, {"text": "Many recent works () suggest the need for domainspecific opinion lexicons, containing semantic orientations of opinion expressions when used in a particular domain (e.g., the word \"predictable\" has opposite semantic orientations when used to define the driving experience of a car or the plot of a movie).", "labels": [], "entities": []}, {"text": "Moreover, within a given domain, the specific target of the opinion is also important to induce the polarity and the intensity of the affective implications of some opinion expressions ( consider for example the word \"cheap\" when referring to the price or to the appearance of an electronic device).", "labels": [], "entities": []}, {"text": "This is especially important to extract opinions from product reviews, where users write their opinions about individual features of a product.", "labels": [], "entities": []}, {"text": "These domain-specific, feature-level opinion lexicons can be manually collected, but it implies a considerable amount of time 125 and effort, especially if a large number of different domains are considered.", "labels": [], "entities": []}, {"text": "In this work, we propose a method to automatically induce feature-level, domain-specific opinion lexicons from an annotated corpus.", "labels": [], "entities": []}, {"text": "As we are committed to reduce the time and effort, we research about the automatic expansion of this kind of lexicons, so we keep the number of required annotated documents as low as possible.", "labels": [], "entities": []}, {"text": "In order to do so, we propose a graph-based algorithm which can be applied to other knowledge propagation problems.", "labels": [], "entities": [{"text": "knowledge propagation", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7444719076156616}]}, {"text": "In the next section, we review some related previous works to contextualize our approach.", "labels": [], "entities": []}, {"text": "In section 3, we define the feature-level opinion lexicons and describe our method to induce and expand them in a semi-supervised manner.", "labels": [], "entities": []}, {"text": "In section 4, we carryout some experiments over a dataset of reviews of three diferent domains.", "labels": [], "entities": []}, {"text": "Finally, we discuss the results and draw some conclusions in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report the results of some experiments aimed to evaluate the quality of the featurelevel opinion lexicons obtained by our method.", "labels": [], "entities": []}, {"text": "All the experiments were done using 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "Each annotated dataset was randomly partitioned into ten subsets.", "labels": [], "entities": []}, {"text": "The results reported for each experiment are the average results obtained in ten different runs, taking a different subset as testing set and the remaining nine subsets as training set (to induce seed lexicons).", "labels": [], "entities": []}, {"text": "To evaluate the lexicons, we compute recall and precision over the terms participating as opinion words in the opinions annotated in the testing set.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9993908405303955}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9987059831619263}]}, {"text": "Recall is the proportion of terms which are contained in the lexicon; precision is the proportion of terms with a correct sentiment orientation in the lexicon.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9896062016487122}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9993979930877686}]}, {"text": "shows the results of the evaluation of the induced and expanded lexicons.", "labels": [], "entities": []}, {"text": "In order to figure out the gain in precision and recall obtained by our expansion method, we induced lexicons for each domain using different numbers of annotated reviews: Results of expansion of lexicons induced from different numbers of annotated reviews.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.999225378036499}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9989674091339111}]}, {"text": "The second and third experiments for each domain are done selecting the number of annotated reviews needed to achieve F 1 scores for the induced lexicon similar to the F 1 scores for the expanded lexicon from the previous experiment. and expanding them using the whole set of unannotated reviews.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9858941038449606}, {"text": "F 1 scores", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9686040878295898}]}, {"text": "For each domain, we show the results of experiments using only nine annotated reviews (one from each subset of reviews of the crossvalidation process), and using all the available annotated reviews.", "labels": [], "entities": []}, {"text": "The second and third experiments for each domain are those where F 1 scores for the induced lexicon is similar to the F 1 scores for the expanded lexicon from the previous experiment.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.981092651685079}, {"text": "F 1 scores", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9750238060951233}]}, {"text": "Thus, we can measure the number of additional annotated reviews needed to obtain similar results without expansion.", "labels": [], "entities": []}, {"text": "Using only nine annotated reviews, the expanded feature-level opinion lexicon achieves 0.8158 of F 1 for the headphones domain, 0.8764 for the hotels domain and 0.8853 for the cars domain, afar better result that using a domain-independent opinion lexicon . To obtain similar F 1 scores without using the expansion method, you should annotate between six and thirteen times more reviews.", "labels": [], "entities": [{"text": "F 1", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9926510155200958}, {"text": "F 1", "start_pos": 276, "end_pos": 279, "type": "METRIC", "confidence": 0.9544396996498108}]}], "tableCaptions": [{"text": " Table 1: Information of the dataset. The number of un- nanotated reviews available for each domain is shown in  parenthesis.", "labels": [], "entities": []}, {"text": " Table 2: Results of expansion of lexicons induced from different numbers of annotated reviews. The second and third  experiments for each domain are done selecting the number of annotated reviews needed to achieve F 1 scores for the  induced lexicon similar to the F 1 scores for the expanded lexicon from the previous experiment.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 215, "end_pos": 225, "type": "METRIC", "confidence": 0.9855181574821472}, {"text": "F 1 scores", "start_pos": 266, "end_pos": 276, "type": "METRIC", "confidence": 0.9731551806131998}]}]}