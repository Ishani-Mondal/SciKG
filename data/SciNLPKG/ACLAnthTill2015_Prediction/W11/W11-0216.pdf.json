{"title": [{"text": "A Study on Dependency Tree Kernels for Automatic Extraction of Protein-Protein Interaction", "labels": [], "entities": [{"text": "Automatic Extraction of Protein-Protein Interaction", "start_pos": 39, "end_pos": 90, "type": "TASK", "confidence": 0.6896061778068543}]}], "abstractContent": [{"text": "Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks as they provide higher accuracy than other approaches.", "labels": [], "entities": [{"text": "relation extraction (RE) tasks", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.870516354839007}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.994899570941925}]}, {"text": "In this paper, we introduce new dependency tree (DT) kernels for RE by improving on previously proposed dependency tree structures.", "labels": [], "entities": [{"text": "RE", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9239825010299683}]}, {"text": "These are further enhanced to design more effective approaches that we call mildly extended dependency tree (MEDT) kernels.", "labels": [], "entities": []}, {"text": "The empirical results on the protein-protein interaction (PPI) extraction task on the AIMed corpus show that tree kernels based on our proposed DT structures achieve higher accuracy than previously proposed DT and phrase structure tree (PST) kernels.", "labels": [], "entities": [{"text": "protein-protein interaction (PPI) extraction", "start_pos": 29, "end_pos": 73, "type": "TASK", "confidence": 0.7245925962924957}, {"text": "AIMed corpus", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.8893011808395386}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9970001578330994}]}], "introductionContent": [{"text": "Relation extraction (RE) aims at identifying instances of pre-defined relation types in text as for example the extraction of protein-protein interaction (PPI) from the following sentence: \"Native C8 also formed a heterodimer with C5, and low concentrations of polyionic ligands such as protamine and suramin inhibited the interaction.\"", "labels": [], "entities": [{"text": "Relation extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8900834083557129}, {"text": "extraction of protein-protein interaction (PPI)", "start_pos": 112, "end_pos": 159, "type": "TASK", "confidence": 0.7828436919621059}]}, {"text": "After identification of the relevant named entities (NE, in this case proteins) C8 and C5, the RE task determines whether there is a PPI relationship between the entities above (which is true in the example).", "labels": [], "entities": [{"text": "RE task", "start_pos": 95, "end_pos": 102, "type": "TASK", "confidence": 0.7680257558822632}]}, {"text": "Kernel based approaches for RE have drawn a lot of interest in recent years since they can exploit a huge amount of features without an explicit representation.", "labels": [], "entities": [{"text": "RE", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9917253255844116}]}, {"text": "Some of these approaches are structure kernels (e.g. tree kernels), which carryout structural similarities between instances of relations, represented as phrase structures or dependency trees, in terms of common substructures.", "labels": [], "entities": []}, {"text": "Other kernels simply use techniques such as bag-of-words, subsequences, etc.", "labels": [], "entities": []}, {"text": "to map the syntactic and contextual information to flat features, and later compute similarity.", "labels": [], "entities": []}, {"text": "One variation of tree kernels is the dependency tree (DT) kernel (.", "labels": [], "entities": []}, {"text": "A DT kernel (DTK) is a tree kernel that is computed on a dependency tree (or subtree).", "labels": [], "entities": []}, {"text": "A dependency tree encodes grammatical relations between words in a sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges.", "labels": [], "entities": []}, {"text": "The main advantage of a DT in comparison with phrase structure tree (PST) is that the former allows for relating two words directly (and in more compact substructures than PST) even if they are far apart in the corresponding sentence according to their lexical word order.", "labels": [], "entities": [{"text": "phrase structure tree (PST)", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7655800382296244}]}, {"text": "Several kernel approaches exploit syntactic dependencies among words for PPI extraction from biomedical text in the form of dependency graphs or dependency paths (e.g. or ).", "labels": [], "entities": [{"text": "PPI extraction from biomedical text", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.9095757007598877}]}, {"text": "However, to the best of our knowledge, there are only few works on the use of DT kernels for this task.", "labels": [], "entities": []}, {"text": "Therefore, exploring the potential of DTKs applied to different structures is a worthwhile research direction.", "labels": [], "entities": []}, {"text": "A DTK, pioneered by, is typically applied to the minimal or smallest common subtree that includes a target pair of entities.", "labels": [], "entities": []}, {"text": "Such subtree reduces unnecessary information by placing word(s) closer to its dependent(s) inside the tree and emphasizes local features of relations.", "labels": [], "entities": []}, {"text": "Nevertheless, there are cases where a minimal subtree might not contain important cue words or predicates.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence where a PPI relation holds between BMP-2 and BMPR-IA, but the minimal subtree does not contain the cue word \"binding\" as shown in: The binding epitopes of BMP-2 for BMPR-IA was characterized using BMP-2 mutant proteins.", "labels": [], "entities": []}, {"text": "In this paper we investigate two assumptions.", "labels": [], "entities": []}, {"text": "The first is that a DTK based on a mild extension of minimal subtrees would produce better results than the DTK on minimal subtrees.", "labels": [], "entities": []}, {"text": "The second is that previously proposed DT structures can be further improved by introducing simplified representation of the entities as well as augmenting nodes in the DT tree structure with relevant features.", "labels": [], "entities": []}, {"text": "This paper presents an evaluation of the above assumptions.", "labels": [], "entities": []}, {"text": "More specifically, the contributions of this paper are the following: \u2022 We propose the use of new DT structures, which are improvement on the structures defined in with the most general (in terms of substructures) DTK, i.e. Partial Tree Kernel (PTK)).", "labels": [], "entities": []}, {"text": "\u2022 We firstly propose the use of the Unlexicalized PTK () with our dependency structures, which significantly improves PTK.", "labels": [], "entities": [{"text": "PTK", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.38759082555770874}]}, {"text": "\u2022 We compare the performance of the proposed DTKs on PPI with the one of PST kernels and show that, on biomedical text, DT kernels perform better.", "labels": [], "entities": []}, {"text": "\u2022 Finally, we introduce a novel approach (called mildly extended dependency tree (MEDT) kernel 1 , which achieves the best performance among various (both DT and PST) tree kernels.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce tree kernels and relation extraction and we also review previous work.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8678722679615021}]}, {"text": "Section 3 describes the unlexicalized PTK (uPTK).", "labels": [], "entities": []}, {"text": "Then, in Section 4, we define our proposed DT structures including MEDT.", "labels": [], "entities": [{"text": "MEDT", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.7159507274627686}]}, {"text": "Section 5 describes the experimental results on the AIMed corpus ( ) and discusses their outcomes.", "labels": [], "entities": [{"text": "AIMed corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.7779059112071991}]}, {"text": "Finally, we conclude with a summary of our study as well as plans for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out several experiments with different dependency structures and tree kernels.", "labels": [], "entities": []}, {"text": "Most importantly, we tested tree kernels on PST and our improved representations for DT.", "labels": [], "entities": [{"text": "PST", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.7762594819068909}]}, {"text": "We used the AIMed corpus ( ) converted using the software provided by . AIMed is the largest benchmark corpus (in terms of number of sentences) for the PPI task.", "labels": [], "entities": [{"text": "AIMed corpus", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.8097107708454132}]}, {"text": "It contains 1,955 sentences, in which are annotated 1,000 positive PPI and 4,834 negative pairs.", "labels": [], "entities": []}, {"text": "We use the Stanford parser 6 for parsing the data.", "labels": [], "entities": [{"text": "Stanford parser 6", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.8920692602793375}, {"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9806289672851562}]}, {"text": "7 The SPECIALIST lexicon tool 8 is used to normalize words to avoid spelling variations and also to provide lemmas.", "labels": [], "entities": []}, {"text": "For training and evaluating tree kernels, we use the SVM-LIGHT-TK toolkit 9).", "labels": [], "entities": [{"text": "SVM-LIGHT-TK toolkit 9", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.8460038105646769}]}, {"text": "We tuned the parameters \u00b5, \u03bb and c following the approach described by, and used biased hyperplane.", "labels": [], "entities": []}, {"text": "All the other parameters are left as their default values.", "labels": [], "entities": []}, {"text": "Our experiments are evaluated with 10-fold cross validation using the same split of the AIMed corpus used by .", "labels": [], "entities": [{"text": "AIMed corpus", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.8958390951156616}]}], "tableCaptions": []}