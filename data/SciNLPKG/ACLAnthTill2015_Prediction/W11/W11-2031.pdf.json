{"title": [{"text": "An Approach to the Automated Evaluation of Pipeline Architectures in Natural Language Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to performing automated evaluations of pipeline architectures in natural language dialogue systems.", "labels": [], "entities": []}, {"text": "Our approach addresses some of the difficulties that arise in such automated evaluations, including the lack of consensus among human an-notators about the correct outputs within the processing pipeline, the availability of multiple acceptable system responses to some user utterances, and the complex relationship between system responses and internal processing results.", "labels": [], "entities": []}, {"text": "Our approach includes the development of a corpus of richly annotated target dialogues, simulations of the pipeline processing that could occur in these dialogues, and an analysis of how system responses vary based on internal processing results within the pipeline.", "labels": [], "entities": []}, {"text": "We illustrate our approach in two implemented virtual human dialogue systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language dialogue systems are typically implemented as complex modular systems, with a range of internal modules performing tasks such as automatic speech recognition (ASR), natural language understanding (NLU), dialogue management (DM), natural language generation (NLG), and speech synthesis (TTS).", "labels": [], "entities": [{"text": "Natural language dialogue", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6116456488768259}, {"text": "automatic speech recognition (ASR)", "start_pos": 146, "end_pos": 180, "type": "TASK", "confidence": 0.8432232042153677}, {"text": "natural language understanding (NLU)", "start_pos": 182, "end_pos": 218, "type": "TASK", "confidence": 0.8006246586640676}, {"text": "dialogue management (DM)", "start_pos": 220, "end_pos": 244, "type": "TASK", "confidence": 0.8392378032207489}, {"text": "natural language generation (NLG)", "start_pos": 246, "end_pos": 279, "type": "TASK", "confidence": 0.7735530734062195}, {"text": "speech synthesis (TTS)", "start_pos": 285, "end_pos": 307, "type": "TASK", "confidence": 0.8008223652839661}]}, {"text": "A common design is for systems to adopt a pipeline architecture.", "labels": [], "entities": []}, {"text": "Ina pipeline, each user utterance is processed in a series of successive processing steps, with the output of each module serving as the input of the next module, until the system's response is determined.", "labels": [], "entities": []}, {"text": "* Now at Saarland University, Germany.", "labels": [], "entities": []}, {"text": "While there are many approaches to dialogue system evaluation (see e.g. (), in many ways, the primary data for assessing the performance of a dialogue system comes from the collection of live interactive dialogues between an implemented system and members of its intended user population.", "labels": [], "entities": [{"text": "dialogue system evaluation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7025003234545389}]}, {"text": "Yet, live dialogue-based evaluation suffers from a number of limitations and drawbacks.", "labels": [], "entities": []}, {"text": "Each dialogue set can be expensive and time-consuming to collect, and may only reflect a specific version of a system under active development.", "labels": [], "entities": []}, {"text": "Additional effort is also generally necessary to identify specific system responses as problematic or unacceptable.", "labels": [], "entities": []}, {"text": "Further annotation and analysis is then necessary to diagnose and pinpoint the cause of the problematic responses, so that the relevant pipeline module(s) maybe improved.", "labels": [], "entities": []}, {"text": "In this paper, we present and discuss an approach to performing automated evaluations of pipeline architectures.", "labels": [], "entities": []}, {"text": "Our approach involves the development of a corpus of annotated target dialogues, starting from Wizard-of-Oz data.", "labels": [], "entities": []}, {"text": "Our automated evaluation assesses the support for these target dialogues in a pipeline system architecture.", "labels": [], "entities": []}, {"text": "It is not designed as a substitute for live system evaluations, but rather as a complement to them which may help to alleviate some of these challenges to understanding system performance and streamlining development.", "labels": [], "entities": []}, {"text": "In particular, unlike the PARADISE framework (, which aims to evaluate dialogue agent strategies -by relating overall user satisfaction to various other metrics (task success, efficiency measures, and qualitative measures) -our approach takes the agent's dialogue strategy for granted (in the form of a set of target dialogues that exemplify the desired strategy), and instead zooms in and aims to directly evaluate the dialogue system's module pipeline.", "labels": [], "entities": []}, {"text": "Specifically, our approach quantifies the ability of the pipeline to replicate the processing steps needed to reproduce a set of target responses.", "labels": [], "entities": []}, {"text": "In our analysis, we place a special emphasis on the possible lack of consensus among human annotators about what the processing results should be.", "labels": [], "entities": []}, {"text": "We do not aim to further analyze the system's live dialogue behavior in terms of user satisfaction, task success, or other global measures.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: TACQ Amani Evaluation Results", "labels": [], "entities": [{"text": "TACQ", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7878201603889465}, {"text": "Amani Evaluation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.5023870021104813}]}, {"text": " Table 3: SimCoach Evaluation Results", "labels": [], "entities": [{"text": "SimCoach Evaluation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7809811532497406}]}]}