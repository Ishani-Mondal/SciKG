{"title": [{"text": "Fine-grained Entity Set Refinement with User Feedback", "labels": [], "entities": []}], "abstractContent": [{"text": "State of the art semi-supervised entity set expansion algorithms produce noisy results, which need to be refined manually.", "labels": [], "entities": [{"text": "entity set expansion", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7094300190607706}]}, {"text": "Sets expanded for intended fine-grained concepts are especially noisy because these concepts are not well represented by the limited number of seeds.", "labels": [], "entities": []}, {"text": "Such sets are usually incorrectly expanded to contain elements of a more general concept.", "labels": [], "entities": []}, {"text": "We show that fine-grained control is necessary for refining such sets and propose an algorithm which uses both positive and negative user feedback for iterative refinement.", "labels": [], "entities": []}, {"text": "Experimental results show that it improves the quality of fine-grained sets significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity set expansion is a well-studied problem with several techniques proposed).", "labels": [], "entities": [{"text": "Entity set expansion", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6844386657079061}]}, {"text": "In practice, semisupervised methods are preferred since they require only a handful of seeds and are more flexible for growing various types of entity sets.", "labels": [], "entities": []}, {"text": "However, they usually produce noisy sets, which need to be refined ( . Finegrained sets such as National Capitals are particularly noisy.", "labels": [], "entities": [{"text": "National Capitals", "start_pos": 96, "end_pos": 113, "type": "DATASET", "confidence": 0.9453600943088531}]}, {"text": "Such concepts are intrinsically hard because they're not well represented by initial seeds.", "labels": [], "entities": []}, {"text": "Moreover, most related instances have a limited number of features, thus making it hard to retrieve them.", "labels": [], "entities": []}, {"text": "We examined a few sets expanded for finegrained concepts and observed that lots of erroneous expansions are elements of a more general concept, whose sense overlaps and subsumes the intended sense.", "labels": [], "entities": []}, {"text": "For example, the concept National Capitals is expanded to contain Major cities.", "labels": [], "entities": []}, {"text": "In such cases, a proposed feature-pruning technique using user-tagged expansion errors to refine sets (Vyas and Pantel 2009) removes some informative features of the target concept.", "labels": [], "entities": []}, {"text": "Moreover, since refining such sets needs more information about the target concept, it is natural to use user-tagged correct expansions as well for the refinement.", "labels": [], "entities": []}, {"text": "In this paper, we refer to the problem of finegrained concepts being erroneously extended as semantic spread.", "labels": [], "entities": []}, {"text": "We show that a rich feature representation of the target concept, coupled with appropriate weighting of features, is necessary for reducing semantic spread when refining finegrained sets.", "labels": [], "entities": []}, {"text": "We propose an algorithm using relevance feedback, including both positive and negative user feedback, for set refinement.", "labels": [], "entities": []}, {"text": "By expanding the set of features and weighting them appropriately, our algorithm is able to retrieve more related instances and provide better ranking.", "labels": [], "entities": []}, {"text": "Experimental results show that it improves the quality of fine-grained sets significantly.", "labels": [], "entities": []}], "datasetContent": [{"text": "Corpus: we used 37 years newspaper corpus 2 which is dependency parsed with the Stanford Parser and has all named entities tagged with Jet 4 NE tagger (we didn't use the NE tags reported by the tagger but only the fact that it is a name).", "labels": [], "entities": []}, {"text": "We use syntactic context, which is the grammatical relation in conjunction with the words as feature, and we replace the word in the candidate NE with *.", "labels": [], "entities": []}, {"text": "Both syntactic contexts in which the candidate entities are the heads and contexts in which the candidate entities are the dependents are used.", "labels": [], "entities": []}, {"text": "The feature set is created from syntactic contexts of all entities tagged in the corpus.", "labels": [], "entities": []}, {"text": "An example common feature for class National Capital is prep_in(ministry, *).", "labels": [], "entities": [{"text": "National Capital", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.913518637418747}]}, {"text": "We remove features in which the dependent is a stop word, and remove a limited number of less useful dependency types such as numerical modifier and determiner.", "labels": [], "entities": []}, {"text": "We use pointwise mutual information (PMI) to weight features for entities, and cosine as the similarity measure between the centroid of the seeds and candidate instances.", "labels": [], "entities": []}, {"text": "PMI scores are generated from the newspaper corpus statistics.", "labels": [], "entities": [{"text": "newspaper corpus statistics", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.9557345708211263}]}, {"text": "Candidates are then ranked by similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9957209229469299}]}, {"text": "We construct each named entity candidate pool by including similar instances with cosine score greater than 0.05 with the centroid of the corresponding golden set.", "labels": [], "entities": []}, {"text": "This ensures that each candidate pool contains tens of thousands of elements so that it contains all similar instances with high probability.", "labels": [], "entities": []}, {"text": "Golden sets 5 : Several golden sets are prepared by hand.", "labels": [], "entities": []}, {"text": "We start from lists from Wikipedia, and then manually refine the sets 6 by removing incorrect instances and adding correct instances found as distributionally-similar instances from the corpus.", "labels": [], "entities": []}, {"text": "The criteria for choosing the lists is 1) our corpus covers most elements of the list, 2) the list represents a fine-grained concept, 3) it contains hundreds of elements for reasons of fairness, since we don't want the added positive examples themselves to overshadow other aspects of the evaluated algorithms.", "labels": [], "entities": []}, {"text": "Based on these criteria, we chose three lists: National Capitals, IT companies 7 and New York City (NYC) neighborhoods.", "labels": [], "entities": [{"text": "National Capitals", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9643011391162872}]}, {"text": "All three sets have more than 200 elements.", "labels": [], "entities": []}, {"text": "User feedback is simulated by checking membership in the golden set.", "labels": [], "entities": []}, {"text": "Since existing contains roughly 65 million sentences and 1.3 billion tokens.", "labels": [], "entities": []}, {"text": "http://nlp.stanford.edu/software/lex-parser.shtml 4 http://cs.nyu.edu/grishman/jet/license.html Golden sets are available for download at http://www.cs.nyu.edu/~min/goldset_37news.tgz Manually checking indicates the golden sets are complete with high probability.", "labels": [], "entities": []}, {"text": "Set contains both software and hardware companies golden sets such as the sets from  are not designed specifically for evaluating refinement on fine-grained concepts and they are quite small for evaluating positive feedback (with less than 70 elements after removing low frequency ones in our corpus), we decided to construct our own.", "labels": [], "entities": []}, {"text": "4) RF-N, relevance feedback algorithm using only negative feedback (selected using the method described in section 4.1), 5) Relevance feedback (RF-all) using both positive and negative user feedback selected using methods from Section 4.1.", "labels": [], "entities": []}, {"text": "We use 6 seeds for all experiments, and set \u03b3=0.25 for all RF experiments.", "labels": [], "entities": []}, {"text": "For each algorithm, we evaluate the results after each iteration as follows: we calculate a centroid feature vector and then rank all candidates based on their similarity to the centroid.", "labels": [], "entities": []}, {"text": "We add sufficient top-ranked candidates to the seed and user-tagged positive items to form a set equal in size to the golden set.", "labels": [], "entities": []}, {"text": "This set, the refined set, is then compared to the golden set.", "labels": [], "entities": []}, {"text": "The following tables show a commonly reported metric, average R-precision 8 of 40 runs starting with randomly picked initial seeds (The first column shows the number of iterations.):.", "labels": [], "entities": []}, {"text": "Performance on class NYC neighborhoods Results show that RF-P outperforms the baseline algorithm by using positive examples with rich contexts rather than the first positive example for each iteration.", "labels": [], "entities": []}, {"text": "The baseline algorithm shows small improvement over 10 iterations.", "labels": [], "entities": []}, {"text": "This shows that simply adding the example which is most similar to the centroid is not very helpful.", "labels": [], "entities": []}, {"text": "Comparing R-precision gain between RF-P and the baseline suggests that selecting informative examples is critical for refining finegrained sets.", "labels": [], "entities": []}, {"text": "By enriching the feature set of the centroid, RF-P is able to retrieve instances with a limited number of features overlapping the original centroid.", "labels": [], "entities": []}, {"text": "RF-N outperforms FMM since it only reweights (penalizes some weights) but doesn't prune out intersection features between user-tagged errors and the centroid.", "labels": [], "entities": []}, {"text": "This flexibility avoids over-penalizing weak but informative features of the intended concept.", "labels": [], "entities": []}, {"text": "For FMM, we observe a small performance gain with successsive iterations over IT companies and NYC neighborhoods but a performance decrease for National Capitals.", "labels": [], "entities": [{"text": "National Capitals", "start_pos": 144, "end_pos": 161, "type": "DATASET", "confidence": 0.9205451011657715}]}, {"text": "Inspection of results shows that FMM tends to retrieve more capital cities for small geographical regions because of removal of weak features for informative sense such as Major Cities.", "labels": [], "entities": [{"text": "FMM", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8400992751121521}]}, {"text": "Combining RF-P and RF-N, RF-all uses both positive informative examples and negative informative examples to expand feature sets of the centroid and weight them appropriately, thus achieving the most performance gain.", "labels": [], "entities": []}, {"text": "RF-N by itself doesn't improve performance significantly.", "labels": [], "entities": [{"text": "RF-N", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.829267144203186}]}, {"text": "Comparing RF-all with RF-P, using informative negative examples helps to improve performance substantially because only when both informative positive examples and informative negative examples are used can we learn a significantly large set of features and appropriate weights for them.", "labels": [], "entities": []}, {"text": "We also implemented a few methods combining positive feedback and FMM, and didn't observe encouraging performance.", "labels": [], "entities": [{"text": "FMM", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.5760124921798706}]}, {"text": "RF-all also has the highest Average Precision (AP) for all sets, thus showing that it provides better ranking over candidates.", "labels": [], "entities": [{"text": "RF-all", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.88636314868927}, {"text": "Average Precision (AP)", "start_pos": 28, "end_pos": 50, "type": "METRIC", "confidence": 0.9084720134735107}]}, {"text": "Due to space limitations, tables of AP are not included.", "labels": [], "entities": [{"text": "AP", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.8461901545524597}]}, {"text": "The quality of the top ranked elements with RF-all can be seen in the precision at rank 50 for the three sets: 84.6%, 81.6%, and 71.7%.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9994401335716248}]}], "tableCaptions": []}