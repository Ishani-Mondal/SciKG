{"title": [{"text": "Using a Wikipedia-based Semantic Relatedness Measure for Document Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "A graph-based distance between Wikipedia articles is defined using a random walk model, which estimates visiting probability (VP) between articles using two types of links: hy-perlinks and lexical similarity relations.", "labels": [], "entities": [{"text": "visiting probability (VP)", "start_pos": 104, "end_pos": 129, "type": "METRIC", "confidence": 0.8542308807373047}]}, {"text": "The VP to and from a set of articles is then computed , and approximations are proposed to make tractable the computation of semantic relatedness between every two texts in a large data set.", "labels": [], "entities": []}, {"text": "The model is applied to document clustering on the 20 Newsgroups data set.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6718760579824448}, {"text": "20 Newsgroups data set", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.8142153695225716}]}, {"text": "Precision and recall are improved in comparison with previous textual distance algorithms.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.993724524974823}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9995208978652954}]}], "introductionContent": [{"text": "Many approaches have been proposed to compute similarity between texts, from lexical overlap measures to statistical topic models that are learned from large corpora.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for using knowledge from a structured, collaborative resource -the Wikipedia hypertext encyclopedia -in order to build a measure of semantic relatedness that we test on a text clustering task.", "labels": [], "entities": [{"text": "text clustering task", "start_pos": 206, "end_pos": 226, "type": "TASK", "confidence": 0.7645524442195892}]}, {"text": "The paper first describes the document graph derived from Wikipedia (Section 2), and then defines a network-based distance using visiting probability (Section 3), along with algorithms for its application to text clustering (Section 4).", "labels": [], "entities": [{"text": "text clustering", "start_pos": 208, "end_pos": 223, "type": "TASK", "confidence": 0.7454405128955841}]}, {"text": "Results over the 20 Newsgroups dataset are shown to be competitive (Section 5), and the relative contributions of cosine lexical similarity and visiting probability are analyzed.", "labels": [], "entities": [{"text": "Newsgroups dataset", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.9017422497272491}]}, {"text": "Our proposal is discussed in the light of previous work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first compute a similarity matrix for the entire 20 Newsgroups data set, with the relatedness score between any two documents being VP T . For tractability, we fixed T = 5 that gives sufficient precision; a larger value only increased computation time.", "labels": [], "entities": [{"text": "Newsgroups data set", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.8701033989588419}, {"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9988670349121094}]}, {"text": "Instead of computing VP T between all possible pairs separately, we fill one row of the matrix at a time using the approximations above.", "labels": [], "entities": []}, {"text": "We set the absorption probability of the random walk 1 \u2212 \u03b1 = 0.2 for this experiment.", "labels": [], "entities": []}, {"text": "Given \u03b1 and T by using the formula in section 3.5, it is possible to compute the error bound of the truncation, and noting that fora smaller \u03b1, fewer steps (T ) are needed to achieve the same approximation precision because of the penalty set to longer paths.", "labels": [], "entities": [{"text": "error", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9727925658226013}, {"text": "precision", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.7966926693916321}]}, {"text": "Conversely, a larger \u03b1 decreases the penalty for longer paths and requires more computation.", "labels": [], "entities": []}, {"text": "For comparison purposes, four similarity matrices were computed.", "labels": [], "entities": [{"text": "similarity", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9429605603218079}]}, {"text": "Indeed, the theoretical apparatus described above can be applied to various types of links in the document network.", "labels": [], "entities": []}, {"text": "In Section 2, we introduced two types of links, namely lexical similarity and actual hyperlinks, and these can be used separately in the model, or as a weighted combination.", "labels": [], "entities": []}, {"text": "The following similarities will be compared: 1.", "labels": [], "entities": []}, {"text": "VP over hyperlinks only (noted VP Hyp ); 2.", "labels": [], "entities": []}, {"text": "VP over lexical similarity links (VP Lex ); 3.", "labels": [], "entities": []}, {"text": "VP over a combination of hyperlinks (0.4) and lexical links (0.6) (noted VP Comb ) -these values gave the best results in our previous applications to word and document similarity tasks (Yazdani and Popescu-Belis, 2010); 4.", "labels": [], "entities": [{"text": "word and document similarity tasks", "start_pos": 151, "end_pos": 185, "type": "TASK", "confidence": 0.6512462139129639}]}, {"text": "no random walk, only cosine similarity between the tf-idf vectors of the documents to be clustered (noted LS , for lexical similarity).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision and Recall for k-means clustering over  the 20 Newsgroups using several well-known methods to  compute text similarity, in comparison to the present pro- posal.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9966431856155396}, {"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9983378648757935}]}]}