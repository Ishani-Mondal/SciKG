{"title": [{"text": "Evaluation without references: IBM1 scores as evaluation metrics", "labels": [], "entities": []}], "abstractContent": [{"text": "Current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations.", "labels": [], "entities": [{"text": "machine translation quality", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7941421667734782}]}, {"text": "We propose a truly automatic evaluation metric based on IBM1 lexicon probabilities which does not need any reference translations.", "labels": [], "entities": []}, {"text": "Several variants of IBM1 scores are systematically explored in order to find the most promising directions.", "labels": [], "entities": []}, {"text": "Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop.", "labels": [], "entities": [{"text": "Statistical Machine Translation Workshop", "start_pos": 135, "end_pos": 175, "type": "TASK", "confidence": 0.8564121574163437}]}, {"text": "Five different European languages are taken into account: English, Span-ish, French, German and Czech.", "labels": [], "entities": []}, {"text": "The results show that the IBM1 scores are competitive with the classic evaluation metrics, the most promising being IBM1 scores calculated on morphemes and POS-4grams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Currently used evaluation metrics such as BLEU), METEOR (Banerjee and), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9984721541404724}, {"text": "METEOR", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9855477809906006}]}, {"text": "While this scenario helps in the design of machine translation systems, it has two major drawbacks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8189027011394501}]}, {"text": "The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7424542903900146}]}, {"text": "The second criticism is methodological: in using reference translation, the problem of evaluating translation quality (e.g., completeness, ordering, domain fit, etc.) is transformed into a kind of paraphrase evaluation in the target language, which is a very difficult problem itself.", "labels": [], "entities": []}, {"text": "In addition, the set of selected references always represents only a small subset of all good translations.", "labels": [], "entities": []}, {"text": "To remedy these drawbacks, we propose a truly automatic evaluation metric which is based on the IBM1 lexicon scores (.", "labels": [], "entities": [{"text": "IBM1 lexicon scores", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.9239924351374308}]}, {"text": "The inclusion of IBM1 scores in translation systems has shown experimentally to improve translation quality (.", "labels": [], "entities": []}, {"text": "They also have been used for confidence estimation for machine translation (.", "labels": [], "entities": [{"text": "confidence estimation", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.6729350388050079}, {"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8213295340538025}]}, {"text": "To the best of our knowledge, these scores have not yet been used as an evaluation metric.", "labels": [], "entities": []}, {"text": "We carryout a systematic comparison between several variants of IBM1 scores.", "labels": [], "entities": []}, {"text": "The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third, fourth) and fifth) shared translation tasks.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 4, "end_pos": 31, "type": "METRIC", "confidence": 0.6212608516216278}]}], "datasetContent": [{"text": "and WMT 2010 test data  The IBM1 probabilities necessary for the IBM1 scores are learnt using the WMT 2010 News Commentary bilingual corpora consisting of the Spanish-English, French-English, German-English and Czech-English parallel texts.", "labels": [], "entities": [{"text": "WMT 2010 test data", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9650894403457642}, {"text": "WMT 2010 News Commentary bilingual corpora", "start_pos": 98, "end_pos": 140, "type": "DATASET", "confidence": 0.9441637098789215}]}, {"text": "Spanish, French, German and English POS tags were produced using the TreeTagger 1 , and the Czech texts are tagged using the COMPOST tagger).", "labels": [], "entities": [{"text": "TreeTagger 1", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9270316362380981}]}, {"text": "The morphemes for all languages are obtained using the Morfessor tool).", "labels": [], "entities": []}, {"text": "The tool is corpus-based and language-independent: it takes a text as input and produces a segmentation of the word forms observed in the text.", "labels": [], "entities": []}, {"text": "The obtained results are not strictly linguistic, however they often resemble a linguistic morpheme segmentation.", "labels": [], "entities": []}, {"text": "Once a morpheme segmentation has been learnt from some text, it can be used for segmenting new texts.", "labels": [], "entities": [{"text": "segmenting new texts", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.8598729173342387}]}, {"text": "In our experiments, the splitting are learnt from the training corpus used for the IBM1 lexicon probabilities.", "labels": [], "entities": [{"text": "IBM1 lexicon probabilities", "start_pos": 83, "end_pos": 109, "type": "DATASET", "confidence": 0.8808929324150085}]}, {"text": "The obtained segmentation is then used for splitting the corresponding source texts and hypotheses.", "labels": [], "entities": []}, {"text": "Detailed corpus statistics are shown in.", "labels": [], "entities": []}, {"text": "Using the obtained IBM1 probabilities of words, morphemes and POS n-grams, the scores described in Section 2 are calculated for the Spanish-English, French-English, German-English and Czech-English translation outputs from each translation direction.", "labels": [], "entities": []}, {"text": "For each of the IBM1 scores, the system level Spearman correlation coefficients \u03c1 with the human ranking are calculated for each document.", "labels": [], "entities": []}, {"text": "In total, 32 correlation coefficients are obtained for each score -four English outputs from the WMT 2010 task, four from the WMT 2009 and eight from the WMT 2008 task, together with sixteen outputs in other four target languages.", "labels": [], "entities": [{"text": "correlation", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9533596634864807}, {"text": "WMT 2010 task", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.845078706741333}, {"text": "WMT 2009", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.923958957195282}, {"text": "WMT 2008 task", "start_pos": 154, "end_pos": 167, "type": "DATASET", "confidence": 0.85503751039505}]}, {"text": "The obtained correlation results were then summarised into the following three values: \u2022 mean a correlation coefficient averaged overall translation outputs;  \u2022 rank> percentage of documents where the particular score has better correlation than the other IBM1 scores; \u2022 rank\u2265 percentage of documents where the particular score has better or equal correlation than the other IBM1 scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the corpora for training IBM1 lexicon models.", "labels": [], "entities": []}, {"text": " Table 2: Average correlations of source-hypothesis (left  column) and hypothesis-source (right column) IBM1  scores.", "labels": [], "entities": []}, {"text": " Table 3: IBM1 hs scores sorted by average correlation (column 1), rank> value (column 2) and rank\u2265 value (column  3). The most promising scores are those calculated on morphemes (MIBM1), POS-3grams (P3IBM1) and POS-4grams  (P4IBM1).", "labels": [], "entities": [{"text": "average correlation", "start_pos": 35, "end_pos": 54, "type": "METRIC", "confidence": 0.5930130183696747}]}, {"text": " Table 4: Average correlations of the investigated IBM1 hs  combinations. The weight values are choosen accord- ing to the average correlation of the particular individual  IBM1 score.", "labels": [], "entities": []}, {"text": " Table 5: rank> (column 1) and rank\u2265 (column 2) values  of the weighted IBM1 hs combinations.", "labels": [], "entities": []}]}