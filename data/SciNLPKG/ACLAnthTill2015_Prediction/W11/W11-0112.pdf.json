{"title": [{"text": "Integrating Logical Representations with Probabilistic Information using Markov Logic", "labels": [], "entities": [{"text": "Integrating Logical Representations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8498145143191019}]}], "abstractContent": [{"text": "First-order logic provides a powerful and flexible mechanism for representing natural language semantics.", "labels": [], "entities": [{"text": "representing natural language semantics", "start_pos": 65, "end_pos": 104, "type": "TASK", "confidence": 0.7042403072118759}]}, {"text": "However, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning.", "labels": [], "entities": [{"text": "word meaning", "start_pos": 123, "end_pos": 135, "type": "TASK", "confidence": 0.6869691461324692}]}, {"text": "This paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of Statistical Relational AI.", "labels": [], "entities": [{"text": "Statistical Relational AI", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.8511330485343933}]}, {"text": "Specifically, we show how Discourse Representation Structures can be combined with distribu-tional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Logic-based representations of natural language meaning have along history.", "labels": [], "entities": [{"text": "Logic-based representations of natural language meaning", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.6135005305210749}]}, {"text": "Representing the meaning of language in a first-order logical form is appealing because it provides a powerful and flexible way to express even complex propositions.", "labels": [], "entities": [{"text": "Representing the meaning of language", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8555349588394165}]}, {"text": "However, systems built solely using first-order logical forms tend to be very brittle as they have noway of integrating uncertain knowledge.", "labels": [], "entities": []}, {"text": "They, therefore, tend to have high precision at the cost of low recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9991658926010132}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9996658563613892}]}, {"text": "Recent advances in computational linguistics have yielded robust methods that use weighted or probabilistic models.", "labels": [], "entities": []}, {"text": "For example, distributional models of word meaning have been used successfully to judge paraphrase appropriateness.", "labels": [], "entities": []}, {"text": "This has been done by representing the word meaning in context as a point in a high-dimensional semantics space.", "labels": [], "entities": []}, {"text": "However, these models typically handle only individual phenomena instead of providing a meaning representation for complete sentences.", "labels": [], "entities": []}, {"text": "It is a long-standing open question how best to integrate the weighted or probabilistic information coming from such modules with logic-based representations in away that allows for reasoning over both.", "labels": [], "entities": []}, {"text": "See, for example,.", "labels": [], "entities": []}, {"text": "The goal of this work is to combine logic-based meaning representations with probabilities in a single unified framework.", "labels": [], "entities": []}, {"text": "This will allow us to obtain the best of both situations: we will have the full expressivity of first-order logic and be able to reason with probabilities.", "labels": [], "entities": []}, {"text": "We believe that this will allow fora more complete and robust approach to natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.6602277358373007}]}, {"text": "In order to perform logical inference with probabilities, we draw from the large and active body of work related to Statistical Relational AI.", "labels": [], "entities": [{"text": "Statistical Relational AI", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.8760662078857422}]}, {"text": "Specifically, we make use of Markov Logic Networks (MLNs) () which employ weighted graphical models to represent first-order logical formulas.", "labels": [], "entities": []}, {"text": "MLNs are appropriate for our approach because they provide an elegant method of assigning weights to first-order logical rules, combining a diverse set of inference rules, and performing inference in a probabilistic way.", "labels": [], "entities": []}, {"text": "While this is a large and complex task, this paper proposes a series of first steps toward our goal.", "labels": [], "entities": []}, {"text": "In this paper, we focus on three natural language phenomena and their interaction: implicativity and factivity, word meaning, and coreference.", "labels": [], "entities": []}, {"text": "Our framework parses natural language into a logical form, adds rule weights computed by external NLP modules, and performs inferences using an MLN.", "labels": [], "entities": []}, {"text": "Our end-to-end approach integrates multiple existing tools.", "labels": [], "entities": []}, {"text": "We use Boxer () to parse natural language into a logical form.", "labels": [], "entities": []}, {"text": "We use Alchemy () for MLN inference.", "labels": [], "entities": [{"text": "MLN inference", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.93653804063797}]}, {"text": "Finally, we use the exemplar-based distributional model of to produce rule weights.", "labels": [], "entities": []}], "datasetContent": [{"text": "Textual entailment offers a good framework for testing whether a system performs correct analyses and thus draws the right inferences from a given text.", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7699912190437317}]}, {"text": "For example, to test whether a system correctly handles implicative verbs, one can use the premise p along with the hypothesis h in (1) below.", "labels": [], "entities": []}, {"text": "If the system analyses the two sentences correctly, it should infer that h holds.", "labels": [], "entities": []}, {"text": "While the most prominent forum using textual entailment is the Recognizing Textual Entailment (RTE) challenge), the RTE datasets do not test the phenomena in which we are interested.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE) challenge", "start_pos": 63, "end_pos": 109, "type": "TASK", "confidence": 0.7178970447608403}, {"text": "RTE datasets", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.8954265415668488}]}, {"text": "For example, in order to evaluate our system's ability to determine word meaning in context, the RTE pair would have to specifically test word sense confusion by having a word's context in the hypothesis be different from the context of the premise.", "labels": [], "entities": [{"text": "word sense confusion", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.6791035234928131}]}, {"text": "However, this simply does not occur in the RTE corpora.", "labels": [], "entities": [{"text": "RTE corpora", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.735077977180481}]}, {"text": "In order to properly test our phenomena, we construct hand-tailored premises and hypotheses based on real-world texts.", "labels": [], "entities": []}, {"text": "In this paper, we focus on three natural language phenomena and their interaction: implicativity and factivity, word meaning, and coreference.", "labels": [], "entities": []}, {"text": "The first phenomenon, implicativity and factivity, is concerned with analyzing the truth conditions of nested propositions.", "labels": [], "entities": []}, {"text": "For example, in the premise of the entailment pair shown in example (1), \"arrange that\" falls under the scope of \"forget to\" and \"fail\" is under the scope of \"arrange that\".", "labels": [], "entities": []}, {"text": "Correctly recognizing nested propositions is necessary for preventing false inferences such as the one in example (2).", "labels": [], "entities": []}, {"text": "(1) p: Ed did not forget to arrange that Dave fail 1 h: Dave failed (2) p: The mayor hoped to build anew stadium 2 h*: The mayor built anew stadium For the second phenomenon, word meaning, we address paraphrasing and hypernymy.", "labels": [], "entities": [{"text": "word meaning", "start_pos": 175, "end_pos": 187, "type": "TASK", "confidence": 0.7438976466655731}]}, {"text": "For example, in (3) \"covering\" is a good paraphrase for \"sweeping\" while \"brushing\" is not.", "labels": [], "entities": []}, {"text": "As a preliminary evaluation of our system, we constructed a set of demonstrative examples to test our ability to handle the previously discussed phenomena and their interactions and ran each example with both a theorem prover and Alchemy.", "labels": [], "entities": []}, {"text": "Note that when running an example in the theorem prover, weights are not possible, so any rule that would be weighted in an MLN is simply treated as a \"hard clause\" following.", "labels": [], "entities": []}, {"text": "We constructed a list of 72 simple examples that exhaustively cover cases of implicativity (positive, negative, null entailments in both positive and negative environments), hypernymy, quantification, and the interaction between implicativity and hypernymy.", "labels": [], "entities": []}, {"text": "The purpose of these simple tests is to ensure that our flattened logical form and truth condition rules correctly maintain the semantics of the underlying DRSs.", "labels": [], "entities": []}], "tableCaptions": []}