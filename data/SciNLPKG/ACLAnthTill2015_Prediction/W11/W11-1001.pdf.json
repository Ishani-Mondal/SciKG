{"title": [{"text": "Automatic Projection of Semantic Structures: an Application to Pairwise Translation Ranking", "labels": [], "entities": [{"text": "Automatic Projection of Semantic Structures", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8408643007278442}, {"text": "Pairwise Translation", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.6521278023719788}]}], "abstractContent": [{"text": "We present a model for the inclusion of semantic role annotations in the framework of confidence estimation for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8091670870780945}]}, {"text": "The model has several interesting properties, most notably: 1) it only requires a linguistic processor on the (generally well-formed) source side of the translation; 2) it does not directly rely on properties of the translation model (hence, it can be applied beyond phrase-based systems).", "labels": [], "entities": []}, {"text": "These features make it potentially appealing for system ranking, translation re-ranking and user feedback evaluation.", "labels": [], "entities": [{"text": "translation re-ranking", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9434036910533905}]}, {"text": "Preliminary experiments in pairwise hypothesis ranking on five confidence estimation benchmarks show that the model has the potential to capture salient aspects of translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to automatically assess the quality of translation hypotheses is a key requirement towards the development of accurate and dependable translation models.", "labels": [], "entities": [{"text": "translation hypotheses", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9005367457866669}]}, {"text": "While it is largely agreed that proper transfer of predicate-argument structures from source to target is a very strong indicator of translation quality, especially in relation to adequacy (, the incorporation of this kind of information in the Statistical Machine Translation (SMT) evaluation pipeline is still limited to few and isolated cases, e.g.,.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT) evaluation pipeline", "start_pos": 245, "end_pos": 302, "type": "TASK", "confidence": 0.8089039959013462}]}, {"text": "In this paper, we propose a general model for the incorporation of predicate-level semantic annotations in the framework of Confidence Estimation (CE) for machine translation, with a specific focus on the sub-problem of pairwise hypothesis ranking.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7935692965984344}, {"text": "pairwise hypothesis ranking", "start_pos": 220, "end_pos": 247, "type": "TASK", "confidence": 0.6517258584499359}]}, {"text": "The model is based on the following underlying assumption: by observing how automatic alignments project semantic annotations from source to target in a parallel corpus, it is possible to isolate features that are characteristic of good translations, such as movements of specific arguments for some classes of predicates.", "labels": [], "entities": []}, {"text": "The presence (or absence) of these features in automatic translations can then be used as an indicator of their quality.", "labels": [], "entities": []}, {"text": "It is important to stress that we are not claiming that the projections preserve the meaning of the original annotation.", "labels": [], "entities": []}, {"text": "Still, it should be possible to observe regularities that can be helpful to rank alternative translation hypotheses.", "labels": [], "entities": []}, {"text": "The general workflow (which can easily be extended to cope with different annotation layers, such as sequences of meaningful phrase boundaries, named entities or sequences of chunks or POS tags) is exemplified in.", "labels": [], "entities": []}, {"text": "During training (on the left), the system receives a parallel corpus of source sentences and the corresponding reference translations.", "labels": [], "entities": []}, {"text": "Source sentences are annotated with a linguistic processor.", "labels": [], "entities": []}, {"text": "The annotations are projected using training alignments, obtaining gold projections that we can use to learn a model that captures correct annotation movements, i.e., observed in reference translations.", "labels": [], "entities": []}, {"text": "At test time, we want to assess the quality of a translation hypothesis given a source sentence.", "labels": [], "entities": []}, {"text": "As shown on the right side of, the first part of the process is the same as during training: the source sentence is annotated, and the annotation is projected onto the translation hypothesis via automatic alignments.", "labels": [], "entities": []}, {"text": "The model is then used 1 to compare the observed projection against the expected projection given the source annotation.", "labels": [], "entities": []}, {"text": "The distance between the two projections (observed and expected) can then be used as a measure of the quality of the hypothesis.", "labels": [], "entities": []}, {"text": "As it only considers one-sided annotations, our framework does not require the availability of comparable linguistic processors and linguistic annotations, tagsets, etc., on both sides of the translation process.", "labels": [], "entities": []}, {"text": "In this way, it overcomes one of the main obstacles to the adoption of linguistic analysis for MT confidence estimation.", "labels": [], "entities": [{"text": "MT confidence estimation", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8867907524108887}]}, {"text": "Furthermore, the fact that source data is generally well-formed lowers the requirements on the linguistic processor in terms of robustness to noisy data, making it possible to employ a wider range of linguistic processors.", "labels": [], "entities": []}, {"text": "Within this framework, in this paper we describe our attempt to bridge Semantic Role Labeling (SRL) and CE by modeling proposition-level semantics for pairwise translation ranking.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.7677809645732244}, {"text": "pairwise translation ranking", "start_pos": 151, "end_pos": 179, "type": "TASK", "confidence": 0.6422591110070547}]}, {"text": "The extent to which this kind of annotations are transferred from source to target has indeed a very high correlation with respect to human quality assessments).", "labels": [], "entities": []}, {"text": "The measure that we propose is then an ideal addition to already established CE measures, e.g.,), as it attempts to explicitly model the adequacy of translation hypotheses as a function of predicateargument structure coverage.", "labels": [], "entities": []}, {"text": "While we are aware of the fact that the current definition of the model can be improved in many different ways, our preliminary investigation, on five English to Spanish translation benchmarks, shows promising accuracy on the difficult task of pairwise translation ranking, even for translations with very few distinguishing features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9916278123855591}, {"text": "translation ranking", "start_pos": 253, "end_pos": 272, "type": "TASK", "confidence": 0.7586951851844788}]}, {"text": "To capture different aspects of the projection of SRL annotations we employ two instances of the abstract architecture shown in.", "labels": [], "entities": [{"text": "SRL annotations", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7590000927448273}]}, {"text": "The first works at the proposition level, and models the correct movement of arguments from source to target.", "labels": [], "entities": []}, {"text": "The second works at the argument level, and models the fluency and adequacy of individual arguments within each predicate-argument structure.", "labels": [], "entities": []}, {"text": "The models that we learn during training are simple phrasebased translation models working on different kinds of sequences, i.e., role labels in the former case and words in the latter.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.6976076662540436}]}, {"text": "To evaluate the adequacy of an automatically projected proposition or argument, we force the corresponding translation model to generate it (via constrained decoding).", "labels": [], "entities": []}, {"text": "The reachability and confidence of each translation are features that we exploit to compare alternative translations, by combining them in a simple voting scheme.", "labels": [], "entities": []}, {"text": "To score systems which are not under our direct control (the typical scenario in CE benchmarks), we introduce a component that generates source-target alignments for any pair of aligned test sentences.", "labels": [], "entities": []}, {"text": "This addition has the nice property of allowing us to handle the translation as a black-box, decoupling the evaluation from a specific system and, in theory, allowing the model to cope with phrase-based, rulebased or hierarchical systems alike, as well as with human-generated translations.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: in Section 2 we will review a selection of related work; in Section 3 we will detail our approach; in Section 4 we will present the results of our evaluation; finally, in Section 5 we will draw our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the results obtained by applying the proposed method to the task of ranking consistency, or pairwise ranking of alternative translations: that is, given a source sentence s, and two candidate translations h 1 and h 2 , decide which one is a better translation for s.", "labels": [], "entities": []}, {"text": "Pairwise ranking is a simplified setting for CE that is general enough to model the selection of the best translation among a finite set of alternatives.", "labels": [], "entities": []}, {"text": "Even though it cannot measure translation quality in isolation, a reliable pairwise ranking model would be sufficient to solve many common practical CE problems, such as system ranking, user feedback filtering or hypotheses re-ranking.", "labels": [], "entities": [{"text": "system ranking", "start_pos": 170, "end_pos": 184, "type": "TASK", "confidence": 0.7963832318782806}, {"text": "user feedback filtering", "start_pos": 186, "end_pos": 209, "type": "TASK", "confidence": 0.678432047367096}]}, {"text": "We ran our experiments on the human assessments.", "labels": [], "entities": []}, {"text": "These datasets will be referred to as wmtYY(t) in the remainder, YY being the last two digits of the year of the workshop and t = n for newswire data or t = e for Europarl data.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.9892311096191406}]}, {"text": "So, for example, wmt08e is the Europarl test set of the 2008 edition 5 of the workshop.", "labels": [], "entities": [{"text": "Europarl test set of the 2008 edition 5 of the workshop", "start_pos": 31, "end_pos": 86, "type": "DATASET", "confidence": 0.9145951867103577}]}, {"text": "As our system is trained on Europarl data, newswire test sets are to be considered out-of-domain.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.992778867483139}]}, {"text": "All the experiments are relative to English to Spanish translations.", "labels": [], "entities": []}, {"text": "The wmt08, wmt09 and wmt10 datasets provide a ranking among systems within the range (1 being the worst system, and 5 the best).", "labels": [], "entities": [{"text": "wmt10 datasets", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.8897141516208649}]}, {"text": "The different datasets contain assessments fora different number of systems, namely: 11 for wmt08(e), 10 for wmt08(n), 9 for wmt09 and 16 for wmt10n.", "labels": [], "entities": []}, {"text": "Generally, multiple annotations are available for each annotated sentence.", "labels": [], "entities": []}, {"text": "In all cases in which multiple assessments are available, we used the average of the assessments.", "labels": [], "entities": []}, {"text": "The wmt07 dataset would be the most interesting of all, in that it provides separate assessments for the two main dimensions of translation quality, adequacy and fluency, as well as system rankings.", "labels": [], "entities": [{"text": "wmt07 dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9365000128746033}]}, {"text": "Unluckily, the number of annotations in this dataset is very small, and after eliminating the ties the numbers are even smaller.", "labels": [], "entities": []}, {"text": "As results on such small numbers would not be very representative, we decided not to include them in our evaluation.", "labels": [], "entities": []}, {"text": "We also evaluated on the dataset described in (, which we will refer to as specia.", "labels": [], "entities": []}, {"text": "As the system is based on Europarl data, it is to be considered an in-domain benchmark.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9939053654670715}]}, {"text": "The dataset includes results produced by four different systems, each translation being annotated by only one judge.", "labels": [], "entities": []}, {"text": "Given the size of the corpus (the output of each system has been annotated on the same set of 4,000 sentences), this dataset is the most representative among those that we considered.", "labels": [], "entities": []}, {"text": "It is also especially interesting for two other reasons: 1) systems are assigned a score ranging from 1 (bad) to 4 (good as it is) based on the number of edits required to produce a publication-ready translation.", "labels": [], "entities": []}, {"text": "Therefore, here we have an absolute measure of translation accuracy, as opposed to relative rankings; 2) each system involved in the evaluation has very peculiar characteristics, hence they are very likely to generate quite different translations for the same input sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9006633162498474}]}], "tableCaptions": [{"text": " Table 1: Results on five confidence estimation bench- marks. An n next to the task name (e.g. wmt08n) stands  for a news (i.e. out of domain) corpus, whereas an e (e.g.  wmt08e) stands for a Europarl (i.e. in domain) corpus.  The specia corpus is in-domain.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 192, "end_pos": 200, "type": "DATASET", "confidence": 0.9478021264076233}]}]}