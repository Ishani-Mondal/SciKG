{"title": [{"text": "Search-based Structured Prediction applied to Biomedical Event Extraction", "labels": [], "entities": [{"text": "Structured Prediction", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.6675764173269272}, {"text": "Biomedical Event Extraction", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7758158842722574}]}], "abstractContent": [{"text": "We develop an approach to biomedical event extraction using a search-based structured prediction framework, SEARN, which converts the task into cost-sensitive classification tasks whose models are learned jointly.", "labels": [], "entities": [{"text": "biomedical event extraction", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7120807568232218}, {"text": "SEARN", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9178997874259949}]}, {"text": "We show that SEARN improves on a simple yet strong pipeline by 8.6 points in F-score on the BioNLP 2009 shared task, while achieving the best reported performance by a joint inference method.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.7264004349708557}, {"text": "F-score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9989653825759888}, {"text": "BioNLP 2009 shared task", "start_pos": 92, "end_pos": 115, "type": "DATASET", "confidence": 0.8479904681444168}]}, {"text": "Additionally, we consider the issue of cost estimation during learning and present an approach called focused costing that improves improves efficiency and predictive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.8170350790023804}]}], "introductionContent": [{"text": "The term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations involving one or more entities from the biomedical literature.", "labels": [], "entities": [{"text": "biomedical event extraction", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.7152935862541199}]}, {"text": "The recent BioNLP 2009 shared task (BioNLP09ST) on event extraction) focused on event types of varying complexity.", "labels": [], "entities": [{"text": "BioNLP 2009 shared task (BioNLP09ST)", "start_pos": 11, "end_pos": 47, "type": "DATASET", "confidence": 0.7033231428691319}, {"text": "event extraction", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7222104966640472}]}, {"text": "Each event consists of a trigger and one or more arguments, the latter being proteins or other events.", "labels": [], "entities": []}, {"text": "Any token in a sentence can be a trigger for one of the nine event types and, depending on their associated event types, triggers are assigned appropriate arguments.", "labels": [], "entities": []}, {"text": "Thus, the task can be viewed as a structured prediction problem in which the output fora given instance is a (possibly disconnected) directed acyclic graph (not necessarily a tree) in which vertices correspond to triggers or protein arguments, and edges represent relations between them.", "labels": [], "entities": []}, {"text": "Despite being a structured prediction task, most of the systems that have been applied to BioNLP09ST to date are pipelines that decompose event extraction into a set of simpler classification tasks.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.7281588762998581}]}, {"text": "Classifiers for these tasks are typically learned independently, thereby ignoring event structure during training.", "labels": [], "entities": []}, {"text": "Typically in such systems, the relationships among these tasks are taken into account by incorporating post-processing rules that enforce certain constraints when combining their predictions, and by tuning classification thresholds to improve the accuracy of joint predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9967104196548462}]}, {"text": "Pipelines are appealing as they are relatively easy to implement and they often achieve state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Because of the nature of the output space, the task is not amenable to sequential or grammar-based approaches (e.g. linear CRFs, HMMs, PCFGs) which employ dynamic programming in order to do efficient inference.", "labels": [], "entities": []}, {"text": "The only joint inference framework that has been applied to BioNLP09ST to date is Markov Logic Networks (MLNs) (.", "labels": [], "entities": []}, {"text": "However, MLNs require task-dependent approximate inference and substantial computational resources in order to achieve state-of-the-art performance.", "labels": [], "entities": []}, {"text": "In this work we explore an alternative joint inference approach to biomedical event extraction using a search-based structured prediction framework, SEARN.", "labels": [], "entities": [{"text": "biomedical event extraction", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6800141632556915}, {"text": "SEARN", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.6424096822738647}]}, {"text": "SEARN is an algorithm that converts the problem of learning a model for structured prediction into learning a set of models for cost-sensitive classification (CSC).", "labels": [], "entities": []}, {"text": "CSC is a task in which each training instance has a vector of misclassification costs associated with it, thus rendering some mistakes on some instances to be more expensive than others).", "labels": [], "entities": []}, {"text": "Compared to a standard pipeline, SEARN is able to achieve better performance because its models are learned jointly.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 33, "end_pos": 38, "type": "TASK", "confidence": 0.3405601382255554}]}, {"text": "Thus, each of them is able to use features representing the predictions made by the others, while taking into account possible mistakes.", "labels": [], "entities": []}, {"text": "In this paper, we make the following contributions.", "labels": [], "entities": []}, {"text": "Using the SEARN framework, we develop a joint inference approach to biomedical event extraction.", "labels": [], "entities": [{"text": "SEARN framework", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.7295568734407425}, {"text": "biomedical event extraction", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7406866947809855}]}, {"text": "We evaluate our approach on the BioNLP09ST dataset and show that SEARN improves on a simple yet strong pipeline by 8.6 points in F-score, while achieving the best reported performance on the task by a joint inference method.", "labels": [], "entities": [{"text": "BioNLP09ST dataset", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9480823874473572}, {"text": "SEARN", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9488608241081238}, {"text": "F-score", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.998906135559082}]}, {"text": "Additionally, we consider the issue of cost estimation and present an approach called focused costing that improves performance.", "labels": [], "entities": [{"text": "cost estimation", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7317055761814117}]}, {"text": "We believe that these contributions are likely to be relevant to applications of SEARN to other natural language processing tasks that involve structured prediction in complex output spaces.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 81, "end_pos": 86, "type": "TASK", "confidence": 0.7833252549171448}]}], "datasetContent": [{"text": "BioNLP09ST comprises three datasets -training, development and test -which consist of 800, 150 and 260 abstracts respectively.", "labels": [], "entities": [{"text": "BioNLP09ST", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8539426922798157}]}, {"text": "After the end of the shared task, an on-line evaluation server was activated in order to allow the evaluation on the test data once per day, without allowing access to the data itself.", "labels": [], "entities": []}, {"text": "We report results using Recall/Precision/F-score over complete events using the approximate span matching/approximate recursive matching variant which was the primary performance criterion in BioNLP09ST.", "labels": [], "entities": [{"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.98104327917099}, {"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.7607226967811584}, {"text": "BioNLP09ST", "start_pos": 192, "end_pos": 202, "type": "DATASET", "confidence": 0.8868944644927979}]}, {"text": "This variant counts a predicted event as a true positive if its trigger is extracted within a one-token extension of the goldstandard trigger.", "labels": [], "entities": []}, {"text": "Also, in the case of nested events, those events below the top-level need their trigger, event type and Theme but not their Cause to be correctly identified for the top-level event to be considered correct.", "labels": [], "entities": [{"text": "Theme", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9961380362510681}]}, {"text": "The same event matching variant was used in defining the loss as described in Sec.", "labels": [], "entities": []}, {"text": "5. A pre-processing step we perform on the training data is to reduce the multi-token triggers in the gold standard to their syntactic heads.", "labels": [], "entities": []}, {"text": "This procedure simplifies the task of assigning arguments to triggers and, as the evaluation variant used allows approximate trigger matching, it does not result in a performance loss.", "labels": [], "entities": []}, {"text": "For syntactic parsing, we use the output of the BLLIP re-ranking parser adapted to the biomedical domain by, as provided by the shared task organizers in the Stanford collapsed dependency format with conjunct dependency propagation.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7375849783420563}, {"text": "BLLIP", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.6154659986495972}]}, {"text": "Lemmatization is performed using morpha ().", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9523887634277344}]}, {"text": "In all our experiments, for CSC learning with PA, the C parameter is set by tuning on 10% of the training data and the number of rounds is fixed to 10.", "labels": [], "entities": [{"text": "CSC learning", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9148894846439362}, {"text": "C", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9580999612808228}]}, {"text": "For SEARN, we set the interpolation parameter \u03b2 to 0.3 and the number of iterations to 12.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.4806983470916748}, {"text": "interpolation parameter \u03b2", "start_pos": 22, "end_pos": 47, "type": "METRIC", "confidence": 0.7210050821304321}]}, {"text": "The costs for each action are obtained by averaging three samples as described in Sec.", "labels": [], "entities": []}, {"text": "\u03b2 and the number of samples are the only parameters that need tuning and we use the development data for this purpose.", "labels": [], "entities": []}, {"text": "First we compare against a pipeline of independently learned classifiers obtained as described in Sec.", "labels": [], "entities": []}, {"text": "4 in order to assess the benefits of joint learning under SEARN using focused costing.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7276628613471985}]}, {"text": "The results shown in demonstrate that SEARN obtains better event extraction performance on both the development and test sets by 7.7 and 8.6 F-score points respectively.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.7170772552490234}, {"text": "event extraction", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7497560977935791}, {"text": "F-score", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9923505187034607}]}, {"text": "The pipeline baseline employed in our experiments is a strong one: it would have ranked fifth in BioNLP09ST and it is 20 F-score points better than the baseline MLN employed by.", "labels": [], "entities": [{"text": "BioNLP09ST", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.579928994178772}, {"text": "F-score", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9983507394790649}]}, {"text": "Nevertheless, the independently learned classifier for triggers misses almost half of the event triggers, from which the subsequent stages cannot recover.", "labels": [], "entities": []}, {"text": "On the other hand, the trigger classifier learned with SEARN overpredicts, but since the Theme and Cause classifiers are learned jointly with it they maintain relatively high precision with substantially higher recall compared to their in- The focused costing approach we proposed contributes to the success of SEARN.", "labels": [], "entities": [{"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9965018033981323}, {"text": "recall", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.998630702495575}]}, {"text": "If we replace it with the default costing approach which uses the whole sentence, the F-score drops by 4.2 points on both development and test datasets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9990825653076172}]}, {"text": "The default costing approach mainly affects the trigger recognition stage, which takes place first.", "labels": [], "entities": [{"text": "trigger recognition", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.9037805497646332}]}, {"text": "Trigger overprediction is more extreme in this case and renders the Theme assignment stage harder to learn.", "labels": [], "entities": [{"text": "Theme assignment", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.8074120283126831}]}, {"text": "While the joint learning of the classifiers ameliorates this issue and the event extraction performance is eventually higher than that of the pipeline, the use of focused costing improves the performance even further.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7114347219467163}]}, {"text": "Note that trigger overprediction also makes training slower, as it results in evaluating more actions for each sentence.", "labels": [], "entities": []}, {"text": "Finally, using one instead of three samples per action decreases the F-score by 1.3 points on the development data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9995670914649963}]}, {"text": "Compared with the MLN approaches applied to BioNLP09ST, our predictive accuracy is better than that of which is the best joint inference performance to date and substantially better than that of in F-score respectively).", "labels": [], "entities": [{"text": "BioNLP09ST", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.7257707715034485}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9164068698883057}, {"text": "F-score", "start_pos": 198, "end_pos": 205, "type": "METRIC", "confidence": 0.9941908717155457}]}, {"text": "Recently, McClosky et al.", "labels": [], "entities": []}, {"text": "(2011) combined multiple decoders fora dependency parser with a reranker, achieving 48.6 in F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9952067732810974}]}, {"text": "While they also extracted structure features for Theme and Cause assignment, their model is restricted to trees (ours can output directed acyclic graphs) and their trigger recognizer is learned independently.", "labels": [], "entities": []}, {"text": "When we train SEARN combining the training and the development sets, we reach 52.3 in F-score, which is better than the performance of the top system in BioNLP09ST (51.95) by which was trained in the same way.", "labels": [], "entities": [{"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9995878338813782}, {"text": "BioNLP09ST", "start_pos": 153, "end_pos": 163, "type": "DATASET", "confidence": 0.7513022422790527}]}, {"text": "The best performance to date is reported by3 in F-score), who experimented with six parsers, three dependency representations and various combinations of these.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9696692824363708}]}, {"text": "They found that different parser/dependency combinations provided the best results on the development and test sets.", "labels": [], "entities": []}, {"text": "A direct comparison between learning frameworks is difficult due to the differences in task decomposition and feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7312438488006592}]}, {"text": "In particular, event extraction results depend substantially on the quality of the syntactic parsing.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8002914488315582}, {"text": "syntactic parsing", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.6756148487329483}]}, {"text": "For example, heuristically correct the syntactic parsing used and report that this improved their performance by four F-score points.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.754621297121048}, {"text": "F-score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9975205063819885}]}], "tableCaptions": [{"text": " Table 1: Recall / Precision / F-score on BioNLP09ST development and test data. Left-to-right: pipeline of  independently learned classifiers, SEARN with focused costing, SEARN with default costing.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9800596833229065}, {"text": "Precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9399704933166504}, {"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.8679832220077515}, {"text": "SEARN", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.8554519414901733}]}]}