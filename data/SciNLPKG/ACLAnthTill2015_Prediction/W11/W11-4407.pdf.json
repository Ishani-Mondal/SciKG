{"title": [{"text": "E-Dictionaries and Finite-State Automata for the Recognition of Named Entities", "labels": [], "entities": [{"text": "Recognition of Named Entities", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.8595728725194931}]}], "abstractContent": [{"text": "In this paper we present a system for named entity recognition and tagging in Serbian that relies on large-scale lexical resources and finite-state transducers.", "labels": [], "entities": [{"text": "named entity recognition and tagging", "start_pos": 38, "end_pos": 74, "type": "TASK", "confidence": 0.7137457013130188}]}, {"text": "Our system recognizes several types of name, temporal and numerical expressions.", "labels": [], "entities": []}, {"text": "Finite-state automata are used to describe the context of named entities , thus improving the precision of recognition.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9976565837860107}]}, {"text": "The widest context was used for personal names and it included the recognition of nominal phrases describing a person's position.", "labels": [], "entities": [{"text": "recognition of nominal phrases describing a person's position", "start_pos": 67, "end_pos": 128, "type": "TASK", "confidence": 0.7981200284428067}]}, {"text": "For the evaluation of the named entity recognition system we used a corpus of 2,300 short agency news.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.6320925354957581}]}, {"text": "Through manual evaluation we precisely identified all omissions and incorrect recognitions which enabled the computation of recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.99915611743927}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9980878233909607}]}, {"text": "The overall recall R = 0.84 for types and R = 0.93 for tokens , and overall precision P = 0.95 for types and P = 0.98 for tokens show that our system gives priority to precision.", "labels": [], "entities": [{"text": "recall R", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.959961861371994}, {"text": "precision P", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9653291404247284}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.99687260389328}]}], "introductionContent": [{"text": "Recognition of named entities (NER) has been a hot topic in Natural Language Processing community for more than fifteen years.", "labels": [], "entities": [{"text": "Recognition of named entities (NER)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9213449954986572}]}, {"text": "Ever since their introduction in the scope of the Sixth Message Understanding Conference () they have not ceased to arouse interest of developers of various NLP applications.", "labels": [], "entities": [{"text": "Sixth Message Understanding Conference", "start_pos": 50, "end_pos": 88, "type": "DATASET", "confidence": 0.7523851990699768}]}, {"text": "The nature of proper names, as a sub-class of named entities, for Serbian was analyzed in () especially in connection with its inflectional and derivational richness.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, beside some small-scale experiments, no effective NER system was yet produced for Serbian.", "labels": [], "entities": [{"text": "NER", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.969727098941803}]}, {"text": "In this paper we present a working system for recognition of various named entities in Serbian newspaper texts, as well as results of the evaluation of this system on a corpus of short agency news.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the results produced by our graphs we used a collection of 2,300 short agency news dated from May 2005 to December 2006.", "labels": [], "entities": []}, {"text": "The size of this corpus is approximately 117,000 simple word forms, and 4,273 sentences.", "labels": [], "entities": []}, {"text": "Thus, an average news item consists of a little less than 2 sentences (1.86), and 51 simple word forms.", "labels": [], "entities": []}, {"text": "This collection of news pertains to Serbian politics, both internal and external.", "labels": [], "entities": []}, {"text": "The graphs were applied to the corpus in the following order: measurement expressions, money expressions, dates, personal names and roles, time of day, geopolitical names.", "labels": [], "entities": []}, {"text": "This order is the simple consequence of the fact that the graphs were applied in the order in which they were actually produced.", "labels": [], "entities": []}, {"text": "The tagged texts where then handed to students who read them carefully and checked all the inserted tags 2 . The students also inserted anew attribute (PROVERA, 'check') into every tag with the value 'OK' if the named entity was correctly recognized and tagged, or 'NOK' if this was not the case or if it was only partially recognized.", "labels": [], "entities": [{"text": "PROVERA", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.9423888921737671}, {"text": "NOK", "start_pos": 266, "end_pos": 269, "type": "METRIC", "confidence": 0.8454781770706177}]}, {"text": "If the named entity was totally missed, the students inserted the appropriate tag with the value 'MISS' in the check attribute.", "labels": [], "entities": [{"text": "MISS", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9866742491722107}]}, {"text": "One such example is: . .", "labels": [], "entities": []}, {"text": "od <RS PROVERA='MISS'>generalnog sekretara NATO <IME PROVERA='MISS'>Jap de Hop Shefera </IME></RS>.", "labels": [], "entities": [{"text": "generalnog sekretara NATO", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.7843072215716044}]}, {"text": "'by NATO Secretary General Jaap de Hoop Scheffer.", "labels": [], "entities": []}, {"text": "'. It was not always easy to decide which value for the check attribute PROVERA is the most appropriate.", "labels": [], "entities": [{"text": "PROVERA", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9215077757835388}]}, {"text": "We at present neglected the fuzzy nature of some named entities and always treated as correct, for instance, geographic place tags, although The results of our experiment are summarized in.", "labels": [], "entities": []}, {"text": "The sample we used contained 9,677 NE tokens and 2,844 NE types, hence 3.4 tokens per type.", "labels": [], "entities": []}, {"text": "At the top of the list of most frequent tokens are names of countries (TOP-C) followed by personal names (NAME), 3,115 and 3,056, respectively, accounting for 32.2% and 31.6% of the total number of tokens.", "labels": [], "entities": []}, {"text": "Names of settlement (TOP-S) follow closely with 28.9%, and these three categories account for 92.7% of the total number of NE tokens.", "labels": [], "entities": [{"text": "TOP-S", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8926800489425659}]}, {"text": "As for NE types, half of them represent personal names, another 20.5% are settlements followed by 10.9% for countries, reaching all together 81.4% of total NE types.", "labels": [], "entities": []}, {"text": "The highest token/type ratio is 10.0 for names of countries and the lowest, one token per type, for temporal expressions representing date periods (DATE-P), which does not come as a surprise.", "labels": [], "entities": [{"text": "DATE-P)", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.8976908028125763}]}, {"text": "If we look at NE tokens only, the precision of our NER system is 0.98 whereas its recall is 0.93, yielding an overall F-measure of 0.95.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9995707869529724}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9995365142822266}, {"text": "F-measure", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9986603260040283}]}, {"text": "The highest precision of 1.0 was reached for date periods, followed by names of countries, and names of settlements, both with a precision of 0.99.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994338154792786}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9904093742370605}]}, {"text": "On the other hand, the lowest precision of 0.76 was achieved for names of water bodies (TOP-W), followed by 0.83 for temporal expressions representing time periods (TIME-P).", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9990766048431396}, {"text": "TOP-W", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9500856995582581}, {"text": "TIME-P", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.934573233127594}]}, {"text": "However, names of water bodies have a recall of 1.0, hence an overall F-measure of 0.87.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9992825388908386}, {"text": "F-measure", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9993500113487244}]}, {"text": "The lowest recall of only 0.56 for NE representing measures can be explained by an oversight in the construction of relevant graphs.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9981552958488464}]}, {"text": "Namely, we failed to include the units for time in the graphs, and since these units appear quite often in newspaper texts the graphs consequently failed to recognize them.", "labels": [], "entities": []}, {"text": "This flaw will, of course, be removed in the future.", "labels": [], "entities": []}, {"text": "Named entities representing measures have also the lowest Fmeasure of 0.71 followed by time periods with 0.77.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9901846647262573}]}, {"text": "On the other end are names of countries, which have an F-measure of as much as 0.99.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9991372227668762}]}, {"text": "If we look at NE types, the results do not differ very much.", "labels": [], "entities": []}, {"text": "The overall precision is 0.95, the recall 0.84, and the F-measure 0.89.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994775652885437}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9993937015533447}, {"text": "F-measure", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9916229844093323}]}, {"text": "The highest possible precision of 1.0 goes once again to date periods, but this time closely followed by currencies (CURR)  Finally, we would like to mention also the success rate of the recognition of a person's position in the society (RS).", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.997748076915741}, {"text": "recognition of a person's position in the society (RS)", "start_pos": 187, "end_pos": 241, "type": "TASK", "confidence": 0.6017097557584444}]}, {"text": "As these are not usually considered as NEs, we did not include them in the general overview table.", "labels": [], "entities": []}, {"text": "However, for 2,991 tokens and 1,129 types related to such expressions, the precision was 0.88 and 0.94 respectively, with a recall of 0.84 for tokens and 0.78 for types, thus making their F-measure almost equal (0.86 vs. 0.85).", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9993676543235779}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9988269209861755}, {"text": "F-measure", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9962282180786133}]}, {"text": "Given the complexity and variety of such expressions this can be perceived as a very successful outcome.", "labels": [], "entities": []}, {"text": "The analysis of the obtained results showed that the causes of omissions and incorrect tagging were various and can be classified as follows: \u2022 Typographic errors in the source text; \u2022 Absence of a name in e-dictionaries; \u2022 Oversights and minor deficiencies in the construction of graphs; \u2022 Failure of a graph to coverall syntactic constructions.", "labels": [], "entities": [{"text": "Absence", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.9841527342796326}]}, {"text": "Not much can be done in case of the first cause but our experiment proved very useful in detecting omissions and deficiencies in our e-dictionaries and graphs.", "labels": [], "entities": []}, {"text": "Reducing the errors and omissions resulting from the fourth cause listed is most demanding in the case of graphs that recognize a person's positions, and it will ask for either production of additional graphs or substantial reconstruction of some of the existing ones.", "labels": [], "entities": []}, {"text": "We will here only mention the most frequent cases.", "labels": [], "entities": []}, {"text": "\u2022 Our graphs failed in many cases when the text contained a list of personal names with their positions, due to alack of a straightforward link between the name and the position.", "labels": [], "entities": []}, {"text": "One example is: Predsednici Bugarske i Srbije Georgi Parvanov i Boris Tadi\u00b4cTadi\u00b4c 'Presidents of Bulgaria and Serbia Georgi Parvanov and Boris Tadi\u00b4cTadi\u00b4c'.", "labels": [], "entities": []}, {"text": "\u2022 Our graphs failed in cases of nested structures as such cases were not envisaged, e.g. portparol glavnog tu\u017eioca Ha\u0161kog tribunala Karle del Ponte Florans Artman 'spokesman for Chief Prosecutor of Hague Tribunal Carla del Ponte, Florence Hartmann'.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The system of Serbian e-dictionaries", "labels": [], "entities": []}]}