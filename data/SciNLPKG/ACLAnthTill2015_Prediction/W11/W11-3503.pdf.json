{"title": [{"text": "Efficient dictionary and language model compression for input method editors", "labels": [], "entities": []}], "abstractContent": [{"text": "Reducing size of dictionary and language model is critical when applying them to real world applications including machine translation and input method editors (IME).", "labels": [], "entities": [{"text": "machine translation and input method editors (IME)", "start_pos": 115, "end_pos": 165, "type": "TASK", "confidence": 0.723779989613427}]}, {"text": "Especially for IME, we have to drastically compress them without sacrificing lookup speed, since IMEs need to be executed on local computers.", "labels": [], "entities": [{"text": "IME", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8476187586784363}]}, {"text": "This paper presents novel lossless compression algorithms for both dictionary and language model based on succinct data structures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical approaches to processing natural language have become popular in recent years.", "labels": [], "entities": []}, {"text": "Input method editor is not an exception and stochastic input methods have been proposed and rolled out to real applications recently).", "labels": [], "entities": [{"text": "Input method editor", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.682565450668335}]}, {"text": "Compared to the traditional rule-based approach, a statistical approach allows us to improve conversion quality more easily with the power of a large amount of data, e.g., Web data.", "labels": [], "entities": []}, {"text": "However, language models and dictionaries which are generated automatically from the Web tend to be bigger than those of manually crafted rules, which makes it hard to execute IMEs on local computers.", "labels": [], "entities": []}, {"text": "The situation is the same in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7482609450817108}]}, {"text": "Language model compression is critical in statistical machine translation.", "labels": [], "entities": [{"text": "Language model compression", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6184078057607015}, {"text": "statistical machine translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6978850166002909}]}, {"text": "Several studies have been proposed in order to scale language model to large data.", "labels": [], "entities": []}, {"text": "Example includes class-based models, entropy-based pruning), Golomb Coding ( and randomized lossy compression.", "labels": [], "entities": []}, {"text": "The main focus of this research is how efficiently the language model, especially n-gram model, can be compressed.", "labels": [], "entities": []}, {"text": "However, in Japanese input methods, lexicon plays more important role in actual conversion than language model, since users don't always type Japanese text by sentence, but by phrase or even byword.", "labels": [], "entities": []}, {"text": "Lexicon coverage is one of the key factors with which to evaluate the overall usability of IMEs.", "labels": [], "entities": [{"text": "Lexicon coverage", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7533652484416962}]}, {"text": "In addition, modern Japanese input methods need to support a variety of features, including autocompletion and reconversion, which accelerate input speeds as well as help users to edit what they typed before.", "labels": [], "entities": [{"text": "autocompletion", "start_pos": 92, "end_pos": 106, "type": "METRIC", "confidence": 0.9588629603385925}]}, {"text": "It is non-trivial to implement a compact dictionary storage which supports these complicated use cases.", "labels": [], "entities": []}, {"text": "In this work, we propose novel lossless compression algorithms for both dictionary and language model based on succinct data structures.", "labels": [], "entities": []}, {"text": "Although the size of our dictionary storage approaches closer to the information-theoretic lower bound, it supports three lookup operations: common prefix lookup, predictive lookup and reverse lookup.", "labels": [], "entities": []}, {"text": "Proposed two data structures are used in our product \"Google Japanese Input\", and its open-source version \"Mozc\".", "labels": [], "entities": [{"text": "Google Japanese Input", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.9191267093022665}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example of dictionary entries", "labels": [], "entities": []}, {"text": " Table 3: summary of language model compression", "labels": [], "entities": [{"text": "language model compression", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6356958250204722}]}, {"text": " Table 4: Cache size and conversion speed", "labels": [], "entities": []}]}