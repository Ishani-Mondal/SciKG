{"title": [{"text": "Exploring User Satisfaction in a Tutorial Dialogue System", "labels": [], "entities": [{"text": "Exploring User Satisfaction in a Tutorial Dialogue", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6475548914500645}]}], "abstractContent": [{"text": "User satisfaction is a common evaluation metric in task-oriented dialogue systems, whereas tutorial dialogue systems are often evaluated in terms of student learning gain.", "labels": [], "entities": []}, {"text": "However, user satisfaction is also important for such systems, since it may predict technology acceptance.", "labels": [], "entities": []}, {"text": "We present a detailed satisfaction questionnaire used in evaluating the BEETLE II system (REVU-NL), and explore the underlying components of user satisfaction using factor analysis.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9759721159934998}]}, {"text": "We demonstrate interesting patterns of interaction between interpretation quality, satisfaction and the dialogue policy , highlighting the importance of more fine-grained evaluation of user satisfaction.", "labels": [], "entities": []}], "introductionContent": [{"text": "User satisfaction is one of the primary evaluation measures for task-oriented spoken dialogue systems (SDS): the goal of an SDS is to accomplish the task, and to keep the user satisfied, so that they will want to continue using the system.", "labels": [], "entities": [{"text": "task-oriented spoken dialogue systems (SDS)", "start_pos": 64, "end_pos": 107, "type": "TASK", "confidence": 0.7061229561056409}]}, {"text": "Typically, the PAR-ADISE methodology () is used to establish a performance function which relates user satisfaction measured through questionnaires to interaction parameters that can be derived from system logs.", "labels": [], "entities": []}, {"text": "This function can then be used to better understand which properties of the interaction have the most impact on the users, and to compare different system versions.", "labels": [], "entities": []}, {"text": "In contrast, tutorial dialogue systems are typically evaluated in terms of student learning gain, by comparing student scores on standardized tests before and after interacting with the system.", "labels": [], "entities": []}, {"text": "This is clearly an important evaluation metric, since it directly assesses the benefit students obtain from using the system.", "labels": [], "entities": []}, {"text": "However, it is also important to evaluate user satisfaction, since it can influence students' willingness to use computer tutors in along run.", "labels": [], "entities": []}, {"text": "Thus, recent studies have looked at factors that could influence user satisfaction in tutorial dialogue, such as different tutoring policies), quality of speech output (Forbes-), and students' prior attitudes towards technology ().", "labels": [], "entities": []}, {"text": "Assessing user satisfaction, however, is not a straightforward task.", "labels": [], "entities": [{"text": "Assessing user satisfaction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7251691222190857}]}, {"text": "As we discuss in more detail in Section 2, user satisfaction is known to be a complex multi-dimensional construct, composed of largely independent factors such as perceived ease of use and perceived usefulness.", "labels": [], "entities": []}, {"text": "Therefore, questionnaires used for assessing satisfaction need to be validated through user studies, and different satisfaction dimensions should be assessed independently.", "labels": [], "entities": []}, {"text": "Therefore, SDS researchers are now starting to use techniques from psychometrics for this purpose).", "labels": [], "entities": [{"text": "SDS", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9650810360908508}]}, {"text": "However, user satisfaction studies tutorial dialogue currently rely on simple questionnaires adapted from either task-oriented SDS or non-dialogue intelligent tutoring systems (; Forbes-, and these questionnaires have not been validated for tutorial dialogue systems.", "labels": [], "entities": []}, {"text": "In this paper, we make the first step towards developing a better user satisfaction questionnaire for tutorial dialogue systems.", "labels": [], "entities": []}, {"text": "We present a user satis-faction evaluation of the BEETLE II tutorial dialogue system.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.8385228514671326}]}, {"text": "Starting with a detailed user satisfaction questionnaire, we employ exploratory factor analysis to discover a set of dimensions for the students' satisfaction with a dialogue-based tutor.", "labels": [], "entities": []}, {"text": "We then use the factors we derived to compare user satisfaction between two versions of our computer tutor that use different policies for generating the tutor's feedback.", "labels": [], "entities": []}, {"text": "We investigate the relationships between the subjective satisfaction dimensions and the objective learning gain metric for the two systems.", "labels": [], "entities": []}, {"text": "Finally, we carryout a more detailed investigation of our prior results on the relationship between user satisfaction and interpretation quality in tutorial dialogue.", "labels": [], "entities": []}, {"text": "Our analysis also provides insights for further improving the questionnaire we developed and gives an example of how user satisfaction metrics developed for task-oriented dialogue can be adapted to different dialogue applications.", "labels": [], "entities": []}, {"text": "It also opens new questions about how different properties of the interaction affect user satisfaction in tutorial dialogue, which can be investigated in future work.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We discuss the approaches for assessing user satisfaction with SDS in Section 2.", "labels": [], "entities": [{"text": "SDS", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9131088852882385}]}, {"text": "In Section 3 we describe the BEETLE II tutorial dialogue system used in this evaluation.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9552287459373474}]}, {"text": "We describe our questionnaire design in Section 4, and describe its use in BEETLE II evaluation in Section 5.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.907818615436554}]}, {"text": "We conclude by discussing the implication of our analysis for tutorial dialogue system evaluation in Section 6.", "labels": [], "entities": [{"text": "tutorial dialogue system evaluation", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.7642958760261536}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Factors derived from the REVU-NL question- naire, with question loadings for the factor to which each  question was assigned. Question text shortened due to  space limitations, full text presented in the appendix.  Non-loading questions in parentheses.", "labels": [], "entities": [{"text": "REVU-NL question- naire", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.9150316715240479}]}]}