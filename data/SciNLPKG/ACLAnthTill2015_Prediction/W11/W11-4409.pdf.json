{"title": [{"text": "Open Source WFST tools for LVCSR cascade development", "labels": [], "entities": [{"text": "LVCSR cascade development", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.7933197220166525}]}], "abstractContent": [{"text": "This paper introduces the Transducersaurus toolkit which provides a set of classes for generating each of the fundamental components of atypical WFST-based ASR cascade, including HMM, Context-dependency, Lexicon, and Grammar transducers, as well as an optional silence class WFST.", "labels": [], "entities": [{"text": "WFST-based ASR cascade", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.46489547689755756}, {"text": "WFST", "start_pos": 275, "end_pos": 279, "type": "DATASET", "confidence": 0.964386522769928}]}, {"text": "The toolkit further implements a small scripting language in order to facilitate the construction of cascades via a variety of popular combination and optimization methods and provides integrated support for the T 3 and Juicer WFST decoders, and both Sphinx and HTK format acoustic models.", "labels": [], "entities": [{"text": "Juicer WFST decoders", "start_pos": 220, "end_pos": 240, "type": "DATASET", "confidence": 0.678430179754893}]}, {"text": "New results for two standard WSJ tasks are also provided, and the toolkit is used to compare a variety of construction and optimization algorithms.", "labels": [], "entities": [{"text": "WSJ tasks", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.7097185850143433}]}, {"text": "These results illustrate the flexibility of the toolkit as well as the tradeoffs of various build algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years the Weighted Finite-State Transducer (WFST) paradigm has gained considerable popularity as a platform for Automatic Speech Recognition (ASR).", "labels": [], "entities": [{"text": "Weighted Finite-State Transducer (WFST) paradigm", "start_pos": 20, "end_pos": 68, "type": "TASK", "confidence": 0.7119358948298863}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 122, "end_pos": 156, "type": "TASK", "confidence": 0.7813681314388911}]}, {"text": "The WFST approach provides an elegant, unified mathematical framework that can be utilized to train, generate, combine and optimize the many heterogenous knowledge sources that typically makeup a modern Large Vocabulary Continuous Speech Recognition (LVCSR) system.", "labels": [], "entities": [{"text": "Large Vocabulary Continuous Speech Recognition (LVCSR)", "start_pos": 203, "end_pos": 257, "type": "TASK", "confidence": 0.7547189220786095}]}, {"text": "This has lead to the development of several excellent general purpose software libraries devoted to the construction and manipulation of WFSTs, including the popular open source OpenFst C++ toolkit.", "labels": [], "entities": []}, {"text": "Much research has also been conducted on the theoretical construction, integration and optimization of WFST models for ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9759637117385864}]}, {"text": "Nevertheless to our knowledge at present there is no open source toolkit devoted to the construction of ASR-specific WFST models.", "labels": [], "entities": [{"text": "ASR-specific WFST", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7660222351551056}]}, {"text": "This lack of available tools represents an obstacle to the wider dissemination and adoption of WFSTbased methods.", "labels": [], "entities": []}, {"text": "In response to this, the current work introduces the), which aims to provide a unified, flexible and transparent approach to the construction of integrated WFST-based ASR cascades, while incorporating recent research results on this important topic.", "labels": [], "entities": [{"text": "WFST-based ASR cascades", "start_pos": 156, "end_pos": 179, "type": "TASK", "confidence": 0.6508082151412964}]}, {"text": "It includes a set of classes for constructing component models as well as a simple Domain Specific Language (DSL) suitable for specifying cascade integration and optimization commands.", "labels": [], "entities": []}, {"text": "It provides integrated support for HTK) and) acoustic models and cascade construction support for both the T 3 and Juicer () WFST decoders.", "labels": [], "entities": [{"text": "Juicer () WFST decoders", "start_pos": 115, "end_pos": 138, "type": "DATASET", "confidence": 0.6558214575052261}]}, {"text": "Where in past complicated development was required, with this toolkit input knowledge sources and a single command are sufficient to build a high-performance system.", "labels": [], "entities": []}, {"text": "In addition to introducing the toolkit, this work contributes new experimental results for two LVCSR tasks from the Wall Street Journal (Paul, 1992) (WSJ) corpus, and provides discussion of alternative cascade build chains.", "labels": [], "entities": [{"text": "Wall Street Journal (Paul, 1992) (WSJ) corpus", "start_pos": 116, "end_pos": 161, "type": "DATASET", "confidence": 0.9507427563269933}]}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the main component models of atypical WFST-based ASR cascade.", "labels": [], "entities": [{"text": "WFST-based ASR cascade", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6847742398579916}]}, {"text": "Section 3 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 describes the cascade integration tool and its capabilities.", "labels": [], "entities": [{"text": "cascade integration", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8535652458667755}]}, {"text": "Section 4 describes new experimental results that explore the flexibility of the Transducersaurus toolkit.", "labels": [], "entities": []}, {"text": "Section 5 provides additional analysis and explores the practical implications of various construction techniques.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed toolkit can be used to generate recognition networks fora variety of different tasks and inputs.", "labels": [], "entities": []}, {"text": "In order to showcase this flexibility, several different experiments were carried out making use of different build chains and two test sets from the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 150, "end_pos": 160, "type": "DATASET", "confidence": 0.9715269207954407}]}, {"text": "A selection of recent results are reported for HTK and Sphinx acoustic models and bot the Juicer and T 3 decoder.", "labels": [], "entities": [{"text": "HTK", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8444043397903442}, {"text": "Juicer and T 3 decoder", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.7837099313735962}]}, {"text": "These results illustrate the correctness of the toolkit in reproducing previous baselines, and also confirm separate results encouraging the SLA-based build chains.", "labels": [], "entities": []}, {"text": "All experiments for this work were performed on an 8 core Intel Xeon based machine running at 3GHz with a 6MB cache and 64GBs of main system memory running the RHEL OS.", "labels": [], "entities": [{"text": "RHEL OS", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.88609978556633}]}, {"text": "As with our previous results from, the experiments covered two popular tasks from the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.9706899225711823}]}, {"text": "The first task, nov92-5k, focuses on the November 1992 ARPA WSJ test set which comprises 330 sentences, and was evaluated using the WSJ 5k non-verbalized vocabulary and the standard WSJ 5k closed bigram language model.", "labels": [], "entities": [{"text": "ARPA WSJ test set", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.8153078258037567}]}, {"text": "The second task, si dt s2-20k, focuses on a subset of the WSJ1 Hub2 test set which comprises 207 sentences.", "labels": [], "entities": [{"text": "WSJ1 Hub2 test set", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.953779935836792}]}, {"text": "The si dt s2-20k task, which is somewhat more difficult, was evaluated using a 64k vocabulary an a large 3-gram LM trained on 222M words from the CSR LM-1 corpus.", "labels": [], "entities": [{"text": "CSR LM-1 corpus", "start_pos": 146, "end_pos": 161, "type": "DATASET", "confidence": 0.9373855789502462}]}, {"text": "In order to help ensure the repeatability of our experiments, open source Sphinx and HTK acoustic models described in were used throughout, and auxiliary parameter values for the T 3 and Juicer decoders were specified as in.", "labels": [], "entities": []}, {"text": "Unless otherwise specified the log semiring was used for all constructions.", "labels": [], "entities": []}, {"text": "The first set of experiments focused on the standard WSJ Nov92-5k test set, the default closed bigram language model and associated pronunciation lexicon.", "labels": [], "entities": [{"text": "WSJ Nov92-5k test set", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.9503418654203415}]}, {"text": "Open source Sphinx format acoustic models were used.", "labels": [], "entities": []}, {"text": "The toolkit was utilized to generate six different cascades, which shared the same fundamental knowledge sources but differed in terms of the optimization procedures applied, and whether an as well as our own recent experiments have shown that SLA composition, which omits the det(LG) operation, performs equally well, thus SLA composition was utilized in all six cascade constructions.", "labels": [], "entities": []}, {"text": "The command used to generate these cascades was specified as $ ./transducersaurus.py --tiedlist mdef --amtype sphinx --grammar bcb05cnp-2g.arpa --lexicon bcb05cnp.dic --convert t --base auto --prefix bcb05s --command \"(C * det(L)).G\" and the value of the --command parameter was simply modified to generate each of the six different variations.", "labels": [], "entities": []}, {"text": "The properties of each of the resulting cascades are described in detail in.", "labels": [], "entities": []}, {"text": "The variation in terms of the number of arcs, states and total size clearly indicates the relative effects of applying different optimization operations to the construction process.", "labels": [], "entities": []}, {"text": "The simplest construction, (C \u2022 det(L)).G results in the smallest cascade in this case.", "labels": [], "entities": []}, {"text": "Subsequent application of determinization increases the initial size of the cascade, while miniminization again reduces the overall size.", "labels": [], "entities": []}, {"text": "This pattern is repeated with the addition of the HMMlevel WFST.", "labels": [], "entities": [{"text": "HMMlevel WFST", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.7624369859695435}]}, {"text": "The set of Sphinx format cascades generated with the transducersaurus.py tool were subsequently evaluated inside of the T 3 decoder and the results of these evaluations are described in.", "labels": [], "entities": []}, {"text": "Although small in this simple task, the effect of optimization techniques can nonetheless still be clearly seen in the difference between the (C \u2022 det(L)).G construction and the determinized and minimized variants.", "labels": [], "entities": []}, {"text": "In general the impact of these optimizations is increased for larger and more complicated models.", "labels": [], "entities": []}, {"text": "Nevertheless the gains are not achieved without a cost.", "labels": [], "entities": []}, {"text": "In particular each additional call to the determinization and minimization algorithms consumes significant additional computing resources and time.", "labels": [], "entities": []}, {"text": "These requirements grow rapidly as the size and complexity of the input models increases.", "labels": [], "entities": []}, {"text": "Thus it is pragmatic to strike a balance between development time, resource requirements and achievable RTF versus WACC.", "labels": [], "entities": [{"text": "RTF", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.5957264304161072}, {"text": "WACC", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.8533985018730164}]}, {"text": "In order to help illustrate this trade-off we also looked at the memory consumption versus time characteristics of the det(LG) determinization operation, the standard C \u2022 (LG) composition, and the SLA composition, CL.G.", "labels": [], "entities": []}, {"text": "The results for the bcb05 cascade used to evaluate the Nov92-5k test set are depicted in, and unequivocally show that the determinization operation is by far the most costly, but also indicate the advantages of SLA composition over the standard variant.", "labels": [], "entities": [{"text": "Nov92-5k test set", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9847862124443054}]}, {"text": "The second set of experiments involved the si dt s2 test set, and a much larger 3-gram language model based on the CSR corpus.", "labels": [], "entities": [{"text": "si dt s2 test set", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7222301244735718}, {"text": "CSR corpus", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.9202789068222046}]}, {"text": "These experiments were carried out to show that the toolkit is a viable choice not just for small models, but can be used in a straightforward manner to also build very large, efficient cascades.", "labels": [], "entities": []}, {"text": "This experiment also illustrates the ability of the toolkit to generate both HTK, and Sphinx based recognition networks and to construct working cascades for both Juicer and the T 3 decoder.", "labels": [], "entities": []}, {"text": "In this case experiments focused on a single construction scheme; the simple yet effective (C \u2022 det(L)).G and the commands utilized to build the cascades are described in and informa- tion regarding arc and state counts as well as overall size is described in while RTF versus WACC results for the three tests are illustrated in.", "labels": [], "entities": [{"text": "G", "start_pos": 104, "end_pos": 105, "type": "METRIC", "confidence": 0.7958464622497559}]}], "tableCaptions": [{"text": " Table 2: WSJ-based WFST cascade characteristics for Sphinx acoustic models. Here min refers to minimization,  det refers to determinization, the \"\u2022\" operator refers to standard composition, and the \".\" operator refers to static  look-ahead composition.", "labels": [], "entities": [{"text": "WSJ-based WFST cascade", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.45336325963338214}]}]}