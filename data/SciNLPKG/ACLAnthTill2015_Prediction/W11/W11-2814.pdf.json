{"title": [{"text": "Combining Hierarchical Reinforcement Learning and Bayesian Networks for Natural Language Generation in Situated Dialogue", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.740069568157196}]}], "abstractContent": [{"text": "Language generators in situated domains face a number of content selection, utterance planning and surface realisation decisions, which can be strictly interdependent.", "labels": [], "entities": [{"text": "utterance planning", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7934216558933258}]}, {"text": "We therefore propose to optimise these processes in a joint fashion using Hierarchical Reinforcement Learning.", "labels": [], "entities": []}, {"text": "To this end, we induce a reward function for content selection and utterance planning from data using the PARADISE framework, and suggest a novel method for inducing a reward function for surface reali-sation from corpora.", "labels": [], "entities": [{"text": "content selection and utterance planning", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.7013611793518066}]}, {"text": "It is based on generation spaces represented as Bayesian Networks.", "labels": [], "entities": []}, {"text": "Results in terms of task success and human-likeness suggest that our unified approach performs better than a baseline optimised in isolation or a greedy or random baseline.", "labels": [], "entities": []}, {"text": "It receives human ratings close to human authors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) systems that work in situated domains and need to generate utterances during an interaction are faced with a number of challenges.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7639273852109909}]}, {"text": "They need to adapt their decisions to a continuously changing interaction history and spatial context as well as to the user's properties, such as their individual information needs and verbal or nonverbal responses to each generated utterance.", "labels": [], "entities": []}, {"text": "Decisions involve the tasks of content selection, utterance planning and surface realisation, which can be in many ways related and interdependent.", "labels": [], "entities": [{"text": "content selection", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7557723224163055}, {"text": "utterance planning", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.936268150806427}, {"text": "surface realisation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7641981542110443}]}, {"text": "For the former two tasks, e.g., there is a tradeoff between how much information to include in an utterance (to increase task success), and how much a user can actually comprehend online.", "labels": [], "entities": []}, {"text": "With regard to surface realisation, decisions are often made according to a language model of the domain.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7582200169563293}]}, {"text": "However, there are other linguistic phenomena, such as alignment (), consistency (, and variation, which influence people's assessment of discourse and generated output).", "labels": [], "entities": [{"text": "consistency", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9881551265716553}]}, {"text": "We therefore argue that it is important to optimise content selection, utterance planning and surface realisation in a unified fashion, and we suggest to use Hierarchical Reinforcement Learning (HRL) with Bayesian networks to achieve this.", "labels": [], "entities": [{"text": "utterance planning", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8655577898025513}]}, {"text": "Reinforcement learning (RL) is an attractive framework for optimising NLG systems, where situations are mapped to actions by maximising along term reward signal (.", "labels": [], "entities": [{"text": "Reinforcement learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8680065035820007}]}, {"text": "HRL has the additional advantage of scaling to large search spaces . Since an HRL agent will ultimately learn the behaviour it is rewarded for, the reward function is arguably the agent's most crucial component.", "labels": [], "entities": []}, {"text": "Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework ().", "labels": [], "entities": [{"text": "PARADISE framework", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.6596235781908035}]}, {"text": "We will use this framework to induce a reward function for content selection and utterance planning.", "labels": [], "entities": [{"text": "content selection", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7280891239643097}, {"text": "utterance planning", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8623971939086914}]}, {"text": "However, since PARADISE relies heavily on task success metrics, it is not ideally suited for surface realisation, which depends more on linguistic phenomena like frequency, consistency and variation.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7313727289438248}, {"text": "consistency", "start_pos": 173, "end_pos": 184, "type": "METRIC", "confidence": 0.9570348858833313}]}, {"text": "Linguistic and psycho-logical studies (cited above) show that such phenomena are worth modelling in an NLG system.", "labels": [], "entities": []}, {"text": "The contribution of this paper is therefore to induce a reward function from human data, specifically suited for surface generation.", "labels": [], "entities": [{"text": "surface generation", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7205030918121338}]}, {"text": "We obtain Bayesian Networks (BNs) from a human corpus and use them to inform the agent's learning process.", "labels": [], "entities": []}, {"text": "We compare their performance against a greedy and a random baseline.", "labels": [], "entities": []}, {"text": "In addition, we suggest to optimise content selection, utterance planning and surface realisation decisions in a joint, rather than isolated, fashion in order to correspond to their interrelated nature.", "labels": [], "entities": [{"text": "utterance planning", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8643178939819336}]}, {"text": "Results in terms of task success and human-likeness show that our combined approach performs better than baselines that were optimised in isolation or act on behalf of the language model alone.", "labels": [], "entities": []}, {"text": "Since generation spaces in our approach can be obtained for any domain for which corpus data is available, it generalises to different domains with limited effort and reduced development time.", "labels": [], "entities": []}], "datasetContent": [{"text": "To get a more reliable idea of the quality and human acceptance of our instructions, we asked 12 participants to rate 96 sets of instructions.", "labels": [], "entities": []}, {"text": "Each set contained a spatial graphical scene with a person, mapped with one human, one jointly learnt, and one instruction learnt in isolation.", "labels": [], "entities": []}, {"text": "Participants were asked to rate navigation instructions to an object, e.g. 'go left and press the yellow button', on a 1-5 Likert scale (where 5 is the best) for their helpfulness on guiding the displayed person to the refer-13 7 female, 5 male with an age average of 25.6. ent.", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.970319539308548}]}, {"text": "Scenes were presented in a random order.", "labels": [], "entities": []}, {"text": "We then asked the participants to circle the object they thought was the intended referent.", "labels": [], "entities": []}, {"text": "Human instructions were rated with a mean of 3.86 (with a standard deviation (SD) of 0.89).", "labels": [], "entities": [{"text": "standard deviation (SD)", "start_pos": 58, "end_pos": 81, "type": "METRIC", "confidence": 0.9387981414794921}]}, {"text": "The jointly learnt instructions were rated with a mean of 3.57 (SD=1.07) and instructions learnt in isolation with a mean of 2.35 (SD=0.85).", "labels": [], "entities": []}, {"text": "The difference between human and jointly learnt is not significant (p < 0.29) according to a t-test.", "labels": [], "entities": []}, {"text": "The effect sizer is 0.14.", "labels": [], "entities": [{"text": "effect sizer", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9129971265792847}]}, {"text": "The difference between human and learnt in isolation is significant at p < 0.001 with an effect sizer of 0.65 and the difference between jointly learnt and learnt in isolation is significant at p < 0.003 and has an effect sizer of 0.53.", "labels": [], "entities": []}, {"text": "Users were able to identify the intended referent in 96% of all cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of generation behaviours with  Precision-Recall and KL-divergence.", "labels": [], "entities": [{"text": "Precision-Recall", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.9889772534370422}, {"text": "KL-divergence", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.774855375289917}]}, {"text": " Table 2: Sample dialogue for the jointly learnt policy. See Section 5.1 for corresponding policies and actions. The  agent starts using a high level navigation strategy. When the user gets confused, it temporarily switches back to low  level; nonverbal behaviour is given in square brackets.", "labels": [], "entities": []}]}