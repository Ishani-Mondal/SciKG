{"title": [{"text": "Dialectal to Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation", "labels": [], "entities": [{"text": "Dialectal to Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation", "start_pos": 0, "end_pos": 99, "type": "TASK", "confidence": 0.5556310496547006}]}], "abstractContent": [{"text": "This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge.", "labels": [], "entities": [{"text": "Arabic-English statistical machine translation (SMT)", "start_pos": 45, "end_pos": 97, "type": "TASK", "confidence": 0.7721273899078369}]}, {"text": "We present a lightweight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words.", "labels": [], "entities": []}, {"text": "Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system.", "labels": [], "entities": [{"text": "MSA analyzer", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.8327098190784454}, {"text": "SMT", "start_pos": 198, "end_pos": 201, "type": "TASK", "confidence": 0.8889374732971191}]}, {"text": "This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9988652467727661}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9690919518470764}]}, {"text": "A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.6581912040710449}]}], "introductionContent": [{"text": "Much work has been done on Modern Standard Arabic (MSA) natural language processing (NLP) and machine translation (MT).", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA) natural language processing (NLP)", "start_pos": 27, "end_pos": 89, "type": "TASK", "confidence": 0.622775599360466}, {"text": "machine translation (MT)", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8509311556816102}]}, {"text": "In comparison, research on dialectal Arabic (DA), the unstandardized spoken varieties of Arabic, is still lacking in NLP in general and MT in particular.", "labels": [], "entities": [{"text": "dialectal Arabic (DA)", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.6529707074165344}, {"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.8216666579246521}]}, {"text": "In this paper we address the issue of MT out-of-vocabulary (OOV) terms and low frequency terms in highly dialectal Arabic text.", "labels": [], "entities": [{"text": "MT out-of-vocabulary (OOV) terms", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7678079207738241}]}, {"text": "We present a light-weight rule-based approach to producing MSA morphological paraphrases of DA OOV words and low frequency words.", "labels": [], "entities": []}, {"text": "However, we don't do lexical translation.", "labels": [], "entities": [{"text": "lexical translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7058216631412506}]}, {"text": "Our approach extends an existing MSA analyzer to two DA varieties (Levantine and Egyptian) with less than 40 morphological clitics.", "labels": [], "entities": [{"text": "MSA analyzer", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.8363174498081207}]}, {"text": "We use 11 morphological transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based statistical MT (SMT) system.", "labels": [], "entities": [{"text": "phrase-based statistical MT (SMT)", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.6894814719756445}]}, {"text": "Our system improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9986951947212219}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9675581455230713}]}, {"text": "A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.6581912040710449}]}, {"text": "The rest of this paper is structured as follows: Section 2 is related work, Section 3 presents linguistic challenges and motivation, Section 4 details our approach and Section 5 presents results evaluating our approach under a variety of conditions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the open-source Moses toolkit () to build two phrase-based SMT systems trained on two different data conditions: a mediumscale MSA-only system trained using a newswire (MSA-English) parallel text with 12M words on the Arabic side (LDC2007E103) and a large-scale MSA/DA-mixed system (64M words on the Arabic side) trained using several LDC corpora including some limited DA data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8219661116600037}]}, {"text": "Both systems use a standard phrase-based architecture.", "labels": [], "entities": []}, {"text": "The parallel corpus is word-aligned using GIZA++.", "labels": [], "entities": []}, {"text": "This is only done on the baseline systems.", "labels": [], "entities": []}, {"text": "For all systems, the English data is tokenized using simple punctuation-based rules.", "labels": [], "entities": []}, {"text": "The Arabic side is segmented according to the Arabic Treebank tokenization scheme () using the MADA+TOKAN morphological analyzer and tokenizer () -v3.1 (.", "labels": [], "entities": [{"text": "Arabic Treebank tokenization", "start_pos": 46, "end_pos": 74, "type": "DATASET", "confidence": 0.7305654088656107}]}, {"text": "The Arabic text is also Alif/Ya normalized.", "labels": [], "entities": []}, {"text": "MADA-produced Arabic lemmas are used for word alignment.", "labels": [], "entities": [{"text": "MADA-produced Arabic lemmas", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.8366814653078715}, {"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8108715713024139}]}, {"text": "Results are presented in terms of BLEU (), NIST) and ME-TEOR () metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9988170862197876}, {"text": "NIST", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.6417766213417053}, {"text": "ME-TEOR", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9502705931663513}]}, {"text": "However, all optimizations were done against the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9935206770896912}]}, {"text": "All evaluation results are case insensitive.", "labels": [], "entities": []}, {"text": "All of the systems we present use the lattice input format to, including the baselines which do not need them.", "labels": [], "entities": []}, {"text": "We do not report on the non-lattice baselines, but in initial experiments we conducted, they did not perform as well as the degenerate lattice version.", "labels": [], "entities": []}, {"text": "The Devtest Set Our devtest set consists of sentences containing at least one non-MSA segment (as annotated by LDC) in the Dev10 audio development data under the DARPA GALE program.", "labels": [], "entities": [{"text": "Dev10 audio development data", "start_pos": 123, "end_pos": 151, "type": "DATASET", "confidence": 0.7778486460447311}]}, {"text": "The data contains broadcast conversational (BC) segments (with three reference translations), and broadcast news (BN) segments (with only one reference, replicated three times).", "labels": [], "entities": []}, {"text": "The data set contained a mix of Arabic dialects, with Levantine Arabic being the most common variety.", "labels": [], "entities": []}, {"text": "The particular nature of the devtest being transcripts of audio data adds some challenges to MT systems trained on primarily written data in news genre.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9886206984519958}]}, {"text": "For instance, each of the source and references in the devtest set contained over 2,600 uh-like speech effect words (uh/ah/oh/eh), while the baseline translation system we used only generated 395.", "labels": [], "entities": []}, {"text": "This led to severe brevity penalty by the BLEU metric.", "labels": [], "entities": [{"text": "brevity", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9568677544593811}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9950998425483704}]}, {"text": "As such, we removed all of these speech effect words in the source, references and our MT system output.", "labels": [], "entities": [{"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.8272393345832825}]}, {"text": "Another similar issue was the overwhelming presence of commas in the English reference compared to the Arabic source: each reference had about 14,200 commas, while the source had only 64 commas.", "labels": [], "entities": []}, {"text": "Our MT system baseline predicted commas in less than half of the reference cases.", "labels": [], "entities": [{"text": "MT system baseline", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8415599266688029}]}, {"text": "Similarly we remove commas from the source, references, and MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.7275469303131104}]}, {"text": "We do this to all the systems we compare in this paper.", "labels": [], "entities": []}, {"text": "However, even with all of this preprocessing, the length penalty was around 0.95 on average in the large system and around 0.85 on average in the medium system.", "labels": [], "entities": [{"text": "length penalty", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9853094816207886}]}, {"text": "As such, we report additional BLEU sub-scores, namely the unigram and bigram precisions (Prec-1 and Prec-2, respectively), to provide additional understanding of the nature of our improvements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.998065173625946}, {"text": "bigram precisions", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.8236580491065979}]}, {"text": "We split this devtest set into two sets: a development set (dev) and a blind test set (test).", "labels": [], "entities": []}, {"text": "We report all our analyses and experiments on the dev set and reserve the test set for best parameter runs at the end of this section.", "labels": [], "entities": []}, {"text": "The splitting is done randomly at the document level.", "labels": [], "entities": []}, {"text": "The dev set has 1,496 sentences with 32,047 untokenized Arabic words.", "labels": [], "entities": []}, {"text": "The test set has 1,568 sentences with 32,492 untokenized Arabic words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the dev set under large and medium training conditions. The baseline is compared to using  dialectal morphological analysis only and analysis plus transfer to MSA. BLEU and METEOR scores are presented  as percentages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9975801706314087}, {"text": "METEOR", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9673558473587036}]}, {"text": " Table 2: Results for the dev set under large training condition, varying the set of words selected for MSA paraphrasing.", "labels": [], "entities": [{"text": "MSA paraphrasing", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.9248867630958557}]}, {"text": " Table 3: Results for the blind test set under large training condition, comparing our best performing settings.", "labels": [], "entities": []}]}