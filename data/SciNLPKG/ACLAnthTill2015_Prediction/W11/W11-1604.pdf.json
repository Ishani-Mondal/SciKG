{"title": [{"text": "Comparing Phrase-based and Syntax-based Paraphrase Generation Erwin Marsi NTNU Sem Saelandsvei 7-9 NO-7491 Trondheim Norway", "labels": [], "entities": [{"text": "NTNU Sem Saelandsvei 7-9 NO-7491 Trondheim Norway", "start_pos": 74, "end_pos": 123, "type": "DATASET", "confidence": 0.8807624663625445}]}], "abstractContent": [{"text": "Paraphrase generation can be regarded as machine translation where source and target language are the same.", "labels": [], "entities": [{"text": "Paraphrase generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9519049227237701}, {"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7232424169778824}]}, {"text": "We use the Moses statistical machine translation toolkit for paraphrasing , comparing phrase-based to syntax-based approaches.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6700248320897421}]}, {"text": "Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch.", "labels": [], "entities": []}, {"text": "Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the challenging properties of natural language is that the same semantic content can typically be expressed by many different surface forms.", "labels": [], "entities": []}, {"text": "As the ability to deal with paraphrases holds great potential for improving the coverage of NLP systems, a substantial body of research addressing recognition, extraction and generation of paraphrases has emerged (.", "labels": [], "entities": [{"text": "recognition, extraction and generation of paraphrases", "start_pos": 147, "end_pos": 200, "type": "TASK", "confidence": 0.7843024049486432}]}, {"text": "Paraphrase Generation can be regarded as a translation task in which source and target language are the same.", "labels": [], "entities": [{"text": "Paraphrase Generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9581379890441895}]}, {"text": "Both Paraphrase Generation and Machine Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions.", "labels": [], "entities": [{"text": "Paraphrase Generation", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.9284253120422363}, {"text": "Machine Translation (MT)", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8561961054801941}, {"text": "Text-To-Text Generation", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7376180589199066}]}, {"text": "Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text.", "labels": [], "entities": []}, {"text": "Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree.", "labels": [], "entities": [{"text": "Paraphrase Generation", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.916841596364975}]}, {"text": "The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation.", "labels": [], "entities": [{"text": "Paraphrase Generation", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8989202380180359}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.8955681920051575}, {"text": "Paraphrase Generation", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.9498848021030426}]}, {"text": "One popular approach -arguably the most successful so far -is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (;.", "labels": [], "entities": [{"text": "Statistical Phrase-based Machine Translation (PBMT)", "start_pos": 62, "end_pos": 113, "type": "TASK", "confidence": 0.7468490515436444}, {"text": "phrase translation rules from aligned bilingual text corpora", "start_pos": 128, "end_pos": 188, "type": "TASK", "confidence": 0.8220763765275478}]}, {"text": "Prior work has explored the use of PBMT for paraphrase generation However, since many researchers believe that PBMT has reached a performance ceiling, ongoing research looks into more structural approaches to statistical MT (.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8941434919834137}, {"text": "MT", "start_pos": 221, "end_pos": 223, "type": "TASK", "confidence": 0.832345187664032}]}, {"text": "Syntaxbased MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.", "labels": [], "entities": [{"text": "Syntaxbased MT", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.5957548320293427}]}, {"text": "Syntactic information might lead to better results in the area of grammatical well-formedness, and unlike phrasebased MT that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns.", "labels": [], "entities": []}, {"text": "While the verdict on whether or not this approach leads to any significant performance gain is still out, a similar line of reasoning would suggest that syntax-based paraphrasing may offer similar advantages over phrase-based paraphrasing.", "labels": [], "entities": []}, {"text": "Considering the fact that the success of PBMT can partly be attributed to the abundance of large parallel corpora, and that sufficiently large parallel corpora are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 182, "end_pos": 203, "type": "TASK", "confidence": 0.9239876866340637}, {"text": "paraphrase generation", "start_pos": 276, "end_pos": 297, "type": "TASK", "confidence": 0.963268518447876}]}, {"text": "At the same time, automatic syntactic analysis introduces errors in the parse trees, as no syntactic parser is perfect.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6950438022613525}]}, {"text": "Likewise, automatic alignment of syntactic phrases maybe prone to errors.", "labels": [], "entities": [{"text": "automatic alignment of syntactic phrases", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.7356512010097503}]}, {"text": "The main contribution of this paper is a systematic comparison between phrase-based and syntax-based paraphrase generation using an off-the-shelf statistical machine translation (SMT) decoder, namely Moses () and the word-alignment tool GIZA++ . Training data derives from anew, large scale (2.1M tokens) paraphrase corpus for Dutch, which has been recently released.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.6924265623092651}, {"text": "statistical machine translation (SMT) decoder", "start_pos": 146, "end_pos": 191, "type": "TASK", "confidence": 0.840946478503091}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the paraphrase corpus from which provides training and test data.", "labels": [], "entities": []}, {"text": "Next, Section 3 describes the paraphrase generation methods and the experimental setup.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.9250178933143616}]}, {"text": "Results are presented in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss our findings and formulate our conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Properties of the manually aligned corpus", "labels": [], "entities": []}]}