{"title": [{"text": "Efficient Matrix-Encoded Grammars and Low Latency Parallelization Strategies for CYK", "labels": [], "entities": [{"text": "CYK", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.6938352584838867}]}], "abstractContent": [{"text": "We present a matrix encoding of context-free grammars, motivated by hardware-level efficiency considerations.", "labels": [], "entities": []}, {"text": "We find efficiency gains of 2.5-9\u00d7 for exhaustive inference and approximately 2\u00d7 for pruned inference, resulting in high-accuracy parsing at over 20 sentences per second.", "labels": [], "entities": []}, {"text": "Our grammar encoding allows fine-grained par-allelism during chart cell population; we present a controlled study of several methods of parallel parsing, and find near-optimal latency reductions as core-count increases .", "labels": [], "entities": [{"text": "parallel parsing", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.6551894247531891}]}], "introductionContent": [{"text": "Constituent parsers are important fora number of information extraction subtasks -e.g., anaphora and coreference resolution and semantic role labeling -and parsing time is often a bottleneck for such applications).", "labels": [], "entities": [{"text": "information extraction subtasks", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7865185538927714}, {"text": "coreference resolution", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.8293018937110901}, {"text": "semantic role labeling", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.6215037306149801}]}, {"text": "Most constituent parsers leverage the dynamic programming \"chart\" structure of the CYK algorithm, even when performing approximate inference.", "labels": [], "entities": []}, {"text": "The inner loop of the CYK algorithm computes an argmax for each constituent span by intersecting the set of observed child categories spanning adjacent substrings with the set of rule productions in the grammar.", "labels": [], "entities": []}, {"text": "This 'grammar intersection' operation is the most computationally intensive component of the algorithm.", "labels": [], "entities": []}, {"text": "Prior work has shown that the grammar encoding can greatly affect parsing efficiency (c.f,,); in this paper we present a matrix encoding that can encode very large grammars to maximize inference efficiency.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.957313597202301}]}, {"text": "This matrix grammar encoding allows a refactoring of the CYK algorithm with two beneficial properties: 1) the number of expensive grammar intersection operations is reduced from O(n 3 ) to O(n 2 ); and 2) since grammar intersection is reduced to a set of matrix operations, the resulting algorithm is amenable to fine-grained parallelization.", "labels": [], "entities": [{"text": "grammar intersection", "start_pos": 211, "end_pos": 231, "type": "TASK", "confidence": 0.7193360328674316}]}, {"text": "Most discussion of parallel parsing concentrates on throughput, the aggregate number of sentences parsed per second on a particular machine.", "labels": [], "entities": [{"text": "parallel parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7054202854633331}]}, {"text": "In this study, we are also interested in applications for which response time is of interest (e.g., realtime speech recognition and machine translation), and thus consider latency, the time to parse a single sentence, as a primary objective.", "labels": [], "entities": [{"text": "realtime speech recognition", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6165355642636617}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7714391946792603}, {"text": "latency", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.945175051689148}]}, {"text": "Given ideally efficient algorithms and hardware, there would be no tradeoff between the two -that is, we would be able to parallelize each sentence across an arbitrary number of processor cores, reducing latency and increasing throughput linearly with core-count.", "labels": [], "entities": []}, {"text": "Unfortunately, hardware constraints and Amdahl's law ensure that we will never achieve that ideal speedup; in practice, we are likely to see some tradeoff.", "labels": [], "entities": []}, {"text": "We will demonstrate interesting patterns of the tradeoff between throughput and latency with various parallelization methods, allowing consumers to tailor parsing strategies to particular application requirements.", "labels": [], "entities": []}, {"text": "Our grammar intersection method is amenable to graphics processors (GPUs) and similar massively-parallel architectures.", "labels": [], "entities": [{"text": "grammar intersection", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7094537615776062}]}, {"text": "In this work, we perform our analysis on a multicore CPU system.", "labels": [], "entities": []}, {"text": "We demonstrate the utility of this approach using a number of different grammars, including the latent variable grammar used by the Berkeley parser ().", "labels": [], "entities": []}, {"text": "We show large speedups compared to a traditional CYK implementation for serial inference, parsing over 20 sentences per second with the Berkeley grammar.", "labels": [], "entities": []}, {"text": "Parallelizing this algorithm reduces average latency to .026 seconds.", "labels": [], "entities": [{"text": "latency", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.865135133266449}]}, {"text": "The remainder of this paper is organized as follows: we begin in Section 2 with background on the CYK algorithm and various general and CYKspecific parallelization considerations.", "labels": [], "entities": []}, {"text": "In Sec-tion 3 we provide a detailed presentation of our grammar encoding, data structures, and intersection method.", "labels": [], "entities": []}, {"text": "In Section 4, we demonstrate their effectiveness on a single core and present controlled experiments comparing several parallelization strategies.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare exhaustive and pruned parsing efficiency with several other competitive parsing implementations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9388028383255005}]}, {"text": "We performed all matrix-encoded parsing and parallelization experiments using the open-source BUBS parser Berkeley parser (), and with the aforementioned 'Adaptive Beam Search' system implemented in BUBS.", "labels": [], "entities": [{"text": "BUBS parser Berkeley parser", "start_pos": 94, "end_pos": 121, "type": "DATASET", "confidence": 0.8364734500646591}, {"text": "BUBS", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.98451828956604}]}, {"text": "The Charniak parser is written in C and parses with a lexicalized grammar.", "labels": [], "entities": []}, {"text": "The BUBS and Berkeley parsers are implemented in Java and parse with a latent-variable grammar.", "labels": [], "entities": [{"text": "BUBS", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9678571820259094}]}, {"text": "A brief note about implementation choices is appropriate here.", "labels": [], "entities": []}, {"text": "Java has often been viewed as notoriously slow, a perception well-established when Java runtime environments were interpreted rather than compiled.", "labels": [], "entities": []}, {"text": "Recent advances in virtual machine technology have largely eliminated the differential between Java and statically-compiled languages such as Fortran and C (c.f.", "labels": [], "entities": []}, {"text": "We performed all trials on a 12-core Linux machine (2 \u00d7 Intel \u00ae Xeon X5650 CPUs).", "labels": [], "entities": []}, {"text": "Each core can execute 2 simultaneous threads, fora total of 24 concurrent threads.", "labels": [], "entities": []}, {"text": "For the parsers implemented in Java, we used the Oracle 1.6.0 26 Virtual Machine.", "labels": [], "entities": [{"text": "Oracle 1.6.0 26 Virtual Machine", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.6465824782848358}]}], "tableCaptions": [{"text": " Table 1: Exhaustive Viterbi parse times (average seconds/sentence, lower is better) over WSJ Section 22 for", "labels": [], "entities": [{"text": "Exhaustive Viterbi parse", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.625203400850296}, {"text": "WSJ Section 22", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.9231882095336914}]}, {"text": " Table 2: Pruned parse times over WSJ Section 22 (av-", "labels": [], "entities": [{"text": "Pruned parse times", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8527485529581705}, {"text": "WSJ Section 22", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9064181447029114}]}]}