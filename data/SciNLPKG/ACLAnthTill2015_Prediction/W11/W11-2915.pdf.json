{"title": [{"text": "Minimally Supervised Domain-Adaptive Parse Reranking for Relation Extraction", "labels": [], "entities": [{"text": "Minimally Supervised Domain-Adaptive Parse Reranking", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.5777863621711731}, {"text": "Relation Extraction", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8963425159454346}]}], "abstractContent": [{"text": "The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE).", "labels": [], "entities": [{"text": "information extraction framework", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.8122058510780334}, {"text": "relation extraction (RE)", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.845771849155426}]}, {"text": "For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted head-driven phrase structure grammar (HPSG) for English.", "labels": [], "entities": []}, {"text": "The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank.", "labels": [], "entities": [{"text": "HPSG treebank", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.9640955626964569}]}, {"text": "It will be shown how the estimated confidence of RE rules learned from then best parses can be exploited for parse reranking.", "labels": [], "entities": [{"text": "parse reranking", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.9162766635417938}]}, {"text": "The acquired rerank-ing model improves the performance of RE in both training and test phases with the new first parses.", "labels": [], "entities": [{"text": "RE", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.6924893856048584}]}, {"text": "The obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9994561076164246}, {"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9637270569801331}, {"text": "RE task", "start_pos": 179, "end_pos": 186, "type": "TASK", "confidence": 0.9101259112358093}]}, {"text": "Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.793117880821228}, {"text": "instance extraction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7236290276050568}, {"text": "generic parsing", "start_pos": 153, "end_pos": 168, "type": "TASK", "confidence": 0.7910079658031464}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9297839999198914}]}, {"text": "The novel method for task-specific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task.", "labels": [], "entities": [{"text": "parse reranking", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8309890329837799}, {"text": "RE task", "start_pos": 144, "end_pos": 151, "type": "TASK", "confidence": 0.8784936368465424}]}], "introductionContent": [{"text": "Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g.,).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7981388568878174}, {"text": "information extraction (IE)", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.8506435453891754}]}, {"text": "The largest challenge is to develop methods that exploit domain knowledge with minimal human effort.", "labels": [], "entities": []}, {"text": "Many IE systems benefit from combining generic NLP components with task-specific extraction methods.", "labels": [], "entities": [{"text": "IE", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.979056179523468}]}, {"text": "Various machine learning approaches have been employed for adapting the IE methods to new domains and extraction tasks (e.g.,).", "labels": [], "entities": [{"text": "IE", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9647589325904846}]}, {"text": "The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances (.", "labels": [], "entities": []}, {"text": "Since the minimally supervised learning starts its bootstrapping from a few semantic examples, no treebanking or any other annotation is required for new domains.", "labels": [], "entities": []}, {"text": "In addition to this inherently domain-adaptable rule-learning component, the framework also employs two language analysis modules: a namedentity (NE) recognizer () and a parser.", "labels": [], "entities": []}, {"text": "NE recognizers are adapted to new domains-if needed-by adding rules for new NE types and extending the gazetteers.", "labels": [], "entities": [{"text": "NE recognizers", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7762344777584076}]}, {"text": "The employed generic data-driven dependency parsers or deeplinguistic handcrafted have not yet been adapted to IE domains and tasks.", "labels": [], "entities": [{"text": "IE domains", "start_pos": 111, "end_pos": 121, "type": "TASK", "confidence": 0.8955360352993011}]}, {"text": "The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself.", "labels": [], "entities": [{"text": "relation extraction (RE) task", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.8466723958651224}]}, {"text": "For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG grammar for English (ERG)).", "labels": [], "entities": []}, {"text": "The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component (, which had been trained on a generic HPSG treebank).", "labels": [], "entities": [{"text": "HPSG treebank", "start_pos": 148, "end_pos": 161, "type": "DATASET", "confidence": 0.9485259652137756}]}, {"text": "The parse ranking had attracted our attention because the first RE tests with the handcrafted grammar revealed recall problems even for the parsable relation mentions.", "labels": [], "entities": [{"text": "RE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.5833873748779297}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9980347752571106}]}, {"text": "Our suspicion to partially blame the generic parse selection was confirmed by our experiments.", "labels": [], "entities": [{"text": "generic parse selection", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8274720112482706}]}, {"text": "In this paper we will show how the estimated confidence of rules learned from then best parses can be exploited for task-specific parse reranking.", "labels": [], "entities": [{"text": "task-specific parse reranking", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.6481525301933289}]}, {"text": "The acquired reranking model improves the performance of RE both in training and test phases.", "labels": [], "entities": [{"text": "RE", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.8656288385391235}]}, {"text": "The task-driven reranking leads to significantly better RE recall by boosting readings that are better suited for RE rule extraction and rule application.", "labels": [], "entities": [{"text": "RE", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.5865423679351807}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.8325462937355042}, {"text": "RE rule extraction", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8913310170173645}, {"text": "rule application", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.8247211277484894}]}, {"text": "The beneficial reranking does not improve the quality of parsing measured by taskindependent performance criteria, not even for the IE domain.", "labels": [], "entities": []}, {"text": "The validation of the adapted parser using a hand-checked HPSG treebank of indomain texts rather shows a deterioration of parsing accuracy.", "labels": [], "entities": [{"text": "HPSG treebank of indomain texts", "start_pos": 58, "end_pos": 89, "type": "DATASET", "confidence": 0.9379348039627076}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9417637586593628}]}, {"text": "But often the incorrect parses selected over less faulty parses support the correct detection of instance mentions.", "labels": [], "entities": []}, {"text": "The novel method for task-specific parse reranking does not require any annotated data beyond the semantic seed, needed anyway for the RE task.", "labels": [], "entities": [{"text": "parse reranking", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8152483999729156}, {"text": "RE task", "start_pos": 135, "end_pos": 142, "type": "TASK", "confidence": 0.8794390559196472}]}, {"text": "Thus it does not require a domain-specific treebank.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the grammar and the associated parse selection model.", "labels": [], "entities": []}, {"text": "Section 3 introduces the RE framework.", "labels": [], "entities": [{"text": "RE", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.6965919137001038}]}, {"text": "Section 4 explains the new task/domainoriented reranking approach.", "labels": [], "entities": []}, {"text": "Section 5 presents the experiments and evaluations.", "labels": [], "entities": []}, {"text": "Special emphasis is placed on the role of reranking for the performance of the RE system.", "labels": [], "entities": [{"text": "RE", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.8014296293258667}]}, {"text": "Section 6 discusses related work.", "labels": [], "entities": []}, {"text": "Finally, Section 7 summarizes the results and suggests directions for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data For several reasons we decided to conduct our experiments on the Nobel Prize award corpus used also in ( Moreover, the availability of aversion of the corpus in which all relation mentions are labelled and a treebank fora subset of the corpus have greatly facilitated the evaluation.", "labels": [], "entities": [{"text": "Nobel Prize award corpus", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.7746120691299438}]}, {"text": "The target relation is prize-awarding event, namely, a relation among four arguments: WIN-NER, PRIZE NAME, PRIZE AREA and YEAR.", "labels": [], "entities": [{"text": "PRIZE", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.4848129451274872}, {"text": "NAME", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.48594826459884644}, {"text": "PRIZE", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.5343378186225891}, {"text": "AREA", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9408349990844727}, {"text": "YEAR", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9616944789886475}]}, {"text": "We take the same seed example as utilized in (, namely, the 1999 Nobel Chemistry winner Ahmed H Zewail in our experiments 2 . The seed looks like an database recond: The corpus contains 2864 documents from BBC, CNN and NYT, together 143289 sentences.", "labels": [], "entities": [{"text": "NYT", "start_pos": 219, "end_pos": 222, "type": "DATASET", "confidence": 0.9580115675926208}]}, {"text": "ERG covers around 70% sentences of the total corpus.", "labels": [], "entities": []}, {"text": "For our experiments we randomly divide the parsable corpus into two parts: training and test corpus, each containing the same number of sentences.", "labels": [], "entities": []}, {"text": "The average sentence length of the total corpus is around 20 words.", "labels": [], "entities": []}, {"text": "If we look at the domain relevant sentences, namely, those contain both person name mentions and prize name mentions, they have an average length of around 30.", "labels": [], "entities": []}, {"text": "Among those relevant ones, the average length of the sentences parsable by ERG is around 25.", "labels": [], "entities": [{"text": "ERG", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8169996738433838}]}, {"text": "Experiments Two phases of experiments are conducted.", "labels": [], "entities": []}, {"text": "In the training phase, we show that reranking improves RE performance.", "labels": [], "entities": [{"text": "RE", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9224281907081604}]}, {"text": "The test phase applies the reranking model resulting from the training phase to the test corpus.", "labels": [], "entities": []}, {"text": "In both phases, two different experiments are conducted 1) Baseline: without reranking; 2) reranking: with parse reranking.", "labels": [], "entities": [{"text": "reranking", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.8499207496643066}]}, {"text": "In the baseline experiment, we keep the first n readings of all sentences and run DARE for rule learning and RE on top of these readings.", "labels": [], "entities": [{"text": "DARE", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9975405931472778}, {"text": "rule learning", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.796210378408432}, {"text": "RE", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9945047497749329}]}, {"text": "The aim is to observe whether correct relation instances can also be detected in lower-ranked readings.", "labels": [], "entities": []}, {"text": "In the second experiments, we aim to investigate whether reranking based on task-feedback and domain knowledge is useful for better extraction performance.", "labels": [], "entities": []}, {"text": "These experiments are conducted only with the best reading after reranking, i. e. the normal setting of RE application.", "labels": [], "entities": [{"text": "RE", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.5585687756538391}]}, {"text": "In none of the experiments, confidence thresholds are employed for improving precision by filtering out less confident rules or instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9971423745155334}]}, {"text": "As we are mainly interested in the effects of reranking on RE recall, we are trying to avoid any other factors that may influence the recall.", "labels": [], "entities": [{"text": "RE", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.487211674451828}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.6479232311248779}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9966362714767456}]}, {"text": "Thus in our experiments confidence estimation scores are only used for reranking.", "labels": [], "entities": []}, {"text": "Qualitative Analysis Given the experimental results, we carryout various qualitative analysis on the results of both parsing and RE.", "labels": [], "entities": [{"text": "parsing", "start_pos": 117, "end_pos": 124, "type": "TASK", "confidence": 0.9702872633934021}]}, {"text": "With respect to parsing, we evaluate the results against the goldstandard treebank before and after reranking.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9916025996208191}, {"text": "goldstandard treebank", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.9147649109363556}]}, {"text": "In addition we evaluate the quality of the extraction rules before and after the reranking.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training phase: Comparison of RE per- formance before and after reranking.", "labels": [], "entities": [{"text": "RE", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9852747917175293}]}, {"text": " Table 2: Test phase: Comparison of RE perfor- mance before and after reranking.", "labels": [], "entities": [{"text": "RE perfor- mance", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.9385106563568115}]}, {"text": " Table 5: Test corpus: distribution of good readings  before and after reranking", "labels": [], "entities": []}]}