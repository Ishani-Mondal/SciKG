{"title": [{"text": "Utilizing Target-Side Semantic Role Labels to Assist Hierarchical Phrase-based Machine Translation", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.6312591036160787}]}], "abstractContent": [{"text": "In this paper we present a novel approach of utilizing Semantic Role Labeling (SRL) information to improve Hierarchical Phrase-based Machine Translation.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.7535403122504553}, {"text": "Hierarchical Phrase-based Machine Translation", "start_pos": 107, "end_pos": 152, "type": "TASK", "confidence": 0.734492838382721}]}, {"text": "We propose an algorithm to extract SRL-aware Synchronous Context-Free Grammar (SCFG) rules.", "labels": [], "entities": [{"text": "SRL-aware Synchronous Context-Free Grammar (SCFG)", "start_pos": 35, "end_pos": 84, "type": "TASK", "confidence": 0.8744685820170811}]}, {"text": "Conventional Hiero-style SCFG rules will also be extracted in the same framework.", "labels": [], "entities": []}, {"text": "Special conversion rules are applied to ensure that when SRL-aware SCFG rules are used in derivation , the decoder only generates hypotheses with complete semantic structures.", "labels": [], "entities": [{"text": "SRL-aware SCFG", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.8466239273548126}]}, {"text": "We perform machine translation experiments using 9 different Chinese-English test-sets.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7723031938076019}]}, {"text": "Our approach achieved an average BLEU score improvement of 0.49 as well as 1.21 point reduction in TER.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9746183156967163}, {"text": "1.21 point reduction", "start_pos": 75, "end_pos": 95, "type": "METRIC", "confidence": 0.8067074616750082}, {"text": "TER", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9855936169624329}]}], "introductionContent": [{"text": "Syntax-based Machine Translation methods have achieved comparable performance to Phrase-based systems.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7280166745185852}]}, {"text": "Hierarchical Phrase-based Machine Translation, proposed by Chiang, uses a general non-terminal label X but does not use linguistic information from the source or the target language.", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Machine Translation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7511208653450012}]}, {"text": "There have been efforts to include linguistic information into machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.759750485420227}]}, {"text": "experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (.", "labels": [], "entities": [{"text": "Packed Forest data structure", "start_pos": 133, "end_pos": 161, "type": "DATASET", "confidence": 0.7967006117105484}]}, {"text": "The string-to-tree () and tree-to-tree methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees.", "labels": [], "entities": []}, {"text": "One problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.", "labels": [], "entities": []}, {"text": "SAMT () and Tree Sequence Alignment () are proposed to amend this problem by allowing non-constituent phrases to be extracted.", "labels": [], "entities": [{"text": "Tree Sequence Alignment", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.6632345418135325}]}, {"text": "The reported results show that while utilizing linguistic information helps, the coverage is more important.", "labels": [], "entities": [{"text": "coverage", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9311419129371643}]}, {"text": "When dealing with formalisms such as semantic role labeling, the coverage problem is also critical.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7060123880704244}]}, {"text": "In this paper we follow Chiang's observation and use SRL labels to augment the extraction of SCFG rules.", "labels": [], "entities": []}, {"text": "I.e., the formalism provides additional information and more rules instead of restrictions that remove existing rules.", "labels": [], "entities": []}, {"text": "This preserves the coverage of rules.", "labels": [], "entities": []}, {"text": "Recently there has been increased attention to use semantic information in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7766760289669037}]}, {"text": "proposed using Semantic Role Labels (SRL) in their tree-to-string machine translation system and demonstrated improvement over conventional tree-to-string methods.", "labels": [], "entities": [{"text": "tree-to-string machine translation", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.6979875167210897}]}, {"text": "developed a framework to reorder the output using information from both the source and the target SRL labels.", "labels": [], "entities": []}, {"text": "In this paper, we explore an approach of using the target side SRL information in addition to a Hierarchical Phrase-based Machine Translation framework.", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Machine Translation", "start_pos": 96, "end_pos": 141, "type": "TASK", "confidence": 0.5543229654431343}]}, {"text": "The proposed method extracts initial phrases with two different heuristics: The first heuristic is used to extract rules that have a general left-hand-side (LHS) non-terminal tag X, 107 Second we must build a flood prevention system , strengthen pre-flood inspections and implement flood prevention measures Figure 1: Example of predicate-argument structure in a sentence i.e., Hiero rules.", "labels": [], "entities": [{"text": "flood prevention", "start_pos": 209, "end_pos": 225, "type": "TASK", "confidence": 0.7257025837898254}, {"text": "flood prevention", "start_pos": 282, "end_pos": 298, "type": "TASK", "confidence": 0.7348452508449554}]}, {"text": "The second will extract phrases that contain information of SRL structures.", "labels": [], "entities": [{"text": "SRL structures", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.9105493128299713}]}, {"text": "The predicate and arguments that the phrase covers will be represented in the LHS non-terminal tags.", "labels": [], "entities": [{"text": "LHS non-terminal tags", "start_pos": 78, "end_pos": 99, "type": "DATASET", "confidence": 0.8148735364278158}]}, {"text": "After that, we obtain rules from the initial phrases in the same way as the Hiero extraction algorithm, which replaces nesting phrases with their corresponding non-terminals.", "labels": [], "entities": [{"text": "Hiero extraction", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7266379743814468}]}, {"text": "By applying this scheme, we will obtain rules that contain SRL information, without sacrificing the coverage of rules.", "labels": [], "entities": []}, {"text": "In this paper, we call such rules SRL-aware SCFG rules.", "labels": [], "entities": [{"text": "SRL-aware SCFG", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.534728080034256}]}, {"text": "During decoding, both the conventional Hiero-style SCFG rules with general tag X and SRL-aware SCFG rules are used in asynchronous Chart Parsing algorithm.", "labels": [], "entities": [{"text": "Chart Parsing", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.7105108946561813}]}, {"text": "Special conversion rules are introduced to ensure that whenever SRL-aware SCFG rules are used in the derivation, a complete predicate-argument structure is built.", "labels": [], "entities": [{"text": "SRL-aware SCFG", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.8126809895038605}]}, {"text": "The main contributions are: 1.", "labels": [], "entities": []}, {"text": "an algorithm to extract SRL-aware SCFG rules using target side SRL information.", "labels": [], "entities": [{"text": "SRL-aware SCFG", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.825377345085144}]}, {"text": "2. an approach to use Hiero rules side-by-side with information-rich SRL-aware SCFG rules, which improves the quality of translation results.", "labels": [], "entities": []}, {"text": "In section 2 we briefly review SCFG-based machine translation and SRL.", "labels": [], "entities": [{"text": "SCFG-based machine translation", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.7073337535063425}, {"text": "SRL", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9548915028572083}]}, {"text": "In section 3, we describe the SRL-aware SCFG rules.", "labels": [], "entities": [{"text": "SRL-aware SCFG", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8101488649845123}]}, {"text": "Section 4 provides the detail of the rule extraction algorithm.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8697010278701782}]}, {"text": "Section 5 presents two alternative methods how to utilize the SRL information.", "labels": [], "entities": [{"text": "SRL", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9549655914306641}]}, {"text": "The experimental results are given in Section 6, followed by analysis and conclusion in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on Chinese to English translation tasks.", "labels": [], "entities": [{"text": "Chinese to English translation tasks", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.6499312400817872}]}, {"text": "The data set we used in the experiments is a subset of the FBIS corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9311058521270752}]}, {"text": "We filter the corpus with maximum sentence length be 30.", "labels": [], "entities": []}, {"text": "The corpus has 2.5 million words in Chinese side and 3.1 million on English side.", "labels": [], "entities": []}, {"text": "We adopted the ASSERT semantic role labeler () to label the English side sentences.", "labels": [], "entities": [{"text": "ASSERT semantic role labeler", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.8083195090293884}]}, {"text": "The parallel sentences are aligned using MGIZA++ ( and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules.", "labels": [], "entities": [{"text": "MGIZA", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8214061856269836}, {"text": "rule extraction", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.720457598567009}, {"text": "SRL-aware SCFG rules", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.746528168519338}]}, {"text": "We used the MosesChart decoder and the Moses toolkit () for tuning and decoding.", "labels": [], "entities": [{"text": "MosesChart decoder", "start_pos": 12, "end_pos": 30, "type": "DATASET", "confidence": 0.9302411377429962}]}, {"text": "The language model is a trigram language model trained on English GIGAWord corpus (V1-V3) using the SRILM toolkit.", "labels": [], "entities": [{"text": "English GIGAWord corpus", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.6513184209664663}, {"text": "SRILM toolkit", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.8986654281616211}]}, {"text": "We used the NIST MT06 test set for tuning, and experimented with an additional 9 test sets, including MT02, 03, 04, 05, 08, and GALE test sets DEV07-dev and DEV07-blind.", "labels": [], "entities": [{"text": "NIST MT06 test set", "start_pos": 12, "end_pos": 30, "type": "DATASET", "confidence": 0.8695224970579147}, {"text": "MT02", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.9261236786842346}, {"text": "GALE test sets DEV07-dev", "start_pos": 128, "end_pos": 152, "type": "DATASET", "confidence": 0.7243935763835907}, {"text": "DEV07-blind", "start_pos": 157, "end_pos": 168, "type": "DATASET", "confidence": 0.8978930711746216}]}, {"text": "DEV07-dev and DEV07-blind are further divided into newswire and weblog parts.", "labels": [], "entities": [{"text": "DEV07-dev", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9808862805366516}, {"text": "DEV07-blind", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.932016909122467}]}, {"text": "We experimented with the proposed method and the alternative methods presented in section 4, and the results of nine test sets are listed in.", "labels": [], "entities": []}, {"text": "As we can observe from the results, the largest improvement we discovered from our proposed method is more than 1 BLEU point, and a significant drop is only observed on one test set, MT03, where we lose 0.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9989748001098633}, {"text": "MT03", "start_pos": 183, "end_pos": 187, "type": "DATASET", "confidence": 0.8579898476600647}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9975416660308838}]}, {"text": "Averaged across all the test sets, the improvement is 0.49 BLEU points on the small training set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9997110962867737}]}, {"text": "When TER is also taken into account, all of the nine test sets showed consistent improvement.", "labels": [], "entities": [{"text": "TER", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9995583891868591}]}, {"text": "The (TER-BLEU)/2 score, which we used as the primary evaluation metric, improved by 0.85 across nine test sets.", "labels": [], "entities": [{"text": "TER-BLEU)/2 score", "start_pos": 5, "end_pos": 22, "type": "METRIC", "confidence": 0.9847424924373627}]}, {"text": "As we expected, the coverage of SRL-aware SCFG rules is not as good as the Hiero rules.", "labels": [], "entities": [{"text": "SRL-aware SCFG", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7091761529445648}, {"text": "Hiero", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9367562532424927}]}, {"text": "We analyzed the top-best derivation of the results.", "labels": [], "entities": []}, {"text": "Only 1836 out of 7235 sentences in the test sets used SRLaware SCFG rules.", "labels": [], "entities": [{"text": "SRLaware SCFG", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.8034807741641998}]}, {"text": "However, the BLEU scores on the 1836 sentences improved from 27.98 in the baseline system to 28.80, while the remaining 5399 sentences only improved from 30.13 to 30.22.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9995408058166504}]}, {"text": "The observation suggests the potential for further improvement if we can increase the coverage by using more data or by modifying the mapping from tags to the structures to make rules more general.", "labels": [], "entities": [{"text": "coverage", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9458121061325073}]}, {"text": "We display the hypothesis of a sentence in to demonstrate a concrete example of improvements obtained by using the method,.", "labels": [], "entities": []}, {"text": "As this figure demonstrate, the SRL-aware SCFG rules enable the system to pick the correct structure and reordering for the verbs trigger and enter.", "labels": [], "entities": [{"text": "SRL-aware SCFG", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8170108199119568}]}, {"text": "Given the results presented in the paper, the question arises as to whether it is prudent to integrate multiple formalisms or labeling systems, such as 113 On the other hand, fully syntactic-based machine translation suffers from low coverage of rules.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7260528057813644}]}, {"text": "The methodology in this paper, in contrast, introduces linguistic information to assist a formalism that does not incorporate linguistic information.", "labels": [], "entities": []}, {"text": "The merits of doing so are obvious.", "labels": [], "entities": []}, {"text": "While most parts of the system are not changed, a portion of the system is considerably improved.", "labels": [], "entities": []}, {"text": "Also, the system encodes the information in the non-terminal tags, which is widely used in other methods such as SAMT.", "labels": [], "entities": [{"text": "SAMT", "start_pos": 113, "end_pos": 117, "type": "TASK", "confidence": 0.9193158149719238}]}, {"text": "However, it is not necessary an optimal solution.", "labels": [], "entities": []}, {"text": "Huang et al in a very recent work ( ) proposed using vector space to represent similarity between the syntactic structures.", "labels": [], "entities": []}, {"text": "This is also an interesting possible direction to explore in the near future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment results on Chinese-English translation tasks, bl-nw and bl-wb are newswire and weblog parts for  DEV07-blind, dv-nw and dv-wb are newswire and weblog parts for DEV07-dev. We present the BLEU scores, TER  scores and (TER-BLEU)/2.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.687419722477595}, {"text": "DEV07-blind", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.9606252312660217}, {"text": "DEV07-dev", "start_pos": 181, "end_pos": 190, "type": "DATASET", "confidence": 0.9575417041778564}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9989305138587952}, {"text": "TER  scores", "start_pos": 220, "end_pos": 231, "type": "METRIC", "confidence": 0.9779388308525085}, {"text": "TER-BLEU)/2", "start_pos": 237, "end_pos": 248, "type": "METRIC", "confidence": 0.9520014524459839}]}]}