{"title": [{"text": "An Unsupervised Alignment Algorithm for Text Simplification Corpus Construction", "labels": [], "entities": [{"text": "Text Simplification Corpus", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.85413126150767}]}], "abstractContent": [{"text": "We present a method for the sentence-level alignment of short simplified text to the original text from which they were adapted.", "labels": [], "entities": [{"text": "sentence-level alignment of short simplified text", "start_pos": 28, "end_pos": 77, "type": "TASK", "confidence": 0.8095653603474299}]}, {"text": "Our goal is to align a medium-sized corpus of parallel text, consisting of short news texts in Spanish with their simplified counterpart.", "labels": [], "entities": []}, {"text": "No training data is available for this task, so we have to rely on unsupervised learning.", "labels": [], "entities": []}, {"text": "In contrast to bilingual sentence alignment, in this task we can exploit the fact that the probability of sentence correspondence can be estimated from lexical similarity between sentences.", "labels": [], "entities": [{"text": "bilingual sentence alignment", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6501888334751129}]}, {"text": "We show that the algoithm employed performs better than a baseline which approaches the problem with a TF*IDF sentence similarity metric.", "labels": [], "entities": []}, {"text": "The alignment algorithm is being used for the creation of a corpus for the study of text simplification in the Spanish language.", "labels": [], "entities": [{"text": "text simplification in the Spanish language", "start_pos": 84, "end_pos": 127, "type": "TASK", "confidence": 0.8127418210109075}]}], "introductionContent": [{"text": "Text simplification is the process of transforming a text into an equivalent which is more understandable fora target user.", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7797484695911407}]}, {"text": "This simplification is beneficial for many groups of readers, such as language learners, elderly persons and people with other special reading and comprehension necessities.", "labels": [], "entities": []}, {"text": "Simplified texts are characterized by a simple and direct style and a smaller vocabulary which substitutes infrequent and otherwise difficult words (such as long composite nouns, technical terms, neologisms and abstract concepts) by simpler corresponding expressions.", "labels": [], "entities": []}, {"text": "Usually unnecessary details are omitted.", "labels": [], "entities": []}, {"text": "Another characteristic trait of simplified texts is that usually only one main idea is expressed by a single sentence.", "labels": [], "entities": []}, {"text": "This also means that in the simplification process complex sentences are often split into several smaller sentences.", "labels": [], "entities": []}, {"text": "The availability of a sentence-aligned corpus of original texts and their simplifications is of paramount importance for the study of simplification and for developing an automatic text simplification system.", "labels": [], "entities": []}, {"text": "The different strategies that human editors employ to simplify texts are varied and have the effect that individual parts of the resulting text may either become shorter or longer than the original text.", "labels": [], "entities": []}, {"text": "An editor may, for example, delete detailed information, making the text shorter.", "labels": [], "entities": []}, {"text": "Or she may split complex sentences into various smaller sentences.", "labels": [], "entities": []}, {"text": "As a result, simplified texts tend to become shorter than the source, but often the number of sentences increases.", "labels": [], "entities": []}, {"text": "Not all of the information presented in the original needs to be preserved but in general all of the information in the simplified text stems from the source text.", "labels": [], "entities": []}, {"text": "The need to align parallel texts arises from a larger need to create a medium size corpus which will allow the study of the editing process of simplifying text, as well as to serve as a gold standard to evaluate a text simplification system.", "labels": [], "entities": []}, {"text": "Sentence alignment for simplified texts is related to, but different from, the alignment of bilingual text and also from the alignment of summaries to an original text.", "labels": [], "entities": [{"text": "Sentence alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9066659510135651}]}, {"text": "Since the alignment of simplified sentences is a case of monolingual alignment the lexical similarity between two corresponding sentences can betaken as an indicator of correspondence.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 briefly introduces text simplification which contex-tualises this piece of research and Section 3 discusses some related work.", "labels": [], "entities": []}, {"text": "In Section 4 we briefly describe the texts we are working with and in Section 5 we present the alignment algorithm.", "labels": [], "entities": [{"text": "alignment", "start_pos": 95, "end_pos": 104, "type": "TASK", "confidence": 0.9056171774864197}]}, {"text": "Section 6 presents the details of the experiment and its results.", "labels": [], "entities": []}, {"text": "Finally, section 7 gives a concluding discussion and an outlook on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are working with a corpus of 200 news articles in Spanish covering the following topics: National News, Society, International News and Culture.", "labels": [], "entities": []}, {"text": "Each of the texts is being adapted by the DILES Research Group from Universidad Aut\u00f3noma de Madrid.", "labels": [], "entities": [{"text": "DILES Research Group from Universidad Aut\u00f3noma de Madrid", "start_pos": 42, "end_pos": 98, "type": "DATASET", "confidence": 0.8823506981134415}]}, {"text": "Original and adapted examples of texts in Spanish can be seen in (the texts are adaptations carried out by DILES for Revista \"La Plaza\").", "labels": [], "entities": [{"text": "Revista \"La Plaza\")", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.849712073802948}]}, {"text": "The texts are being processed using part-of-speech tagging, named entity recognition, and parsing in order to create an automatically annotated corpus.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6969636380672455}, {"text": "named entity recognition", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.6060389379660288}]}, {"text": "The bi-texts are first aligned using the tools to be described in this paper and then post-edited with the help of a bi-text editor provided in the GATE framework).", "labels": [], "entities": [{"text": "GATE framework", "start_pos": 148, "end_pos": 162, "type": "DATASET", "confidence": 0.9052473604679108}]}, {"text": "shows the texts in the alignment editor.", "labels": [], "entities": []}, {"text": "This tool is however insufficient for our purposes since it does not provide mechanisms for uploading the alignments produced outside the GATE framework and for producing stand-alone versions of the bi-texts; we have therefore extended the functionalities of the tool for the purpose of corpus creation.", "labels": [], "entities": [{"text": "GATE framework", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.7897785604000092}, {"text": "corpus creation", "start_pos": 287, "end_pos": 302, "type": "TASK", "confidence": 0.7180764973163605}]}, {"text": "Our goal is to align a larger corpus of Spanish short news texts with their simplified counterparts.", "labels": [], "entities": []}, {"text": "At the moment, however, we only have a small sample of this corpus available.", "labels": [], "entities": []}, {"text": "The size of this corpus sample is 1840 words of simplified text (145 sentences) which correspond to 2456 (110 sentences) of source text.", "labels": [], "entities": []}, {"text": "We manually created a gold standard which includes all the correct alignments between simplified and source sentences.", "labels": [], "entities": []}, {"text": "The results of the classifier were calculated against this gold standard.", "labels": [], "entities": []}, {"text": "As a baseline we used a TF*IDF score based method which chooses for each sentence in the simplified text the sentence with the minimal word vector distance.", "labels": [], "entities": [{"text": "TF*IDF score", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.8525041937828064}]}, {"text": "The procedure is as follows: each sentence in the original and simplified document is represented in the vector space model using a term vector.", "labels": [], "entities": []}, {"text": "Each term (e.g. token) is weithed using as TF the frequency of the term in the document and IDF = log(N + 1/M t + 1) where Mt is the number of sentences 1 containing t and N is the number of sentences in the corpus (counts are obtained from the set of documents to align).", "labels": [], "entities": [{"text": "TF", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.990925669670105}, {"text": "IDF", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9967840909957886}]}, {"text": "As similarity metric between vectors we use the cosine of the angle between the two vectors given in the following formula: Here s 1 and s 2 are the sentence vectors and w i,s k is the weight of term i in sentence s k . We align all simplified sentences (i.e. for the time being no cutoff has been used to identify new material in the simplified text).", "labels": [], "entities": []}, {"text": "For the calculation of the first baseline we calculate IDF over the sentences in whole corpus.", "labels": [], "entities": [{"text": "IDF", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9993855953216553}]}, {"text": "argue that that the relevant unit for this calculation should be each document for the following reason: Some words are much more frequent in some texts than they are in others.", "labels": [], "entities": []}, {"text": "For example the word unicorn is relatively infrequent in English and it it may also be infrequent in a given colletion of texts.", "labels": [], "entities": []}, {"text": "So this word is highly discriminative and it's IDF will be relatively high.", "labels": [], "entities": [{"text": "IDF", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9981813430786133}]}, {"text": "Ina specific text about imagenary creatures, however, the same word unicorn maybe much more frequent and hence it's discrimiative power is much lower.", "labels": [], "entities": []}, {"text": "For this reason we calcuated a second baseline, where we calculate the IDF only on the sentences of the relevanct texts.", "labels": [], "entities": [{"text": "IDF", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9604357481002808}]}, {"text": "Results of aligning all sentences in our sample corpus using both the baseline and the HMM algorithms are given in If we compare these results to those presented by, we can observe that we obtain a comparable precision, but the recall improves dramatically from 55.8% (with their specific feature setting) to 82.4%.", "labels": [], "entities": [{"text": "precision", "start_pos": 209, "end_pos": 218, "type": "METRIC", "confidence": 0.9975753426551819}, {"text": "recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9995525479316711}]}, {"text": "Our TF*IDF baselines are not directly comparable comparable to Nelken and Shieber's results.", "labels": [], "entities": [{"text": "TF*IDF baselines", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.4662977382540703}]}, {"text": "The reason why we cannot compare our results directly is that Nelken and Shieber use supervised learning in order to optimize the transformation of TF*IDF scores into probabilities and we had no training data available.", "labels": [], "entities": [{"text": "TF*IDF scores", "start_pos": 148, "end_pos": 161, "type": "METRIC", "confidence": 0.5982497856020927}]}, {"text": "We included the additional scores for our system, when no transition probabilities are included in the calculation of the optimal alignment sequence and the score comes only from the probabilies of our clalculation of lexical similarity between sentences (alignment only).", "labels": [], "entities": []}, {"text": "These scores show that a large part of the good performance comes from lexical similarity and sequencial classification only give an additional final boost, a fact which was already observed by Nelken and Shieber.", "labels": [], "entities": []}, {"text": "We also attribute the fact that the system alrives at stable values after two iterations to the same efect: lexical similarity seems to have a much bigger effect on the general performance.", "labels": [], "entities": []}, {"text": "Still our probability-based similarity meassure clearly outperforms the TF*IDF baselines.", "labels": [], "entities": [{"text": "TF*IDF baselines", "start_pos": 72, "end_pos": 88, "type": "METRIC", "confidence": 0.5213292613625526}]}], "tableCaptions": []}