{"title": [], "abstractContent": [{"text": "During language acquisition, children learn to segment speech into phonemes, syllables, morphemes, and words.", "labels": [], "entities": []}, {"text": "We examine word segmentation specifically, and explore the possibility that children might have general-purpose chunking mechanisms to perform word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7739455699920654}, {"text": "word segmentation", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.7496316432952881}]}, {"text": "The Voting Experts (VE) and Bootstrapped Voting Experts (BVE) algorithms serve as computational models of this chunking ability.", "labels": [], "entities": []}, {"text": "VE finds chunks by searching fora particular information-theoretic signature: low internal entropy and high boundary entropy.", "labels": [], "entities": []}, {"text": "BVE adds to VE the ability to incorporate information about word boundaries previously found by the algorithm into future segmentations.", "labels": [], "entities": [{"text": "BVE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5238337516784668}, {"text": "VE", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.6502109169960022}]}, {"text": "We evaluate the general chunking model on phonemically-encoded corpora of child-directed speech, and show that it is consistent with empirical results in the developmental literature.", "labels": [], "entities": []}, {"text": "We argue that it offers a parsimonious alternative to special-purpose linguistic models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to extract words from fluent speech appears as early as the seventh month in human).", "labels": [], "entities": []}, {"text": "Models of this ability have emerged from such diverse fields as linguistics, psychology and computer science.", "labels": [], "entities": []}, {"text": "Many of these models make unrealistic assumptions about child language learning, or rely on supervision, or are specific to speech or language.", "labels": [], "entities": []}, {"text": "Here we present an alternative: a general unsupervised model of chunking that performs very well on word segmentation tasks.", "labels": [], "entities": [{"text": "word segmentation tasks", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.8050680160522461}]}, {"text": "We will examine the Voting Experts, Bootstrapped Voting Experts, and Phoneme to Morpheme algorithms in Section 2.", "labels": [], "entities": []}, {"text": "Each searches fora general, information-theoretic signature of chunks.", "labels": [], "entities": []}, {"text": "Each can operate in either a fully unsupervised setting, where the input is a single continuous sequence of phonemes, or a semi-supervised setting, where the input is a sequence of sentences.", "labels": [], "entities": []}, {"text": "In Section 4, we evaluate these general chunking methods on phonetically-encoded corpora of child-directed speech, and compare them to a representative set of computational models of early word segmentation.", "labels": [], "entities": [{"text": "early word segmentation", "start_pos": 183, "end_pos": 206, "type": "TASK", "confidence": 0.6335715154806772}]}, {"text": "Section 4.4 presents evidence that words optimize the information-theoretic signature of chunks.", "labels": [], "entities": []}, {"text": "Section 5 discusses segmentation methods in light of what is known about the segmentation abilities of children.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.9875020980834961}]}], "datasetContent": [{"text": "In this section, we evaluate the general chunking algorithms VE, BVE, and PtM in both the continuous, unsupervised paradigm of and the incremental, semi-supervised paradigm assumed by bootstrapping algorithms like MBDP-1.", "labels": [], "entities": [{"text": "BVE", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8435459136962891}, {"text": "MBDP-1", "start_pos": 214, "end_pos": 220, "type": "DATASET", "confidence": 0.9301709532737732}]}, {"text": "We briefly describe the artificial input used by Saffran et al., and then turn to the broader problem of word segmentation in natural languages by evaluating against corpora drawn from the CHILDES database (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7193074077367783}, {"text": "CHILDES database", "start_pos": 189, "end_pos": 205, "type": "DATASET", "confidence": 0.8840458989143372}]}, {"text": "We evaluate segmentation quality at two levels: boundaries and words.", "labels": [], "entities": []}, {"text": "At the boundary level, we compute the Boundary Precision (BP), which is simply the percentage of induced boundaries that were correct, and Boundary Recall (BR), which is the percentage of true boundaries that were recovered by the algorithm.", "labels": [], "entities": [{"text": "Boundary Precision (BP)", "start_pos": 38, "end_pos": 61, "type": "METRIC", "confidence": 0.9747413158416748}, {"text": "Boundary Recall (BR)", "start_pos": 139, "end_pos": 159, "type": "METRIC", "confidence": 0.9730599522590637}]}, {"text": "These measures are commonly combined into a single metric, the Boundary Fscore (BF), which is the harmonic mean of BP and BR: BF = (2 \u00d7 BP \u00d7 BR)/(BP + BR).", "labels": [], "entities": [{"text": "Boundary Fscore (BF)", "start_pos": 63, "end_pos": 83, "type": "METRIC", "confidence": 0.9569056987762451}, {"text": "BP", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.9025139212608337}, {"text": "BR", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.8778208494186401}, {"text": "BF", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9928489923477173}]}, {"text": "Generally, higher BF scores correlate with finding correct chunks more frequently, but for completeness we also compute the Word Precision (WP), which is the percentage of induced words that were correct, and the Word Recall (WR), which is the percentage of true words that were recovered exactly by the algorithm.", "labels": [], "entities": [{"text": "BF", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9939781427383423}, {"text": "Word Precision (WP)", "start_pos": 124, "end_pos": 143, "type": "METRIC", "confidence": 0.8338181018829346}, {"text": "Word Recall (WR)", "start_pos": 213, "end_pos": 229, "type": "METRIC", "confidence": 0.8341609954833984}]}, {"text": "These measures can naturally be combined into a single F-score, the Word F-score (WF): WF = (2 \u00d7 WP \u00d7 WR)/(WP + WR).", "labels": [], "entities": [{"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9701249003410339}, {"text": "Word F-score (WF)", "start_pos": 68, "end_pos": 85, "type": "METRIC", "confidence": 0.892236328125}, {"text": "WF", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.8995620608329773}]}], "tableCaptions": [{"text": " Table 1: Results for the BR87 corpus with unconstrained  processing of the corpus. Algorithms in italics are semi- supervised.", "labels": [], "entities": [{"text": "BR87 corpus", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9637580513954163}]}, {"text": " Table 2: Performance of various algorithms on the Brown  corpus from CHILDES. Other than VE and All Points,  values are taken from (", "labels": [], "entities": [{"text": "Brown  corpus from CHILDES", "start_pos": 51, "end_pos": 77, "type": "DATASET", "confidence": 0.8892897516489029}, {"text": "VE", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9876554608345032}, {"text": "All Points", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9546299278736115}]}]}