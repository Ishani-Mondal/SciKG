{"title": [], "abstractContent": [{"text": "Distributed models of semantics assume that word meanings can be discovered from \"the company they keep.\"", "labels": [], "entities": []}, {"text": "Many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a document.", "labels": [], "entities": []}, {"text": "In contrast, this paper proposes a structured vectorial semantic framework, in which semantic vectors are defined and composed in syntactic context.", "labels": [], "entities": []}, {"text": "As such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse.", "labels": [], "entities": []}, {"text": "Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9097379446029663}]}], "introductionContent": [{"text": "Distributed semantic representations like Latent Semantic Analysis), probabilistic LSA), Latent Dirichlet Allocation (, or relational clustering () have garnered widespread interest because of their ability to quantitatively capture 'gist' semantic content.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.5839282472928365}]}, {"text": "Two modeling assumptions underlie most of these models.", "labels": [], "entities": []}, {"text": "First, the typical assumption is that words in the same document are an unstructured bag of words.", "labels": [], "entities": []}, {"text": "This means that word order and syntactic structure are ignored in the resulting vectorial representations of meaning, and the only relevant relationship between words is the 'same-document' relationship.", "labels": [], "entities": []}, {"text": "Second, these semantic models are not compositional in and of themselves.", "labels": [], "entities": []}, {"text": "They require some external process to aggregate the meaning representations of words to form phrasal or sentential meaning; at best, they can jointly represent whole strings of words without the internal relationships.", "labels": [], "entities": []}, {"text": "This paper introduces structured vectorial semantics (SVS) as a principled response to these weaknesses of vector space models.", "labels": [], "entities": [{"text": "structured vectorial semantics (SVS)", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.6728126506010691}]}, {"text": "In this framework, the syntax-semantics interface is fully interactive: semantic vectors exist in syntactic context, and any composition of semantic vectors necessarily produces a hypothetical syntactic parse.", "labels": [], "entities": []}, {"text": "Since semantic information is used in syntactic disambiguation, we would expect practical improvements in parsing accuracy by accounting for the interactive interpretation process.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7291578948497772}, {"text": "parsing", "start_pos": 106, "end_pos": 113, "type": "TASK", "confidence": 0.9648521542549133}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8918173313140869}]}, {"text": "Others have incorporated syntactic information with vector-space semantics, challenging the bagof-words assumption.", "labels": [], "entities": []}, {"text": "Syntax and semantics maybe jointly generated with Bayesian methods (); syntactic structure maybe coupled to the basis elements of a semantic space; clustered semantics maybe used as a pre-processing step (; or, semantics maybe learned in some defined syntactic context).", "labels": [], "entities": []}, {"text": "These techniques are interactive, but their semantic models are not syntactically compositional.", "labels": [], "entities": []}, {"text": "SVS is a generative model of sentences that uses a variant of the last strategy to incorporate syntax at preterminal tree nodes, but is inherently compositional.", "labels": [], "entities": []}, {"text": "provide a general framework for semantic vector composition: where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor).", "labels": [], "entities": [{"text": "semantic vector composition", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6418374379475912}]}, {"text": "In this initial work of theirs, they leave out any notion of syntactic context, focusing on additive and multiplicative vector composition (with some variations): Since the structured vectorial semantics proposed here maybe viewed within this framework, our discussion will begin from their definition in Section 2.1.", "labels": [], "entities": []}, {"text": "model also fits inside Mitchell and Lapata's framework, and like SVS, it includes syntactic context.", "labels": [], "entities": []}, {"text": "Their semantic vectors use syntactic information as relations between multiple vectors in arriving at a final meaning representation.", "labels": [], "entities": []}, {"text": "The emphasis, however, is on selectional preferences of individual words; resulting representations are similar to word-sense disambiguation output, and do not construct phrase-level meaning from word meaning.", "labels": [], "entities": []}, {"text": "Mitchell and Lapata's more recent work (2009) combines syntactic parses with distributional semantics; but the underlying compositional model requires (as other existing models would) an interpolation of the vector composition results with a separate parser.", "labels": [], "entities": []}, {"text": "It is thus not fully interactive.", "labels": [], "entities": []}, {"text": "Though the proposed structured vectorial semantics maybe defined within Equation 1, the end output necessarily includes not only a semantic vector, but a full parse hypothesis.", "labels": [], "entities": []}, {"text": "This slightly shifts the focus from the semantically-centered Equation 1 to an accounting of meaning that is necessarily interactive (between syntax and semantics); vector composition and parsing are then twin lenses by which the process maybe viewed.", "labels": [], "entities": []}, {"text": "Thus, unlike previous models, a unique phrasal vectorial semantic representation is composed during decoding.", "labels": [], "entities": []}, {"text": "Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 161, "end_pos": 168, "type": "TASK", "confidence": 0.9595319032669067}]}, {"text": "The structured vectorial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed: lexicalized parsing) and relational clustering (akin to latent annotations ().", "labels": [], "entities": []}, {"text": "Because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (, syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and intransitive verbs).", "labels": [], "entities": []}, {"text": "This pessimistically isolates the contribution of semantics on parsing accuracyit will only show parsing gains where semantic information does not overlap with distributional syntactic information.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9572740793228149}, {"text": "accuracyit", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.8590051531791687}]}, {"text": "Evaluations show that interactively considering semantic information with syntax has the predicted positive impact on parsing accuracy over syntax alone; it also lowers per-word perplexity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9335102438926697}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes SVS as both vector composition and parsing; Section 3 shows how relational-clustering SVS subsumes PCFG-LAs; and Section 4 evaluates modeling assumptions and empirical performance.", "labels": [], "entities": [{"text": "SVS", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9426401257514954}, {"text": "SVS subsumes PCFG-LAs", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.623943010965983}]}], "datasetContent": [{"text": "Sections 02-21 of the Wall Street Journal (WSJ) corpus were used as training data; Section 23 was used as test data with reported parsing results on sentences greater than length 40.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 22, "end_pos": 54, "type": "DATASET", "confidence": 0.9545719453266689}]}, {"text": "Punctuation was left in for all reported evaluations.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9878596663475037}]}, {"text": "Trees were binarized, and syntactic states were thoroughly split into subcategorization classes.", "labels": [], "entities": []}, {"text": "As previously discussed, unlike tests on state-of-the-art automatically statesplitting parsers, this isolates the contribution of semantics.", "labels": [], "entities": []}, {"text": "The baseline 83.57 F-measure is comparable to before the inclusion of head annotations.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8402618765830994}]}, {"text": "Subsequently, each branch was annotated with ahead relation l ID or a modifier relation l MOD according to a binarized version of headword percolation rules, and the headword was propagated up from its head constituent.", "labels": [], "entities": []}, {"text": "The most frequent headwords (e.g., h 1 , . .", "labels": [], "entities": []}, {"text": ", h 50 ) were stored, and the rest were assigned a constant, 'unk' headword category.", "labels": [], "entities": []}, {"text": "From counts on the binary rules of these annotated trees, the \u03b8 M , \u03b8 L , \u03b8 P-Vit(G) , \u03c0 G\ud97b\udf59 , and \u03c0 G probabilities for headword-lexicalization SVS were obtained.", "labels": [], "entities": []}, {"text": "Modifier relations l MOD were deterministically augmented with their syntactic context; both c and l symbols appearing fewer than 10 times in the whole corpus were assigned 'unknown' categories.", "labels": [], "entities": [{"text": "MOD", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.7625002264976501}]}, {"text": "These lexicalized models served as a baseline, but the augmented trees from which they were derived were also inputs to the EM algorithm in Section 3.1.", "labels": [], "entities": []}, {"text": "Each parameter in the model or training algorithm was examined, with \ud97b\udf59I\ud97b\udf59 = {1, 5, 10, 15, 20} clusters, random initialization from reproducible seeds, and a varying numbers of EM iterations.", "labels": [], "entities": [{"text": "\ud97b\udf59I", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.7893681228160858}]}, {"text": "The implemented parser had few adjustments from a plain CKY parser other than these vectors.", "labels": [], "entities": []}, {"text": "No approximate inference was used, with no beam for candidate parses and no re-ranking.", "labels": [], "entities": []}, {"text": "shows example clusters for one of the headword models used, where EM clustered 1,000 headwords into 10 concepts in 10 iterations.", "labels": [], "entities": []}, {"text": "The lists are parts of th\u00ea P \u03b8 H (h \u03b3 \ud97b\udf59 lci \u03b3 ) model.", "labels": [], "entities": []}, {"text": "As such, each of the 10 clusters will only produce headwords in light of some syntactic constituent.", "labels": [], "entities": []}, {"text": "The figure shows how distributed concepts produce headwords for transitive past-tense verbs.", "labels": [], "entities": []}, {"text": "Note that the probability distributions for different headwords are quite uneven, again confirming that some clusters are more specific, and others are more general.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: a) Unsmoothed lexicalized CKY parsers versus 10 semantic clusters. Evaluations were run  with EM trained to 10 iterations. b) Average dependence of parsing performance on number of semantic  clusters. Averages are taken over different random seeds, with EM running 4 or 10 iterations.", "labels": [], "entities": []}]}