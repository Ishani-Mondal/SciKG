{"title": [{"text": "Machine Reading Between the Lines: A Simple Evaluation Framework for Extracted Knowledge Bases", "labels": [], "entities": [{"text": "Machine Reading Between the Lines", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8591843485832215}]}], "abstractContent": [{"text": "The traditional method to evaluate a knowledge extraction system is to measure precision and recall.", "labels": [], "entities": [{"text": "knowledge extraction", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7244179546833038}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9987976551055908}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9980741739273071}]}, {"text": "But this method only partially measures the quality of a knowledge base (KB) as it cannot predict whether a KB is useful or not.", "labels": [], "entities": []}, {"text": "One of the ways in which a KB can be useful is if it is able to deduce implicit information from text which standard information extraction techniques cannot extract.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7514894008636475}]}, {"text": "We propose a novel, simple evaluation framework called \"Machine Reading between the Lines\" (MRbtL) which measures the usefulness of extracted KBs by determining how much they can help improve a relation extraction system.", "labels": [], "entities": [{"text": "Machine Reading between the Lines\" (MRbtL", "start_pos": 56, "end_pos": 97, "type": "TASK", "confidence": 0.7397430092096329}, {"text": "relation extraction", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.7866175174713135}]}, {"text": "In our experiments, we compare two KBs which both have high precision and recall according to annotators who evaluate the knowledge in the KBs independently from any application or context.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9990895986557007}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9994354844093323}]}, {"text": "But, we show that one outperforms the other in terms of MRbtL experiments, as it can accurately deduce more new facts from the output of a relation extractor more accurately.", "labels": [], "entities": [{"text": "MRbtL", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.6205734014511108}]}, {"text": "In short, one extracted KB can read between the lines to identify extra information , whereas the other one cannot.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating knowledge bases (KBs), and especially extracted KBs can be difficult.", "labels": [], "entities": []}, {"text": "Researchers typically measure the accuracy of extracted KBs by measuring precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9991471767425537}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9996644258499146}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9985407590866089}]}, {"text": "But this only partially measures the value of a KB.", "labels": [], "entities": []}, {"text": "Size and correctness are important intrinsic measures, but a KB that states \"1 is an integer, 2 is an integer, . .", "labels": [], "entities": [{"text": "Size", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8806410431861877}, {"text": "correctness", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9765470027923584}]}, {"text": "\" contains an infinite number of correct facts, but is not very useful for most tasks.", "labels": [], "entities": []}, {"text": "Researchers have proposed a variety of applications as testbeds for evaluating the usefulness of knowledge bases, and the Recognizing Textual Entailment Challenge () has received increasing attention as an interesting testbed.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment Challenge", "start_pos": 122, "end_pos": 162, "type": "TASK", "confidence": 0.6824911162257195}]}, {"text": "However, evaluating a knowledge base on RTE requires implementing a functioning RTE system, which is in itself a nontrivial task.", "labels": [], "entities": []}, {"text": "Furthermore, even if a particular kind of knowledge could be useful for RTE, it may not help improve an RTE system's score unless all of the other knowledge required for the complex inferences in this task are already present.", "labels": [], "entities": [{"text": "RTE", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9812294244766235}]}, {"text": "In short, an effective KB evaluation framework is one that: \u2022 is easy to implement \u2022 is able to measure a KB's utility on a valued application such as relation extraction In response, we propose a task called \"Machine Reading between the Lines\" (MRbtL).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.7500064074993134}, {"text": "Machine Reading between the Lines\" (MRbtL)", "start_pos": 210, "end_pos": 252, "type": "TASK", "confidence": 0.8174036343892416}]}, {"text": "In this task, a relation extraction system first extracts abase set of facts from a corpus.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.810613751411438}]}, {"text": "An extracted KB is then used to deduce new facts from the output of the relation extractor.", "labels": [], "entities": []}, {"text": "The KB is evaluated on the precision and amount of \"new\" facts that can be inferred.", "labels": [], "entities": [{"text": "KB", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7736713290214539}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9995487332344055}]}, {"text": "We also argue that MRbtL evaluation is more rigorous than asking an annotator to evaluate the usefulness of a stand-alone piece of knowledge, because it forces the annotator to consider the application of the knowledge in a specific context.", "labels": [], "entities": [{"text": "MRbtL evaluation", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6129966825246811}]}, {"text": "In addition, success on MRbtL provides an immediate benefit to relation extraction, an area which many NLP practitioners care about.", "labels": [], "entities": [{"text": "MRbtL", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.7504338622093201}, {"text": "relation extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.9432371258735657}]}], "datasetContent": [{"text": "We perform MRbtL experiments on extractions from S10 and HYPER.", "labels": [], "entities": [{"text": "MRbtL", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.7016373872756958}, {"text": "HYPER", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.819056510925293}]}, {"text": "S10 uses a dataset of 40 actions from the lexical units in the frames that inherit from the Transitive action frame in FrameNet.", "labels": [], "entities": []}, {"text": "The document collection has 15,088 documents downloaded from the Web for the 40 action words.", "labels": [], "entities": []}, {"text": "We use the annotated Web corpus for HYPER with semantic role information.", "labels": [], "entities": []}, {"text": "We measure the quality of our implicit extractions by taking a random sample of 100 and having two judges classify each extraction for accuracy and redundancy (as per the definitions in Sec 3) in the context of the sentence and document from which it was extracted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9985882639884949}]}, {"text": "As per our earlier work, the pre- cision of S10 is 93% at 85% recall, whereas for HYPER the precision is 89% at 90% recall.", "labels": [], "entities": [{"text": "pre- cision of S10", "start_pos": 29, "end_pos": 47, "type": "METRIC", "confidence": 0.8068151354789734}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9980868101119995}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9988014698028564}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9960054755210876}]}, {"text": "At a first glance, both of the systems look impressive to someone by just looking at the precision/recall numbers.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9992644190788269}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.8257896900177002}]}, {"text": "shows the results of our Machine Reading between the Lines experiment.", "labels": [], "entities": [{"text": "Machine Reading between the Lines", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.846479308605194}]}, {"text": "These extractions are based on 15,000 occurrences of the 40 action words, but as we scale the extractors to new action words, we should increasingly be able to read between the lines of texts.", "labels": [], "entities": []}, {"text": "Hence, we observe that even when both S10 and HYPER report similar (and high) precision and recall, they report significantly different scores on MRbtL experiments.", "labels": [], "entities": [{"text": "HYPER", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9373013377189636}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9989199638366699}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9965464472770691}]}, {"text": "From, we clearly see that HY-PER outperforms S10.", "labels": [], "entities": [{"text": "HY-PER", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9263294339179993}]}, {"text": "HYPER's implicit extractions are nearly 30% more accurate than S10's, and roughly half as redundant.", "labels": [], "entities": []}, {"text": "Extrapolating from the accuracy and redundancy scores in the evaluated sample, HYPER extracts 41,659 correct, nonredundant relationships compared with 7602 extractions for S10 from the Web corpus that does not appear explicitly in the documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9991046786308289}]}, {"text": "Example extractions indicate that HYPER's stronger performance on MRbtL is because its extracted pre and postconditions generalize to hypernyms.", "labels": [], "entities": [{"text": "HYPER", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9833747148513794}, {"text": "MRbtL", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8905898928642273}]}, {"text": "From the sentence \"Doctors need to heal patients..\", HYPER extracts medical practitioner(doctors) indicating that doctors are of type medical practitioner which is an accurate and non-redundant extraction.", "labels": [], "entities": []}, {"text": "Here, medical practitioner is a precondition for action heal.", "labels": [], "entities": []}, {"text": "But S10 concludes that doctors are of type doctor (a Wordnet subclass of medical practitioner) which is a redundant extraction.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9487466216087341}]}, {"text": "Another example: from the sentence \"When a sharp object, like a fingernail or thorn, scrapes along your skin . .", "labels": [], "entities": []}, {"text": "\", the MRbtL system extracted that the fingernail is an object, since the instrument of a scraping action needs to bean object.", "labels": [], "entities": [{"text": "MRbtL", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8871461153030396}]}, {"text": "Both annotators considered this extraction correct, but redundant, since the sentence explicitly mentions that a fingernail is a kind of object.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The knowledge base extracted by HYPER", "labels": [], "entities": [{"text": "HYPER", "start_pos": 42, "end_pos": 47, "type": "TASK", "confidence": 0.4553973078727722}]}]}