{"title": [{"text": "Automatic Acquisition of Huge Training Data for Bio-Medical Named Entity Recognition", "labels": [], "entities": [{"text": "Bio-Medical Named Entity Recognition", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.6485911384224892}]}], "abstractContent": [{"text": "Named Entity Recognition (NER) is an important first step for BioNLP tasks, e.g., gene normalization and event extraction.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.763730525970459}, {"text": "gene normalization", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.8209840953350067}, {"text": "event extraction", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.7847004234790802}]}, {"text": "Employing supervised machine learning techniques for achieving high performance recent NER systems require a manually annotated corpus in which every mention of the desired semantic types in a text is annotated.", "labels": [], "entities": []}, {"text": "However, great amounts of human effort is necessary to build and maintain an annotated corpus.", "labels": [], "entities": []}, {"text": "This study explores a method to build a high-performance NER without a manually annotated corpus, but using a comprehensible lexical database that stores numerous expressions of semantic types and with huge amount of unanno-tated texts.", "labels": [], "entities": []}, {"text": "We underscore the effectiveness of our approach by comparing the performance of NERs trained on an automatically acquired training data and on a manually annotated corpus .", "labels": [], "entities": [{"text": "NERs", "start_pos": 80, "end_pos": 84, "type": "TASK", "confidence": 0.9652552604675293}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is the task widely used to detect various semantic classes such as genes (), proteins (), and diseases in the biomedical field.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7708063125610352}]}, {"text": "A na\u00edve approach to NER handles the task as a dictionary-matching problem: Prepare a dictionary (gazetteer) containing textual expressions of named entities of specific semantic types.", "labels": [], "entities": [{"text": "NER", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9628494381904602}]}, {"text": "Scan an input text, and recognize a text span as a named entity if the dictionary includes the expression of the span.", "labels": [], "entities": []}, {"text": "Although this approach seemingly works well, it presents some critical issues.", "labels": [], "entities": []}, {"text": "First, the dictionary must be comprehensive so that every NE mention can be found in the dictionary.", "labels": [], "entities": []}, {"text": "This requirement for dictionaries is stringent because new terminology is being produced continuously, especially in the biomedical field.", "labels": [], "entities": []}, {"text": "Second, this approach might suffer from an ambiguity problem in which a dictionary includes an expression as entries for multiple semantic types.", "labels": [], "entities": []}, {"text": "For this reason, we must use the context information of an expression to make sure that the expression stands for the target semantic type.", "labels": [], "entities": []}, {"text": "reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) () and Conditional Random Field (CRF)) to NER, which can address these issues.", "labels": [], "entities": []}, {"text": "In this approach, NER is formalized as a classification problem in which a given expression is classified into a semantic class or other (non-NE) expressions.", "labels": [], "entities": [{"text": "NER", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9709463119506836}]}, {"text": "Because the classification problem is usually modeled using supervised learning methods, we need a manually annotated corpus for training NER classifier.", "labels": [], "entities": [{"text": "NER classifier", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.8435193598270416}]}, {"text": "However, preparing manually annotated corpus fora target domain of text and semantic types is cost-intensive and time-consuming because human experts are needed to reliably annotate NEs in text.", "labels": [], "entities": []}, {"text": "For this reason, manually annotated corpora for NER are often limited to a specific domain and covers a small amount of text.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel method for automatically acquiring training data for NER from a comprehensible lexical database and huge amounts of unlabeled text.", "labels": [], "entities": [{"text": "NER", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.945500373840332}]}, {"text": "This paper presents four contribu- tions: 1.", "labels": [], "entities": []}, {"text": "We show the ineffectiveness of a na\u00edve dictionary-matching for acquiring a training data automatically and the significance of the quality of training data for supervised NERs 2.", "labels": [], "entities": [{"text": "NERs", "start_pos": 171, "end_pos": 175, "type": "TASK", "confidence": 0.8990083932876587}]}, {"text": "We explore the use of reference information that bridges the lexical database and unlabeled text for acquiring high-precision and low-recall training data 3.", "labels": [], "entities": []}, {"text": "We develop two strategies for expanding NE annotations, which improves the recall of the training data 4.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9992386102676392}]}, {"text": "The proposed method acquires a large amount of high-quality training data rapidly, decreasing the necessity of human efforts", "labels": [], "entities": []}], "datasetContent": [{"text": "As a preliminary experiment, we acquired training data using a na\u00edve dictionary-matching approach.", "labels": [], "entities": []}, {"text": "We obtained the training data from all 2009 MED-LINE abstracts with an all gene and protein dictionary in Entrez Gene.", "labels": [], "entities": [{"text": "Entrez Gene", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.9547869861125946}]}, {"text": "The training data consisted of nine hundred million tokens.", "labels": [], "entities": []}, {"text": "We constructed a NER classifier using only four million tokens of the training data because of memory limitations.", "labels": [], "entities": [{"text": "NER classifier", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7645551860332489}]}, {"text": "For evaluation, we used the Epigenetics and Post-translational Modification (EPI) corpus    NER on four measures: Accuracy (a), Precision (P), Recall (R), and F1-measure (F1).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9981251358985901}, {"text": "Precision (P)", "start_pos": 128, "end_pos": 141, "type": "METRIC", "confidence": 0.9235988110303879}, {"text": "Recall (R)", "start_pos": 143, "end_pos": 153, "type": "METRIC", "confidence": 0.9424100965261459}, {"text": "F1-measure (F1)", "start_pos": 159, "end_pos": 174, "type": "METRIC", "confidence": 0.8949932307004929}]}, {"text": "We used the strict matching criterion that a predicted named entity is correct if and only if the left and the right boundaries are both correct.", "labels": [], "entities": []}, {"text": "presents the evaluation results of this experiment.", "labels": [], "entities": []}, {"text": "The first model \"dictionary matching\" performs exact dictionary-matching on the test corpus.", "labels": [], "entities": [{"text": "dictionary matching", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7537532448768616}]}, {"text": "It achieves a 40.78 F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.994046688079834}]}, {"text": "The second model \"trained on acquired data\" uses the training data acquired automatically for constructing NER classifier.", "labels": [], "entities": [{"text": "NER classifier", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.6803925633430481}]}, {"text": "It scores very low-performance (14.27 F1-score), even compared with the simple dictionarymatching NER.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9977176189422607}]}, {"text": "Exploring the annotated training data, we investigate why this machine learning approach shows extremely low performance.", "labels": [], "entities": []}, {"text": "presents an example of the acquired training data.", "labels": [], "entities": []}, {"text": "The word \"AM\" in the example (a) is correct because it is gene name, although \"AM\" in the example (b) is incorrect because \"AM\" in (b) is the abbreviation of ante meridiem, which means before noon.", "labels": [], "entities": [{"text": "AM", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.906997561454773}]}, {"text": "This is a very common problem, especially with abbreviations and acronyms.", "labels": [], "entities": []}, {"text": "If we use this noisy training data for learning, then the result of NER might below because of such ambiguity.", "labels": [], "entities": [{"text": "NER", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.6349478363990784}]}, {"text": "It is very difficult to resolve errors in the training data even with the help of machine learning methods.", "labels": [], "entities": []}, {"text": "The training data automatically generated using the proposed method have about 48,000,000 tokens and 3,000,000 gene mentions.", "labels": [], "entities": []}, {"text": "However, we used only about 10% of this data because of the computational cost.", "labels": [], "entities": []}, {"text": "For evaluation, we chose to use the BioNLP 2011 Shared Task EPI corpus and evaluation measures described in Section 2.1.", "labels": [], "entities": [{"text": "BioNLP 2011 Shared Task EPI corpus", "start_pos": 36, "end_pos": 70, "type": "DATASET", "confidence": 0.9142683347066244}]}, {"text": "In the previous section, we proposed three methods for automatic training data acquisition.", "labels": [], "entities": [{"text": "automatic training data acquisition", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.6051132827997208}]}, {"text": "We first investigate the effect of these methods on the performance of NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9587884545326233}]}, {"text": "The first method \"dictionary matching\" simply performs exact string matching with the Entrez Gene dictionary on the evaluation corpus.", "labels": [], "entities": [{"text": "dictionary matching", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.724528580904007}, {"text": "Entrez Gene dictionary", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8115614851315817}]}, {"text": "It achieves a 40.78 F1-measure; this F1-measure will be used as the baseline performance.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9946720004081726}, {"text": "F1-measure", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9964221715927124}]}, {"text": "The second method, as described in Section 2.1, \"svm\" uses training data generated automatically from the Entrez Gene and unlabeled texts without reference information of the Entrez Gene.", "labels": [], "entities": [{"text": "Entrez Gene", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.886457085609436}]}, {"text": "The third method, \"+ reference\" exploits the reference information of the Entrez Gene.", "labels": [], "entities": [{"text": "Entrez Gene", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9017125070095062}]}, {"text": "This method drastically improves the performance.", "labels": [], "entities": []}, {"text": "As shown in, this model achieves the highest precision (69.25%) with comparable recall (39.12%) to the baseline model with a 50.00 F1-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9995591044425964}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.999396562576294}, {"text": "F1-measure", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9933310151100159}]}, {"text": "The fourth method, \"+ coordination\", uses coordination analysis results to expand the initial automatic annotation.", "labels": [], "entities": []}, {"text": "Compared to the \"+ reference\" model, the annotation expansion based on coordination analysis greatly improves the recall (+8.32%) with only a slight decrease of the precision (-2.46%).", "labels": [], "entities": [{"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9997193217277527}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9994807839393616}]}, {"text": "The last method \"+ self-training\" applies a self-training technique to improve the performance further.", "labels": [], "entities": []}, {"text": "This model achieves the highest recall (51.18%) among all models with a reasonable cost in the precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9997486472129822}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9989737272262573}]}, {"text": "To analyze the effect of self-training, we evaluated the performance of this model for each iteration.", "labels": [], "entities": []}, {"text": "shows the F1-measure of the model as iterations increase.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9989713430404663}]}, {"text": "It did not converge even for the last iteration.", "labels": [], "entities": []}, {"text": "The size of the training data at the 17th iteration was used in experiment.", "labels": [], "entities": []}, {"text": "It is the same to the size of the training data for other methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of features used in machine learning  process.", "labels": [], "entities": []}, {"text": " Table 2: Results of the preliminary experiment.", "labels": [], "entities": []}]}