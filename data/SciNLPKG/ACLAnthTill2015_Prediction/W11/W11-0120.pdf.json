{"title": [{"text": "Measuring the semantic relatedness between words and images", "labels": [], "entities": []}], "abstractContent": [{"text": "Measures of similarity have traditionally focused on computing the semantic relatedness between pairs of words and texts.", "labels": [], "entities": [{"text": "computing the semantic relatedness between pairs of words and texts", "start_pos": 53, "end_pos": 120, "type": "TASK", "confidence": 0.7226860374212265}]}, {"text": "In this paper, we construct an evaluation framework to quantify cross-modal semantic relationships that exist between arbitrary pairs of words and images.", "labels": [], "entities": []}, {"text": "We study the effectiveness of a corpus-based approach to automatically derive the semantic relatedness between words and images, and perform empirical evaluations by measuring its correlation with human annotators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, a large body of research in natural language processing has focused on formalizing word meanings.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6719825466473898}, {"text": "formalizing word meanings", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.8080838322639465}]}, {"text": "Several resources developed to date (e.g., WordNet) have enabled a systematic encoding of the semantics of words and exemplify their usage in different linguistic frameworks.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9560205340385437}]}, {"text": "As a result of this formalization, computing semantic relatedness between words has been possible and has been used in applications such as information extraction and retrieval, query reformulation, word sense disambiguation, plagiarism detection and textual entailment.", "labels": [], "entities": [{"text": "computing semantic relatedness between words", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.7522939264774322}, {"text": "information extraction and retrieval", "start_pos": 140, "end_pos": 176, "type": "TASK", "confidence": 0.8042494654655457}, {"text": "query reformulation", "start_pos": 178, "end_pos": 197, "type": "TASK", "confidence": 0.716700479388237}, {"text": "word sense disambiguation", "start_pos": 199, "end_pos": 224, "type": "TASK", "confidence": 0.689488927523295}, {"text": "plagiarism detection", "start_pos": 226, "end_pos": 246, "type": "TASK", "confidence": 0.729001060128212}, {"text": "textual entailment", "start_pos": 251, "end_pos": 269, "type": "TASK", "confidence": 0.742024153470993}]}, {"text": "In contrast, while research has shown that the human cognitive system is sensitive to visual information and incorporating a dual linguistic-and-pictorial representation of information can actually enhance knowledge acquisition, the meaning of an image in isolation is not welldefined and it is mostly task-specific.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 206, "end_pos": 227, "type": "TASK", "confidence": 0.7137623876333237}]}, {"text": "A given image, for instance, maybe simultaneously labeled by a set of words using an automatic image annotation algorithm, or classified under a different set of semantic tags in the image classification task, or simply draw its meaning from a few representative regions following image segmentation performed in an object localization framework.", "labels": [], "entities": [{"text": "image classification task", "start_pos": 183, "end_pos": 208, "type": "TASK", "confidence": 0.8020392656326294}]}, {"text": "Given that word meanings can be acquired and disambiguated using dictionaries, we can perhaps express the meaning of an image in terms of the words that can be suitably used to describe it.", "labels": [], "entities": []}, {"text": "Specifically, we are interested to bridge the semantic gap () between words and images by exploring ways to harvest the information extracted from visual data in a general framework.", "labels": [], "entities": []}, {"text": "While a large body of work has focused on measuring the semantic similarity of words (e.g.,), or the similarity between images based on image content (e.g.,), very few researchers have considered the measure of semantic relatedness 1 between words and images.", "labels": [], "entities": []}, {"text": "But, how exactly is an image related to a given word?", "labels": [], "entities": []}, {"text": "In reality, quantification of such a crossmodal semantic relation is impossible without supplying it with a proper definition.", "labels": [], "entities": []}, {"text": "Our work seeks to address this challenge by constructing a standard evaluation framework to derive a semantic relatedness metric for arbitrary pairs of words and images.", "labels": [], "entities": []}, {"text": "In our work, we explore methods to build a representation model consisting of a joint semantic space of images and words by combining techniques widely adopted in computer vision and natural language processing, and we evaluate the hypothesis that we can automatically derive a semantic relatedness score using this joint semantic space.", "labels": [], "entities": []}, {"text": "Importantly, we acknowledge that it is significantly harder to decode the semantics of an image, as its interpretation relies on a subjective and perceptual understanding of its visual components.", "labels": [], "entities": []}, {"text": "Despite this challenge, we believe this is a worthy research direction, as many important problems can benefit from the association of image content in relation to word meanings, such as automatic image annotation, image retrieval and classification (e.g., () as well as tasks in the domains of of text-to-image synthesis, image harvesting and augmentative and alternative communication.", "labels": [], "entities": [{"text": "image retrieval and classification", "start_pos": 215, "end_pos": 249, "type": "TASK", "confidence": 0.7383850365877151}, {"text": "text-to-image synthesis", "start_pos": 298, "end_pos": 321, "type": "TASK", "confidence": 0.7573850750923157}, {"text": "image harvesting", "start_pos": 323, "end_pos": 339, "type": "TASK", "confidence": 0.7689320147037506}]}], "datasetContent": [{"text": "For our experiments, we randomly select 167 synsets 3 from ImageNet, covering a wide range of concepts such as plants, mammals, fish, tools, vehicles etc.", "labels": [], "entities": []}, {"text": "We perform a simple pre-processing step using Tree Tagger ( and extract only the nouns.", "labels": [], "entities": []}, {"text": "Multiwords are explicitly recognized as collocations or named entities in the synset.", "labels": [], "entities": []}, {"text": "Not considering part-of-speech distinctions, the vocabulary for synset words is 352.", "labels": [], "entities": []}, {"text": "The vocabulary for gloss words is 777.", "labels": [], "entities": []}, {"text": "The shared vocabulary between them is 251.", "labels": [], "entities": []}, {"text": "There area total of 230,864 images associated with the 167 synsets, with an average of 1383 images per synset.", "labels": [], "entities": []}, {"text": "We randomly select an image for each synset, thus obtaining a set of 167 test images in total.", "labels": [], "entities": []}, {"text": "The technique explained in Section 3 is used to generate visual codewords for each image in this dataset.", "labels": [], "entities": []}, {"text": "Each image is first pre-processed to have a maximum side length of 300 pixels.", "labels": [], "entities": []}, {"text": "Next, SIFT descriptors are obtained by densely sampling the image on 20x20 overlapping patches spaced 10 pixels apart.", "labels": [], "entities": []}, {"text": "K-means clustering is applied on a random subset of 10 million SIFT descriptors to derive a visual vocabulary of 1,000 codewords.", "labels": [], "entities": []}, {"text": "Each descriptor is then quantized into a visual codeword by assigning it to the nearest cluster.", "labels": [], "entities": []}, {"text": "To create the gold-standard relatedness annotation, for each test image, six nouns are randomly selected from its associated synset and gloss words, and six other nouns are again randomly selected from the shared vocabulary words.", "labels": [], "entities": []}, {"text": "In all, we have 167 x 12 = 2004 word-image pairs as our test dataset.", "labels": [], "entities": []}, {"text": "Similar to previous word similarity evaluations, we ask human annotators to rate each pair on a scale of 0 to 10 to indicate their degree of semantic relatedness using the evaluation framework outlined below, with 0 being totally unrelated and 10 being perfectly synonymous with each other.", "labels": [], "entities": []}, {"text": "To ensure quality ratings, for each: A sample of test images with their synset words and glosses : The number in parenthesis represents the numerical association of the word with the image (0-10).", "labels": [], "entities": []}, {"text": "Human annotations reveal different degree of semantic relatedness between the image and words in the synset or gloss.", "labels": [], "entities": []}, {"text": "Finally, the average of all 15 annotations for each word-image pair is taken as its gold-standard relatedness score . Note that only the pairs of images and words are provided to the annotators, and not their synsets and gloss definitions.", "labels": [], "entities": []}, {"text": "The set of standard criteria underlying the cross-modal similarity evaluation framework shown here is inspired by the semantic relations defined in Wordnet.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.9544396996498108}]}, {"text": "These criteria were provided to the human annotators, to help them decide whether a word and an image are related to each other.", "labels": [], "entities": []}, {"text": "(e.g. an image of troops at war vs the word \"gun\") ? Criterion (1) basically tests for synonym relation.", "labels": [], "entities": [{"text": "synonym relation", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.9362848103046417}]}, {"text": "Criteria and are modeled after the hyponymhypernym and meronym-holonym relations in WordNet, which are prevalent among nouns.", "labels": [], "entities": []}, {"text": "Note that none of the criteria is preemptive over the others.", "labels": [], "entities": []}, {"text": "Rather, we provide these criteria as guidelines in a subjective evaluation framework, similar to the word semantic similarity task in.", "labels": [], "entities": []}, {"text": "Importantly, criterion (4) models dissimilar but related concepts, or any other relation that indicates frequent association, while criterion (5) serves to provide additional distinction for pairs of words and images on a higher level of relatedness toward similarity.", "labels": [], "entities": []}, {"text": "In, we show sample images from our test dataset, along with the annotations provided by the human annotators.", "labels": [], "entities": []}, {"text": "Following, who argued that word meanings are graded over their senses, we believe that the meaning of an image is not limited to a set of \"best fitting\" tags, but rather it exists as a distribution over arbitrary words with varying degrees of association.", "labels": [], "entities": []}, {"text": "Specifically, the focus of our experiments is to investigate the correlation between automatic measures of such relatedness scores with respect to human judgments.", "labels": [], "entities": []}, {"text": "To construct the joint semantic space of words and images, we use the SVD described in Section 4 to reduce the number of dimensions.", "labels": [], "entities": []}, {"text": "To build each model, we use the 167 synsets from ImageNet and their associated images (minus the held out test data), hence accounting for 167 latent dimensions.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9427262544631958}]}, {"text": "We first represent the synsets as a collection of documents D, each document containing visual codewords used to describe their associated images as well as textual words extracted from their gloss and synset words.", "labels": [], "entities": []}, {"text": "Thus, computing a cross-modal relatedness distance amounts to comparing the cosine similarity of vectors representing an image to the vector representing a word in the term-document vector space.", "labels": [], "entities": []}, {"text": "Note that, unlike textual words, an image is represented by multiple visual codewords.", "labels": [], "entities": []}, {"text": "Prior to computing the actual cosine distance, we perform a weighted addition of vectors representing each visual codeword for that image.", "labels": [], "entities": []}, {"text": "To illustrate, consider a single document d i , representing the synset \"snail,\" which consists of {cw0, cw555, cw23, cw124, cw876, snail, freshwater, mollusk, spiral, shell}, where cwX represents a particular visual codeword indexed from 0-999 8 , and the textual words are nouns extracted from the associated synset and gloss.", "labels": [], "entities": []}, {"text": "Given a test image I, it can be expressed as a bag of visual codewords {cw 1 , ...", "labels": [], "entities": []}, {"text": "We first represent each visual codeword in I as a vector of length |D| using term-frequency inversedocument-frequency (tf idf ) weighting, e.g., cw k =<0.4*d 1 , 0.2*d 2 , ...", "labels": [], "entities": []}, {"text": ", 0.9*d m >, where m=167, and perform an addition of k such vectors to form a final vector vi . To measure the semantic relatedness between image I and a word w, e.g., \"snail,\" we simply compute the cosine similarity between vi and v w , where v w is also a vector of length |D| calculated using tf idf . This paper seeks answers to the following questions.", "labels": [], "entities": []}, {"text": "First, what is the relation between the discriminability of the visual codewords and their ability to capture semantic relatedness between a word and an image, as compared to the gold-standard annotation by humans?", "labels": [], "entities": []}, {"text": "Second, given the unbalanced dataset of images and words, can we use a relatively small number of visual codewords to derive such semantic relatedness measures reliably?", "labels": [], "entities": []}, {"text": "Third, what is the efficiency of an unsupervised vector semantic model in measuring such relatedness, and is it applicable to large datasets?", "labels": [], "entities": []}, {"text": "Analogous to text-retrieval methods, we measure the discriminability of the visual codewords using two weighting factors.", "labels": [], "entities": []}, {"text": "The first is term-frequency (tf), which measures the number of times a codeword appears in all images fora particular synset, while the second, image-term-frequency (itf), captures the number of images using the codeword in a synset.", "labels": [], "entities": [{"text": "term-frequency (tf)", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.878842681646347}]}, {"text": "For the two weighting schemes, we apply normalization by using the total number of codewords fora synset (for tf weighting) and the total number of images in a synset (for itf weighting).", "labels": [], "entities": []}, {"text": "We are interested to quantify the relatedness for pairs of words and images under two scenarios.", "labels": [], "entities": []}, {"text": "By ranking the 12 words associated with an image in reverse order of their relatedness to the image, we can determine the ability of our models to identify the most related words fora given image (imagecentered).", "labels": [], "entities": []}, {"text": "In the second scenario, we measure the relatedness of words and images regardless of the synset they belong to, thus evaluating the ability of our methods to capture the relatedness between any word and any image.", "labels": [], "entities": []}, {"text": "This allows us to capture the correlation in an (arbitrary-image) scenario.", "labels": [], "entities": []}, {"text": "For the evaluations, we use the Spearman's Rank correlation.", "labels": [], "entities": [{"text": "Spearman's Rank correlation", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.5712337717413902}]}, {"text": "To place our results in perspective, we implemented two baselines and an upper bound for each of the two scenarios above.", "labels": [], "entities": []}, {"text": "The Random baseline randomly assigns ratings to each word-image pair on the same 0 to 10 scale, and then measures the correlation to the human gold-standard.", "labels": [], "entities": []}, {"text": "The Vector-Based (VB) method is a stronger baseline aimed to study the correlation performance in the absence of dimensionality reduction.", "labels": [], "entities": []}, {"text": "As an upper bound, the Inter-Human-Agreement (IHA) measures the correlation of the rating by each annotator against the average of the ratings of the rest of the annotators, averaged over the 167 synsets (for the image-centered scenario) and over the 2004 word-image pairs (for the arbitrary-image scenario).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation of automatically generated scores with human annotations on cross-modal semantic  relatedness, as performed on the ImageNet test dataset of 2004 pairs of word and image. Correlation  figures scoring the highest within a weighting scheme are marked in bold, while those scoring the highest  across weighting schemes and within a visual vocabulary size are underlined.", "labels": [], "entities": [{"text": "ImageNet test dataset of 2004 pairs", "start_pos": 137, "end_pos": 172, "type": "DATASET", "confidence": 0.9371691644191742}]}]}