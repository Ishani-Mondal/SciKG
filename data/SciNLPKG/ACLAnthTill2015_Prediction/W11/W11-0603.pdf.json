{"title": [{"text": "Unsupervised syntactic chunking with acoustic cues: computational models for prosodic bootstrapping", "labels": [], "entities": [{"text": "Unsupervised syntactic chunking", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5755835374196371}]}], "abstractContent": [{"text": "Learning to group words into phrases without supervision is a hard task for NLP systems , but infants routinely accomplish it.", "labels": [], "entities": []}, {"text": "We hypothesize that infants use acoustic cues to prosody, which NLP systems typically ignore.", "labels": [], "entities": []}, {"text": "To evaluate the utility of prosodic information for phrase discovery, we present an HMM-based unsupervised chunker that learns from only transcribed words and raw acoustic correlates to prosody.", "labels": [], "entities": [{"text": "phrase discovery", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.8823191225528717}]}, {"text": "Unlike previous work on unsupervised parsing and chunking, we use neither gold standard part-of-speech tags nor punctuation in the input.", "labels": [], "entities": []}, {"text": "Evaluated on the Switchboard corpus, our model outperforms several baselines that exploit either lexical or prosodic information alone, and, despite producing a flat structure, performs competitively with a state-of-the-art unsupervised lexical-ized parser, with a substantial advantage in precision.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.931605190038681}, {"text": "precision", "start_pos": 290, "end_pos": 299, "type": "METRIC", "confidence": 0.9984655380249023}]}, {"text": "Our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for language-learning infants.", "labels": [], "entities": []}], "introductionContent": [{"text": "Young children routinely learn to group words into phrases, yet computational methods have so far struggled to accomplish this task without supervision.", "labels": [], "entities": []}, {"text": "Previous work on unsupervised grammar induction has made progress by exploiting information such as gold-standard part of speech tags (e.g.) or punctuation (e.g.).", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6948860188325247}]}, {"text": "While this information maybe available in some NLP contexts, our focus here is on the computational problem facing language-learning infants, who do not have access to either part of speech tags or punctuation.", "labels": [], "entities": []}, {"text": "However, infants do have access to certain cues that have not been well explored by NLP researchers focused on grammar induction from text.", "labels": [], "entities": [{"text": "grammar induction from text", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.8206084370613098}]}, {"text": "In particular, we consider the cues to syntactic structure that might be available from prosody (roughly, the structure of speech conveyed through rhythm and intonation) and its acoustic realization.", "labels": [], "entities": []}, {"text": "The idea that prosody provides important initial cues for grammar acquisition is known as the prosodic bootstrapping hypothesis, and is wellestablished in the field of language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 168, "end_pos": 188, "type": "TASK", "confidence": 0.7150864899158478}]}, {"text": "Experimental work has provided strong support for this hypothesis, for example by showing that infants begin learning basic rhythmic properties of their language prenatally and that 9-month-olds use prosodic cues to distinguish verb phrases from nonconstituents (.", "labels": [], "entities": []}, {"text": "However, as far as we know, there has so far been no direct computational evaluation of the prosodic bootstrapping hypothesis.", "labels": [], "entities": []}, {"text": "In this paper, we provide the first such evaluation by exploring the utility of acoustic cues for unsupervised syntactic chunking, i.e., grouping words into non-hierarchical syntactic phrases.", "labels": [], "entities": [{"text": "syntactic chunking", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7202927470207214}, {"text": "grouping words into non-hierarchical syntactic phrases", "start_pos": 137, "end_pos": 191, "type": "TASK", "confidence": 0.7891807754834493}]}, {"text": "Nearly all previous work on unsupervised grammar induction has focused on learning hierarchical phrase structure) or dependency structure (); we are aware of only one previous paper on unsupervised syntactic chunking ().", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6776559154192606}]}, {"text": "Ponvert et al. describe a simple method for chunking that uses only bigram counts and punctuation; when the chunks are combined using a rightbranching structure, the resulting trees achieve unlabeled bracketing precision and recall that is competitive with other unsupervised parsers.", "labels": [], "entities": [{"text": "precision", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.9839528203010559}, {"text": "recall", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.998793363571167}]}, {"text": "The sys-tem's dependence on punctuation renders it inappropriate for addressing the questions we are interested inhere, but its good performance reccommends syntactic chunking as a profitable approach to the problem of grammar induction, especially since chunks can be learned using much simpler models than are needed for hierarchical structure.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 219, "end_pos": 236, "type": "TASK", "confidence": 0.7340788096189499}]}, {"text": "The models used in this paper are all variants of HMMs.", "labels": [], "entities": []}, {"text": "Our baseline models are standard HMMs that learn from either lexical or prosodic observations only; we also consider three types of models (including a coupled HMM) that incorporate both lexical and prosodic observations, but vary the degree to which syntactic and prosodic variables are tied together in the latent structure of the models.", "labels": [], "entities": []}, {"text": "In addition, we compare the use of hand-annotated prosodic information (ToBI annotations) to the use of direct acoustic measures (specifically, duration measures) as the prosodic observations.", "labels": [], "entities": []}, {"text": "All of our models are unsupervised, receiving no bracketing information during training.", "labels": [], "entities": []}, {"text": "The results of our experiments strongly support the prosodic bootstrapping hypothesis: we find that using either ToBI annotations or acoustic measures in addition to lexical observations (i.e., word sequences) vastly improves chunking performance over any source of information alone.", "labels": [], "entities": []}, {"text": "Interestingly, our best results are achieved using a combination of words and acoustic information as input, rather than words and ToBI annotations.", "labels": [], "entities": []}, {"text": "Our best combined model achieves an F-score of 41% when evaluated on the lowest level of syntactic structure in the Switchboard corpus 1 , as compared to 25% fora words-only model and only 3% for an acousticsonly model.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9995737671852112}, {"text": "Switchboard corpus 1", "start_pos": 116, "end_pos": 136, "type": "DATASET", "confidence": 0.8938730756441752}]}, {"text": "Although the combined model's score is still fairly low, additional results suggest that our corpus of transcribed naturalistic speech is significantly more difficult for unsupervised parsing than the written text that is typically used for training.", "labels": [], "entities": []}, {"text": "Specifically, we find that a state-of the-art unsupervised lexicalized parser, the Common Cover Link (CCL) parser, achieves only 38% unlabeled bracketing F-score on our corpus, as compared to published results of 76% on WSJ10 (English) and 59% on Negra10 (German).", "labels": [], "entities": [{"text": "F-score", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9160952568054199}, {"text": "WSJ10", "start_pos": 220, "end_pos": 225, "type": "DATASET", "confidence": 0.9507746696472168}]}, {"text": "Interestingly, we find that when evaluated against full parse trees, our best chunker achieves an F-score comparable to that of CCL despite positing only flat structure.", "labels": [], "entities": [{"text": "F-score", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.998891294002533}]}, {"text": "Before describing our models and experiments in more detail, we first present a brief review of relevant information about prosody and its relationship to syntax, including previous work combining prosody and syntax in supervised parsing systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments were performed on part of the Nite XML Toolkit edition of the Switchboard corpus ().", "labels": [], "entities": [{"text": "Nite XML Toolkit edition of the Switchboard corpus", "start_pos": 46, "end_pos": 96, "type": "DATASET", "confidence": 0.7400626912713051}]}, {"text": "Specifically, we gathered all conversations which have been annotated for syntax, ToBI, and Mississippi State phonetic alignments (which lack punctuation).", "labels": [], "entities": [{"text": "Mississippi State phonetic alignments", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.7758941650390625}]}, {"text": "The syntactic parses, word sequences, and ToBI break indices were handannotated by trained linguists, while the Mississippi State phonetic alignments were automatically produced by a forced alignment of the speech signal to a pronunciation-dictionary based phone sequence, providing an estimate of the beginning and end time of each phone.", "labels": [], "entities": [{"text": "syntactic parses", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7561142146587372}]}, {"text": "A small number of annotation errors (in which the beginning and end times of some phones had been swapped) were corrected by hand.", "labels": [], "entities": []}, {"text": "This corpus has 74 conversations with two sides each.", "labels": [], "entities": []}, {"text": "We split this corpus into an 80%/10%/10% train/dev/test 8 partition by dividing the entire corpus into ten-sentence chunks, assigning the first eight to the training partition, and the ninth and tenth to the dev and test partitions, respectively.", "labels": [], "entities": []}, {"text": "We then removed all sentences containing only one or two words.", "labels": [], "entities": []}, {"text": "Sentences this short have a trivial parse, and are usually formulaic discourse responses (, which may influence their prosody.", "labels": [], "entities": []}, {"text": "The final corpus statistics are presented in.", "labels": [], "entities": []}, {"text": "We use the Penn Treebank parsed version of Switchboard for evaluation.", "labels": [], "entities": [{"text": "Penn Treebank parsed version of Switchboard", "start_pos": 11, "end_pos": 54, "type": "DATASET", "confidence": 0.9336488842964172}]}, {"text": "This version uses a slightly different tokenization from the Mississippi State transcriptions that were used as input to the models, so we transformed the Penn treebank tokenization to agree with the Mississippi State tokenization (primarily by concatenating clitics to their base words-i.e. \"do\" and \"'nt\" into \"don't\"-and splitting multi-word expressions).", "labels": [], "entities": [{"text": "Mississippi State transcriptions", "start_pos": 61, "end_pos": 93, "type": "DATASET", "confidence": 0.8814489245414734}, {"text": "Penn treebank tokenization", "start_pos": 155, "end_pos": 181, "type": "DATASET", "confidence": 0.9485707481702169}]}, {"text": "We also removed all gold-standard nodes spanning only Trace or PUNC (recall that the input to the models did not include punctuation) and collapsed all unary productions.", "labels": [], "entities": []}, {"text": "In all evaluations, we convert our models' output tag sequence to a set of matched brackets by inserting a left bracket preceding each word tagged B tag and aright bracket following each word tagged E.", "labels": [], "entities": []}, {"text": "This procedure occasionally results in a sentence with an unmatched opening bracket.", "labels": [], "entities": []}, {"text": "If the unmatched opening bracket is one word from the end of the sentence, we delete it, otherwise we insert a closing bracket at the end of the sentence.", "labels": [], "entities": []}, {"text": "shows example input sequences together with example output tags and their corresponding bracketings.", "labels": [], "entities": []}, {"text": "Previous work on chunking, most notably the 2000 CONLL shared task (), has defined gold standard chunks that are useful for finding grammatical relations but which do not correspond to any particular linguistic notion.", "labels": [], "entities": []}, {"text": "It is not clear that such chunks should play a role in language acquisition, so instead we evaluate against traditional syntactic constituents from Penn Treebankstyle parses in two different ways.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7247176021337509}, {"text": "Penn Treebankstyle parses", "start_pos": 148, "end_pos": 173, "type": "DATASET", "confidence": 0.9657381176948547}]}, {"text": "Our first evaluation method compares the output of the chunkers to what call clumps, which are just syntactic constituents that span only terminals.", "labels": [], "entities": []}, {"text": "We created our clump goldstandard by taking the parse trees resulting from the preprocessing described above and deleting nodes that span a non-terminal.", "labels": [], "entities": []}, {"text": "ample gold-standard parse tree with the clumps in boxes.", "labels": [], "entities": [{"text": "gold-standard parse tree", "start_pos": 6, "end_pos": 30, "type": "DATASET", "confidence": 0.6864433884620667}]}, {"text": "This evaluation avoids penalizing chunkers for not positing hierarchical structure, but rewards chunkers only for finding very low-level structure.", "labels": [], "entities": []}, {"text": "In the interest of making no a priori assumptions about the kinds of phrases our unsupervised method recovers, we also evaluate our completely flat, nonrecursive chunks directly against the fully recursive parses in the treebank.", "labels": [], "entities": []}, {"text": "To do so, we turn our chunked utterance into a flat tree by simply putting brackets around the entire utterance as in.", "labels": [], "entities": []}, {"text": "This evaluation penalizes chunkers for never positing hierarchical structure, but makes no assumptions about which kinds of phrases ought to be found.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data set statistics", "labels": [], "entities": []}, {"text": " Table 2: Scores for all models, evaluated on clumps. In- put is words (Wds), break indices (BI), and/or acoustics.", "labels": [], "entities": [{"text": "break indices (BI)", "start_pos": 78, "end_pos": 96, "type": "METRIC", "confidence": 0.9049313187599182}]}, {"text": " Table 3: % words in a chunk, % utterances with > 0  chunks, and mean chunk length and chunks per utterance.", "labels": [], "entities": [{"text": "mean chunk length", "start_pos": 65, "end_pos": 82, "type": "METRIC", "confidence": 0.7805512547492981}]}]}