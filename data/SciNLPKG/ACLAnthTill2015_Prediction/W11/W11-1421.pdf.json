{"title": [{"text": "Bilingual Random Walk Models for Automated Grammar Correction of ESL Author-Produced Text", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel noisy channel model for correcting text produced by English as a second language (ESL) authors.", "labels": [], "entities": [{"text": "correcting text produced by English as a second language (ESL) authors", "start_pos": 43, "end_pos": 113, "type": "TASK", "confidence": 0.8954217158831083}]}, {"text": "We model the English word choices made by ESL authors as a random walk across an undirected bipartite dictionary graph composed of edges between English words and associated words in an au-thor's native language.", "labels": [], "entities": []}, {"text": "We present two such models, using cascades of weighted finite-state transducers (wFSTs) to model language model priors, random walk-induced noise, and observed sentences, and expectation maxi-mization (EM) to learn model parameters after Park and Levy (2011).", "labels": [], "entities": []}, {"text": "We show that such models can make intelligent word substitutions to improve grammaticality in an unsu-pervised setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "How do language learners make word choices as they compose text in a language in which they are not fluent?", "labels": [], "entities": []}, {"text": "Anyone who has attempted to learn a foreign language can attest to spending a great deal of time leafing through the pages of a bilingual dictionary.", "labels": [], "entities": []}, {"text": "However, dictionaries, especially those without a wealth of example sentences or accompanying word sense information, can often lead even the most scrupulous of language learners in the wrong direction.", "labels": [], "entities": []}, {"text": "Consider an example: the English noun \"head\" has several senses, e.g. the physical head and the head of an organization.", "labels": [], "entities": []}, {"text": "However, the Japanese atama can only mean the physical head or mind, and likewise shuchou, meaning \"chief,\" can only map to the second sense of head.", "labels": [], "entities": []}, {"text": "A native English speaker and Japanese learner faced with the choice of these two words and no additional explanation of which Japanese word corresponds to which sense is liable to make a mistake on the flip of a coin.", "labels": [], "entities": []}, {"text": "One could of course conceive of more subtle examples where the semantics of a set of choices are not so blatantly orthogonal.", "labels": [], "entities": []}, {"text": "\"Complete\" and \"entire\" are synonyms, but they are not necessarily interchangeable.", "labels": [], "entities": []}, {"text": "\"Complete stranger\" is a common two-word phrase, but \"entire stranger\" sounds completely strange, if not entirely ungrammatical, to the native English speaker, who will correct \"entire\" to \"complete\" in a surprisingly automatic fashion.", "labels": [], "entities": []}, {"text": "Thus, correct word choice in non-native language production is essential not only to the preservation of intended meaning, but also to fluent expression of the correct meaning.", "labels": [], "entities": []}, {"text": "The development of software to correct ESL text is valuable for both learning and communication.", "labels": [], "entities": [{"text": "correct ESL text", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7618311842282613}]}, {"text": "A language learner provided instant grammaticality feedback during self-study is less likely to fall into patterns of misuse, and the comprehension difficulties one may encounter when corresponding with non-native speakers would be ameliorated by an automated system to improve text fluency.", "labels": [], "entities": []}, {"text": "Additionally, since machine-translated text is often ungrammatical, automated grammar correction algorithms can be deployed as part of a machine translation system to improve the quality of output.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.734637975692749}, {"text": "machine translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7385996580123901}]}, {"text": "We propose that word choice production errors on the part of the language learner can be modeled as follows.", "labels": [], "entities": [{"text": "word choice production", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8090000748634338}]}, {"text": "Given an observed word and an undirected bipartite graph with nodes representing words in one of two languages, i.e. English and the sentence author's native tongue, and edges between words in each language and their dictionary translation in the other (see for an example), there exists some function f \u2192 that defines the parameters of a random walk along graph edges, conditioned on the source word.", "labels": [], "entities": []}, {"text": "By composing this graph with a language model prior such as an ngram model or probabilistic context-free grammar, we can \"correct\" an observed sentence by inferring the most likely unobserved sentence from which it originated.", "labels": [], "entities": []}, {"text": "More concretely, given that we know f , we can compute argmax w p(w |w, f, \u03b8), where w is the observed sentence, \u03b8 is the language model, and w is the \"corrected,\" unobserved sentence.", "labels": [], "entities": [{"text": "argmax w p", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9501006205876669}]}, {"text": "Under this view, some w drawn from the distribution \u03b8 is subjected to some noise process f , which perturbs the sentence author's intended meaning and outputs w.", "labels": [], "entities": []}, {"text": "Together, these constitute a noisy channel model from information theory.", "labels": [], "entities": [{"text": "information theory", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8544655442237854}]}, {"text": "All that remains is to learn an appropriate f , for which we will employ unsupervised methods, namely expectation maximization.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we will discuss related work.", "labels": [], "entities": []}, {"text": "In Section 3, we will present the implementation, methodology and results of two experiments with different f . In Section 4, we will discuss our experimental results, and we will conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present the results of two experiments with different random walk parametrizations.", "labels": [], "entities": []}, {"text": "We begin by describing our dataset, then proceed to an overview of our model and experimental procedures, and finally detail the experiments themselves.", "labels": [], "entities": []}, {"text": "We use the dataset of Park and Levy (2011), a collection of approximately 25,000 essays comprised of 478,350 sentences scraped from web postings made by Korean ESL students studying for the Test of English as a Foreign Language (TOEFL).", "labels": [], "entities": []}, {"text": "Of these, we randomly select 10,000 sentences for training, 504 as a development set, and 1017 held out for final model evaluation.", "labels": [], "entities": []}, {"text": "Our English-Korean dictionary is scraped from http://endic2009.naver.com, a widelyused and trusted online dictionary source in South Korea.", "labels": [], "entities": []}, {"text": "We are unfortunately unaware of any freely available, downloadable English-Korean dictionary databases.", "labels": [], "entities": []}, {"text": "The most probable unobserved sentence w from which the observed sentence w was generated under our model, argmax w p(w |\u03b8)p(w|w , f, \u03b8), can be read off from the input of the transducer produced during the decoding process.", "labels": [], "entities": [{"text": "argmax w p", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9488295118014017}]}, {"text": "In order to evaluate its quality versus the observed ESL sentence, we use the METEOR 2 and BLEU evaluation metrics for machine translation).", "labels": [], "entities": [{"text": "METEOR 2", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9590706527233124}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9929515719413757}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.6960296928882599}]}, {"text": "This evaluation is performed using a set of human-corrected sentences gathered via Amazon Mechanical Turk, an online service where workers are paid to perform a short task, and further filtered for correctness by an undergraduate research assistant.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.8962459365526835}]}, {"text": "8 workers were assigned to correct each sentence from the development and evaluation sets described in Section 3.1, and so after filtering we had 8 or fewer unique corrected versions per sentence available for evaluation.", "labels": [], "entities": []}, {"text": "We note that the use of METEOR and BLEU is justified inasmuch as the process of grammar correction is translation from an ungrammatical \"language\" to a grammatical one (Park and Levy, 2011).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9432072639465332}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9960100650787354}, {"text": "grammar correction", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7391144335269928}]}, {"text": "However, it is far from perfect, as we shall see shortly.", "labels": [], "entities": []}, {"text": "While human evaluation is far too costly to attempt at every step during development, it is very worthwhile to examine our corrections through a human eye for final evaluation, especially given the somewhat tenuous suitability of METEOR and BLEU for our evaluation task.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 230, "end_pos": 236, "type": "METRIC", "confidence": 0.846929132938385}, {"text": "BLEU", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.995636522769928}]}, {"text": "In order to facilitate this, we designed a simple task, again using Amazon Mechanical Turk, where native English speakers are presented with side-by-side ESL and corrected sentences and asked to choose which is more correct.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.9126491943995158}]}, {"text": "Workers are instructed to \"judge whether the corrected sentence improves the grammaticality and/or fluency of the ESL sentence without changing the ESL sentence's basic meaning.\"", "labels": [], "entities": []}, {"text": "They are then presented with two questions per sentence pair: 1.", "labels": [], "entities": []}, {"text": "Question: \"Between the two sentences listed above, which is more correct?\"", "labels": [], "entities": []}, {"text": "Answer choices: \"ESL sentence is more correct,\" \"Corrected sentence is more correct,\" \"Both are equally correct,\" and, \"The sentences are identical.\"", "labels": [], "entities": []}, {"text": "2. Question: \"Is the meaning of the corrected sentence significantly different from that of the ESL sentence?\"", "labels": [], "entities": []}, {"text": "Answer choices: \"Yes, the two sentences do not mean the same thing,\" and, \"No, the two sentences have roughly the same meaning.\"", "labels": [], "entities": []}, {"text": "Each task is 10 sentences long, 3 of which are identical filler sentences.", "labels": [], "entities": []}, {"text": "When a worker mislabels more than one sentence as identical in any single task, the results for that task are thrown out and resubmitted for another worker to complete.", "labels": [], "entities": []}, {"text": "We additionally require that each sentence pair be judged by 5 unique, U.S.-based workers.", "labels": [], "entities": []}, {"text": "Motivation and Noise Model For our first experiment, we assume that the probability of arriving at some word w \ud97b\udf59 = w after a random walk of length 2 from an observed word w is uniform across all w.", "labels": [], "entities": []}, {"text": "This is perhaps not the most plausible model, but it serves as a baseline by which we can evaluate more complex models.", "labels": [], "entities": []}, {"text": "More concretely, we use a single parameter \u03bb modeling the probability of walking two steps along the dictionary graph from an observed English word w to its Korean definition(s), and then back to some other English word w \ud97b\udf59 = w.", "labels": [], "entities": []}, {"text": "Since we treat unobserved words as transducer input and observed words as output, \u03bb is normalized by |{w|w \ud97b\udf59 = w }|, i.e. the number of edges with different input and output per input word, and p(w|w) = 1 \u2212 \u03bb such that w p(w|w ) = 1.", "labels": [], "entities": []}, {"text": "Motivation and Noise Model For our second experiment, we hypothesize that there is an inverse relationship between unobserved word frequency and random walk path probability.", "labels": [], "entities": []}, {"text": "We motivate this by observing that when a language learner produces a common word, it is likely that she either meant to use that word or used it in place of a rarer word that she did not know.", "labels": [], "entities": []}, {"text": "Likewise, when she uses a rare word, it is likely that she chose it above any of the common words that she knows.", "labels": [], "entities": []}, {"text": "If the word that she chose was erroneous, then, it is most likely that she did not mean to use a common word but could have meant to use a different rare word with a subtle semantic difference.", "labels": [], "entities": []}, {"text": "Hence, we should always prefer to replace observed words, regardless of their frequency, with rare words unless the language model overwhelmingly prefers a common word.", "labels": [], "entities": []}, {"text": "In order to model this hypothesis, we introduce a second parameter \u03b1 < 0 to which power the unigram frequency of each unobserved word w , freq(w ), is raised.", "labels": [], "entities": [{"text": "freq", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9596765041351318}]}, {"text": "The resulting full model is p(w|w ) w\ud97b\udf59 =w = freq(w ) \u03b1 \u03bb |{w|w\ud97b\udf59 =w }| and p(w|w) = 1 \u2212 freq(w) \u03b1 \u03bb.", "labels": [], "entities": []}, {"text": "We approximate the full model to simple coin flips by bucketing the unique word frequencies from the language model and initializing each bucket using its average frequency and some appropriate initial values of \u03b1 and \u03bb, leaving us with a number of parameters equal to the number of frequency buckets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table  2. We discuss these in greater detail in Section 4.", "labels": [], "entities": []}, {"text": " Table 1: METEOR and BLEU scores for all experiments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9937872886657715}, {"text": "BLEU scores", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9686388373374939}]}, {"text": " Table 2: Final parameter values after 10 iterations for Ex- periment 1 with 5 and 10 word random walk candidate  limits.", "labels": [], "entities": []}, {"text": " Table 3: Final parameter values after 10 iterations for Ex- periment 2 with 5 and 10 word random walk candidate  limits.", "labels": [], "entities": []}, {"text": " Table 4: Manual judgments of model-corrected sentence  quality between experiments. If all models are in agree- ment, a sentence is marked as Same, and Different oth- erwise. We judge a set of sentence corrections as Good  if all of the corrections made between models improve  sentence grammaticality, Harmless if the corrections do  not significantly improve or reduce grammaticality, and  Bad if at least one of the corrections is either ungram- matical or changes the sentence meaning. Only corrected  sentences are listed.", "labels": [], "entities": [{"text": "Harmless", "start_pos": 304, "end_pos": 312, "type": "METRIC", "confidence": 0.9563431739807129}]}]}