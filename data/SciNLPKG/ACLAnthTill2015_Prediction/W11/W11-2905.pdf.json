{"title": [{"text": "A Word Clustering Approach to Domain Adaptation: Effective Parsing of Biomedical Texts", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7297024726867676}]}], "abstractContent": [{"text": "We present a simple and effective way to perform out-of-domain statistical parsing by drastically reducing lexical data sparseness in a PCFG-LA architecture.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7152687907218933}]}, {"text": "We replace terminal symbols with unsupervised word clusters acquired from a large newspaper corpus augmented with biomedical target-domain data.", "labels": [], "entities": []}, {"text": "The resulting clusters are effective in bridging the lexical gap between source-domain and target-domain vocabularies.", "labels": [], "entities": []}, {"text": "Our experiments combine known self-training techniques with unsupervised word clustering and produce promising results , achieving an error reduction of 21% on anew evaluation set for biomedical text with manual bracketing annotations.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.7440068423748016}, {"text": "error reduction", "start_pos": 134, "end_pos": 149, "type": "METRIC", "confidence": 0.9844495356082916}]}], "introductionContent": [{"text": "If Natural Language Processing were the Olympics, statistical parsing would be the combination of \"long jump\" and \"100 meters\": a discipline where performance is evaluated in light of raw metric data in a very specific arena.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6488804519176483}, {"text": "statistical parsing", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.828837513923645}]}, {"text": "Leaving aside this far-fetched metaphor, it is a fact that statistical constituent-based parsing has long been subjected to an evaluation process that can almost be qualified as addicted to its own test set.", "labels": [], "entities": [{"text": "statistical constituent-based parsing", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.5815681318442026}]}, {"text": "However, the gap between this intrinsic evaluation methodology, which is only able to provide a ranking of some parser/treebank pairs using a given metric, and the growing need for accurate wide coverage parsers suitable for coping with an unlimited stream of new data, is currently being tackled more widely.", "labels": [], "entities": []}, {"text": "Thus, the task of parsing out-of-domain text becomes crucial.", "labels": [], "entities": [{"text": "parsing out-of-domain text", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.9078591465950012}]}, {"text": "Various techniques have been proposed to adapt existing parsing models to new genres: domain adaptation via self training (, co-training (, treebank and target transformation, source-domain target data matching prior to self-training (, and recently, uptraining techniques (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7917856276035309}]}, {"text": "Although very diverse in practice, these techniques are all designed to overcome the syntactic and lexical gaps that exist between source domain and target domain data.", "labels": [], "entities": []}, {"text": "Interestingly, the lexical gap found for English (Sekine, 1997) can only be wider for out-of-domain parsing of languages that are morphologically richer.", "labels": [], "entities": []}, {"text": "Indeed, the relatively small size of their annotated treebanks and their levels of lexical variation are already a stress case for most statistical parsing models, without adding the extreme challenges caused by lexical out-of-domain variation.", "labels": [], "entities": []}, {"text": "In this paper, we take the PCFG-LA framework (, implemented by, and explore a combination of known self-training techniques with a novel application of unsupervised word clustering () that was successfully used to reduce lexical data sparseness for French parsing).", "labels": [], "entities": [{"text": "PCFG-LA framework", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.9178900718688965}, {"text": "word clustering", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.7232033312320709}, {"text": "French parsing", "start_pos": 249, "end_pos": 263, "type": "TASK", "confidence": 0.6789530217647552}]}], "datasetContent": [{"text": "For our parsing experiments, we used the PCFG-LA algorithm of, implemented by.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9759540557861328}]}, {"text": "The treebank used was the FTB (cf. Section 2).", "labels": [], "entities": [{"text": "FTB", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8864415884017944}]}, {"text": "More precisely, we used the version of the treebank as defined by, which has a 28 POS tagset and some multiword expressions replaced by regular syntactic structures.", "labels": [], "entities": []}, {"text": "We used the standard training (80%), dev (10%), and test (10%) split, containing respectively 9881, 1235 and 1235 sentences from the Le Monde newspaper.", "labels": [], "entities": [{"text": "Le Monde newspaper", "start_pos": 133, "end_pos": 151, "type": "DATASET", "confidence": 0.9347832004229227}]}, {"text": "For unsupervised clustering, we first systematically applied the desinflection process of  freely available at CNRTL 7 . Though this newspaper is less formal than Le Monde, it is still journalistic, so we consider it as being in the source domain.", "labels": [], "entities": [{"text": "CNRTL 7", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9604134261608124}]}, {"text": "The mixed clusters were obtained by concatenating the L'Est R\u00e9publicain corpus and the EmeaFrU (cf. Section 2), herafter ER+EMEA.", "labels": [], "entities": [{"text": "L'Est R\u00e9publicain corpus", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.931710402170817}, {"text": "EmeaFrU", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9511938691139221}]}, {"text": "We did not investigate any weighting techniques for building the source corpus for mixed clusters.", "labels": [], "entities": []}, {"text": "On both the ER and ER+EMEA corpora, we ran Brown clustering with 1000 clusters for the desinflected forms appearing at least 60 times.", "labels": [], "entities": [{"text": "ER+EMEA corpora", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.5861242264509201}]}, {"text": "Having performed desinflection and different types of clustering, we trained PCFG-LA grammars on the FTB training set using four settings for terminal symbols: raw uses original word forms; dfl uses desinflected word forms; clt-er uses clusters of desinflected forms computed over the ER corpus, with a process described in detail by; clt-er-emea is the same as clt-er, but with mixed clusters over the ER+EMEA corpus.", "labels": [], "entities": [{"text": "FTB training set", "start_pos": 101, "end_pos": 117, "type": "DATASET", "confidence": 0.9799808065096537}, {"text": "ER+EMEA corpus", "start_pos": 403, "end_pos": 417, "type": "DATASET", "confidence": 0.6283655762672424}]}, {"text": "Having obtained these initial grammars, we used each to parse the EmeaFrU unlabeled corpus (with appropriate desinflection and clustering preprocessing for each of the four terminal symbol settings).", "labels": [], "entities": [{"text": "EmeaFrU unlabeled corpus", "start_pos": 66, "end_pos": 90, "type": "DATASET", "confidence": 0.9153191447257996}]}, {"text": "We then performed self-training experiments adding up to 200k predicted parses from EmeaFrU to the FTB training set, and training new grammars for each such enlarged training set.", "labels": [], "entities": [{"text": "EmeaFrU", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9547752141952515}, {"text": "FTB training set", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.9707175294558207}]}, {"text": "shows the effect of self-training on parsing the EMEA and FTB dev sets.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9624130725860596}, {"text": "EMEA", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9042078256607056}, {"text": "FTB dev sets", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9015512466430664}]}, {"text": "Unsurprisingly, the baseline parser (raw setting without selftraining) has a 5 point drop in F-measure when parsing the EMEA compared to the FTB.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9987082481384277}, {"text": "FTB", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.9122601747512817}]}, {"text": "Consistent with previous results on English biomedical texts (, self-training helps in parsing the EMEA, with more predicted parses generally leading to better performance on the EMEA (and worse performance on the FTB).", "labels": [], "entities": [{"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9862599968910217}, {"text": "EMEA", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.6987282037734985}, {"text": "EMEA", "start_pos": 179, "end_pos": 183, "type": "DATASET", "confidence": 0.9341858625411987}, {"text": "FTB", "start_pos": 214, "end_pos": 217, "type": "DATASET", "confidence": 0.8528946042060852}]}, {"text": "Concerning the different settings for terminal symbols, for source-domain data we reproduce (right side of) the findings of: parsing desinflected forms (dfl) increases performance, and parsing unsupervised clusters of desinflected forms (clt-er and clter-emea) is even better.", "labels": [], "entities": []}, {"text": "For target-domain data, we find that desinflection does help, and even achieves better performance than source clusters.", "labels": [], "entities": []}, {"text": "This can be explained by the fact that the desinflection process provides forms that still follow French morphology, so the general handling of unknown words (with classes of suffixes) does apply.", "labels": [], "entities": []}, {"text": "In contrast, terminological tokens are (hopefully) more frequently absent from the ER corpus than the ER+EMEA corpus, and they are replaced by the UNK token more often for source clusters (clter) than for mixed clusters (clt-er-emea).", "labels": [], "entities": []}, {"text": "Indeed, for the EMEA dev set, 1,466 tokens are UNK in the clt-er setting, while only 729 tokens are UNK in the clt-er-emea setting.", "labels": [], "entities": [{"text": "EMEA dev set", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.9164442618687948}]}, {"text": "shows final parsing results on the EMEA test set for each of the four terminal symbol settings, with and without self-training (using 200k parses from EmeaFrU).", "labels": [], "entities": [{"text": "EMEA test set", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9715227683385214}, {"text": "EmeaFrU", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.9686286449432373}]}, {"text": "We evaluated using F-Measure on labeled precision and recall, ignoring punctuation, and calculated the significance of differences between settings.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9949894547462463}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9024989008903503}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9979634284973145}]}, {"text": "The clt-er-emea setting gives the best overall performance, with or without self-training.", "labels": [], "entities": []}, {"text": "When comparing clt-eremea with self-training (best overall) to raw without self-training (baseline), we obtain a 21% error", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the EMEA dev and test sets.  alpha-lc stands for tokens converted to lowercase and  containing at least one letter. Unknown tokens/types  are those absent from the FTB training set.", "labels": [], "entities": [{"text": "EMEA dev and test sets", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8857139229774476}, {"text": "FTB training set", "start_pos": 188, "end_pos": 204, "type": "DATASET", "confidence": 0.9653756618499756}]}]}