{"title": [{"text": "University of Illinois System in HOO Text Correction Shared Task", "labels": [], "entities": [{"text": "HOO Text Correction Shared", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.7198161482810974}]}], "abstractContent": [{"text": "In this paper, we describe the University of Illinois system that participated in Helping Our Own (HOO), a shared task in text correction.", "labels": [], "entities": [{"text": "Helping Our Own (HOO)", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.6149737685918808}, {"text": "text correction", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.8302608132362366}]}, {"text": "We target several common errors, such as articles, prepositions, word choice, and punctuation errors, and we describe the approaches taken to address each error type.", "labels": [], "entities": []}, {"text": "Our system is based on a combination of classifiers, combined with adaptation techniques for article and preposition detection.", "labels": [], "entities": [{"text": "article and preposition detection", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.6845744699239731}]}, {"text": "We ranked first in all three evaluation metrics (Detection, Recognition and Correction) among six participating teams.", "labels": [], "entities": [{"text": "Recognition and Correction)", "start_pos": 60, "end_pos": 87, "type": "METRIC", "confidence": 0.6876074820756912}]}, {"text": "We also present type-based scores on preposition and article error correction and demonstrate that our approach achieves best performance in each task.", "labels": [], "entities": [{"text": "preposition and article error correction", "start_pos": 37, "end_pos": 77, "type": "TASK", "confidence": 0.5282653629779815}]}], "introductionContent": [{"text": "The Text Correction task addresses the problem of detecting and correcting mistakes in text.", "labels": [], "entities": [{"text": "Text Correction task", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7621914148330688}, {"text": "detecting and correcting mistakes in text", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.7580448736747106}]}, {"text": "This task is challenging, since many errors are not easy to detect, such as context-sensitive spelling mistakes that involve confusing valid words in a language (e.g. \"there\" and \"their\").", "labels": [], "entities": []}, {"text": "Recently, text correction has taken an interesting turn by focusing on contextsensitive errors made by English as a Second Language (ESL) writers.", "labels": [], "entities": [{"text": "text correction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8497283756732941}]}, {"text": "The HOO shared task () focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community.", "labels": [], "entities": [{"text": "HOO shared task", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5746441384156545}]}, {"text": "This paper presents our entry in the HOO shared task.", "labels": [], "entities": [{"text": "HOO shared task", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7471523284912109}]}, {"text": "We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques for article and preposition detection.", "labels": [], "entities": [{"text": "preposition detection", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.6946983933448792}]}, {"text": "Our system ranked first in all three evaluation metrics.", "labels": [], "entities": []}, {"text": "The description of the evaluation schema and the results of the participating teams can be found in.", "labels": [], "entities": []}, {"text": "We also evaluate the performance of two system components (Sec.", "labels": [], "entities": []}, {"text": "2), those that target article and preposition errors, and compare them to the performance of other teams (Sec. 3).", "labels": [], "entities": []}], "datasetContent": [{"text": "The task evaluation uses three metrics, Detection, Recognition, and Correction.", "labels": [], "entities": [{"text": "Detection", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9109657406806946}, {"text": "Recognition", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.8724384903907776}, {"text": "Correction", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9795656204223633}]}, {"text": "In each metric, Recall, Precision and F-score are computed relative to the total number of edits in the corpus (see fora description of the scoring metrics and for the overall ranking of the individual systems).", "labels": [], "entities": [{"text": "Recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9992146492004395}, {"text": "Precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9974014759063721}, {"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9973788261413574}]}, {"text": "We thought that it would also be interesting to see how the systems compare for two very common error types: articles and prepositions . We have done a comprehensive and slightly different evaluation, computed relative to the edits that involve articles or prepositions, respectively, for each error type . We also evaluate these two tasks by comparing the accuracy of the data before running the system (the \"baseline\") to the accuracy of the data after running the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 357, "end_pos": 365, "type": "METRIC", "confidence": 0.9975971579551697}, {"text": "accuracy", "start_pos": 428, "end_pos": 436, "type": "METRIC", "confidence": 0.9984970092773438}]}, {"text": "This evaluation shows whether the system reduces or increases the number of errors in the 5 show evaluation by error type only for Recall because it is not possible to compute Precision for many other error types.", "labels": [], "entities": [{"text": "Recall", "start_pos": 131, "end_pos": 137, "type": "DATASET", "confidence": 0.5965229272842407}, {"text": "Precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9774076342582703}]}, {"text": "Since it is easy to obtain high recall by proposing many edits (neglecting the precision performance) and, similarly, easy to obtain high precision by just proposing no edits, we present results sorted by F-score rather than by recall and/or precision, as in.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.997424840927124}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9975385665893555}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9947490096092224}, {"text": "F-score", "start_pos": 205, "end_pos": 212, "type": "METRIC", "confidence": 0.9954279661178589}, {"text": "recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9971945285797119}, {"text": "precision", "start_pos": 242, "end_pos": 251, "type": "METRIC", "confidence": 0.99831223487854}]}, {"text": "For the same reason, we also choose the best run of each system based on this measure rather than choosing runs that are doing well just on one of the relevant measures (and, likely very poorly on the other).", "labels": [], "entities": []}, {"text": "6 For articles, we consider all article edits (see).", "labels": [], "entities": []}, {"text": "For prepositions, replacements involving the top 36 most frequent English prepositions are considered; they account for all preposition replacements made by the participating systems.   data.", "labels": [], "entities": []}, {"text": "The accuracy and the baseline are computed as described in and the results are shown in.: Accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997304081916809}, {"text": "baseline", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9536764621734619}, {"text": "Accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9995438456535339}]}, {"text": "\"Baseline\" is the proportion of correct examples in the data.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 1, "end_pos": 9, "type": "METRIC", "confidence": 0.9739497900009155}]}], "tableCaptions": [{"text": " Table 1: System components. The column \"Relative frequency\" shows the the proportion of a given error type in the  pilot data. The category \"Article\" is based on the statistics for determiner errors, the majority of which involve articles.", "labels": [], "entities": [{"text": "Relative frequency", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.9889731705188751}]}, {"text": " Table 2: Adaptation to the typical errors. F-score on  detection on the pilot data. Error statistics are found in  10-fold cross-validation .", "labels": [], "entities": [{"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9996223449707031}, {"text": "Error", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9854224920272827}]}, {"text": " Table 3: Type-based performance: Articles. For each  team, the F-scores for the best run are shown. Results  only shown for the teams that address these errors.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9947097301483154}]}, {"text": " Table 4: Type-based performance: Prepositions. For  each team, the F-scores for the best run are shown.", "labels": [], "entities": [{"text": "Prepositions", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9852249026298523}, {"text": "F-scores", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9936699271202087}]}, {"text": " Table 5: Accuracy results. \"Baseline\" is the proportion  of correct examples in the data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985841512680054}, {"text": "Baseline", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9781234860420227}]}]}