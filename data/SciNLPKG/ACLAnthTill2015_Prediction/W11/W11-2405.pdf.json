{"title": [], "abstractContent": [{"text": "Common evaluation metrics for paraphrase patterns do not necessarily correlate with extrinsic recognition task performance.", "labels": [], "entities": [{"text": "extrinsic recognition task", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.6935443977514902}]}, {"text": "We propose a metric which gives weight to lexical variety in paraphrase patterns; our proposed metric has a positive correlation with paraphrase recognition task performance, with a Pearson correlation of 0.5~0.7 (k=10, with \"strict\" judgment) in a statistically significant level (p-value<0.01).", "labels": [], "entities": [{"text": "paraphrase recognition task", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.8879767060279846}, {"text": "Pearson correlation", "start_pos": 182, "end_pos": 201, "type": "METRIC", "confidence": 0.9792168140411377}]}], "introductionContent": [{"text": "We propose a diversity-aware paraphrase evaluation metric called DIMPLE 1 , which boosts the scores of lexically diverse paraphrase pairs.", "labels": [], "entities": [{"text": "DIMPLE 1", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8678615987300873}]}, {"text": "Paraphrase pairs or patterns are useful in various NLP related research domains, since there is a common need to automatically identify meaning equivalence between two or more texts.", "labels": [], "entities": []}, {"text": "Consider a paraphrase pair resource that links \"killed\" to \"assassinated\" (in the rest of this paper we denote such a rule as \u2329\"killed\" 2 , \"assassinated\" 3 \u232a).", "labels": [], "entities": []}, {"text": "In automatic evaluation for Machine Translation (MT) (;, this rule may enable a metric to identify phrase-level semantic similarity between a system response containing \"killed\", and a reference translation containing \"assassinated\".", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8644782781600953}]}, {"text": "Similarly in query expansion for information retrieval (IR) (, this rule may enable a system to 1 DIversity-aware Metric for Pattern Learning Experiments Source term/phrase that contains \"killed\" Paraphrase that contains \"assassinated\" expand the query term \"killed\" with the paraphrase \"assassinated\", in order to match a potentially relevant document containing the expanded term.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.8664714097976685}]}, {"text": "To evaluate paraphrase patterns during pattern discovery, ideally we should use an evaluation metric that strongly predicts performance on the extrinsic task (e.g. fluency and adequacy scores in MT, mean average precision in IR) where the paraphrase patterns are used.", "labels": [], "entities": [{"text": "pattern discovery", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7306178361177444}, {"text": "mean average precision", "start_pos": 199, "end_pos": 221, "type": "METRIC", "confidence": 0.6284042795499166}]}, {"text": "Many existing approaches use a paraphrase evaluation methodology where human assessors judge each paraphrase pair as to whether they have the same meaning.", "labels": [], "entities": []}, {"text": "Over a set of paraphrase rules for one source term, Expected Precision (EP) is calculated by taking the mean of precision, or the ratio of positive labels annotated by assessors ().", "labels": [], "entities": [{"text": "Expected Precision (EP)", "start_pos": 52, "end_pos": 75, "type": "METRIC", "confidence": 0.9575738072395324}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9538204669952393}]}, {"text": "The weakness of this approach is that EP is an intrinsic measure that does not necessarily predict how well a paraphrase-embedded system will perform in practice.", "labels": [], "entities": [{"text": "EP", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.941926896572113}]}, {"text": "For example, a set of paraphrase pairs \u2329\"killed\", \"shot and killed\"\u232a, \u2329\"killed\", \"reported killed\"\u232a \u2026 \u2329\"killed\", \"killed in\"\u232a may receive a perfect score of 1.0 in EP; however, these patterns do not provide lexical diversity (e.g. \u2329\"killed\", \"assassinated\"\u232a) and therefore may not perform well in an application where lexical diversity is important.", "labels": [], "entities": [{"text": "EP", "start_pos": 164, "end_pos": 166, "type": "METRIC", "confidence": 0.900299072265625}]}, {"text": "The goal of this paper is to provide empirical evidence to support the assumption that the proposed paraphrase evaluation metric DIMPLE correlates better with paraphrase recognition task metric scores than previous metrics do, by rewarding lexical diverse patterns.", "labels": [], "entities": [{"text": "paraphrase recognition task", "start_pos": 159, "end_pos": 186, "type": "TASK", "confidence": 0.8362329602241516}]}], "datasetContent": [{"text": "We use the Pearson product-moment correlation coefficient to measure correlation between two vectors consisting of intrinsic and extrinsic scores on paraphrase patterns, following previous meta-evaluation research.", "labels": [], "entities": [{"text": "Pearson product-moment correlation coefficient", "start_pos": 11, "end_pos": 57, "type": "METRIC", "confidence": 0.7580067738890648}]}, {"text": "By intrinsic score, we mean a theory-based direct assessment result on the paraphrase patterns.", "labels": [], "entities": []}, {"text": "By extrinsic score, we mean to measure how much the paraphrase recognition component helps the entire system to achieve a task.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.713016152381897}]}, {"text": "The correlation score is 1 if there is a perfect positive correlation, 0 if there is no correlation and -1 if there is a perfect negative correlation.", "labels": [], "entities": [{"text": "correlation score", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9842732846736908}]}, {"text": "Using a task performance score to evaluate a paraphrase generation algorithm has been studied previously.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8206250369548798}]}, {"text": "A common issue in extrinsic evaluations is that it is hard to separate out errors, or contributions from other possibly complex modules.", "labels": [], "entities": []}, {"text": "This paper presents an approach which can predict task performance in more simple experimental settings.", "labels": [], "entities": []}, {"text": "Ideally, paraphrase metric scores should correlate well with task performance metrics.", "labels": [], "entities": []}, {"text": "To insulate the experiment from external, uncontrollable factors (e.g. errors from other task components), we created three datasets with slightly different characteristics, where the essential task of recognizing meaning equivalence between different surface texts can be conducted.", "labels": [], "entities": []}, {"text": "The numbers of positive-labeled pairs that we extracted for the three corpus, MSRPC, RTE and CQAE are 3900, 2805 and 27397 respectively.", "labels": [], "entities": [{"text": "MSRPC", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9425727128982544}, {"text": "RTE", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.5333872437477112}, {"text": "CQAE", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8646157383918762}]}, {"text": "We created unique pairs consisting of a system response (often sen-tence-length) and an answer nugget as positive examples, where the system response is judged by human as containing or expressing the meaning of the nugget.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Correlation between intrinsic paraphrase  metrics and extrinsic paraphrase recognition task me- trics where DIMPLE's Q score is based on lenient  judgment. Bold figures indicate statistical significance  of the correlation statistics (null-hypothesis tested:  \"there is no correlation\", p-value<0.01).", "labels": [], "entities": [{"text": "paraphrase recognition task", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7839503884315491}]}, {"text": " Table 3. Same as the Table 2, except that the Q  score is based on strict judgment.", "labels": [], "entities": [{"text": "Q  score", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9845820069313049}]}]}