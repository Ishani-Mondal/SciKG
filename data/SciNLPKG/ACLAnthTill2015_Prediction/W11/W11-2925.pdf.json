{"title": [{"text": "Comparing the Use of Edited and Unedited Text in Parser Self-Training", "labels": [], "entities": []}], "abstractContent": [{"text": "We compare the use of edited text in the form of newswire and unedited text in the form of discussion forum posts as sources for training material in a self-training experiment involving the Brown reranking parser and a test set of sentences from an online sports discussion forum.", "labels": [], "entities": []}, {"text": "We find that grammars induced from the two automatically parsed corpora achieve similar Parseval f-scores, with the grammars induced from the discussion forum material being slightly superior.", "labels": [], "entities": [{"text": "Parseval f-scores", "start_pos": 88, "end_pos": 105, "type": "METRIC", "confidence": 0.9436880648136139}]}, {"text": "An error analysis reveals that the two types of grammars do behave differently.", "labels": [], "entities": []}], "introductionContent": [{"text": "There have been several successful attempts in recent years to employ automatically parsed data in semi-and unsupervised approaches to parser domain adaptation.", "labels": [], "entities": [{"text": "parser domain adaptation", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.8702049255371094}]}, {"text": "We turn our attention to adapting a Wall-Street-Journal-trained parser to user-generated content from an online sports discussion forum.", "labels": [], "entities": []}, {"text": "The sentences on the discussion forum are produced by a group of speakers who are communicating with each other about a shared interest and are discussing the same events, but, who, given the open, unedited nature of the medium itself, do not follow an in-house writing style.", "labels": [], "entities": []}, {"text": "Our particular aim in this paper is to compare the use of discussion forum comments as a source of unlabelled training material to the use of edited, professionally written sentences on the same theme.", "labels": [], "entities": []}, {"text": "We hypothesise that the well-formed sentences will be more suitable as training material since they are likely to be closer syntactically to the source domain Wall Street Journal (WSJ) sentences than the noisier discussion forum sentences, while at the same time, remaining lexically close to the target domain, thus acting as a type of \"self-training bridging corpus\").", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) sentences", "start_pos": 159, "end_pos": 194, "type": "DATASET", "confidence": 0.9141419189316886}]}, {"text": "demonstrate that a WSJtrained parser can be adapted to the fiction domains of the Brown corpus by performing a type of self-training that involves the use of the twostage Brown reranking parser).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.8612141311168671}]}, {"text": "Their training protocol is as follows: sentences from the LA Times are parsed using the first-stage parser) and reranked in the second stage.", "labels": [], "entities": [{"text": "LA Times", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9181235134601593}]}, {"text": "These parse trees are added to the original WSJ training set and the first-stage parser is retrained.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.8593353231747946}]}, {"text": "The sentences from the target domain, in this case, Brown corpus sentences are then parsed using the newly trained first-stage parser and reranked using the original reranker, resulting in a Parseval f-score increase from 85.2% to 87.8%.", "labels": [], "entities": [{"text": "Parseval f-score increase", "start_pos": 191, "end_pos": 216, "type": "METRIC", "confidence": 0.891211728254954}]}, {"text": "later show that the same procedure can be used to adapt a WSJtrained parser to biomedical text.", "labels": [], "entities": []}, {"text": "They also try an experiment which is very similar to the experiment described in this paper.", "labels": [], "entities": []}, {"text": "Instead of using Medline abstracts as training material, they use sentences from a biology textbook under the assumption that the parses produced for these sentences will be more accurate (and thus more reliable as training data) than the sentences in the abstracts since they are closer to the source domain.", "labels": [], "entities": []}, {"text": "They find, however, that the textbook sentences are less effective than the target domain material.", "labels": [], "entities": []}, {"text": "We attempt to repeat the experiment with Web 2.0 data, believing that the two setups are sufficiently different for our experiment to be worthwhile -our bridging corpus is closely related in subject matter to our target corpus (both referring to the same events) but quite different inform (professionally edited versus an unedited mix of writing styles), whereas their bridging corpus is less closely related in con-tent (biology textbooks versus Medline abstracts) and more closely related inform (both professionally edited and syntactically well-formed).", "labels": [], "entities": []}], "datasetContent": [{"text": "We retrain the Brown parser using the selftraining protocol of, that is, we retrain the first-stage parser using combinations of trees produced by the reranking parser for sentences from Sections 2 to 21 of the WSJ section of the Penn Treebank and from FootballTrainEdited|Discussion.", "labels": [], "entities": [{"text": "WSJ section of the Penn Treebank", "start_pos": 211, "end_pos": 243, "type": "DATASET", "confidence": 0.9260840018590292}, {"text": "FootballTrainEdited|Discussion", "start_pos": 253, "end_pos": 283, "type": "DATASET", "confidence": 0.9255412022272745}]}, {"text": "We then parse the sentences in FootballDev using the retrained first-stage parser and the original reranker.", "labels": [], "entities": [{"text": "FootballDev", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9861131310462952}]}, {"text": "Because we have approximately five times the number of sentences in FootballTrainDiscussion than in FootballTrainEdited, we first train five different FootballTrainDiscussion grammars.", "labels": [], "entities": [{"text": "FootballTrainDiscussion", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.989629328250885}, {"text": "FootballTrainEdited", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.9897578954696655}, {"text": "FootballTrainDiscussion grammars", "start_pos": 151, "end_pos": 183, "type": "DATASET", "confidence": 0.9259842038154602}]}, {"text": "The graph in shows the results on FootballDev when the training data contains disjoint subsections of FootballTrainDiscussion, each containing 200,000 sentences, along with varying amount of WSJ2-21 trees.", "labels": [], "entities": [{"text": "FootballDev", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9920377731323242}, {"text": "FootballTrainDiscussion", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.9911885261535645}, {"text": "WSJ2-21 trees", "start_pos": 191, "end_pos": 204, "type": "DATASET", "confidence": 0.8497498333454132}]}, {"text": "This gives us an idea of the amount of variation we might expect within one training set source -the f-score noise is roughly 1.5 points wide (= 3 boxes in the graph).", "labels": [], "entities": []}, {"text": "We now turn to the main experiment of the paper, i.e. the comparison of FootballTrainDiscussion and FootballTrainEdited.", "labels": [], "entities": [{"text": "FootballTrainDiscussion", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.9894463419914246}, {"text": "FootballTrainEdited", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.98964524269104}]}, {"text": "The graph in   grammars on FootballDev.", "labels": [], "entities": [{"text": "FootballDev.", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9793868660926819}]}, {"text": "Note that a baseline grammar which is trained on one copy of WSJ02-21 and no automatically parsed data achieves a Parseval f-score of 79.7.", "labels": [], "entities": [{"text": "WSJ02-21", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9812105894088745}, {"text": "Parseval f-score", "start_pos": 114, "end_pos": 130, "type": "METRIC", "confidence": 0.9273590743541718}]}, {"text": "The results in appear to refute our original hypothesis, suggesting that there is very little difference between the two corpora, with the user-generated content of FootballTrainDiscussion emerging as slightly superior on our development set.", "labels": [], "entities": [{"text": "FootballTrainDiscussion", "start_pos": 165, "end_pos": 188, "type": "DATASET", "confidence": 0.9746436476707458}]}, {"text": "The only time that the FootballTrainEdited curve is above the FootballTrainDiscussion is when the size of the original WSJ training set is restricted.", "labels": [], "entities": [{"text": "FootballTrainEdited", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.8894956707954407}, {"text": "FootballTrainDiscussion", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.9868213534355164}, {"text": "WSJ training set", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.9356570243835449}]}, {"text": "This is an intuitively appealing result -in this scenario, the sentences in the FootballTrainEdited corpus are making up for the lack of WSJ trained material, although it is not clear whether this is because the FootballTrainEdited sentences are slightly longer than the FootballTrainDiscussion sentences (see) or because they contain more WSJ-like constructions.", "labels": [], "entities": [{"text": "FootballTrainEdited corpus", "start_pos": 80, "end_pos": 106, "type": "DATASET", "confidence": 0.9903996586799622}, {"text": "FootballTrainEdited sentences", "start_pos": 212, "end_pos": 241, "type": "DATASET", "confidence": 0.9624367654323578}, {"text": "FootballTrainDiscussion sentences", "start_pos": 271, "end_pos": 304, "type": "DATASET", "confidence": 0.9714226126670837}]}], "tableCaptions": [{"text": " Table 1: Basic Statistics on the Web 2.0 datasets: number of sentences, average sentence length, median sentence  length and standard deviation", "labels": [], "entities": [{"text": "Basic Statistics on the Web 2.0 datasets", "start_pos": 10, "end_pos": 50, "type": "DATASET", "confidence": 0.6252494411809104}]}, {"text": " Table 2: Effect of Self-Training Broken Down by Sentence Length", "labels": [], "entities": []}, {"text": " Table 3: Effect of Self-Training Broken Down by Number of Coordinating Conjunctions in a Sentence", "labels": [], "entities": []}, {"text": " Table 4: Effect of Self-Training Broken Down by Number of Unknown Words in a Sentence", "labels": [], "entities": []}]}