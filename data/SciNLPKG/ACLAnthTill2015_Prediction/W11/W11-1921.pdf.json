{"title": [{"text": "Coreference Resolution System using Maximum Entropy Classifier", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9522389471530914}]}], "abstractContent": [{"text": "In this paper, we present our supervised learning approach to coreference resolution in ConLL corpus.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9685238599777222}, {"text": "ConLL corpus", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.8798082172870636}]}, {"text": "The system relies on a maximum entropy-based classifier for pairs of mentions, and adopts a rich linguisitical-ly motivated feature set, which mostly has been introduced by Soon et al (2001), and experiment with alternaive resolution process , preprocessing tools,and classifiers.", "labels": [], "entities": [{"text": "alternaive resolution", "start_pos": 212, "end_pos": 233, "type": "TASK", "confidence": 0.7559636235237122}]}, {"text": "We optimize the system's performance for M-UC (Vilain et al, 1995), BCUB (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) .", "labels": [], "entities": [{"text": "BCUB", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6177468299865723}, {"text": "CEAF (Luo, 2005)", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.8235042095184326}]}], "introductionContent": [{"text": "The coreference resolution is the task in which all expressions refer to the same entity in a discourse will be identified.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9310451745986938}]}, {"text": "As the core of natural language processing, coreference resolution is significant to message understanding, information extraction, text summarization, information retrieval, information filtration, and machine translation.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9214897453784943}, {"text": "message understanding", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8240129351615906}, {"text": "information extraction", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.8177746832370758}, {"text": "text summarization", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.735963761806488}, {"text": "information retrieval", "start_pos": 152, "end_pos": 173, "type": "TASK", "confidence": 0.8085455894470215}, {"text": "information filtration", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.7889070510864258}, {"text": "machine translation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.8114900887012482}]}, {"text": "A considerable engineering efforts is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.90761598944664}]}, {"text": "The backbone of our system can be split into two subproblems: mention detection and creation of entitly.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7043620944023132}]}, {"text": "We train a mention detector on the training texts.", "labels": [], "entities": [{"text": "mention detector", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.55092553794384}]}, {"text": "Once the mentions are identified, coreference resolution involves partitioning them into subsets corresponding to the same entity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.973578006029129}]}, {"text": "This problem is cast into the binary classification problem of deciding whether two given mentions are coreferent.", "labels": [], "entities": []}, {"text": "Our system relies on maximum entropy-based classifier for pairs of mentions.", "labels": [], "entities": []}, {"text": "Our system relies on a rich linguistically motivated feature set.", "labels": [], "entities": []}, {"text": "Our system architecture makes it possible to define other kinds of features: atmoic word and markable features.", "labels": [], "entities": []}, {"text": "This approach to feature engineering is suitable not only for knowledge-rich but also for knowledge-poor datasets.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7789340019226074}]}, {"text": "Finally, we use the bestfirst clustering to create the coreference chains.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we have evaluated our mention detector module, which is train by the ConLL training data.", "labels": [], "entities": [{"text": "mention detector", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.6546610742807388}, {"text": "ConLL training data", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.8571037451426188}]}, {"text": "It regards all the token as the candidate, and cast it into the mention detector, and the detector decides it is mention or not.", "labels": [], "entities": []}, {"text": "The mention detector's result is shown in Table4.", "labels": [], "entities": []}, {"text": "Second, we have evaluated our system with the system mention, and we use the previous mention detector to determine the mention boundary.", "labels": [], "entities": []}, {"text": "As follow, we list the system perfomance of using MUC, B-CUB,CEAF (E) , CEAF (M) , BLANC (Recasens and Hovy, in prep) in  Finally, we have evaluated our system with the gold mentions, which mention's boundary is corect.", "labels": [], "entities": [{"text": "MUC", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7123838663101196}, {"text": "B-CUB,CEAF", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9015602469444275}, {"text": "CEAF (M)", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.8939821422100067}, {"text": "BLANC", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9959491491317749}]}, {"text": "The system performance is shown in Result of system shows a big difference between using gold mentions and using system mentions.", "labels": [], "entities": []}, {"text": "In comparison to the system using system mentions, we see that the F-score rises significantly by 4.21-15.53 for the system using gold mentions.", "labels": [], "entities": [{"text": "F-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9983170032501221}]}, {"text": "It is worth noting that the F-scorer when using the B-CUB metric, the system using system mention rise-s 2.12 for system using gold mention.", "labels": [], "entities": [{"text": "F-scorer", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9851876497268677}, {"text": "B-CUB", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9452055096626282}]}, {"text": "Although this is surprising, in my opinion this correlation is because the mention detection recall more candidate mention, and the BCUB metric is benefit for the mention which is merge into the erroneous chain.", "labels": [], "entities": [{"text": "BCUB", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9669119715690613}]}], "tableCaptions": [{"text": " Table 1: Final system's training data distribution", "labels": [], "entities": [{"text": "Final system's training data", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.7208284497261047}]}, {"text": " Table 4: Performance of mention detector on the de- velopment set", "labels": [], "entities": [{"text": "mention detector", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9228416085243225}]}]}