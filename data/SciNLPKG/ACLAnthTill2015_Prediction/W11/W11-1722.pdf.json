{"title": [{"text": "Towards a Unified Approach for Opinion Question Answering and Summarization", "labels": [], "entities": [{"text": "Opinion Question Answering", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6081793010234833}, {"text": "Summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.6511878371238708}]}], "abstractContent": [{"text": "The aim of this paper is to present an approach to tackle the task of opinion question answering and text summarization.", "labels": [], "entities": [{"text": "opinion question answering", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7061426440874735}, {"text": "text summarization", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7260993421077728}]}, {"text": "Following the guidelines TAC 2008 Opinion Sum-marization Pilot task, we propose new methods for each of the major components of the process.", "labels": [], "entities": [{"text": "TAC 2008 Opinion Sum-marization", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.7045396566390991}]}, {"text": "In particular, for the information retrieval, opinion mining and summarization stages.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8221229314804077}, {"text": "opinion mining", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.8864990770816803}, {"text": "summarization", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.9862167835235596}]}, {"text": "The performance obtained improves with respect to the state of the art by approximately 12.50%, thus concluding that the suggested approaches for these three components are adequate.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the birth of the Social Web, users play a crucial role in the content appearing on the Internet.", "labels": [], "entities": []}, {"text": "With this type of content increasing at an exponential rate, the field of Opinion Mining (OM) becomes essential for analyzing and classifying the sentiment found in texts.", "labels": [], "entities": [{"text": "of Opinion Mining (OM)", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7557788987954458}, {"text": "classifying the sentiment found in texts", "start_pos": 130, "end_pos": 170, "type": "TASK", "confidence": 0.8143529891967773}]}, {"text": "Nevertheless, real-world applications of OM often require more than an opinion mining component.", "labels": [], "entities": [{"text": "OM", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9719038605690002}, {"text": "opinion mining", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7268083393573761}]}, {"text": "On the one hand, an application should allow a user to query about opinions in natural language.", "labels": [], "entities": []}, {"text": "Therefore, Question Answering (QA) techniques must be applied in order to determine the information required by the user and subsequently retrieve and analyze it.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8589540839195251}]}, {"text": "On the other hand, opinion mining offers mechanisms to automatically detect and classify sentiments in texts, overcoming the issue given by the high volume of such information present on the Internet.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.82487553358078}]}, {"text": "However, in many cases, even the result of the opinion processing by an automatic system still contains large quantities of information, which are still difficult to deal with manually.", "labels": [], "entities": [{"text": "opinion processing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8306310474872589}]}, {"text": "For example, for questions such as \"Why do people like George Clooney?\" we can find thousands of answers on the Web.", "labels": [], "entities": []}, {"text": "Therefore, finding the relevant opinions expressed on George Clooney, classifying them and filtering only the positive opinions is not helpful enough for the user.", "labels": [], "entities": []}, {"text": "He/she will still have to sift through thousands of texts snippets, containing relevant, but also much redundant information.", "labels": [], "entities": []}, {"text": "For that, we need to use Text Summarization (TS) techniques.", "labels": [], "entities": [{"text": "Text Summarization (TS)", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8439788579940796}]}, {"text": "TS provides a condensed version of one or several documents (i.e., a summary) which can be used as a substitute of the original ones.", "labels": [], "entities": []}, {"text": "In this paper, we will concentrate on proposing adequate solutions to tackle the issue of opinion question answering and summarization.", "labels": [], "entities": [{"text": "opinion question answering", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.6773050526777903}, {"text": "summarization", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.9758446216583252}]}, {"text": "Specifically, we will propose methods to improve the task of question answering and summarization over opinionated data, as defined in the TAC 2008 \"Opinion Summarization pilot\" . Given the performance improvements obtained, we conclude that the approaches we proposed for these three components are adequate.", "labels": [], "entities": [{"text": "question answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8801326751708984}, {"text": "summarization", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.983937680721283}, {"text": "TAC 2008", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.8846385776996613}]}], "datasetContent": [{"text": "The objective of this section is to describe the corpus used and the experiments performed with the data provided in TAC 2008 Opinion Summarization Pilot 7 task.", "labels": [], "entities": [{"text": "TAC 2008 Opinion Summarization Pilot 7 task", "start_pos": 117, "end_pos": 160, "type": "DATASET", "confidence": 0.8950902989932469}]}, {"text": "The approaches analyzed comprise: \u2022 OQA&S: The three components explained in the previous section (information retrieval, opinion mining and summarization) were bound together in order to produce summaries that include the answer to opinionated questions.", "labels": [], "entities": [{"text": "OQA&S", "start_pos": 36, "end_pos": 41, "type": "TASK", "confidence": 0.6117899318536123}, {"text": "information retrieval", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7629130184650421}, {"text": "opinion mining", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7841292917728424}, {"text": "summarization", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.9784618616104126}]}, {"text": "First, the most relevant passages of length 1 and 3 are retrieved by the IR module, as in the aforementioned approach, and then the subjective information is found and classified within them using the OM approaches described in the previous section.", "labels": [], "entities": []}, {"text": "Further on, we incorporate the TS module, to select and extract the most relevant opinionated facts from the pool of subjective information identified by the OM module.", "labels": [], "entities": [{"text": "TS", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.6650106310844421}]}, {"text": "We generate opinionoriented summaries of compression rates ranging from 10% to 50%.", "labels": [], "entities": []}, {"text": "In the end, four different approaches result from the integration of the three components: IRp1-OMaprox1-TS; IRp1-OMaprox2-TS; IRp3-OMaprox1-TS; and IRp3-OMaprox2-TS.", "labels": [], "entities": [{"text": "IRp1-OMaprox1-TS", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.7904973030090332}, {"text": "IRp3-OMaprox2-TS", "start_pos": 149, "end_pos": 165, "type": "DATASET", "confidence": 0.7935632467269897}]}, {"text": "Moreover, apart from these approaches, two baselines were also defined.", "labels": [], "entities": []}, {"text": "On the one hand, we sug-gest a baseline using the list of snippets provided by the TAC organization (QA-snippets).", "labels": [], "entities": []}, {"text": "This baseline produces a summary by joining all the answers in the snippets that related to the same topic On the other hand, we took as a second baseline the approach from our participation in TAC 2008 (DLSIUAES), without not taking into account any information retrieval or question answering system to retrieve the fragments of information which maybe relevant to the query.", "labels": [], "entities": [{"text": "TAC 2008 (DLSIUAES)", "start_pos": 194, "end_pos": 213, "type": "DATASET", "confidence": 0.8368165254592895}, {"text": "question answering", "start_pos": 276, "end_pos": 294, "type": "TASK", "confidence": 0.7162484228610992}]}, {"text": "In contrast, this was performed by computing the cosine similarity 8 between each sentence in the blog and the query.", "labels": [], "entities": [{"text": "cosine similarity 8", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.8204135100046793}]}, {"text": "After all the potential relevant sentences for the query were identified, they were classified in terms of subjectivity and polarity, and the most relevant ones were selected for the final summary.", "labels": [], "entities": []}, {"text": "Since we used the corpus provided at the Opinion Summarization Pilot task, and we followed similar guidelines, we should evaluate our OQA&S approach in the same way as participant systems were assessed.", "labels": [], "entities": [{"text": "OQA&S", "start_pos": 134, "end_pos": 139, "type": "TASK", "confidence": 0.6764340202013651}]}, {"text": "However, the evaluation methodology proposed differs slightly from the one carried out in the competition.", "labels": [], "entities": []}, {"text": "The reason why we took such decision was due to the fact that the evaluation carried out in TAC had some limitations, and therefore was not suitable for our purposes.", "labels": [], "entities": [{"text": "TAC", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.8845708966255188}]}, {"text": "In this manner, our evaluation is also based on the gold-standard nuggets provided by TAC, but in addition we proposed an extended version of them, by adding other pieces of information that are also relevant to the topics.", "labels": [], "entities": [{"text": "TAC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8611884713172913}]}, {"text": "In this section, all the issues concerning the evaluation are explained.", "labels": [], "entities": []}, {"text": "These comprise the original evaluation method used in the Opinion Summarization Pilot task at TAC (Section 4.3.1) , its drawbacks (Section 4.3.2), and the extended version for the evaluation method we propose (Section 4.3.3).", "labels": [], "entities": [{"text": "Opinion Summarization Pilot task", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.7828110009431839}, {"text": "TAC", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.7878775596618652}]}, {"text": "Further on, the results obtained together with a wide discussion, as well as its comparison with the baselines and the TAC participants is provided in Section 4.4.", "labels": [], "entities": [{"text": "TAC participants", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.7839080393314362}]}, {"text": "Within the Opinion Summarization Pilot task, each summary was evaluated according to its con-8 http://www.d.umn.edu/ tpederse/text-similarity.html tent using the Pyramid method (.", "labels": [], "entities": [{"text": "Opinion Summarization Pilot task", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.7781797647476196}]}, {"text": "A list of nuggets was provided and the assessors used such list of nuggets to count the number of nuggets a summary contained.", "labels": [], "entities": []}, {"text": "Depending on the number of nuggets the summary included and the importance of each one given by their weight, the values for recall, precision and F-measure were obtained.", "labels": [], "entities": [{"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9995730519294739}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9997124075889587}, {"text": "F-measure", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9993002414703369}]}, {"text": "An example of several nuggets corresponding to different topics can be seen in, where the weight for each one is also shown in brackets.", "labels": [], "entities": []}, {"text": "The evaluation method suggested at TAC requires a lot of human effort when it comes to identify the relevant fragments of information (nuggets) and compute how many of them a summary contains, resulting in a very costly and time-consuming task.", "labels": [], "entities": [{"text": "TAC", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.6142709851264954}]}, {"text": "This is a general problem associated to the evaluation of summaries, which makes the task of summarization evaluation especially hard and difficult.", "labels": [], "entities": [{"text": "evaluation of summaries", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.6826621492703756}, {"text": "summarization evaluation", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.9800803065299988}]}, {"text": "But, apart from this, when an exhaustive examination of the nuggets used in TAC is done, some other problems arised which are worth mentioning.", "labels": [], "entities": [{"text": "TAC", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.8158143162727356}]}, {"text": "The average number of nuggets for each topic is 27, and this would mean, that longer summaries will be highly penalized, because it will contain more useless information according to the nuggets.", "labels": [], "entities": []}, {"text": "After analyzing in detail all the provided nuggets, we mainly classified the possible problems into six groups, which are: same idea, despite not being identical.", "labels": [], "entities": []}, {"text": "In these cases, we are counting a single piece of information in the summary twice, if the idea that nuggets expressed is included.", "labels": [], "entities": []}, {"text": "3. Moreover, the meaning of one nugget can be deduced from another's, which is also related to the problem stated before.", "labels": [], "entities": []}, {"text": "4. Some of the nuggets are not very clear in meaning (e.g. \"hot\", \"fun\").", "labels": [], "entities": []}, {"text": "This would mean that a summary might include such terms in a different context, thus, obtaining incorrectly that it is revelant when might be out of context.", "labels": [], "entities": []}, {"text": "5. A sentence in the original blog can be covered by several nuggets.", "labels": [], "entities": []}, {"text": "For instance, both nuggets \"it is an honest book\" and \"it is a great book\" correspond to the same sentence \"It was such a great bookhonest and hard to read (content not language difficulty)\".", "labels": [], "entities": []}, {"text": "In this case, it is not clear how to proceed with the evaluation; whether to count both nuggets or just one of them.", "labels": [], "entities": []}, {"text": "6. Some information which is also relevant for the topic is not present in any nugget.", "labels": [], "entities": []}, {"text": "For instance: \"I go to Starbucks because they generally provide me better service\".", "labels": [], "entities": []}, {"text": "Although it is relevant with respect to the topic and it appears in a number of summaries, it would be not counted because it has not been chosen as a nugget.", "labels": [], "entities": []}, {"text": "Since we are interested in testing a wide range of approaches involving IR, OM and TS, sticking to the rules to the original TAC evaluation would mean that a lot of time as well as human effort will be required, as well as not accounting for important information that summaries may contain in addition to the one expressed by the nuggets.", "labels": [], "entities": []}, {"text": "Therefore, taking as a basis the nuggets provided at TAC, we set out a modified version of them.", "labels": [], "entities": [{"text": "TAC", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9319101572036743}]}, {"text": "The underlying idea behind this is to create an extended set of nuggets that serve as a reference for assessing the content of the summaries.", "labels": [], "entities": []}, {"text": "In this manner, we will map each original nugget with the set of sentences in the original blogs that are most similar to it, thus generating a gold-standard summary for each topic.", "labels": [], "entities": []}, {"text": "For creating this extended gold-standard nuggets we compute the cosine similarity 9 between The cosine similarity was computed using Pedersen's every nugget and all the sentences in the blog related to the same topic.", "labels": [], "entities": [{"text": "cosine similarity 9", "start_pos": 64, "end_pos": 83, "type": "METRIC", "confidence": 0.7759515643119812}]}, {"text": "We empirically established a similarity threshold of 0.5, meaning that if a sentence was equal or above such similarity value, it will be considered also relevant.", "labels": [], "entities": []}, {"text": "One main disadvantage of such a lower threshold value is that we can consider relevant sentences that share the same vocabulary but in fact they are not relevant to the summary.", "labels": [], "entities": []}, {"text": "In order to avoid this, once we had identified all the most similar sentences to each nugget, we carried out a manual analysis to discard cases like this.", "labels": [], "entities": []}, {"text": "Having created the extended set of nuggets, we grouped all of them pertaining to the same topic, and considered it a gold-standard summary.", "labels": [], "entities": []}, {"text": "Now, the average number of nuggets per topic is 53, which we have increased by twice the number of original nuggets provided at TAC.", "labels": [], "entities": [{"text": "TAC", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9548635482788086}]}, {"text": "Further on, our summaries are compared against this new gold-standard using ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9825056791305542}]}, {"text": "This tool computes the number of different kinds of overlap n-grams between an automatic summary and a human-made summary.", "labels": [], "entities": []}, {"text": "For our evaluation, we compute ROUGE-1 (unigrams), ROUGE-2 (bigrams), ROUGE-SU4 (it measures the overlap of skip-bigrams between a candidate summary and a set of reference summaries with a maximum skip distance of 4), and ROUGE-L (Longest Common Subsequence between two texts).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9284163117408752}, {"text": "ROUGE-2", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9837512373924255}, {"text": "ROUGE-SU4", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9887442588806152}, {"text": "ROUGE-L", "start_pos": 222, "end_pos": 229, "type": "METRIC", "confidence": 0.9863714575767517}]}, {"text": "The results and discussion are next provided.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of our OQA&S approaches", "labels": [], "entities": [{"text": "OQA&S", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.6481682062149048}]}, {"text": " Table 4: Comparison with other systems", "labels": [], "entities": []}]}