{"title": [{"text": "Link Type Based Pre-Cluster Pair Model for Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.9701735973358154}]}], "abstractContent": [{"text": "This paper presents our participation in the CoNLL-2011 shared task, Modeling Unrestricted Coreference in OntoNotes.", "labels": [], "entities": [{"text": "Modeling Unrestricted Coreference in OntoNotes", "start_pos": 69, "end_pos": 115, "type": "TASK", "confidence": 0.8389878392219543}]}, {"text": "Corefer-ence resolution, as a difficult and challenging problem in NLP, has attracted a lot of attention in the research community fora longtime.", "labels": [], "entities": [{"text": "Corefer-ence resolution", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7856090664863586}]}, {"text": "Its objective is to determine whether two mentions in apiece of text refer to the same entity.", "labels": [], "entities": []}, {"text": "In our system, we implement mention detection and coreference resolution seperately.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7211018651723862}, {"text": "coreference resolution", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.946321040391922}]}, {"text": "For mention detection, a simple classification based method combined with several effective features is developed.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9531836807727814}]}, {"text": "For coreference resolution , we propose a link type based pre-cluster pair model.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.977213978767395}]}, {"text": "In this model, pre-clustering of all the mentions in a single document is first performed.", "labels": [], "entities": []}, {"text": "Then for different link types, different classification models are trained to determine wheter two pre-clusters refer to the same entity.", "labels": [], "entities": []}, {"text": "The final clustering results are generated by closest-first clustering method.", "labels": [], "entities": []}, {"text": "Official test results for closed track reveal that our method gives a MUC F-score of 59.95%, a B-cubed F-score of 63.23%, and a CEAF F-score of 35.96% on development dataset.", "labels": [], "entities": [{"text": "MUC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.638424277305603}, {"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.5982323288917542}, {"text": "B-cubed", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.995924711227417}, {"text": "F-score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.6561276912689209}, {"text": "CEAF", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.5057936906814575}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.613904595375061}]}, {"text": "When using gold standard mention boundaries, we achieve MUC F-score of 55.48%, B-cubed F-score of 61.29%, and CEAF F-score of 32.53%.", "labels": [], "entities": [{"text": "MUC", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.7222354412078857}, {"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.8204185366630554}, {"text": "B-cubed F-score", "start_pos": 79, "end_pos": 94, "type": "METRIC", "confidence": 0.7939615249633789}, {"text": "CEAF", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.5220560431480408}, {"text": "F-score", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.5452830195426941}]}], "introductionContent": [{"text": "The task of coreference resolution is to recognize all the mentions (also known as noun phrases, including names, nominal mentions and pronouns) in a text and cluster them into equivalence classes where each quivalence class refers to a real-world entity or abstract concept.", "labels": [], "entities": [{"text": "coreference resolution is to recognize all the mentions (also known as noun phrases, including names, nominal mentions and pronouns) in a text and cluster them into equivalence classes where each quivalence class refers to a real-world entity or abstract concept", "start_pos": 12, "end_pos": 274, "type": "Description", "confidence": 0.8476572632789612}]}, {"text": "The CoNLL-2011 shared task 1 uses OntoNotes 2 as the evaluation corpus.", "labels": [], "entities": [{"text": "CoNLL-2011 shared task 1", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8507089912891388}]}, {"text": "The coreference layer in OntoNotes constitutes one part of a multi-layer, integrated annotation of the shallow semantic structures in the text with high interannotator agreement.", "labels": [], "entities": []}, {"text": "In addition to coreference, this data set is also tagged with syntactic trees, high coverage verb and some noun propositions, partial verb and noun word senses, and 18 named entity types.", "labels": [], "entities": []}, {"text": "The main difference between OntoNotes and another wellknown coreference dataset ACE is that the former does not label any singleton entity cluster, which has only one reference in the text.", "labels": [], "entities": []}, {"text": "We can delete all the singleton clusters as a postprocessing step for the final results.", "labels": [], "entities": []}, {"text": "Alternatively, we can also first train a classifier to separate singleton mentions from the rest and apply this mention detection step before coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.947266697883606}]}, {"text": "In this work we adopt the second strategy.", "labels": [], "entities": []}, {"text": "In our paper, we use a traditional learning based pair-wise model for this task.", "labels": [], "entities": []}, {"text": "For mention detection, we first extract all the noun phrases in the text and then use a classification model combined with some effective features to determine whether each noun phrase is actually a mention.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8722258508205414}]}, {"text": "The features include word features, POS features in the given noun phrase and its context, string matching feature in its context, SRL features, and named entity features among others.", "labels": [], "entities": [{"text": "string matching", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.6634765863418579}, {"text": "SRL", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.8736633658409119}]}, {"text": "More details will be given in Section 3.", "labels": [], "entities": []}, {"text": "From our in-house experiments, the final Fscores for coreference resolution can be improved by this mention detection part.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9976834058761597}, {"text": "coreference resolution", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.9680436551570892}, {"text": "mention detection", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.6241190135478973}]}], "datasetContent": [{"text": "We present our evaluation results on development dataset for CoNLL-2011 shared Task in,.", "labels": [], "entities": [{"text": "CoNLL-2011 shared Task", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.7920120159784952}]}, {"text": "Official test results are given in.", "labels": [], "entities": []}, {"text": "Three different evaluation metrics were used: MUC (, B 3 (Bagga and Baldwin, 1998) and CEAF ().", "labels": [], "entities": [{"text": "MUC", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.6462879776954651}, {"text": "CEAF", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.684923529624939}]}, {"text": "Finally, the average scores of these three metrics are used to rank the participating systems.", "labels": [], "entities": []}, {"text": "The difference between is whether gold standard mention boundaries are given.", "labels": [], "entities": []}, {"text": "Here \"mention boundaries\" means a more broad concept than the mention definition we gave earlier.", "labels": [], "entities": []}, {"text": "We should also detect real mentions from them.", "labels": [], "entities": []}, {"text": "From the tables, we can see that the scores can be improved litttle by using gold standard mention boundaries.", "labels": [], "entities": []}, {"text": "Also the results from  better than using an unified classification model.", "labels": [], "entities": []}, {"text": "For official test results, our system did not perform as well as we had expected.", "labels": [], "entities": []}, {"text": "Some possible reasons are as follows.", "labels": [], "entities": []}, {"text": "First, verbs that are coreferential with a noun phrase are also tagged in OntoNotes.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9447087049484253}]}, {"text": "For example, \"grew \" and \"the strong growth\" should be linked in the following case: \"Sales of passenger cars grew 22%.", "labels": [], "entities": []}, {"text": "The strong growth followed yearto-year increases.\"", "labels": [], "entities": []}, {"text": "But we cannot solve this kind of problem in our system.", "labels": [], "entities": []}, {"text": "Second, we should perform feature selection to avoid some useless features harming the scores.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7096026688814163}]}, {"text": "Meanwhile, we did not make full use of the WordNet, PropBank and other background knowledge sources as features to represent pre-cluster pairs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9795154929161072}, {"text": "PropBank", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.7345200777053833}]}], "tableCaptions": [{"text": " Table 3: Evaluation results on development dataset with- out gold mention boundaries", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results on development dataset with  gold mention boundaries", "labels": [], "entities": [{"text": "gold mention", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.6731885075569153}]}, {"text": " Table 5: Evaluation results on development dataset  with gold mention boundaries using unified classification  model", "labels": [], "entities": []}, {"text": " Table 6: Evaluation results on test dataset without gold  mention boundaries", "labels": [], "entities": []}, {"text": " Table 6 and Table 7. Three different evaluation  metrics were used: MUC (", "labels": [], "entities": [{"text": "MUC", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.7300454378128052}]}]}