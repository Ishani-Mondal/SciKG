{"title": [{"text": "Building and using comparable corpora for domain-specific bilingual lexicon extraction", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a series of experiments aimed at inducing and evaluating domain-specific bilingual lexica from comparable corpora.", "labels": [], "entities": []}, {"text": "First, a small English-Slovene comparable corpus from health magazines was manually constructed and then used to compile a large comparable corpus on health-related topics from web corpora.", "labels": [], "entities": []}, {"text": "Next, a bilingual lexicon for the domain was extracted from the corpus by comparing context vectors in the two languages.", "labels": [], "entities": []}, {"text": "Evaluation of the results shows that a 2-way translation of context vectors significantly improves precision of the extracted translation equivalents.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.999104917049408}]}, {"text": "We also show that it is sufficient to increase the corpus for one language in order to obtain a higher recall, and that the increase of the number of new words is linear in the size of the corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9991569519042969}]}, {"text": "Finally, we demonstrate that by lowering the frequency threshold for context vectors, the drop in precision is much slower than the increase of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9994103908538818}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9980097413063049}]}], "introductionContent": [{"text": "Research into using comparable corpora in NLP has gained momentum in the past decade largely due to limited availability of parallel data for many language pairs and domains.", "labels": [], "entities": []}, {"text": "As an alternative to already established parallel approaches (e.g.) the comparable corpusbased approach relies on texts in two or more languages which are not parallel but nevertheless share several parameters, such as topic, time of publication and communicative goal).", "labels": [], "entities": []}, {"text": "The main advantage of this approach is the simpler, faster and more time efficient compilation of comparable corpora, especially from the rich web data).", "labels": [], "entities": []}, {"text": "In this paper we describe the compilation process of a large comparable corpus of texts on healthrelated topics for Slovene and English that were published on the web.", "labels": [], "entities": []}, {"text": "Then we report on a set of experiments we conducted in order to automatically extract translation equivalents for terms from the health domain.", "labels": [], "entities": []}, {"text": "The parameters we tested and analysed are: 1-and 2-way translations of context vectors with a seed lexicon, the size of the corpus used for bilingual lexicon extraction, and the word frequency threshold for vector construction.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.6596396068731943}, {"text": "vector construction", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.7171834260225296}]}, {"text": "The main contribution of this paper is a much-desired language-and domain-independent approach to bootstrapping bilingual lexica with minimal manual intervention as well as minimal reliance on the existing linguistic resources.", "labels": [], "entities": [{"text": "bootstrapping bilingual lexica", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.8435791532198588}]}, {"text": "The paper is structured as follows: in the next section we give an overview of previous work relevant for our research.", "labels": [], "entities": []}, {"text": "In Section 3 we present the construction of the corpus.", "labels": [], "entities": []}, {"text": "Section 4 describes the experiments for bilingual lexicon extraction the results of which are reported, evaluated and discussed in Section 5.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.7567672928174337}]}, {"text": "We conclude the paper with final remarks and ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "At this stage of our research we have limited the experiments to nouns.", "labels": [], "entities": []}, {"text": "This speeds up and simplifies our task but we believe it still gives an adequate insight into the usefulness of the approach fora particular domain since nouns carry the highest domain-specific terminological load.", "labels": [], "entities": []}, {"text": "Automatic evaluation of the results was performed against a gold standard lexicon of health-related terms that was obtained from the top-ranking nouns in the English health domain model of the initial corpus and that at the same time appeared in the comprehensive dictionary of medical terms mediLexicon 2 and were missing from the general bilingual seed dictionary.", "labels": [], "entities": []}, {"text": "The gold standard 2 http://www.medilexicon.com contains 360 English single-word terms with their translations into Slovene.", "labels": [], "entities": [{"text": "gold standard 2", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8227960069974264}]}, {"text": "If more than one translation variant is possible fora single English term, all variants appear in the gold standard and any of these translations suggested by the algorithm is considered as correct.", "labels": [], "entities": []}, {"text": "Below we present the results of three experiments that best demonstrate the performance and impact of the key parameters for bilingual lexicon extraction from comparable corpora that we were testing in this research.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.6950397690137228}]}, {"text": "The evaluation measure for precision used throughout this research is mean reciprocal rank) on first ten translation candidates.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9988705515861511}, {"text": "reciprocal rank)", "start_pos": 75, "end_pos": 91, "type": "METRIC", "confidence": 0.8882025082906088}]}, {"text": "Recall is calculated as the percentage of goldstandard entries we were able to calculate translation candidates for.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9899598360061646}]}, {"text": "Additionally, a global recall impact of our methods is shown as the overall number of entries for which we were able to calculate translation candidates.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9981933236122131}]}, {"text": "Unless stated otherwise, the frequency threshold for the generation of context vectors in the experiments was set to 50.", "labels": [], "entities": []}, {"text": "We begin with the results of 1-and 2-way context vector translations that we tested on the initial 1-million-word corpus we constructed from health magazines as well as on a corpus of the same size we extracted from the web.", "labels": [], "entities": []}, {"text": "We compared the results of our method with that proposed in () strengthening our claim that it is the additional information in the reverse dictionary that makes the significant impact, not the reversing itself.", "labels": [], "entities": []}, {"text": "As shows, using two general dictionaries (2-way two dict) significantly improves the results as anew dictionary brings additional information.", "labels": [], "entities": []}, {"text": "That it is the dictionary improving the results is proven by using just one, inverted dictionary in the 2-way manner, which produced worse results than the 1-way approach (2-way inverse dict).", "labels": [], "entities": []}, {"text": "The approach of is also based on new dictionary knowledge since using only one inverted dictionary with their 2-way method yielded results that were almost identical to the 1-way computation.", "labels": [], "entities": []}, {"text": "Using rank, not similarity score in averaging results proved to be a good approach (2-way Chiao two dict), but not as efficient as our approach which uses similarity scores (2-way two dict).", "labels": [], "entities": [{"text": "similarity score", "start_pos": 16, "end_pos": 32, "type": "METRIC", "confidence": 0.954599142074585}]}, {"text": "Our approach yields higher precision and is also easier to compute.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993127584457397}]}, {"text": "Namely, for every candidate pair only the reverse similarity score has to be computed, and not all similarity scores for every inverse pair to obtain a rank value.", "labels": [], "entities": [{"text": "reverse similarity score", "start_pos": 42, "end_pos": 66, "type": "METRIC", "confidence": 0.7688559691111246}, {"text": "similarity", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9499967098236084}]}, {"text": "Therefore, only the 2-way translation setting averaging on similarity scores is used in the rest of the experiments.", "labels": [], "entities": []}, {"text": "It is interesting that the results on the web corpus have a higher precision but a lower recall (0.355 on the initial corpus and 0.198 on the web corpus).", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9982788562774658}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9992408752441406}]}, {"text": "Higher precision can be explained with the domain modelling technique that was used to extract web data, which may have contributed to a terminologically more homogenous collection of documents in the health domain.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9963480830192566}]}, {"text": "On the other hand, the lower recall can be explained with the extracted web documents being less terminologically loaded than the initial corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9995138645172119}]}, {"text": "shows that precision with regard to the gold standard is more or less constant with an average of 0.68 if we disregard the first two measurements that are probably bad estimates since the intersection with the gold standard is small (as shown in) and evens out as the size of the corpus increases.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9993742108345032}]}, {"text": "When analyzing recall against the gold standard we seethe typical logarithmic recall behavior when depicted as a function of corpus size.", "labels": [], "entities": []}, {"text": "On the other hand, when we consider the number of new translation equivalents (i.e. the number of source words that do not appear in the seed dictionary), the function behaves almost linearly (see).", "labels": [], "entities": []}, {"text": "This can be explained with the fact that in the dictionary the most frequent words are best represented.", "labels": [], "entities": []}, {"text": "Because of that we can observe a steady increase in the number of words not present in the seed lexicon that pass the frequency threshold with the increasing corpus size.", "labels": [], "entities": []}, {"text": "Finally, we study the impact of the word frequency threshold for context vector generation on the quality and amount of the extracted translation equivalents on the six million corpora in both languages.", "labels": [], "entities": [{"text": "context vector generation", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6269547740618387}]}, {"text": "As can be seen in, by lowering the frequency criterion, the F1 measure increases showing greater gain in recall than loss in precision.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9806425273418427}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9992300271987915}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9940947890281677}]}, {"text": "For calculating recall, the number of new words passing the frequency criterion is normalized with the assumed number of obtainable lexicon entries set to 7.203 (the number of new words obtained with the lowest frequency criterion).", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.7617155313491821}]}, {"text": "This is a valuable insight since the threshold can beset according to different project scenarios.", "labels": [], "entities": []}, {"text": "If, for example, lexicographers can be used in order to check the translation candidates and choose the best ones among them, the threshold may well be left low and they will still be able to identify the correct translation very quickly.", "labels": [], "entities": []}, {"text": "If, on the other hand, the results will be used directly by another application, the threshold will be raised in order to reduce the amount of noise introduced by the lexicon for the following processing stages.", "labels": [], "entities": []}, {"text": "For a more qualitative inspection of the results we performed manual evaluation on a random sample of 100 translation equivalents that are not in the general seed dictionary or present in our gold standard.", "labels": [], "entities": []}, {"text": "We were interested in finding out to what extent these translation equivalents belong to the health domain and if their quality is comparable to the results of the automatic evaluation.", "labels": [], "entities": []}, {"text": "Manual evaluation was performed on translation equivalents extracted from the comparable corpus containing 18 million English words and 6 million Slovene words, where the frequency threshold was set to 50.", "labels": [], "entities": []}, {"text": "51% of the manually evaluated words belonged to the health domain, 23% were part of general vocabulary, 10% were proper names and the rest were acronyms and errors arising from PoS-tagging and lemmatization in the ukWaC corpus.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 214, "end_pos": 226, "type": "DATASET", "confidence": 0.98946213722229}]}, {"text": "Overall, in 45% the first translation equivalent was correct and additional 11% contained the correct translation among the ten best-ranked candidates.", "labels": [], "entities": []}, {"text": "For 44 % of the extracted translation equivalents no appropriate translation was suggested.", "labels": [], "entities": []}, {"text": "Among the evaluated health-domain terms, 61% were translated correctly with the first candidate and for the additional 20% the correct translation appeared among the first 10 candidates.", "labels": [], "entities": []}, {"text": "Of the 19% health-domain terms with no appropriate translation suggestion, 4 terms, that is 21% of the wrongly translated terms, were translated as direct hypernyms and could loosely be considered as correct (e.g. the English term bacillus was translated as mikroorganizem into Slovene, which means microorganism).", "labels": [], "entities": []}, {"text": "Even most other translation candidates were semantically closely related, in fact, there was only one casein the manually inspected sample that provided completely wrong translations.", "labels": [], "entities": []}, {"text": "Manual evaluation shows that the quality of translations for out-of-goldstandard terms is consistent with the results of automatic evaluation.", "labels": [], "entities": []}, {"text": "A closer look revealed that we were able to obtain translation equivalents not only for the general vocabulary but especially terms relevant for the health domain, and furthermore, that their quality is also considerably higher than for the general vocabulary which is not of our primary interest in this research.", "labels": [], "entities": []}, {"text": "The results could be further improved by filtering out the noise obtained from errors in PoS-tagging and lemmatization and, more importantly, by identifying proper names.", "labels": [], "entities": []}, {"text": "Multi-word expressions should also be tackled as they present problems, especially in cases of 1:many mappings, such as the English single-word term immunodeficiency that is translated with a multi-word expression in Slovene (imunska pomanjkljivost).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision regarding the corpus source and  the translation method", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.914516270160675}]}, {"text": " Table 2: Precision, recall, number of translated  words and number of new words (not found in the  dictionary) obtained with different corpus sizes", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9962034821510315}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9977733492851257}]}, {"text": " Table 3: Precision, number of new words and F1  obtained with different frequency thresholds", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9847367405891418}, {"text": "F1", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9995741248130798}]}]}