{"title": [{"text": "Efficient Parallel CKY Parsing on GPUs", "labels": [], "entities": []}], "abstractContent": [{"text": "Low-latency solutions for syntactic parsing are needed if parsing is to become an integral part of user-facing natural language applications.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7605224847793579}]}, {"text": "Unfortunately, most state-of-the-art constituency parsers employ large prob-abilistic context-free grammars for disam-biguation, which renders them impractical for real-time use.", "labels": [], "entities": []}, {"text": "Meanwhile, Graphics Processor Units (GPUs) have become widely available, offering the opportunity to alleviate this bottleneck by exploiting the fine-grained data parallelism found in the CKY algorithm.", "labels": [], "entities": []}, {"text": "In this paper, we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm.", "labels": [], "entities": []}, {"text": "We use the Compute Unified Device Architecture (CUDA) programming model to reimplement a state-of-the-art parser, and compare its performance on two recent GPUs with different architectural features.", "labels": [], "entities": []}, {"text": "Our best results show a 26-fold speedup compared to a sequential C implementation .", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic parsing of natural language is the task of analyzing the grammatical structure of sentences and predicting their most likely parse trees (see).", "labels": [], "entities": [{"text": "Syntactic parsing of natural language", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9174062371253967}, {"text": "predicting their most likely parse", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.6033679485321045}]}, {"text": "These parse trees can then be used in many ways to enable natural language processing applications like machine translation, question answering, and information extraction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.806310772895813}, {"text": "question answering", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.9213339686393738}, {"text": "information extraction", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.8547106087207794}]}, {"text": "Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.", "labels": [], "entities": [{"text": "syntactic constituency parsers", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.6278641621271769}]}, {"text": "The CKY dynamic programming algorithm ( is then be used to find the most likely parse tree fora given sentence of length n in O(|G|n 3 ) time.", "labels": [], "entities": []}, {"text": "While often ignored, the grammar constant |G| typically dominates the runtime in practice.", "labels": [], "entities": []}, {"text": "This is because grammars with high accuracy) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9953082203865051}]}, {"text": "Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz ().", "labels": [], "entities": []}, {"text": "This opens up new opportunities for increasing the speed of parsers.", "labels": [], "entities": []}, {"text": "We present a general approach for parallelizing the CKY algorithm that can handle arbitrary context-free grammars (Section 2).", "labels": [], "entities": []}, {"text": "We make no assumptions about the size of the grammar and we demonstrate the efficacy of our approach by implementing a decoder for the state-of-the-art latent variable grammars of) (a.k.a. Berkeley Parser) on a Graphics Processor Unit (GPU).", "labels": [], "entities": []}, {"text": "We first present an overview of the general architecture of GPUs and the efficient synchronization provided by the Compute Unified Device Architecture (CUDA ( ) programming model (Section 3).", "labels": [], "entities": []}, {"text": "We then discuss how the hundreds of cores available on a GPU can enable a fine-grained parallel execution of the CKY algorithm.", "labels": [], "entities": []}, {"text": "We explore the design space with different thread mappings onto the GPU and discuss how the various synchronization methods might be applied in this context.", "labels": [], "entities": [{"text": "GPU", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9406947493553162}]}, {"text": "Key to our approach is the observation that the computation needs to be parallelized over grammar rules rather than chart cells.", "labels": [], "entities": []}, {"text": "While this might have been difficult to do with previous parallel computing architectures, the CUDA model provides us with fine-grained parallelism and synchronization options that make this possible.", "labels": [], "entities": []}, {"text": "We empirically evaluate the various parallel implementations on two NVIDIA GPUs (GTX480 and GTX285) in Section 5.", "labels": [], "entities": []}, {"text": "We observe that some parallelization options are architecture dependent, emphasizing that a thorough understanding of the programming model and the underlying hardware is needed to achieve good results.", "labels": [], "entities": []}, {"text": "Our implementation on NVIDIA's GTX480 using CUDA results in a 26-fold speedup compared to the original sequential C implementation.", "labels": [], "entities": []}, {"text": "On the GTX285 GPU we obtain a 14-fold speedup.", "labels": [], "entities": [{"text": "GTX285 GPU", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9549916684627533}]}, {"text": "Parallelizing natural language parsers has been studied previously (see Section 6), however, previous work has focused on scenarios where only a limited level of coarse-grained parallelism could be utilized, or the underlying hardware required unrealistic restrictions on the size of the contextfree grammar.", "labels": [], "entities": [{"text": "Parallelizing natural language parsers", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7627675086259842}]}, {"text": "To the best of our knowledge, this is the first GPU-based parallel syntactic parser using a state-of-the-art grammar.", "labels": [], "entities": [{"text": "GPU-based parallel syntactic parser", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.5143880993127823}]}], "datasetContent": [{"text": "We implemented the various versions of the parallel CKY algorithm discussed in the previous sections in CUDA and measured the runtime on two different NVIDIA GPUs used in a quad-core desktop environment: GTX285 and GTX480.", "labels": [], "entities": [{"text": "GTX480", "start_pos": 215, "end_pos": 221, "type": "DATASET", "confidence": 0.8903654217720032}]}, {"text": "The detailed specifications of the experimental platforms are listed in.", "labels": [], "entities": []}, {"text": "The grammar we used is extracted from the publicly available parser of.", "labels": [], "entities": []}, {"text": "It has 1,120 nonterminal symbols including 636 preterminal symbols.", "labels": [], "entities": []}, {"text": "The grammar has 852,591 binary rules and 114,419 unary rules.", "labels": [], "entities": []}, {"text": "We used the first 1000 sentences from Section 22 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993) as our benchmark set.", "labels": [], "entities": [{"text": "Section 22 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993)", "start_pos": 38, "end_pos": 132, "type": "DATASET", "confidence": 0.8992238586599176}]}, {"text": "We verified for each sentence that our parallel implementation obtains exactly the same parse tree and score as the sequential implementation.", "labels": [], "entities": []}, {"text": "We compare the execution times of various versions of the parallel parser in CUDA, varying the mapping, synchronization methods and memory access patterns.", "labels": [], "entities": []}, {"text": "shows the speedup of the different parallel parsers on the two GPUs: Thread stands for the thread-based mapping and Block for the block-based one.", "labels": [], "entities": [{"text": "Thread", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9937580823898315}]}, {"text": "The default parallelizes over spans, while SS stands for sequential spans, meaning that the computation on spans is executed sequentially.", "labels": [], "entities": []}, {"text": "GA stands for global atomic synchronization, SA for shared atomic synchronization, and PR for the parallel reduction.", "labels": [], "entities": [{"text": "PR", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9590826630592346}]}, {"text": "SH stands for the transformed access pattern for the scores array in the shared memory.", "labels": [], "entities": []}, {"text": "tex:rule stands for loading the rule information from texture memory and tex:scores for loading the scores array from texture memory.", "labels": [], "entities": []}, {"text": "tex:both means both the tex:rule and tex:scores are applied.", "labels": [], "entities": []}, {"text": "The exhaustive sequential CKY parser was written in C and is reasonably optimized, taking 5.5 seconds per sentence (or 5,505 seconds for the 1000 benchmark sentences).", "labels": [], "entities": []}, {"text": "This is comparable to the better implementations presented in.", "labels": [], "entities": []}, {"text": "As can be seen in, the fastest configuration on the GTX285 is Block+PR+SS+tex:scores, which shows a 17.4\u00d7 speedup against the sequential parser.", "labels": [], "entities": [{"text": "Block+PR+SS+tex:scores", "start_pos": 62, "end_pos": 84, "type": "METRIC", "confidence": 0.7391205231348673}]}, {"text": "On the GTX480, Block+PR is the fastest, showing a 25.8\u00d7 speedup.", "labels": [], "entities": [{"text": "PR", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.5723753571510315}]}, {"text": "Their runtimes were 0.32 seconds/sentence and 0.21 seconds/sentence, respectively.", "labels": [], "entities": []}, {"text": "It is noteworthy that the fastest configuration differs for the two devices.", "labels": [], "entities": []}, {"text": "We provide an explanation later in this section.", "labels": [], "entities": []}, {"text": "On both the GTX285 and the GTX480, Thread+GA shows the worst performance as global atomic synchronization is very costly.", "labels": [], "entities": [{"text": "Thread+GA", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.895992636680603}]}, {"text": "Thread+GA on the GTX285 is even about 8 times slower than the sequential CKY parser.", "labels": [], "entities": [{"text": "Thread+GA", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9449539581934611}, {"text": "GTX285", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.9548391699790955}]}, {"text": "Note that although it is still the slowest one, Thread+GA on the GTX480 actually shows a 4.5\u00d7 speedup.", "labels": [], "entities": [{"text": "Thread+GA", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9338802893956503}]}, {"text": "On the GTX285, Thread+SA, Block+SA, and Block+PR show 6.4\u00d7, 8.1\u00d7, and 10.1\u00d7 speedups, respectively.", "labels": [], "entities": [{"text": "Thread", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9907427430152893}]}, {"text": "Perhaps somewhat surprisingly, parallelizing over spans actually hurts performance.", "labels": [], "entities": [{"text": "parallelizing over spans", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.876003642876943}]}, {"text": "By not serializing the computations for spans, we can get speedups of 13% for Thread+SA+SS over Thread+SA and about 40% for Block+SA+SS and Block+PR+SS over their parallel spans versions.", "labels": [], "entities": []}, {"text": "In thread-based mapping, the atomic operations on shared memory are the bottleneck, so that sequential processing of spans makes only a small difference.", "labels": [], "entities": []}, {"text": "On the other hand, in Block+SA and Block+PR on the GTX285, the global memory bandwidth is the major limiting factor since the same rule is loaded from the global memory redundantly for each span when we parallelize over spans.", "labels": [], "entities": [{"text": "GTX285", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9467722773551941}]}, {"text": "Hence, executing spans sequentially removes the redundant global memory loads and substantially improves the performance.", "labels": [], "entities": []}, {"text": "On the GTX285, the transformed access pattern for the scores array along with accesses to the shared memory (Block+PR+SH) improves the performance by about 10%, showing a 11.1\u00d7 speedup.", "labels": [], "entities": []}, {"text": "Placing the scores array in texture memory improves all implementations.", "labels": [], "entities": []}, {"text": "The reduced binding cost due to the array reorganization results in additional gains of about 25% for Block+PR+SS+tex:scores and Block+PR+tex:scores against Block+PR+SS and Block+PR (for a total speedup of 17.4\u00d7 and 13.0\u00d7, respectively).", "labels": [], "entities": []}, {"text": "However, placing the rule information in texture memory improves the performance little as there are many more accesses to the scores array than to the rule information.", "labels": [], "entities": []}, {"text": "The GTX480 is the Fermi architecture, with many features added to the GTX285.", "labels": [], "entities": []}, {"text": "The number of cores doubled from 240 to 480, but the number of SMs was halved from 30 to 15.", "labels": [], "entities": []}, {"text": "The biggest difference is the introduction of L1 cache as well as the shared memory per SM.", "labels": [], "entities": []}, {"text": "For these reasons, all parallel implementations are faster on the GTX480 than on the GTX285.", "labels": [], "entities": []}, {"text": "On the GTX480, parallelizing over spans (SS) does not improve the performance, but actually degrades it.", "labels": [], "entities": [{"text": "parallelizing over spans (SS)", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7882390568653742}]}, {"text": "This is because this GPU has L1 cache and a higher global memory bandwidth, so that reducing the parallelism actually limits the performance.", "labels": [], "entities": []}, {"text": "Utilizing texture memory or shared memory for the scores array does not help either.", "labels": [], "entities": []}, {"text": "This is because the GTX480 hardware already caches the scores array into the L1 cache.", "labels": [], "entities": []}, {"text": "Interestingly, the ranking of the various parallelization configurations in terms of speedup is architecture dependent: on the GTX285, the blockbased mapping and sequential span processing are preferred, and the parallel reduction is preferred over shared-memory atomic operations.", "labels": [], "entities": []}, {"text": "Using texture memory is also helpful on the GTX285.", "labels": [], "entities": []}, {"text": "On the GTX480, block-based mapping is also preferred but sequential spans mapping is not.", "labels": [], "entities": []}, {"text": "The parallel reduction is clearly better than sharedmemory atomic operations, and there is no need for utilizing texture memory on the GTX480.", "labels": [], "entities": []}, {"text": "It is important to understand how the different design choices affect the performance, since one different choices might be necessary for grammars with different numbers of symbols and rules.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental platforms specifications.", "labels": [], "entities": []}]}