{"title": [{"text": "A Dependency Based Statistical Translation Model", "labels": [], "entities": [{"text": "Dependency Based Statistical Translation", "start_pos": 2, "end_pos": 42, "type": "TASK", "confidence": 0.631908230483532}]}], "abstractContent": [{"text": "We present a translation model based on dependency trees.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9655808210372925}]}, {"text": "The model adopts a tree-to-string approach and extends Phrase-Based translation (PBT) by using the dependency tree of the source sentence for selecting translation options and for reordering them.", "labels": [], "entities": [{"text": "Phrase-Based translation (PBT)", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.7743634045124054}]}, {"text": "Decoding is done by translating each node in the tree and combining its translations with those of its head in alternative orders with respect to its siblings.", "labels": [], "entities": [{"text": "Decoding", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9517266154289246}]}, {"text": "Reordering of the siblings exploits a heu-ristic based on the syntactic information from the parse tree which is learned from the corpus.", "labels": [], "entities": []}, {"text": "The decoder uses the same phrase tables produced by a PBT system for looking up translations of single words or of partial sub-trees.", "labels": [], "entities": []}, {"text": "A mathematical model is presented and experimental results are discussed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several efforts are being made to incorporate syntactic analysis into phrase-base statistical translation (PBT), which represents the state of the art in terms of robustness in modeling local word reordering and efficiency in decoding.", "labels": [], "entities": [{"text": "phrase-base statistical translation (PBT)", "start_pos": 70, "end_pos": 111, "type": "TASK", "confidence": 0.7422078251838684}]}, {"text": "Syntactic analysis is meant to improve some of the pitfalls of PBT: Translation options selection: candidate phrases for translation are selected as consecutive ngrams.", "labels": [], "entities": [{"text": "PBT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.8191387057304382}, {"text": "Translation options selection", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.8042878111203512}]}, {"text": "This may miss to consider certain syntactic phrases if their component words are far apart.", "labels": [], "entities": []}, {"text": "Phrase reordering: especially for languages with different word order, e.g. subject-verbobject (SVO) and subject-object-verb (SVO) languages, long distance reordering is a problem.", "labels": [], "entities": [{"text": "Phrase reordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.922175258398056}, {"text": "long distance reordering", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.6244141757488251}]}, {"text": "This has been addressed with a distance based distortion model), lexicalized phrase reordering), by hierarchical phrase reordering model ( or by reordering the nodes in a dependency tree () Movement of translations of fertile words: a word with fertility higher than one can be translated into several words that do not occur consecutively.", "labels": [], "entities": []}, {"text": "For example, the Italian sentence \"Lui partir\u00e0 domani\" translates into German as \"Er wird morgen abreisen\".", "labels": [], "entities": []}, {"text": "The Italian word \"partir\u00e0\" (meaning \"will leave\") translates into \"wird gehen\" in German, but the infinite \"abreisen\" goes to the end of the sentence with a movement that might be quite long.", "labels": [], "entities": []}, {"text": "Reordering of phrases is necessary because of different word order typologies of languages: constituent word order like SOV for Hindi vs. SVO for English; order of modifiers like noun-adjective for French, Italian vs. adjective-noun in English.", "labels": [], "entities": []}, {"text": "tackle this issue by introducing a reordering approach based on manual rules that are applied to the parse tree produced by a dependency parser.", "labels": [], "entities": []}, {"text": "However the splitting phenomenon mentioned above requires more elaborate solutions than simple reordering grammatical rules.", "labels": [], "entities": []}, {"text": "Several schemes have been proposed for improving PBMT systems based on dependency trees.", "labels": [], "entities": [{"text": "PBMT", "start_pos": 49, "end_pos": 53, "type": "TASK", "confidence": 0.9373824000358582}]}, {"text": "Our approach extends basic PBT as de-scribed in) with the following differences: we perform tree-to-string translation.", "labels": [], "entities": [{"text": "tree-to-string translation", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.7635823488235474}]}, {"text": "The dependency tree of the source language sentence allows identifying syntactically meaningful phrases as translation options, instead of ngrams.", "labels": [], "entities": []}, {"text": "However these phrases are then still looked up in a Phrase Translation quite similarly to PBT.", "labels": [], "entities": [{"text": "Phrase Translation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7205750942230225}, {"text": "PBT", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.934942901134491}]}, {"text": "Thus we avoid the sparseness problem that other methods based on treelets suffer).", "labels": [], "entities": []}, {"text": "reordering of phrases is carried out traversing the dependency tree and selecting as options phrases that are children of each head.", "labels": [], "entities": []}, {"text": "Hence afar away but logically connected portion of a phrase can be included in the reordering.", "labels": [], "entities": []}, {"text": "phrase combination is performed by combining the translations of anode with those of its head.", "labels": [], "entities": [{"text": "phrase combination", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7741162180900574}]}, {"text": "Hence only phrases that have a syntactic relation are connected.", "labels": [], "entities": []}, {"text": "The Language Model (LM) is still consulted to ensure that the combination is proper, and the overall score of each translation is carried along.", "labels": [], "entities": []}, {"text": "when all the links in the parse tree have been reduced, the root node contains candidate translations for the whole sentences alternative visit orderings of the tree may produce different translations so the final translation is the one with the highest score.", "labels": [], "entities": []}], "datasetContent": [{"text": "Moses () is used as a baseline phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9219357967376709}]}, {"text": "The following tools and data were used in our experiments: 1) the IRSTLM toolkit) is used to train a 5-gram language mod-el with Kneser-Ney smoothing on a set of 4.5 million sentences from the Italian Wikipedia.", "labels": [], "entities": []}, {"text": "2) the Europarl version 6 corpus, consisting of 1,703,886 sentence pairs, is used for training.", "labels": [], "entities": [{"text": "Europarl version 6 corpus", "start_pos": 7, "end_pos": 32, "type": "DATASET", "confidence": 0.9571516662836075}]}, {"text": "A tuning set of 2000 sentences from ACL WMT 2007 is used to tune the parameters.", "labels": [], "entities": [{"text": "ACL WMT 2007", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9067650238672892}]}, {"text": "3) the model is trained with lexical reordering.", "labels": [], "entities": []}, {"text": "4) the model is tuned with mert 5) the official test set from, consisting of 2000 sentences, is used as test set.", "labels": [], "entities": []}, {"text": "6) the open-source parser DeSR is used to parse Italian sentences, trained on the Evalita 2009 corpus ().", "labels": [], "entities": [{"text": "Evalita 2009 corpus", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.9307279189427694}]}, {"text": "Parser domain adaptation is obtained by adding to this corpus a set of 1200 sentences from the ACL WMT 2005 test set, parsed by DeSR and then corrected by hand.", "labels": [], "entities": [{"text": "Parser domain adaptation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8495978911717733}, {"text": "ACL WMT 2005 test set", "start_pos": 95, "end_pos": 116, "type": "DATASET", "confidence": 0.8806527733802796}, {"text": "DeSR", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.883231520652771}]}, {"text": "Both the training corpora and the test set had to be cleaned in order to normalize tokens: for example the English versions contained possessives split like this \"Florence' s\".", "labels": [], "entities": []}, {"text": "We applied the same tokenizer used by the parser which conforms to the PTB standard.", "labels": [], "entities": [{"text": "PTB standard", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9746103584766388}]}, {"text": "DeSR achieved a Labeled Accuracy Score of 88.67% at Evalita 2009, but for the purpose of translation, just the Unlabeled Accuracy is relevant, which was 92.72%.", "labels": [], "entities": [{"text": "Labeled Accuracy Score", "start_pos": 16, "end_pos": 38, "type": "METRIC", "confidence": 0.7744355400403341}, {"text": "Evalita 2009", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9120536744594574}, {"text": "translation", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9785434603691101}]}, {"text": "The table below shows the results of our decoder (Desrt) in the translation from Italian to English, compared to a baseline Moses system trained on the same corpora and to the online version of Google translate.", "labels": [], "entities": []}, {"text": "Desrt was run with abeam size of 10, since experiments showed no improvements with a larger beam size.", "labels": [], "entities": [{"text": "Desrt", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9248328804969788}]}, {"text": "We show two versions of Desrt, one with parse trees as obtained by the parser and one (Desrt gold) where the trees were corrected by hand.", "labels": [], "entities": []}, {"text": "The difference is minor and this confirms that the Since we used the same phrase table produced by Moses also for Desrt, Moses has an advantage, because it can lookup n-grams that do not correspond to grammatical phrases, which Desrt never considers.", "labels": [], "entities": []}, {"text": "In order to determine how this affects the results, we tested Moses restricting its choice to phrases corresponding to treelets form the parse tree.", "labels": [], "entities": []}, {"text": "The result is shown in the row in the table labeled as \"Moses tree phrases\".", "labels": [], "entities": []}, {"text": "The score is lower, as expected, but this confirms that Desrt makes quite good use of the portion of the phrase table it uses.", "labels": [], "entities": []}, {"text": "Since the version of the reordering algorithm we used produces a single reordering, the Desrt decoder has linear complexity on the length of the sentence.", "labels": [], "entities": []}, {"text": "Indeed, despite being written in Python and having to query the PT as a network service, it is quite faster than Moses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Results of the experiments.", "labels": [], "entities": []}]}