{"title": [{"text": "Discriminative features in reversible stochastic attribute-value grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "Reversible stochastic attribute-value grammars (de Kok et al., 2011) use one model for parse disambiguation and fluency ranking.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.8697819411754608}]}, {"text": "Such a model encodes preferences with respect to syntax, fluency, and appropriate-ness of logical forms, as weighted features.", "labels": [], "entities": []}, {"text": "Reversible models are built on the premise that syntactic preferences are shared between parse disambiguation and fluency ranking.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.7966870963573456}]}, {"text": "Given that reversible models also use features that are specific to parsing or generation , there is the possibility that the model is trained to rely on these directional features.", "labels": [], "entities": [{"text": "parsing or generation", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8468019167582194}]}, {"text": "If this is true, the premise that preferences are shared between parse disambiguation and fluency ranking does not hold.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8374084532260895}]}, {"text": "In this work, we compare and apply feature selection techniques to extract the most discrim-inative features from directional and reversible models.", "labels": [], "entities": []}, {"text": "We then analyse the contributions of different classes of features, and show that reversible models do rely on task-independent features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reversible stochastic attribute-value grammars) provide an elegant framework that fully integrates parsing and generation.", "labels": [], "entities": []}, {"text": "The most important contribution of this framework is that it uses one conditional maximum entropy model for fluency ranking and parse disambiguation.", "labels": [], "entities": [{"text": "fluency ranking", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.8107419908046722}, {"text": "parse disambiguation", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.9421701431274414}]}, {"text": "This constraint is: Where C = T \u222a L, \u02dc p(c) is the empirical probability of a set of constraints c, and\u02dcpand\u02dc and\u02dcp(c, d) the joint probability of a set of constraints c and a derivation d.", "labels": [], "entities": []}, {"text": "Reversible stochastic-attribute grammars rest on the premise that preferences are shared between language comprehension and production.", "labels": [], "entities": []}, {"text": "For instance, in Dutch, subject fronting is preferred over direct object fronting.", "labels": [], "entities": [{"text": "subject fronting", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7545700669288635}, {"text": "direct object fronting", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7288234631220499}]}, {"text": "If models for parse disambiguation and fluency ranking do not share preferences with respect to fronting, it would be difficult fora parser to recover the logical form that was the input to a generator.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.9247752130031586}]}, {"text": "Reversible models incorporate features that are specific to parse disambiguation and fluency ranking, as well as features that are used for both tasks.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.924805611371994}]}, {"text": "Previous work has shown through feature analysis that taskindependent features are indeed useful in directional models.", "labels": [], "entities": []}, {"text": "However, since reversible models assign just one weight to each feature regardless the task, one particular concern is that much of their discriminatory power is provided by task-specific features.", "labels": [], "entities": []}, {"text": "If this is true, the premise that similar preferences are used in parsing and generation does not hold.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9693074822425842}]}, {"text": "In this work, we will isolate the most discriminative features of reversible models through feature selection, and make a quantitative and qualitative analysis of these features.", "labels": [], "entities": []}, {"text": "Our aim is to to verify that reversible models do rely on features used both in parsing and generation.", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7216634353001913}]}, {"text": "To find the most effective features of a model, we need an effective feature selection method.", "labels": [], "entities": []}, {"text": "Section 2 describes three such methods: grafting, graftinglight, and gain-informed selection.", "labels": [], "entities": []}, {"text": "These methods are compared empirically in Section 4 using the experimental setup described in Section 3.", "labels": [], "entities": []}, {"text": "We then use the best feature selection method to perform quantitative and qualitative analyses of reversible models in Sections 5 and 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate each selection method stepwise.", "labels": [], "entities": []}, {"text": "We train and evaluate a model on the best-n features according to each selection method, for n = [0..1711].", "labels": [], "entities": []}, {"text": "In each case, the feature weights are estimated with TinyEst using a 1 norm coefficient of 0.0002.", "labels": [], "entities": []}, {"text": "This stepwise evaluation allows us to capture the effectiveness of each method.", "labels": [], "entities": []}, {"text": "Parse disambiguation and fluency ranking models are evaluated on the WR-P-P-H corpus that was described in Section 3.1, using CA and GTM scores respectively.", "labels": [], "entities": [{"text": "Parse", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7835513949394226}, {"text": "WR-P-P-H corpus", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.87237548828125}]}, {"text": "shows the performance of the feature selection methods for parse disambiguation.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.9762919843196869}]}, {"text": "This graph shows that that both grafting methods are far more effective than gain-informed selection.", "labels": [], "entities": []}, {"text": "We can also see that only a small number of features is required to construct a competitive model.", "labels": [], "entities": []}, {"text": "Selecting more features improves the model only gradually.", "labels": [], "entities": []}, {"text": "shows the performance of the feature selection methods in fluency ranking.", "labels": [], "entities": []}, {"text": "Again, we seethe same trend as in parse disambiguation.", "labels": [], "entities": []}, {"text": "The grafting and grafting-light methods outperform gain-informed selection, with the grafting method coming out on top.", "labels": [], "entities": []}, {"text": "In feature selection, even a smaller number of features is required to train an effective model.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7610827386379242}]}, {"text": "After selecting more than approximately 50 features, adding features only improves the model very gradually.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 provides class-based counts of the 300 most  discriminative features for the parse disambiguation  and reversible models. Since the n-gram features are  not active during parse disambiguation, they are not  selected for the parse disambiguation model. All  other classes of features are used in the parse dis- ambiguation model. The reversible model uses all  classes of features.", "labels": [], "entities": []}, {"text": " Table 2: Per-class contribution to the improvement of the  model over the base baseline in parse disambiguation.", "labels": [], "entities": []}, {"text": " Table 3: Per-class counts of the best 300 features accord- ing to the grafting method.", "labels": [], "entities": []}, {"text": " Table 4: Per-class contribution to the improvement of the  model over the baseline in fluency ranking.", "labels": [], "entities": []}, {"text": " Table 5: Classes giving a net positive distribution, with  normalized contributions.", "labels": [], "entities": []}, {"text": " Table 6: The twenty most discriminative features of the  reversible model, and their polarities.", "labels": [], "entities": []}]}