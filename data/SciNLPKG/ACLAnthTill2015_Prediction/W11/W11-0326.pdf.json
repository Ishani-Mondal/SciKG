{"title": [{"text": "Composing Simple Image Descriptions using Web-scale N-grams", "labels": [], "entities": [{"text": "Composing Simple Image Descriptions", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6847755685448647}]}], "abstractContent": [{"text": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world.", "labels": [], "entities": []}, {"text": "In turn, it can also help us in the quest to generate natural language that describes this world in a human manner.", "labels": [], "entities": []}, {"text": "We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams.", "labels": [], "entities": []}, {"text": "Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch.", "labels": [], "entities": [{"text": "summarizes or retrieves pre-existing text relevant to an image", "start_pos": 31, "end_pos": 93, "type": "TASK", "confidence": 0.8634590440326266}]}, {"text": "Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description making for more human-like annotations than previous approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Gaining a better understanding of natural language, and especially natural language associated with images helps drive research in both computer vision and natural language processing (e.g.,,,).", "labels": [], "entities": []}, {"text": "In this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular, in a simple yet highly effective approach to compose image descriptions in natural language.", "labels": [], "entities": []}, {"text": "Automatic generation of image descriptions differs from automatic image tagging (e.g.,) in that we aim to generate complex phrases or sentences describing images rather than predicting individual words.", "labels": [], "entities": [{"text": "Automatic generation of image descriptions", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8580019235610962}, {"text": "image tagging", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.754300445318222}]}, {"text": "These natural language descriptions can be useful fora variety of applications, including image retrieval, automatic video surveillance, and providing image interpretations for visually impaired people.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.8072596192359924}, {"text": "automatic video surveillance", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.6193738281726837}, {"text": "image interpretations", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.7551238536834717}]}, {"text": "Our work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (), or summarizing existing text fragments associated with an image (e.g.,,).", "labels": [], "entities": [{"text": "summarizing existing text fragments associated with an image", "start_pos": 145, "end_pos": 205, "type": "TASK", "confidence": 0.8094929829239845}]}, {"text": "Second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related (but subtly different) work in automatic caption generation creates news-worthy text) or encyclopedic text) that is contextually relevant to the image, but not closely pertinent to the specific content of the image.", "labels": [], "entities": [{"text": "automatic caption generation", "start_pos": 151, "end_pos": 179, "type": "TASK", "confidence": 0.7534154852231344}]}, {"text": "Third, we aim to build a general image description method as compared to work that requires domain specific hand-written grammar rules ().", "labels": [], "entities": [{"text": "image description", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.714329183101654}]}, {"text": "Last, we allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs ().", "labels": [], "entities": []}, {"text": "In this work, we propose a novel surface realization technique based on web-scale n-gram data.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7516341805458069}]}, {"text": "Our approach consists of two steps: (n-gram) phrase selection and (n-gram) phrase fusion.", "labels": [], "entities": [{"text": "phrase selection", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7461755573749542}, {"text": "phrase fusion", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.7349252700805664}]}, {"text": "The first step -phrase selection -collects candidate phrases that maybe potentially useful for generating the description of a given image.", "labels": [], "entities": [{"text": "phrase selection", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7326915860176086}]}, {"text": "This step naturally accommodates uncertainty in image recognition inputs as well as synonymous words and word re-ordering to improve fluency.", "labels": [], "entities": [{"text": "image recognition inputs", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.7637560367584229}]}, {"text": "The second step -phrase fusion -finds the optimal compatible set of phrases using dynamic programming to compose anew (and more complex) phrase that describes the image.", "labels": [], "entities": [{"text": "phrase fusion", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7072619944810867}]}, {"text": "We compare the performance of our proposed approach to three baselines based on conventional techniques: language models, parsers, and templates.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our approach is highly effective for composing image descriptions: it generates mostly appealing and presentable language, while permitting creative writing at times (see.", "labels": [], "entities": [{"text": "composing image descriptions", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.7211405038833618}]}, {"text": "We conclude from our exploration that (1) it is viable to generate simple textual descriptions that are germane to the specific image content, and that (2) world knowledge implicitly encoded in natural language (e.g., web-scale ngram data) can help enhance image content recognition.", "labels": [], "entities": [{"text": "image content recognition", "start_pos": 257, "end_pos": 282, "type": "TASK", "confidence": 0.6701911389827728}]}], "datasetContent": [{"text": "To construct the training corpus for language models, we crawled Wikipedia pages that describe our object set.", "labels": [], "entities": []}, {"text": "For evaluation, we use the UIUC PAS-CAL sentence dataset 3 which contains upto five human-generated sentences that describing 1000 images.", "labels": [], "entities": [{"text": "UIUC PAS-CAL sentence dataset", "start_pos": 27, "end_pos": 56, "type": "DATASET", "confidence": 0.8261153995990753}]}, {"text": "Note that all of the approaches presented in Template This is a picture of two motorbikes, three persons, one building and one tree.", "labels": [], "entities": []}, {"text": "The first shiny motorbike is against the second shiny motorbike, and by the first black person.", "labels": [], "entities": []}, {"text": "The second shiny motorbike is by the first black person, and by the second shiny person, and by the third pink person.", "labels": [], "entities": []}, {"text": "In this work, we limit the choice of function words to only those words that are likely to be necessary in the final output.", "labels": [], "entities": []}, {"text": "For instance, we disallow function words such as \"who\" or \"or\".", "labels": [], "entities": []}, {"text": "Before presenting evaluation results, we present some samples of image descriptions generated by 4 different approaches in and 4.", "labels": [], "entities": []}, {"text": "Notice that only the PHRASE FUSION approach is able to include interesting and adequate verbs, such as \"eating\" or \"looking\" in, and \"operating\" in.", "labels": [], "entities": [{"text": "PHRASE", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9584208130836487}, {"text": "FUSION", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.7308675050735474}]}, {"text": "Note that the choice of these action verbs is based only on the co-occurrence statistics encoded in n-grams, without relying on the vision component that specializes inaction recognition.", "labels": [], "entities": []}, {"text": "These examples therefore demonstrate that world knowledge implicitly encoded in natural language can help enhance image content recognition.", "labels": [], "entities": [{"text": "image content recognition", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.7074751754601797}]}, {"text": "Automatic Evaluation: BLEU () is a widely used metric for automatic evaluation of machine translation that measures the ngram precision of machine generated sentences with respect to human generated sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9965123534202576}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.725126326084137}, {"text": "ngram precision", "start_pos": 120, "end_pos": 135, "type": "METRIC", "confidence": 0.8386655449867249}]}, {"text": "Because our task can be viewed as machine translation from images to text, BLEU () may seem This limitation does not apply to TEMPLATE.", "labels": [], "entities": [{"text": "machine translation from images to text", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.8014029363791147}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9990012049674988}]}, {"text": "However, there is larger inherent variability in generating sentences from images than translating a sentence from one language to another.", "labels": [], "entities": []}, {"text": "In fact two people viewing the same picture may produce quite different descriptions.", "labels": [], "entities": []}, {"text": "This means BLEU could penalize many correctly generated sentences, and be poorly correlated with human judgment of quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9937832355499268}]}, {"text": "Nevertheless we report BLEU scores in absence of any other automatic evaluation method that serves our needs perfectly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9993466734886169}]}, {"text": "The results are shown in -first column shows BLEU score considering exact matches, second column shows BLEU with full credit for synonyms.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9765471816062927}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9974274039268494}]}, {"text": "To give a sense of upper bound and to see some limitations of the BLEU score, we also compute the BLEU score between human-generated sentences by computing the BLEU score of the first human sentence with respect to the others.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9724007248878479}, {"text": "BLEU score", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9848097860813141}, {"text": "BLEU score", "start_pos": 160, "end_pos": 170, "type": "METRIC", "confidence": 0.9808548092842102}]}, {"text": "There is one important factor to consider when interpreting.", "labels": [], "entities": [{"text": "interpreting", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.9741981625556946}]}, {"text": "The four approaches explored in this paper are purposefully prolific writers in that they generate many more sentences than the number of sentences in the image descriptions written by humans (available in the UIUC PASCAL dataset).", "labels": [], "entities": [{"text": "UIUC PASCAL dataset", "start_pos": 210, "end_pos": 229, "type": "DATASET", "confidence": 0.8970008293787638}]}, {"text": "In this work, we do not perform sentence selection to reduce the number of sentences in the final output.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.750658392906189}]}, {"text": "Rather, we focus on the quality of each generated sentence.", "labels": [], "entities": []}, {"text": "The consequence of producing many more sentences in our output is overall lower BLEU scores, because BLEU precision penalizes spurious repetitions of the same word, which necessarily occurs when generating more sentences.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9783366918563843}, {"text": "BLEU precision", "start_pos": 101, "end_pos": 115, "type": "METRIC", "confidence": 0.828511655330658}]}, {"text": "This is not an issue for comparing different approaches however, as we generate the same number of sentences for each method.", "labels": [], "entities": []}, {"text": "From, we find that our final approach -PHRASE FUSION based on web-scale n-grams performs the best.", "labels": [], "entities": [{"text": "PHRASE", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9973502159118652}, {"text": "FUSION", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.7472653388977051}]}, {"text": "Notice that there are two different evaluations for PHRASE FUSION: the first one is evaluated for the best combination of phrases (Equation), while the second one is evaluated for the best combination of phrases that contained at least one gerund.", "labels": [], "entities": [{"text": "PHRASE FUSION", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.45656782388687134}, {"text": "Equation)", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9782783687114716}]}, {"text": "Human Evaluation: As mentioned earlier, BLEU score has some drawbacks including obliviousness to correctness of grammar and inability to evaluate the creativity of a composition.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9784547090530396}]}, {"text": "To directly quantify these aspects that could not be addressed by BLEU, we perform human judgments on 120 instances for the four proposed methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9668297171592712}]}, {"text": "Evaluators do not have any computer vision or natural language generation background.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7487897674242655}]}, {"text": "We consider the following three aspects to evaluate the our image descriptions: creativity, fluency, and relevance.", "labels": [], "entities": []}, {"text": "For simplicity, human evaluators assign one set of scores for each aspect per image.", "labels": [], "entities": []}, {"text": "The scores range from 1 to 3, where 1 is very good, 2 is ok, and 3 is bad.", "labels": [], "entities": []}, {"text": "The definition and guideline for each aspect is: [Creativity] How creative is the generated sentence?", "labels": [], "entities": []}, {"text": "1 There is creativity either based on unexpected words (in particular, verbs), or describing things in a poetic way.", "labels": [], "entities": []}, {"text": "2 There is minor creativity based on re-ordering words that appeared in the triple 3 None.", "labels": [], "entities": []}, {"text": "Looks like a robot talking.", "labels": [], "entities": []}, {"text": "[Fluency] How grammatically correct is the generated sentence?", "labels": [], "entities": []}, {"text": "1 Mostly perfect English phrase or sentence.", "labels": [], "entities": []}, {"text": "2 There are some errors, but mostly comprehensible.", "labels": [], "entities": []}, {"text": "[Relevance] How relevant is the generated description to the given image?", "labels": [], "entities": []}, {"text": "Notice that the relevance score of TEMPLATE is better than that of LANGUAGE MODEL, even though both approaches generate descriptions that consist of an almost identical set of words.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.9775629043579102}, {"text": "TEMPLATE", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.7899652719497681}, {"text": "LANGUAGE MODEL", "start_pos": 67, "end_pos": 81, "type": "METRIC", "confidence": 0.6739895641803741}]}, {"text": "This is presumably because the output from LANGUAGE MODEL contains grammatically incorrect sentences that are not comprehendable enough to the evaluators.", "labels": [], "entities": [{"text": "LANGUAGE MODEL", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.4871772676706314}]}, {"text": "The relevance score of PHRASE FUSION is also slightly worse than that of TEMPLATE, presumably because PHRASE FUSION often generates poetic or creative expressions, as shown in, which can be considered a deviation from the image content.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9712315499782562}]}, {"text": "Error Analysis There are different sources of errors.", "labels": [], "entities": []}, {"text": "Some errors are due to mistakes in the original visual recognition input.", "labels": [], "entities": []}, {"text": "For example, in the 3rd image in, the color of sky is predicted to be \"golden\".", "labels": [], "entities": []}, {"text": "In the 4th image, the wall behind the table is recognized as \"sky\", and in the 6th image, the parrots are recognized as \"person\".", "labels": [], "entities": []}, {"text": "Other errors are from surface realization.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7315380573272705}]}, {"text": "For instance, in the 8th image, PHRASE FUSION selects the preposition \"under\", presumably because dogs are typically under the chair rather than on the chair according to Google n-gram statistics.", "labels": [], "entities": [{"text": "PHRASE", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9330560564994812}, {"text": "FUSION", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.6799260377883911}]}, {"text": "In the 5th image, an unexpected word \"burning\" is selected to make the resulting output idiosyncratic.", "labels": [], "entities": []}, {"text": "Word sense disambiguation sometimes causes a problem in surface realization as well.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6317433218161265}, {"text": "surface realization", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7313339114189148}]}, {"text": "In the 3rd image, the word \"way\" is chosen to represent \"path\" or \"street\" by the image recognizer.", "labels": [], "entities": []}, {"text": "However, a different sense of way -\"very\" -is being used in the final output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Automatic Evaluation: BLEU measured at 1", "labels": [], "entities": [{"text": "Automatic Evaluation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7763925194740295}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9970816969871521}]}, {"text": " Table 3: Human Evaluation: the scores range over 1 to 3,  where 1 is very good, 2 is ok, 3 is bad.", "labels": [], "entities": []}]}