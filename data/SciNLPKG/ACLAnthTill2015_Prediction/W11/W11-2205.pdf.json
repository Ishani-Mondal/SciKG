{"title": [{"text": "Evaluating unsupervised learning for natural language processing tasks", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6623567442099253}]}], "abstractContent": [{"text": "The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research.", "labels": [], "entities": []}, {"text": "The primary advantage of these methods is that they do not require annotated data to learn a model.", "labels": [], "entities": []}, {"text": "However, this advantage makes them difficult to evaluate against a manually labeled gold standard.", "labels": [], "entities": []}, {"text": "Using un-supervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.7275225222110748}]}, {"text": "Instead , we argue that the rarely used in-context evaluation is more appropriate and more informative , as it takes into account the way these methods are likely to be applied.", "labels": [], "entities": []}, {"text": "Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsuper-vised natural language processing.", "labels": [], "entities": []}], "introductionContent": [{"text": "The development of unsupervised learning methods for natural language processing (NLP) tasks has become an important and popular area of research.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.742287848676954}]}, {"text": "The main attraction of these methods is that they can learn a model using only unlabeled data.", "labels": [], "entities": []}, {"text": "This is an important advantage, as unlabeled text in digital form is in abundance, while labeled datasets are usually expensive to construct.", "labels": [], "entities": []}, {"text": "While methods such as crowdsourcing ( can help reduce this cost, in tasks for which specialist knowledge is required, such as part-of-speech (PoS) tagging or syntactic parsing, labeling datasets in this fashion can be substantially harder.", "labels": [], "entities": [{"text": "part-of-speech (PoS) tagging", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.6446098387241364}, {"text": "syntactic parsing", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.6731157600879669}]}, {"text": "Nevertheless, the advantage of requiring only unlabeled data to learn a model renders the evaluation of unsupervised learning methods to be more challenging than that of their supervised counterparts.", "labels": [], "entities": []}, {"text": "This is primarily because the output of unsupervised methods does not contain labels that would be found in a manually constructed gold standard.", "labels": [], "entities": []}, {"text": "Simplistically expressed, no labels for model learning means that there are no labels in the output.", "labels": [], "entities": []}, {"text": "As a result, the standard evaluation paradigm of comparing against a gold standard using a performance measure such as accuracy or F-score cannot be used, at least not in the way it would be used in evaluating supervised methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9992782473564148}, {"text": "F-score", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9821391701698303}]}, {"text": "Since methods are proposed or rejected by researchers, and papers describing these methods are assessed by their peers partly on the basis of such results, the issue of evaluation is an important one.", "labels": [], "entities": []}, {"text": "Before we proceed, it is important to characterize the unsupervised learning methods we are considering, as the term unsupervised is used in multiple ways in the literature.", "labels": [], "entities": []}, {"text": "In this work we focus on methods that use only unlabeled data to learn a model and do not involve any form of supervision at any stage.", "labels": [], "entities": []}, {"text": "Thus we exclude methods that use seeds such as the dictionaries of PoS tags used by and rules for producing labeled output, e.g. those proposed by.", "labels": [], "entities": []}, {"text": "We also exclude methods for which the data used to learn a model does not contain any of the labels we are learning to predict, but it does contain other information that we use in the learning process.", "labels": [], "entities": []}, {"text": "For example, the multilingual PoS induction approach of assumes no supervision for the language whose PoS tags are being induced, but it assumes access to a labeled dataset of a different language.", "labels": [], "entities": [{"text": "PoS induction", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.8331862986087799}]}, {"text": "We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2).", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.8757959306240082}]}, {"text": "While PoS tagging is not the only task for which unsupervised learning methods are popular, its relative simplicity and the variety of evaluation paradigms employed make it a useful case study.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.8673947155475616}]}, {"text": "Based on this survey, we show that evaluation against a PoS tagging gold standard is not only difficult, but it can be misleading as well.", "labels": [], "entities": [{"text": "PoS tagging gold standard", "start_pos": 56, "end_pos": 81, "type": "DATASET", "confidence": 0.7232322543859482}]}, {"text": "The reason for this is that the unsupervised learning methods used, while they produce output that correlates with PoS tags, perform a different task, namely clustering-based word representation induction (.", "labels": [], "entities": [{"text": "clustering-based word representation induction", "start_pos": 158, "end_pos": 204, "type": "TASK", "confidence": 0.7710711285471916}]}, {"text": "Instead, we argue that in-context evaluation is more appropriate and more informative, as it takes into account the application context in which these methods are intended to be used (Section 3).", "labels": [], "entities": []}, {"text": "Finally, bearing the issue of evaluation in mind, we propose some directions for future work in unsupervised learning for NLP (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "All the papers on unsupervised PoS tagging mentioned in the previous section agree on the fact that its evaluation, at least using clustering evaluation measures, is difficult.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.8886405229568481}]}, {"text": "This is an important problem for other NLP tasks (e.g. anaphora resolution, word sense induction) in which systems produce clusters that need to be mapped to gold standard classes.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7133757770061493}, {"text": "word sense induction)", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7707206010818481}]}, {"text": "In their recent position paper, argue that the problem lies in ignoring the context in which clustering is performed.", "labels": [], "entities": []}, {"text": "They distinguish between two such contexts.", "labels": [], "entities": []}, {"text": "The first one is the use of clustering as a pre-processing step fora downstream task, in which the evaluation of the latter is used to evaluate the former.", "labels": [], "entities": []}, {"text": "The second context is that of data exploration in order to assist a human to analyze a large dataset.", "labels": [], "entities": [{"text": "data exploration", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7316723465919495}]}, {"text": "In this case, performance might not be as straightforward to assess, since it relies on many external factors among which the human computer interaction interface used is likely to be crucial.", "labels": [], "entities": []}, {"text": "We cumulatively refer to these evaluation paradigms as in-context evaluation.", "labels": [], "entities": []}, {"text": "Returning to unsupervised PoS tagging and NLP, the extrinsic evaluation of and Van Gael et al.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8353754580020905}]}, {"text": "(2009) falls under the pre-processing paradigm.", "labels": [], "entities": []}, {"text": "The approach of falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of.", "labels": [], "entities": [{"text": "data exploration", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7164071351289749}]}, {"text": "In-context evaluation can be used to assess the performance of unsupervised learning methods for tasks other than clustering-based word representation approaches.", "labels": [], "entities": []}, {"text": "For example, topic modeling ( has recently been used and evaluated in approaches to learning models of selectional preferences ().", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9003256857395172}]}, {"text": "The issues affecting the evaluation of unsupervised learning methods are not restricted to PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.862767219543457}]}, {"text": "discussed similar issues in the context of unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7354683578014374}]}, {"text": "Note that some of them arise due to the fact unsupervised dependency parsing produces unlabeled directed edges which are interpreted as denoting headdependent relations.", "labels": [], "entities": []}, {"text": "However, there are linguistic phenomena where unless the edges are labeled with a specific interpretation, both directions could be considered correct, e.g. the relation between modal verb and main verb.", "labels": [], "entities": []}, {"text": "Even though evaluation against a syntactic parsing gold standard is useful, we argue that in-context evaluation of the output of unsupervised dependency parsers is likely to be more informative and more appropriate.", "labels": [], "entities": []}, {"text": "Despite the criticism against clustering evaluation measures as well as other methods for comparing the output of unsupervised learning methods against a gold standard, we argue that they are still useful.", "labels": [], "entities": []}, {"text": "The various measures proposed, along with their inherent biases and definitions of clustering quality, provide quantitative analysis of the behavior of unsupervised learning methods by assessing correlations between their output and a gold standard.", "labels": [], "entities": []}, {"text": "This can be very useful when developing such methods, as their use is admittedly simpler than the in-context evaluation paradigms discussed.", "labels": [], "entities": []}, {"text": "However, they are not as informative as in-context evaluation and they should not be used to draw strong conclusions about the usefulness of a method.", "labels": [], "entities": []}, {"text": "Acknowledging that the evaluation of unsupervised learning for NLP is better performed incontext instead of against a labeled gold standard leads to the use of more appropriate experimental setups.", "labels": [], "entities": []}, {"text": "Sometimes unsupervised learning methods are restricted to learning models using the unlabeled gold standard against which they are evaluated subsequently.", "labels": [], "entities": []}, {"text": "Thus, they neither take full advantage of nor they demonstrate their main strength, which is that they can use as much data as possible.", "labels": [], "entities": []}, {"text": "Using the pre-processing paradigm, clusteringbased word representations induced from a large unlabeled dataset would be evaluated according to whether they improve the performance of the downstream task they are evaluated with, whose evaluation is likely to be on a different dataset.", "labels": [], "entities": []}, {"text": "This use of clustering-based word representation is sometimes referred to as semi-supervised learning and has been shown to be effective in a variety of tasks, including named entity recognition, shallow parsing and syntactic dependency parsing (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 170, "end_pos": 194, "type": "TASK", "confidence": 0.6277115742365519}, {"text": "shallow parsing", "start_pos": 196, "end_pos": 211, "type": "TASK", "confidence": 0.6610423922538757}, {"text": "syntactic dependency parsing", "start_pos": 216, "end_pos": 244, "type": "TASK", "confidence": 0.6541240115960439}]}, {"text": "The use of large datasets would also help assess the scalability of the unsupervised methods proposed, as the amount of data that can be handled efficiently by an unsupervised method can be as important as the range of linguistic intuitions it can capture.", "labels": [], "entities": []}, {"text": "To examine this trade-off, it would be informative to show performance curves with different amounts of data, which should be straightforward to produce under the pre-processing evaluation paradigm.", "labels": [], "entities": []}, {"text": "An added benefit is that, as discussed by, assessing clustering stability using multiple runs and sub-samples of a dataset can help establish whether a particular combination of clustering algorithm and user-defined parameters (including the number of clusters to be discovered) is able to discover an appropriate clustering of the dataset considered.", "labels": [], "entities": []}, {"text": "Avoiding comparisons against a labeled gold standard would also remove the temptation of adapting it to the output of the unsupervised learning method.", "labels": [], "entities": []}, {"text": "For example, in unsupervised PoS tagging authors sometimes simplify the gold standard by collapsing the original 45 PoS tags of the Penn treebank to 17, e.g. by removing the distinctions between different noun tags.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8734436333179474}, {"text": "Penn treebank", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9482104778289795}]}, {"text": "While such simplifications are linguistically plausible, they substitute one problem for another, as methods are no longer penalized for missing some of the finer distinctions, but they are penalized for making them.", "labels": [], "entities": []}, {"text": "Perhaps more importantly, they result in fitting the gold standard to the output of the method being evaluated, which is unlikely to be informative.", "labels": [], "entities": []}, {"text": "Another related issue is that since unsupervised learning methods do not need labeled data, it is a tempting and common practice to learn a model and report results on the same dataset, which usually consists of all the labeled data available and which is used to tune the parameters of the method evaluated.", "labels": [], "entities": []}, {"text": "This is equivalent to reporting results for supervised learning methods on the development set, while it is generally accepted that results on a separate test set on which no parameter tuning is allowed provide better performance estimates.", "labels": [], "entities": []}, {"text": "The use of the pre-processing evaluation paradigm with a supervised learning approach for the downstream task is likely to result in use the standard distinction between training, development and test set for the evaluation of unsupervised learning methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary the results reported for the three configurations (DP-learned, DP-fixed, PY-fixed) of the  unsupervised PoS tagger of Van Gael et al. (2009) and the two baselines (no PoS tags, supervised PoS tags).  Except for VI, higher scores mean better performance. The clustering evaluation scores (VI, V-measure, V- beta) are obtained by comparing against a PoS gold standard, while F-score and accuracy scores are obtained  by extrinsinc evaluation using shallow parsing.", "labels": [], "entities": [{"text": "F-score", "start_pos": 391, "end_pos": 398, "type": "METRIC", "confidence": 0.9851704239845276}, {"text": "accuracy", "start_pos": 403, "end_pos": 411, "type": "METRIC", "confidence": 0.9976626634597778}]}]}