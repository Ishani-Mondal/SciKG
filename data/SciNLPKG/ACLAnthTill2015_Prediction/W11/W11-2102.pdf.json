{"title": [{"text": "A Lightweight Evaluation Framework for Machine Translation Reordering", "labels": [], "entities": [{"text": "Machine Translation Reordering", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.878446360429128}]}], "abstractContent": [{"text": "Reordering is a major challenge for machine translation between distant languages.", "labels": [], "entities": [{"text": "Reordering", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9352776408195496}, {"text": "machine translation between distant languages", "start_pos": 36, "end_pos": 81, "type": "TASK", "confidence": 0.871062695980072}]}, {"text": "Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality.", "labels": [], "entities": []}, {"text": "Here we present a simple framework for evaluating word order independently of lexical choice by comparing the sys-tem's reordering of a source sentence to reference reordering data generated from manually word-aligned translations.", "labels": [], "entities": []}, {"text": "When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming end-to-end machine translation experiments.", "labels": [], "entities": []}, {"text": "A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion.", "labels": [], "entities": []}, {"text": "We show that how the alignments are generated can significantly effect the robust-ness of the evaluation.", "labels": [], "entities": []}, {"text": "We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation.", "labels": [], "entities": [{"text": "English to Japanese machine translation", "start_pos": 105, "end_pos": 144, "type": "TASK", "confidence": 0.6304173648357392}]}], "introductionContent": [{"text": "Statistical machine translation systems can perform poorly on distant language pairs such as English and Japanese.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6291716893513998}]}, {"text": "Reordering errors area major source of poor or misleading translations in such systems (.", "labels": [], "entities": []}, {"text": "Unfortunately the standard evaluation metrics used by the statistical machine translation community are relatively insensitive to the long-distance reordering phenomena encountered when translating between such languages ( . The ability to rapidly evaluate the impact of changes on a system can significantly accelerate the experimental cycle.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.6251313984394073}]}, {"text": "Ina large statistical machine translation system, we should ideally be able to experiment with separate components without retraining the complete system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6532470683256785}]}, {"text": "Measures such as perplexity have been successfully used to evaluate language models independently in speech recognition eliminating some of the need for end-to-end speech recognition experiments.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.8554580509662628}]}, {"text": "In machine translation, alignment error rate has been used with some mixed success to evaluate word-alignment algorithms but no standard evaluation frameworks exist for other components of a machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7929129004478455}, {"text": "alignment error rate", "start_pos": 24, "end_pos": 44, "type": "METRIC", "confidence": 0.7037814855575562}]}, {"text": "Unfortunately, BLEU () and other metrics that work with the final output of a machine translation system are both insensitive to reordering phenomena and relatively time-consuming to compute: changes to the system may require the realignment of the parallel training data, extraction of phrasal statistics and translation of a test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9988691210746765}, {"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.694743275642395}]}, {"text": "As training sets grow in size, the cost of end-to-end experimentation can become significant.", "labels": [], "entities": []}, {"text": "However, it is not clear that measurements made on any single part of the system will correlate well with human judgments of the translation quality of the whole system.", "labels": [], "entities": []}, {"text": "Following and, showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform stateof-the-art phrase-based and hierarchical machine translation systems.", "labels": [], "entities": []}, {"text": "This result is corroborated by whose results suggest that both phrase-based and hierarchical translation systems fail to capture long-distance reordering phenomena.", "labels": [], "entities": []}, {"text": "In this paper we describe a lightweight framework for measuring the quality of the reordering components in a machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7282061874866486}]}, {"text": "While our framework can be applied to any translation system in which it is possible to derive a token-level alignment from the input source tokens to the output target tokens, it is of particular practical interest when applied to a system that performs reordering as a preprocessing step (.", "labels": [], "entities": []}, {"text": "In this case, as we show, it allows for extremely rapid and sensitive analysis of changes to parser, reordering rules and other reordering components.", "labels": [], "entities": []}, {"text": "In our framework we evaluate the reordering proposed by a system separately from its choice of target words by comparing it to a reference reordering of the sentence generated from a manually wordaligned translation.", "labels": [], "entities": []}, {"text": "Unlike previous work, our approach does not rely on the system's output matching the reference translation lexically.", "labels": [], "entities": []}, {"text": "This makes the evaluation more robust as there maybe many ways to render a source phrase in the target language and we would not wish to penalize one that simply happens not to match the reference.", "labels": [], "entities": []}, {"text": "In the next section we review related work on reordering for translation between distant language pairs and automatic approaches to evaluating reordering in machine translation.", "labels": [], "entities": [{"text": "translation between distant language pairs", "start_pos": 61, "end_pos": 103, "type": "TASK", "confidence": 0.8651651740074158}, {"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7120838761329651}]}, {"text": "We then describe our evaluation framework including certain important details of how our reference reorderings were created.", "labels": [], "entities": []}, {"text": "We evaluate the framework by analyzing how robustly it is able to predict improvements in subjective translation quality for an English to Japanese machine translation system.", "labels": [], "entities": [{"text": "English to Japanese machine translation", "start_pos": 128, "end_pos": 167, "type": "TASK", "confidence": 0.6566225826740265}]}, {"text": "Finally, we describe ways in which the framework has facilitated development of the reordering components in our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "To create the word-aligned translations from which we generate our reference reordering data, we used a novel alignment-oriented translation method.", "labels": [], "entities": []}, {"text": "The method (described in more detail below) seeks to generate reference reorderings that a machine translation system might reasonably be expected to achieve. has analyzed the extent to which translations seen in a parallel corpus can be broken down into clean phrasal units: they found that most sentence pairs contain examples of reordering that violate phrasal cohesion, i.e. the corresponding words in the target language are not completely contiguous or solely aligned to the corresponding source phrase.", "labels": [], "entities": []}, {"text": "These reordering phenomena are difficult for current statistical translation models to learn directly.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.6108694970607758}]}, {"text": "We therefore deliberately chose to create reference data that avoids these phenomena as much as possible by having a single annotator generate both the translation and its word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.6827536672353745}]}, {"text": "Our word-aligned translations are created with a bias towards simple phrasal reordering.", "labels": [], "entities": []}, {"text": "Our analysis of the correlation between reordering scores computed on reference data created from such alignment-oriented translations with scores computed on references generated from standard professional translations of the same sentences suggests that the alignment-oriented translations are more useful for evaluating a current state-of-the-art system.", "labels": [], "entities": []}, {"text": "We note also that while prior work has conjectured that automatically generated alignments area suitable replacement for manual alignments in the context of reordering evaluation, our results suggest that this is not the case at least for the language pair we consider, English-Japanese.", "labels": [], "entities": []}, {"text": "We now present our lightweight reordering evaluation framework; this consists of (1) a method for generating reference reordering data from manual word-alignments; and (2) a reordering metric for scoring a sytem's proposed reordering against this reference data; and (3) a stand-alone evaluation tool.", "labels": [], "entities": []}, {"text": "While the framework we propose can be applied to any machine translation system in which a reordering of the source sentence can be inferred from the translation process, it has proven particularly useful applied to a system that performs reordering as a separate preprocessing step.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7163025736808777}]}, {"text": "Such pre-ordering approaches () can be criticized for greedily committing to a single reordering early in the pipeline but in practice they have been shown to perform extremely well on language pairs that require long distance reordering and have been successfully combined with other more integrated reordering models ().", "labels": [], "entities": []}, {"text": "The performance of a parser-based pre-ordering component is a function of the reordering rules and parser; it is therefore desirable that these can be evaluated efficiently.", "labels": [], "entities": []}, {"text": "Both parser and reordering rules maybe evaluated using end-to-end automatic metrics such as BLEU score or inhuman evaluations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9849947690963745}]}, {"text": "Parsers may also be evaluated using intrinsic treebank metrics such as labeled accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.6137024760246277}]}, {"text": "Unfortunately these metrics are either expensive to compute or, as we show, unpredictive of improvements inhuman perceptions of translation quality.", "labels": [], "entities": []}, {"text": "Having found that the fuzzy reordering score proposed here is well-correlated with changes inhuman judgements of translation quality, we established a stand-alone evaluation tool that takes a set of reordering rules and a parser and computes the reordering scores on a set of reference reorderings.", "labels": [], "entities": []}, {"text": "This has become the most frequently used method for evaluating changes to the reordering component in our system and has allowed teams working on parsing, for instance, to contribute significant improvements quite independently.", "labels": [], "entities": [{"text": "parsing", "start_pos": 146, "end_pos": 153, "type": "TASK", "confidence": 0.9618120193481445}]}, {"text": "We wish to determine whether our evaluation framework can predict which changes to reordering components will result in statistically significant improvements in subjective translation quality of the end-to-end system.", "labels": [], "entities": []}, {"text": "To that end we created a number of systems that differ only in terms of reordering components (parser and/or reordering rules).", "labels": [], "entities": []}, {"text": "We then analyzed the corpus-and sentence-level correlation of our evaluation metric with judgements of human translation quality.", "labels": [], "entities": []}, {"text": "Previous work has compared either quite separate systems, e.g., or systems that are artificially different from each other ( . There has also been a tendency to measure corpus-level correlation.", "labels": [], "entities": []}, {"text": "We are more interested in comparing systems that differ in a realistic manner from one another as would typically be required in development.", "labels": [], "entities": []}, {"text": "We also believe sentence-level correlation is more important than corpus-level correlation since good sentence-level correlation implies that a metric can be used for detailed analysis of a system and potentially to optimize it.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Human judgements of translation quality for 1. Parsers and 2. Rules.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9639915227890015}]}, {"text": " Table 6: Pearson's correlation (r) and Spearman's rank  correlation (\u03c1) with subjective translation quality at  sentence-level.", "labels": [], "entities": [{"text": "Pearson's correlation (r)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9413905342419943}, {"text": "Spearman's rank  correlation (\u03c1)", "start_pos": 40, "end_pos": 72, "type": "METRIC", "confidence": 0.7679652401379177}]}, {"text": " Table 2: Reference reordering data generated via various methods: (i) alignment-oriented vs. standard translation, (ii)  manual vs. automatic word alignment", "labels": [], "entities": [{"text": "Reference reordering", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8811760544776917}, {"text": "word alignment", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.7139837741851807}]}, {"text": " Table 3: Pairwise significance in subjective evaluation (0 = not significant, * = 90 percent, ** = 95 percent).", "labels": [], "entities": [{"text": "Pairwise significance", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7608499825000763}]}, {"text": " Table 4: Pairwise significance in fuzzy reordering score (0 = not significant, * = 90 percent, ** = 95 percent).", "labels": [], "entities": [{"text": "Pairwise significance", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7149248719215393}]}, {"text": " Table 5: Pairwise significance in BLEU score (0 = not significant, * = 90 percent, ** = 95 percent).", "labels": [], "entities": [{"text": "Pairwise", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9335840344429016}, {"text": "significance", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.4991365671157837}, {"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9739018082618713}]}, {"text": " Table 8: Intrinsic parser metrics on WSJ dev set.", "labels": [], "entities": [{"text": "WSJ dev set", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9357965389887491}]}]}