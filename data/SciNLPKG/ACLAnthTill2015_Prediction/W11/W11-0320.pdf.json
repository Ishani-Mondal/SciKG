{"title": [{"text": "Probabilistic Word Alignment under the L 0 -norm", "labels": [], "entities": [{"text": "Probabilistic Word Alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5921754837036133}]}], "abstractContent": [{"text": "This paper makes two contributions to the area of single-word based word alignment for bilingual sentence pairs.", "labels": [], "entities": [{"text": "single-word based word alignment", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.6341072395443916}]}, {"text": "Firstly, it integrates the-seemingly rather different-works of (Bodrumlu et al., 2009) and the standard prob-abilistic ones into a single framework.", "labels": [], "entities": []}, {"text": "Secondly, we present two algorithms to optimize the arising task.", "labels": [], "entities": []}, {"text": "The first is an iterative scheme similar to Viterbi training, able to handle large tasks.", "labels": [], "entities": []}, {"text": "The second is based on the inexact solution of an integer program.", "labels": [], "entities": []}, {"text": "While it can handle only small corpora, it allows more insight into the quality of the model and the performance of the iterative scheme.", "labels": [], "entities": []}, {"text": "Finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing IBM-3 Viterbi alignments.", "labels": [], "entities": []}], "introductionContent": [{"text": "The training of single word based translation models () is an essential building block for most state-of-the-art translation systems.", "labels": [], "entities": []}, {"text": "Indeed, even more refined translation models () are initialized by the parameters of single word based ones.", "labels": [], "entities": []}, {"text": "The exception is here the joint approach of, but its refinement by again relies on the wellknown IBM models.", "labels": [], "entities": []}, {"text": "Traditionally) single word based models are trained by the EM-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.", "labels": [], "entities": []}, {"text": "Refinements that also allow symmetrized models are based on bipartite graph matching ( or quadratic assignment problems).", "labels": [], "entities": []}, {"text": "Recently, proposed the first method that treats a nondecomposable problem by handling all sentence pairs at once and via integer linear programming.", "labels": [], "entities": []}, {"text": "Their (non-probabilistic) approach finds dictionaries with a minimal number of entries.", "labels": [], "entities": []}, {"text": "However, the approach does not include a position model.", "labels": [], "entities": []}, {"text": "In this work we combine the two strategies into a single framework.", "labels": [], "entities": []}, {"text": "That is, the dictionary sparsity objective of Bodrumlu et al. will become a regularity term in our framework.", "labels": [], "entities": []}, {"text": "It is combined with the maximal alignment probability of every sentence pair, where we consider the models IBM-1, IBM-2 and HMM.", "labels": [], "entities": [{"text": "IBM-1", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.9217125177383423}, {"text": "IBM-2", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.8873369097709656}]}, {"text": "This allows us to write dictionary sparsity as the (non-convex) L 0 norm of the dictionary parameters of the respective models.", "labels": [], "entities": []}, {"text": "For supervised training, regularity terms are quite common, e.g. ().", "labels": [], "entities": []}, {"text": "For the unsupervised problem addressed in this paper they have recently been introduced in the form of posterior constraints (.", "labels": [], "entities": []}, {"text": "In related fields of NLP lately Dirichlet priors have been investigated, e.g..", "labels": [], "entities": []}, {"text": "We present two strategies to handle the objective function addressed in this paper.", "labels": [], "entities": []}, {"text": "One of these schemes relies, like (), on integer linear programming (see e.g.), but due to the large-scale nature of our problem we solve only the LP-relaxation, followed by successive strengthening.", "labels": [], "entities": []}, {"text": "For the latter, we develop our own, exponentially large set of cuts and show that it can be handled as a polynomially sized system, though in practice this is too inefficient.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed strategies on both small scale and (where applicable) large scale tasks.", "labels": [], "entities": []}, {"text": "We compare to standard EM with sums over alignments, where for the IBM-1 and the HMM we use GIZA++.", "labels": [], "entities": [{"text": "IBM-1", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9718400239944458}]}, {"text": "In addition, we evaluate several variants (our implementations) of the HMM, with non-parametric and parametric alignment models.", "labels": [], "entities": []}, {"text": "Note that for the nonparametric variant we estimate distributions for the first aligned position, for the parametric all initial positions are equally likely.", "labels": [], "entities": []}, {"text": "For the IBM-2 we consider the non-parametric variant and hence our implementation.", "labels": [], "entities": [{"text": "IBM-2", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.932658851146698}]}, {"text": "We also evaluate our schemes on the task without regularization.", "labels": [], "entities": []}, {"text": "All experiments in this work were executed on a 3 GHz Core 2 Duo machine with 8 GB of memory, where up to 4 GB were actually used.", "labels": [], "entities": []}, {"text": "The iterative scheme was run for 15 iterations, where it was nearly converged.", "labels": [], "entities": []}, {"text": "This setting was also used for our own EM-implementations.", "labels": [], "entities": []}, {"text": "Solely for GIZA++ we used the standard setting of 5 iterations, and the implemented smoothing process.", "labels": [], "entities": []}, {"text": "For the IBM-2 and HMM we follow the standard strategy to first train an IBM-1 with the same objective function.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: For large corpora, the proposed scheme outper- forms Viterbi training and sometimes even our EM.", "labels": [], "entities": []}, {"text": " Table 2: Alignment accuracy (weighted f-measure) for  different algorithms. We use a dictionary penalty of  \u03bb = 5 and the standard EM (GIZA++ for IBM-1 and  parametric HMM, our implementation otherwise) train- ing scheme with 5 iterations for each model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.7597910761833191}, {"text": "EM", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9187286496162415}]}, {"text": " Table 3: Ratios of the best known integer solution and the  best known lower bounds for all considered tasks.", "labels": [], "entities": []}]}