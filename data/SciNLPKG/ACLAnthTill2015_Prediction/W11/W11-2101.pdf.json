{"title": [{"text": "A Grain of Salt for the WMT Manual Evaluation *", "labels": [], "entities": [{"text": "WMT Manual Evaluation", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.6667745312054952}]}], "abstractContent": [{"text": "The Workshop on Statistical Machine Translation (WMT) has become one of ACL's flagship workshops, held annually since 2006.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.8627631862958273}]}, {"text": "In addition to soliciting papers from the research community, WMT also features a shared translation task for evaluating MT systems.", "labels": [], "entities": [{"text": "WMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.7206035256385803}, {"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.9782552123069763}]}, {"text": "This shared task is notable for having manual evaluation as its cornerstone.", "labels": [], "entities": []}, {"text": "The Workshop's overview paper, playing a descriptive and administrative role, reports the main results of the evaluation without delving deep into analyzing those results.", "labels": [], "entities": []}, {"text": "The aim of this paper is to investigate and explain some interesting idiosyncrasies in the reported results, which only become apparent when performing a more thorough analysis of the collected annotations.", "labels": [], "entities": []}, {"text": "Our analysis sheds some light on how the reported results should (and should not) be interpreted, and also gives rise to some helpful recommendation for the organizers of WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 171, "end_pos": 174, "type": "TASK", "confidence": 0.5158008337020874}]}], "introductionContent": [{"text": "The Workshop on Statistical Machine Translation (WMT) has become an annual feast for MT researchers.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.8579313258330027}, {"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9918349981307983}]}, {"text": "Of particular interest is WMT's shared translation task, featuring a component for manual evaluation of MT systems.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.616081178188324}, {"text": "shared translation task", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.5619006951649984}, {"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9598276019096375}]}, {"text": "The friendly competition is a source of inspiration for participating teams, and the yearly overview paper) provides a concise report of the state of the art.", "labels": [], "entities": []}, {"text": "However, the amount of interesting data collected every year (the system outputs * This work has been supported by the grants EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the Czech Republic), P406/10/P259, MSM 0021620838, and DARPA GALE program under Contract No. HR0011-06-2-0001.", "labels": [], "entities": [{"text": "EuroMatrixPlus", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.9214299321174622}, {"text": "P406/10/P259", "start_pos": 210, "end_pos": 222, "type": "DATASET", "confidence": 0.8591756701469422}, {"text": "DARPA GALE", "start_pos": 244, "end_pos": 254, "type": "DATASET", "confidence": 0.5219684690237045}]}, {"text": "We are grateful to our students, colleagues, and the three reviewers for various observations and suggestions. and, most importantly, the annotator judgments) is quite large, exceeding what the WMT overview paper can afford to analyze with much depth.", "labels": [], "entities": [{"text": "WMT overview paper", "start_pos": 194, "end_pos": 212, "type": "DATASET", "confidence": 0.866906464099884}]}, {"text": "In this paper, we take a closer look at the data collected in last year's workshop, WMT10 1 , and delve a bit deeper into analyzing the manual judgments.", "labels": [], "entities": [{"text": "WMT10 1", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.8410083949565887}]}, {"text": "We focus mainly on the English-to-Czech task, as it included a diverse portfolio of MT systems, was a heavily judged language pair, and also illustrates interesting \"contradictions\" in the results.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9644426107406616}]}, {"text": "We try to explain such points of interest, and analyze what we believe to be the positive and negative aspects of the currently established evaluation procedure of WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.8550927639007568}]}, {"text": "Section 2 examines the primary style of manual evaluation: system ranking.", "labels": [], "entities": []}, {"text": "We discuss how the interpretation of collected judgments, the computation of annotator agreement, and document that annotators' individual preferences may render two systems effectively incomparable.", "labels": [], "entities": []}, {"text": "Section 3 is devoted to the impact of embedding reference translations, while Section 4 and Section 5 discuss some idiosyncrasies of other WMT shared tasks and manual evaluation in general.", "labels": [], "entities": [{"text": "embedding reference translations", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6265532970428467}, {"text": "WMT shared tasks", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.8647426764170328}]}], "datasetContent": [{"text": "We have already seen that the comprehensibility test by blind post-editing provides a different picture of the systems than the official ranking.", "labels": [], "entities": []}, {"text": "(2011) introduced a third \"quiz-based evaluation\".", "labels": [], "entities": []}, {"text": "The quiz-like evaluation used the Englishto-Czech WMT10 systems, applied to different texts: short text snippets were translated and annotators were asked to answer three yes/no questions complementing each snippet.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.5193219780921936}]}, {"text": "The order of the systems was rather different from the official WMT10 results: CU-TECTO won the quiz-based evaluation despite being the fourth in WMT10.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8745920658111572}, {"text": "WMT10", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.9639609456062317}]}, {"text": "Because the texts were different in WMT10 and the quiz-based evaluation, we asked a small group of annotators to apply the ranking technique on the text snippets.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9022679328918457}]}, {"text": "While not exactly comparable to the WMT10 ranking, the WMT10 ranking was confirmed: CU-TECTO was again among the lowestscoring systems and Google won the ranking.", "labels": [], "entities": [{"text": "WMT10 ranking", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8907714188098907}, {"text": "WMT10 ranking", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.8266502618789673}]}, {"text": "Bojar (2011) applies the error-flagging manual evaluation by to four systems of WMT09 English-to-Czech task.", "labels": [], "entities": [{"text": "WMT09 English-to-Czech task", "start_pos": 80, "end_pos": 107, "type": "DATASET", "confidence": 0.8147672414779663}]}, {"text": "Again, the overall order of the systems is somewhat different when ranked by the number of errors flagged.", "labels": [], "entities": []}, {"text": "Mireia use a coarser but linguistically motivated error classification for Catalan-Spanish and suggest that differences in ranking are caused by annotators treating some types of errors as more serious.", "labels": [], "entities": []}, {"text": "In short, different types of manual evaluations lead to different results even when identical systems and texts are evaluated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the collected rankings, quality of references and kappas across language pairs. In  general, a block yields a set of five rank labels, which yields a set of  5", "labels": [], "entities": []}, {"text": " Table 2: Sentence-level ranking scores for the  WMT10 English-Czech language pair. The \"\u2265  others\" and \"> others\" scores reproduced here  exactly match numbers published in the WMT10  overview paper. A boldfaced score marks the best  system in a given row (besides the reference).", "labels": [], "entities": [{"text": "WMT10 English-Czech language pair", "start_pos": 49, "end_pos": 82, "type": "DATASET", "confidence": 0.8972830176353455}, {"text": "WMT10  overview paper", "start_pos": 178, "end_pos": 199, "type": "DATASET", "confidence": 0.8780269026756287}]}, {"text": " Table 3: Summary of two variants of kappa: S  (or K as it is reported in the WMT10 paper) and  our proposed Scott's \u03c0. We report inter-vs. intra- annotator agreement and collected from all com- parisons (\"incl. ref.\") vs. collected only from  comparisons without the reference (\"excl. ref.\")  because it is generally easier to agree that the ref- erence is better than the other systems. This table  is based on all language pairs.", "labels": [], "entities": [{"text": "WMT10 paper", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.9418801069259644}]}, {"text": " Table 5. As it turns out, English-Correlation of Block Count  Source  Target  vs. \"\u2265 Others\"  English Czech  -0.558  English Spanish  -0.434  Czech  English  -0.290  Spanish English  -0.240  English French  -0.227  English German  -0.161  French  English  -0.024  German English  0.146  Overall  -0.092", "labels": [], "entities": [{"text": "Czech  -0.558  English Spanish  -0.434  Czech  English  -0.290  Spanish English  -0.240  English French  -0.227  English German  -0.161  French  English  -0.024  German English  0.146", "start_pos": 103, "end_pos": 286, "type": "DATASET", "confidence": 0.8034343848625819}]}, {"text": " Table 5: Pearson's correlation between the num- ber of blocks where a system was ranked and the  system's \"\u2265 others\" score. (The reference itself is  not included among the considered systems).", "labels": [], "entities": []}, {"text": " Table 6: Pairwise comparisons extracted from  sentence-level rankings of the WMT10 English- Czech News Task. Re-evaluated to reproduce the  numbers published in WMT10 overview paper.  Bold in column A and row B means that system  A is pairwise better than system B.", "labels": [], "entities": [{"text": "WMT10 English- Czech News Task", "start_pos": 78, "end_pos": 108, "type": "DATASET", "confidence": 0.9433236420154572}, {"text": "WMT10 overview paper", "start_pos": 162, "end_pos": 182, "type": "DATASET", "confidence": 0.8967196544011434}]}, {"text": " Table 7: Additional annotation of 63 CU-BOJAR  (B) vs. CU-TECTO (T) sentences by two annota- tors.", "labels": [], "entities": []}, {"text": " Table 8: Blurry picture of pairwise rankings of  CU-BOJAR vs. CU-TECTO. Wins in bold.", "labels": [], "entities": [{"text": "CU-BOJAR", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9399016499519348}, {"text": "CU-TECTO", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.7115606069564819}]}, {"text": " Table 10: Pearson's correlation of the relative per- centage of blocks where the reference was in- cluded in the ranking and the final \"\u2265 others\"  of the system (the reference itself is not included  among the considered systems).", "labels": [], "entities": []}, {"text": " Table 11: The number of post-edits per system for  each language pair to complement", "labels": [], "entities": []}, {"text": " Table 12: BLEU scores of sample five systems in  English-to-Czech combination task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9988969564437866}]}]}