{"title": [{"text": "Sentiment Classification Using Semantic Features Extracted from WordNet-based Resources", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9768318235874176}]}], "abstractContent": [{"text": "In this paper, we concentrate on the 3 of the tracks proposed in the NTCIR 8 MOAT, concerning the classification of sentences according to their opinionatedness, relevance and polarity.", "labels": [], "entities": [{"text": "NTCIR 8 MOAT", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9022435943285624}]}, {"text": "We propose a method for the detection of opinions, relevance, and polarity classification, based on ISR-WN (a resource for the multidimensional analysis with Relevant Semantic Trees of sentences using different WordNet-based information sources).", "labels": [], "entities": [{"text": "detection of opinions, relevance, and polarity classification", "start_pos": 28, "end_pos": 89, "type": "TASK", "confidence": 0.6514627834161123}, {"text": "ISR-WN", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.9032027721405029}]}, {"text": "Based on the results obtained, we can conclude that the resource and methods we propose are appropriate for the task, reaching the level of state-of-the-art approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, textual information has become one of the most important sources of knowledge to extract useful and heterogeneous data.", "labels": [], "entities": []}, {"text": "Texts can provide from factual information such as descriptions, lists of characteristics or instructions to opinionated information such as reviews, emotions or feelings.", "labels": [], "entities": []}, {"text": "This heterogeneity has motivated that dealing with the identification and extraction of opinions and sentiments in texts require special attention.", "labels": [], "entities": [{"text": "identification and extraction of opinions and sentiments in texts", "start_pos": 55, "end_pos": 120, "type": "TASK", "confidence": 0.8731958005163405}]}, {"text": "In fact, the development of different tools to help government information analysts, companies, political parties, economists, etc to automatically get feelings from news and forums is a challenging task ().", "labels": [], "entities": []}, {"text": "Many researchers such as,,, and many others have been working in this way and related areas.", "labels": [], "entities": []}, {"text": "Moreover, in the course of years we find along tradition on developing Question Answering (QA) systems.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.8435900390148163}]}, {"text": "However, in recent years, researchers have concentrated on the development of Opinion Questions Answering (OQA) systems (.", "labels": [], "entities": [{"text": "Opinion Questions Answering (OQA)", "start_pos": 78, "end_pos": 111, "type": "TASK", "confidence": 0.7460929105679194}]}, {"text": "This new task has to deal with different problems such as Sentiment Analysis where documents must be classified according to sentiments and subjectivity features.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9675714373588562}]}, {"text": "Therefore, anew kind of evaluation that takes into account this new issue is needed.", "labels": [], "entities": []}, {"text": "One of the competitions that establishes the benchmark for opinion question answering systems, in a monolingual and cross-lingual setting, is the NTCIR Multilingual Opinion Analysis Task (MOAT) . In this competition, researchers work hard to achieve better results on Opinion Analysis, introducing different techniques.", "labels": [], "entities": [{"text": "opinion question answering", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.706333339214325}, {"text": "NTCIR Multilingual Opinion Analysis Task (MOAT)", "start_pos": 146, "end_pos": 193, "type": "TASK", "confidence": 0.7887896597385406}, {"text": "Opinion Analysis", "start_pos": 268, "end_pos": 284, "type": "TASK", "confidence": 0.7519041895866394}]}, {"text": "In this paper, we only concentrate on three tracks proposed in the NTCIR 8 MOAT, concerning to the classification of sentences according to their opinionatedness, relevance and polarity.", "labels": [], "entities": [{"text": "NTCIR 8 MOAT", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9239864349365234}]}, {"text": "We propose a method for the detection of opinions, relevance and polarity classification, based on ISR-WN which is a resource for the multidimensional analysis with Relevant Semantic Trees of sentences using different WordNet-based information sources.", "labels": [], "entities": [{"text": "detection of opinions, relevance and polarity classification", "start_pos": 28, "end_pos": 88, "type": "TASK", "confidence": 0.6537819989025593}, {"text": "ISR-WN", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.9438223242759705}]}], "datasetContent": [{"text": "In this section we concentrated on measuring the influence of each Dimension (resource) taken separately and jointly in our proposal.", "labels": [], "entities": []}, {"text": "Also, we have compared our results with the best results obtained by the participant systems in the NTCIR 8 MOAT competition.", "labels": [], "entities": [{"text": "NTCIR 8 MOAT competition", "start_pos": 100, "end_pos": 124, "type": "DATASET", "confidence": 0.8183525204658508}]}], "tableCaptions": [{"text": " Table 3. Final Domain Vectors Pos-Neg", "labels": [], "entities": []}, {"text": " Table 4. Results on each task. Precision (P), Recall (R)  and F-Measure (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9459595382213593}, {"text": "Recall (R)", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9574550539255142}, {"text": "F-Measure (F)", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.955765426158905}]}, {"text": " Table 5. Results without normalized vectors. Precision  (P), Recall (R) and F-Measure (F).", "labels": [], "entities": [{"text": "Precision  (P)", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9442028999328613}, {"text": "Recall (R)", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9619712233543396}, {"text": "F-Measure (F)", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9610786586999893}]}]}