{"title": [{"text": "Cats Rule and Dogs Drool!: Classifying Stance in Online Debate", "labels": [], "entities": [{"text": "Classifying Stance", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.935162216424942}]}], "abstractContent": [{"text": "A growing body of work has highlighted the challenges of identifying the stance a speaker holds towards a particular topic, a task that involves identifying a holistic subjective disposition.", "labels": [], "entities": []}, {"text": "We examine stance classification on a corpus of 4873 posts across 14 topics on ConvinceMe.net, ranging from the playful to the ideological.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.9727233350276947}, {"text": "ConvinceMe.net", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9116702675819397}]}, {"text": "We show that ideological debates feature a greater share of rebuttal posts, and that rebuttal posts are significantly harder to classify for stance, for both humans and trained classifiers.", "labels": [], "entities": []}, {"text": "We also demonstrate that the number of subjective expressions varies across debates, a fact correlated with the performance of systems sensitive to sentiment-bearing terms.", "labels": [], "entities": []}, {"text": "We present results for iden-tifing rebuttals with 63% accuracy, and for identifying stance on a per topic basis that range from 54% to 69%, as compared to un-igram baselines that vary between 49% and 60%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9993832111358643}]}, {"text": "Our results suggest that methods that take into account the dialogic context of such posts might be fruitful.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work has highlighted the challenges of identifying the STANCE that a speaker holds towards a particular political, social or technical topic.", "labels": [], "entities": []}, {"text": "Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (;.", "labels": [], "entities": [{"text": "Classifying stance", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.905918300151825}]}, {"text": "Our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion websites vary a great deal.", "labels": [], "entities": []}, {"text": "One important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts, which varies in our data from 34% to 80%.", "labels": [], "entities": []}, {"text": "The ability to explicitly rebut a previous post gives these debates both monologic and dialogic properties; Compare Figure 1 to.", "labels": [], "entities": []}, {"text": "We believe that discussions containing many rebuttal links require a different type of analysis than other types of debates or discussions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Characteristics of Different Topics. Topics below the line are considered \"ideological\". Normalized LIWC  variable z-scores are significant when more than 1.94 standard deviations away from the mean (two-tailed).", "labels": [], "entities": []}, {"text": " Table 2: Human Agreement on Rebuttal Classification", "labels": [], "entities": [{"text": "Rebuttal Classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.9348815977573395}]}, {"text": " Table 4: Accuracies achieved using different feature sets and 10-fold cross validation as compared to the human  topline from MTurk. Best accuracies are shown in bold for each topic in each row. KEY: Human topline results  (Turk). Unigram features (Uni). Linguistics Inquiry Word Count features (LIWC). Generalized dependency features  containing MPQA terms (GdepO) & POS tags (GdepP). NaiveBayes was used, no attribute selection was applied.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.987038254737854}, {"text": "MTurk", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.9508123993873596}, {"text": "KEY", "start_pos": 196, "end_pos": 199, "type": "METRIC", "confidence": 0.8526778221130371}]}]}