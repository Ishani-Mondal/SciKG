{"title": [{"text": "Semantic Networks and Spreading Activation Process for QA improvement on text answers", "labels": [], "entities": []}], "abstractContent": [{"text": "Question Answering (QA) systems try to find precise answers to natural language questions.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8480473160743713}]}, {"text": "QA extraction result is often an amount of text candidate answers which requires some validation and ranking criteria.", "labels": [], "entities": [{"text": "QA extraction", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8561989068984985}]}, {"text": "This paper presents an automatic answer appreciation technique where extracted candidate answers are represented in a question dedicated associative knowledge base, a semantic network.", "labels": [], "entities": [{"text": "answer appreciation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7105935215950012}]}, {"text": "A spreading activation algorithm looks for semantically related candidate answers, that reinforce each other.", "labels": [], "entities": []}, {"text": "The purpose is to enhance the best answers by rising their weight.", "labels": [], "entities": []}, {"text": "This article concludes with evaluation details for an experiment with text answers to Portuguese questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question Answering (QA) systems receive natural language written questions and look for exact answers.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8549896240234375}]}, {"text": "If we ask \"Que anima\u00ed e o Cocas?\"", "labels": [], "entities": []}, {"text": "(in English: What kind of animal is Cocas?), a QA system could answer: peluche (plush doll), r\u00e3 (true frog), sapo (toad), verde (has the green color), or even porco (pig) if found as the specie of some pet named Cocas ( the Portuguese name for Kermit The Frog, from The Muppet Show).", "labels": [], "entities": []}, {"text": "Most open domain QA systems look for answers in Web documents or plain text files, possibly in static and local collections [ [.", "labels": [], "entities": []}, {"text": "Candidate answers are expressions that fit into the expected type of answer for the question category that are related to the question focus, in some document.", "labels": [], "entities": []}, {"text": "To handle multiple acceptable answers within a wide list of bogus results is a challenge.", "labels": [], "entities": []}, {"text": "Even morphologically different answers might have some semantic connection that can be useful for an informed choice.", "labels": [], "entities": []}, {"text": "We propose a method to increase QA systems accuracy by executing an answer appreciation phase over the extraction result that goes beyond the usual QA answer validation.", "labels": [], "entities": [{"text": "QA systems", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.859527975320816}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9102724194526672}, {"text": "QA answer validation", "start_pos": 148, "end_pos": 168, "type": "TASK", "confidence": 0.7357595364252726}]}], "datasetContent": [{"text": "To evaluate the effect of this method we setup an experiment with a collection of Definition and What questions, written in Portuguese language.", "labels": [], "entities": [{"text": "What", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.8954263925552368}]}, {"text": "After the extraction phase all candidate answers were stored in a database, along with their weight, and then assessed by a human as corrector incorrect.", "labels": [], "entities": []}, {"text": "Then we collected some statistic elements for each question: hit on system (first) answer (yes/no); first correct answer on rank (FC); first wrong answer on rank (FW); hit rate in top 5 and in top 10.", "labels": [], "entities": [{"text": "first wrong answer on rank (FW)", "start_pos": 135, "end_pos": 166, "type": "METRIC", "confidence": 0.5215671546757221}, {"text": "hit rate", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9531177282333374}]}, {"text": "Definition questions had a total of 742 candidate answers, from which 112 (or 15%) were short expressions considered factoids.", "labels": [], "entities": []}, {"text": "For category What, we had a collection of 316 results, from which 174 (or 55%) were factoid answers.", "labels": [], "entities": []}, {"text": "Afterward, we applied the semantic network based answer appreciation method for answer list reranking.", "labels": [], "entities": [{"text": "answer appreciation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7430121004581451}, {"text": "answer list reranking", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.6905616919199625}]}, {"text": "Each answer, eventually having an updated weight, was again stored in a database and the statistic report tool was run.", "labels": [], "entities": []}, {"text": "1 shows the average results for each question category, for both spectrum similarity (SS) and combined spectrum (CS) techniques.", "labels": [], "entities": [{"text": "spectrum similarity (SS)", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.7387201607227325}]}, {"text": "Looking at SS technique result, for Definition type, we see no improvement in system best answer (the first in the rank), keeping the success rate from the extraction phase, 72.73%.", "labels": [], "entities": []}, {"text": "The FC indicator is slightly worse, because the average first correct answer position has fallen to 1.64.", "labels": [], "entities": [{"text": "FC", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.998602569103241}]}, {"text": "Indicators Top5 and Top10 show a progress in the concentration of correct answers at the top.", "labels": [], "entities": []}, {"text": "In the second line, SS-15 is the same technique but there is an upper bound to the bonus weight increment that an answer can receive.", "labels": [], "entities": [{"text": "SS-15", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.6633250713348389}]}, {"text": "After some tests, the upper bound that allowed better results was 15.", "labels": [], "entities": []}, {"text": "With SS-15 the indicators for system correct answer, FC, FW and Top5 are better than without the bonus limit.", "labels": [], "entities": [{"text": "FC", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9963271021842957}, {"text": "FW", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9938940405845642}]}, {"text": "The result for technique CS shows a bigger gain.", "labels": [], "entities": []}, {"text": "Again, the bonus limited variant of the technique works better.", "labels": [], "entities": []}, {"text": "The fifth line shows a combination of SS-15 and CS-15 techniques.", "labels": [], "entities": []}, {"text": "It seems to be no benefit over a single technique.", "labels": [], "entities": []}, {"text": "Answers for What category start with a lower hit rate in Top5 and Top10, at extraction phase.", "labels": [], "entities": []}, {"text": "Techniques SS and SS-15 improve all indicators.", "labels": [], "entities": [{"text": "SS", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9417104721069336}]}, {"text": "The CS technique has a distinct behavior in each variant.", "labels": [], "entities": []}, {"text": "Pure CS technique deteriorates the system success rate from 60% to 26.67%, although the number of correct answers in the first five remains.", "labels": [], "entities": []}, {"text": "With CS-15, the system answer success rate keeps equal to extraction phase, at 60%.", "labels": [], "entities": []}, {"text": "The remaining indicators denote a small improvement in results.", "labels": [], "entities": []}], "tableCaptions": []}