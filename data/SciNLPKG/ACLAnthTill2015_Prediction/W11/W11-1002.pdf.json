{"title": [{"text": "Structured vs. Flat Semantic Role Representations for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Structured vs. Flat Semantic Role Representations", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.5754628827174505}, {"text": "Machine Translation Evaluation", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.855931282043457}]}], "abstractContent": [{"text": "We argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the MEANT family of semantic MT evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.8770595788955688}]}, {"text": "Our analysis finds that both properly structured and flattened representations fail to adequately account for the contribution of each semantic frame to the overall sentence.", "labels": [], "entities": []}, {"text": "We then show that the correlation of HMEANT, the human variant of MEANT, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence.", "labels": [], "entities": [{"text": "correlation", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9575331211090088}, {"text": "HMEANT", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.5159582495689392}, {"text": "MEANT", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.8718212842941284}]}, {"text": "The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame's contribution gives HMEANT higher correlations than the previously best-performing flattened model, as well as HTER.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.8539656400680542}, {"text": "HTER", "start_pos": 232, "end_pos": 236, "type": "DATASET", "confidence": 0.8125792145729065}]}], "introductionContent": [{"text": "In this paper we provide a more concrete answer to the question: what would be a better representation, structured or flat, of the roles in semantic frames to be used in a semantic machine translation (MT) evaluation metric?", "labels": [], "entities": [{"text": "semantic machine translation (MT) evaluation metric", "start_pos": 172, "end_pos": 223, "type": "TASK", "confidence": 0.8452526852488518}]}, {"text": "The weighting scheme we propose uses a simple length-based heuristic that reflects the assumption that a semantic frame that covers more tokens contributes more to the overall sentence translation.", "labels": [], "entities": [{"text": "sentence translation", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.7372942566871643}]}, {"text": "We demonstrate empirically that when the degree of each frame's contribution to its sentence is taken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for SRL MT evaluation metrics.", "labels": [], "entities": [{"text": "SRL MT evaluation", "start_pos": 234, "end_pos": 251, "type": "TASK", "confidence": 0.8477224707603455}]}, {"text": "For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9914507269859314}, {"text": "machine translation evaluation", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.7650955319404602}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9982975125312805}, {"text": "NIST", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.790482759475708}, {"text": "METEOR", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9613513946533203}, {"text": "PER", "start_pos": 192, "end_pos": 195, "type": "METRIC", "confidence": 0.9308565258979797}, {"text": "WER", "start_pos": 212, "end_pos": 215, "type": "METRIC", "confidence": 0.9859175086021423}]}, {"text": "These metrics are excellent at ranking overall systems by averaging their scores over entire documents.", "labels": [], "entities": []}, {"text": "However, as MT systems improve, the shortcomings of such metrics are becoming more apparent.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9883959293365479}]}, {"text": "Though containing roughly the correct words, MT output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9828788042068481}]}, {"text": "This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularly poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions-which human judges have no trouble distinguishing. and, for example, study situations where BLEU strongly disagrees with human judgment of translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 368, "end_pos": 372, "type": "METRIC", "confidence": 0.9558852314949036}]}, {"text": "Newer avenues of research seek substitutes for n-gram based MT evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.893840491771698}]}, {"text": "One line of research emphasizes more the structural correctness of translation.", "labels": [], "entities": []}, {"text": "propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality.", "labels": [], "entities": []}, {"text": "However, the problem remains that a grammatical translation can achieve a high syntaxbased score yet still make significant errors arising from confusion of semantic roles.", "labels": [], "entities": []}, {"text": "On the other hand, despite the fact that non-automatic, manually evaluated metrics, such as HTER), are more adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use.", "labels": [], "entities": []}, {"text": "There has also been work on explicitly evaluating MT adequacy by aggregating over a very large set of linguistic features ( and textual entailment ().", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.991260826587677}]}], "datasetContent": [{"text": "A blueprint for more direct assessment of meaning preservation across translation was outlined by, in which translation utility is manually evaluated with respect to the accuracy of semantic role labels.", "labels": [], "entities": [{"text": "meaning preservation across translation", "start_pos": 42, "end_pos": 81, "type": "TASK", "confidence": 0.7501696795225143}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9932059049606323}]}, {"text": "A good translation is one from which human readers may successfully understand at least the basic event structure-\"who did what to whom, when, where and why\")-which represents the most essential meaning of the source utterances.", "labels": [], "entities": []}, {"text": "Adopting this principle, the MEANT family of metrics compare the semantic frames in reference translations against those that can be reconstructed from machine translation output.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.7781451940536499}]}, {"text": "Preliminary results reported in ( confirm that the blueprint model outperforms BLEU and similar n-gram oriented evaluation metrics in correlation against human adequacy judgments, but does not fare as well as HTER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9975506663322449}, {"text": "HTER", "start_pos": 209, "end_pos": 213, "type": "DATASET", "confidence": 0.677759051322937}]}, {"text": "The more complete study of introduces MEANT and its human variants HMEANT, which implement an extended version of blueprint methodology.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.8669679164886475}]}, {"text": "Experimental results show that HMEANT correlates against human adequacy judgments as well as the more expensive HTER, even though HMEANT can be evaluated using lowcost untrained monolingual semantic role annotators while still maintaining high inter-annotator agreement (both are far superior to BLEU or other surface oriented evaluation metrics).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 296, "end_pos": 300, "type": "METRIC", "confidence": 0.9889538288116455}]}, {"text": "The study also shows that replacing the human semantic role labelers with an automatic shallow semantic parser yields an approximation that is still vastly superior to BLEU while remaining about 80% as closely correlated with human adequacy judgments as HTER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9974551796913147}]}, {"text": "Along with additional improvements to the accuracy of the MEANT family of metrics, Lo and Wu (2011b) study the impact of each individual semantic role to the metric's correlation against human adequacy judgments, as well as the time cost for humans to reconstruct the semantic frames and compare the translation accuracy of the role fillers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9989458918571472}, {"text": "accuracy", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.8084012866020203}]}, {"text": "In general, the MEANT family of SRL MT evaluation metrics (Lo and Wu, 2011a,b) evaluate the translation utility as follows.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.896196186542511}, {"text": "SRL MT evaluation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8269841472307841}]}, {"text": "First, semantic role labeling is performed (either manually or automatically) on both the reference translation (REF) and the machine translation output (MT) to obtain the semantic frame structure.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.6968780159950256}]}, {"text": "Then, the semantic predicates, roles and fillers reconstructed from the MT output are compared to those in the reference translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.8687814474105835}]}, {"text": "The number of correctly and partially correctly annotated arguments of each type in each frame of the MT output are collected in this step: In the following three subsections, we describe how the translation utility is calculated using these counts in (a) the original blueprint model, (b) the first version of HMEANT and MEANT using structured role representations, and (c) the more accu- rate flattened-role implementation of HMEANT and MEANT.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9656004905700684}]}, {"text": "The evaluation data for our experiments consists of 40 sentences randomly drawn from the DARPA GALE program Phase 2.5 newswire evaluation corpus containing Chinese input sentence, English reference translations, and the machine translation from three different state-of-the-art GALE systems.", "labels": [], "entities": [{"text": "DARPA GALE program Phase 2.5 newswire evaluation corpus", "start_pos": 89, "end_pos": 144, "type": "DATASET", "confidence": 0.9021032154560089}]}, {"text": "The Chinese and the English reference translation have both been annotated with gold standard PropBank) semantic role labels.", "labels": [], "entities": []}, {"text": "The weights w pred , w core , w adj , w j and w partial can be estimated by optimizing correlation against human adequacy judgments, using any of the many standard optimization search techniques.", "labels": [], "entities": []}, {"text": "In the work of Lo and Wu (2011b), the correlations of all individual roles with the human adequacy judgments were found to be non-negative, therefore we found grid search to be quite adequate for estimating the weights.", "labels": [], "entities": []}, {"text": "We use linear weighting because we would like to keep the metric's interpretation simple and intuitive.", "labels": [], "entities": []}, {"text": "Following the benchmark assessment in NIST MetricsMaTr 2010, we assess the performance of the semantic MT evaluation metric at the sentence level using the summeddiagonal-of-confusion-matrix score.", "labels": [], "entities": [{"text": "NIST MetricsMaTr 2010", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.9096037149429321}, {"text": "MT evaluation", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.7938352823257446}]}, {"text": "The human adequacy judgments were obtained by showing all three MT outputs together with the Chinese source input to a human reader.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.931135356426239}]}, {"text": "The human reader was instructed to order the sentences from the three MT systems according to the accuracy of meaning in the translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.8924402594566345}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9982701539993286}]}, {"text": "For the MT output, we ranked the sentences from the three MT systems according to their evaluation metric scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9767858982086182}]}, {"text": "By comparing the two sets of rankings, a confusion matrix is formed.", "labels": [], "entities": []}, {"text": "The summed diagonal of confusion matrix is the percentage of the total count when a particular rank by the metric's score exactly matches the human judgments.", "labels": [], "entities": []}, {"text": "The range of possible values of summed diagonal of confusion matrix is, where 1 means all the systems' ranks determined by the metric are identical with that of the human judgments and 0 means all the systems' ranks determined by the metric are different from that of the human judgment.", "labels": [], "entities": []}, {"text": "Since the summed diagonal of confusion matrix scores only assess the absolute ranking accuracy, we also report the Kendall's \u03c4 rank correlation coefficients, which measure the correlation of the proposed metric against human judgments with respect to their relative ranking of translation adequacy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.8549274802207947}, {"text": "Kendall's \u03c4 rank correlation", "start_pos": 115, "end_pos": 143, "type": "METRIC", "confidence": 0.6835171282291412}]}, {"text": "A higher the value for \u03c4 indicates the more similar the ranking by the evaluation metric to the human judgment.", "labels": [], "entities": []}, {"text": "The range of possible values of correlation", "labels": [], "entities": []}], "tableCaptions": []}