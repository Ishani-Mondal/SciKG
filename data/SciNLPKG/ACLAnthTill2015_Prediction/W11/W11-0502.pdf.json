{"title": [{"text": "Towards Multi-Document Summarization of Scientific Articles:Making Interesting Comparisons with SciSumm", "labels": [], "entities": [{"text": "Multi-Document Summarization of Scientific Articles", "start_pos": 8, "end_pos": 59, "type": "TASK", "confidence": 0.8368284583091736}]}], "abstractContent": [{"text": "We present a novel unsupervised approach to the problem of multi-document summariza-tion of scientific articles, in which the document collection is a list of papers cited together within the same source article, otherwise known as a co-citation.", "labels": [], "entities": [{"text": "multi-document summariza-tion of scientific articles", "start_pos": 59, "end_pos": 111, "type": "TASK", "confidence": 0.7104486346244812}]}, {"text": "At the heart of the approach is a topic based clustering of fragments extracted from each co-cited article and relevance ranking using a query generated from the context surrounding the co-cited list of papers.", "labels": [], "entities": []}, {"text": "This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found.", "labels": [], "entities": []}, {"text": "We present a system called SciSumm that embodies this approach and apply it to the 2008 ACL Anthology.", "labels": [], "entities": [{"text": "2008 ACL Anthology", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.8837878704071045}]}, {"text": "We evaluate this summa-rization system for relevant content selection using gold standard summaries prepared on principle based guidelines.", "labels": [], "entities": [{"text": "relevant content selection", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6170125206311544}]}, {"text": "Evaluation with gold standard summaries demonstrates that our system performs better in content selection than an existing summarization system (MEAD).", "labels": [], "entities": [{"text": "content selection", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.6795495748519897}]}, {"text": "We present a detailed summary of our findings and discuss possible directions for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we present a novel, unsupervised approach to multi-document summarization of scientific articles.", "labels": [], "entities": [{"text": "multi-document summarization of scientific articles", "start_pos": 59, "end_pos": 110, "type": "TASK", "confidence": 0.7981836318969726}]}, {"text": "While the field of multi-document summarization has achieved impressive results with collections of news articles, summarization of collections of scientific articles is a strikingly different problem.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.5785096883773804}, {"text": "summarization of collections of scientific articles", "start_pos": 115, "end_pos": 166, "type": "TASK", "confidence": 0.8914916614691416}]}, {"text": "Multi-document summarization of news articles amounts to synthesizing details about the same story as it has unfolded over a variety of reports, some of which contain redundant information.", "labels": [], "entities": [{"text": "Multi-document summarization of news articles", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8237980127334594}]}, {"text": "In contrast, each scientific article tells its own research story.", "labels": [], "entities": []}, {"text": "Even with papers that address similar research questions, the argument being made is different.", "labels": [], "entities": []}, {"text": "Instead of collecting and arranging details into a single, synthesized story, the task is to abstract away from the specific details of individual papers and to find the common threads that unite them and make sense of the document collection as a whole.", "labels": [], "entities": []}, {"text": "Another challenge with summarization of scientific literature becomes clear as one compares alternative reviews of the same literature.", "labels": [], "entities": [{"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9886401891708374}]}, {"text": "Each review author brings their own unique perspective and questions to bear in their reading and presentation of that literature.", "labels": [], "entities": []}, {"text": "While this is true of other genres of documents that have been the target of multi-document summarization work in the past, we don't find query oriented approaches to multi-document summarization of scientific articles.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.5290803462266922}, {"text": "multi-document summarization of scientific articles", "start_pos": 167, "end_pos": 218, "type": "TASK", "confidence": 0.7834163308143616}]}, {"text": "One contribution of this work is a technical approach to query oriented multidocument summarization of scientific articles that has been evaluated in comparison with a competitive baseline that is not query oriented.", "labels": [], "entities": [{"text": "query oriented multidocument summarization of scientific articles", "start_pos": 57, "end_pos": 122, "type": "TASK", "confidence": 0.6719582889761243}]}, {"text": "The evaluation demonstrates the advantage of the query oriented approach for this type of summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9733585119247437}]}, {"text": "We present a system called SciSumm that summarizes document collections that are composed of lists of papers cited together within the same source article, otherwise known as a co-citation.", "labels": [], "entities": []}, {"text": "Using the context of the co-citation in the source article, we generate a query that allows us to generate a summary in a query-oriented fashion.", "labels": [], "entities": []}, {"text": "The extracted por-tions of the co-cited articles are then assembled into clusters that represent the main themes of the articles that relate to the context in which they were cited.", "labels": [], "entities": []}, {"text": "Our evaluation demonstrates that SciSumm achieves higher quality summaries than the MEAD summarization system).", "labels": [], "entities": [{"text": "MEAD summarization", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.6405177414417267}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We present an overview of relevant literature in Section 2.", "labels": [], "entities": []}, {"text": "The end-to-end summarization pipeline has been described in Section 3 . Section 4 presents an evaluation of summaries generated from the system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9418677091598511}]}, {"text": "We end the paper with conclusions and some interesting further research directions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Ina typical evaluation of a multi-document summarization system, gold standard summaries are evaluated against fixed length generated summaries.", "labels": [], "entities": []}, {"text": "Summarization conferences such as DUC have competitions where different summarization systems compete on a standard task of generating summaries fora publicly available dataset.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9548651576042175}, {"text": "DUC", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8785964846611023}]}, {"text": "The summaries generated using each individual summarization system are then evaluated against the summaries prepared by human annotators.", "labels": [], "entities": []}, {"text": "Summarization of scientific article is a novel task and hence no test collection of gold standard summaries exist.", "labels": [], "entities": [{"text": "Summarization of scientific article", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8733481466770172}]}, {"text": "Thus, it was necessary to prepare our own evaluation corpus, consisting of gold standard multi-document summaries fora set of randomly selected co-citations.", "labels": [], "entities": []}, {"text": "An important target user population for multidocument summarization of scientific articles is graduate students.", "labels": [], "entities": [{"text": "multidocument summarization of scientific articles", "start_pos": 40, "end_pos": 90, "type": "TASK", "confidence": 0.7296265840530396}]}, {"text": "Hence to get a measure of how well the summarization system is performing, we asked 2 graduate students who have been working in the computational linguistics community to create gold standard summaries of a fixed length (8 sentences \u223c 200 words) for ten different randomly selected co-citations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9849231243133545}]}, {"text": "The students were given guidelines to prepare summaries based on the design goals of the SciSumm system, but not any of its technical details.", "labels": [], "entities": []}, {"text": "Thus, for 10 co-citations, we obtained two different gold standard summaries.", "labels": [], "entities": []}, {"text": "For ROUGE-1 the average score between each pair of gold standard summaries was 0.518 (Min = 0.388, Max = 0.686).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8885910511016846}, {"text": "Min", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9855613112449646}, {"text": "Max", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9500768780708313}]}, {"text": "Similarly for ROUGE-2 the average score was 0.242 (Min = 0.119, Max=0.443).", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.5921748280525208}, {"text": "average score", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.9155208766460419}, {"text": "Min", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9828117489814758}, {"text": "Max", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9563354253768921}]}, {"text": "While these scores do not have a well-calibrated meaning to them, they give an indication of the complexity of the task.", "labels": [], "entities": []}, {"text": "Since the annotators were creating extractive summaries which could justify the co-citation, they had to pay special attention to the section where the cocitation came from.", "labels": [], "entities": []}, {"text": "One can consider this similar to the sense making process a reader might go through when using the citing paper as a lens through which to interpret the cited literature.", "labels": [], "entities": [{"text": "sense making", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.7330737411975861}]}, {"text": "Note that while SciSumm provides users with an interactive interface that supports navigation between documents, the gold standard summaries are static.", "labels": [], "entities": []}, {"text": "Thus, our evaluation is designed to measure the quality of the content selection when taking into consideration the citation context.", "labels": [], "entities": []}, {"text": "This would also help us to evaluate the influence exerted by the citation context in the gold standard summaries.", "labels": [], "entities": [{"text": "gold standard summaries", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.8878176808357239}]}, {"text": "In future work, we will evaluate the usability of the SciSumm system using a task based evaluation.", "labels": [], "entities": []}, {"text": "In the absence of any other multi-document summarization systems in the domain of scientific article summarization, we used a widely used and freely available multi-document summarization system called MEAD) as our baseline.", "labels": [], "entities": [{"text": "scientific article summarization", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.6629095176855723}, {"text": "MEAD", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.5550167560577393}]}, {"text": "MEAD uses centroid based summarization to create informative clusters of topics.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8132336139678955}]}, {"text": "We use the default configuration of MEAD in which MEAD uses length, position and centroid for ranking each sentence.", "labels": [], "entities": []}, {"text": "We did not use query focussed summarization with MEAD.", "labels": [], "entities": [{"text": "query focussed summarization", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6945657233397166}, {"text": "MEAD", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8519672751426697}]}, {"text": "We evaluate its performance with the same gold standard summaries we use to evaluate SciSumm.", "labels": [], "entities": []}, {"text": "For generating a summary from our system we used sentences from the tiles which gets clustered in the top ranked cluster.", "labels": [], "entities": []}, {"text": "When that entire cluster is exhausted we move onto the next highly ranked cluster.", "labels": [], "entities": []}, {"text": "In this way we prepare a summary comprising of 8 sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average ROUGE results. * represents improve- ment significant at p < .05,  \u2020 at p < .01.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.911565363407135}, {"text": "improve- ment significant", "start_pos": 46, "end_pos": 71, "type": "METRIC", "confidence": 0.9510290175676346}]}]}