{"title": [{"text": "Top-down recognizers for MCFGs and MGs", "labels": [], "entities": [{"text": "MCFGs", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.8830391764640808}, {"text": "MGs", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.4756975769996643}]}], "abstractContent": [{"text": "This paper defines a normal form for MCFGs that includes strongly equivalent representations of many MG variants, and presents an incremental priority-queue-based TD rec-ognizer for these MCFGs.", "labels": [], "entities": []}, {"text": "After introducing MGs with overt phrasal movement, head movement and simple adjunction are added without change in the recognizer.", "labels": [], "entities": []}, {"text": "The MG representation can be used directly, so that even rather sophisticated analyses of properly non-CF languages can be defined very succinctly.", "labels": [], "entities": []}, {"text": "As with the similar stack-based CF-methods, finite memory suffices for the recognition of infinite languages, and a fully connected left context for probabilistic analysis is available at every point.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the years after proposed that human languages are weakly and strongly \"mildly context sensitive\" (MCS), it was discovered that many independently proposed grammar formalisms define exactly the same MCS languages.", "labels": [], "entities": []}, {"text": "The languages defined by Joshi's tree adjoining grammars (TAGs) are exactly the same as those defined by aversion of Steedman's combinatory categorial grammars, and the same as those defined by head wrapping grammars.", "labels": [], "entities": []}, {"text": "A slightly larger class of languages is defined by another variant of TAGs (set-local multicomponent), by aversion of Pollard's generalized phrase structure grammars called multiple context free grammars (MCFGs), and by a wide range of minimalist grammar (MG) formalizations of Chomskian syntax ().", "labels": [], "entities": []}, {"text": "These remarkable convergences provide evidence from across grammatical traditions that something like these MCS proposals maybe approximately right, and so it is natural to consider psychological models that fit with these proposals.", "labels": [], "entities": []}, {"text": "With a range of performance models fora range of MCS grammars, it becomes possible to explore how grammatical dependencies interact with other factors in the conditioning of human linguistic performance.", "labels": [], "entities": []}, {"text": "For context free grammars (CFGs), perhaps the simplest parsing model is top-down: beginning with the prediction of a sentence, rules are applied to the leftmost predicted category until a terminal element is reached, which is then checked against the input.", "labels": [], "entities": [{"text": "context free grammars (CFGs)", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7503949205080668}]}, {"text": "This parsing method is of interest in psychological modeling not only because it uses the grammar in a very transparent way, but also it is because it is predictive in away that maybe similar to human parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9662961363792419}]}, {"text": "At every point in analyzing a sentence from left to right, the structure that has been constructed is fully connected: grammatical relationships among the elements that have been heard have been guessed, and there are no pieces of structure which have not been integrated.", "labels": [], "entities": []}, {"text": "Consequently, this structure can be interpreted by a standard compositional semantics and maybe appropriate for \"incremental\" models of sentence interpretation (cf..", "labels": [], "entities": [{"text": "sentence interpretation", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.7384212166070938}]}, {"text": "And like human parsing, when used with backtracking or abeam search, TD memory demands need not continually increase with sentence length: a fixed bound on stack depth and on backtrack or beam depth suffices for infinitely many sentences.", "labels": [], "entities": []}, {"text": "Furthermore, TD parsing provides explicit, relevant \"left contexts\" for probabilistic conditioning.", "labels": [], "entities": [{"text": "TD parsing", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.8956381678581238}]}, {"text": "But it has not been clear until recently how to apply this method to Chomskian syntax or any of the other MCS grammar formalisms.", "labels": [], "entities": [{"text": "MCS grammar formalisms", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7187666893005371}]}, {"text": "There have been some proposals along these lines, but they have either been unnecessarily complex or applicable to only a restricted to range of grammatical proposals.", "labels": [], "entities": []}, {"text": "This paper extends TD parsing to minimalist context free grammars (MCFGs) in a certain normal form and presents minimalist grammars (MGs) as a succinct representation for some of those MCFGs.", "labels": [], "entities": [{"text": "TD parsing", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.9568026661872864}]}, {"text": "With this extension, the TD parsing method handles an infinite range of MCFGs that encompasses, strongly and weakly, an infinite range of (many variants of) MGs in a very transparent and direct way.", "labels": [], "entities": [{"text": "TD parsing", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.691781610250473}]}, {"text": "The parsing method can be defined incomplete detail very easily, and, abstracting away from limitations of time and memory, it is provably sound and complete for all those grammars.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9633584022521973}]}, {"text": "The TD recognizer for MCFGs is presented in \u00a74, generalizing and adapting ideas from earlier work).", "labels": [], "entities": []}, {"text": "Instead of using a stack memory, this recognizer uses a \"priority queue,\" which just means that we can access all the elements in memory, sorting them into left-to-right order.", "labels": [], "entities": []}, {"text": "Since a very wide range of grammatical proposals can be expressed in this formalism and parsed transparently by this method, it is straightforward to compute fully explicit and syntactically sophisticated parses of the sorts of sentences used in psycholinguistic studies.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}