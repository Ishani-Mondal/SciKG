{"title": [{"text": "Learning word-level dialectal variation as phonological replacement rules using a limited parallel corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores two different methods of learning dialectal morphology from a small parallel corpus of standard and dialect-form text, given that a computational description of the standard morphology is available.", "labels": [], "entities": []}, {"text": "The goal is to produce a model that translates individual lexical dialectal items to their standard dialect counterparts in order to facilitate dialectal use of available NLP tools that only assume standard-form input.", "labels": [], "entities": []}, {"text": "The results show that a learning method based on induc-tive logic programming quickly converges to the correct model with respect to many phono-logical and morphological differences that are regular in nature.", "labels": [], "entities": []}], "introductionContent": [{"text": "In our work with the Basque language, a morphological description and analyzer is available for the standard language, along with other tools for processing the language ().", "labels": [], "entities": []}, {"text": "However, it would be convenient to be able to analyze variants and dialectal forms as well.", "labels": [], "entities": []}, {"text": "As the dialectal differences within the Basque language are largely lexical and morphophonological, analyzing the dialectal forms would in effect require a separate morphological analyzer that is able to handle the unique lexical items in the dialect together with the differing affixes and phonological changes.", "labels": [], "entities": []}, {"text": "Morphological analyzers are traditionally handwritten by linguists, most commonly using some variant of the popular finite-state morphology approach ().", "labels": [], "entities": [{"text": "Morphological analyzers", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8872224986553192}]}, {"text": "This entails having an expert model a lexicon, inflectional and derivational paradigms as well as phonological alternations, and then producing a morphological analyzer/generator in the form of a finite-state transducer.", "labels": [], "entities": []}, {"text": "As the development of such wide-coverage morphological analyzers is labor-intesive, the hope is that an analyzer fora variant could be automatically learned from a limited parallel standard/dialect corpus, given that an analyzer already exists for the standard language.", "labels": [], "entities": []}, {"text": "This is an interesting problem because a good solution to it could be applied to many other tasks as well: to enhancing access to digital libraries (containing diachronic and dialectal variants), for example, or to improving treatment of informal registers such as SMS messages and blogs, etc.", "labels": [], "entities": [{"text": "treatment of informal registers such as SMS messages and blogs", "start_pos": 225, "end_pos": 287, "type": "TASK", "confidence": 0.6572441756725311}]}, {"text": "In this paper we evaluate two methods of learning a model from a standard/variant parallel corpus that translates a given word of the dialect to its standardform equivalent.", "labels": [], "entities": []}, {"text": "Both methods are based on finitestate phonology.", "labels": [], "entities": []}, {"text": "The variant we use for experiments is Lapurdian, 1 a dialect of Basque spoken in the Lapurdi (fr. Labourd) region in the Basque Country.", "labels": [], "entities": []}, {"text": "Because Basque is an agglutinative, highly inflected language, we believe some of the results can be extrapolated to many other languages facing similar challenges.", "labels": [], "entities": []}, {"text": "One of the motivations for the current work is that there area large number of NLP tools available and in development for standard Basque (also called Batua): a morphological analyzer, a POS tagger, a dependency analyzer, an MT engine, among Sometimes also called Navarro-Labourdin or Labourdin.", "labels": [], "entities": [{"text": "MT", "start_pos": 225, "end_pos": 227, "type": "TASK", "confidence": 0.9718335866928101}]}], "datasetContent": [{"text": "We have measured the quality of different approaches by the usual parameters of precision, recall and the harmonic combination of them, the F 1 -score, and analyzed how the different options in the two approaches affect the results of these three parameters.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9995092153549194}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9992929697036743}, {"text": "F 1 -score", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9845294207334518}]}, {"text": "Given that we, especially in method 1, extract quite a large number of rules and that each 44 input word generates a very large number of candidates if we use all the rules extracted, it is possible to produce a high recall on the conversion of unknown dialect words to the standard form.", "labels": [], "entities": [{"text": "recall", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.9989396929740906}]}, {"text": "However, the downside is that this naturally leads to low precision as well, which we try to control by introducing a number of filters to remove some of the candidates output by the rules.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9989245533943176}]}, {"text": "As mentioned above, we use two filters: (1) an obligatory filter which removes all candidate words that are not found in the standard Basque (by using an existing standard Basque morphological analyzer), and (2) using an optional filter which, given several candidates in the standard Basque, picks the most frequently occurring one by a unigram count from the separate newspaper corpus.", "labels": [], "entities": []}, {"text": "This latter filter turns out to serve a much more prominent role in improving the results of method 1, while it is almost completely negligible for method 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Characteristics of the parallel corpus used for  experiments.", "labels": [], "entities": []}, {"text": " Table 2: Values obtained for Precision, Recall and F- scores with method 1 by changing the minimum fre- quency of the correspondences to construct rules for  foma. The rest of the options are the same in all three  experiments: only one rule is applied within a word.", "labels": [], "entities": [{"text": "Precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9956841468811035}, {"text": "Recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9790721535682678}, {"text": "F- scores", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9700397054354349}]}, {"text": " Table 3: Values obtained for Precision, Recall and F- score with method 1 by changing the threshold frequency  of the correspondences and applying a post-filter.", "labels": [], "entities": [{"text": "Precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9969668984413147}, {"text": "Recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9914705157279968}, {"text": "F- score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9931941032409668}]}, {"text": " Table 4: Method 1. Exp1: frequency 2; 2 rules applied;  in parallel; without contextual conditioning. Exp2: fre- quency 1; 1 rule applied; with contextual conditioning.  Exp3: frequency 2; 2 rules applied; in parallel; with con- textual conditioning.", "labels": [], "entities": [{"text": "fre- quency 1", "start_pos": 109, "end_pos": 122, "type": "METRIC", "confidence": 0.9352373778820038}]}, {"text": " Table 5: Experiments with the ILP method using a thresh- old of 1-4 (times a word-pair is seen) to trigger rule learn- ing. The figures in parentheses are the same results with  the added postprocessing unigram filter that, given sev- eral output candidates of the standard dialect, chooses the  most frequent one.", "labels": [], "entities": [{"text": "thresh- old", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.6903142929077148}]}, {"text": " Table 6: The best results (per F 1 -score of the two meth- ods). The parameters of method 1 included using only  those string transformations that occur at least 2 times in  the training data, and limiting rule application to a maxi- mum of 2 times within a word, and including a unigram  post-filter. Rules were contextually conditioned. For  method 2, all the examples (threshold 1) in the training  data were used as positive and negative evidence, with- out a unigram filter.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9551495760679245}]}]}