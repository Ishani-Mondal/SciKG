{"title": [{"text": "The People's Web meets Linguistic Knowledge: Automatic Sense Alignment of Wikipedia and WordNet", "labels": [], "entities": [{"text": "Automatic Sense Alignment", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6148240168889364}, {"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.4332748353481293}]}], "abstractContent": [{"text": "We propose a method to automatically align WordNet synsets and Wikipedia articles to obtain a sense inventory of higher coverage and quality.", "labels": [], "entities": []}, {"text": "For each WordNet synset, we first extract a set of Wikipedia articles as alignment candidates; in a second step, we determine which article (if any) is a valid alignment, i.e. is about the same sense or concept.", "labels": [], "entities": []}, {"text": "In this paper, we go significantly beyond state-of-the-art word overlap approaches, and apply a threshold-based Personalized PageRank method for the disambiguation step.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.7563038766384125}]}, {"text": "We show that WordNet synsets can be aligned to Wikipedia articles with a performance of up to 0.78 F 1-Measure based on a comprehensive, well-balanced reference dataset consisting of 1,815 manually annotated sense alignment candidates.", "labels": [], "entities": [{"text": "F 1-Measure", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.9072221517562866}]}, {"text": "The fully-aligned resource as well as the reference dataset is publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical semantic resources often used as sense inventories area prerequisite in automatic processing of human language.", "labels": [], "entities": []}, {"text": "In the last few years, there has been arise in research aligning different resources to overcome the knowledge acquisition bottleneck and coverage problems pertinent to any single resource.", "labels": [], "entities": []}, {"text": "In this paper, we address the task of aligning WordNet noun synsets and Wikipedia articles to obtain a sense inventory of higher coverage and quality.", "labels": [], "entities": []}, {"text": "WordNet, a lexical database for English, is extensively used in the NLP community and is a de-facto standard resource in many NLP tasks, especially in current WSD research).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9567842483520508}]}, {"text": "WordNet's manually defined comprehensive taxonomy motivates many researchers to utilize it.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9445176720619202}]}, {"text": "However, as WordNet is maintained by only a small group of experts, it is hard to cope with neologisms, named entities, or rare usages on a large scale (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9042292237281799}]}, {"text": "In order to compensate for WordNet's lack of coverage, Wikipedia has turned out to be a valuable resource in the NLP community.", "labels": [], "entities": []}, {"text": "Wikipedia has the advantage of being constantly updated by thousands of voluntary contributors.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.92042076587677}]}, {"text": "It is multilingual and freely available containing a tremendous amount of encyclopedic knowledge enriched with hyperlink information.", "labels": [], "entities": []}, {"text": "In the past, researchers have explored the alignment of Wikipedia categories and WordNet synsets (e.g.,;).", "labels": [], "entities": []}, {"text": "However, using the categories instead of the articles causes three limitations: First, the number of Wikipedia categories (about 0.5 million in the English edition) is much smaller compared to the number of articles (about 3.35 million).", "labels": [], "entities": []}, {"text": "Secondly, the category system in Wikipedia is not structured consistently.", "labels": [], "entities": []}, {"text": "And finally, disregarding the article level neglects the huge amount of textual content provided by the articles.", "labels": [], "entities": []}, {"text": "Therefore, attempts to align WordNet synsets and Wikipedia articles (instead of categories) have been recently made.", "labels": [], "entities": []}, {"text": "This has three major benefits.", "labels": [], "entities": []}, {"text": "First of all, as WordNet and Wikipedia were found to be partly complementary on the word sense level, an aligned resource would increase the coverage of senses (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.9352846145629883}]}, {"text": "Second, word senses contained in both resources can then be represented by relational information from WordNet and encyclopedic information from Wikipedia in a multilingual manner yielding an enriched knowledge representation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9520451426506042}]}, {"text": "And finally, the third major benefit of the alignment is the ability to automatically acquire sense-tagged corpora in a mono-and multilingual fashion.", "labels": [], "entities": []}, {"text": "For each WordNet synset, the text of the aligned Wikipedia article (or all sentences or paragraphs in Wikipedia that contain a link to the article) can be automatically extracted similar to the approach proposed by.", "labels": [], "entities": []}, {"text": "Automatically generated sense-tagged corpora can be used to, e.g., counter the bottleneck of supervised WSD methods that rely on such sense-tagged text collections, which are rare.", "labels": [], "entities": [{"text": "WSD", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9529238939285278}]}, {"text": "Further, due to the cross-lingual links in Wikipedia, also corpora in different languages can be constructed easily.", "labels": [], "entities": []}, {"text": "Our contribution to this paper is two-fold.", "labels": [], "entities": []}, {"text": "First, we propose a novel two-step approach to align WordNet synsets and Wikipedia articles.", "labels": [], "entities": []}, {"text": "We model the task as a word sense disambiguation problem applying the Personalized PageRank algorithm proposed by as it is state-of-the-art in WSD and combine it with a word overlap measure, which increases the overall performance.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7323440710703532}]}, {"text": "Second, we generate and introduce a well-balanced reference dataset for evaluation consisting of 1,815 manually annotated sense alignment candidates.", "labels": [], "entities": []}, {"text": "WordNet synsets and their corresponding Wikipedia article candidates are sampled along their distinctive properties such as synset size, domain, or the location in the WordNet taxonomy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9162935614585876}, {"text": "WordNet taxonomy", "start_pos": 168, "end_pos": 184, "type": "DATASET", "confidence": 0.9390736818313599}]}, {"text": "An evaluation on this dataset let us generalize the performance to a full alignment between WordNet and Wikipedia, which is publicly available for further research activities.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9589025378227234}]}], "datasetContent": [{"text": "Publicly available evaluation datasets as provided by and, are either quite small or follow a different annotation scheme.", "labels": [], "entities": []}, {"text": "Others consist of randomly sampled synsets, which do not properly represent the distribution of synsets in WordNet following specific properties.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9350218176841736}]}, {"text": "For example, the dataset used in (Ponzetto and Navigli, 2010) consists of only 2 sense pairs, whose lemmas are monosemous in WordNet and Wikipedia (e.g. the lemma specifier corresponds to one synset in WordNet and one article in Wikipedia).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9363341331481934}]}, {"text": "As this property holds for one-third of all WordNet noun synsets, it is crucial for the choice of the alignment method and thus, should be represented in the evaluation dataset adequately.", "labels": [], "entities": [{"text": "WordNet noun synsets", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.8723402420679728}]}, {"text": "Therefore, our goal in this paper is to compile a well-balanced dataset to cover different domains and properties.", "labels": [], "entities": []}, {"text": "Synsets can be characterized with respect to their so-called assigned Unique Beginner, their synset size, and their location within the WordNet taxonomy.", "labels": [], "entities": [{"text": "WordNet taxonomy", "start_pos": 136, "end_pos": 152, "type": "DATASET", "confidence": 0.9466058313846588}]}, {"text": "The Unique Beginners group synsets in semantically related fields: Inter-annotator agreement synsets for which more than one Wikipedia candidate article is returned.", "labels": [], "entities": []}, {"text": "In summary, for example, the synset <article, clause: a separate section of a legal document> has a synset size of 2, is assigned to the Unique Beginner communication, has a shortest path to the root element of length 6, and has 5 extracted Wikipedia candidate articles.", "labels": [], "entities": []}, {"text": "Based on these distinctive properties, we sampled 320 noun synsets yielding 1,815 sense pairs to be annotated, i.e. 5.7 Wikipedia articles per synset on average.", "labels": [], "entities": []}, {"text": "The exact proportion of synsets with respect to their properties is detailed in in the first four columns.", "labels": [], "entities": []}, {"text": "The manual sense alignment is performed by three human annotators.", "labels": [], "entities": [{"text": "manual sense alignment", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6340111196041107}]}, {"text": "The annotators were provided sense alignment candidate pairs, each consisting of a WordNet synset and a Wikipedia article.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9484308362007141}]}, {"text": "The annotation task was to label each sense pair either as alignment or not.", "labels": [], "entities": []}, {"text": "outlines the class distribution for three annotators and the majority decision.", "labels": [], "entities": []}, {"text": "The most sense alignment candidates were annotated as non-alignments; only between 210 and 244 sense pairs were considered as alignments (extracted for 320 WordNet synsets).", "labels": [], "entities": [{"text": "WordNet synsets", "start_pos": 156, "end_pos": 171, "type": "DATASET", "confidence": 0.9337531924247742}]}, {"text": "To assess the reliability of the annotators' decision, we computed the pairwise observed inter-annotator agreement A O and the chance-corrected agreement \u03ba ( . The agreement values are shown in.", "labels": [], "entities": [{"text": "reliability", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9607146978378296}, {"text": "pairwise observed inter-annotator agreement A O", "start_pos": 71, "end_pos": 118, "type": "METRIC", "confidence": 0.6446224053700765}]}, {"text": "The average observed agreement A O is 0.9721, while the multi-\u03ba is 0.8727 indicating high reliability.", "labels": [], "entities": [{"text": "agreement A O", "start_pos": 21, "end_pos": 34, "type": "METRIC", "confidence": 0.9094129602114359}, {"text": "reliability", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9666688442230225}]}, {"text": "The final dataset was compiled by means of a majority decision.", "labels": [], "entities": []}, {"text": "Given 1,815 sense alignment candidate pairs, 1,588 were annotated as non-alignments, while 227 were annotated as alignments.", "labels": [], "entities": []}, {"text": "215 synsets were aligned with one article, while 6 synsets were aligned with two articles.", "labels": [], "entities": []}, {"text": "Interesting to note is that the aligned samples are uniformly distributed among the different sampling dimensions as shown in (right column).", "labels": [], "entities": []}, {"text": "It demonstrates that WordNet synsets of different properties are contained in Wikipedia.", "labels": [], "entities": []}, {"text": "On the other side, 99 synsets, i.e. approx. 1/3 of the sampled synsets, could not be aligned.", "labels": [], "entities": []}, {"text": "Most of them are not contained in Wikipedia at all, e.g. the synset <dream (someone or something wonderful)> or <outside, exterior (the region that is outside of something)>.", "labels": [], "entities": []}, {"text": "Others are not explicitly encoded on the article level such as the synset <quatercentennial, quatercentenary (the 400th anniversary (or the celebration of it))>, which is part of the more general Wikipedia article <Anniversary>.", "labels": [], "entities": []}, {"text": "In our experiments, we represent a WordNet synset either by itself (in the direct version ppr d ) or by its set of synonymous word senses and its gloss and examples (in the basic version ppr ).", "labels": [], "entities": []}, {"text": "Optionally, we include hyponym and hypernym synsets to extend the sense representation of a synset:: Results for the automatic alignment given synset; (HYPER): all hypernym synsets of the given synset; (HYPO): all hyponym synsets of the given synset; (HYP2): all hypernym and hyponym synsets of the given synset.", "labels": [], "entities": []}, {"text": "A Wikipedia article is represented by either its first paragraph 6 as it usually contains a compact description of the article or its whole article text.", "labels": [], "entities": []}, {"text": "The article title and additional assigned information such as categories or redirects can also betaken into account: (P): first paragraph of Wikipedia article (with a minimum length of 200 characters 7 ); (TXT): the whole article text; (T): article title; (C): all categories assigned to the article; (R): all redirects assigned to the article.", "labels": [], "entities": []}, {"text": "lists the performance of our approach for different experimental settings.", "labels": [], "entities": []}, {"text": "We evaluate our approach in terms of F 1 -Measure (F 1 = 2 * P * RP +R ), where P is the precision and R the recall.", "labels": [], "entities": [{"text": "F 1 -Measure", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.964299887418747}, {"text": "F 1 = 2 * P * RP +R )", "start_pos": 51, "end_pos": 72, "type": "METRIC", "confidence": 0.6985111968083815}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9992833733558655}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9990859031677246}]}, {"text": "The precision P determines the ratio of correct alignments to all alignments assigned by the algorithm.", "labels": [], "entities": [{"text": "precision P", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9448016583919525}]}, {"text": "The recall R identifies the number of correct alignments to the total number of correct alignments in the gold standard.", "labels": [], "entities": [{"text": "recall R", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9806778728961945}]}, {"text": "Further, we provide an accuracy measure Acc, which denotes the percentage of the correctly identified alignments and non-alignments.", "labels": [], "entities": [{"text": "accuracy measure", "start_pos": 23, "end_pos": 39, "type": "METRIC", "confidence": 0.9596610963344574}, {"text": "Acc", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.6894553899765015}]}, {"text": "Overall, the Personalized PageRank approach outperforms the cosine similarity.", "labels": [], "entities": []}, {"text": "cos achieves an F 1 -Measure of 0.738, while ppr d reaches 0.754 and ppr even 0.776, which is a performance gain of 2.1% and 5.1%, respectively.", "labels": [], "entities": [{"text": "F 1 -Measure", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.9579708278179169}]}, {"text": "This, in fact, strengthens our motivation to employ semantic relatedness based approaches instead of a simple word overlap approach.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 110, "end_pos": 122, "type": "TASK", "confidence": 0.7023283839225769}]}, {"text": "For example, the synset <Johannesburg> and its corresponding Wikipedia article is not aligned based on the cosine approach as only three terms overlap.", "labels": [], "entities": []}, {"text": "However, the ppr and ppr d approach classify the synset-article pair as alignment as there exists semantic relatedness between \"large economy\" and \"commercial center\" occurring in the textual sense representations.", "labels": [], "entities": []}, {"text": "The performance differences between ppr d and ppr correlate with the synset representation.", "labels": [], "entities": []}, {"text": "On the one hand, utilizing the SYN representation, ppr d outperforms the ppr approach.", "labels": [], "entities": []}, {"text": "This shows the effect of disambiguating the WordNet synset beforehand.", "labels": [], "entities": [{"text": "WordNet synset", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9175962805747986}]}, {"text": "On the other hand, when presenting the synset together with its hypernym or both, hypernyms and hyponyms, ppr yields the best performance.", "labels": [], "entities": []}, {"text": "This might be due to the fact that a Wikipedia article often contains more general terms, i.e. hypernym concepts, especially within the first paragraph of a Wikipedia article.", "labels": [], "entities": []}, {"text": "All combinations yield higher performance compared to the stand-alone classifiers.", "labels": [], "entities": []}, {"text": "For example, for the setting SYN+HYPER and P+T+C, cos yields 0.738, ppr 0.765, and the combination of both 0.781   performance, which is an improvement of 5.8% and 2.1% compared to the cos and ppr approach, respectively.", "labels": [], "entities": []}, {"text": "The performance gain originates from higher precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9975971579551697}]}, {"text": "All similarity measures yield better performance representing the WordNet synset together with their hypernym synsets regardless of the representation of the Wikipedia article.", "labels": [], "entities": []}, {"text": "As stated before, this might be due to the fact that Wikipedia articles often contain hypernym concepts in their textual representation.", "labels": [], "entities": []}, {"text": "Further, each synset has exactly one direct hypernym concept, while the number of hyponym concepts is not limited.", "labels": [], "entities": []}, {"text": "This can cause a very noisy description of a synset, not focusing on the textual representation of the actual sense.", "labels": [], "entities": []}, {"text": "When representing the Wikipedia sense, the categories always boost the performance, while redirects are not helpful and can yield even a performance drop.", "labels": [], "entities": []}, {"text": "The reason might be that redirects contain much noisy information, e.g. spelling variations.", "labels": [], "entities": []}, {"text": "The rand and the mfs baselines achieve an F 1 -Measure of 0.527 and 0.534, respectively.", "labels": [], "entities": [{"text": "F 1 -Measure", "start_pos": 42, "end_pos": 54, "type": "METRIC", "confidence": 0.959920272231102}]}, {"text": "They always assign a sense even only 221 of 320 synsets can be aligned to Wikipedia.", "labels": [], "entities": []}, {"text": "If we only consider the 221 synsets for which an alignment exist, the mfs baseline achieves an F 1 -Measure of 0.76, i.e. for 146 out of 221 synsets the aligned Wikipedia article is the most frequent sense as we defined it in Section 3.2.", "labels": [], "entities": [{"text": "F 1 -Measure", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9814758002758026}]}, {"text": "The human annotators show a pairwise agreement \u03ba between 0.866 and 0.878, which serves as an upper bound for this task.", "labels": [], "entities": []}, {"text": "For each measure and its best performing experimental setting as listed in, we calculate the agreement with the annotators' alignments (see).", "labels": [], "entities": []}, {"text": "The combined approach ppr + cos achieves the highest agreement values \u03ba, between 0.728 and 0.740.", "labels": [], "entities": []}, {"text": "These values show that the automatic annotation is fairly reliable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sampling by properties and # manual alignments", "labels": [], "entities": [{"text": "Sampling", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9828631281852722}]}, {"text": " Table 2: Annotations per class", "labels": [], "entities": []}, {"text": " Table 4: Results for the automatic alignment", "labels": [], "entities": [{"text": "automatic alignment", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6350082606077194}]}, {"text": " Table 5: Agreement (\u03ba) between automatic and human annotators", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9814106822013855}]}, {"text": " Table 6: Confusion matrix (Setting: ppr + cos , SYN+HYPER, P+T+C)", "labels": [], "entities": [{"text": "HYPER", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.6889065504074097}]}]}