{"title": [{"text": "Bayesian Network Automata for Modelling Unbounded Structures", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a framework which unifies graphical model theory and formal language theory through automata theory.", "labels": [], "entities": [{"text": "graphical model theory", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.6429621080557505}]}, {"text": "Specifically, we propose Bayesian Network Automata (BNAs) as a formal framework for specifying graphical models of arbitrarily large structures, or equivalently, specifying probabilistic grammars in terms of graphical models.", "labels": [], "entities": []}, {"text": "BNAs use a formal automaton to specify how to construct an arbitrarily large Bayesian Network by connecting multiple copies of a bounded Bayesian Network.", "labels": [], "entities": []}, {"text": "Using a combination of results from graphical models and formal language theory, we show that, fora large class of au-tomata, the complexity of inference with a BNA is bounded by the complexity of inference in the bounded Bayesian Network times the complexity of inference for the equivalent stochastic automaton.", "labels": [], "entities": []}, {"text": "This illustrates that BNAs provide a useful framework for developing and analysing models and algorithms for structure prediction.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.8008276224136353}]}], "introductionContent": [{"text": "Work in Computational Linguistics has developed increasingly sophisticated probabilistic models of language.", "labels": [], "entities": []}, {"text": "For example, Latent Probabilistic Context-Free Grammars (LPCFGs) () have been developed with latent head labels, multiple latent variables decorating each nonterminal, and a hierarchy of latent nonterminal subcategories (.", "labels": [], "entities": []}, {"text": "In this paper we propose a general framework which facilitates the specification and parsing of such complex models by exploiting graphical models to express local bounded statistical relationships, while still allowing the use of grammar formalisms to express the unbounded nature of natural language.", "labels": [], "entities": []}, {"text": "Graphical models were developed as a unification of probability theory and graph theory.", "labels": [], "entities": []}, {"text": "They have proved a powerful framework for specifying and reasoning about probabilistic models.", "labels": [], "entities": []}, {"text": "Dynamic Bayesian Networks extend this framework to models which describe arbitrarily long sequences.", "labels": [], "entities": []}, {"text": "There has been work applying ideas from graphical models to more complex unbounded structures, such as natural language parse trees (e.g. (), but the power of the architectures proposed for such extensions have not been formally characterised.", "labels": [], "entities": []}, {"text": "The formal power of systems for specifying arbitrarily large structures has been studied extensively in the area of formal language theory.", "labels": [], "entities": [{"text": "formal language theory", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.6641587416330973}]}, {"text": "Formal language theory has proved a wide range of equivalences and subsumptions between grammar formalisms, the most well known example being the Chomsky hierarchy (i.e. finite languages < regular languages < context-free languages < context-sensitive languages < recursively enumerable languages).", "labels": [], "entities": [{"text": "Formal language theory", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.761267085870107}]}, {"text": "It has also demonstrated the equivalence between formal grammars and formal automata (e.g. finite state automata generate regular languages, push-down automata generate context-free languages, Turing machines generate recursively enumerable languages).", "labels": [], "entities": []}, {"text": "While grammar formalisms are generally more readable than automata, they appear in a wide variety of notational variants, whereas automata provide a relatively consistent framework in which different grammar formalisms can be compared.", "labels": [], "entities": []}, {"text": "Automata also provide a clearer connection to Dynamic Bayesian Networks.", "labels": [], "entities": []}, {"text": "For these reasons, this paper uses automata theory rather than grammars, although many of the ideas are trivially transferable to grammars.", "labels": [], "entities": []}, {"text": "We propose Bayesian Network Automata (BNAs) as a framework for specifying stochastic automata which generate arbitrarily large (i.e. unbounded) structures.", "labels": [], "entities": []}, {"text": "A BNA is a standard stochastic automaton, but uses a Bayesian Network (BN) to specify its stochastic transition function.", "labels": [], "entities": []}, {"text": "For example, the specification of a stochastic push-down automaton (PDA) requires a specification of the conditional probability distribution over push(X) and pop actions given the current state, input symbol, and top stack symbol.", "labels": [], "entities": []}, {"text": "This distribution can be specified in a BN.", "labels": [], "entities": [{"text": "BN", "start_pos": 40, "end_pos": 42, "type": "DATASET", "confidence": 0.6366683840751648}]}, {"text": "While not changing the theoretical properties of the stochastic automaton, this use of BNs allows us to merge algorithms and theoretical results from BNs with those for the stochastic automaton, and thereby with those for the equivalent probabilistic grammar formalism.", "labels": [], "entities": []}, {"text": "In the PDA example, the algorithm discussed in section 5.2 allows us to sum overall possible values for X in push(X) while at the same time summing overall possible parse tree structures, as is used in unsupervised grammar induction.", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 202, "end_pos": 232, "type": "TASK", "confidence": 0.7163248459498087}]}, {"text": "We argue that this merging with graphical model theory simplifies the specification of more complex stochastic transition functions or grammar rules (e.g. (), and reduces the need for adhoc solutions to such inference problems.", "labels": [], "entities": []}, {"text": "BNAs can also be seen as a generalisation of Dynamic Bayesian Networks to more complex unbounded structures.", "labels": [], "entities": []}, {"text": "BNAs simplify the specification and analysis of these more complex BNs by drawing a distinction between a finite set of statistical relationships, represented in a Bayesian Network, and the recursive nature of the unbounded structures, represented by the control mechanisms of the automaton.", "labels": [], "entities": []}, {"text": "To illustrate the usefulness of this framework, we exploit the separation between the automaton and the Bayesian Network to provide bounds on the complexity of inference in some classes of BNAs.", "labels": [], "entities": []}, {"text": "In particular, fora large class of grammar formalisms, the complexity of marginalising over variables in the Bayesian Network and the complexity of marginalising over structures from the automaton are independent factors in the complexity of inference with the BNA.", "labels": [], "entities": []}, {"text": "We also exploit results from formal language theory to characterise the power of previously proposed models in terms of the generative capacity of their automata.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we first define the framework of Bayesian Network Automata, and discuss its key properties.", "labels": [], "entities": []}, {"text": "This provides the formal mechanisms we need to compare various Bayesian Network architectures that have been previously proposed.", "labels": [], "entities": []}, {"text": "We then provide results on the efficiency of inference in BNAs.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}