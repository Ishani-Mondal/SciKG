{"title": [{"text": "Generating Example Contexts to Illustrate a Target Word Sense", "labels": [], "entities": []}], "abstractContent": [{"text": "Learning a vocabulary word requires seeing it in multiple informative contexts.", "labels": [], "entities": []}, {"text": "We describe a system to generate such contexts fora given word sense.", "labels": [], "entities": []}, {"text": "Rather than attempt to do word sense disambiguation on example contexts already generated or selected from a corpus, we compile information about the word sense into the context generation process.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6990393102169037}]}, {"text": "To evaluate the sense-appropriateness of the generated contexts compared to WordNet examples, three human judges chose which word sense(s) fit each example , blind to its source and intended sense.", "labels": [], "entities": []}, {"text": "On average, one judge rated the generated examples as sense-appropriate, compared to two judges for the WordNet examples.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.943863570690155}]}, {"text": "Although the system's precision was only half of Word-Net's, its recall was actually higher than WordNet's, thanks to covering many senses for which WordNet lacks examples.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996672868728638}, {"text": "Word-Net", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9733623266220093}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9997797608375549}, {"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9556001424789429}, {"text": "WordNet", "start_pos": 149, "end_pos": 156, "type": "DATASET", "confidence": 0.9431725740432739}]}], "introductionContent": [{"text": "Learning word meaning from example contexts is an important aspect of vocabulary learning.", "labels": [], "entities": [{"text": "Learning word meaning from example contexts", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7967403133710226}, {"text": "vocabulary learning", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8195036351680756}]}, {"text": "Contexts give clues to semantics but also convey many other lexical aspects, such as parts of speech, morphology, and pragmatics, which help enrich a person's word knowledge base).", "labels": [], "entities": []}, {"text": "Accordingly, one key issue in vocabulary instruction is how to find or create good example contexts to help children learn a particular sense of a word.", "labels": [], "entities": []}, {"text": "Hand-vetting automatically generated contexts can be easier than hand-crafting them from scratch (;.", "labels": [], "entities": []}, {"text": "This paper describes what we believe is the first system to generate example contexts fora given target sense of a polysemous word.", "labels": [], "entities": []}, {"text": "characterized good contexts for helping children learn vocabulary and generated them fora target part of speech, but not a given word sense.", "labels": [], "entities": []}, {"text": "addressed the polysemy issue, but in a system for selecting contexts rather than for generating them.", "labels": [], "entities": []}, {"text": "Generation can supply more contexts fora given purpose, e.g. teaching children, than WordNet or a fixed corpus contains.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9547367095947266}]}, {"text": "Section 2 describes a method to generate sensetargeted contexts.", "labels": [], "entities": []}, {"text": "Section 3 compares them to WordNet examples.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9151310324668884}]}], "datasetContent": [{"text": "To evaluate our method, we picked 8 target words from a list of polysemous vocabulary words used in many domains and hence important for children to learn.", "labels": [], "entities": []}, {"text": "Four of them are nouns: advantage (with 3 synsets), content, force (10), and retreat (7).", "labels": [], "entities": [{"text": "retreat", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9640381932258606}]}, {"text": "Four are verbs: dash (6), decline, direct, and reduce (20).", "labels": [], "entities": [{"text": "reduce", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9716349244117737}]}, {"text": "Some of these words can have other parts of speech, but we exclude those senses, leaving 73 senses in total.", "labels": [], "entities": []}, {"text": "We use their definitions from WordNet because it is a widely used, comprehensive sense inventory.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9726427793502808}]}, {"text": "Some alternative sense inventories might be unsuitable.", "labels": [], "entities": []}, {"text": "For instance, children's dictionaries may lack WordNet's rare senses or hypernym relations.", "labels": [], "entities": []}, {"text": "We generated contexts for these 73 word senses as described in Section 2, typically 3 examples for each word sense.", "labels": [], "entities": []}, {"text": "To reduce the evaluation burden on our human judges, we chose just one context for each word sense, and for words with more than 10 senses we chose a random sample of them.", "labels": [], "entities": []}, {"text": "To avoid unconscious bias, we chose random contexts rather than the best ones, which a human would likelier pick if vetting the generated contexts by hand.", "labels": [], "entities": []}, {"text": "For comparison, we also evaluated WordNet examples (23 in total) where available.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.911365270614624}]}, {"text": "We gave three native English-speaking collegeeducated judges the examples to evaluate independently, blind to their intended sense.", "labels": [], "entities": []}, {"text": "They filled in a table for each target word.", "labels": [], "entities": []}, {"text": "The left column listed the examples (both generated and WordNet) in random order, one per row.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9176541566848755}]}, {"text": "The top row gave the WordNet definition of each synset, one per column.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9590111970901489}]}, {"text": "Judges were told: For each example, put a 1 in the column for the sense that best fits how the example uses the target word.", "labels": [], "entities": []}, {"text": "If more than one sense fits, rank them 1, 2, etc.", "labels": [], "entities": []}, {"text": "Use the last two columns only to say that none of the senses fit, or you can't tell, and why.", "labels": [], "entities": [{"text": "tell", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9501755833625793}]}, {"text": "(Only 10 such cases arose.)", "labels": [], "entities": []}, {"text": "We measured inter-rater reliability at two levels.", "labels": [], "entities": []}, {"text": "At the fine-grained level, we measured how well the judges agreed on which one sense fit the example best.", "labels": [], "entities": []}, {"text": "The value of Fleiss' Kappa (Shrout and Fleiss 1979) was 42%, considered moderate.", "labels": [], "entities": [{"text": "Fleiss' Kappa", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.6769183874130249}]}, {"text": "At the coarse-grained level, we measured how well judges agreed on which sense(s) fit at all.", "labels": [], "entities": []}, {"text": "Here Fleiss' Kappa was 48%, also considered moderate.", "labels": [], "entities": [{"text": "Fleiss' Kappa", "start_pos": 5, "end_pos": 18, "type": "METRIC", "confidence": 0.8308463990688324}]}, {"text": "We evaluated the examples on three criteria.", "labels": [], "entities": []}, {"text": "Yield is the percentage of intended senses for which we generate at least one example -whether it fits or not.", "labels": [], "entities": []}, {"text": "For the 73 synsets, this percentage is 92%.", "labels": [], "entities": []}, {"text": "Moreover, we typically generate 3 examples fora word sense.", "labels": [], "entities": []}, {"text": "In comparison, only 34% of the synsets have even a single example in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9762450456619263}]}, {"text": "(Fine-grained) precision is the percentage of examples that the intended sense fits best according to the judges.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9735420942306519}]}, {"text": "Human judges often disagree, so we prorate this percentage by the percentage of judges who chose the intended sense as the best fit.", "labels": [], "entities": []}, {"text": "The result is algebraically equivalent to computing precision separately according to each judge, and then averaging the results.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.997149646282196}]}, {"text": "Precision for generated examples was 36% for those 23 synsets and 27% for all 67 synsets with generated examples.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9949860572814941}]}, {"text": "Although we expected WordNet to be a gold standard, its precision for the 23 synsets having examples was 52% -far less than 100%.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9473593831062317}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9995294809341431}]}, {"text": "This low precision suggests that the WordNet contexts to illustrate different senses were often not informative enough for the judges to distinguish them from all the other senses.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.998678982257843}]}, {"text": "For example, the WordNet example reduce one's standard of living is attached to the sense -lessen and make more modest.\u2016 However, this sense is hard to distinguish from -lower in grade or rank or force somebody into an undignified situation.\u2016 In fact, two judges did not choose the first sense, and one of them chose the second sense as the best fit.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.9498897194862366}]}, {"text": "Coarse-grained precision is similar, but based on how often the intended sense fits the example at all, whether or not it fits best.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9753373265266418}]}, {"text": "Coarse-grained precision was 67% for the 23 WordNet examples, 40% for the examples generated for those 23 synsets, and 33% for all 67 generated examples.", "labels": [], "entities": [{"text": "Coarse-grained", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9704364538192749}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8342351317405701}, {"text": "WordNet examples", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.9394136369228363}]}, {"text": "Coarse-grained precision is important because fine-grained semantic distinctions do not matter in illustrating a core sense of a word.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9644606709480286}]}, {"text": "The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required.", "labels": [], "entities": []}, {"text": "Rather than attempt to identify a single definitive partition of a target word's synsets into coarse senses, we implicitly define a coarse sense as the subset of synsets rated by a judge as fitting a given example.", "labels": [], "entities": []}, {"text": "Thus the clustering into coarse senses is not only judge-specific but example-specific: different, possibly overlapping sets of synsets may fit different examples.", "labels": [], "entities": []}, {"text": "Recall is the percentage of synsets that fit their generated examples.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9886447191238403}]}, {"text": "Algebraically it is the product of precision and yield.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9995452761650085}, {"text": "yield", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9632614850997925}]}, {"text": "Fine-grained recall was 25% for the generated examples, compared to only 18% for the WordNet examples.", "labels": [], "entities": [{"text": "Fine-grained", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9233788847923279}, {"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9407625794410706}, {"text": "WordNet examples", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.9605664014816284}]}, {"text": "Coarse-grained recall was 30% for the generated examples, compared to 23% for the WordNet examples.", "labels": [], "entities": [{"text": "Coarse-grained", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9787549376487732}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9379445910453796}, {"text": "WordNet examples", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.9612993597984314}]}, {"text": "shows how yield, inter-rater agreement, and coarse and fine precision for the 8 target words vary with their number of synsets.", "labels": [], "entities": [{"text": "yield", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9934713840484619}, {"text": "agreement", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.7544336318969727}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.8204882740974426}]}, {"text": "With so few words, this analysis is suggestive, not conclusive.", "labels": [], "entities": []}, {"text": "We plot all four metrics on the same scale to save space, but only the last two metrics have directly comparable values, However, it is still meaningful to compare how they vary.", "labels": [], "entities": []}, {"text": "Precision and inter-rater reliability generally appear to decrease with the number of senses.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9908209443092346}, {"text": "reliability", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.8414480686187744}]}, {"text": "As polysemy increases, the judges have more ways to disagree with each other and with our program.", "labels": [], "entities": []}, {"text": "Yield is mostly high, but might be lower for words with many senses, due to deficient document corpora for rare senses.", "labels": [], "entities": [{"text": "Yield", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9741317629814148}]}, {"text": "Errors occur when 1) the corpus is missing a word sense; 2) LDA fails to find good sense indicators; or 3) Context Generation fails to generate a sense-appropriate context.", "labels": [], "entities": [{"text": "Context Generation", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8257712721824646}]}, {"text": "Our method succeeds when (1) the target sense occurs in the corpus, (2) LDA finds good indicators for it, and (3) Context Generation uses them to construct a sense-appropriate context.", "labels": [], "entities": [{"text": "Context Generation", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8439356684684753}]}, {"text": "For example, the first sense of advantage is -the quality of having a superior or more favorable position,\u2016 for which we obtain the sense indicators support, work, time, order, life, knowledge, mind, media, human, market, experience, nature, make, social, information, child, individual, cost, people, power, good, land, strategy, and company, and generate (among others) the context \u2026knowledge gave him an advantage\u2026.", "labels": [], "entities": [{"text": "work, time, order, life, knowledge, mind, media, human, market, experience, nature, make, social, information, child, individual, cost, people, power, good, land, strategy, and company", "start_pos": 158, "end_pos": 342, "type": "Description", "confidence": 0.8491897913424865}]}, {"text": "Errors occur when any of these 3 steps fails.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9719461798667908}]}, {"text": "Step 1 fails for the sense -reduce in scope while retaining essential elements\u2016 of reduce because it is so general that no good example exists in the corpus for it.", "labels": [], "entities": []}, {"text": "Step 2 fails for the sense of force in -the force of his eloquence easily persuaded them\u2016 because its sense indicators are men, made, great, page, man, time, general, day, found, side, called, and house.", "labels": [], "entities": []}, {"text": "None of these words are precise enough to convey the sense.", "labels": [], "entities": []}, {"text": "Step 3 fails for the sense of advantage as -(tennis) first point scored after deuce,\u2016 with sense indicators point, game, player, tennis, set, score, points, ball, court, service, serve, called, win, side, players, play, team, games, match, wins, won, net, deuce, line, opponent, and turn.", "labels": [], "entities": []}, {"text": "This list looks suitably tennisrelated.", "labels": [], "entities": []}, {"text": "However, the generated context \u2026the player has an advantage\u2026 fits the first sense of advantage; here the indicator player for the tennis sense is misleading.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generated examples vs. WordNet", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.8905553221702576}]}]}