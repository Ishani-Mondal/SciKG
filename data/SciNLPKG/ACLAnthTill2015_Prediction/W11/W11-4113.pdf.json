{"title": [{"text": "AVATecH: Audio/Video Technology for Humanities Research", "labels": [], "entities": []}], "abstractContent": [{"text": "In the AVATecH project the Max-Planck Institute for Psycholinguistics (MPI) and the Fraunhofer institutes HHI and IAIS aim to significantly speedup the process of creating annotations of audiovisual data for humanities research.", "labels": [], "entities": [{"text": "HHI", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.7748451232910156}]}, {"text": "For this we integrate state-of-the-art audio and video pattern recognition algorithms into the widely used ELAN annotation tool.", "labels": [], "entities": [{"text": "audio and video pattern recognition", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.7680391669273376}]}, {"text": "To address the problem of heterogeneous annotation tasks and recordings we provide modular components extended by adaptation and feedback mechanisms to achieve competitive annotation quality within significantly less annotation time.", "labels": [], "entities": []}, {"text": "Currently we are designing a large-scale end-user evaluation of the project.", "labels": [], "entities": []}], "introductionContent": [{"text": "The AVATecH project 1 is a collaborative research project between the Max-Planck Institute for Psycholinguistics (MPI) on the one hand and the Fraunhofer Institutes HHI and IAIS on the other hand.", "labels": [], "entities": [{"text": "Fraunhofer Institutes HHI", "start_pos": 143, "end_pos": 168, "type": "DATASET", "confidence": 0.82353542248408}, {"text": "IAIS", "start_pos": 173, "end_pos": 177, "type": "DATASET", "confidence": 0.7654780745506287}]}, {"text": "The aim of the project is to enable researchers in the field of humanities to significantly speedup their annotation process.", "labels": [], "entities": []}, {"text": "This process is inevitable, for example, for carrying out deep linguistic studies (.", "labels": [], "entities": []}, {"text": "To reach this goal the Fraunhofer institutes provide audio and video pattern recognition technology for (semi-) automatic extraction of content related annotations.", "labels": [], "entities": [{"text": "audio and video pattern recognition", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.6964838445186615}, {"text": "automatic extraction of content related annotations", "start_pos": 112, "end_pos": 163, "type": "TASK", "confidence": 0.7280059456825256}]}, {"text": "By integrating them into the common annotation process for linguistic research we expect a significant reduction of the overall annotation time.", "labels": [], "entities": []}, {"text": "High potential of such technologies and tools for increasing annotation speed has been shown in).", "labels": [], "entities": [{"text": "speed", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.6280874013900757}]}, {"text": "The central ideas in the AVATecH project are to adapt models to the given annotation scenario and exploit iterative feedback from the human annotator.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation, we are interested in two sets of information: qualitative statements from the human annotators that assess the annotation experience when using the AVATecH system and a quantitative evaluation of the actual annotation speed-up compared to manual annotation.", "labels": [], "entities": []}, {"text": "During the qualitative evaluation we are interested how the work of annotators is supported by automatic analysis, and whether annotators think that the system is beneficial.", "labels": [], "entities": []}, {"text": "In the evaluation, subjects annotate their own material with ELAN supported by recognizers.", "labels": [], "entities": []}, {"text": "We record their expe-rience with a questionnaire.", "labels": [], "entities": []}, {"text": "Within the qualitative evaluation we aim to coverall three annotation tasks mentioned above.", "labels": [], "entities": []}, {"text": "The questionnaire consists of 20 questions addressing the quality of the ELAN interface, the productivity using recognizers during annotating, the quality of the delivered results and the overall experience.", "labels": [], "entities": []}, {"text": "We aim to incorporate at least thirty MPI researchers or students who want to annotate their data with recognizer support.", "labels": [], "entities": []}, {"text": "Within quantitative evaluation, we want to measure the speed-up for annotation by using automatic tools.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9611829519271851}]}, {"text": "We ask a limited number of people to perform a specific annotation task fora subset of field recordings or studio recordings from our corpus, both with and without support from automatic analysis.", "labels": [], "entities": []}, {"text": "The annotation will consist of labeling the speech of the interviewee, such that the researcher can quickly browse from answer to answer (for interview recordings) and labeling the gesture of the person in the video, so that the researcher can quickly see when a gesture begins, ends and what kind of motion is associated to it (for studio recordings).", "labels": [], "entities": []}, {"text": "To measure the annotation speed we have defined a metric that can be used not only for our evaluation, but can give a good insight about the general annotation speed of a researcher.", "labels": [], "entities": []}, {"text": "This is not straightforward to assess, because researchers work with recordings of varying complexity (e.g. having few or many relevant events per time unit) and they are looking for different information in them, hence creating very different annotations (from very basic to multi-level, complex annotations with long descriptions).", "labels": [], "entities": []}, {"text": "Our general metric is based on two measures: 1) the number of created annotation blocks per unit of time; 2) the length of the media file.", "labels": [], "entities": []}, {"text": "These values considered together will allow assessing the average annotation speed and complexity of annotation created fora given media file.", "labels": [], "entities": []}, {"text": "Calculating these measures will be done by extending ELAN to record the overall annotation time from opening to closing the project file and to log certain annotation events, e.g., \"created anew label\" or \"created anew segment\", with corresponding timestamps.", "labels": [], "entities": []}, {"text": "However, if the same data is annotated twice by the same subject, the second annotation will be biased as the subject already knows the structure of the file.", "labels": [], "entities": []}, {"text": "Therefore we penalize the automatic annotation and do it first.", "labels": [], "entities": []}, {"text": "Moreover, we split the runs over two days (1 st day: annotation with automatic tools, 2 nd day manual annotation).", "labels": [], "entities": []}, {"text": "Also we measure active vs. passive annotation time, i.e., waiting for recognizers to finish processing.", "labels": [], "entities": []}, {"text": "This can be achieved with proper logging of the times when recognizers start and finish their execution.", "labels": [], "entities": []}, {"text": "As population we will incorporate five to ten advanced ELAN users.", "labels": [], "entities": []}], "tableCaptions": []}