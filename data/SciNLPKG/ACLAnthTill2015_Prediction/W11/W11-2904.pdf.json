{"title": [{"text": "Finding the Most Probable String and the Consensus String: an Algorithmic Study", "labels": [], "entities": []}], "abstractContent": [{"text": "The problem of finding the most probable string fora distribution generated by a weighted finite automaton or a probabilistic grammar is related to a number of important questions: computing the distance between two distributions or finding the best translation (the most probable one) given a prob-abilistic finite state transducer.", "labels": [], "entities": []}, {"text": "The problem is undecidable with general weights and is N P-hard if the automaton is probabilis-tic.", "labels": [], "entities": []}, {"text": "We give a pseudo-polynomial algorithm which computes the most probable string in time polynomial in the inverse of the probability of the most probable string itself, both for probabilistic finite automata and proba-bilistic context-free grammars.", "labels": [], "entities": []}, {"text": "We also give a randomised algorithm solving the same problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "When using probabilistic machines to define distributions over sets of strings, the usual and best studied problems are those of parsing and of finding the most probable explanation of a given string (the most probable parse).", "labels": [], "entities": [{"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9754613637924194}]}, {"text": "These problems, when dealing with probabilistic (generating) finite state automata, hidden Markov Models (HMMs) or probabilistic context-free grammars depend on the ambiguity of the machine: indeed, if there can be different parses for the same string, then the probability of the string is obtained by summing over the different parses.", "labels": [], "entities": []}, {"text": "A more difficult problem we study here is that of finding the most probable string; this string is also known as the consensus string.", "labels": [], "entities": []}, {"text": "The problem of finding the most probable string was first addressed in the computational linguistics community by: he proved the problem to be N P-hard if we consider tree grammars, and as a corollary he gave the same result for context-free grammars.", "labels": [], "entities": []}, {"text": "showed that, in the case of HMMs, the problem of finding whether the most most probable string of a given length n is at least p is N P-Complete.", "labels": [], "entities": []}, {"text": "Moreover, he points that his technique cannot be applied to show the N P-completeness of the problem when n is not prespecified because the most probable string can be exponentially long.", "labels": [], "entities": []}, {"text": "Casacuberta and de la proved the problem to be N P-hard, using techniques developed for linguistic decoding (Casacuberta and de la Higuera, 1999): their result holds for probabilistic finite state automata and for probabilistic transducers even when these are acyclic: in the transducer case the related (and possibly more important) question is that of finding the most probable translation.", "labels": [], "entities": []}, {"text": "The problem was also addressed with motivations in bioinformatics by.", "labels": [], "entities": []}, {"text": "Their technique relies on reductions from maximal cliques.", "labels": [], "entities": []}, {"text": "As an important corollary of their hardness results they prove that the L 1 and L \u221e distances between distributions represented by HMMs are also hard to compute: indeed being able to compute such distances would enable to find (as aside product) the most probable string.", "labels": [], "entities": []}, {"text": "This result was then applied on probabilistic finite automata in () and the L k distance, for each odd k was proved to be intractable.", "labels": [], "entities": []}, {"text": "An essential consequence of these results is that finding the most probable translation given some probabilistic (non deterministic) finite state transducer is also at least as hard.", "labels": [], "entities": []}, {"text": "It can be shown) that solving this problem consists in finding the most probable string inside the set of all acceptable translations, and this set is structured as a probabilistic finite automaton.", "labels": [], "entities": []}, {"text": "Therefore, the most probable translation problem is also N P-hard.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.962275505065918}]}, {"text": "On the other hand, in the framework of multiplicity automata or of accepting probabilistic finite automata (also called Rabin automata), the problem of the existence of a string whose weight is above (or under) a specific threshold is known to be undecidable ().", "labels": [], "entities": []}, {"text": "In the case where the weight of each individual edge is between 0 and 1, the score can be interpreted as a probability.", "labels": [], "entities": []}, {"text": "The differences reside in the fact that in multiplicity automata the sum of the probabilities of all strings does not need to be bounded; this is also the case for Rabin automata, as each probability corresponds to the probability fora given string to belong to the language.", "labels": [], "entities": []}, {"text": "In this paper we attempt to better understand the status of the problem and provide algorithms which find a string of probability higher than a given threshold in time polynomial in the inverse of this threshold.", "labels": [], "entities": []}, {"text": "These algorithms give us pragmatic answers to the consensus string problem as it is possible to use the probabilistic machine to define a threshold and to use our algorithms to find, in this way, the most probable string.", "labels": [], "entities": []}, {"text": "We will first (Section 2) give the different definitions concerning automata theory, distributions over strings and complexity theory.", "labels": [], "entities": []}, {"text": "In Section 3 we show that we can compute the most probable string in time polynomial in the inverse of the probability of this most probable string but in the bounded case, i.e. when we are looking fora string of length smaller than some given bound.", "labels": [], "entities": []}, {"text": "In Section 4 we show how we can compute such bounds.", "labels": [], "entities": []}, {"text": "In Section 5 the algorithms are experimentally compared and we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report here some experiments in which we compared both algorithms over probabilistic automata.", "labels": [], "entities": []}, {"text": "In order to have languages where the most probable string is not very short, we generated a set of random automata with a linear topology, only one initial state and one final state, and where transitions were added leading from each state to all previous states labelled by all the symbols of the vocabulary.", "labels": [], "entities": []}, {"text": "The probabilities on the edges and the final state were set assigning to them randomly (uniformly) distributed numbers in the range  In our experiments, the exact algorithm is systematically faster than the one that uses sampling.", "labels": [], "entities": []}, {"text": "Alternative settings which would be favourable to the randomized algorithm are still to be found.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Execution time of Algorithm 1 (sampling) and  Algorithm 2 (exact) for 4 state automata", "labels": [], "entities": [{"text": "Execution time", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9074339270591736}]}]}