{"title": [{"text": "IWCS is endorsed by SIGSEM, the ACL special interest group on computational semantics. iii Programme Chairs", "labels": [], "entities": [{"text": "IWCS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8189021348953247}]}], "abstractContent": [{"text": "This paper describes recent work on the DynDial project * towards incremental semantic interpretation in dialogue.", "labels": [], "entities": [{"text": "incremental semantic interpretation in dialogue", "start_pos": 66, "end_pos": 113, "type": "TASK", "confidence": 0.7697441816329956}]}, {"text": "We outline our domain-general grammar-based approach, using a variant of Dynamic Syntax integrated with Type Theory with Records and a Davidsonian event-based semantics.", "labels": [], "entities": []}, {"text": "We describe a Java-based implementation of the parser, used within the Jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.", "labels": [], "entities": [{"text": "Jindigo framework", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.8948846757411957}]}], "introductionContent": [{"text": "In the past few years, there has been a surge of interests in utilizing text mining techniques to provide in-depth bio-related information services.", "labels": [], "entities": [{"text": "text mining", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.7410491108894348}]}, {"text": "With an increasing number of publications reporting on protein-protein interactions (PPIs), much effort has been made in extracting information from biomedical articles using natural language processing (NLP) techniques.", "labels": [], "entities": []}, {"text": "Several shared tasks, such as LLL and BioCreative, have been arranged for the BioNLP community to compare different methodologies for biomedical information extraction.", "labels": [], "entities": [{"text": "biomedical information extraction", "start_pos": 134, "end_pos": 167, "type": "TASK", "confidence": 0.6272062957286835}]}, {"text": "Comparing to LLL and BioCreative which primarily focus on a simple representation of relations of bio-molecules, i.e. protein-protein interaction, the BioNLP'09 Shared Task involves the recognition of bio-molecular events in scientific abstracts, such as gene expression, transcription, protein catabolism, localization and binding, plus (positive or negative) regulation of proteins.", "labels": [], "entities": []}, {"text": "The task concerns the detailed behavior of bio-molecules, and can be used to support the development of biomedical-related databases.", "labels": [], "entities": []}, {"text": "In the BioNLP'09 shared task evaluation, the system constructed by UTurku achieved an F-score of 51.95% on the core task, the best results among all the participants.", "labels": [], "entities": [{"text": "UTurku", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.849557638168335}, {"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9994646906852722}]}, {"text": "In this paper, we describe a system, called HVS-BioEvent, which employs the hidden vector state model (HVS) to automatically extract biomedical events from biomedical literature.", "labels": [], "entities": []}, {"text": "The HVS model has been successfully employed to extract PPIs.", "labels": [], "entities": []}, {"text": "However, it is not straightforward to extend the usage of the HVS model for biomedical events extraction.", "labels": [], "entities": [{"text": "biomedical events extraction", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.7892117897669474}]}, {"text": "There are two main challenges.", "labels": [], "entities": []}, {"text": "First, comparing to the trigger words used for PPIs which are often expressed as single words or at most two words, the trigger words for biomedical event are more complex.", "labels": [], "entities": []}, {"text": "For example, controlled at transcriptional and post-transcriptional levels, spanning over 6 words, is considered as the trigger word for the regulation event.", "labels": [], "entities": []}, {"text": "In addition, the same word can be the trigger word for different types of biomedical events in different context.", "labels": [], "entities": []}, {"text": "Second, biomedical events consist of both simple events and complex events.", "labels": [], "entities": []}, {"text": "While simple events are more similar to PPIs which only involve binary or pairwise relations, complex events involve both n-ary (n > 2) and nested relations.", "labels": [], "entities": []}, {"text": "For example, a regulation event may take another event as its theme or cause which represents a structurally more complex relation.", "labels": [], "entities": []}, {"text": "Being able to handle both simple and complex events thus poses a huge challenge to the development of our HVS-BioEvent system.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the overall process of the HVSBioEvent system, which consists of three steps, trigger words identification, semantic parsing based on the HVS model, and biomedical events extraction from the HVS parse results.", "labels": [], "entities": [{"text": "trigger words identification", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.6097439825534821}, {"text": "semantic parsing", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.7102568000555038}, {"text": "biomedical events extraction", "start_pos": 172, "end_pos": 200, "type": "TASK", "confidence": 0.7455304066340128}, {"text": "HVS parse", "start_pos": 210, "end_pos": 219, "type": "TASK", "confidence": 0.70463427901268}]}, {"text": "Experimental results are discussed in section 3.", "labels": [], "entities": []}, {"text": "Finally, section 4 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested all possible combinations of enabling or disabling our three pathway feature vectors in computing pathway scores.", "labels": [], "entities": []}, {"text": "To measure how well a scoring function performs, we followed Wubben's approach of comparing ConceptNet shortest path conceptual relatedness to the Finkelstein-353 psychological dataset ].", "labels": [], "entities": []}, {"text": "This dataset contains 353 word pairs, each with a similarity score from 0 to 10 based on psychological free word association between two words.", "labels": [], "entities": []}, {"text": "Because ConceptNet does not contain nodes for all words used in the Finkelstein-353, we ignored word pairs that could not be found in ConceptNet.", "labels": [], "entities": []}, {"text": "Wubben calculated the correlation between the word pair rankings in the Finkelstein-353 and ConceptNet similarity (using simple edge count as pathway cost) to obtain a Pearson's correlation of .47].", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 168, "end_pos": 189, "type": "METRIC", "confidence": 0.8257770935694376}]}, {"text": "Studies using Wikipedia are able to obtain even higher correlations than .47, however we find this work incomparable to our study due to the wider coverage and different structure of Wikipedia [Strube and ].", "labels": [], "entities": []}, {"text": "Our experimental design differs from that of Wubben's on two fronts.", "labels": [], "entities": []}, {"text": "Wubben used bidirectional pathways, whereas our work only focuses on directed pathways for each pair in the dataset, ranking the directed pathways from word A to word B in the Finkelstein-353.", "labels": [], "entities": []}, {"text": "We chose this approach because psychological subjects were presented with word A first, followed byword B, in which we assume that subjects were more prone to make the directed connections.", "labels": [], "entities": []}, {"text": "Furthermore, because we are not using a simple edge-counting algorithm, we were notable to efficiently examine all of ConceptNet due to computational limitations.", "labels": [], "entities": []}, {"text": "Instead, we ran several cases in which we examine all pathways between concepts in the Finkelstein set within ConceptNet, subject to a maximum number of pathway nodes.", "labels": [], "entities": []}, {"text": "For example, if we only allow a maximum of three pathway nodes, then we only analyze the set of found pathways containing up to three nodes.", "labels": [], "entities": []}, {"text": "If a pathway is not found between two concepts within a three node pathway, but we know that the concept exists in ConceptNet, then we set the score equal to 0.", "labels": [], "entities": []}, {"text": "We were able to reach a maximum of six pathway nodes before calculations became infeasible.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the WordSimilarity-353 () test collection, which is a commonly used gold standard for the semantic relatedness task.", "labels": [], "entities": [{"text": "WordSimilarity-353 () test collection", "start_pos": 32, "end_pos": 69, "type": "DATASET", "confidence": 0.8596708476543427}, {"text": "semantic relatedness task", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.72203462322553}]}, {"text": "It provides average human judgments scores of the degree of relatedness for 353 word pairs.", "labels": [], "entities": []}, {"text": "The collection contains classically similar word pairs such as street -avenue and topically related pairs such as hotel -reservation.", "labels": [], "entities": []}, {"text": "However, no distinction was made while judging and the instruction was to rate the general degree of semantic relatedness.", "labels": [], "entities": [{"text": "judging", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9644774198532104}]}, {"text": "As a corpus we chose the British National Corpus (BNC) . It is one of the largest standardized English corpora and contains approximately 5.9 million sentences.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 25, "end_pos": 54, "type": "DATASET", "confidence": 0.9673250516255697}]}, {"text": "Choosing this text collection enables us to build a general purpose network that is not specifically created for the considered work pairs and ensures a realistic overall connectedness of the network as well as abroad coverage.", "labels": [], "entities": []}, {"text": "In this paper we created a network from 2 million sentences of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.964661180973053}]}, {"text": "It contains 27.5 million nodes out of which 635.000 are object nodes and the rest are relation and attribute nodes.", "labels": [], "entities": []}, {"text": "The building time including parsing was approximately 4 days.", "labels": [], "entities": [{"text": "parsing", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9808214902877808}]}, {"text": "Following the common practice in related work, we compared our scores to the human judgements using the Spearman rank-order correlation coefficient.", "labels": [], "entities": [{"text": "Spearman rank-order correlation coefficient", "start_pos": 104, "end_pos": 147, "type": "METRIC", "confidence": 0.6830973848700523}]}, {"text": "The results can be found in(a) with a comparison to previous results on the WordSimilarity-353 collection.", "labels": [], "entities": [{"text": "WordSimilarity-353 collection", "start_pos": 76, "end_pos": 105, "type": "DATASET", "confidence": 0.9774484038352966}]}, {"text": "Our first result overall word pairs is relatively low compared to the currently best performing systems.", "labels": [], "entities": []}, {"text": "However, we noticed that many poorly rated word pairs contained at least one word with low frequency.", "labels": [], "entities": []}, {"text": "Excluding these considerably improved the result to 0.50.", "labels": [], "entities": []}, {"text": "On this reduced set of word pairs our scores are in the region of approaches which make use of the Wikipedia category network, the WordNet taxonomic relations or Roget's thesaurus.", "labels": [], "entities": [{"text": "WordNet taxonomic relations", "start_pos": 131, "end_pos": 158, "type": "DATASET", "confidence": 0.9426755309104919}]}, {"text": "This is a promising result as it indicates that our approach based on automatically generated networks has the potential of competing with those using manually created resources if we increase the corpus size.", "labels": [], "entities": []}, {"text": "While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller -2 million sentences versus 1 million full Wikipedia articles () or 215MB versus 1.6 Terabyte ( . The extent to which corpus size influences our results is subject to further research.", "labels": [], "entities": []}, {"text": "We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following).", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 111, "end_pos": 122, "type": "DATASET", "confidence": 0.961800754070282}]}, {"text": "Taking the same low-frequency cut as above, we can see that our approach performs equally well on both sets.", "labels": [], "entities": []}, {"text": "This is remarkable as different methods tend to be more appropriate to calculate either one or the other ( ).", "labels": [], "entities": []}, {"text": "In particular, WordNet based measures are well known to be better suited to measure similarity than relatedness due to its hierarchical, taxonomic structure ( ).", "labels": [], "entities": []}, {"text": "The fact that our system achieves equal results on the subset indicates that it matches human judgement of semantic relatedness beyond specific types of relations.", "labels": [], "entities": []}, {"text": "This could be due to the associative structure of the network.", "labels": [], "entities": []}, {"text": "We have tested our approach on a set of fourteen challenge problems from and subsequent papers, designed to exercise the principal features of weighted abduction and show its utility for solving natural language interpretation problems.", "labels": [], "entities": [{"text": "natural language interpretation", "start_pos": 195, "end_pos": 226, "type": "TASK", "confidence": 0.6253045499324799}]}, {"text": "The knowledge bases used for each of these problems are sparse, consisting of only the axioms required for solving the problems plus a few distractors.", "labels": [], "entities": []}, {"text": "An example of a relatively simple problem is #5 in the table below, resolving \"he\" in the text I saw my doctor last week.", "labels": [], "entities": []}, {"text": "He told me to get more exercise.", "labels": [], "entities": []}, {"text": "where we are given a knowledge base that says a doctor is a person and a male person is a \"he\".", "labels": [], "entities": []}, {"text": "Solving the problem requires assuming the doctor is male.", "labels": [], "entities": []}, {"text": "The logical form fragment to prove is (\u2203 x)he(x), where we know doctor(D).", "labels": [], "entities": []}, {"text": "A problem of intermediate difficulty (#7) is resolving the three lexical ambiguities in the sentence The plane taxied to the terminal.", "labels": [], "entities": []}, {"text": "where we are given a knowledge base saying that airplanes and wood smoothers are planes, planes moving on the ground and people taking taxis are both described as \"taxiing\", and computer terminals and airport terminals are both terminals.", "labels": [], "entities": []}, {"text": "An example of a difficult problem is #12, finding the coherence relation, thereby resolving the pronoun \"they\", between the sentences The police prohibited the women from demonstrating.", "labels": [], "entities": []}, {"text": "The axioms specify relations between fearing, not wanting, and prohibiting, as well as the defeasible transitivity of causality and the fact that a causal relation between sentences makes the discourse coherent.", "labels": [], "entities": []}, {"text": "The weights in the axioms were mostly distributed evenly across the conjuncts in the antecedents and summed to 1.2.", "labels": [], "entities": []}, {"text": "For each of these problems, we compare the performance of the method described herewith a manually constructed gold standard and also with a method based on Kate and Mooney's (KM) approach.", "labels": [], "entities": []}, {"text": "In this method, weights were assigned to the reversed clauses based on the negative log of the sum of weights in the original clause.", "labels": [], "entities": []}, {"text": "This approach does not capture different weights for different antecedents of the same rule, and so has less fidelity to weighted abduction than our approach.", "labels": [], "entities": []}, {"text": "In each case, we used Alchemy's probabilistic inference to determine the most probable explanation (MPE).", "labels": [], "entities": [{"text": "probable explanation (MPE)", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.7150489330291748}]}, {"text": "In some of the problems the system should make more than one assumption, so there are 22 assumptions in total overall 14 problems in the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 137, "end_pos": 150, "type": "DATASET", "confidence": 0.9214203953742981}]}, {"text": "Using our method, 18 of the assumptions were found, while 15 were found using the KM method.", "labels": [], "entities": []}, {"text": "shows the number of correct assumptions found and the running time for the two approaches for each problem.", "labels": [], "entities": []}, {"text": "Our method in particular provides good coverage, with a recall of over 80% of the assumptions made in the gold standard.", "labels": [], "entities": [{"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9647335410118103}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9990590214729309}, {"text": "gold standard", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.9120238423347473}]}, {"text": "It has a shorter running time overall, approximately 5.3 seconds versus 8.7 seconds for the reversal method.", "labels": [], "entities": []}, {"text": "This is largely due to one problem in the test set, problem #9, where the running time for the KM method is relatively high because the technique finds a less sparse network, leading to larger cliques.", "labels": [], "entities": []}, {"text": "There were two problems in the test set that neither approach could solve.", "labels": [], "entities": []}, {"text": "One of these contains predicates that have a large number of arguments, leading to large clique sizes.", "labels": [], "entities": []}, {"text": "According to the SMH, sentences like (3-a) are preferably interpreted with their strong meaning in (3-b).", "labels": [], "entities": []}, {"text": "All/Most/Four of the dots are connected to each other.", "labels": [], "entities": []}, {"text": "where Q is ALL, MOST or F OUR.", "labels": [], "entities": [{"text": "ALL", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9954534769058228}, {"text": "MOST", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9953448176383972}, {"text": "F", "start_pos": 24, "end_pos": 25, "type": "METRIC", "confidence": 0.9826704859733582}, {"text": "OUR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8720345497131348}]}, {"text": "The PCH, on the other hand, predicts differences between the three quantifiers.", "labels": [], "entities": [{"text": "PCH", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8481944799423218}]}, {"text": "While the strong meaning of reciprocal all can be checked in polynomial time, verifying the strong interpretation of reciprocal most  The second experiment employed a picture verification task using clearly disambiguating pictures for strong vs. intermediate readings.", "labels": [], "entities": []}, {"text": "Unfortunately, the quantifiers we used in the last experiment are all upward monotone in their right argument and therefore their strong interpretation implies the intermediate reading.", "labels": [], "entities": []}, {"text": "Hence, even if the diagrams supporting the strong reading were judged to be true, we still wouldn't know which interpretation subjects had in mind.", "labels": [], "entities": []}, {"text": "Luckily, in sentences that contain nonmonotone quantifiers neither reading entails the other.", "labels": [], "entities": []}, {"text": "We therefore chose the quantifiers all but one, most and exactly n in (6).", "labels": [], "entities": []}, {"text": "All but one and exactly four are clearly non-monotone.", "labels": [], "entities": []}, {"text": "For most, if we take the implicature most, but not all into account, it is possible to construct strong pictures in away that the other readings are ruled out pragmatically.", "labels": [], "entities": []}, {"text": "Crucially, the strong reading of all but one is still PTIME computable, although it is more complex than all.", "labels": [], "entities": [{"text": "PTIME", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.5814245939254761}]}, {"text": "For instance, for verifying a model of size n, only then subsets of size n \u2212 1 have to be considered.", "labels": [], "entities": []}, {"text": "By contrast, verifying the strong meaning of (6-b,c) is intractable.", "labels": [], "entities": []}, {"text": "Sample diagrams are depicted in(a) and 1(b).", "labels": [], "entities": []}, {"text": "For strong pictures, the PCH predicts lower acceptance rates for (6-b,c) than for (6-a).", "labels": [], "entities": [{"text": "acceptance", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9825921058654785}]}, {"text": "In order to find out whether the strong readings of (6-b,c) are dispreferred or completely unavailable we also paired them with false control diagrams (see).", "labels": [], "entities": []}, {"text": "The wrong pictures differed from the strong ones in that a single line was removed from the completely connected subset.", "labels": [], "entities": []}, {"text": "If the strong reading is available for these two sentences at all, we expect more positive judgments following a strong diagram than following a false control.", "labels": [], "entities": []}, {"text": "Furthermore, we included ambigu- Secondly, as mentioned in the introduction we wanted to investigate whether availability of the strong reading in sentences with counting or proportional quantifiers depends on the size of the model.", "labels": [], "entities": []}, {"text": "The strong meaning of (6-b,c) maybe easy to verify in small universes, but not in larger ones.", "labels": [], "entities": []}, {"text": "To test this possibility we manipulated the number of dots.", "labels": [], "entities": []}, {"text": "Small models always contained four dots and large models six dots.", "labels": [], "entities": []}, {"text": "We chose small models only consisting of four dots because this is the smallest number for which the strong meaning can be distinguished from the intermediate interpretation, so we could be sure that the task would be doable at all . For the more complex six-dot pictures we presented sentences with exactly five instead of exactly three.", "labels": [], "entities": []}, {"text": "Example diagrams are given in   36 German native speakers (mean age 26.9 years; 23 female) read reciprocal quantified sentences on a computer screen in a self-paced fashion.", "labels": [], "entities": []}, {"text": "When they finished reading the sentence, it disappeared from the screen and a dot picture was presented for which a truth value judgment had to be provided within a time limit of 10s 6 . Participants received feedback about how fast they had responded.", "labels": [], "entities": []}, {"text": "This was done to trigger the first interpretation they had in mind.", "labels": [], "entities": []}, {"text": "We collected judgments and judgment times, but because of space limitations will only report the former.", "labels": [], "entities": []}, {"text": "The experiment started with a practice session of 10 trials, followed by the experiment with 138 trials in an individually randomized order.", "labels": [], "entities": []}, {"text": "An  p < .01).", "labels": [], "entities": []}, {"text": "This main effect was due to an across-the-board preference (7.3% on average) of ambiguous pictures to pictures disambiguating towards an intermediate interpretation.", "labels": [], "entities": []}, {"text": "Lower bound analyses: We computed a logit mixed effects model analysis including quantifier, truth (strong vs. wrong), complexity and their interactions as fixed effects and participants and items as random effects.", "labels": [], "entities": []}, {"text": "The only reliable effect was the fixed effect of quantifier (estimate = 3.31; z = 8.10; p < .01).", "labels": [], "entities": [{"text": "estimate", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9204220771789551}]}, {"text": "The effect of truth was marginal (estimate = 0.72; z = 1.77; p = .07).", "labels": [], "entities": [{"text": "estimate", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9788967967033386}]}, {"text": "As it turned Participants were very fast.", "labels": [], "entities": []}, {"text": "On average they spent 2.5s reading the sentence and 1.8s to provide a judgment.", "labels": [], "entities": []}, {"text": "out, a simpler model taking into account only these two main effects and the random effects accounted for the data with a comparable fit.", "labels": [], "entities": []}, {"text": "This was revealed by a comparison of the log-likelihood of the saturated and the simpler model (\u03c7 2 (8) = 12.36; p = .14).", "labels": [], "entities": []}, {"text": "Thus, complexity had no significant influence on the judgments.", "labels": [], "entities": []}, {"text": "The simple model revealed a significant main effect of truth (estimate = 0.67; z = 4.08; p < .01) which was due to 7.9% more 'true' judgments on average in the strong conditions than in the wrong conditions.", "labels": [], "entities": [{"text": "truth", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9395614862442017}, {"text": "estimate", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9175079464912415}]}, {"text": "The main effect of quantifier was also significant (most vs. all/exactly: estimate = 3.21; z = 15.10; p < .01).", "labels": [], "entities": []}, {"text": "This was due to more than 60% acceptance for all most conditions but much lower acceptance for the other two quantifiers.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9911734461784363}, {"text": "acceptance", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9929875731468201}]}, {"text": "We analyzed the data by computing separate logit mixed effect models with fixed effects of truth, complexity and their interaction for all three quantifiers and simplified the models when a fixed effect failed to contribute to model fit.", "labels": [], "entities": []}, {"text": "The best model for all but one contained only the fixed effect of truth which was reliable (estimate = 1.04; z = 3.47; p < .01), but neither complexity nor the interaction enhanced model fit (\u03c7 2 (2) = 1.04; p = .60).", "labels": [], "entities": []}, {"text": "Thus, independently of complexity strong pictures were more often accepted than wrong pictures.", "labels": [], "entities": []}, {"text": "The same held for most (fixed effect of truth: estimate = 0.98; z = 2.71; p < .01).", "labels": [], "entities": [{"text": "fixed effect of truth: estimate", "start_pos": 24, "end_pos": 55, "type": "METRIC", "confidence": 0.8522929847240448}]}, {"text": "Exactly n was different in that the fixed effect of truth and the interaction didn't matter (\u03c7 2 (2) = 2.68; p = .26), but complexity was significant (estimate = \u22120.97; z = \u22122.96; p < .01).", "labels": [], "entities": [{"text": "complexity", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.979961633682251}, {"text": "estimate", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9809064269065857}]}, {"text": "This effect was due to more errors in complex pictures than in simpler ones.", "labels": [], "entities": []}, {"text": "So far, we have been investigating reciprocal sentences with the upward monotone quantifiers all, most, four (Exp.", "labels": [], "entities": []}, {"text": "1) and the non-monotone quantifiers all but one and exactly n (Exp. 2).", "labels": [], "entities": []}, {"text": "As it looks, only all licenses a strong interpretation easily.", "labels": [], "entities": []}, {"text": "This finding may follow from the monotonicity plus implicatures.", "labels": [], "entities": []}, {"text": "According to's SMH strong readings are preferred in sentences with upward monotone quantificational antecedents.", "labels": [], "entities": [{"text": "SMH", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9404533505439758}]}, {"text": "For downward monotone quantifiers, on the other hand, intermediate readings should be preferred to strong readings.", "labels": [], "entities": []}, {"text": "The reverse preferences are triggered by opposite entailment patterns.", "labels": [], "entities": []}, {"text": "In the present experiment we compared upward monotone more than n with downward monotone antecedents fewer than n+2.", "labels": [], "entities": []}, {"text": "We paired diagrams like(f) vs.(e) with the two sentences in (7) according to a 2 (monotonicity) \u00d7 2 (truth) factorial design.", "labels": [], "entities": []}, {"text": "The diagrams of the first type were identical to the strong pictures of the last experiment.", "labels": [], "entities": []}, {"text": "With monotone increasing quantifiers they were ambiguous between strong and intermediate interpretations while in the monotone decreasing cases they disambiguated towards a strong interpretation.", "labels": [], "entities": []}, {"text": "The second type of pictures disambiguated towards weak readings in monotone increasing quantifiers, but were ambiguous for monotone decreasing quantificational antecedents.", "labels": [], "entities": []}, {"text": "On the basis of the first two experiments we expected high acceptance of both picture types with monotone increasing quantifiers, but much lower acceptance rates for (7-b) with strong than with ambiguous pictures.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9541783928871155}, {"text": "acceptance", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.970533549785614}]}, {"text": "We constructed six items and collected three judgments from each participant in each condition.", "labels": [], "entities": []}, {"text": "The experiment was run together with Experiment 2 using the same method.", "labels": [], "entities": []}, {"text": "Using a lemmatised and POS tagged version of the BNC, a list of adjacent AN pair candidates was extracted with simple regex-based queries targeting sequences composed of [Det/Art-A-N] (i.e. pairs expressing attributive modification of a nominal head like 'that little house').", "labels": [], "entities": [{"text": "BNC", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9499945044517517}]}, {"text": "In order to ensure the computational attainability of the successive steps, the candidate list was filtered by frequency (> 400) obtaining 1,367 different AN pairs.", "labels": [], "entities": []}, {"text": "A new version of the BNC was then prepared to represent the selected AN lemma pairs as a single token; for example, while in the original BNC the phrase [nice houses] consists in two separate POS-tagged lemmas, nice_AJ and house_NN, in the processed corpus it appears as a single entry nice_AJ_house_NN).", "labels": [], "entities": [{"text": "BNC", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8991823792457581}]}, {"text": "The corpus was also processed by stop-word removal (very high frequency items, mainly functional morphemes).", "labels": [], "entities": []}, {"text": "The re-tokenization process of the BNC enables us to extract independent context vectors for each AN pair in our list (v3) and their corresponding constituents (A and N, respectively v1 and v2), while ensuring that the extracted vectors do not contain overlapping information.", "labels": [], "entities": []}, {"text": "The same preprocessing steps were carried out to extract VN pair candidates.", "labels": [], "entities": [{"text": "VN pair candidates", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.8552600542704264}]}, {"text": "Sequences composed of [V-(Det/Art)-N] with an optional determiner were targeted and filtered by frequency (> 400), resulting in a first list of 545 VN pairs.", "labels": [], "entities": []}, {"text": "This list contained a large amount of noise due to lemmatisation and POS-tagging problems (e.g. housing association), and it also contained many very frequent lexicalized items (e.g. thank goodness).", "labels": [], "entities": []}, {"text": "The list was manually cleaned, resulting in 193 different VN pairs.", "labels": [], "entities": []}, {"text": "We also collected event durations from Amazon's Mechanical Turk (MTurk), an online marketplace from Amazon where requesters can find workers to solve Human Intelligence Tasks (HITs) for small amounts of money.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (MTurk)", "start_pos": 39, "end_pos": 71, "type": "DATASET", "confidence": 0.8428825054849897}]}, {"text": "Prior work has shown that human judgments from MTurk can often be as reliable as trained annotators () or subjects in controlled lab studies (, particularly when judgments are aggregated over many MTurk workers (\"Turkers\").", "labels": [], "entities": []}, {"text": "Our motivation for using Turkers is to better analyze system errors.", "labels": [], "entities": []}, {"text": "For example, if we give humans an event in isolation (no sentence context), how well can they guess the durations assigned by the Pan et. al. annotators?", "labels": [], "entities": []}, {"text": "This measures how big the gap is between a system that looks only at the event, and a system that integrates all available context.", "labels": [], "entities": []}, {"text": "To collect event durations from MTurk, we presented Turkers with an event from the TimeBank (a superset of the events annotated by ) and asked them to decide whether the event was most likely to take seconds, minutes, hours, days, weeks, months, years or decades.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8961173892021179}, {"text": "TimeBank", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.6174899935722351}]}, {"text": "We had events annotated in two different contexts: in isolation, where only the event itself was given (e.g., \"allocated\"), and in subject-object context, where a minimal phrase including the event and its subject and object was given (e.g., \"the mayor allocated funds\").", "labels": [], "entities": []}, {"text": "In both types of tasks, we asked 10 Turkers to label each event, and they were paid $0.0025 for each annotation ($0.05 fora block of 20 events).", "labels": [], "entities": []}, {"text": "To filter out obvious spammers, we added a test item randomly to each block, e.g., adding the event \"minutes\" and rejecting work from Turkers who labeled this anything other than the duration minutes.", "labels": [], "entities": []}, {"text": "The resulting annotations give duration distributions for each of our events.", "labels": [], "entities": [{"text": "duration", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9895488023757935}]}, {"text": "For example, when presented the event \"remodeling\", 1 Turker responded with days, 6 with weeks, 2 with months and 1 with years.", "labels": [], "entities": []}, {"text": "These annotations suggest that we generally expect \"remodeling\" to take weeks, but it may sometimes take more or less.", "labels": [], "entities": []}, {"text": "To produce a single fine-grained label from these distributions, we take the duration bin with the largest number of Turker annotations, e.g. for \"remodeling\", we would produce the label weeks.", "labels": [], "entities": [{"text": "duration", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9714150428771973}]}, {"text": "To produce a single coarse-grained label, we use the label less-than-a-day if the fine-grained label was seconds, minutes or hours and more-than-a-day otherwise.", "labels": [], "entities": []}, {"text": "As discussed in Section 3, we convert the minimum and maximum duration annotations into labels by converting each to seconds using ISO standards and calculating the arithmetic mean.", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 165, "end_pos": 180, "type": "METRIC", "confidence": 0.9407601952552795}]}, {"text": "If the mean is \u2264 86400 seconds, it is considered less-than-a-day for the coarse-grained task.", "labels": [], "entities": []}, {"text": "The fine-grained buckets are similarly calculated, e.g., X is labeled days if 86400 < X \u2264 604800.", "labels": [], "entities": []}, {"text": "The  evaluation does not include a decades bucket, but our system still uses \"decades\" in its queries.", "labels": [], "entities": []}, {"text": "We optimized all parameters of both the supervised and unsupervised systems on the training set, only running on test after selecting our best performing model.", "labels": [], "entities": []}, {"text": "We compare to the majority class as a baseline,: System accuracy compared against supervised and majority class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9310619831085205}]}, {"text": "* indicates statistical significance (McNemar's Test, two-tailed) against majority class at the p < 0.01 level, \u2020 at p < 0.05 tagging all events as more-than-a-day in the coarse-grained task and months in the fine-grained task.", "labels": [], "entities": [{"text": "McNemar's Test", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.5398511290550232}]}, {"text": "To evaluate our models, we use simple accuracy on the coarse-grained task, and approximate agreement matching as in  on the fine-grained task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9993658661842346}]}, {"text": "In this approximate agreement, a guess is considered correct if it chooses either the gold label or its immediate neighbor (e.g., hours is correct if minutes, hours or days is the gold class).", "labels": [], "entities": []}, {"text": "Pan et al. use this approach since human labeling agreement is low (44.4%) on the exact agreement fine-grained task., and our improved model with new features (Supervised, all).", "labels": [], "entities": []}, {"text": "The new model performs similarily to the Pan model on the in-domain Test set, but better on the out-of-domain financial news articles in the TestWSJ test.", "labels": [], "entities": [{"text": "Pan", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.778333306312561}, {"text": "TestWSJ test", "start_pos": 141, "end_pos": 153, "type": "DATASET", "confidence": 0.9566506445407867}]}, {"text": "On the latter, the new model improves over Pan et al. by 1.3% absolute on the coarse-grained task, and by 4.1% absolute on the fine-grained task.", "labels": [], "entities": []}, {"text": "We report results from the maximum entropy model as it slightly outperformed the naive bayes and support vector machine models .  The aim of this work is not only to produce an automatic quantification system, but also, if possible, to learn about the linguistic phenomena surrounding the underspecification of quantification.", "labels": [], "entities": []}, {"text": "Because of this, we choose a tree-based classifier which has the advantage of letting us seethe rules that are created by the system and thereby may allow us to make some linguistic observations with regard to the cooccurrence of certain quantification classes with certain grammatical constructions.", "labels": [], "entities": []}, {"text": "We use an off-theshelf implementation of the C4.5 classifier included in the Weka data mining software.", "labels": [], "entities": [{"text": "Weka data mining software", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.9322143644094467}]}, {"text": "We perform a 6-fold cross-validation on the gold standard and report class precision, recall and F-score.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8495298027992249}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9483222365379333}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9996902942657471}, {"text": "F-score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9969050288200378}]}, {"text": "We constructed two machine learning tasks to exploit the annotated spatial information to determine what contributions the information is making to narrative structure.", "labels": [], "entities": []}, {"text": "The first task evaluates the prediction of NARRATION and BACKGROUND/ ELABORATION relations based on pairs of spatial clauses.", "labels": [], "entities": [{"text": "BACKGROUND", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.8970283269882202}, {"text": "ELABORATION", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.5182773470878601}]}, {"text": "The second task evaluates the prediction of spatial information types, based on the other spatial information types in that clause, in individual clauses where the NARRATION relation holds.", "labels": [], "entities": []}, {"text": "For our experiments, we randomly select 167 synsets 3 from ImageNet, covering a wide range of concepts such as plants, mammals, fish, tools, vehicles etc.", "labels": [], "entities": []}, {"text": "We perform a simple pre-processing step using Tree Tagger (  and extract only the nouns.", "labels": [], "entities": []}, {"text": "Multiwords are explicitly recognized as collocations or named entities in the synset.", "labels": [], "entities": []}, {"text": "Not considering part-of-speech distinctions, the vocabulary for synset words is 352.", "labels": [], "entities": []}, {"text": "The vocabulary for gloss words is 777.", "labels": [], "entities": []}, {"text": "The shared vocabulary between them is 251.", "labels": [], "entities": []}, {"text": "There area total of 230,864 images associated with the 167 synsets, with an average of 1383 images per synset.", "labels": [], "entities": []}, {"text": "We randomly select an image for each synset, thus obtaining a set of 167 test images in total.", "labels": [], "entities": []}, {"text": "The technique explained in Section 3 is used to generate visual codewords for each image in this dataset.", "labels": [], "entities": []}, {"text": "Each image is first pre-processed to have a maximum side length of 300 pixels.", "labels": [], "entities": []}, {"text": "Next, SIFT descriptors are obtained by densely sampling the image on 20x20 overlapping patches spaced 10 pixels apart.", "labels": [], "entities": []}, {"text": "K-means clustering is applied on a random subset of 10 million SIFT descriptors to derive a visual vocabulary of 1,000 codewords.", "labels": [], "entities": []}, {"text": "Each descriptor is then quantized into a visual codeword by assigning it to the nearest cluster.", "labels": [], "entities": []}, {"text": "To create the gold-standard relatedness annotation, for each test image, six nouns are randomly selected from its associated synset and gloss words, and six other nouns are again randomly selected from the shared vocabulary words.", "labels": [], "entities": []}, {"text": "In all, we have 167 x 12 = 2004 word-image pairs as our test dataset.", "labels": [], "entities": []}, {"text": "Similar to previous word similarity evaluations, we ask human annotators to rate each pair on a scale of 0 to 10 to indicate their degree of semantic relatedness using the evaluation framework outlined below, with 0 being totally unrelated and 10 being perfectly synonymous with each other.", "labels": [], "entities": []}, {"text": "To ensure quality ratings, for each word-image pair we used 15 annotators from Amazon Mechanical: A sample of test images with their synset words and glosses : The number in parenthesis represents the numerical association of the word with the image (0-10).", "labels": [], "entities": [{"text": "Amazon Mechanical", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9565187096595764}]}, {"text": "Human annotations reveal different degree of semantic relatedness between the image and words in the synset or gloss.", "labels": [], "entities": []}, {"text": "Finally, the average of all 15 annotations for each word-image pair is taken as its gold-standard relatedness score . Note that only the pairs of images and words are provided to the annotators, and not their synsets and gloss definitions.", "labels": [], "entities": []}, {"text": "The set of standard criteria underlying the cross-modal similarity evaluation framework shown here is inspired by the semantic relations defined in Wordnet.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.9544396996498108}]}, {"text": "These criteria were provided to the human annotators, to help them decide whether a word and an image are related to each other.", "labels": [], "entities": []}, {"text": "(e.g. an image of troops at war vs the word \"gun\") ? Criterion (1) basically tests for synonym relation.", "labels": [], "entities": [{"text": "synonym relation", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.9362848103046417}]}, {"text": "Criteria (2) and (3) are modeled after the hyponymhypernym and meronym-holonym relations in WordNet, which are prevalent among nouns.", "labels": [], "entities": []}, {"text": "Note that none of the criteria is preemptive over the others.", "labels": [], "entities": []}, {"text": "Rather, we provide these criteria as guidelines in a subjective evaluation framework, similar to the word semantic similarity task in.", "labels": [], "entities": []}, {"text": "Importantly, criterion (4) models dissimilar but related concepts, or any other relation that indicates frequent association, while criterion (5) serves to provide additional distinction for pairs of words and images on a higher level of relatedness toward similarity.", "labels": [], "entities": []}, {"text": "In, we show sample images from our test dataset, along with the annotations provided by the human annotators.", "labels": [], "entities": []}, {"text": "Publicly available evaluation datasets as provided by and, are either quite small or follow a different annotation scheme.", "labels": [], "entities": []}, {"text": "Others consist of randomly sampled synsets, which do not properly represent the distribution of synsets in WordNet following specific properties.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9350218176841736}]}, {"text": "For example, the dataset used in (Ponzetto and Navigli, 2010) consists of only 2 sense pairs, whose lemmas are monosemous in WordNet and Wikipedia (e.g. the lemma specifier corresponds to one synset in WordNet and one article in Wikipedia).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9363341331481934}]}, {"text": "As this property holds for one-third of all WordNet noun synsets, it is crucial for the choice of the alignment method and thus, should be represented in the evaluation dataset adequately.", "labels": [], "entities": [{"text": "WordNet noun synsets", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.8723398049672445}]}, {"text": "Therefore, our goal in this paper is to compile a well-balanced dataset to cover different domains and properties.", "labels": [], "entities": []}, {"text": "Synsets can be characterized with respect to their so-called assigned Unique Beginner, their synset size, and their location within the WordNet taxonomy.", "labels": [], "entities": [{"text": "WordNet taxonomy", "start_pos": 136, "end_pos": 152, "type": "DATASET", "confidence": 0.9466058313846588}]}, {"text": "The Unique Beginners group synsets in semantically related fields: Inter-annotator agreement synsets for which more than one Wikipedia candidate article is returned.", "labels": [], "entities": []}, {"text": "In summary, for example, the synset <article, clause: a separate section of a legal document> has a synset size of 2, is assigned to the Unique Beginner communication, has a shortest path to the root element of length 6, and has 5 extracted Wikipedia candidate articles.", "labels": [], "entities": []}, {"text": "Based on these distinctive properties, we sampled 320 noun synsets yielding 1,815 sense pairs to be annotated, i.e. 5.7 Wikipedia articles per synset on average.", "labels": [], "entities": []}, {"text": "The exact proportion of synsets with respect to their properties is detailed in in the first four columns.", "labels": [], "entities": []}, {"text": "The manual sense alignment is performed by three human annotators.", "labels": [], "entities": [{"text": "manual sense alignment", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6340111196041107}]}, {"text": "The annotators were provided sense alignment candidate pairs, each consisting of a WordNet synset and a Wikipedia article.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9484308362007141}]}, {"text": "The annotation task was to label each sense pair either as alignment or not.", "labels": [], "entities": []}, {"text": "outlines the class distribution for three annotators and the majority decision.", "labels": [], "entities": []}, {"text": "The most sense alignment candidates were annotated as non-alignments; only between 210 and 244 sense pairs were considered as alignments (extracted for 320 WordNet synsets).", "labels": [], "entities": [{"text": "WordNet synsets", "start_pos": 156, "end_pos": 171, "type": "DATASET", "confidence": 0.9337531924247742}]}, {"text": "To assess the reliability of the annotators' decision, we computed the pairwise observed inter-annotator agreement A O and the chance-corrected agreement \u03ba (Artstein and Poesio, 2008) . The agreement values are shown in.", "labels": [], "entities": [{"text": "reliability", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9557234048843384}, {"text": "pairwise observed inter-annotator agreement A O", "start_pos": 71, "end_pos": 118, "type": "METRIC", "confidence": 0.654603918393453}]}, {"text": "The average observed agreement A O is 0.9721, while the multi-\u03ba is 0.8727 indicating high reliability.", "labels": [], "entities": [{"text": "agreement A O", "start_pos": 21, "end_pos": 34, "type": "METRIC", "confidence": 0.9094129602114359}, {"text": "reliability", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9666688442230225}]}, {"text": "The final dataset was compiled by means of a majority decision.", "labels": [], "entities": []}, {"text": "Given 1,815 sense alignment candidate pairs, 1,588 were annotated as non-alignments, while 227 were annotated as alignments.", "labels": [], "entities": []}, {"text": "215 synsets were aligned with one article, while 6 synsets were aligned with two articles.", "labels": [], "entities": []}, {"text": "Interesting to note is that the aligned samples are uniformly distributed among the different sampling dimensions as shown in (right column).", "labels": [], "entities": []}, {"text": "It demonstrates that WordNet synsets of different properties are contained in Wikipedia.", "labels": [], "entities": []}, {"text": "On the other side, 99 synsets, i.e. approx. 1/3 of the sampled synsets, could not be aligned.", "labels": [], "entities": []}, {"text": "Most of them are not contained in Wikipedia at all, e.g. the synset <dream (someone or something wonderful)> or <outside, exterior (the region that is outside of something)>.", "labels": [], "entities": []}, {"text": "Others are not explicitly encoded on the article level such as the synset <quatercentennial, quatercentenary (the 400th anniversary (or the celebration of it))>, which is part of the more general Wikipedia article <Anniversary>.", "labels": [], "entities": []}, {"text": "While more research on recognizing ENTAILMENT or CONTRADICTION between sentences pairs is necessary, it is important to recognize new relations that cannot be analysed in existing frameworks in order to provide Internet users with the information they need.", "labels": [], "entities": [{"text": "ENTAILMENT", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.7814342975616455}]}, {"text": "Thus, We assume that unrelated sentence pairs will be discarded before classification, in this experiment we focus only on the recognition of CONFINEMENT relations.", "labels": [], "entities": []}, {"text": "So our goal in this experiment is to classify between CONFINEMENT and NOT CONFINEMENT.", "labels": [], "entities": []}, {"text": "We will evaluate determining whether CONFINEMENT sentence pairs are Explicit or Implicit in future.", "labels": [], "entities": [{"text": "Explicit", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9774327278137207}, {"text": "Implicit", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8320332169532776}]}, {"text": "In our experiment, we used a gold data for structural alignment to evaluate semantic feature extraction.", "labels": [], "entities": [{"text": "structural alignment", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7369992733001709}, {"text": "semantic feature extraction", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.739823559919993}]}, {"text": "The amount of steroids or period of time that causes side effects differs from person to person.", "labels": [], "entities": []}, {"text": "We have evaluated our procedure on the RTE-2 dataset 8 , see . The RTE-2 dataset contains the development and the test set, both including 800 text-hypothesis pairs.", "labels": [], "entities": [{"text": "RTE-2 dataset 8", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9481170574824015}, {"text": "RTE-2 dataset", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9738475382328033}]}, {"text": "Each dataset consists of four subsets, which correspond to typical success and failure settings in different applications: information extraction (IE), information retrieval (IR), question answering (QA), and summarization (SUM).", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.8429369568824768}, {"text": "information retrieval (IR)", "start_pos": 152, "end_pos": 178, "type": "TASK", "confidence": 0.8339288711547852}, {"text": "question answering (QA)", "start_pos": 180, "end_pos": 203, "type": "TASK", "confidence": 0.8613820433616638}, {"text": "summarization (SUM)", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.825866162776947}]}, {"text": "In total, 200 pairs were collected for each application in each dataset.", "labels": [], "entities": []}, {"text": "As a baseline we have processed the datasets with an empty knowledge base.", "labels": [], "entities": []}, {"text": "Then we have done 2 runs, first, using axioms extracted from WordNet 3.0 plus FrameNet, and, second, using axioms extracted from the WordNet 2.0 definitions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9587123990058899}, {"text": "WordNet 2.0 definitions", "start_pos": 133, "end_pos": 156, "type": "DATASET", "confidence": 0.9196141759554545}]}, {"text": "In both runs the depth parameter was set to 3.", "labels": [], "entities": [{"text": "depth", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9844827055931091}]}, {"text": "The development set was used to train the threshold as described in the previous section.", "labels": [], "entities": []}, {"text": "9 contains results of our experiments.", "labels": [], "entities": []}, {"text": "Accuracy was calculated as the percentage of pairs correctly judged.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9970276951789856}]}, {"text": "The results suggest that the proposed method seems to be promising as compared to the other systems evaluated on the same task.", "labels": [], "entities": []}, {"text": "Our best run gives 63% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9998138546943665}]}, {"text": "Two systems participating the RTE-2 Challenge had 73% and 75% accuracy, two systems achieved 62% and 63%, while most of the systems achieved 55%-61%, cf..", "labels": [], "entities": [{"text": "RTE-2 Challenge", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.5380379855632782}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9995800852775574}]}, {"text": "For our best run (WN 3.0 + FN), we present the accuracy data for each application separately (table 2).", "labels": [], "entities": [{"text": "FN", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.7053351998329163}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.999345600605011}]}, {"text": "The distribution of the performance of Mini-TACITUS on the four datasets corresponds to the average performance of systems participating in RTE-2 as reported by . The most challenging task in RTE-2 appeared to be IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 213, "end_pos": 215, "type": "TASK", "confidence": 0.967303991317749}]}, {"text": "QA and IR follow, and finally, SUM was titled the \"easiest\" task, with a performance significantly higher than that of any other task.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.932597279548645}, {"text": "IR", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.7702494859695435}, {"text": "SUM", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9684712886810303}]}, {"text": "It is worth noting that the performance of Mini-TACITUS increases with the increasing time of processing.", "labels": [], "entities": []}, {"text": "We use the time parameter t for restricting the processing time.", "labels": [], "entities": []}, {"text": "The smaller t is, the fewer chances Mini-TACITUS has for applying all relevant axioms.", "labels": [], "entities": []}, {"text": "The experiments carried out suggest that optimizing the system computationally could lead to producing significantly better results.", "labels": [], "entities": []}, {"text": "Tracing the reasoning process, we found out that given along sentence and a short processing time Mini-TACITUS had time to construct only a few interpretations, and the real best interpretation was not always among them.", "labels": [], "entities": []}, {"text": "The lower performance of the system using the KB based on axioms extracted from extended WordNet can be easily explained.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9462112188339233}]}, {"text": "At the moment we define non-merge constraints (see section 2) for the input propositions only.", "labels": [], "entities": []}, {"text": "The axioms extracted from the synset definitions introduce a lot of new lexemes into the logical form, since these axioms define words with the help of other words rather than abstract concepts.", "labels": [], "entities": []}, {"text": "These new lexemes, especially those which are frequent in English, result in undesired mergings (e.g., mergings of frequent prepositions), since no non-merge constraints are defined for them.", "labels": [], "entities": []}, {"text": "In order to fix this problem, we will need to implement dynamic non-merge constraints which will be added on the fly if anew lexeme is introduced during reasoning.", "labels": [], "entities": []}, {"text": "The WN 3.0 + FN axiom set does not fall into this problem, because these axioms operate on frames and synsets rather than on lexemes.", "labels": [], "entities": []}, {"text": "In addition, for the run using axioms derived from FrameNet, we have evaluated how well we do in assigning frames and frame roles.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8925291299819946}]}, {"text": "For Mini-TACITUS, semantic role labeling is a by-product of constructing the best interpretation.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7006377577781677}]}, {"text": "But since this task is considered to be important as such in the NLP community, we provide an additional evaluation for it.", "labels": [], "entities": []}, {"text": "As a gold standard we have used the FrameAnnotated Corpus for Textual Entailment, FATE, see.", "labels": [], "entities": [{"text": "FrameAnnotated Corpus", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.8217842280864716}, {"text": "Textual Entailment", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.6284705996513367}, {"text": "FATE", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.5050833821296692}]}, {"text": "This corpus provides frame and semantic role label annotations for the RTE-2 challenge test set.", "labels": [], "entities": [{"text": "RTE-2 challenge test set", "start_pos": 71, "end_pos": 95, "type": "DATASET", "confidence": 0.7879286557435989}]}, {"text": "It is important to http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/ 9 Interpretation costs were normalized to the number of propositions in the input.", "labels": [], "entities": []}, {"text": "\"Time\" stands for the value of the time parameter -processing time per sentence, in minutes; \"Numb. of ax.\" stands for the average number of axioms per sentence.", "labels": [], "entities": [{"text": "Time", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.9772467613220215}, {"text": "Numb. of ax.", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.890251874923706}]}, {"text": "In order to get a better understanding of which parts of our KB are useful for computing entailment and for which types of entailment, in future, we are planning to use the detailed annotation of the RTE-2 dataset describing the source of the entailment which was produced by . We would like to thank one of our reviewers forgiving us this idea.", "labels": [], "entities": [{"text": "RTE-2 dataset", "start_pos": 200, "end_pos": 213, "type": "DATASET", "confidence": 0.9419308602809906}]}, {"text": "FATE was annotated with the FrameNet 1.3 labels, while we have been using 1.5 version for extracting axioms.", "labels": [], "entities": [{"text": "FATE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9497255086898804}, {"text": "FrameNet 1.3 labels", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9089456001917521}]}, {"text": "However,  note that FATE annotates only those frames which are relevant for computing entailment.", "labels": [], "entities": [{"text": "FATE", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.5878849625587463}]}, {"text": "Since Mini-TACITUS makes all possible frame assignments fora sentence, we provide only the recall measure for the frame match and leave the precision out.", "labels": [], "entities": [{"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9992196559906006}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9972645044326782}]}, {"text": "The FATE corpus was also used as a gold standard for evaluating the Shalmaneser system) which is a state-of-the-art system for assigning FrameNet frames and roles.", "labels": [], "entities": [{"text": "FATE corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9144507944583893}]}, {"text": "In table 2 we replicate results for Shalmaneser alone and Shalmaneser boosted with the WordNet Detour to FrameNet).", "labels": [], "entities": [{"text": "WordNet Detour", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.8996637761592865}]}, {"text": "The WN-FN Detour extended the frame labels assigned by Shalmaneser with the labels related via the FrameNet hierarchy or by the WordNet inheritance relation, cf..", "labels": [], "entities": []}, {"text": "In frame matching, the number of frame labels in the gold standard annotation that can also be found in the system annotation (recall) was counted.", "labels": [], "entities": [{"text": "frame matching", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.8141656517982483}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9933851361274719}]}, {"text": "Role matching was evaluated only on the frames that are correctly annotated by the system.", "labels": [], "entities": [{"text": "Role matching", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8677879869937897}]}, {"text": "The number of role labels in the gold standard annotation that can also be found in the system annotation (recall) as well as the number of role labels found by the system which also occur in the gold standard (precision) were counted.", "labels": [], "entities": [{"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9953083395957947}, {"text": "precision", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.9594500660896301}]}, {"text": "shows that given FrameNet axioms, the performance of Mini-TACITUS on semantic role labeling is compatible with those of the system specially designed to solve this task.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.6561514834562937}]}, {"text": "A wide variety of machine-learning techniques has been used for NLP tasks with various instantiations of feature sets and target class encodings.", "labels": [], "entities": []}, {"text": "For dialogue processing, it is still an open issue which techniques are the most suitable for which task.", "labels": [], "entities": [{"text": "dialogue processing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9616251587867737}]}, {"text": "We used two different types of classifiers to test their performance on our dialogue data: a probabilistic one and a rule inducer.", "labels": [], "entities": []}, {"text": "As a probabilistic classifier we used Bayes Nets.", "labels": [], "entities": []}, {"text": "This classifier estimates probabilities rather than produce predictions, which is often more useful because this allows us to rank predictions.", "labels": [], "entities": []}, {"text": "Bayes Nets estimate the conditional probability distribution on the values of the class attributes given the values of the other attributes.", "labels": [], "entities": []}, {"text": "As a rule induction algorithm we chose).", "labels": [], "entities": []}, {"text": "The advantage of a rule inducer is that the regularities discovered in the data are represented as human-readable rules.", "labels": [], "entities": []}, {"text": "The results of all experiments were obtained using 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "As a baseline it is common practice to use the majority class tag, but for our data sets such a baseline is not very useful because of the relatively low frequencies of the tags in some dimensions.", "labels": [], "entities": []}, {"text": "Instead, we use a baseline The A \u00af ugmented M \u00af ulti-party I \u00af nteraction meeting corpus consists of multimodal task-oriented human-human multi-party dialogues in English, for more information visit (http://www.amiproject.org/ 4 Difference between the time that a turn starts and the moment the previous turn ends.", "labels": [], "entities": []}, {"text": "These features were computed using the PRAAT tool . We examined both raw and normalized versions of these features.", "labels": [], "entities": [{"text": "PRAAT", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.5834240913391113}]}, {"text": "Speaker-normalized features were obtained by computing z-scores (z = (X-mean)/standard deviation) for the feature, where mean and standard deviation were calculated from all functional segments produced by the same speaker in the dialogues.", "labels": [], "entities": [{"text": "X-mean)/standard deviation)", "start_pos": 70, "end_pos": 97, "type": "METRIC", "confidence": 0.8144658088684082}]}, {"text": "We also used normalizations by first speaker turn and by previous speaker turn.", "labels": [], "entities": []}, {"text": "In order to reduce the effect of imbalances in the data, it is partitioned ten times.", "labels": [], "entities": []}, {"text": "Each time a different 10% of the data is used as test set and the remaining 90% as training set.", "labels": [], "entities": []}, {"text": "The procedure is repeated ten times so that in the end, every instance has been used exactly once for testing and the scores are averaged.", "labels": [], "entities": []}, {"text": "The cross-validation was stratified, i.e. the 10 folds contained approximately the same proportions of instances with relevant tags as in the entire dataset. that is based on a single feature, namely, the tag of the previous dialogue utterance (see)).", "labels": [], "entities": []}, {"text": "Several metrics have been proposed for the evaluation of a classifier's performance: error metrics and performance metrics.", "labels": [], "entities": []}, {"text": "The word-based error rate metric, introduced in, measures the percentage of words that were placed in a segment perfectly identical to that in the reference.", "labels": [], "entities": [{"text": "word-based error rate metric", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.7297981306910515}]}, {"text": "The dialogue act based metric (DER) was proposed in.", "labels": [], "entities": [{"text": "dialogue act based metric (DER)", "start_pos": 4, "end_pos": 35, "type": "METRIC", "confidence": 0.5396120165075574}]}, {"text": "In this metric a word is considered to be correctly classified if and only if it has been assigned the correct dialogue act type and it lies in exactly the same segment as the corresponding word of the reference.", "labels": [], "entities": []}, {"text": "We will use the combined DER sc error metric to evaluate joint segmentation (s) and classification (c): T okens with wrong boundaries and/or function class total number of tokens \u00d7 100 To assess the quality of classification results, the standard F-score metric is used, which represents the balance between precision and recall.", "labels": [], "entities": [{"text": "DER sc error metric", "start_pos": 25, "end_pos": 44, "type": "METRIC", "confidence": 0.8344147205352783}, {"text": "F-score metric", "start_pos": 247, "end_pos": 261, "type": "METRIC", "confidence": 0.9739123582839966}, {"text": "precision", "start_pos": 308, "end_pos": 317, "type": "METRIC", "confidence": 0.9993335604667664}, {"text": "recall", "start_pos": 322, "end_pos": 328, "type": "METRIC", "confidence": 0.9965156316757202}]}, {"text": "We started extracting WN taxonomy from the hypernym relations in the current version of WN (3.0), a network of 117,798 nouns grouped in 82,155 synsets.", "labels": [], "entities": []}, {"text": "We also extracted WN meronymy relations, i.e., 22,187 synset pairs, split into 12,293 \"member\", 9,097 \"part\" and 797 \"substance\", to constitute the first part-whole dataset.", "labels": [], "entities": []}, {"text": "In order to replicate our methodology, we also extracted 89 part-whole relation word pairs annotated with WN senses from the SemEval-2007 Task 4 datasets ( . We kept the positive examples from the training and test datasets, 3 excluding redundant pairs, and correcting a couple of errors.", "labels": [], "entities": [{"text": "SemEval-2007 Task 4 datasets", "start_pos": 125, "end_pos": 153, "type": "DATASET", "confidence": 0.7896725535392761}]}, {"text": "This data is also annotated with the meronymy sub-relations inspired from the classification of , but five subtypes instead of WN's three, although \"member-collection\" can safely be assumed to correspond to WN's \"member\" meronymy.", "labels": [], "entities": [{"text": "WN", "start_pos": 127, "end_pos": 129, "type": "DATASET", "confidence": 0.9099287986755371}, {"text": "WN", "start_pos": 207, "end_pos": 209, "type": "DATASET", "confidence": 0.9290842413902283}]}, {"text": "We will call this sub-relation Member, be it from WN or from SemEval.", "labels": [], "entities": []}, {"text": "We also tried to get similar datasets from the SemEval-2010 Task 8 but, not being annotated with WN senses, they are useless for our purposes.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.6106472512086233}]}, {"text": "illustrates a WN-extracted meronymy pair from our corpus , encoded in our own xml format.", "labels": [], "entities": []}, {"text": "Synsets are presented with the standard WN sense keys for each word, the recommended reference for stability from one WN release to another.", "labels": [], "entities": []}, {"text": "5 <pair relationOrder=\"(e1, e2)\" comment=\"meronym part\" source=\"WordNet-3.0\"> <e1 synset=\"head%1:06:04\" isInstance=\"No\">  Our first method of evaluation is against the M&L phrase similarity dataset (see section 4).", "labels": [], "entities": [{"text": "WordNet-3.0", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9441806674003601}, {"text": "M&L phrase similarity dataset", "start_pos": 168, "end_pos": 197, "type": "DATASET", "confidence": 0.7003636161486307}]}, {"text": "This consists of around 200 short phrase pairs rated by human subjects on a scale of 1-7 for their semantic similarity.", "labels": [], "entities": []}, {"text": "Each phrase is comprised of two words in the form verb-object, noun-noun, or adjectivenoun, extracted from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9548360705375671}]}, {"text": "The authors applied quite sophisticated heuristics based on phrase frequency and WordNet word similarity in an attempt to produce a set which exhibits an even spread of subjective similarities, from near-synonymy to near-total unrelatedness.", "labels": [], "entities": []}, {"text": "Their analysis of human ratings confirmed that they were reasonably successful in this.", "labels": [], "entities": []}, {"text": "shows the performance of the models upon the M&L dataset, in terms of Spearman's rank correlation.", "labels": [], "entities": [{"text": "M&L dataset", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.7117016464471817}, {"text": "Spearman's rank correlation", "start_pos": 70, "end_pos": 97, "type": "METRIC", "confidence": 0.6388107016682625}]}, {"text": "Two additional columns are included for reference: the inter-annotator agreement reported by M&L (which in this experiment serves as an upper-bound), calculated using leave-one-out sampling, and the results from the best-performing model reported by M&L for each phrase class.", "labels": [], "entities": []}, {"text": "Our additive distance-based model performs fairly competitively on this task, and is superior to all methods on verb-object combinations.", "labels": [], "entities": []}, {"text": "Interestingly, compositional expectation fairs relatively poorly, turning in a respectable performance only for noun-noun combinations and performing particularly poorly on verb-object combinations.", "labels": [], "entities": []}, {"text": "This last observation is at odds with the surprisingly good performance on verb sense disambiguation previously observed using expectation vectors, leading us to speculate whether this was rather a symptom of the distance-based approach used in that work (although it should be noted that there are many confounding differences separating the tasks and models in these works).", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7588753501574198}]}, {"text": "Unsurprisingly, the bigram measure performs very poorly across the board.", "labels": [], "entities": []}, {"text": "When interpreting these figures it is worth bearing in mind that, differences in our approaches to composition aside, unlike Mitchell & Lapata we are operating on unlemmatized data.", "labels": [], "entities": []}, {"text": "While the form of the M&L dataset makes it suitable for use as a Gold Standard, it does come with certain limitations.", "labels": [], "entities": [{"text": "M&L dataset", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.9009742736816406}]}, {"text": "Most notably, the phrases comprise only two words.", "labels": [], "entities": []}, {"text": "While this is a logical starting point for assessing compositional models, it gives little scope for testing the ability to capture structural aspects of composition (as M&L anyway restrict phrase pairs to identically structured phrase types, this point is all but moot).", "labels": [], "entities": []}, {"text": "Related to this is the fact that while the heuristics applied in generating the M&L dataset attempt to generate superficially different yet synonymous phrases (e.g \"reduce amount\", \"cut cost\"), there are very few cases of polysemy or superficial similarity (e.g. \"stout Russian\", \"Russian stout\" or \"arresting music\", \"arresting criminals\") which is an important confounding issue for compositional models.", "labels": [], "entities": [{"text": "M&L dataset", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.6741033121943474}]}, {"text": "In the next section we outline a complementary evaluation approach with which we attempt to address some of these issues.", "labels": [], "entities": []}, {"text": "A restricted register of about 300 noun, verb and adjective lemmas was selected with the aid of Wordnet and BNC frequency information.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9786544442176819}, {"text": "BNC", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.6641605496406555}]}, {"text": "Sentences were then automatically selected from the BNC with the constraint that each sentence was at least 3 words in length and a certain minimum proportion of its lemmas belonged to the restricted register.", "labels": [], "entities": [{"text": "BNC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.9040287137031555}]}, {"text": "This minimum proportion was tweaked such that the entire BNC generated approximately 1000 qualifying sentences.", "labels": [], "entities": [{"text": "BNC", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8440853357315063}]}, {"text": "The aim was to produce a manageably sized collection of real-world phrases wherein a range of similarities and similarity types (both semantic and superficial) existed between a proportion of the phrases.", "labels": [], "entities": []}, {"text": "In selecting the register, the purpose was therefore to find a compact set of words which exhibited both a high degree of ambiguity (polysemy) and interchangeability (synonymy).", "labels": [], "entities": []}, {"text": "Because words satisfying the former requirement tend to be very frequent (e.g. the auxiliary verbs), while those satisfying the latter tend to be very rare, this task was difficult.", "labels": [], "entities": []}, {"text": "An additional complication was that the types of phrase selected from the corpus were found to be highly sensitive to the specific words in the register, with certain words resulting in a disproportionate contingent of highly synonymous idiomatic phrases being selected (\"let's take the following\", \"consider the following\", \"look at the following\" etc), which was considered undesirable.", "labels": [], "entities": []}, {"text": "In the end a lot of judgement was exercised in selecting the register.", "labels": [], "entities": []}, {"text": "For each phrase in the dataset, the two most similar candidate phrases also in the dataset were identified according to each of our three similarity measures.", "labels": [], "entities": []}, {"text": "An additional two candidate phrases were selected at random to act as a control.", "labels": [], "entities": []}, {"text": "This resulted in at most eight candidate sentences for each source sentence, and less where different methods selected the same phrases.", "labels": [], "entities": []}, {"text": "Agreement between BIGRAM and VECTORBAG was 19.7% (which is both surprising and reassuring, considering the size of the dataset and how different these approaches are).", "labels": [], "entities": [{"text": "BIGRAM", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.8213030099868774}, {"text": "VECTORBAG", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.591057538986206}]}, {"text": "Agreement between these and the novel COMPEXP method was markedly less, at 12.7% and 9.1% respectively.", "labels": [], "entities": []}, {"text": "Agreement between the random control and each of the methods was in keeping with chance (<0.4%).", "labels": [], "entities": [{"text": "chance", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9821686148643494}]}, {"text": "To aid annotation, further steps were taken to reduce the size of the dataset and increase the proportion of subjectively similar phrases expressed.", "labels": [], "entities": []}, {"text": "For each method, the top candidate phrase attributed to each source phrase was ranked amongst those attributed to all source phrases (according to the actual similarity score attributed).", "labels": [], "entities": []}, {"text": "The source phrases were then ordered according to the minimum of these ranks, and the lower 50% were discarded.", "labels": [], "entities": []}, {"text": "This resulted in a set of 500 source sentences to which at least one of the methods had attributed a candidate sentence with relatively high confidence.", "labels": [], "entities": []}, {"text": "Agreement between methods after this step was 28.6%, 18.6% and 13.9% respectively (a uniform 50% increase).", "labels": [], "entities": []}, {"text": "English-speaking subjects were invited to participate in an annotation process via a website.", "labels": [], "entities": []}, {"text": "Upon visiting the site subjects were presented with a source sentence, and its set of \"similar\" candidate sentences as chosen by the four approaches, presented in a random order.", "labels": [], "entities": []}, {"text": "In cases where methods had agreed, fewer than eight sentences were displayed (i.e. there were no visible repetitions).", "labels": [], "entities": []}, {"text": "The annotators were asked to identify the two candidate sentences which were \"most similar in meaning\" to the source sentence, and to award an explicit first and second place accordingly.", "labels": [], "entities": []}, {"text": "Upon completing a question, participants progressed onto another selected at random from those having received the fewest annotations so far.", "labels": [], "entities": []}, {"text": "Participants were required to identify two sentences in every case, no matter how relevant they thought their meanings were in absolute terms, but were free to cease answering questions at any point.", "labels": [], "entities": []}, {"text": "No knowledge of the methods used to generate or select the sentences, or of the purpose of the study, was made available to the annotators.", "labels": [], "entities": []}, {"text": "Approximately 90 mostly native English speakers participated in the annotation process.", "labels": [], "entities": []}, {"text": "The number of questions answered by each annotator followed a roughly geometric distribution, with maximum, median and minimum of 266, 8 and 1 respectively.", "labels": [], "entities": []}, {"text": "The median time taken to answer each question was 24 seconds.", "labels": [], "entities": []}, {"text": "Average Kappa for random pairs of responses was 0.25 for annotators' first choices alone, and 0.39 when first and second choices are treated equally.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9904801249504089}]}, {"text": "As we are gathering psycholinguistic data, and not developing a gold standard fora supposed underlying objective classification, such moderate levels of agreement are not problematic.", "labels": [], "entities": []}, {"text": "What is important for our purposes is that, given the number of annotators involved, the observed levels of agreement are highly significant.", "labels": [], "entities": [{"text": "agreement", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9330899119377136}]}, {"text": "The distribution of agreement levels was more-or-less uniform, with a slight dip in the mid-range.", "labels": [], "entities": []}, {"text": "Interestingly there was negligible correlation between inter-annotator agreement and the average time taken to answer each question, indicating that seemingly \"hard\" questions did not take appreciably longer to answer than \"easier\" questions.", "labels": [], "entities": []}, {"text": "presents a summary the agreement between each of the phrasal similarity methods and the votes of the human annotators.", "labels": [], "entities": []}, {"text": "Results are separated into annotators' first choices only, and their combined first and second choices.", "labels": [], "entities": []}, {"text": "The figures in parentheses are the raw percentage of votes awarded to each method.", "labels": [], "entities": []}, {"text": "As there was some corroboration between the methods themselves, these total more than 100% across methods.", "labels": [], "entities": []}, {"text": "The figures outside of the parentheses are agreements expressed as a proportion of chance, taking any such corroboration into account.", "labels": [], "entities": []}, {"text": "There was only slight variation in the relative balance of scores when stratified according to annotator agreement: the random control unsurprisingly showed an increase at the lowest agreement levels, with BIGRAM and VECTORBAG increasing slightly with agreement, and COMPEXP peaking in the midrange.", "labels": [], "entities": [{"text": "BIGRAM", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9940407872200012}, {"text": "COMPEXP", "start_pos": 267, "end_pos": 274, "type": "METRIC", "confidence": 0.9685442447662354}]}, {"text": "Despite its ignorance of word order and context, the most successful method in this experiment is the bag of vectors.", "labels": [], "entities": []}, {"text": "Arguably more remarkable is the success of the relatively na\u00efve string similarity measure.", "labels": [], "entities": []}, {"text": "The fact that these methods also show a surprisingly high degree of agreement with each other (28.6%), suggests that a fair proportion of the phrase similarities present in our dataset can be adequately identified simply by the word forms that comprise them, without recourse to distributional information.", "labels": [], "entities": []}, {"text": "Our compositional expectation model is less successful overall, though still receiving several times as many votes as the random control.", "labels": [], "entities": []}, {"text": "The fact that its agreement with the two baselines is comparatively low would suggest that it is identifying a different kind of similarity.", "labels": [], "entities": []}, {"text": "Given that the mechanisms of compositional expectation are least understood, some kind of qualitative analysis may provide useful insight.", "labels": [], "entities": []}, {"text": "To this end, tables 3 and 4 show a selection of 10 phrases deemed most similar by the COMPEXP model, that were unanimously awarded first place by the human annotators or unanimously rejected respectively.", "labels": [], "entities": []}, {"text": "Rejected phrases are only shown for cases where there was a clear favourite phrase which had been selected by one of the competing models (also shown).", "labels": [], "entities": []}, {"text": "The examples were hand-picked for illustrative purposes from qualifying lists of two to three times the size.", "labels": [], "entities": []}, {"text": "The phrase pairs in table 3 can be broadly categorized in terms of their structural and semantic similarities.", "labels": [], "entities": []}, {"text": "While one can observe cases of phrases which have near-synonymous meanings in spite of markedly different wording or structure (B, D, F, G, I, J), there also seem to exist pairs which have essentially equivalent structures, yet are somewhat more loosely related in meaning (A, C, E, H).", "labels": [], "entities": []}, {"text": "Note that this is a very informal analysis and a lot of overlap between these classes can be acknowledged.", "labels": [], "entities": []}, {"text": "which exhibit similar structure relate more loosely in their subject matter (C, D, E, J); nonetheless, parts-of-speech and aspects of semantic category do seem to be largely preserved (dark-peaceful, room-river-beach, in-by, along-around slowly-vaguely, Isobel-Gaily) resulting in some cases in what might better be described as analogy than synonymy.", "labels": [], "entities": []}, {"text": "Where phrases do deviate in structure, their similarities are much less prescriptive; in many cases they seem to imply an almost rhetorical relationship (B, F, G, I).", "labels": [], "entities": []}, {"text": "These observations seem to indicate that compositional expectation is capable of capturing both structural and semantic aspects of similarity, with a leaning towards the former.", "labels": [], "entities": []}, {"text": "With some notable exceptions (E), the phrases chosen by the competing methods -and preferred by the annotators -tend to exhibit a more literal or topical relationship with the source phrase (in keeping with how these methods are formulated).", "labels": [], "entities": []}, {"text": "We should point out though that while the qualitative observations made here do seem to hold in some measure across the dataset, it is very difficult -and necessarily left to future work -to objectify them; it remains possible that the some of the patterns identified are due to chance and the limited set of phrases comprising our dataset.", "labels": [], "entities": []}, {"text": "The evaluation measure adopted in the RTE challenges is accuracy, i.e. the percentage of pairs correctly judged by a TE system.", "labels": [], "entities": [{"text": "RTE challenges", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.8284414112567902}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995552897453308}]}, {"text": "In the last RTE-5 and RTE-6 campaigns, participating groups were asked to run ablation tests, to evaluate the contribution of publicly available knowledge resources to the systems' performances.", "labels": [], "entities": [{"text": "RTE-5", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.7679826021194458}, {"text": "RTE-6", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.4661588668823242}]}, {"text": "Such ablation tests consist of removing one module at a time from a system, and rerunning the system on the test set with the other modules, except the one tested.", "labels": [], "entities": []}, {"text": "The results obtained were not satisfactory, since the impact of a certain resource on system performances is really dependent on how it is used by the system.", "labels": [], "entities": []}, {"text": "In some cases, resources like WordNet demonstrated to be very useful, while for other systems their contribution is limited or even damaging, as observed also in . To provide a more detailed evaluation of the capabilities of a TE system to address specific inference types, in we propose a methodology fora qualitative evaluation of TE systems, that takes advantage of the decomposition of T-H pairs into monothematic pairs (described in Section 3).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.949720025062561}]}, {"text": "The assumption is that the more a system is able to correctly solve the linguistic phenomena underlying the entailment relation separately, the more the system should be able to correctly judge more complex pairs, in which different phenomena are present and interact in a complex way.", "labels": [], "entities": []}, {"text": "According to such assumption, the higher the accuracy of a system on the monothematic pairs and the compositional strategy, the better its performances on the original RTE pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9990691542625427}]}, {"text": "The precision a system gains on single phenomena should be maintained over the general data set, thanks to suitable mechanisms of meaning combination.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9939504861831665}, {"text": "meaning combination", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7096430063247681}]}, {"text": "A number of quantitative and qualitative indicators about strength and weaknesses of TE systems result from the application of this methodology.", "labels": [], "entities": [{"text": "TE", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9387618899345398}]}, {"text": "Comparing the qualitative analysis obtained for two TE systems, the authors show that several systems' behaviors can be explained in terms of the correlation between the accuracy on monothematic pairs and the accuracy on the corresponding original pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.995496392250061}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9945425987243652}]}, {"text": "Ina component based framework, such analysis would allow a separate evaluation of TE modules, focusing on their ability to correctly address the inference types they are built to deal with.", "labels": [], "entities": []}, {"text": "We conducted an evaluation study to judge the \"goodness\" of the granularity model proposed.", "labels": [], "entities": []}, {"text": "In this study the annotators were asked to annotate granularity relations between two given paragraphs.", "labels": [], "entities": []}, {"text": "Paragraph-based analysis was preferred to event-word-based analysis because people reason much more easily with paragraph descriptions than with individual event mentions 2 . The annotation set consisted of paragraph pairs from three domains: travel articles (confluence.org), Timebank annotated data , and Wikipedia articles on games.", "labels": [], "entities": [{"text": "Timebank annotated data", "start_pos": 277, "end_pos": 300, "type": "DATASET", "confidence": 0.8864595293998718}]}, {"text": "We selected a total of 37 articles: 10 articles about travel, 10 about games, and 17 from Timebank.", "labels": [], "entities": [{"text": "Timebank", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9904168844223022}]}, {"text": "Both paragraphs of a given question were selected from the same article and referred to the same overall concept.", "labels": [], "entities": []}, {"text": "Restricting ourselves to the assessment-grounding configuration discussed above, we treat contextual polarity inference as a binary classification problem with two inputs: the INPUT EVENT event-level polarity (derived from the assessment clause) and the main verb of the grounding clause (henceforth FUNCTOR VERB).", "labels": [], "entities": [{"text": "contextual polarity inference", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.6225124299526215}, {"text": "INPUT EVENT", "start_pos": 176, "end_pos": 187, "type": "METRIC", "confidence": 0.864147812128067}, {"text": "FUNCTOR VERB)", "start_pos": 300, "end_pos": 313, "type": "METRIC", "confidence": 0.8439323902130127}]}, {"text": "The goal of the classifier is to correctly predict the polarity of the target NP (direct object to the functor verb) given these inputs.", "labels": [], "entities": []}, {"text": "As mentioned in \u00a72, we may categorize the functor verbs in our lexicon into preservers and reversers.", "labels": [], "entities": []}, {"text": "Two sources populate our lexicon.", "labels": [], "entities": []}, {"text": "First, positively subjective verbs from the MPQA subjectivity lexicon were marked as preservers and negatively subjective verbs were marked as reversers (1249 verbs total).", "labels": [], "entities": [{"text": "MPQA subjectivity lexicon", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.8841326634089152}]}, {"text": "For example, E dislike is a reverser.", "labels": [], "entities": [{"text": "E dislike", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.6688913702964783}]}, {"text": "Second, 487 verbs were culled from FrameNet () based on their membership in six entailment classes: verbs of injury, destruction, lacking, benefit, creation, and having.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9041604399681091}]}, {"text": "Class membership was determined by identifying 124 FrameNet frames aligning with one or more classes, then manually selecting from these frames verbs whose class membership was unambiguous.", "labels": [], "entities": []}, {"text": "Verbs of benefit, creation, and having were marked as preservers.", "labels": [], "entities": []}, {"text": "Verbs of injury, destruction, and lacking were marked as reversers.", "labels": [], "entities": [{"text": "lacking", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9833306670188904}]}, {"text": "Our system (SYS) classifies objects in context as follows: If the functor verb is a preserver, the target NP is assigned the same polarity as the input event polarity.", "labels": [], "entities": []}, {"text": "If the functor verb is a reverser, the target NP is assigned the opposite of the input event polarity.", "labels": [], "entities": []}, {"text": "This procedure is modulated by the presence of negation, as detected by a neg relation in the dependency parse.", "labels": [], "entities": []}, {"text": "Under negation, a preserver acts like a reverser, and vice versa.", "labels": [], "entities": []}, {"text": "We tested the performance of this system (SYS) on our annotated corpus against three baselines.", "labels": [], "entities": []}, {"text": "The first baseline (B-Functor) attempts to determine the importance of the input event to the calculation.", "labels": [], "entities": [{"text": "B-Functor", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9576489329338074}]}, {"text": "It thus ignores the preceding context, and attempts to classify the target object from the functor verb directly, based on the verb's polarity in the MPQA subjectivity lexicon.", "labels": [], "entities": [{"text": "MPQA subjectivity lexicon", "start_pos": 150, "end_pos": 175, "type": "DATASET", "confidence": 0.8775985240936279}]}, {"text": "It has poor precision and recall, reflecting both the importance of the assessment context for object polarity and the fact that the functor verbs are often not lexically sentiment bearing (e.g., predicates of possession).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9989057779312134}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9990338087081909}]}, {"text": "The second baseline (B-Input), conversely, ignores the functor verb and uses the input event polarity as listed in the MPQA lexicon (modulo negation) for object classification.", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.9536233246326447}, {"text": "object classification", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7322740703821182}]}, {"text": "The purpose of this baseline is to approximate a classifier that predicts target polarity solely from the global/contextual polarity of the preceding clause.", "labels": [], "entities": []}, {"text": "This has sharply increased precision, indicating contextual information's importance.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993252754211426}]}, {"text": "The third baseline (B-MLE) picked the majority object class (+), and had the highest precision, indicating the general bias in our corpus for positive objects.", "labels": [], "entities": [{"text": "B-MLE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9820038676261902}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9988341927528381}]}, {"text": "shows the performance (precision vs. recall) of our system compared to the three baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9990220069885254}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9875363707542419}]}, {"text": "Its precision is significantly higher, but its F-score is limited by the lower coverage of our manually constructed lexicon.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994813799858093}, {"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9987555742263794}]}, {"text": "SYS-MPQA and SYS-Frame show the performance of the system when the functor lexicon is limited to the MPQA and Framenet predicates, respectively.", "labels": [], "entities": []}, {"text": "Both are high precision sources of functor prediction, and pick out somewhat distinct predicates (given the recall gain of combining them).", "labels": [], "entities": [{"text": "functor prediction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7054903656244278}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9967987537384033}]}, {"text": "SYS+Maj and SYS+Sim are attempts to handle the low recall of SYS caused by functor verbs in the test data which aren't in the system's lexicon.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9986575841903687}]}, {"text": "SYS+Maj simply assigns these out-of-vocabulary verbs to the majority class: preservers.", "labels": [], "entities": [{"text": "SYS+Maj", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7551143169403076}]}, {"text": "SYS+Sim classifies out-of-vocabulary verbs as preservers or reversers based on their relative similarity to the known preservers and reversers selected from FrameNet -an unknown verb is categorized as a preserver if its average similarity to preservers is greater than its average similarity to reversers.", "labels": [], "entities": []}, {"text": "Similarity was determined according to the Jiang-Conrath distance measure, which based on links in WordNet . (Note: this process cannot occur for words not found in WordNet -e.g. misspellings -hence the less than perfect recall).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9862317442893982}, {"text": "recall", "start_pos": 221, "end_pos": 227, "type": "METRIC", "confidence": 0.8920987844467163}]}, {"text": "These two systems outperform all baselines, but have indistinguishable F-scores (if misspellings are excluded, SYS+Sim has a Recall of 0.99 and F-score of 0.91).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.991500973701477}, {"text": "Recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.998343825340271}, {"text": "F-score", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.9982060194015503}]}, {"text": "Most of the precision errors incurred by our systems were syntactic: incorrect parsing, incorrect extraction of the object, or faulty negation handling (e.g., negative quantifiers or verbs).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9967694282531738}]}, {"text": "26% of errors are due to word-sense disambiguation.", "labels": [], "entities": []}, {"text": "The verbs spoil and own each have positive and negative uses (own can mean defeat), but only one sense was registered in the lexicon, leading to errors.", "labels": [], "entities": []}, {"text": "The lion's share of these errors (22%) were due to the use of hate and similar expressions to convey jealousy (e.g. I was mad at him because he had both Boardwalk and Park Place).", "labels": [], "entities": [{"text": "errors", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9759467840194702}, {"text": "Boardwalk and Park Place", "start_pos": 153, "end_pos": 177, "type": "DATASET", "confidence": 0.7305664867162704}]}, {"text": "In these scenarios, although the assessment is negative, the event-level polarity of the grounding clause event type is positive (because it is desired), a fact which our current system cannot handle.", "labels": [], "entities": []}, {"text": "One way forward would be to apply WSD techniques to distinguish jealous from non-jealous uses of predicates of dislike.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9856348633766174}]}, {"text": "The collected responses of each AMT task were ranked separately by each of the above similarity and association measures.", "labels": [], "entities": [{"text": "AMT task", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.9131766855716705}]}, {"text": "We classify the ranked responses into \"keep\" (higher-scoring) and \"reject\" (lowerscoring) classes by defining a specific threshold for each list.", "labels": [], "entities": [{"text": "keep", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9658896923065186}]}, {"text": "Then we evaluated the accuracy of each filtration approach by computing their precision and recall on correct \"keep\" items (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9993789196014404}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9992600083351135}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9978582262992859}]}, {"text": "In this table the baseline score shows the accuracy of the responses of each AMT task before using automatic filtration techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9993823766708374}, {"text": "AMT task", "start_pos": 77, "end_pos": 85, "type": "TASK", "confidence": 0.9107231795787811}]}, {"text": "It should be added that collecting data by using AMT is rather cheap and fast, so we are more interested in higher precision (achieving highly accurate data) than higher recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9977741837501526}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9985190033912659}]}, {"text": "Lower recall means we lose some data, which is not too expensive to collect.", "labels": [], "entities": [{"text": "recall", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9996019005775452}]}, {"text": "As can be seen in table 2, within the object-location data set, we gained the best precision (0.7832) by using log-odds with relatively high recall (0.6690).", "labels": [], "entities": [{"text": "object-location data set", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.7353082299232483}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9990100860595703}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.998932421207428}]}, {"text": "Target-response pairs that were approved or rejected contrary to automatic predictions were due primarily to the specificity of the response location.", "labels": [], "entities": []}, {"text": "In the part-whole task, the best precision (0.9010) was achieved by using WN matrix similarities but again we lost a noticeable portion of data (recall= 0.2516).", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9991863369941711}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9989363551139832}]}, {"text": "Rejected target-response pairs from the higher-scoring part-whole set were often due to responses that named attributes, rather than parts, of the target item (e.g. croissant -flaky).", "labels": [], "entities": []}, {"text": "Many responses were too general (e.g, gong -material).", "labels": [], "entities": []}, {"text": "Many target-response pairs would have fallen under the next-to.r relation rather than any of the meronymic relations.", "labels": [], "entities": []}, {"text": "The majority of the approved target-response pairs from the lower-scoring part-whole set were due to obvious, \"common sense responses that would usually be inferred rather than explicitly stated, particularly body parts (e.g, bunny -brain).", "labels": [], "entities": []}, {"text": "The baseline accuracy of the nearby objects task is quite high (precision=0.8934, recall=1.0), and we gain the best precision by using WN average pairwise similarity (0.9855) by removing lower-scoring part of AMT responses (recall=0.3215).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9856491684913635}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9991325736045837}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9991814494132996}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9991538524627686}, {"text": "WN average pairwise similarity", "start_pos": 135, "end_pos": 165, "type": "METRIC", "confidence": 0.7012147083878517}, {"text": "AMT responses", "start_pos": 209, "end_pos": 222, "type": "TASK", "confidence": 0.7323857247829437}, {"text": "recall", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.9931715726852417}]}, {"text": "The high precision in all automatic techniques is due primarily to the fact that the open-ended nature of the task resulted in a large number of target-response pairs that, while not pertinent to the next-to.r relation, could be labeled by other relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9993520379066467}]}, {"text": "Again, the open-ended nature of the nearby objects task resulted in the lowest percentage of rejected high-scoring pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Formal labels for an example sentence.", "labels": [], "entities": []}, {"text": " Table 4: Datasets used in experiments.", "labels": [], "entities": []}, {"text": " Table 7: Per-word semantic accuracy after pruning label sets in Train-Null+AAlign (and testing with  Test-Null-AAlign).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9675207138061523}, {"text": "AAlign", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.956558108329773}]}, {"text": " Table 2: Performance on GEOQUERY.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8973916172981262}]}, {"text": " Table 3: Performance on CLANG.", "labels": [], "entities": []}, {"text": " Table 6: Number of relations in PropBank for the first 31,450 sentences.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9373509883880615}]}, {"text": " Table 7: Axioms used during evaluation, number of instances, accuracy and productivity. Results are reported", "labels": [], "entities": [{"text": "Axioms", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9964832067489624}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9996033310890198}]}, {"text": " Table 1: Performance on each problem in our test set, comparing two encodings of weighted abduction  into Markov logic networks and a gold standard.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy and error reduction of models using various features", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9949473142623901}]}, {"text": " Table 1: Pair extraction statistics", "labels": [], "entities": [{"text": "Pair extraction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9365529716014862}]}, {"text": " Table 2: Exp.1: Distribution of annotation categories (in percent)", "labels": [], "entities": []}, {"text": " Table 4: Exp. 1: Precision for the class \"yes\" (entailment) at 30% Recall", "labels": [], "entities": [{"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9967544674873352}, {"text": "Recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9846234917640686}]}, {"text": " Table 5: Exp. 2: Distribution of annotation categories on German corpora (in percent)", "labels": [], "entities": []}, {"text": " Table 6: Exp. 2: Predictors in the logreg model (*: p<0.05; **: p<0.01; ***: p<0.001)", "labels": [], "entities": []}, {"text": " Table 7: Exp. 2: Precision for the class \"yes\" at 30% recall", "labels": [], "entities": [{"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9975095987319946}]}, {"text": " Table 1: Adjective-Noun pairs. Mantel tests of correlation (max. correlation = 1)", "labels": [], "entities": [{"text": "correlation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9832488298416138}, {"text": "correlation", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.592163622379303}]}, {"text": " Table 2: Verb-Noun pairs. Mantel tests of correlation (max. correlation = 1)", "labels": [], "entities": [{"text": "correlation", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9875695109367371}, {"text": "correlation", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.5674893260002136}]}, {"text": " Table 3: AN pairs. Observation-based neighbour analysis (max. score = 367)", "labels": [], "entities": [{"text": "Observation-based neighbour analysis", "start_pos": 20, "end_pos": 56, "type": "METRIC", "confidence": 0.8478643496831259}]}, {"text": " Table 4: AN pairs. Prediction-based neighbour analysis (max. score = 367)", "labels": [], "entities": [{"text": "Prediction-based neighbour analysis", "start_pos": 20, "end_pos": 55, "type": "METRIC", "confidence": 0.8007937868436178}]}, {"text": " Table 6: AN pairs. Gold-standard comparison of shared neighbours (max. score = 7340)", "labels": [], "entities": []}, {"text": " Table 7: VN pairs. Gold-standard comparison of shared neighbours (max. score = 1200)", "labels": [], "entities": []}, {"text": " Table 1: Class distribution over 600 instances", "labels": [], "entities": [{"text": "Class distribution", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7797265350818634}]}, {"text": " Table 1: Relation and Spatial Clause Distribution", "labels": [], "entities": [{"text": "Relation", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.7630815505981445}]}, {"text": " Table 2: Spatial Rhetorical Relation Distribution per Corpus", "labels": [], "entities": [{"text": "Spatial Rhetorical Relation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8746136029561361}]}, {"text": " Table 3: Agreement and Kappa Statistics for Relation and Spatial Codings", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8546602129936218}, {"text": "Kappa", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9101840257644653}, {"text": "Relation and Spatial Codings", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.665319450199604}]}, {"text": " Table 4: Na\u00a8\u0131veNa\u00a8\u0131ve Bayes Classification Accuracy and F-Measures for Task 1", "labels": [], "entities": [{"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Bayes Classification", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.6278077960014343}, {"text": "Accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.5205867886543274}, {"text": "F-Measures", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9891831874847412}]}, {"text": " Table 5: Single and Combined Spatial Feature Performance", "labels": [], "entities": []}, {"text": " Table 6: K* Classification Accuracy and F-Measures for Task 2", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.701206386089325}, {"text": "F-Measures", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9977438449859619}]}, {"text": " Table 7: K* Classification Accuracy and F-Measures for Task 2 Boosted Vectors", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8405774235725403}, {"text": "F-Measures", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9979391694068909}]}, {"text": " Table 2: Correlation of automatically generated scores with human annotations on cross-modal semantic  relatedness, as performed on the ImageNet test dataset of 2004 pairs of word and image. Correlation  figures scoring the highest within a weighting scheme are marked in bold, while those scoring the highest  across weighting schemes and within a visual vocabulary size are underlined.", "labels": [], "entities": [{"text": "ImageNet test dataset of 2004 pairs", "start_pos": 137, "end_pos": 172, "type": "DATASET", "confidence": 0.937169204155604}]}, {"text": " Table 1: Sampling by properties and # manual alignments", "labels": [], "entities": [{"text": "Sampling", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9828631281852722}]}, {"text": " Table 2: Annotations per class", "labels": [], "entities": []}, {"text": " Table 4: Results for the automatic alignment", "labels": [], "entities": [{"text": "automatic alignment", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6350083947181702}]}, {"text": " Table 5: Agreement (\u03ba) between automatic and human annotators", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.981410562992096}]}, {"text": " Table 6: Confusion matrix (Setting: ppr + cos , SYN+HYPER, P+T+C)", "labels": [], "entities": [{"text": "HYPER", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.6889065504074097}]}, {"text": " Table 1: Semantic templates for recognizing CONFINEMENT", "labels": [], "entities": [{"text": "CONFINEMENT", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.3625393211841583}]}, {"text": " Table 2: Data set (Counts of sentences out of parenthesis and statements in parentheses)", "labels": [], "entities": []}, {"text": " Table 3: Results of recognizing confinement relations with our proposal system", "labels": [], "entities": []}, {"text": " Table 4: Instances of incorrect classification", "labels": [], "entities": []}, {"text": " Table 1: Statistics for extracted axioms", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results for the RTE-2 test set", "labels": [], "entities": [{"text": "RTE-2 test set", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8532456358273824}]}, {"text": " Table 3: Evaluation of frames/roles labeling towards FATE", "labels": [], "entities": [{"text": "FATE", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.4062778055667877}]}, {"text": " Table 1: Distribution of functional tags across dimensions and general-purpose functions for the AMI corpus (in", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.7851260900497437}]}, {"text": " Table 2: Overview of F-scores and DER sc for the baseline (BL) and the classifiers for joint segmentation and", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9580102562904358}, {"text": "DER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.988849937915802}, {"text": "joint segmentation", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.6387177407741547}]}, {"text": " Table 3: Overview of F-scores and DER sc for the baseline (BL) and the classifiers upon joint segmentation", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9665580987930298}, {"text": "DER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9833402037620544}]}, {"text": " Table 4: Overview of F-scores and DER sc of the global classifiers for the AMI data based on added previous", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.967070996761322}, {"text": "DER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.99725741147995}, {"text": "AMI data", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9055717885494232}]}, {"text": " Table 5: Overview of F-scores and DER sc of global classifiers for the AMI data per DIT ++ dimension.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9131407737731934}, {"text": "DER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9970910549163818}, {"text": "AMI data", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9109342694282532}]}, {"text": " Table 7: Most frequent basic ambiguities that break the 1btpd hypothesis in SemCor", "labels": [], "entities": [{"text": "SemCor", "start_pos": 77, "end_pos": 83, "type": "TASK", "confidence": 0.6085572838783264}]}, {"text": " Table 8: Logit mixed effects model for the response variable \"one-basic-type-per-discourse (1btpd) holds\"  (SemCor; random effects: discourse and lemma; significances: -: p > 0.05; ***: p < 0.001)", "labels": [], "entities": []}, {"text": " Table 1: Number of pairs extracted by the tests  Error Category Test  WordNet  SemEval  0  349 1.57% 0  0%  Semantic  4  550 4.47% 7 7.87%", "labels": [], "entities": [{"text": "WordNet  SemEval  0", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.6640490690867106}]}, {"text": " Table 1: Spearman's rank correlation between human and computational similarity ratings for M&L dataset.", "labels": [], "entities": []}, {"text": " Table 1: a) Unsmoothed lexicalized CKY parsers versus 10 semantic clusters. Evaluations were run  with EM trained to 10 iterations. b) Average dependence of parsing performance on number of semantic  clusters. Averages are taken over different random seeds, with EM running 4 or 10 iterations.", "labels": [], "entities": []}, {"text": " Table 1: The new dialogue taxonomy, with mappings to Li & Roth where applicable, and percentage  distribution in the Enron sample", "labels": [], "entities": []}, {"text": " Table 3: Examples of questions correctly classified", "labels": [], "entities": []}, {"text": " Table 4: Recall and precision by category", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9825316071510315}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9993625283241272}]}, {"text": " Table 3: Performance of systems and baselines  for contextual evaluativity classification", "labels": [], "entities": []}, {"text": " Table 1: Summary of AMT tasks, payments and the completion time", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.6203486323356628}, {"text": "AMT tasks", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.8523386418819427}]}, {"text": " Table 2: The accuracy of automatic filtering approaches", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995243549346924}, {"text": "automatic filtering", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6913410872220993}]}, {"text": " Table 1: Results: Pearson's correlation for varying formulas and improvement over the traditional edge- only approach. Row labels indicate which feature vectors were activated for the experiment.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 19, "end_pos": 40, "type": "METRIC", "confidence": 0.8477483789126078}]}, {"text": " Table 1: (a) Spearman ranking correlation coefficient results for our approach and comparison with  previous approaches. (b) Separate results for similarity and relatedness subset.", "labels": [], "entities": [{"text": "Spearman ranking correlation coefficient", "start_pos": 14, "end_pos": 54, "type": "METRIC", "confidence": 0.7131740972399712}]}, {"text": " Table 2: Experimental results based on 10 fold cross-validation.", "labels": [], "entities": []}, {"text": " Table 3: Per-class performance comparison in F-score (%) between HVS-BioEvent and UTurku.", "labels": [], "entities": [{"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9980534315109253}, {"text": "HVS-BioEvent", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9584868550300598}, {"text": "UTurku", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9357522130012512}]}]}