{"title": [{"text": "Improving MT Word Alignment Using Aligned Multi-Stage Parses", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9676785469055176}, {"text": "MT Word Alignment", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7864488561948141}]}], "abstractContent": [{"text": "We use hand-coded rules and graph-aligned logical dependencies to reorder English text towards Chinese word order.", "labels": [], "entities": []}, {"text": "We obtain a 1.5% higher F-score for Giza++ compared to running with unprocessed text.", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9995461106300354}]}, {"text": "We describe this research and its implications for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9964904189109802}]}], "introductionContent": [{"text": "Some statistical machine translation (SMT) systems use pattern-based rules acquired from linguistically processed bitexts.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 5, "end_pos": 42, "type": "TASK", "confidence": 0.780505046248436}]}, {"text": "They acquire these rules through the alignment of a parsed structure in one language with a raw string in the other language or the alignment of source/target language parse trees (.", "labels": [], "entities": []}, {"text": "This paper shows that machine translation (MT) can also benefit by aligning a \"deeper\" level of analysis than parsed text, which includes semantic role labeling, regularization of passives and wh constructions, etc.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8174802422523498}, {"text": "semantic role labeling", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.6189825038115183}, {"text": "regularization of passives and wh constructions", "start_pos": 162, "end_pos": 209, "type": "TASK", "confidence": 0.6588661472002665}]}, {"text": "We create GLARF representations () for English and Chinese sentences, in the form of directed acyclic graphs.", "labels": [], "entities": []}, {"text": "We describe two graph-based techniques for reordering English sentences to be closer to that of corresponding Chinese sentences.", "labels": [], "entities": []}, {"text": "One technique is based on manually created rules and the other is based on an automatic alignment of GLARF representations of Chinese/English sentences.", "labels": [], "entities": [{"text": "GLARF representations of Chinese/English sentences", "start_pos": 101, "end_pos": 151, "type": "TASK", "confidence": 0.4470863342285156}]}, {"text": "After reordering, we align words of the reordered English with the words of the Chinese, using the Giza++ word aligner).", "labels": [], "entities": []}, {"text": "For both techniques, the resulting alignment has a higher F-score than Giza++ on raw text (a 0.7% to 1.5% absolute improvement).", "labels": [], "entities": [{"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9995904564857483}]}, {"text": "In principle, our reordered text can be used to improve any Chinese/English SMT system for which Giza++ (or other word aligners) are part of the processing pipeline.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.739111602306366}]}, {"text": "These experiments area first step in using GLARF-style analyses for MT, potentially improving systems that already perform well with aligned text lacking large gaps in surface alignment.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9924628734588623}]}, {"text": "We hypothesize that SMT systems are most likely to benefit from deep analysis for structures where source and target language word order differs the most.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9905320405960083}]}, {"text": "We propose using deep analysis to reorder such structures in one language to more closely reflect the word order of the other language.", "labels": [], "entities": []}, {"text": "The text would be reordered at two stages in an SMT system: (1) prior to acquiring a translation model; and (2) either prior to translation (if source text is reordered) or after translation (if target text is reordered).", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9888027310371399}]}, {"text": "Our system moves large constituents (e.g., noun post-modifiers) to bring English word order closer to that of parallel Chinese sentences.", "labels": [], "entities": []}, {"text": "This improves word alignment and is likely to improve SMT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7842228412628174}, {"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9938868880271912}]}, {"text": "For this work we use two English/Chinese bitext corpora developed by the Linguistic Data Consortium (LDC): the Tides FBIS corpus and the GALE Y1 Q4 Chinese/English Word-Alignment corpus.", "labels": [], "entities": [{"text": "Tides FBIS corpus", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.8870978951454163}, {"text": "GALE Y1 Q4 Chinese/English Word-Alignment corpus", "start_pos": 137, "end_pos": 185, "type": "DATASET", "confidence": 0.6733701303601265}]}, {"text": "We used 2300 aligned sentences from FBIS for development purposes.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8673411011695862}]}, {"text": "We divided the GALE corpus into into a 3407 sentence development subcorpus (DEV) and a 1505 sentence test subcorpus (TEST).", "labels": [], "entities": [{"text": "GALE corpus", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.7855250835418701}, {"text": "TEST", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.7141999006271362}]}, {"text": "We used the LDC's manual alignments of the FBIS corpus to score these data.", "labels": [], "entities": [{"text": "LDC", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.9397688508033752}, {"text": "FBIS corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9184832870960236}]}], "datasetContent": [], "tableCaptions": []}