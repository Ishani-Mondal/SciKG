{"title": [], "abstractContent": [{"text": "We propose the use of a nonparametric Bayesian model, the Hierarchical Dirichlet Process (HDP), for the task of Word Sense Induction.", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.7702800830205282}]}, {"text": "Results are shown through comparison against Latent Dirich-let Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task.", "labels": [], "entities": [{"text": "Latent Dirich-let Allocation (LDA)", "start_pos": 45, "end_pos": 79, "type": "METRIC", "confidence": 0.8683878282705942}]}, {"text": "We find that the two models achieve similar levels of induction quality, while the HDP confers the advantage of automatically inducing a variable number of senses per word, as compared to manually fixing the number of senses a priori, as in LDA.", "labels": [], "entities": []}, {"text": "This flexibility allows for the model to adapt to terms with greater or lesser polysemy, when ev-idenced by corpus distributional statistics.", "labels": [], "entities": []}, {"text": "When trained on out-of-domain data, experimental results confirm the model's ability to make use of a restricted set of topically coherent induced senses, when then applied in a restricted domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Induction (WSI) is the task of automatically discovering latent senses for each word type, across a collection of that word's tokens situated in context.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7487225383520126}]}, {"text": "WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory.", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7434826493263245}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7763670335213343}]}, {"text": "This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning.", "labels": [], "entities": []}, {"text": "A large body of related work can be found in.", "labels": [], "entities": []}, {"text": "Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of.", "labels": [], "entities": [{"text": "B", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9899028539657593}, {"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 80, "end_pos": 113, "type": "METRIC", "confidence": 0.9193728069464365}, {"text": "WSI", "start_pos": 211, "end_pos": 214, "type": "TASK", "confidence": 0.846767246723175}]}, {"text": "A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types.", "labels": [], "entities": []}, {"text": "Nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (.", "labels": [], "entities": []}, {"text": "In this work we first independently verify the results of B&L, and then tackle the limitation on fixing the number of senses through the use of the Hierarchical Dirichlet Process (HDP) (), a nonparametric Bayesian model.", "labels": [], "entities": [{"text": "B", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.800876796245575}]}, {"text": "We show this approach leads to results of similar quality as LDA, when using a bag-of-words context model, in addition to allowing for variability in the number of senses across different words and domains.", "labels": [], "entities": []}, {"text": "When trained on a restricted domain corpus for which manually labeled sense data was present, we verify that the model maybe tuned to posit a similar number of senses as determined by human judges.", "labels": [], "entities": []}, {"text": "When trained on a broader domain collection, we show that the number of induced senses increase, inline with the intuition that a wider set of genres should lead to a greater diversity in underlying meanings.", "labels": [], "entities": []}, {"text": "Automatically inducing the proper number of senses has great practical implications, especially in areas that require word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.7033190826574961}]}, {"text": "For instance, inducing more senses for bank helps to tell differ-ent word senses apart for naturally more ambiguous words, and inducing less senses for job helps to prevent assigning too fined-grained senses in case the same words in two similar contexts are mistakenly regarded as carrying different senses.", "labels": [], "entities": []}, {"text": "As in prior work including B&L, we rely on the intuition that the senses of words are hinted at by their contextual information.", "labels": [], "entities": [{"text": "B&L", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.5763784646987915}]}, {"text": "From the perspective of a generative process, neighboring words of a target are generated by the target's underlying sense.", "labels": [], "entities": []}, {"text": "Both LDA and HDP define graphical models that generate collections of discrete data.", "labels": [], "entities": []}, {"text": "The sense of a target word is first drawn from a distribution and then the context of this word is generated according to that distribution.", "labels": [], "entities": []}, {"text": "But while LDA assumes a fixed, finite set of distributions, the HDP draws from an infinite set of distributions generated by a Dirichlet Process.", "labels": [], "entities": []}, {"text": "This section details the distinction.", "labels": [], "entities": []}, {"text": "shows the LDA model for word sense induction.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8141502141952515}]}, {"text": "The conventional notion of document is replaced by a pseudo-document, consisting of every word in an N m -word window centered on the target item.", "labels": [], "entities": []}, {"text": "w m,n is the n-th token of the m-th pseudodocument for target word w. s m,n is the corresponding sense for w m,n . Suppose there are K senses for the target word w, then the distribution over a context word w m,n is:", "labels": [], "entities": []}], "datasetContent": [{"text": "Model B&L experimented with variations to the LDA model that allowed for generating multiple layers of features, such as smaller (5w) and larger (10w) bag-of-word contexts, and syntactic features.", "labels": [], "entities": []}, {"text": "The additional complexity beyond the standard model led to only tenuous performance gains.", "labels": [], "entities": []}, {"text": "Normal LDA, when trained on pseudo-documents built from 10 words of surrounding context, performed only slightly below their best reported results.", "labels": [], "entities": []}, {"text": "Especially as our goal here was to investigate the sensespecification problem, rather than eking out further improvements in the base WSI evaluation measure, we chose to compare a standard LDA model to HDP, both strictly using a 10 word context.", "labels": [], "entities": []}, {"text": "Test Data Following B&L, we perform WSI on nouns.", "labels": [], "entities": [{"text": "B", "start_pos": 20, "end_pos": 21, "type": "METRIC", "confidence": 0.9610211849212646}, {"text": "WSI", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.944283127784729}]}, {"text": "The evaluation data comes from the WSI task of.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.5039186179637909}]}, {"text": "It is derived from the Wall Street Journal portion of the Penn TreeBank ( and contains 15,852 instances of excerpts on 35 nouns.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn TreeBank", "start_pos": 23, "end_pos": 71, "type": "DATASET", "confidence": 0.9443446844816208}]}, {"text": "All the nouns are hand-annotated with their OntoNotes senses (), with an average of 3.9 senses per word.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9240036606788635}]}, {"text": "Evaluation Method WSI is an unsupervised task that results in sense clusters with no explicit mapping to manually annotated sense data.", "labels": [], "entities": []}, {"text": "To derive such a mapping, we follow the supervised evaluation strategy of.", "labels": [], "entities": []}, {"text": "Annotated senses from SemEval-2007 are partitioned into a standard mapping set (72%), a dev set (14%) and a test set (14%).", "labels": [], "entities": []}, {"text": "After an WSI system has tagged the elements in the mapping set with their \"cluster IDs\", then a cluster to sense derivation is constructed by simply assigning to each cluster the manual sense label that has the highest in-cluster frequency.", "labels": [], "entities": []}, {"text": "Once such a mapping has been established, then results on the dev or test set are reported based on treating cluster assignment as a WSD operation.", "labels": [], "entities": []}, {"text": "Training Data As out-of-domain source, we extracted 930K instances of the 35 nouns from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 92, "end_pos": 121, "type": "DATASET", "confidence": 0.970074454943339}]}, {"text": "As in-domain source we extracted another 930K instances from WSJ in years 87/88/90/94.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9656886458396912}]}, {"text": "All pseudodocuments use the \u00b110 contextual window.", "labels": [], "entities": []}, {"text": "We trained the LDA and HDP models on the WSJ and BNC datasets separately.", "labels": [], "entities": [{"text": "WSJ and BNC datasets", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.778428703546524}]}, {"text": "In their experiments with LDA, B&L iteratively tried 3 up to 9 senses, and then reported the number that led to best results in evaluation (4 senses for WSJ, 8 for BNC).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.7246437668800354}, {"text": "BNC", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.5953354239463806}]}, {"text": "We repeated this approach for LDA, with hyperparameters \u03b1 = 0.02 and \u03b2 = 0.1.", "labels": [], "entities": []}, {"text": "For the HDP model, we tuned hyper-parameters on the SemEval-2007 dev set.", "labels": [], "entities": [{"text": "SemEval-2007 dev set", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.8348745505015055}]}, {"text": "See for results, averaged over 5 runs of LDA and 3 runs of HDP.", "labels": [], "entities": [{"text": "LDA", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.5320565700531006}]}, {"text": "We report several findings based on this experiment.", "labels": [], "entities": []}, {"text": "First, for the LDA models trained on WSJ and BNC, our F1 measures are 0.8% lower than reported by B&L.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9444114565849304}, {"text": "BNC", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8358242511749268}, {"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9996028542518616}, {"text": "B&L", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.9481411178906759}]}, {"text": "Second, based on our own experiment, the HDP model performance is slightly better than that of LDA when training with BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.8306449055671692}]}, {"text": "Third, the HDP model appears to better adapt to data in other domains.", "labels": [], "entities": []}, {"text": "When switching the training set from WSJ (in-domain) to BNC (out-of-domain), we, along with B&L, found a 2.3% drop with LDA models.", "labels": [], "entities": [{"text": "BNC", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9634407162666321}, {"text": "B", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.7763211727142334}]}, {"text": "However, with the HDP model, there is only a 1% drop in F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9986948370933533}]}, {"text": "Moreover, even trained on out-ofdomain data, HDP can still better infer the number of senses from the test data, which is illustrated next.", "labels": [], "entities": []}, {"text": "shows the number of senses induced from each dataset.", "labels": [], "entities": []}, {"text": "When training on WSJ and test on SemEval-2007, HDP induced the correct number of senses (3.9 on average) from test, while LDA did this by assuming 4 senses from the training data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.9820504188537598}, {"text": "SemEval-2007", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.8618408441543579}]}, {"text": "When there is a domain mismatch between training (BNC) and test, which comes from the 1989 WSJ), the LDA model preferred far more than the annotated number of senses (7.4 vs. 3.9), largely due to the fact that it assumed 8 senses during training.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.8078295588493347}]}, {"text": "However, even though the HDP model induced more senses (9.4) when training on the broader coverage BNC set, it still inferred a much reduced average of 4.6 senses on test.", "labels": [], "entities": [{"text": "BNC set", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.8009975850582123}]}, {"text": "The BNC, being a balanced corpus, covers more diverse genres than the WSJ: we would expect it to lead to a more inclusive model of word sense.", "labels": [], "entities": [{"text": "BNC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8509250283241272}, {"text": "WSJ", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9381451606750488}, {"text": "word sense", "start_pos": 131, "end_pos": 141, "type": "TASK", "confidence": 0.7144389003515244}]}, {"text": "illustrates this comparison through the difference between sense numbers.", "labels": [], "entities": []}, {"text": "For the 35 humanannotated nouns, HDP induced the number of senses mostly within an error of \u00b12, whereas LDA tended to prefer 3 \u2212 6 more senses than recognized by an-  notators (on average the HDP model was off by 1.6 senses, as compared to 3.6 by LDA).", "labels": [], "entities": []}, {"text": "Finally, the F1 performance of HDP is 1.9% better than LDA (85.7% vs. 83.8%).", "labels": [], "entities": [{"text": "F1", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9997557997703552}, {"text": "LDA", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.7174667119979858}]}, {"text": "We further evaluated the LDA model by training separately for each of the 35 nouns, first setting as the number of topics the amount induced by HDP (on average, 5.8/9.4 senses for WSJ/BNC), then using the number of senses as used by the human annotators in SemEval-2007 (an average of 3.8).", "labels": [], "entities": [{"text": "WSJ/BNC", "start_pos": 180, "end_pos": 187, "type": "DATASET", "confidence": 0.821140726407369}]}, {"text": "As seen in, in each of these cases HDP remained the superior model.", "labels": [], "entities": [{"text": "HDP", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8013025522232056}]}], "tableCaptions": [{"text": " Table 1: F-measure when training with WSJ (in-domain) and", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9977307915687561}, {"text": "WSJ", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.845710039138794}]}, {"text": " Table 2: The average number of senses the LDA and HDP", "labels": [], "entities": []}, {"text": " Table 3: F1 measure when training LDA with three other set-", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9984830021858215}, {"text": "measure", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.5329561233520508}]}]}