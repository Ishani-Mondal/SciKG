{"title": [{"text": "An N-gram frequency database reference to handle MWE extraction in NLP applications", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.99442058801651}]}], "abstractContent": [{"text": "The identification and extraction of Multiword Expressions (MWEs) currently deliver satisfactory results.", "labels": [], "entities": [{"text": "identification and extraction of Multiword Expressions (MWEs", "start_pos": 4, "end_pos": 64, "type": "TASK", "confidence": 0.6108571775257587}]}, {"text": "However, the integration of these results into a wider application remains an issue.", "labels": [], "entities": []}, {"text": "This is mainly due to the fact that the association measures (AMs) used to detect MWEs require a critical amount of data and that the MWE dictionaries cannot account for all the lexical and syntactic variations inherent in MWEs.", "labels": [], "entities": []}, {"text": "In this study, we use an alternative technique to overcome these limitations.", "labels": [], "entities": []}, {"text": "It consists in defining an n-gram frequency database that can be used to compute AMs on-the-fly, allowing the extraction procedure to efficiently process all the MWEs in a text, even if they have not been previously observed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword Expressions (MWEs) are commonly defined as \"recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages\".", "labels": [], "entities": [{"text": "Multiword Expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8463984251022338}]}, {"text": "Their importance in the field of natural language processing (NLP) is undeniable.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.8169032235940298}]}, {"text": "Although composed of several words, these sequences are nonetheless considered as simple units with regard to part-of-speech at the lexical as well as syntactic levels.", "labels": [], "entities": []}, {"text": "Their identification is therefore essential to the efficiency of applications such as parsing (), machine translation (), information extraction, or information retrieval (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9768710732460022}, {"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7925346195697784}, {"text": "information extraction", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.863099217414856}, {"text": "information retrieval", "start_pos": 149, "end_pos": 170, "type": "TASK", "confidence": 0.8065535426139832}]}, {"text": "In these systems, the principle of syntactic or semantic/informational unit is particularly important.", "labels": [], "entities": []}, {"text": "Although the identification and extraction of MWEs now deliver satisfactory results, their integration into a broader applicative context remains problematic ().", "labels": [], "entities": [{"text": "identification and extraction of MWEs", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.6978756189346313}]}, {"text": "The explanations for this situation are twofold.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most evaluations of MWE extraction systems are based on human judgments and restrict the validation process to the n-best candidates.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.9819485247135162}]}, {"text": "Inevitably partial, this method is unable to estimate performance in terms of recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9977189898490906}]}, {"text": "To overcome these limitations, we use the evaluation method described by.", "labels": [], "entities": []}, {"text": "They propose an automatic method that consists in computing both recall and precision using various n-best samples.", "labels": [], "entities": [{"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9989079236984253}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9938521385192871}]}, {"text": "It involves the formation of a golden standard (i.e. a list of MWEs manually identified in a corpus) and a sorted list of MWEs extracted automatically by applying AM on the same corpus.", "labels": [], "entities": []}, {"text": "The recall and precision rates are therefore calculated by comparing the n-best (where n increases from 0 till n in steps of x) to the golden 5.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9994357228279114}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9989438652992249}]}, {"text": "For the purposes of comparison, we also limited the size of the n-grams indexed in Le Soir to 5 words.", "labels": [], "entities": [{"text": "Le Soir", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.8351844847202301}]}, {"text": "6. In order to model a contemporary language, we only kept the frequencies observed in texts written between standard list 7 .  The estimation of the parameters allowed us to establish a specific evaluation framework.", "labels": [], "entities": []}, {"text": "Two sets of parameters were defined depending on whether they apply to Google (ATS + HDV) or to the references built from Le Soir (ATS + LEMMA).", "labels": [], "entities": [{"text": "Google (ATS + HDV", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.804998254776001}, {"text": "Le Soir (ATS + LEMMA)", "start_pos": 122, "end_pos": 143, "type": "DATASET", "confidence": 0.7083481124469212}]}, {"text": "From a practical standpoint, we limited the MWE extraction to nominal units of size inferior to five in order to meet the characteristics of our test corpus (the annotations of which are limited to nominal sequences), on the one hand, and to allow comparability of results on the other hand (the n-grams from Google do not exceed the order 5).", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9648326337337494}]}, {"text": "Initially, we considered the extraction of MWEs in the whole evaluation corpus.", "labels": [], "entities": [{"text": "extraction of MWEs", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.6696440180142721}]}, {"text": "Results displayed in provide an advantage over the use of a reference with respect to the extraction carried out on the test corpus only.", "labels": [], "entities": []}, {"text": "In addition, we see a clear improvement in performance with respect to that obtainable with a dictionary of MWEs.", "labels": [], "entities": []}, {"text": "11. References constructed on the basis of the newspaper Le Soir have been reindexed from a lemmatized text.", "labels": [], "entities": [{"text": "newspaper Le Soir", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.6771379510561625}]}, {"text": "12. The MWE dictionary used in this experiment was ini-  Ina second step, we wanted to test the efficiency of our references in the more adverse context of a short text.", "labels": [], "entities": [{"text": "MWE dictionary", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8825312852859497}]}, {"text": "We randomly selected 3K words of our test corpus to simulate a short text while maintaining a sufficient number of MWEs (i.e. 151 nominal MWEs).", "labels": [], "entities": []}, {"text": "Results shown in further confirm our first experience and validate our concept of a reference in areal application context.", "labels": [], "entities": []}, {"text": "Beyond validating the use of a frequency base, these results also confirm the general idea that the size of the corpus used for the reference matters.", "labels": [], "entities": []}, {"text": "The differences between the references of 500K, 1000K and 5000K words showed a continuous improvement both in precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9994552731513977}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9958985447883606}]}, {"text": "The results obtained with the Google reference are more surprising, since they do not meet that growing trend.", "labels": [], "entities": [{"text": "Google reference", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8926900625228882}]}, {"text": "However, given the number of errors that those n-grams contain (mainly due to the OCR-ization and tokenitially derived from the corpus of 5000K words used to build the corresponding reference.", "labels": [], "entities": []}], "tableCaptions": []}