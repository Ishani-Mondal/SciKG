{"title": [{"text": "The Effect of Automatic Tokenization, Vocalization, Stemming, and POS Tagging on Arabic Dependency Parsing", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.671826034784317}, {"text": "Arabic Dependency Parsing", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.5545516908168793}]}], "abstractContent": [{"text": "We use an automatic pipeline of word tokenization, stemming, POS tagging, and vocalization to perform real-world Arabic dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.8095761239528656}, {"text": "Arabic dependency parsing", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.6171474854151408}]}, {"text": "In spite of the high accuracy on the modules, the very few errors in tokenization, which reaches an accuracy of 99.34%, lead to a drop of more than 10% in parsing, indicating that no high quality dependency parsing of Arabic, and possibly other morphologically rich languages, can be reached without (semi-)perfect tokenization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9981096982955933}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9981715679168701}, {"text": "parsing", "start_pos": 155, "end_pos": 162, "type": "TASK", "confidence": 0.979022204875946}]}, {"text": "The other module components, stemming, vocalization, and part of speech tagging, do not have the same profound effect on the dependency parsing process.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7264041006565094}, {"text": "dependency parsing", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.8273873329162598}]}], "introductionContent": [{"text": "Arabic is a morphologically rich language in which words maybe composed of several tokens and hold several syntactic relations.", "labels": [], "entities": []}, {"text": "We define word to be a whitespace delimited unit and token to be (part of) a word that has a syntactic function.", "labels": [], "entities": []}, {"text": "For example, the word wsytzwjhA (\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59)(English: And he will marry her) consists of 4 tokens: a conjunction w, a future marker s, a verb inflected for the singular masculine in the perfective form ytzwj, and a feminine singular 3 rd person object pronoun.", "labels": [], "entities": []}, {"text": "Parsing such a word requires tokenization, and performing dependency parsing in the tradition of the CoNLL-X () and) also requires part of speech tagging, lemmatization, linguistic features, and vocalization, all of which were in the human annotated gold standard form in the shared task.", "labels": [], "entities": [{"text": "Parsing such a word", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8586770594120026}, {"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.709342435002327}, {"text": "CoNLL-X", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.8802763223648071}, {"text": "speech tagging", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.7098667621612549}]}, {"text": "The current study aims at measuring the effect of a pipeline of non gold standard tokenization, lemmatization, vocalization, linguistic features and POS tagging on the quality of Arabic dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.732842892408371}, {"text": "Arabic dependency parsing", "start_pos": 179, "end_pos": 204, "type": "TASK", "confidence": 0.5852208236853281}]}, {"text": "We only assume that we have gold standard sentence boundaries since we do not agree with the sentence boundaries in the data, and introducing our own will have a complicating effect on evaluation.", "labels": [], "entities": []}, {"text": "The CoNLL shared tasks of 2006 and 2007 used gold standard components in all fields, which is not realistic for Arabic, or for any other language.", "labels": [], "entities": []}, {"text": "For Arabic and other morphologically rich languages, it maybe more unrealistic than it is for English, for example, since the CoNLL 2007 Arabic dataset has tokens, rather than white space delimited words, as entries.", "labels": [], "entities": [{"text": "CoNLL 2007 Arabic dataset", "start_pos": 126, "end_pos": 151, "type": "DATASET", "confidence": 0.9662395119667053}]}, {"text": "A single word may have more than one syntactically functional token.", "labels": [], "entities": []}, {"text": "Dependency parsing has been selected in belief that it is more suitable for Arabic than constituent-based parsing.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7876188457012177}, {"text": "constituent-based parsing", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.6609849333763123}]}, {"text": "All grammatical relations in Arabic are binary asymmetrical relations that exist between the tokens of a sentence.", "labels": [], "entities": []}, {"text": "According to Jonathan: \"In general the Arabic notion of dependency and that defined in certain modern versions e.g. Tesniere (1959) rest on common principles\".", "labels": [], "entities": []}, {"text": "With a tokenization accuracy of 99.34%, a POS tagging accuracy of 96.39%, and with the absence of linguistic features and the use of word stems instead of lemmas, the Labeled Attachment Score drops from 74.75% in the gold standard experiment to 63.10% in the completely automatic experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.7101463079452515}, {"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.6458525210618973}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.5070667266845703}]}, {"text": "Most errors area direct result of tokenization errors, which indicates that despite the high accuracy on tokenization, it is still not enough to produce satisfactory parsing numbers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9985437393188477}]}], "datasetContent": [{"text": "The data used for the current study is the same data set used for the CoNLL (2007) shared task, with the same division into training set, and test set.", "labels": [], "entities": [{"text": "CoNLL (2007) shared task", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.8619513114293417}]}, {"text": "This design helps in comparing results in away that enables us to measure the effect of automatic pre-processing on parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 116, "end_pos": 123, "type": "TASK", "confidence": 0.9735228419303894}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.8551402688026428}]}, {"text": "The data is in the CoNLL column format.", "labels": [], "entities": [{"text": "CoNLL column format", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.8996595939000448}]}, {"text": "In this format, each token is represented through columns each of which has some specific information.", "labels": [], "entities": []}, {"text": "The first column is the ID, the second the token, the third the lemma, the fourth the coarse-grained POS tag, the fifth the POS tag, and the sixth column is a list of linguistic features.", "labels": [], "entities": []}, {"text": "The last two columns of the vector include the head of the token and the dependency relation between the token and its head.", "labels": [], "entities": []}, {"text": "Linguistic features are an unordered set of syntactic and/or morphological features, separated by a vertical bar (|), or an underscore if not available.", "labels": [], "entities": []}, {"text": "The features in the CoNLL 2007 Arabic dataset represent case, mood, definiteness, voice, number, gender and person.", "labels": [], "entities": [{"text": "CoNLL 2007 Arabic dataset", "start_pos": 20, "end_pos": 45, "type": "DATASET", "confidence": 0.944458469748497}]}, {"text": "The data used for training the stemmer/tokenizer is taken from the Arabic Treebank ().", "labels": [], "entities": [{"text": "Arabic Treebank", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.9620268046855927}]}, {"text": "Care has been taken not to use the parts of the ATB that are also used in the Prague Arabic Dependency Treebank ( since the PADT and the ATB share material.", "labels": [], "entities": [{"text": "ATB", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9538748264312744}, {"text": "Prague Arabic Dependency Treebank", "start_pos": 78, "end_pos": 111, "type": "DATASET", "confidence": 0.9696766287088394}, {"text": "PADT", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.9013389945030212}]}, {"text": "The official evaluation metric in the CoNLL 2007 shared task on dependency parsing was the labeled attachment score (LAS), i.e., the percentage of tokens for which a system has predicted the correct HEAD and DEPREL, but results reported also included unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and the label accuracy (LA), i.e., the percentage of tokens with correct DEPREL.", "labels": [], "entities": [{"text": "CoNLL 2007 shared task", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8163674771785736}, {"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8149254322052002}, {"text": "labeled attachment score (LAS)", "start_pos": 91, "end_pos": 121, "type": "METRIC", "confidence": 0.8506325582663218}, {"text": "DEPREL", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.8949807286262512}, {"text": "unlabeled attachment score (UAS)", "start_pos": 251, "end_pos": 283, "type": "METRIC", "confidence": 0.7677869151035944}, {"text": "label accuracy (LA)", "start_pos": 343, "end_pos": 362, "type": "METRIC", "confidence": 0.8497416615486145}]}, {"text": "We will use the same metrics here.", "labels": [], "entities": []}, {"text": "One major difference between the parsing experiments which were performed in the 2007 shared task and the ones performed here is vocalization.", "labels": [], "entities": []}, {"text": "The data set which was used in the shared task was completely vocalized with both word-internal short vowels and case markings.", "labels": [], "entities": []}, {"text": "Since vocalization in such a perfect form is almost impossible to produce automatically, we have decided to primarily use unvocalized data instead.", "labels": [], "entities": []}, {"text": "We have removed the word internal short vowels as well as the case markings from both the training set and the test set.", "labels": [], "entities": []}, {"text": "This has the advantage of representing naturally occurring Arabic more closely, and the disadvantage of losing information that is only available through vocalization.", "labels": [], "entities": []}, {"text": "We will, however, report on the effect of vocalization on dependency parsing in the discussion.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.858037680387497}]}, {"text": "To give an estimate of the effects vocalization has on dependency parsing, we have replicated the original task with the vocalized data, and then re-run the experiment with the unvocalized version.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.834834635257721}]}, {"text": "The results of the experiment indicate that vocalization has a positive effect on the quality of the parsing output, which maybe due to the fact that ambiguity decreases with vocalization.", "labels": [], "entities": []}, {"text": "Labeled attachment score drops from 74.77% on the vocalized data to 74.16% on unvocalized data.", "labels": [], "entities": [{"text": "Labeled attachment score", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.6609794696172079}]}, {"text": "Unlabeled attachment score drops from 84.09% to 83.53% and labeled accuracy score from 85.68% to 85.44%.", "labels": [], "entities": [{"text": "Unlabeled attachment score", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.7445715665817261}, {"text": "accuracy score", "start_pos": 67, "end_pos": 81, "type": "METRIC", "confidence": 0.9370777606964111}]}, {"text": "The difference is minimal, and is expected to be even smaller with automatic vocalization", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 presents the results:", "labels": [], "entities": []}, {"text": " Table 3: effect of fine-grained POS", "labels": [], "entities": [{"text": "POS", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.31544551253318787}]}, {"text": " Table 4: Automatic dependency parsing experiment", "labels": [], "entities": [{"text": "Automatic dependency parsing", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6679753661155701}]}, {"text": " Table 5: Dependency parsing Evaluation on Correctly  vs. Incorrectly Tokenized Sentences", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8879139721393585}]}, {"text": " Table 7: Using the ATB tagset with the PADT dataset", "labels": [], "entities": [{"text": "ATB tagset", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9254403412342072}, {"text": "PADT dataset", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8901770114898682}]}, {"text": " Table 8: Including the Penn full tagset in the reduced  experiments", "labels": [], "entities": [{"text": "Penn full tagset", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9062129457791647}]}, {"text": " Table 9: Vocalization Effect on Dependency  Parsing", "labels": [], "entities": [{"text": "Dependency  Parsing", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.6233702600002289}]}]}