{"title": [{"text": "The BM-I2R Haitian-Cr\u00e9ole-to-English translation system description for the WMT 2011 evaluation campaign", "labels": [], "entities": [{"text": "BM-I2R Haitian-Cr\u00e9ole-to-English translation", "start_pos": 4, "end_pos": 48, "type": "TASK", "confidence": 0.6745752493540446}, {"text": "WMT 2011 evaluation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.6419032017389933}]}], "abstractContent": [{"text": "This work describes the Haitian-Cr\u00e9ole to En-glish statistical machine translation system built by Barcelona Media Innovation Center (BM) and Institute for Infocomm Research (I2R) for the 6th Workshop on Statistical Machine Translation (WMT 2011).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6094771722952524}, {"text": "Barcelona Media Innovation Center (BM)", "start_pos": 99, "end_pos": 137, "type": "DATASET", "confidence": 0.9218496765409198}, {"text": "Statistical Machine Translation (WMT 2011)", "start_pos": 204, "end_pos": 246, "type": "TASK", "confidence": 0.7968867974621909}]}, {"text": "Our system carefully processes the available data and uses it in a standard phrase-based system enhanced with a source context semantic feature that helps conducting a better lexical selection and a feature orthogonalization procedure that helps making MERT optimization more reliable and stable.", "labels": [], "entities": [{"text": "MERT optimization", "start_pos": 253, "end_pos": 270, "type": "TASK", "confidence": 0.9511268436908722}]}, {"text": "Our system was ranked first (among a total of 9 participant systems) by the conducted human evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "During years there has been a big effort to produce natural language processing tools that try to understand well written sentences, but the question is how well do these tools work to analyze the contents of SMS.", "labels": [], "entities": []}, {"text": "For example, not even syntactic tools like stemming can bring to common stems words that have been shortened (like Xmas or Christmas).", "labels": [], "entities": []}, {"text": "This paper describes our participation on the 6th Workshop on Statistical Machine Translation (WMT 2011).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT 2011)", "start_pos": 62, "end_pos": 104, "type": "TASK", "confidence": 0.8505095498902457}]}, {"text": "The featured task from the workshop was to translate Haitian-Cr\u00e9ole SMS messages into English.", "labels": [], "entities": [{"text": "translate Haitian-Cr\u00e9ole SMS messages", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.7976406365633011}]}, {"text": "According to the WMT 2011 organizers, these text messages (SMS) were sent by people in Haiti in the aftermath of the January 2010 earthquake.", "labels": [], "entities": [{"text": "WMT 2011", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.5844950675964355}]}, {"text": "Our objective in this featured task is to translate from Haitian-Cr\u00e9ole into English either using raw or clean data.", "labels": [], "entities": [{"text": "translate from Haitian-Cr\u00e9ole", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.8986424207687378}]}, {"text": "We propose to build an SMT system which could be used for both raw and clean data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9914373755455017}]}, {"text": "Our baseline system is an standard phrase-based SMT system built with Moses (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8978328108787537}]}, {"text": "Starting from this system we propose to introduce a semantic feature function based on latent semantic indexing (.", "labels": [], "entities": []}, {"text": "Additionally, as a total different approximation, we propose to orthogonalize the standard feature functions of the phrase-based table using the Gram-Schmidt methodology.", "labels": [], "entities": []}, {"text": "Then, we experimentally combine both enhancements.", "labels": [], "entities": []}, {"text": "The only difference among the raw and clean SMT system were the training sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9939875602722168}]}, {"text": "In order to translate the clean data, we propose to normalize the corpus of short messages given very scarce resources.", "labels": [], "entities": []}, {"text": "We only count with a small set of parallel corpus at the level of sentence of chat and standard language.", "labels": [], "entities": []}, {"text": "A nice normalization methodology can allow to make the task of communication easier.", "labels": [], "entities": []}, {"text": "We propose a statistical normalization technique using the scarce resources we have based on a combination of statistical machine translation techniques.", "labels": [], "entities": [{"text": "statistical normalization", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.8166874349117279}, {"text": "statistical machine translation", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.6678834656874338}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes the phrase-based SMT system which is used as a reference system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8377364873886108}]}, {"text": "Next, section 3 describes our approximation to introduce semantics in the baseline system.", "labels": [], "entities": []}, {"text": "Section 4 reports our idea of orthogonalizing the feature functions in the translation table.", "labels": [], "entities": []}, {"text": "Section 5 details the data processing and the data conversion from raw to clean.", "labels": [], "entities": []}, {"text": "As follows, section 6 shows the translation results.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9688846468925476}]}, {"text": "Finally, section 7 reports most relevant conclusions of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report the details of the used data preprocessing and raw to clean data conversion.", "labels": [], "entities": []}, {"text": "In this section we report the results of the approaches proposed in previous sections.", "labels": [], "entities": []}, {"text": "The clean data seems to benefit from the semantic features and the orthofeatures separately.", "labels": [], "entities": []}, {"text": "However, the raw data seems not to benefit from the orthofeatures and keep the similar performance to the baseline system when using the semantic feature.", "labels": [], "entities": []}, {"text": "Although, this trend is clear, the results are not conclusive.", "labels": [], "entities": []}, {"text": "Therefore, we decided to participate in the evaluation with the full system (including the semantic features and orthofeatures) in the clean track and with the system including the semantic feature in the raw track.", "labels": [], "entities": []}, {"text": "Actually, we used those systems that performed best in the development set.", "labels": [], "entities": []}, {"text": "Additionally, results with the semantic feature may not be significantly better than the baseline system, but we have seen it actually heps to improve lexical selection in practice in previous works).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data Statistics before and after training prepro- cessing. Number of words are from the English side.", "labels": [], "entities": []}, {"text": " Table 2: BLEU results for the raw data. Best results in  bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9980059266090393}]}, {"text": " Table 3: BLEU results for the clean data. Best results in  bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976389408111572}]}]}