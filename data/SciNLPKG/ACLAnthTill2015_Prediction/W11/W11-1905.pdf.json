{"title": [{"text": "Exploring Lexicalized Features for Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9359243810176849}]}], "abstractContent": [{"text": "In this paper, we describe a coreference solver based on the extensive use of lexical features and features extracted from dependency graphs of the sentences.", "labels": [], "entities": [{"text": "coreference solver", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9276847839355469}]}, {"text": "The solver uses Soon et al.", "labels": [], "entities": [{"text": "solver", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9891982674598694}]}, {"text": "(2001)'s classical resolution algorithm based on a pairwise classification of the mentions.", "labels": [], "entities": [{"text": "classical resolution", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7016569077968597}]}, {"text": "We applied this solver to the closed track of the CoNLL 2011 shared task (Pradhan et al., 2011).", "labels": [], "entities": [{"text": "CoNLL 2011 shared task", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.8328788578510284}]}, {"text": "We carried out a systematic optimization of the feature set using cross-validation that led us to retain 24 features.", "labels": [], "entities": []}, {"text": "Using this set, we reached a MUC score of 58.61 on the test set of the shared task.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9725340306758881}]}, {"text": "We analyzed the impact of the features on the development set and we show the importance of lexicalization as well as of properties related to dependency links in coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 163, "end_pos": 185, "type": "TASK", "confidence": 0.9483200907707214}]}], "introductionContent": [{"text": "In this paper, we present our contribution to the closed track of the 2011 CoNLL shared task).", "labels": [], "entities": [{"text": "2011 CoNLL shared task", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.5990020632743835}]}, {"text": "We started from a baseline system that uses's architecture and features.", "labels": [], "entities": []}, {"text": "Mentions are identified by selecting all noun phrases and possessive pronouns.", "labels": [], "entities": []}, {"text": "Then, the resolution algorithm relies on a pairwise classifier that determines whether two mentions corefer or not.", "labels": [], "entities": [{"text": "resolution", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.940338671207428}]}, {"text": "Lexicalization has proved effective in numerous tasks of natural language processing such as partof-speech tagging or parsing.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.8223808109760284}]}, {"text": "However, lexicalized models require a good deal of annotated data to avoid overfit.", "labels": [], "entities": []}, {"text": "The data set used in the shared task has a considerable size compared to corpora traditionally used in coreference resolutionthe training set comprises 2,374 documents.", "labels": [], "entities": [{"text": "coreference resolutionthe training", "start_pos": 103, "end_pos": 137, "type": "TASK", "confidence": 0.9153536160786947}]}, {"text": "See fora previous work using an earlier version of this dataset.", "labels": [], "entities": []}, {"text": "Leveraging this size, we investigated the potential of lexicalized features.", "labels": [], "entities": []}, {"text": "Besides lexical features, we created features that use part-of-speech tags and semantic roles.", "labels": [], "entities": []}, {"text": "We also constructed features using dependency tree paths and labels by converting the constituent trees provided in the shared task into dependency graphs.", "labels": [], "entities": []}, {"text": "The final feature set was selected through an automated feature selection procedure using crossvalidation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Baseline figures using string match and alias  properties, and our Soon baseline using decision trees  with the C4.5 induction program and logistic regression  (LR). MD stands for mention detection.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 190, "end_pos": 207, "type": "TASK", "confidence": 0.674833670258522}]}, {"text": " Table 2: Impact of the postprocessing step on the devel- opment set.", "labels": [], "entities": []}, {"text": " Table 4: Scores on development set, on the test set, and on the test set with given mention boundaries: recall (R),  precision (P), and harmonic mean (F1). The official CoNLL score is computed as the mean of MUC, BCUB, and  CEAFE.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.955564334988594}, {"text": "precision (P)", "start_pos": 118, "end_pos": 131, "type": "METRIC", "confidence": 0.9576056450605392}, {"text": "harmonic mean (F1)", "start_pos": 137, "end_pos": 155, "type": "METRIC", "confidence": 0.8976075530052186}, {"text": "MUC", "start_pos": 209, "end_pos": 212, "type": "DATASET", "confidence": 0.5519284605979919}, {"text": "BCUB", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.8431757688522339}, {"text": "CEAFE", "start_pos": 225, "end_pos": 230, "type": "DATASET", "confidence": 0.9213802814483643}]}, {"text": " Table 3: The final feature set and, for each feature, the  degradation in performance when leaving out this feature  from the set. All evaluations were carried out on the de- velopment set. The features marked with a dagger  \u2020 orig- inate from the", "labels": [], "entities": []}]}