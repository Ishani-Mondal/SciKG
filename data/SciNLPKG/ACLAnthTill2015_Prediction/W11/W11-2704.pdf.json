{"title": [{"text": "Task-Based Evaluation of NLG Systems: Control vs Real-World Context", "labels": [], "entities": []}], "abstractContent": [{"text": "Currently there is little agreement about, or even discussion of, methodologies for task-based evaluation of NLG systems.", "labels": [], "entities": []}, {"text": "I discuss one specific issue in this area, namely the importance of control vs the importance of ecological validity (real-world context), and suggest that perhaps we need to put more emphasis on ecological validity in NLG evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-based extrinsic evaluation of a Natural Language Generation (NLG) system involves measuring the impact of an NLG system on how well subjects perform a task.", "labels": [], "entities": [{"text": "Task-based extrinsic evaluation of a Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.6604709002104673}]}, {"text": "It is usually regarded as the 'gold standard' for NLG evaluation, and it is the only type of evaluation which will be seriously considered by many external user communities.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.8799881041049957}]}, {"text": "Despite the importance of task-based evaluations, however, there is surprisingly little discussion (or agreement) in the NLG community about how these should be carried out.", "labels": [], "entities": []}, {"text": "In recent years there has been a fair amount of discussion about the appropriate use of corpus-based metrics, and there seems (de facto) to be some level of agreement about evaluations based on opinions of human subjects.", "labels": [], "entities": []}, {"text": "But there is little discussion and much diversity in task-based evaluation methodology.", "labels": [], "entities": []}, {"text": "In this paper I focus on one one specific methodological issue, which is the relative importance of control and ecological validity (real-world context).", "labels": [], "entities": []}, {"text": "An ideal task-based evaluation would be controlled, that is the impact of NLG texts would be compared against the impact of controlled or baseline texts in a manner which minimises confounding factors.", "labels": [], "entities": []}, {"text": "It would also be ecologically valid, that is the evaluation would be carried out by representative realworld users in a real-world context while performing real-world tasks.", "labels": [], "entities": []}, {"text": "Unfortunately, because of pragmatic constraints including time, money, and ethical approval, it is not always possible to achieve both of these goals.", "labels": [], "entities": []}, {"text": "So which is more important?", "labels": [], "entities": []}, {"text": "The methodologies currently used for task-based evaluation in NLG largely derive from the HumanComputer Interaction community, which in turn are largely based on methodologies for experiments in cognitive psychology.", "labels": [], "entities": []}, {"text": "Now, psychologists place much more emphasis on control than on ecological validity; they regard control as absolutely essential, but (with some exceptions) they see little wrong with conducting experiments on unrepresentative subjects (undergraduates) in artificial contexts (psychology labs).", "labels": [], "entities": []}, {"text": "Indeed many psychologists are now embracing web-based experiments, where they do not even know who the subjects are and what contexts they are working in.", "labels": [], "entities": []}, {"text": "For the research goals of psychologists, this probably makes sense.", "labels": [], "entities": []}, {"text": "But the research goals of the NLG community are different from the research goals of the psychological community; should we place more emphasis on ecological validity than they do, and lesson control?", "labels": [], "entities": []}, {"text": "My own opinions on this matter are changing.", "labels": [], "entities": []}, {"text": "Five years ago, I would have echoed the feeling that control is all-important.", "labels": [], "entities": []}, {"text": "Now, though, I am beginning to think that in order to achieve both NLG's scientific goals (understanding language and computation) and NLG's technological goals (developing useful real-world technology), we need to put more emphasis on ecological validity in our evaluations.", "labels": [], "entities": []}, {"text": "2 Evaluation which is both controlled and in real-world context: STOP and DIAG An ideal evaluation is one which is both controlled and done in a real-world context.", "labels": [], "entities": [{"text": "STOP", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.945152223110199}, {"text": "DIAG", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9890080690383911}]}, {"text": "An example is the evaluation of the STOP system.", "labels": [], "entities": [{"text": "STOP", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.7549736499786377}]}, {"text": "which generated tailored smoking-cessation advice based on the user's response to a questionnaire ().", "labels": [], "entities": []}, {"text": "The STOP project was a collaboration with medical colleagues, and the STOP evaluation (which was designed by the medics) was carried out as a randomised controlled clinical trial.", "labels": [], "entities": [{"text": "STOP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.7051258683204651}, {"text": "STOP evaluation", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.6000770330429077}]}, {"text": "We recruited 2500 smokers, and sent one-third of them STOP letters, one-third a non-tailored (canned) letter, and one-third a letter which just thanked them for being in out study.", "labels": [], "entities": [{"text": "STOP", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.7250591516494751}]}, {"text": "After 6 months we asked participants if they had stopped smoking; we tested saliva samples from people who said they had quit in order to verify their smoking status.", "labels": [], "entities": []}, {"text": "The result of this evaluation was that the STOP tailored letters were no more effective than the control non-tailored letter.", "labels": [], "entities": [{"text": "STOP", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.6755943894386292}]}, {"text": "The STOP evaluation cost about UK\u00a375,000, and took about 20 months to design, organise, and carryout.", "labels": [], "entities": [{"text": "STOP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9279102087020874}]}, {"text": "The STOP evaluation was carried out in a realworld context; the letters were sent to actual smokers, and we measured whether they quit smoking.", "labels": [], "entities": [{"text": "STOP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.869467556476593}]}, {"text": "It was also controlled, since the impact of STOP letters was compared to the impact of non-tailored letters.", "labels": [], "entities": [{"text": "STOP letters", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.7211624085903168}]}, {"text": "However there was a lot of 'noise' (in the statistical sense) in the STOP evaluation, because different people (with different personalities, attitudes towards smoking, personal circumstances, etc) received the tailored and non-tailored letters, and this impacted smoking-cessation rates in the the three groups.", "labels": [], "entities": [{"text": "STOP", "start_pos": 69, "end_pos": 73, "type": "TASK", "confidence": 0.7172025442123413}]}, {"text": "Another evaluation which was controlled and was done at least partially in a real-world context was the evaluation of the DIAG-NLP intelligent tutoring system (di).", "labels": [], "entities": []}, {"text": "In this experiment, 75 students (the appropriate subject group for this tutoring system) were divided into three groups: two groups interacted with two versions of the DIAG-NLP system, and a third interacted with a control version of DIAG which did not include any NLG.", "labels": [], "entities": [{"text": "DIAG", "start_pos": 234, "end_pos": 238, "type": "DATASET", "confidence": 0.9435427188873291}]}, {"text": "Effectiveness was measured by learning gain (change in knowledge, measured by differences in scores in a pre-test and post-test), which is standard in the tutoring system domain.", "labels": [], "entities": []}, {"text": "The evaluation showed that students learned more from the second (more advanced) version of the DIAG-NLP system than from the non-NLG version of DIAG.", "labels": [], "entities": [{"text": "DIAG", "start_pos": 145, "end_pos": 149, "type": "DATASET", "confidence": 0.9630048274993896}]}, {"text": "The DIAG-NLP evaluation was controlled, and it was real-world in the sense that it used representative subjects and measured real-world outcome.", "labels": [], "entities": []}, {"text": "However, it appears (the paper is not completely explicit about this) that the evaluation assessed learning about a topic (fixing a home heating system) which was not part of the student's normal curriculum; if this is the case, then the evaluation was not 100% in a real-world context.", "labels": [], "entities": []}, {"text": "We picked 24 data sets (scenarios) based on historical data from babies who had been in NICU 5 years previously, and for each data set created three presentations: visualisation, computergenerated text, and human-written text.", "labels": [], "entities": [{"text": "NICU", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8717749118804932}]}, {"text": "For each data set, we also asked expert consultants what actions should betaken by medical staff.", "labels": [], "entities": []}, {"text": "We then asked 35 medical staff (doctors and nurses of varied expertise levels) to look at the scenarios using a mix of presentations, in a Latin Square design; eg, 1/3 of the subjects saw the visualisation of scenario 1 data, 1/3 saw the computer-generated summary of scenario 1 data, and 1/3 saw the human-written summary of this data.", "labels": [], "entities": []}, {"text": "Also each subject saw the same number of scenarios in each condition, this reduced the impact of individual differences between subjects.", "labels": [], "entities": []}, {"text": "Subjects were asked to make decisions about appropriate medical actions (or say no action should be taken), and responses were compared to the 'gold standard' recommendations from the consultants.", "labels": [], "entities": []}, {"text": "The result was that decision performance was best with the human-written summaries; there was no significant difference between overall decision performance with the computer-generated summaries and the visualisation (although at the level of individual scenarios, computer texts were more effective in some scenarios, and visualisations was more effective in other scenarios).", "labels": [], "entities": []}, {"text": "The BT45 evaluation cost about UK\u00a320,000, and took about 6 months to design, organise, and carryout.", "labels": [], "entities": []}, {"text": "The BT45 evaluation was carefully controlled However, it was not done in a real-world context.", "labels": [], "entities": [{"text": "BT45 evaluation", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9092256128787994}]}, {"text": "Doctors and nurses satin an experiment room (not in the ward) and looked at data from babies they did not remember (as opposed to babies whom they knew well because they has been looking after them for the past few weeks); they also did not visually observe the babies, which is a very important information source for NICU staff.", "labels": [], "entities": []}, {"text": "Many other task-based evaluations of NLG systems have been controlled but not done in a realworld context, including the very first task-based NLG evaluation I am aware of, by.", "labels": [], "entities": []}, {"text": "Young developed four algorithms for generating instructional texts, and tested these by asking 26 students to follow the instructions generated by the various algorithms on several scenarios, and measured error rates in carrying out the instructions.", "labels": [], "entities": []}, {"text": "The instructions involved carrying out actions on campus (going to labs, playing in soccer matches, etc).", "labels": [], "entities": []}, {"text": "The students did not actually carryout these actions, instead they interacted with a 'text-based virtual reality system'.", "labels": [], "entities": []}, {"text": "Hence the evaluation was controlled but not carried out in real-world context.", "labels": [], "entities": []}], "datasetContent": [{"text": "The next Babytalk system (after BT45) was BT-NURSE; it generated summaries of 12-hours of clinical data, to support nursing shift handover (Hunter et al., 2011).", "labels": [], "entities": [{"text": "Babytalk", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.8106578588485718}, {"text": "BT45", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9104554653167725}, {"text": "BT-NURSE", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.8367855548858643}]}, {"text": "We initially expected to evaluate BT-NURSE using a similar methodology to the BT45 evaluation.", "labels": [], "entities": [{"text": "BT-NURSE", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9182800054550171}, {"text": "BT45 evaluation", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9380871653556824}]}, {"text": "However the medical people involved in BabyTalk complained that it was unrealistic to evaluate the system in an artificial controlled context, where clinical staff were looking at data out of context.", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9000405073165894}]}, {"text": "So instead we evaluated BT-NURSE by installing the system in the NICU, so that nurses used it to get information about babies they were actually caring for.", "labels": [], "entities": [{"text": "BT-NURSE", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.48883095383644104}, {"text": "NICU", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9629340767860413}]}, {"text": "The primary outcome measure was subjective ratings by nurses as to the helpfulness of BT-NURSE texts; and indeed most nurses thought the texts were helpful.", "labels": [], "entities": [{"text": "BT-NURSE texts", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.8463523089885712}]}, {"text": "All in all cost was probably about UK\u00a350,000, and the entire process (including the software engineering) took about 18 months.", "labels": [], "entities": []}, {"text": "The BT-NURSE evaluation was not controlled; we did not compare the computer generated texts to anything else, and indeed did not directly measure any task outcome variable, instead we solicited opinions as to utility.", "labels": [], "entities": [{"text": "BT-NURSE", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.6735951900482178}]}, {"text": "It was however ecologically valid, since it was carried out by asking nurses (real-world users) to use BT-NURSE for care planning (realworld task) in a real-world context (on-ward, involving babies the nurses were familiar with and could visually observe).", "labels": [], "entities": [{"text": "BT-NURSE", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.9329647421836853}]}], "tableCaptions": []}