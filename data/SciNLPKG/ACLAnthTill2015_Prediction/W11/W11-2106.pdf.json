{"title": [{"text": "TESLA at WMT 2011: Translation Evaluation and Tunable Metric", "labels": [], "entities": [{"text": "TESLA at WMT 2011", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.5932532399892807}, {"text": "Translation Evaluation", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8919586837291718}]}], "abstractContent": [{"text": "This paper describes the submission from the National University of Singapore to the WMT 2011 Shared Evaluation Task and the Tunable Metric Task.", "labels": [], "entities": [{"text": "WMT 2011 Shared Evaluation Task", "start_pos": 85, "end_pos": 116, "type": "DATASET", "confidence": 0.6678305029869079}, {"text": "Tunable Metric Task", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8442899386088053}]}, {"text": "Our entry is TESLA in three different configurations: TESLA-M, TESLA-F, and the new TESLA-B.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.717272162437439}, {"text": "TESLA-M", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.5475899577140808}, {"text": "TESLA-F", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.6938303709030151}, {"text": "TESLA-B", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.8552336692810059}]}], "introductionContent": [], "datasetContent": [{"text": "Linear-programming-based Analysis) was first proposed in.", "labels": [], "entities": [{"text": "Linear-programming-based Analysis", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7426604330539703}]}, {"text": "The simplest variant, TESLA-M (M stands for minimal), is based on Ngram matching, and utilizes light-weight linguistic analysis including lemmatization, part-of-speech tagging, and WordNet synonym relations.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9462525248527527}, {"text": "part-of-speech tagging", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.6919327676296234}]}, {"text": "TESLA-B (B stands for basic) additionally takes advantage of bilingual phrase tables to model phrase synonyms.", "labels": [], "entities": [{"text": "TESLA-B", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5639745593070984}]}, {"text": "It is anew configuration proposed in this paper.", "labels": [], "entities": []}, {"text": "The most sophisticated configuration TESLA-F (F stands for full) additionally uses language models and a ranking support vector machine instead of simple averaging.", "labels": [], "entities": [{"text": "TESLA-F", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9918671250343323}]}, {"text": "TESLA-F was called TESLA in.", "labels": [], "entities": [{"text": "TESLA-F", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8980653882026672}, {"text": "TESLA in", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.6378190964460373}]}, {"text": "In this paper, we rationalize the naming convention by using TESLA to refer to the whole family of metrics.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9427042007446289}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Sections 2 to 4 describe the TESLA variants TESLA-M, TESLA-B, and TESLA-F, respectively.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.7186990976333618}, {"text": "TESLA-B", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.7362227439880371}, {"text": "TESLA-F", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.7696871161460876}]}, {"text": "Section 5 describes MT tuning with TESLA.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9795591831207275}, {"text": "TESLA", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9478076100349426}]}, {"text": "Section 6 shows experimental results for the evaluation and the tunable metric task.", "labels": [], "entities": []}, {"text": "The last section concludes the paper.", "labels": [], "entities": []}, {"text": "We evaluate TESLA using the publicly available data from WMT 2009 for into-English and outof-English translation.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.8065842986106873}, {"text": "WMT 2009", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9449779987335205}]}, {"text": "The pivot language phrase tables and language models are built using the WMT 2009 training data.", "labels": [], "entities": [{"text": "WMT 2009 training data", "start_pos": 73, "end_pos": 95, "type": "DATASET", "confidence": 0.9640890061855316}]}, {"text": "The SVM rank model for TESLA-F is trained on manual rankings from WMT 2008.", "labels": [], "entities": [{"text": "TESLA-F", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.5209389925003052}, {"text": "WMT 2008", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9529872536659241}]}, {"text": "The results for TESLA-M and TESLA-F have previously been reported in . We add results for the new variant TESLA-B here.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7728576064109802}, {"text": "TESLA-F", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.6915562152862549}]}, {"text": "show the sentence-level consistency and system-level Spearman's rank correlation, respectively for into-English translation.", "labels": [], "entities": [{"text": "consistency", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.7784276604652405}, {"text": "Spearman's rank correlation", "start_pos": 53, "end_pos": 80, "type": "METRIC", "confidence": 0.7833135947585106}]}, {"text": "For comparison, we include results for some of the best performing metrics in WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.8282155990600586}]}, {"text": "show the same results for out-of-English translation.", "labels": [], "entities": []}, {"text": "We do not include the English-Czech language pair in our experiments, as we unfortunately do not have good linguistic resources for the Czech language.", "labels": [], "entities": []}, {"text": "The new TESLA-B metric proves to be competitive to its siblings and is often on par with the more sophisticated TESLA-F metric.", "labels": [], "entities": [{"text": "TESLA-B metric", "start_pos": 8, "end_pos": 22, "type": "METRIC", "confidence": 0.6847316026687622}, {"text": "TESLA-F metric", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.5370582044124603}]}, {"text": "The exception is the English-German language pair, where TESLA-B has very low system-level correlation.", "labels": [], "entities": [{"text": "TESLA-B", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.8014578223228455}]}, {"text": "We have two possible explanations for this.", "labels": [], "entities": []}, {"text": "First, the systemlevel correlation is computed on a very small sample size (the ranked list of MT systems).", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9506974220275879}]}, {"text": "This makes the system-level correlation score more volatile compared to the sentence-level consistency score which is computed on thousands of sentence pairs.", "labels": [], "entities": [{"text": "correlation score", "start_pos": 28, "end_pos": 45, "type": "METRIC", "confidence": 0.8589848279953003}, {"text": "sentence-level consistency score", "start_pos": 76, "end_pos": 108, "type": "METRIC", "confidence": 0.6543742616971334}]}, {"text": "Second, German has a relatively free word order which potentially makes word alignment and phrase table extraction more noisy.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7888083755970001}, {"text": "phrase table extraction", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.685951034228007}]}, {"text": "Interestingly, all participating metrics in WMT 2009 had low system-level correlation for the English-German language pair.: Out-of-English system-level Spearman's rank correlation on WMT 2009 data", "labels": [], "entities": [{"text": "WMT 2009", "start_pos": 44, "end_pos": 52, "type": "TASK", "confidence": 0.5748592615127563}, {"text": "Out-of-English system-level Spearman's rank correlation", "start_pos": 125, "end_pos": 180, "type": "METRIC", "confidence": 0.6613744894663492}, {"text": "WMT 2009 data", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.9332143266995748}]}], "tableCaptions": [{"text": " Table 1: Into-English sentence-level consistency on  WMT 2009 data", "labels": [], "entities": [{"text": "WMT 2009 data", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9288193186124166}]}, {"text": " Table 2: Into-English system-level Spearman's rank  correlation on WMT 2009 data", "labels": [], "entities": [{"text": "Spearman's rank  correlation", "start_pos": 36, "end_pos": 64, "type": "METRIC", "confidence": 0.6415601298213005}, {"text": "WMT 2009 data", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.8924543460210165}]}, {"text": " Table 3: Out-of-English sentence-level consistency  on WMT 2009 data", "labels": [], "entities": [{"text": "WMT 2009 data", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9570614099502563}]}, {"text": " Table 4: Out-of-English system-level Spearman's  rank correlation on WMT 2009 data", "labels": [], "entities": [{"text": "Spearman's  rank correlation", "start_pos": 38, "end_pos": 66, "type": "METRIC", "confidence": 0.6926252916455269}, {"text": "WMT 2009 data", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.8626447121302286}]}, {"text": " Table 5: Automatic evaluation scores on held out  test portion for the tunable metric task. The best re- sult in each column is printed in bold.", "labels": [], "entities": [{"text": "re- sult", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9386212031046549}]}]}