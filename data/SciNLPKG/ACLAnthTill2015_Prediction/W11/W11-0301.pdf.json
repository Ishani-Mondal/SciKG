{"title": [{"text": "Modeling Syntactic Context Improves Morphological Segmentation", "labels": [], "entities": [{"text": "Modeling Syntactic Context Improves Morphological Segmentation", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.8623334318399429}]}], "abstractContent": [{"text": "The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underuti-lized in text processing systems.", "labels": [], "entities": []}, {"text": "This paper proposes a novel model for morphological segmentation that is driven by this connection.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7752647995948792}]}, {"text": "Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words.", "labels": [], "entities": []}, {"text": "Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmen-tation of Arabic.", "labels": [], "entities": [{"text": "POS categorization", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7504725754261017}]}], "introductionContent": [{"text": "A tight connection between morphology and syntax is well-documented in linguistic literature.", "labels": [], "entities": []}, {"text": "In many languages, morphology plays a central role in marking syntactic structure, while syntactic relations help to reduce morphological ambiguity.", "labels": [], "entities": [{"text": "marking syntactic structure", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.8758426308631897}]}, {"text": "Therefore, in an unsupervised linguistic setting which is rife with ambiguity, modeling this connection can be particularly beneficial.", "labels": [], "entities": []}, {"text": "However, existing unsupervised morphological analyzers take little advantage of this linguistic property.", "labels": [], "entities": []}, {"text": "In fact, most of them operate at the vocabulary level, completely ignoring sentence context.", "labels": [], "entities": []}, {"text": "This design is not surprising: atypical morphological analyzer does not have access to syntac- The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/morphsyn/.", "labels": [], "entities": []}, {"text": "tic information, because morphological segmentation precedes other forms of sentence analysis.", "labels": [], "entities": [{"text": "sentence analysis", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7364114224910736}]}, {"text": "In this paper, we demonstrate that morphological analysis can utilize this connection without assuming access to full-fledged syntactic information.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.740204244852066}]}, {"text": "In particular, we focus on two aspects of the morphosyntactic connection: \u2022 Morphological consistency within POS categories.", "labels": [], "entities": []}, {"text": "Words within the same syntactic category tend to select similar affixes.", "labels": [], "entities": []}, {"text": "This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.", "labels": [], "entities": []}, {"text": "\u2022 Morphological realization of grammatical agreement.", "labels": [], "entities": [{"text": "Morphological realization of grammatical agreement", "start_pos": 2, "end_pos": 52, "type": "TASK", "confidence": 0.9164244532585144}]}, {"text": "In many morphologically rich languages, agreement between syntactic dependents is expressed via correlated morphological markers.", "labels": [], "entities": []}, {"text": "For instance, in Semitic languages, gender and number agreement between nouns and adjectives is expressed using matching suffixes.", "labels": [], "entities": []}, {"text": "Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "labels": [], "entities": [{"text": "wordlevel analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7796676754951477}]}, {"text": "In both cases, we do not assume that the relevant syntactic information is provided, but instead jointly induce it as part of morphological analysis.", "labels": [], "entities": []}, {"text": "We capture morpho-syntactic relations in a Bayesian model that grounds intra-word decisions in sentence-level context.", "labels": [], "entities": []}, {"text": "Like traditional unsupervised models, we generate morphological structure from a latent lexicon of prefixes, stems, and suffixes.", "labels": [], "entities": []}, {"text": "In addition, morphological analysis is guided by a latent variable that clusters together words with similar affixes, acting as a proxy for POS tags.", "labels": [], "entities": []}, {"text": "Moreover, a sequence-level component further refines the analysis by correlating segmentation decisions between adjacent words that exhibit morphological agreement.", "labels": [], "entities": []}, {"text": "We encourage this behavior by encoding a transition distribution over adjacent words, using string match cues as a proxy for grammatical agreement.", "labels": [], "entities": []}, {"text": "We evaluate our model on the standard Arabic treebank.", "labels": [], "entities": [{"text": "Arabic treebank", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.7809829711914062}]}, {"text": "Our full model yields 86.2% accuracy, outperforming the best published results () by 8.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9997290968894958}]}, {"text": "We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.", "labels": [], "entities": []}, {"text": "Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.8763777911663055}]}], "datasetContent": [{"text": "Performance metrics To enable comparison with previous approaches, we adopt the evaluation set-up of.", "labels": [], "entities": []}, {"text": "They evaluate segmentation accuracy on a per token basis, using recall, precision and F1-score computed on segmentation points.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.9784733057022095}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9099639058113098}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9995953440666199}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9991545677185059}, {"text": "F1-score", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9990476965904236}]}, {"text": "We also follow a transductive testing scenario where the same (unlabeled) data is used for both training and testing the model.", "labels": [], "entities": []}, {"text": "Data set We evaluate segmentation performance on the Penn Arabic Treebank (ATB).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9797160625457764}, {"text": "Penn Arabic Treebank (ATB)", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.9765379329522451}]}, {"text": "It consists of about 4,500 sentences of modern Arabic obtained from newswire articles.", "labels": [], "entities": []}, {"text": "Following the preprocessing procedures of that exclude certain word types (such as abbreviations and digits), we obtain a corpus of 120,000 tokens and 20,000 word types.", "labels": [], "entities": []}, {"text": "Since our full model operates over sentences, we train the model on the entire ATB, but evaluate on the exact portion used by.", "labels": [], "entities": [{"text": "ATB", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.9407034516334534}]}, {"text": "Pre-defined tunable parameters and testing regime In all our experiments, we set \u03b3 l = 1 2 (for length of morpheme types) and \u03b3 |S| = 1 2 (for number of morpheme segments of each word.)", "labels": [], "entities": []}, {"text": "To encourage a small set of affix types relative to stem types, we set \u03b3 \u2212 = \u03b3 + = 1 1.1 (for sizes of the affix lexicons) and \u03b3 0 = 1 10,000 (for size of the stem lexicon.)", "labels": [], "entities": []}, {"text": "We employ a sparse Dirichlet prior for the type-level models (for morphemes and POS tag) by setting \u03b1 = 0.1.", "labels": [], "entities": []}, {"text": "For the token-level models, we set hyperparameters for Dirichlet priors \u03b1 w|t = 10 \u22125 (PCT09) and the Morfessor system (Morfessor-CAT).", "labels": [], "entities": [{"text": "PCT09", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.7854303121566772}]}, {"text": "For our full model (+TOKEN-SEG) and its simplifications (BASIC, +POS, +TOKEN-POS), we perform five random restarts and show the mean scores.", "labels": [], "entities": [{"text": "BASIC", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9515287280082703}]}, {"text": "The sample standard deviations are shown in brackets.", "labels": [], "entities": []}, {"text": "The last column shows results of a paired t-test against the preceding model: ++ (significant at 1%), + (significant at 5%), \u223c (not significant), -(test not applicable).", "labels": [], "entities": []}, {"text": "(for unsegmented tokens) and \u03b1 t|t = 1.0 (for POS tags transition.)", "labels": [], "entities": []}, {"text": "To encourage adjacent words that exhibit morphological agreement to have the same final suffix, we set \u03b2 1 = 0.6, \u03b2 2 = 0.1, In all the experiments, we perform five runs using different random seeds and report the mean score and the standard deviation.", "labels": [], "entities": []}, {"text": "Baselines Our primary comparison is against the morphological segmenter of which yields the best published results on the ATB corpus.", "labels": [], "entities": [{"text": "ATB corpus", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.9790416955947876}]}, {"text": "In addition, we compare against the Morfessor Categories-MAP system.", "labels": [], "entities": []}, {"text": "Similar to our model, their system uses latent variables to induce clustering over morphemes.", "labels": [], "entities": []}, {"text": "The difference is in the nature of the clustering: the Morfessor algorithm associates a latent variable for each morpheme, grouping morphemes into four broad categories but not introducing dependencies between affixes directly.", "labels": [], "entities": []}, {"text": "For both systems, we quote their performance reported by.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the Arabic Treebank (ATB) data  set: We compare our models against Poon et al.", "labels": [], "entities": [{"text": "Arabic Treebank (ATB) data  set", "start_pos": 25, "end_pos": 56, "type": "DATASET", "confidence": 0.9503714101655143}]}, {"text": " Table 2: Segmentation performance on words that begin  with prefix \"Al\" (determiner) and end with suffix \"At\"  (plural noun suffix). The mean F1 scores are computed  using all boundaries of words in this set. For each word,  we also determine if both affixes are recovered while ig- noring any other boundaries between them. The other  two columns report this accuracy at both the type-level  and the token-level.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9755144715309143}, {"text": "accuracy", "start_pos": 361, "end_pos": 369, "type": "METRIC", "confidence": 0.9984129667282104}]}, {"text": " Table 3: Segmentation performance on words that have  the same final suffix as their preceding words. The F1  scores are computed based on all boundaries within the  words, but the accuracies are obtained using only the final  suffixes.", "labels": [], "entities": [{"text": "F1  scores", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.977726936340332}]}]}