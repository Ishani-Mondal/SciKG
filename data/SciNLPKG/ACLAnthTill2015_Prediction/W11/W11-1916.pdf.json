{"title": [{"text": "Mention Detection: Heuristics for the OntoNotes annotations", "labels": [], "entities": [{"text": "Mention Detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6594416797161102}]}], "abstractContent": [{"text": "Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.6935108453035355}]}, {"text": "Including exact matching mention detection in this shared task added anew and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method.", "labels": [], "entities": [{"text": "exact matching mention detection", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6573337018489838}]}, {"text": "We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.8696647882461548}]}, {"text": "These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B 3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively , and a final task score of 47.10.", "labels": [], "entities": [{"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.7844992876052856}, {"text": "MUC", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.9738444089889526}, {"text": "final task score", "start_pos": 167, "end_pos": 183, "type": "METRIC", "confidence": 0.8957032163937887}]}], "introductionContent": [{"text": "Coreference resolution is concerned with identifying mentions of entities in text and determining which mentions are referring to the same entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9227915406227112}]}, {"text": "Previously the focus in the field has been on the latter task.", "labels": [], "entities": []}, {"text": "Typically, mentions were considered correct if their span was within the true span of a gold mention, and contained the headword.", "labels": [], "entities": []}, {"text": "This task) has set a harder challenge by only considering exact matches to be correct.", "labels": [], "entities": []}, {"text": "Our system uses an unsupervised approach based on a generative model.", "labels": [], "entities": []}, {"text": "Unlike previous work, we did not use the Bllip or Wikipedia data described in.", "labels": [], "entities": [{"text": "Bllip or Wikipedia data", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.7278312146663666}]}, {"text": "This was necessary for the system to be eligible for the closed task.", "labels": [], "entities": []}, {"text": "The system detects mentions by finding the maximal projection of every noun and pronoun.", "labels": [], "entities": []}, {"text": "For the OntoNotes corpus this approach posed several problems.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.7492205500602722}]}, {"text": "First, the annotation scheme explicitly rejects noun phrases in certain constructions.", "labels": [], "entities": []}, {"text": "And second, it includes coreference for events as well as things.", "labels": [], "entities": []}, {"text": "In preliminary experiments on the development set, we found that spurious mentions were our primary source of error.", "labels": [], "entities": []}, {"text": "Using an oracle to exclude all spurious mentions at evaluation time yielded improvements ranging from five to thirty percent across the various metrics used in this task.", "labels": [], "entities": []}, {"text": "Thus, we decided to focus our efforts on methods for detecting and filtering spurious mentions.", "labels": [], "entities": [{"text": "detecting and filtering spurious mentions", "start_pos": 53, "end_pos": 94, "type": "TASK", "confidence": 0.7422059237957}]}, {"text": "To improve mention detection, we filtered mentions both before and after coreference resolution.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7165942639112473}, {"text": "coreference resolution", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8966515362262726}]}, {"text": "Filters prior to coreference resolution were constructed based on the annotation scheme and particular cases that should never be mentions (e.g. single word spans with the EX tag).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.9739956855773926}]}, {"text": "Filters after coreference resolution were constructed based on analysis of common errors on the development set.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.9553633332252502}]}, {"text": "These changes led to considerable improvement in mention detection precision.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.9683919250965118}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9524462819099426}]}, {"text": "The heuristics used in post-resolution filtering had a significant negative impact on recall, but this cost was out-weighed by the improvements in precision.", "labels": [], "entities": [{"text": "post-resolution filtering", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6486393362283707}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9984455704689026}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9985207915306091}]}, {"text": "Overall, the use of these filters led to a significant improvement in F 1 across all the coreference resolution evaluation metrics considered in the task., and briefly below.", "labels": [], "entities": [{"text": "F 1", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9925563633441925}, {"text": "coreference resolution evaluation", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.8944620887438456}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Mention detection performance with various  subsets of the filters.", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8017455637454987}]}, {"text": " Table 2: Precision for coreference resolution on the dev  set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9914506077766418}, {"text": "coreference resolution", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.9797084927558899}]}, {"text": " Table 3: Recall for coreference resolution on the dev set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9610806703567505}, {"text": "coreference resolution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.9833460450172424}]}, {"text": " Table 4: F 1 scores for coreference resolution on the dev  set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.986330658197403}, {"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.968945175409317}]}, {"text": " Table 5: Complete results on the test set", "labels": [], "entities": []}]}