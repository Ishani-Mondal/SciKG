{"title": [{"text": "Optimising Natural Language Generation Decision Making For Situated Dialogue", "labels": [], "entities": [{"text": "Optimising Natural Language Generation Decision Making", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8648554583390554}]}], "abstractContent": [{"text": "Natural language generators are faced with a multitude of different decisions during their generation process.", "labels": [], "entities": []}, {"text": "We address the joint opti-misation of navigation strategies and referring expressions in a situated setting with respect to task success and human-likeness.", "labels": [], "entities": []}, {"text": "To this end, we present a novel, comprehensive framework that combines supervised learning, Hierarchical Reinforcement Learning and a hierarchical Information State.", "labels": [], "entities": []}, {"text": "A human evaluation shows that our learnt instructions are rated similar to human instructions, and significantly better than the supervised learning baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) systems are typically faced with a multitude of decisions during their generation process due to nondeterminacy between a semantic input to a generator and its realised output.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.759292314449946}]}, {"text": "This is especially true in situated settings, where sudden changes of context can occur at anytime.", "labels": [], "entities": []}, {"text": "Sources of uncertainty include (a) the situational context, such as visible objects, or task complexity, (b) the user, including their behaviour and reactions, and (c) the dialogue history, including shared knowledge or patterns of linguistic consistency () and alignment ().", "labels": [], "entities": []}, {"text": "Previous work on context-sensitive generation in situated domains includes and . Stoia et al. present a supervised learning approach for situated referring expression generation (REG).", "labels": [], "entities": [{"text": "context-sensitive generation", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.7032274156808853}, {"text": "situated referring expression generation (REG)", "start_pos": 137, "end_pos": 183, "type": "TASK", "confidence": 0.7502058148384094}]}, {"text": "Garoufi and Koller use techniques from AI planning for the combined generation of navigation instructions and referring expressions (RE).", "labels": [], "entities": [{"text": "navigation instructions and referring expressions (RE)", "start_pos": 82, "end_pos": 136, "type": "TASK", "confidence": 0.5941809788346291}]}, {"text": "More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers, corpus-based statistical knowledge, tree-based stochastic models, maximum entropybased ranking), combinatorial pattern discovery), instance-based ranking, chart generation), planning, or probabilistic generation spaces to name just a few.", "labels": [], "entities": [{"text": "combinatorial pattern discovery", "start_pos": 246, "end_pos": 277, "type": "TASK", "confidence": 0.7185514767964681}, {"text": "chart generation", "start_pos": 304, "end_pos": 320, "type": "TASK", "confidence": 0.7454592883586884}]}, {"text": "More recently, there have been several approaches towards using Reinforcement Learning (RL) ( or Hierarchical Reinforcement Learning (HRL) for NLG decision making.", "labels": [], "entities": [{"text": "NLG decision making", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.8192702333132426}]}, {"text": "All of these approaches have demonstrated that HRL/RL offers a powerful mechanism for learning generation policies in the absence of complete knowledge about the environment or the user.", "labels": [], "entities": [{"text": "HRL/RL", "start_pos": 47, "end_pos": 53, "type": "TASK", "confidence": 0.7041521469751993}]}, {"text": "It overcomes the need for large amounts of handcrafted knowledge or data in rule-based or supervised learning accounts.", "labels": [], "entities": []}, {"text": "On the other hand, RL can have difficulties to find an optimal policy in a large search space, and is therefore often limited to small-scale applications.", "labels": [], "entities": [{"text": "RL", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9805801510810852}]}, {"text": "Pruning the search space of a learning agent by including prior knowledge is therefore attractive, since it finds solutions faster, reduces computational demands, incorporates expert knowledge, and scales to complex problems.", "labels": [], "entities": []}, {"text": "Sug-gestions to use such prior knowledge include and, who hand-craft rules of prior knowledge obvious to the system designer.", "labels": [], "entities": [{"text": "Sug-gestions", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9198850989341736}]}, {"text": "suggests using Hierarchical Abstract Machines to partially prespecify dialogue strategies, and Heeman (2007) uses a combination of RL and Information State (IS) to also pre-specify dialogue strategies.", "labels": [], "entities": []}, {"text": "presents an approach of combining PartiallyObservable Markov Decision Processes with conventional dialogue systems.", "labels": [], "entities": []}, {"text": "The Information State approach is well-established in dialogue management (e.g., and).", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7292764186859131}]}, {"text": "It allows the system designer to specify dialogue strategies in a principled and systematic way.", "labels": [], "entities": []}, {"text": "A disadvantage is that random design decisions need to be made in cases where the best action, or sequence of actions, is not obvious.", "labels": [], "entities": []}, {"text": "The contribution of this paper consists in a comprehensive account of constrained Hierarchical Reinforcement Learning through a combination with a hierarchical Information State (HIS), which is informed by prior knowledge induced from decision trees.", "labels": [], "entities": []}, {"text": "We apply our framework to the generation of navigation strategies and referring expressions in a situated setting, jointly optimised for task success and linguistic consistency.", "labels": [], "entities": []}, {"text": "An evaluation shows that humans prefer our learnt instructions to the supervised learning-based instructions, and rate them equal to human instructions.", "labels": [], "entities": []}, {"text": "Simulation-based results show that our semi-learnt approach learns more quickly than the fully-learnt baseline, which makes it suitable for large and complex problems.", "labels": [], "entities": []}, {"text": "Our approach differs from Heeman's in that we transfer it to NLG and to a hierarchical setting.", "labels": [], "entities": []}, {"text": "Although Heeman was able to show that his combined approach learns more quickly than pure RL, it is limited to small-scale systems.", "labels": [], "entities": []}, {"text": "Our 'divide-and-conquer' approach, on the other hand, scales up to large search spaces and allows us to address complex problems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We asked 11 participants 12 to rate altogether 132 sets of instructions, where each set contained a spatial graphical scene containing a person, mapped with one human, one learnt, and one supervised 6 female, 5 male with an age average of 26.4.", "labels": [], "entities": []}, {"text": "Instructions consisted of a navigation instruction followed by a referring expression.", "labels": [], "entities": []}, {"text": "Subjects were asked to rate instructions on a 1-5 Likert scale (where 5 is the best) for their helpfulness on guiding the displayed person from its origin to pressing the intended button.", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.9366456270217896}]}, {"text": "We selected six different scenarios for the evaluation: (a) only one button is present, (b) two buttons are present, the referent and a distractor of the same colour as the referent, (c) two buttons are present, the referent and a distractor of a different colour than the referent, (d) one micro landmark is present and one distractor of the same colour as the referent, (e) one micro landmark is present and one distractor of a different colour than the referent.", "labels": [], "entities": []}, {"text": "All scenarios oc- curred twice in each evaluation sheet, their specific instances were drawn from the GIVE-2 corpus at random.", "labels": [], "entities": [{"text": "GIVE-2 corpus", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.913772702217102}]}, {"text": "Scenes and instructions were presented in a randomised order.", "labels": [], "entities": []}, {"text": "presents an example evaluation scene.", "labels": [], "entities": []}, {"text": "Finally, we asked subjects to circle the object they thought was the intended referent.", "labels": [], "entities": []}, {"text": "Subjects rated the human instructions with an average of 3.82, the learnt instructions with an average of 3.55, and the supervised learning instructions with an average of 2.39.", "labels": [], "entities": []}, {"text": "The difference between human and learnt is not significant.", "labels": [], "entities": []}, {"text": "The difference between learnt and supervised learning is significant at p < 0.003, and the difference between human and supervised learning is significant at p < 0.0002.", "labels": [], "entities": []}, {"text": "In 96% of all cases, users were able to identify the intended referent.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Action set of the learning agents and Information States.", "labels": [], "entities": []}, {"text": " Table 2. The state-action space size of a flat learn- ing agent would be |S \u00d7 A| = 10 11 , the hierarchical  setting has a state-action space size of 2.4 \u00d7 10 7 .  The average state-action space size of all subtasks is  |S \u00d7 A|/14 = 1.7 \u00d7 10 7 . Generation actions can  be primitive or composite. While the former corre- spond to single generation decisions, the latter rep- resent separate generation subtasks", "labels": [], "entities": []}]}