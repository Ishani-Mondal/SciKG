{"title": [{"text": "Bayesian Extraction of Minimal SCFG Rules for Hierarchical Phrase-based Translation", "labels": [], "entities": [{"text": "Bayesian Extraction of Minimal SCFG", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.580408763885498}, {"text": "Hierarchical Phrase-based Translation", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.7473193804423014}]}], "abstractContent": [{"text": "We present a novel approach for extracting a minimal synchronous context-free grammar (SCFG) for Hiero-style statistical machine translation using a non-parametric Bayesian framework.", "labels": [], "entities": [{"text": "Hiero-style statistical machine translation", "start_pos": 97, "end_pos": 140, "type": "TASK", "confidence": 0.5201010555028915}]}, {"text": "Our approach is designed to extract rules that are licensed by the word alignments and heuristically extracted phrase pairs.", "labels": [], "entities": []}, {"text": "Our Bayesian model limits the number of SCFG rules extracted, by sampling from the space of all possible hierarchical rules; additionally our informed prior based on the lexical alignment probabilities biases the grammar to extract high quality rules leading to improved generalization and the automatic identification of commonly re-used rules.", "labels": [], "entities": []}, {"text": "We show that our Bayesian model is able to extract minimal set of hierarchical phrase rules without impacting the translation quality as measured by the BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9985803365707397}]}], "introductionContent": [{"text": "Hierarchical phrase-based (Hiero) machine translation has attracted significant interest within the Machine Translation community.", "labels": [], "entities": [{"text": "Hierarchical phrase-based (Hiero) machine translation", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7293839028903416}, {"text": "Machine Translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.83965003490448}]}, {"text": "It extends phrase-based translation by automatically inferring asynchronous grammar from an aligned bitext.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.8005297482013702}]}, {"text": "The synchronous context-free grammar links non-terminals in source and target languages.", "labels": [], "entities": []}, {"text": "Decoding in such systems employ a modified CKYparser that is integrated with a language model.", "labels": [], "entities": []}, {"text": "The primary advantage of Hiero-style systems lie in their unsupervised model of syntax for translation: allowing long-distance reordering and capturing certain syntactic constructions, particularly those that involve discontiguous phrases.", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9667114019393921}]}, {"text": "It has been demonstrated to be a successful framework with comparable performance with other statistical frameworks and suitable for large-scale corpora ().", "labels": [], "entities": []}, {"text": "However, one of the major difficulties in Hiero-style systems has been on learning a concise and general synchronous grammar from the bitext.", "labels": [], "entities": []}, {"text": "While most of the research in Hiero-style systems is focused on the improving the decoder, and in particular the link to the language model, comparatively few papers have considered the inference of the probabilistic SCFG from the word alignments.", "labels": [], "entities": []}, {"text": "A majority of the systems employ the classic ruleextraction algorithm) which extracts rules by replacing possible sub-spans (permitted by the word alignments) with a non-terminal and then using relative frequencies to estimate the probabilistic synchronous context-free grammar.", "labels": [], "entities": []}, {"text": "One of the issues in building Hiero-style systems is in managing the size of the synchronous grammar.", "labels": [], "entities": []}, {"text": "The original approach extracts a larger number of rules when compared to a phrase-based system on the same data leading to practical issues in terms of memory requirements and decoding speed.", "labels": [], "entities": []}, {"text": "Extremely large Hiero phrase tables may also lead to statistical issues, where the probability mass has to be shared by more rules: the probability p(e|f ) has to be shared by all the rules having the same source side string f , leading to fragmentation and resulting in many rules having very poor probability.", "labels": [], "entities": []}, {"text": "Approaches to improve the inference (the induction of the SCFG rules from the bitext) typically follows two streams.", "labels": [], "entities": []}, {"text": "One focusses on filtering the extracted hierarchical rules either by removing redundancy ( or by filtering rules based on certain patterns (, while the other stream is concerned about alternative approaches for learning the synchronous grammar (.", "labels": [], "entities": []}, {"text": "This paper falls under the latter category and we use a non-parametric Bayesian approach for rule extraction for Hiero-style systems.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.8481238782405853}]}, {"text": "Our objective in this paper is to provide a principled rule extraction method using a Bayesian framework that can extract the minimal SCFG rules without reducing the BLEU score.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7659761309623718}, {"text": "BLEU score", "start_pos": 166, "end_pos": 176, "type": "METRIC", "confidence": 0.9777456223964691}]}], "datasetContent": [{"text": "We use the English-Spanish data from WMT-10 shared task for the experiments to evaluate the effectiveness of our Bayesian rule extraction approach.", "labels": [], "entities": [{"text": "WMT-10 shared task", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.8360693454742432}, {"text": "Bayesian rule extraction", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.5675377448399862}]}, {"text": "We used the entire shared task training set except the UN data for training translation model and the language model was trained with the same set and an additional 2 million sentences from the UN data, using SRILM toolkit with Knesser-Ney discounting.", "labels": [], "entities": [{"text": "UN data", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.8138397634029388}, {"text": "training translation", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.5776400715112686}]}, {"text": "We tuned the feature weights on the WMT-10 devset using MERT and evaluate on the test set by computing lower-cased BLEU score) using the WMT-10 standard evaluation script.", "labels": [], "entities": [{"text": "WMT-10 devset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9762513637542725}, {"text": "MERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9194568991661072}, {"text": "BLEU score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9829760789871216}, {"text": "WMT-10 standard evaluation script", "start_pos": 137, "end_pos": 170, "type": "DATASET", "confidence": 0.937758594751358}]}, {"text": "We use Kriya -an in-house implementation of hierarchical phrase-based translation written predominantly in Python.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6864942312240601}]}, {"text": "Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning and LM integration.", "labels": [], "entities": [{"text": "SCFG rule extraction", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.8462042808532715}]}, {"text": "We use the 7 features (4 translation model features, extracted rules penalty, word penalty and language model) as is typical in Hiero-style systems.", "labels": [], "entities": []}, {"text": "For tuning the feature weights, we have adapted the MERT implementation in Moses 1 for use with Kriya as the decoder.", "labels": [], "entities": [{"text": "MERT implementation in Moses 1", "start_pos": 52, "end_pos": 82, "type": "DATASET", "confidence": 0.7147894740104676}]}, {"text": "We started by training and evaluating the two baseline systems using i) two non-terminals and ii) one non-terminal, which were trained using the conventional heuristic extraction approach.", "labels": [], "entities": []}, {"text": "For the baseline with one non-terminal, we modified the heuristic rule extraction algorithm appropriately 2 . As part of the baseline methods to be applied to minimize the number of SCFG rules, We also wanted to assess the effect of a simpler rule filtering, where the idea is to filter the heuristically extracted rules based on certain patterns.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7146116346120834}, {"text": "rule filtering", "start_pos": 243, "end_pos": 257, "type": "TASK", "confidence": 0.7199396193027496}]}, {"text": "Our first baseline filtering strategy uses the heuristic methods in in order to minimize the number of rules 3 . For the other baseline filtering experiments, we retained only one non-terminal rules and then further limited it by retaining only non-monotone one nonterminal rules; in both cases the terminal rules were retained.", "labels": [], "entities": []}, {"text": "shows the results for baseline and the rule filtering experiments.", "labels": [], "entities": [{"text": "rule filtering", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7507385015487671}]}, {"text": "Restricting rule extraction to just one non-terminal doesn't affect the BLEU score significantly and this justifies the simpler model used in this paper.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7395513653755188}, {"text": "BLEU score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9812223017215729}]}, {"text": "Secondly, we find significant reduction in the BLEU for the pattern-based filtering strategy and this is because we only use the initial rule set obtained by greedy filtering without augmenting it with other specific patterns.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9995179176330566}]}, {"text": "The other two filtering methods reduced the BLEU further but not significantly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.996100664138794}]}, {"text": "The second column in the table gives the number of SCFG rules filtered for the devset, which is typically much less than the full set of rules.", "labels": [], "entities": []}, {"text": "We later use this to put in perspective the effective reduction in the model size achieved by our Bayesian model.", "labels": [], "entities": []}, {"text": "We can ideally compare our Bayesian rule extraction using Gibbs sampling with straints relating to two non-terminals (such as, no adjacent nonterminals in source side) does not apply for the one non-terminal case.", "labels": [], "entities": [{"text": "Bayesian rule extraction", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6229007939497629}]}, {"text": "3 It should be noted that we didn't use the augmentations to the initial rule set () and our objective is to find the impact of the filtering approaches.", "labels": [], "entities": []}, {"text": "the baselines and the filtering approaches.", "labels": [], "entities": []}, {"text": "However, running our Gibbs sampler on the full set of phrase pairs demand sampling to be distributed, possibly with approximation (?; ?), which we reserve for our future work.", "labels": [], "entities": []}, {"text": "In this work, we focus on evaluating our Gibbs sampler on reasonable sized set of phrase pairs with corresponding baselines.", "labels": [], "entities": []}, {"text": "We filter the initial phrase pairs based on their frequency using three different thresholds, viz.", "labels": [], "entities": []}, {"text": "20, 10 and 3-resulting in smaller sets of initial phrase pairs because we throw out infrequent phrase pairs (the threshold-20 case is the smallest initial set of phrase pairs).", "labels": [], "entities": []}, {"text": "This allows us to run our sampler as a stand-alone instance for the three sets, obviating the need for distributed sampling.", "labels": [], "entities": []}, {"text": "shows the number of unique phrase pairs in each set.", "labels": [], "entities": []}, {"text": "While, the filtering reduces the number of phrase pairs to a small fraction of the total phrase pairs, it also increases the unknown words (OOV) in the test set by a factor between 1.8 and 3.", "labels": [], "entities": [{"text": "unknown words (OOV)", "start_pos": 125, "end_pos": 144, "type": "METRIC", "confidence": 0.6226763606071473}]}, {"text": "In order to address this issue due to the OOV words, we additionally added non-decomposable phrase pairs having just one word at either source or target side,: Effect of different priors and \u03b1 hon Threshold-20 set.", "labels": [], "entities": []}, {"text": "The two priors correspond to the lexical prior l x in the first step and the base distribution P 0 in the second step.", "labels": [], "entities": []}, {"text": "The coverage rules (about 1.8 million) were added separately to the SCFG rules induced by both heuristic algorithm and Gibbs sampler.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9528203010559082}]}, {"text": "This is justified because we only add the rules that cannot be decomposed further by both rule extraction approaches.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.7296897172927856}]}, {"text": "However, note that both approaches can independently induce rules that overlap with the coverage rules set and in such cases we simply add the original corpus count to the counts returned by the respective rule extraction method.", "labels": [], "entities": []}, {"text": "The Gibbs sampler considers the phrase pairs in random order at each iteration and induces SCFG rules by sampling a derivation for each phrase pair.", "labels": [], "entities": []}, {"text": "Given a phrase pair x with raw corpus frequency f x , we simply scale the count for its sampled derivation r by its frequency f x . Alternately, we also experimented with independently sampling for each instance of the phrase pair and found their performances to be comparable.", "labels": [], "entities": []}, {"text": "Sampling phrase pairs once and then scaling the sampled derivation, help us to speedup the sampling process.", "labels": [], "entities": []}, {"text": "In our experiments, we ran the Gibbs sampler for 2000 iterations with a burn-in period of 200, collecting counts every 50 iterations.", "labels": [], "entities": []}, {"text": "We set the concentration parameter \u03b1 h to be 0.5 based on our experiments detailed later in this section.", "labels": [], "entities": [{"text": "concentration parameter \u03b1 h", "start_pos": 11, "end_pos": 38, "type": "METRIC", "confidence": 0.8198259919881821}]}, {"text": "The BLEU scores for the SCFG learned from the Gibbs sampler are shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9994171857833862}]}, {"text": "We first note that, the threshold-20 set has lower baseline BLEU than threshold-10 and threshold-3 sets, as can be expected because threshold-20 set uses a much smaller subset of the full set of phrase pairs to extract hierarchical rules.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9903709292411804}]}, {"text": "The Bayesian approach results in a maximum BLEU score reduction of 1.6 for the sets using thresholds 10 and 3, compared to the one nonterminal baseline.", "labels": [], "entities": [{"text": "BLEU score reduction", "start_pos": 43, "end_pos": 63, "type": "METRIC", "confidence": 0.9876572887102762}]}, {"text": "The two non-terminal baseline is also provided to place our results in perspective.", "labels": [], "entities": []}, {"text": "shows the model size, including the coverage rules for the two rule extraction approaches.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7892755270004272}]}, {"text": "The number of extracted rules, excluding the coverage rules are shown within the parenthesis.", "labels": [], "entities": [{"text": "coverage", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8984493613243103}]}, {"text": "The last column shows the reduction in the model size for both with and without the coverage rules; yielding a maximum absolute reduction of 67.17% for the threshold-3 phrase pairs set.", "labels": [], "entities": []}, {"text": "It can be seen that the number of rules are far fewer than the rules extracted using the baseline heuristic methods for filtering detailed in.", "labels": [], "entities": []}, {"text": "Interestingly, we obtain a smaller model size, even as we decrease the threshold to include more initial phrase pairs used as input to the inference procedure, e.g. a 67.17% reduction over the rules extracted from the threshold-3 phrase pairs v.s. a 27.7% reduction for threshold-10.", "labels": [], "entities": []}, {"text": "These results show that our model is capable of extracting high-value Hiero-style SCFG rules, albeit with a reduction in the BLEU score.", "labels": [], "entities": [{"text": "Hiero-style SCFG", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.5284440815448761}, {"text": "BLEU score", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9786407053470612}]}, {"text": "However, our current approach offers scope for improvement in several avenues, for example we can use annealing to perturb the initial sampling iterations to encourage the Gibbs sampler to explore several derivations for each phrase pair.", "labels": [], "entities": []}, {"text": "Though this might result in slightly large models than the current ones, we still expect substantial reduction than the original Hiero rule extraction.", "labels": [], "entities": [{"text": "Hiero rule extraction", "start_pos": 129, "end_pos": 150, "type": "DATASET", "confidence": 0.7800208528836569}]}, {"text": "In future, we also plan to sample the hyperparameter \u03b1 h , instead of using a fixed value.", "labels": [], "entities": []}, {"text": "shows the effect of different values of the concentration parameter \u03b1 hand the priors used in the model.", "labels": [], "entities": []}, {"text": "The order of priors in each setting correspond to the prior used in deciding the ruletype and identifying the non-terminal span for sampling a derivation.", "labels": [], "entities": []}, {"text": "We found the geometric mean to work better in both cases.", "labels": [], "entities": []}, {"text": "We further found that the concentration parameter \u03b1 h value 0.5 gives the best BLEU score.", "labels": [], "entities": [{"text": "concentration parameter \u03b1 h", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.7876229584217072}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9994329810142517}]}], "tableCaptions": [{"text": " Table 1: Kriya: Baseline and Filtering experiments.  \u2020: This is the initial rule set used in Iglesias et al. (2009) obtained  by greedy filtering. Rows 4 and 5 represents the filtering that uses single non-terminal rules with row 4 allowing  monotone rules in addition to the non-monotone (reordering) rules.", "labels": [], "entities": []}, {"text": " Table 2: Phrase-pair statistics for different frequency  threshold", "labels": [], "entities": [{"text": "Phrase-pair", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9237514138221741}]}, {"text": " Table 3: BLEU scores: Heuristic vs Bayesian rule extraction", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982336759567261}]}, {"text": " Table 4: Model compression: Heuristic vs Bayesian rule extraction", "labels": [], "entities": [{"text": "Model compression", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.780275285243988}]}]}