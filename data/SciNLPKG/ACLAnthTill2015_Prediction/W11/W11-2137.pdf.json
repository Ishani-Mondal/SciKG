{"title": [{"text": "RegMT System for Machine Translation, System Combination, and Evaluation", "labels": [], "entities": [{"text": "RegMT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7078131437301636}, {"text": "Machine Translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8312548398971558}, {"text": "Evaluation", "start_pos": 62, "end_pos": 72, "type": "TASK", "confidence": 0.48690342903137207}]}], "abstractContent": [{"text": "We present the results we obtain using our RegMT system, which uses transductive regression techniques to learn mappings between source and target features of given parallel corpora and use these mappings to generate machine translation outputs.", "labels": [], "entities": [{"text": "RegMT", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8380460739135742}]}, {"text": "Our training instance selection methods perform feature decay for proper selection of training instances , which plays an important role to learn correct feature mappings.", "labels": [], "entities": []}, {"text": "RegMT uses L 2 regularized regression as well as L 1 regular-ized regression for sparse regression estimation of target features.", "labels": [], "entities": [{"text": "RegMT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9141906499862671}]}, {"text": "We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F 1 measure over target features as a metric for evaluating translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9716582894325256}, {"text": "RegMT", "start_pos": 152, "end_pos": 157, "type": "DATASET", "confidence": 0.619598925113678}, {"text": "F 1 measure", "start_pos": 195, "end_pos": 206, "type": "METRIC", "confidence": 0.969627857208252}]}], "introductionContent": [{"text": "Regression can be used to find mappings between the source and target feature sets derived from given parallel corpora.", "labels": [], "entities": []}, {"text": "Transduction learning uses a subset of the training examples that are closely related to the test set without using the model induced by the full training set.", "labels": [], "entities": [{"text": "Transduction learning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9462049007415771}]}, {"text": "In the context of statistical machine translation, translations are performed at the sentence level and this enables us to select a small number of training instances for each test instance to guide the translation process.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6345988015333811}]}, {"text": "This also gives us a computational advantage when considering the high dimensionality of the problem as each sentence can be mapped to many features.", "labels": [], "entities": []}, {"text": "The goal in transductive regression based machine translation (RegMT) is both reducing the computational burden of the regression approach by reducing the dimensionality of the training set and the feature set and also improving the translation quality by using transduction.", "labels": [], "entities": [{"text": "transductive regression based machine translation (RegMT)", "start_pos": 12, "end_pos": 69, "type": "TASK", "confidence": 0.7762048169970512}]}, {"text": "We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F 1 measure over target features as a metric for evaluating translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9716582894325256}, {"text": "RegMT", "start_pos": 152, "end_pos": 157, "type": "DATASET", "confidence": 0.619598925113678}, {"text": "F 1 measure", "start_pos": 195, "end_pos": 206, "type": "METRIC", "confidence": 0.969627857208252}]}, {"text": "RegMT work builds on our previous regression-based machine translation results) especially with instance selection and additional graph decoding capability.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.6975702941417694}, {"text": "instance selection", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7407583594322205}]}, {"text": "We present our results to this year's challenges.", "labels": [], "entities": []}, {"text": "Outline: Section 2 gives an overview of the RegMT model.", "labels": [], "entities": [{"text": "RegMT", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.7335836887359619}]}, {"text": "In section 3, we present our training instance selection techniques and WMT'11 results.", "labels": [], "entities": [{"text": "WMT'11", "start_pos": 72, "end_pos": 78, "type": "TASK", "confidence": 0.3398209512233734}]}, {"text": "In section 4, we present the graph decoding results on the Haitian Creole-English translation task.", "labels": [], "entities": [{"text": "Haitian Creole-English translation task", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.6316046863794327}]}, {"text": "Section 5 presents our system combination results using reranking with the RegMT score.", "labels": [], "entities": [{"text": "RegMT score", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.6692079305648804}]}, {"text": "Section 6 evaluates the F 1 measure that we use for the automatic evaluation metrics challenge.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9775470693906149}]}, {"text": "The last section present our contributions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have used FDA and dice algorithms to select training sets for the out-of-domain challenge test sets used in).", "labels": [], "entities": []}, {"text": "The parallel corpus contains about 1.9 million training sentences and the test set contain 3003 sentences.", "labels": [], "entities": []}, {"text": "We built separate Moses systems using all of the parallel corpus for the language pairs en-de, de-en, enes, and es-en.", "labels": [], "entities": []}, {"text": "We created training sets using all en-de de-en en-es es-en: Performance for the out-of-domain task of).", "labels": [], "entities": []}, {"text": "ALL corresponds to the baseline system using all of the parallel corpus.", "labels": [], "entities": []}, {"text": "words list the size of the target words used in millions. of the features of the test set to select training instances.", "labels": [], "entities": []}, {"text": "The results given in show that we can achieve similar BLEU performance using about 7% of the parallel corpus target words (200,000 instances) using dice and about 16% using FDA.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9957755208015442}]}, {"text": "In the out-of-domain translation task, we are able to reduce the training set size to achieve a performance close to the baseline.", "labels": [], "entities": [{"text": "out-of-domain translation task", "start_pos": 7, "end_pos": 37, "type": "TASK", "confidence": 0.7193654974301656}]}, {"text": "We maybe able to achieve better performance in this out-of-domain task as well as explained in (Bicici and Yuret, 2011).", "labels": [], "entities": []}, {"text": "We use target sentence F 1 measure over the target features as a translation performance evaluation metric.", "labels": [], "entities": []}, {"text": "We optimize the parameters of the RegMT model with the F 1 measure comparing the target vector with the estimate we get from the RegMT model.", "labels": [], "entities": [{"text": "RegMT", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8488146662712097}, {"text": "F 1 measure", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9706660310427347}, {"text": "RegMT", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.9215115308761597}]}, {"text": "where BER is the balanced error rate, precis precision, and rec is recall.", "labels": [], "entities": [{"text": "BER", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9995902180671692}, {"text": "balanced error rate", "start_pos": 17, "end_pos": 36, "type": "METRIC", "confidence": 0.7671905557314554}, {"text": "precis", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9464059472084045}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.6839028000831604}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9980387091636658}]}, {"text": "The evaluation techniques measure the effectiveness of the learning models in identifying the features of the target sentence making minimal error to increase the performance of the decoder and its translation quality.", "labels": [], "entities": []}, {"text": "We use gapped word sequence kernels () when using F 1 for evaluating translations since a given translation system may not be able to translate a given word but can correctly identify the surrounding phrase.", "labels": [], "entities": []}, {"text": "For instance, let the reference translation be the following sentence: a sound compromise has been reached Some possible translations for the reference are given in together with their BLEU () and F 1 scores for comparison.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9990473389625549}, {"text": "F 1", "start_pos": 197, "end_pos": 200, "type": "METRIC", "confidence": 0.9438793361186981}]}, {"text": "F 1 score does not have a brevity penalty but a brief translation is penalized by a low recall value.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9654694398244222}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.999138593673706}]}, {"text": "We use up to 3 tokens as gaps.", "labels": [], "entities": []}, {"text": "F 1 measure is able to increase the ranking of Trans 4 by using a gapped sequence kernel, which can be preferrable to Trans 3 . We note that a missing token corresponds to varying decreases in the n-gram precision used in the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 226, "end_pos": 236, "type": "METRIC", "confidence": 0.96450275182724}]}, {"text": "A sentence containing m tokens has m 1-grams, m \u2212 1 2-grams, and m \u2212 n + 1 n-grams.", "labels": [], "entities": []}, {"text": "A missing token degrades the performance more in higher order n-gram precision values.", "labels": [], "entities": []}, {"text": "A missing token decreases n-gram precision by 1 m for 1-grams and by n m\u2212n+1 for n-grams.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9847285151481628}]}, {"text": "Based on this observation, we use F 1 measure with gapped word sequence kernels to evaluate translations.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9538284540176392}]}, {"text": "Gapped features allows us to consider the surrounding phrase fora missing token as present in the translation.", "labels": [], "entities": []}, {"text": "Let the reference sentence be represented with ab c d e f where a-f, x, y, z correspond to tokens in the sentence.", "labels": [], "entities": []}, {"text": "Then, Trans 3 has the form ab x y f, and Trans 4 has the form ac y f.", "labels": [], "entities": []}, {"text": "Then, F 1 ranks Trans 4 higher than Trans 3 for orders greater than 3 as there are two consecutive word errors in Trans 3 . F 1 can also prefer a missing token rather than a word error as we see by comparing Trans and Trans 5 and it can still prefer contiguity over a gapped sequence as we see by comparing Trans 5 and Trans 6 in.", "labels": [], "entities": []}, {"text": "We calculate the correlation of F 1 with BLEU on the en-de development set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9837064146995544}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9976275563240051}]}, {"text": "We use 5-grams with the F 1 measure as this increases the correlation with 4-gram BLEU.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9779815872510275}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.995749831199646}]}, {"text": "gives the correlation results using both Pearson's correlation score and Spearman's correlation score.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9755095839500427}, {"text": "Pearson's correlation score", "start_pos": 41, "end_pos": 68, "type": "METRIC", "confidence": 0.9294453561306}, {"text": "Spearman's correlation score", "start_pos": 73, "end_pos": 101, "type": "METRIC", "confidence": 0.6395624130964279}]}, {"text": "Spearman's correlation score is a better metric for comparing the relative orderings.", "labels": [], "entities": [{"text": "Spearman's correlation score", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.601679764688015}]}], "tableCaptions": [{"text": " Table 1: Performance for the out-of-domain task  of", "labels": [], "entities": []}, {"text": " Table 2: System combination results.", "labels": [], "entities": []}, {"text": " Table 3: BLEU vs. F 1 on sample sentence translation task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998979389667511}, {"text": "F 1", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9452629387378693}, {"text": "sample sentence translation task", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.668825700879097}]}]}