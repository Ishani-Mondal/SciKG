{"title": [{"text": "The Role of Information Extraction in the Design of a Document Triage Application for Biocuration", "labels": [], "entities": []}], "abstractContent": [{"text": "Traditionally, automated triage of papers is performed using lexical (unigram, bigram, and sometimes trigram) features.", "labels": [], "entities": [{"text": "automated triage of papers", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7187496870756149}]}, {"text": "This paper explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag-of-words models.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.8722912549972535}]}, {"text": "Our classifier includes lexico-syntactic patterns and more-complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category.", "labels": [], "entities": []}, {"text": "Our experimental results show that the IE-based features can improve performance over unigram and bigram features alone.", "labels": [], "entities": []}, {"text": "We present intrinsic evaluation results of full-text document classification experiments to determine automatically whether a paper should be considered of interest to biologists at the Mouse Genome Informatics (MGI) system at the Jackson Laboratories.", "labels": [], "entities": [{"text": "full-text document classification", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6823428074518839}]}, {"text": "We also further discuss issues relating to design and deployment of our classifiers as an application to support scientific knowledge cu-ration at MGI.", "labels": [], "entities": [{"text": "MGI", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.9550189971923828}]}], "introductionContent": [{"text": "A long-standing promise of Biomedical Natural Language Processing is to accelerate the process of literature-based 'biocuration', where published information must be carefully and appropriately translated into the knowledge architecture of a biomedical database.", "labels": [], "entities": [{"text": "Biomedical Natural Language Processing", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.8831809312105179}]}, {"text": "Typically, biocuration is a manual activity, performed by specialists with expertise in both biomedicine and the computational representation of the target database.", "labels": [], "entities": []}, {"text": "It is widely acknowledged as a vital lynch-pin of biomedical informatics.", "labels": [], "entities": []}, {"text": "A key step in biocuration is the initial triage of documents in order to direct to specialists only the documents appropriate for them.", "labels": [], "entities": []}, {"text": "This classification ()(Hersh W, 2005) can be followed by a step in which desired information is extracted and appropriately standardized and formalized for entry into the database.", "labels": [], "entities": []}, {"text": "Both these steps can be enhanced by suitably powerful Natural Language Processing (NLP) technology.", "labels": [], "entities": []}, {"text": "In this paper, we address text mining as a step within the broader context of developing both infrastructure and tools for biocuration support within the Mouse Genome Informatics (MGI) system at the Jackson Laboratories.", "labels": [], "entities": [{"text": "text mining", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8059746623039246}]}, {"text": "We previously identified 'document triage' as a crucial bottleneck () within MGI's biocuration workflow.", "labels": [], "entities": [{"text": "document triage'", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7657164831956228}, {"text": "MGI", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.6916053891181946}]}, {"text": "Our research explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag-of-words models.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.8728334903717041}]}, {"text": "These features are employed by a classifier to perform the triage step.", "labels": [], "entities": []}, {"text": "The features include lexicosyntactic patterns as well as more-complex features, such as a pattern coupled with its extracted noun, where the noun is represented both as a lexical term and by its semantic category.", "labels": [], "entities": []}, {"text": "Our experimental results show that the IE-based enhanced features can improve performance over unigram and bigram features alone.", "labels": [], "entities": []}, {"text": "Evaluating the performance of BioNLP tools is not trivial.", "labels": [], "entities": []}, {"text": "So-called intrinsic metrics measure the performance of a tool against some gold standard of performance, while extrinsic ones ( measure how much the overall biocuration process is benefited.", "labels": [], "entities": []}, {"text": "Such metrics necessarily involve the deployment of the software in-house for testing by biocurators, and require a large-scale softwareengineering infrastructure effort.", "labels": [], "entities": []}, {"text": "In this paper, we present intrinsic evaluation results of full-text document classification experiments to determine automatically whether a paper should be considered of interest to MGI curators.", "labels": [], "entities": [{"text": "full-text document classification", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.6308315396308899}]}, {"text": "We plan in-house deployment and extrinsic evaluation in near-term work.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8176188170909882}]}, {"text": "Our work should be considered as the first step of a broader process within which (a) the features used in this particular classification approach will be reengineered so that they maybe dynamically recreated in any new domain by a reusable component, (b) this component is deployed into reusable infrastructure that also includes document-, annotationand feature-storage capabilities that support scaling and reuse, and (c) the overall functionality can then be delivered as a software application to biocurators themselves for extrinsic evaluation in any domain they choose.", "labels": [], "entities": []}, {"text": "Within the 'SciKnowMine' project, we are constructing such a framework (, and this work reported here forms a prototype component that we plan to incorporate into a live application.", "labels": [], "entities": []}, {"text": "We describe the underlying NLP research here, and provide context for the work by describing the overall design and implementation of the SciKnowMine infrastructure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We randomly partitioned our text corpus into 5 subsets of 2,965 documents each.", "labels": [], "entities": []}, {"text": "We used the first 4 subsets as the training set, and reserved the fifth subset as a blind test set.", "labels": [], "entities": []}, {"text": "In preliminary experiments, we found that the classifiers consistently benefitted from feature selection when we discarded low-frequency features.", "labels": [], "entities": []}, {"text": "This helps to keep the classifier from overfitting to the training data.", "labels": [], "entities": []}, {"text": "For each type of feature, we set a frequency threshold \u03b8 and discarded any features that occurred fewer than \u03b8 times in the training set.", "labels": [], "entities": []}, {"text": "We chose these \u03b8 values empirically by performing 4-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "We evaluated \u03b8 values ranging from 1 to 50, and chose the value that produced the highest F score.", "labels": [], "entities": [{"text": "F score", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9903489947319031}]}, {"text": "The \u03b8 values that were selected are: 7 for unigrams, 50 for bigrams, 35 for patterns, 50 for lexical extractions, and 5 for semantic extractions.", "labels": [], "entities": [{"text": "semantic extractions", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7159320563077927}]}, {"text": "Finally, we trained an SVM classifier on the entire training set and evaluated the classifier on the test set.", "labels": [], "entities": []}, {"text": "We computed Precision (P), Recall (R), and the F score, which is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9603542536497116}, {"text": "Recall (R)", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9683430939912796}, {"text": "F score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9940712749958038}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9994843006134033}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9951978325843811}]}, {"text": "Precision and recall were equally weighted, so this is sometimes called an F1 score.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9908393025398254}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9987198114395142}, {"text": "F1 score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9863404035568237}]}, {"text": "shows the results obtained by using each of the features in isolation.", "labels": [], "entities": []}, {"text": "The lexical extraction features are shown as 'lexExts' and the semantic extraction features are shown as 'semExts'.", "labels": [], "entities": []}, {"text": "We also experimented with using a hybrid extraction feature, 'hybridExts', which replaced a lexical extraction noun with its semantic category when one was available but left the noun as the extraction term when no semantic category was known.", "labels": [], "entities": []}, {"text": "shows that the bigram features produced the best Recall (65.87%) and F-Score (74.05%), while the hybrid extraction features produced the best Precision (85.52%) but could not match the bigrams in terms of recall.", "labels": [], "entities": [{"text": "Recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9917417764663696}, {"text": "F-Score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9987932443618774}, {"text": "Precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9544363617897034}, {"text": "recall", "start_pos": 205, "end_pos": 211, "type": "METRIC", "confidence": 0.9986456036567688}]}, {"text": "This is not surprising because the extraction features on their own are quite specific, often requiring 3-4 words to match.", "labels": [], "entities": []}, {"text": "Next, we experimented with adding the IE-based features to the bigram features to allow the classifier to choose among both feature sets and get the best of both worlds.", "labels": [], "entities": []}, {"text": "Combining bigrams with IE-based Our 5-way random split left 2 documents aside, which we ignored for our experiments.", "labels": [], "entities": []}, {"text": "features did in fact yield the best results.", "labels": [], "entities": []}, {"text": "Using bigrams and lexical extraction features achieved both the highest recall (66.83%) and the highest F score (74.93%).", "labels": [], "entities": [{"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9997195601463318}, {"text": "F score", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9943763613700867}]}, {"text": "In terms of overall F score, we see a relatively modest gain of about 1% by adding the lexical extraction features to the bigram features, which is primarily due to the 1% gain in recall.", "labels": [], "entities": [{"text": "F score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9907879531383514}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.995941162109375}]}, {"text": "However, precision is of paramount importance for many applications because users don't want to wade through incorrect predictions.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9992917776107788}]}, {"text": "So it is worth noting that adding the hybrid extraction features to the bigram features produced a 2.5% increase in precision (84.57% \u2192 87.10%) with just a 1% drop in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9996448755264282}, {"text": "recall", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9991796612739563}]}, {"text": "This recall/precision trade-off is likely to be worthwhile for many real-world application settings, including biocuration.", "labels": [], "entities": [{"text": "recall", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9989981055259705}, {"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9786916375160217}]}], "tableCaptions": [{"text": " Table 7: Triage classifier performance using different sets  of features.", "labels": [], "entities": [{"text": "Triage classifier", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9185469448566437}]}]}