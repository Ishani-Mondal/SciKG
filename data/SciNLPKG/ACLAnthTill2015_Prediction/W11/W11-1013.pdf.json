{"title": [{"text": "Combining statistical and semantic approaches to the translation of ontologies and taxonomies", "labels": [], "entities": []}], "abstractContent": [{"text": "Ontologies and taxonomies are widely used to organize concepts providing the basis for activities such as indexing, and as background knowledge for NLP tasks.", "labels": [], "entities": [{"text": "indexing", "start_pos": 106, "end_pos": 114, "type": "TASK", "confidence": 0.9797062277793884}]}, {"text": "As such, translation of these resources would prove useful to adapt these systems to new languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9641749262809753}]}, {"text": "However, we show that the nature of these resources is significantly different from the \"free-text\" paradigm used to train most statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 128, "end_pos": 159, "type": "TASK", "confidence": 0.6217390497525533}]}, {"text": "In particular , we see significant differences in the linguistic nature of these resources and such resources have rich additional semantics.", "labels": [], "entities": []}, {"text": "We demonstrate that as a result of these linguistic differences, standard SMT methods, in particular evaluation metrics, can produce poor performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9952378273010254}]}, {"text": "We then look to the task of leverag-ing these semantics for translation, which we approach in three ways: by adapting the translation system to the domain of the resource; by examining if semantics can help to predict the syntactic structure used in translation; and by evaluating if we can use existing translated taxonomies to disambiguate translations.", "labels": [], "entities": [{"text": "translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9631397128105164}]}, {"text": "We present some early results from these experiments , which shed light on the degree of success we may have with each approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Taxonomies and ontologies are data structures that organise conceptual information by establishing relations among concepts, hierarchical and partitive relations being the most important ones.", "labels": [], "entities": []}, {"text": "Nowadays, ontologies have a wide range of uses in many domains, for example, finance, bio-medicine () () and libraries.", "labels": [], "entities": []}, {"text": "These resources normally attach labels in natural language to the concepts and relations that define their structure, and these labels can be used fora number of purposes, such as providing user interface localization , multilingual data access, information extraction) and natural language generation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 246, "end_pos": 268, "type": "TASK", "confidence": 0.8500761091709137}, {"text": "natural language generation", "start_pos": 274, "end_pos": 301, "type": "TASK", "confidence": 0.6346971491972605}]}, {"text": "It seems natural that for applications that use such ontologies and taxonomies, translation of the natural language descriptions associated with them is required in order to adapt these methods to new languages.", "labels": [], "entities": []}, {"text": "Currently, there has been some work on this in the context of ontology localisation, such as and, , and.", "labels": [], "entities": []}, {"text": "However, this work has focused on the casein which exact or partial translations are found in other similar resources such as bilingual lexica.", "labels": [], "entities": []}, {"text": "Instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6886790792147318}]}, {"text": "Current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7560037970542908}, {"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6943025688330332}]}, {"text": "Such techniques have been extrapolated to the translation of taxonomies and ontologies, in which the \"context\" of a taxonomy or ontology label corresponds to the ontology structure that surrounds the label in question.", "labels": [], "entities": []}, {"text": "This structure, which is made up of the lexical information provided by labels and the semantic information provided by the ontology structure, defines the sense of the concept and can be exploited in the disambiguation process ().", "labels": [], "entities": []}], "datasetContent": [{"text": "Given the linguistic differences in taxonomy and ontology labels, it seems necessary to investigate the effectiveness of various metrics for the evaluation of translation quality.", "labels": [], "entities": []}, {"text": "There area number of metrics that are widely used for evaluating translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7553781867027283}]}, {"text": "Here we will focus on some of the most widely used, namely BLEU (), NIST), METEOR (Banerjee and) and WER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9980387091636658}, {"text": "NIST", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.5587975978851318}, {"text": "METEOR", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9888757467269897}, {"text": "WER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9748817682266235}]}, {"text": "However, it is not clear which of these methods correlate best with human evaluation, particularly for the ontologies with short labels.", "labels": [], "entities": []}, {"text": "To evaluate this we collected a mixture of ontologies with short labels on the topics of human diseases, agriculture, geometry and project management, producing 437 labels.", "labels": [], "entities": []}, {"text": "These were translated with web translation services from English to Spanish, in particu-  We then created a data set by mixing the translations from the web translation services with a number of translations from the source ontologies, to act as a control.", "labels": [], "entities": []}, {"text": "We then gave these translations to 3 evaluators, who scored them for adequacy and fluency as described in.", "labels": [], "entities": []}, {"text": "Finally, we calculated the Pearson correlation coefficient between the automatic scores and the manual scores obtained.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 27, "end_pos": 58, "type": "METRIC", "confidence": 0.9539555112520853}]}, {"text": "These are presented in table 3 and.", "labels": [], "entities": []}, {"text": "As we can see from these results, one metric, namely METEOR, seems to perform best in evaluating the quality of the translations.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9728087782859802}]}, {"text": "In fact this is not surprising as there is a clear mathematical deficiency that both NIST and BLEU have for evaluating translations for very short labels like the ones we have here.", "labels": [], "entities": [{"text": "NIST", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.931965172290802}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9967947602272034}]}, {"text": "To illustrate this, we recall the formulation of BLEU as given in (): Where BP is a brevity penalty, w n a weight value and p n represents the n-gram precision, indicating how many times a particular n-gram in the source text is found among the target translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9957311749458313}, {"text": "BP", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9959514141082764}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.8944429159164429}]}, {"text": "We note, however, that for very short labels it is highly likely that p n will be zero.", "labels": [], "entities": []}, {"text": "This creates a significant issue, as from the equation above, if any of the values of p n are zero, the overall score, BLEU, will also be zero.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9995203018188477}]}, {"text": "For the results above we chose N = 2, and corrected for single-word labels.", "labels": [], "entities": []}, {"text": "However, the scores were still significantly worse, similar problems affect the NIST metric.", "labels": [], "entities": [{"text": "NIST metric", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.9109919667243958}]}, {"text": "As such, for the taxonomy and ontology translation task we do not recommend using BLEU or NIST as an evaluation metric.", "labels": [], "entities": [{"text": "ontology translation", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.850449949502945}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9909981489181519}, {"text": "NIST", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.9067862629890442}]}, {"text": "We note that METEOR is a more sophisticated method than WER and, as expected, performs better.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9626720547676086}, {"text": "WER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.8498249650001526}]}], "tableCaptions": [{"text": " Table 1: Lexical Analysis of labels", "labels": [], "entities": [{"text": "Lexical Analysis of labels", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8706421107053757}]}, {"text": " Table 2: Number of translation for pages in Wikipedia", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8552474975585938}]}, {"text": " Table 3: Correlation between manual evaluation results  and automatic evaluation scores", "labels": [], "entities": []}, {"text": " Table 4: Results of domain-adapted translation.  *  Lower  WER scores are better", "labels": [], "entities": [{"text": "domain-adapted translation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.557371973991394}, {"text": "WER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9962888956069946}]}, {"text": " Table 5: Probability of syntactic relationship given a se- mantic relationship in IFRS labels", "labels": [], "entities": [{"text": "IFRS", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.814120888710022}]}, {"text": " Table 6: Probability of cross-lingual preservation of syntax given semantic relationship in IFRS. Note here s refers to  the source language and t to the target language. Error values are 95% of standard deviation.", "labels": [], "entities": [{"text": "cross-lingual preservation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7095850855112076}, {"text": "IFRS", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.712012767791748}, {"text": "Error", "start_pos": 172, "end_pos": 177, "type": "METRIC", "confidence": 0.9982740879058838}]}, {"text": " Table 7: Results of selecting translation by structural  comparison.  *  Lower WER scores are better", "labels": [], "entities": [{"text": "WER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9928243160247803}]}]}