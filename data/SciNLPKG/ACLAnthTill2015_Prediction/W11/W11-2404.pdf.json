{"title": [{"text": "Is it Worth Submitting this Run? Assess your RTE System with a Good Sparring Partner", "labels": [], "entities": [{"text": "RTE", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.6250218749046326}]}], "abstractContent": [{"text": "We address two issues related to the development of systems for Recognizing Textual Entailment.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.8760707378387451}]}, {"text": "The first is the impossibility to capitalize on lessons learned over the different datasets available, due to the changing nature of traditional RTE evaluation settings.", "labels": [], "entities": [{"text": "RTE evaluation", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.8938682079315186}]}, {"text": "The second is the lack of simple ways to assess the results achieved by our system on a given training corpus, and figure out its real potential on unseen test data.", "labels": [], "entities": []}, {"text": "Our contribution is the extension of an open-source RTE package with an automatic way to explore the large search space of possible configurations, in order to select the most promising one over a given dataset.", "labels": [], "entities": []}, {"text": "From the developers' point of view, the efficiency and ease of use of the system, together with the good results achieved on all previous RTE datasets, represent a useful support , providing an immediate term of comparison to position the results of their approach.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.8411953449249268}]}], "introductionContent": [{"text": "Research on textual entailment (TE) has received a strong boost by the Recognizing Textual Entailment (RTE) Challenges, organized yearly to gather the community around a shared evaluation framework.", "labels": [], "entities": [{"text": "textual entailment (TE)", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7696170568466186}, {"text": "Recognizing Textual Entailment (RTE) Challenges", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.6770472739424024}]}, {"text": "Within such framework, besides the intrinsic difficulties of the task (i.e. deciding, given a set of TextHypothesis pairs, if the hypotheses can be inferred from the meaning of the texts), the development of RTE systems has to confront with a number of additional problems and uncertainty factors.", "labels": [], "entities": [{"text": "RTE", "start_pos": 208, "end_pos": 211, "type": "TASK", "confidence": 0.960336446762085}]}, {"text": "First of all, since RTE systems are usually based on complex architectures that integrate a variety of tools and resources, it is per se very difficult to tune them and define the optimal configuration given anew dataset.", "labels": [], "entities": []}, {"text": "In general, when participating to the evaluation challenges there's no warranty that the submitted runs are those obtained with the best possible configuration allowed by the system.", "labels": [], "entities": []}, {"text": "Second, the evaluation settings change along the years.", "labels": [], "entities": []}, {"text": "Variations in the length of the texts, the origin of the pairs, the balance between positive and negative examples, and the type of entailment decisions allowed, reflect the need to move from easier and more artificial settings to more complex and natural ones.", "labels": [], "entities": []}, {"text": "However, in contrast with other more stable tasks in terms of evaluation settings and metrics (e.g. machine translation), such changes make it difficult to capitalize on the experience obtained by participants throughout the years.", "labels": [], "entities": [{"text": "machine translation)", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.8100880881150564}]}, {"text": "Third, looking at RTE-related literature and the outcomes of the six campaigns organised so far, the conclusions that can be drawn are often controversial.", "labels": [], "entities": [{"text": "RTE-related", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.8980824947357178}]}, {"text": "For instance, it is not clear whether the availability of larger amounts of training data correlates with better performance () or not (, even within the same evaluation setting.", "labels": [], "entities": []}, {"text": "In addition, ablation tests carried out in recent editions of the challenge do not allow for definite conclusions about the actual usefulness of tools and resources, even the most popular ones ().", "labels": [], "entities": [{"text": "ablation", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9907709360122681}]}, {"text": "Finally, the best performing systems often have different natures from one year to another, showing alternations of deep ( and shallow approaches () ranked at the top positions.", "labels": [], "entities": []}, {"text": "In light of these considerations, it would be useful for sys-tems developers to have: i) automatic ways to support systems' tuning at a training stage, and ii) reliable terms of comparison to validate their hypotheses, and position the results of their work before submitting runs for evaluation.", "labels": [], "entities": []}, {"text": "In this paper we address these needs by extending an open-source RTE package (EDITS 1 ) with a mechanism that automatizes the selection of the most promising configuration over a training dataset.", "labels": [], "entities": []}, {"text": "We prove the effectiveness of such extension showing that it allows not only to achieve good performance on all the available RTE Challenge datasets, but also to improve the official results, achieved with the same system, through ad hoc configurations manually defined by the developers team.", "labels": [], "entities": [{"text": "RTE Challenge datasets", "start_pos": 126, "end_pos": 148, "type": "DATASET", "confidence": 0.7952397465705872}]}, {"text": "On one side, in the spirit of the collaborative nature of open source projects, we extend an existing tool with a useful functionality that was still missing.", "labels": [], "entities": []}, {"text": "On the other side, we provide a good \"sparring partner\" for system developers, to be used as a fast and free term of comparison to position the results of their work.", "labels": [], "entities": []}, {"text": "2 \"Coping\" with configurability EDITS ( ) is an open source RTE package, which offers a modular, flexible, and adaptable working environment to experiment with the RTE task over different datasets.", "labels": [], "entities": [{"text": "EDITS", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.6421511173248291}, {"text": "RTE task", "start_pos": 164, "end_pos": 172, "type": "TASK", "confidence": 0.8732059001922607}]}, {"text": "The package allows to: i) create an entailment engine by defining its basic components (i.e. algorithms, cost schemes, rules, and optimizers); ii) train such entailment engine over an annotated RTE corpus to learn a model; and iii) use the entailment engine and the model to assign an entailment judgement and a confidence score to each pair of an un-annotated test corpus.", "labels": [], "entities": []}, {"text": "A key feature of EDITS is represented by its high configurability, allowed by the availability of different algorithms, the possibility to integrate different sets of lexical entailment/contradiction rules, and the variety of parameters for performance optimization (see also.", "labels": [], "entities": []}, {"text": "Although configurability is per se an important aspect (especially for an open-source and general purpose system), there is another side of the coin.", "labels": [], "entities": []}, {"text": "In principle, in order to select the most promising configuration over a given development set, one should exhaustively run a huge number of training/evaluation routines.", "labels": [], "entities": []}, {"text": "Such num-1 http://edits.fbk.eu/ ber corresponds to the total number of configurations allowed by the system, which result from the possible combinations of parameter settings.", "labels": [], "entities": []}, {"text": "When dealing with enlarging dataset sizes, and the tight time constraints usually posed by the evaluation campaigns, this problem becomes particularly challenging, as developers are hardly able to run exhaustive training/evaluation routines.", "labels": [], "entities": []}, {"text": "As recently shown by the EDITS developers team, such situation results in running a limited number of experiments with the most \"reasonable\" configurations, which consequently might not lead to the optimal solution ( . The need of a mechanism to automatically obtain the most promising solution on one side, and the constraints posed by the evaluation campaigns on the other side, arise the necessity to optimize this procedure.", "labels": [], "entities": []}, {"text": "Along this direction, the objective is good a trade-off between exhaustive experimentation with all possible configurations (unfeasible), and educated guessing (unreliable).", "labels": [], "entities": [{"text": "guessing", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.767325758934021}]}, {"text": "The remainder of this section tackles this issue introducing an optimization strategy based on genetic algorithms, and describing its adaptation to extend EDITS with the new functionality.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were carried out over the datasets used in the six editions of the RTE Challenge (\"Main\" task data from RTE1 to RTE6).", "labels": [], "entities": [{"text": "RTE Challenge", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.8911263644695282}, {"text": "RTE1", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.789543628692627}, {"text": "RTE6", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.7056207656860352}]}, {"text": "For each dataset we obtained the best model by training EDITS-GA over the development set, and evaluating the resulting model on the test pairs.", "labels": [], "entities": []}, {"text": "To this aim, the optimization process is iterated overall the available algorithms in order to select the best combination of parameters.", "labels": [], "entities": []}, {"text": "As termination criterion, we set to 20 the maximum number of iterations.", "labels": [], "entities": []}, {"text": "To increase efficiency, we extended EDITS to pre-process each dataset using the tokenizer and stemmer available in Lucene . This pre-processing phase is automatically activated when the EDITS-GA has to process non-annotated datasets.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.9810015559196472}]}, {"text": "However, we also annotated the RTE corpora with the Stanford parser plugin (downloadable from the ED-ITS websitein order to run the syntax-based algorithms available (e.g. tree edit distance).", "labels": [], "entities": [{"text": "ED-ITS websitein", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.9339988231658936}]}, {"text": "The number of boolean parameters used to generate the configurations is 18.", "labels": [], "entities": []}, {"text": "In light of this figure, it becomes evident that the number of possible configurations is too large (2 18 =262,144) for an exhaustive training/evaluation routine over each dataset . However, with an average of 5 reproductions on each iteration, EDITS-GA makes an average of 100 configurations for each algorithm.", "labels": [], "entities": []}, {"text": "Thanks to EDITS-GA, the average number of evaluated configurations fora single dataset is reduced to around 400 6 . Our results are summarized in, showing the total number of participating systems in each RTE Challenge, together with the highest, lowest, and average scores they achieved.", "labels": [], "entities": [{"text": "RTE Challenge", "start_pos": 205, "end_pos": 218, "type": "TASK", "confidence": 0.5548839271068573}]}, {"text": "Moreover, the official results obtained by EDITS are compared with the performance achieved with EDITS-GA on the same data.", "labels": [], "entities": [{"text": "EDITS", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8493077754974365}, {"text": "EDITS-GA", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9047393798828125}]}, {"text": "We can observe that, for all datasets, the results achieved by EDITS-GA significantly improve (up to 4.51%) the official EDITS results.", "labels": [], "entities": [{"text": "EDITS-GA", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.8140028119087219}, {"text": "EDITS", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.7893158197402954}]}, {"text": "It's also worth mentioning that such scores are always higher than the average ones obtained by participants.", "labels": [], "entities": []}, {"text": "This confirms that EDITS-GA can be potentially used by RTE systems developers as a strong term of comparison to assess the capabilities of their own system.", "labels": [], "entities": []}, {"text": "Since time is a crucial factor for RTE systems, it is important to remark that EDITS-GA allows to converge on a promising configuration quite efficiently.", "labels": [], "entities": [{"text": "RTE", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9498431086540222}, {"text": "EDITS-GA", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.5021385550498962}]}, {"text": "As can be seen in, the whole process takes around 9 minutes 7 for the smaller datasets (RTE1 to RTE5), and less than 2 hours fora very large dataset (RTE6).", "labels": [], "entities": [{"text": "RTE1", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8529207110404968}]}, {"text": "Such time analysis further proves the effectiveness of the extended EDITS-GA framework.", "labels": [], "entities": []}, {"text": "For the sake of completeness we gave a look at the differences between the \"educated guessing\" done by the EDITS developers for the official RTE submissions, and the \"optimal\" configuration automatically selected by EDITS-GA.", "labels": [], "entities": [{"text": "RTE submissions", "start_pos": 141, "end_pos": 156, "type": "DATASET", "confidence": 0.668059766292572}, {"text": "EDITS-GA", "start_pos": 216, "end_pos": 224, "type": "DATASET", "confidence": 0.9482674598693848}]}, {"text": "Surprisingly, in some cases, even a minor difference in the selected parameters leads to figurations, over small datasets (RTE1 to RTE5).", "labels": [], "entities": []}, {"text": "6 With these settings, training EDITS-GA over small datasets (RTE1 to RTE5) takes about 9 minutes each.", "labels": [], "entities": []}, {"text": "All time figures are calculated on an Intel(R) Xeon(R), CPU X3440 @ 2.53GHz, 8 cores with 8 GB RAM.", "labels": [], "entities": []}, {"text": "significant gaps in the results.", "labels": [], "entities": []}, {"text": "For instance, in RTE6 dataset, the \"guessed\" configuration ( ) was based on the lexical overlap algorithm, setting the cost of replacing H terms without an equivalent in T to the minimal Levenshtein distance between such words and any word in T.", "labels": [], "entities": [{"text": "RTE6 dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9291698932647705}]}, {"text": "EDITS-GA estimated, as a more promising solution, a combination of lexical overlap with a different cost scheme (based on the IDF of the terms in T).", "labels": [], "entities": [{"text": "EDITS-GA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.91202312707901}, {"text": "IDF", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9986750483512878}]}, {"text": "In addition, in contrast with the \"guessed\" configuration, stop-words filtering was selected as an option, eventually leading to a 4.51% improvement over the official RTE6 result.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: RTE results (acc. for RTE1-RTE5, F-meas. for RTE6). For each participant, only the best run is considered.", "labels": [], "entities": [{"text": "RTE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9190444946289062}, {"text": "acc", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9806442260742188}, {"text": "RTE1-RTE5", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.7895646691322327}, {"text": "F-meas", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9892594814300537}, {"text": "RTE6", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.7105455994606018}]}]}