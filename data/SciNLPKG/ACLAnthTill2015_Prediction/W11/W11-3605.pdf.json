{"title": [{"text": "Soundex-based Translation Correction in Urdu-English Cross-Language Information Retrieval", "labels": [], "entities": [{"text": "Soundex-based Translation Correction", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6972927351792654}, {"text": "Cross-Language Information Retrieval", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.5996275047461191}]}], "abstractContent": [{"text": "Cross-language information retrieval is difficult for languages with few processing tools or resources such as Urdu.", "labels": [], "entities": [{"text": "Cross-language information retrieval", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7605147163073221}]}, {"text": "An easy way of translating content words is provided by Google Translate, but due to lexicon limitations named entities (NEs) are transliterated letter by letter.", "labels": [], "entities": [{"text": "translating content words", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.8901665806770325}]}, {"text": "The resulting NEs errors (zynydyny zdn for Zinedine Zi-dane) hurts retrieval.", "labels": [], "entities": []}, {"text": "We propose to replace English non-words in the translation output.", "labels": [], "entities": []}, {"text": "First, we determine phonetically similar English words with the Soundex algorithm.", "labels": [], "entities": []}, {"text": "Then, we choose among them by a modified Levenshtein distance that models correct transliteration patterns.", "labels": [], "entities": []}, {"text": "This strategy yields an improvement of 4% MAP (from 41.2 to 45.1, monolingual 51.4) on the FIRE-2010 dataset.", "labels": [], "entities": [{"text": "MAP", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9990993738174438}, {"text": "FIRE-2010 dataset", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.9832161366939545}]}], "introductionContent": [{"text": "Cross-language information retrieval (CLIR) research is the study of systems that accept queries in one language and return text documents in a different language.", "labels": [], "entities": [{"text": "Cross-language information retrieval (CLIR)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8483487268288931}]}, {"text": "CLIR is of considerable practical importance in countries with many languages like India.", "labels": [], "entities": []}, {"text": "One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan.", "labels": [], "entities": []}, {"text": "There are around 60 million speakers of Urdu -48 million in India and 11 million in Pakistan (.", "labels": [], "entities": []}, {"text": "Despite this large number of speakers, NLP for Urdu is still at a fairly early stage.", "labels": [], "entities": []}, {"text": "Studies have been conducted on POS tagging, corpus construction (), word segmentation (, lexicographic sorting (, and information extraction ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9125486314296722}, {"text": "corpus construction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7982238829135895}, {"text": "word segmentation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7967932224273682}, {"text": "lexicographic sorting", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7238796651363373}, {"text": "information extraction", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.817270815372467}]}, {"text": "Many other processing tasks are still missing, and the size of the Urdu internet is minuscule compared to English and other major languages, making Urdu a prime candidate fora CLIR source language.", "labels": [], "entities": []}, {"text": "A particular challenge which Urdu poses for CLIR is its writing system.", "labels": [], "entities": []}, {"text": "Even though it is a Central Indo-Aryan language and closely related to Hindi, its development was shaped predominantly by Persian and Arabic, and it is written in PersoArabic script rather than Devanagari.", "labels": [], "entities": []}, {"text": "CLIR with a target language that uses another script needs to transliterate) any material that cannot be translated (typically out-ofvocabulary items like Named Entities).", "labels": [], "entities": []}, {"text": "The difficulties of Perso-Arabic in this respect are (a), some vowels are represented by letters which are also consonants and (b), short vowels are customarily omitted.", "labels": [], "entities": []}, {"text": "For example, in e \u00a9 u \u00f1 \u00a9 u \u00f0 (Winona) the first \u00f0 is used for the W but the second is used for O.", "labels": [], "entities": [{"text": "Winona)", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.906137228012085}]}, {"text": "Also the i sound is missing after \u00f0 (W).", "labels": [], "entities": []}, {"text": "In this paper, we consider Urdu-English CLIR.", "labels": [], "entities": [{"text": "Urdu-English CLIR", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.43436335027217865}]}, {"text": "Starting from a readily available baseline (using Google Translate to obtain English queries), we show that transliteration of Named Entities, more specifically missing vowels, is indeed a major factor in wrongly answered queries.", "labels": [], "entities": []}, {"text": "We reconstruct missing vowels in an unsupervised manner through an approximate string matching procedure based on phonetic similarity and orthographic similarity by using Soundex code and Levenshtein distance respectively, and find a clear improvement over the baseline.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 188, "end_pos": 208, "type": "METRIC", "confidence": 0.6279255151748657}]}], "datasetContent": [{"text": "Document Collection and Queries Our experiments are based on the FIRE-2010 2 English data, consisting of documents and queries, as our experimental materials.", "labels": [], "entities": [{"text": "FIRE-2010 2 English data", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.9347068816423416}]}, {"text": "The document collection consists of about 124,000 documents from the Englishlanguage newspaper \"The Telegraph India\" 3 from 2004-07.", "labels": [], "entities": [{"text": "Englishlanguage newspaper \"The Telegraph India\" 3 from 2004-07", "start_pos": 69, "end_pos": 131, "type": "DATASET", "confidence": 0.9513809561729432}]}, {"text": "The average length of a document was 40 words.", "labels": [], "entities": []}, {"text": "The FIRE query collection consists of 50 English queries which were of the same domain as that of the document collection.", "labels": [], "entities": [{"text": "FIRE query collection", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7217159867286682}]}, {"text": "The average number of relevant documents fora query was 76 (with a minimum of 13 and a maximum of 228).", "labels": [], "entities": []}, {"text": "The first author, who has an advanced knowledge of Urdu, translated the English FIRE queries manually into Urdu.", "labels": [], "entities": [{"text": "translated the English FIRE queries", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.46568967700004577}]}, {"text": "One of the resulting Urdu query is shown in, together with the Google translations back into English (GTR) which form the basis of the CLIR queries in the simplest model.", "labels": [], "entities": []}, {"text": "Every query has a title, and a description, both of which we used for retrieval.", "labels": [], "entities": []}, {"text": "The bottom row (entity) shows the Translate output and from the best model (Soundex matching with modified Levenshtein distance).", "labels": [], "entities": []}, {"text": "The bold-faced terms correspond to names that are corrected successfully, increasing the query's precision from 49% to 86%.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9984627962112427}]}, {"text": "Cross-lingual IR setup We implemented the models described in Section 2, using the Terrier IR engine () for retrieval from the FIRE-2010 English document collection.", "labels": [], "entities": [{"text": "FIRE-2010 English document collection", "start_pos": 127, "end_pos": 164, "type": "DATASET", "confidence": 0.9687004834413528}]}, {"text": "We used the PL2 weighting model with the term frequecy normalisation parameter of 10.99.", "labels": [], "entities": [{"text": "frequecy normalisation parameter", "start_pos": 46, "end_pos": 78, "type": "METRIC", "confidence": 0.9588679671287537}]}, {"text": "The document collection and the queries were stemmed using the Porter Stemmer.", "labels": [], "entities": [{"text": "document collection", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7898110449314117}, {"text": "Porter Stemmer", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.984695702791214}]}, {"text": "We applied all translation strategies defined in Section 2 as query expansion modules that enrich the Google Translate output with new relevant query terms.", "labels": [], "entities": []}, {"text": "Ina pre-experiment, we experimented with adding either only the single most similar term for each OOV item (1-best) or the best n terms (n-best).", "labels": [], "entities": [{"text": "OOV", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.8161336183547974}]}, {"text": "We consistently found better results for 1-best and report results for this condition only.", "labels": [], "entities": [{"text": "1-best", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9569259285926819}]}, {"text": "Monolingual model We also computed a monolingual English model which did not use the translated Urdu queries but the original English ones instead.", "labels": [], "entities": []}, {"text": "The result for this model can be seen as an upper bound for Urdu-English CLIR models.", "labels": [], "entities": []}, {"text": "Evaluation We report two evaluation measures.", "labels": [], "entities": []}, {"text": "The first one is Mean Average Precision (MAP), an evaluation measure that is highest when all correct items are ranked at the top.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 17, "end_pos": 45, "type": "METRIC", "confidence": 0.9710245331128439}]}, {"text": "MAP measures the global quality of the ranked document list; however improvements in MAP could result from an improved treatment of marginally relevant documents, while it is the quality of the top-ranked documents that is most important in practice and correlates best with extrinsic measures).", "labels": [], "entities": []}, {"text": "Therefore we also consider P@5, the precision of the five top-ranked documents.", "labels": [], "entities": [{"text": "P@5", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.968464990456899}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9987426400184631}]}, {"text": "shows the results of our experiments.", "labels": [], "entities": []}, {"text": "Monolingual English retrieval achieves a MAP of 51.4, while the CLIR baseline (Google Translate only -GTR) is 41.3.", "labels": [], "entities": [{"text": "Monolingual English retrieval", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7228561639785767}, {"text": "MAP", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9993744492530823}]}, {"text": "We expect the results of our experiments to fall between these two extremes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A sample query", "labels": [], "entities": []}, {"text": " Table 2: Results for Urdu-English CLIR models on  the FIRE 2010 collection (Mean Average Precision  and Precision of top five documents)", "labels": [], "entities": [{"text": "FIRE 2010 collection", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.9685324430465698}]}]}