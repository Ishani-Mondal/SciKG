{"title": [{"text": "Lightly-Supervised Training for Hierarchical Phrase-Based Machine Translation", "labels": [], "entities": [{"text": "Hierarchical Phrase-Based Machine Translation", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.6888662427663803}]}], "abstractContent": [{"text": "In this paper we apply lightly-supervised training to a hierarchical phrase-based statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6091479361057281}]}, {"text": "We employ bitexts that have been built by automatically translating large amounts of monolingual data as additional parallel training corpora.", "labels": [], "entities": []}, {"text": "We explore different ways of using this additional data to improve our system.", "labels": [], "entities": []}, {"text": "Our results show that integrating a second translation model with only non-hierarchical phrases extracted from the automatically generated bitexts is a reasonable approach.", "labels": [], "entities": []}, {"text": "The translation performance matches the result we achieve with a joint extraction on all training bitexts while the system is kept smaller due to a considerably lower overall number of phrases.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9556494951248169}]}], "introductionContent": [{"text": "We investigate the impact of an employment of large amounts of unsupervised parallel data as training data fora statistical machine translation (SMT) system.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.7766149938106537}]}, {"text": "The unsupervised parallel data is created by automatically translating monolingual source language corpora.", "labels": [], "entities": []}, {"text": "This approach is called lightly-supervised training in the literature and has been introduced by.", "labels": [], "entities": []}, {"text": "In contrast to Schwenk, we do not apply lightly-supervised training to a conventional phrase-based system () but to a hierarchical phrase-based translation (HPBT) system.", "labels": [], "entities": [{"text": "phrase-based translation (HPBT)", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.8062443971633911}]}, {"text": "In hierarchical phrase-based translation) a weighted synchronous context-free grammar is induced from parallel text, the search is based on CYK+ parsing) and typically carried out using the cube pruning algorithm.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.5898926456769308}]}, {"text": "In addition to the contiguous lexical phrases as in standard phrase-based translation, the hierarchical phrasebased paradigm also allows for phrases with gaps which are called hierarchical phrases.", "labels": [], "entities": []}, {"text": "A generic non-terminal symbol serves as a placeholder that marks the gaps.", "labels": [], "entities": []}, {"text": "In this paper we study several different ways of incorporating unsupervised training data into a hierarchical system.", "labels": [], "entities": []}, {"text": "The basic techniques we employ are the use of multiple translation models and a distinction of the hierarchical and the non-hierarchical (i.e. lexical) part of the translation model.", "labels": [], "entities": []}, {"text": "We report experimental results on the large-scale NIST Arabic-English translation task and show that lightly-supervised training yields significant gains over the baseline.", "labels": [], "entities": [{"text": "NIST Arabic-English translation task", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.7466162443161011}]}], "datasetContent": [{"text": "We use the open source Jane toolkit (  for our experiments, a hierarchical phrasebased translation software written in C++.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.6735624372959137}]}, {"text": "The empirical evaluation of all our systems on the two standard metrics BLEU () and TER () is presented in Table 5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9978896975517273}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9967545866966248}]}, {"text": "We have also checked the results for statistical significance over the baseline.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.722041130065918}]}, {"text": "The confidence intervals have been computed using bootstrapping for BLEU and Cochran's approximate ratio variance for TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9974130988121033}, {"text": "approximate ratio variance", "start_pos": 87, "end_pos": 113, "type": "METRIC", "confidence": 0.8855922420819601}, {"text": "TER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9858317375183105}]}, {"text": "When we combine the full baseline phrase table with the unsupervised phrase table or the lexical part of it, we either use common scaling factors for their source-to-target and target-to-source translation costs, or we use common scaling factors but mark entries from the unsupervised table with a binary feature, or we optimize the four translation features separately for each of the two tables as part of the log-linear model combination.", "labels": [], "entities": []}, {"text": "Including the unsupervised data leads to a substantial gain on the unseen test set of up to +1.0% BLEU absolute.", "labels": [], "entities": [{"text": "BLEU absolute", "start_pos": 98, "end_pos": 111, "type": "METRIC", "confidence": 0.9684427976608276}]}, {"text": "The different ways of combining the manually produced data with the unsupervised have little impact on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.9698062539100647}]}, {"text": "This holds specifically for the combination with only the lexical phrases, which, when marked with a binary feature, is able to obtain the same results as the full (joint extraction) system but with much less phrases.", "labels": [], "entities": []}, {"text": "We compared the decoding speed of these two setups and observed that the system with less phrases is clearly faster (5.5 vs. 2.6 words per second, measured on MT08).", "labels": [], "entities": [{"text": "MT08", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.901140034198761}]}, {"text": "The memory requirements of the systems do not differ greatly as we are using a binarized representation of the phrase table with ondemand loading.", "labels": [], "entities": []}, {"text": "All setups consume slightly less than 16 gigabytes of RAM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics for the preprocessed Arabic- English parallel training corpus. In the corpus, numer- ical quantities have been replaced by a special category  symbol.", "labels": [], "entities": [{"text": "Arabic- English parallel training corpus", "start_pos": 47, "end_pos": 87, "type": "DATASET", "confidence": 0.52408800025781}]}, {"text": " Table 2: Data statistics for the preprocessed Arabic part  of the dev and test corpora. In the corpus, numerical  quantities have been replaced by a special category sym- bol.", "labels": [], "entities": []}, {"text": " Table 3: Data statistics for the Arabic-English unsuper- vised training corpus after selection of the most reliable  sentence pairs. In the corpus, numerical quantities have  been replaced by a special category symbol.", "labels": [], "entities": []}, {"text": " Table 4: Phrase table sizes. The phrase tables have been filtered towards a larger set of test corpora containing a total  of 2.3 million running words.", "labels": [], "entities": []}, {"text": " Table 5: Results for the NIST Arabic-English translation task (truecase). The 90% confidence interval is given for the  baseline system as well as for the system with joint phrase extraction. Results in bold are significantly better than the  baseline.", "labels": [], "entities": [{"text": "NIST Arabic-English translation task", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.7510591000318527}, {"text": "phrase extraction", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.7205844968557358}]}]}