{"title": [{"text": "The First Surface Realisation Shared Task: Overview and Evaluation Results", "labels": [], "entities": [{"text": "First Surface Realisation Shared Task", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.7225785791873932}]}], "abstractContent": [{"text": "The Surface Realisation (SR) Task was anew task at Generation Challenges 2011, and had two tracks: (1) Shallow: mapping from shallow input representations to realisations; and (2) Deep: mapping from deep input representations to realisations.", "labels": [], "entities": [{"text": "Surface Realisation (SR) Task", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.866300493478775}]}, {"text": "Five teams submitted six systems in total, and we additionally evaluated human toplines.", "labels": [], "entities": []}, {"text": "Systems were evaluated automatically using a range of intrinsic met-rics.", "labels": [], "entities": []}, {"text": "In addition, systems were assessed by human judges in terms of Clarity, Readability and Meaning Similarity.", "labels": [], "entities": []}, {"text": "This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods.", "labels": [], "entities": [{"text": "SR Task Tracks", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.6238669951756796}]}], "introductionContent": [], "datasetContent": [{"text": "We computed scores using the following wellknown automatic evaluation metrics: 1 For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below).", "labels": [], "entities": []}, {"text": "Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands.", "labels": [], "entities": [{"text": "Text normalisation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7578720152378082}]}, {"text": "N-best, ranked system outputs: Ranked 5-best outputs were scored using a weighted average of the sentence-level scores for each metric, with these sentence-level weighted sums averaged across all outputs.", "labels": [], "entities": []}, {"text": "The weight w i assigned to the ith system output was in inverse proportion to its rank r i (K = 5): Missing outputs: Missing outputs were scored as zero (one for TER); in the n-best evaluation, missing or duplicate outputs were scored as 0 (1 for TER).", "labels": [], "entities": [{"text": "TER", "start_pos": 162, "end_pos": 165, "type": "METRIC", "confidence": 0.9791836142539978}, {"text": "TER", "start_pos": 247, "end_pos": 250, "type": "METRIC", "confidence": 0.9298033714294434}]}, {"text": "Since coverage was high for all systems (97% for OSU; 100% for all others), we only report results for all sentences (with the missing output penalty), rather than separately reporting scores for just the covered items.", "labels": [], "entities": [{"text": "coverage", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9922472238540649}, {"text": "OSU", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8385227918624878}]}, {"text": "We assessed three criteria in the human evaluations: Clarity, Readability and Meaning Similarity.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9530053734779358}]}, {"text": "We used continuous sliders as rating tools (see), because raters tend to prefer them ().", "labels": [], "entities": []}, {"text": "Slider positions were mapped to values from 0 to 100 (best).", "labels": [], "entities": []}, {"text": "The instructions relating to Clarity and Readability read as follows: The first criterion you need to assess is Clarity.", "labels": [], "entities": []}, {"text": "How clear (easy to understand) is the highlighted sentence within the context of the text extract?", "labels": [], "entities": []}, {"text": "The second criterion to assess is Readability.", "labels": [], "entities": []}, {"text": "This is sometimes called 'fluency', and your task is to decide how well the highlighted sentence reads; is it good fluent English, or does it have grammatical errors, awkward constructions, etc.", "labels": [], "entities": []}, {"text": "Note that you should assess Clarity separately from Readability: it is possible fora text to be completely clear, yet not read well; conversely, it is possible fora text to read very well, and its meaning to be unclear.", "labels": [], "entities": []}, {"text": "Please rate the highlighted sentence by moving each slider to the position that corresponds to your rating.", "labels": [], "entities": []}, {"text": "The part of the instructions relating to Meaning Similarity was as follows: This time you are being shown two extracts which are identical except for the highlighted sentences.", "labels": [], "entities": [{"text": "Meaning Similarity", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9186689555644989}]}, {"text": "You need to read both sentences within their context, and then decide how close in meaning the second sentence is to the first.: Automatic metric scores for human test data (PTB Section 24 100-sentence subset), including system-level scores (sys), mean of sentence-level scores (avg) and mean of weighted n-best scores (nb).", "labels": [], "entities": [{"text": "PTB Section 24 100-sentence subset", "start_pos": 174, "end_pos": 208, "type": "DATASET", "confidence": 0.9334780097007751}, {"text": "mean of sentence-level scores (avg)", "start_pos": 248, "end_pos": 283, "type": "METRIC", "confidence": 0.7235006008829389}, {"text": "mean of weighted n-best scores (nb)", "start_pos": 288, "end_pos": 323, "type": "METRIC", "confidence": 0.7745008915662766}]}, {"text": "the second sentence is to the first, the further to the right you need to place the slider.", "labels": [], "entities": []}, {"text": "For each test data item, raters were first shown the screen for the Readability and Clarity assessment (as shown in), followed by the screen for Meaning Similarity assessment (see).", "labels": [], "entities": []}, {"text": "We displayed system outputs as they were.", "labels": [], "entities": []}, {"text": "Raters were instructed to disregard spaces before punctuation and similar whitespace problems.", "labels": [], "entities": []}, {"text": "Some systems produced lower-cased outputs, others (like the STUMABA-D one output of which is shown in) produced outputs with capitalisations.", "labels": [], "entities": [{"text": "STUMABA-D", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.6061978936195374}]}, {"text": "All experiments use a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from each system and for each test set item.", "labels": [], "entities": []}, {"text": "Following detailed instructions, raters first did three practice examples, followed by the texts to be rated, in an order randomised for each rater.", "labels": [], "entities": []}, {"text": "Evaluations were carried out via a web interface.", "labels": [], "entities": []}, {"text": "Raters were encouraged to take breaks, and in the case of the 2-hour long SR-Shallow evaluation they were required to take breaks.", "labels": [], "entities": [{"text": "SR-Shallow evaluation", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.8433274626731873}]}, {"text": "In both experiments we used native-speaker raters from cohorts of 3rd-year undergraduate and postgraduate students (from Oxford, UCL, KCL and Sussex universities) currently doing, or having recently completed, a degree in linguistics.", "labels": [], "entities": []}, {"text": "In the SR-Deep evaluation we used 6 raters evaluating half the test set each (roughly 1 hour).", "labels": [], "entities": []}, {"text": "In the SRShallow evaluation we used 5 raters each evaluating the whole test set (2 hours).", "labels": [], "entities": [{"text": "SRShallow", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.7427764534950256}]}, {"text": "Their progress was logged at 10min intervals, and they received gift vouchers for their time.", "labels": [], "entities": []}, {"text": "In the following section, for each experiment we report the F-ratio as determined by a one-way ANOVA with the evaluation criterion in question as the dependent variable and System as the grouping factor.", "labels": [], "entities": [{"text": "F-ratio", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9982140064239502}, {"text": "ANOVA", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9347318410873413}]}, {"text": "F is the ratio of between-groups variability over within-group (or residual) variability, i.e. the larger the value of F, the more of the variability observed in the data is accounted for by the grouping factor, here System, relative to what variability remains within the groups.", "labels": [], "entities": [{"text": "F", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9746847748756409}]}, {"text": "We also report homogeneous subsets (sets of systems among which there are no significant differences) of systems as determined by a post-hoc Tukey's HSD analysis (with a significance threshold of 0.05).", "labels": [], "entities": [{"text": "Tukey's HSD analysis", "start_pos": 141, "end_pos": 161, "type": "DATASET", "confidence": 0.7016890645027161}]}, {"text": "pus' in the table).", "labels": [], "entities": []}, {"text": "The results look similar across the three evaluation criteria: STUMABA-S has the highest mean, followed by DCU, but with no statistically significant difference between them; ATT is third and UCM fourth for Readability and Meaning Similarity, and the two systems are joint third for Clarity.", "labels": [], "entities": [{"text": "STUMABA-S", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.4191047251224518}, {"text": "DCU", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.5326332449913025}, {"text": "ATT", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.8866358399391174}, {"text": "UCM", "start_pos": 192, "end_pos": 195, "type": "DATASET", "confidence": 0.8628766536712646}, {"text": "Readability and Meaning Similarity", "start_pos": 207, "end_pos": 241, "type": "TASK", "confidence": 0.5972150191664696}]}, {"text": "Rankings are identical across the three criteria for the systems in the Deep Track, with STUMABA-D first in all three cases, and OSU second.", "labels": [], "entities": [{"text": "STUMABA-D", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.8208187222480774}, {"text": "OSU", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.8097955584526062}]}, {"text": "For the shallow systems and Clarity: F (4,495) = 49.402, p < .001; Readability: F (4,495) = 52.839, p < .001; and Meaning Similarity: F (4,495) = 82.565, p < .001.", "labels": [], "entities": [{"text": "F", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9386017918586731}]}, {"text": "For the deep systems and Clarity: F (2,294) = 120.020, p < .001; Readability: F (2,294) = 162.22, p < .001; and Meaning Similarity: F (2,294) = 197.27, p < .001.", "labels": [], "entities": [{"text": "F", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.8547803163528442}]}], "tableCaptions": [{"text": " Table 4: Automatic metric scores for automatic test data (PTB Section 23), including system-level scores (sys), mean  of sentence-level scores (avg) and mean of weighted n-best scores (nb).", "labels": [], "entities": [{"text": "PTB Section 23)", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.957168385386467}, {"text": "mean  of sentence-level scores (avg)", "start_pos": 113, "end_pos": 149, "type": "METRIC", "confidence": 0.7478478465761457}, {"text": "mean of weighted n-best scores (nb)", "start_pos": 154, "end_pos": 189, "type": "METRIC", "confidence": 0.7447125092148781}]}, {"text": " Table 5: Automatic metric scores for human test data (PTB Section 24 100-sentence subset), including system-level  scores (sys), mean of sentence-level scores (avg) and mean of weighted n-best scores (nb).", "labels": [], "entities": [{"text": "PTB Section 24 100-sentence subset)", "start_pos": 55, "end_pos": 90, "type": "DATASET", "confidence": 0.9587699770927429}, {"text": "mean of sentence-level scores (avg)", "start_pos": 130, "end_pos": 165, "type": "METRIC", "confidence": 0.7325778773852757}]}, {"text": " Table 6: Tukey's HSD (\u03b1 = 0.05) homogeneous subsets for mean of sentence-level scores on automatic test data.", "labels": [], "entities": [{"text": "HSD", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5171992778778076}]}, {"text": " Table 7: Tukey's HSD (\u03b1 = 0.05) homogeneous subsets for mean of sentence-level scores on human test data.", "labels": [], "entities": [{"text": "HSD", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5474411249160767}]}]}