{"title": [{"text": "Influence of Parser Choice on Dependency-Based MT", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.7893555760383606}]}], "abstractContent": [{"text": "Accuracy of dependency parsers is one of the key factors limiting the quality of dependency-based machine translation.", "labels": [], "entities": [{"text": "dependency-based machine translation", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.6289773285388947}]}, {"text": "This paper deals with the influence of various dependency parsing approaches (and also different training data size) on the overall performance of an English-to-Czech dependency-based statistical translation system implemented in the Treex framework.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7402670681476593}, {"text": "Treex framework", "start_pos": 234, "end_pos": 249, "type": "DATASET", "confidence": 0.9256025850772858}]}, {"text": "We also study the relationship between parsing accuracy in terms of un-labeled attachment score and machine translation quality in terms of BLEU.", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9716372489929199}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9329988956451416}, {"text": "un-labeled attachment score", "start_pos": 68, "end_pos": 95, "type": "METRIC", "confidence": 0.6550383965174357}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.6648824661970139}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9977038502693176}]}], "introductionContent": [{"text": "In the last years, statistical n-gram models dominated the field of Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.849291718006134}]}, {"text": "However, their results are still far from perfect.", "labels": [], "entities": []}, {"text": "Therefore we believe it makes sense to investigate alternative statistical approaches.", "labels": [], "entities": []}, {"text": "This paper is focused on an analysis-transfer-synthesis translation system called TectoMT whose transfer representation has a shape of a deep-syntactic dependency tree.", "labels": [], "entities": []}, {"text": "The system has been introduced by\u017dabokrtsk\u00b4yby\u02c7by\u017dabokrtsk\u00b4by\u017dabokrtsk\u00b4y et al..", "labels": [], "entities": []}, {"text": "The translation direction under consideration is Englishto-Czech.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9655274748802185}]}, {"text": "It has been shown by that the current accuracy of the dependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9988012313842773}]}, {"text": "In other words, the parsing phase is responsible fora large portion of translation errors.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9764918684959412}, {"text": "translation errors", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8566982746124268}]}, {"text": "The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors have already been addressed by improvements based on Hidden Markov Tree Models for lexical and syntactic choice as shown by\u017dabokrtsk\u00b4y by\u02c7by\u017dabokrtsk\u00b4by\u017dabokrtsk\u00b4y and Popel, and by context sensitive translation models based on maximum entropy as described by.", "labels": [], "entities": []}, {"text": "Our study proceeds along two directions.", "labels": [], "entities": []}, {"text": "First, we train two state-of-the-art dependency parsers on training sets with varying size.", "labels": [], "entities": []}, {"text": "Second, we use five parsers based on different parsing techniques.", "labels": [], "entities": []}, {"text": "In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric).", "labels": [], "entities": [{"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.963251531124115}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8850635290145874}, {"text": "Unlabeled Attachment Score", "start_pos": 77, "end_pos": 103, "type": "METRIC", "confidence": 0.8523183663686117}, {"text": "UAS", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.7219162583351135}, {"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9761762022972107}]}, {"text": "The motivation behind the first set of experiments is that we can extrapolate the learning curve and try to predict how new advances in dependency parsing can affect MT quality in the future.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7509533166885376}, {"text": "MT", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.9919087290763855}]}, {"text": "The second experiment series is motivated by the hypothesis that parsers based on different approaches are likely to have a different distribution of errors, even if they can have competitive performance in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 207, "end_pos": 214, "type": "TASK", "confidence": 0.963255763053894}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.7167279124259949}]}, {"text": "In dependency parsing metrics, all types of incorrect edges typically have the same weight, 1 but some incorrect edges can be more harmful than others from the MT viewpoint.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.802073746919632}]}, {"text": "Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7191283106803894}, {"text": "UAS", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.7430291771888733}]}, {"text": "Due to growing popularity of dependency syntax in the last years, there area number of dependency parsers available.", "labels": [], "entities": []}, {"text": "The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in), (, and (, and two constituency parsers and (, whose outputs were converted to dependency structures by.", "labels": [], "entities": []}, {"text": "As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.8373321890830994}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The overall translation pipeline, within which the parsers are tested, is described in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 lists the parsers under consideration and their main features.", "labels": [], "entities": []}, {"text": "Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9828560948371887}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.998995840549469}]}], "datasetContent": [{"text": "The dependency trees needed for training the parsers and evaluating their UAS were created from the Penn Treebank data (enriched first with internal noun phrase structure applied via scripts provided by) by Penn Converter) with the -conll2007 option (PennConv for short).", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.9919340213139852}, {"text": "Penn Converter", "start_pos": 207, "end_pos": 221, "type": "DATASET", "confidence": 0.9421867430210114}, {"text": "PennConv", "start_pos": 251, "end_pos": 259, "type": "DATASET", "confidence": 0.975119411945343}]}, {"text": "All the parsers were evaluated on the same datasection 23.", "labels": [], "entities": []}, {"text": "All the parsers were trained on sections 02-21, except for the Stanford parser which was trained on sections 01-21.", "labels": [], "entities": []}, {"text": "We were able to retrain the parser models only for MST and Malt.", "labels": [], "entities": [{"text": "MST", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8682354688644409}, {"text": "Malt", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8669987916946411}]}, {"text": "For the other parsers we used pretrained models available on the Internet: CJ's default model ec50spfinal, Stanford's wsjPCFG.ser.gz model, and ZPar's english.tar.gz.", "labels": [], "entities": []}, {"text": "The model of ZPar is trained on data converted to dependencies using Penn2Malt tool, which selects the last member of a coordination as the head.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.913266122341156}]}, {"text": "To be able to compare ZPar's output with the other parsers, we postprocessed it by a simple ConjAsHead code that converts this style of coordinations to the one used in CoNLL2007, where the conjuction is the head.", "labels": [], "entities": [{"text": "CoNLL2007", "start_pos": 169, "end_pos": 178, "type": "DATASET", "confidence": 0.8872060775756836}]}, {"text": "Translation experiments were evaluated using reference translations from the new-dev2009 data set, provided by the organizors of shared translation task with the Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9558174014091492}, {"text": "new-dev2009 data set", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.8501046101252238}, {"text": "shared translation task", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.7062811553478241}, {"text": "Statistical Machine Translation", "start_pos": 174, "end_pos": 205, "type": "TASK", "confidence": 0.6678286989529928}]}], "tableCaptions": [{"text": " Table 1: The effect of training data size on parsing accu- racy and on translation performance with MST.", "labels": [], "entities": [{"text": "parsing accu- racy", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8402461856603622}, {"text": "translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9697371125221252}]}, {"text": " Table 2: The effect of training data size on parsing accu- racy and on translation performance with Malt.", "labels": [], "entities": [{"text": "parsing accu- racy", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8586186766624451}, {"text": "translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9715964198112488}]}, {"text": " Table 3: Dependency parsers tested in the translation pipeline.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7656292915344238}, {"text": "translation pipeline", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.9164153337478638}]}]}