{"title": [], "abstractContent": [{"text": "We describe our submissions to the WMT11 shared MT evaluation task: MTeRater and MTeRater-Plus.", "labels": [], "entities": [{"text": "WMT11 shared MT evaluation task", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.5825289130210877}, {"text": "MTeRater", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.8174208998680115}, {"text": "MTeRater-Plus", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.8249721527099609}]}, {"text": "Both are machine-learned metrics that use features from e-rater R , an automated essay scoring engine designed to assess writing proficiency.", "labels": [], "entities": []}, {"text": "Despite using only features from e-rater and without comparing to translations, MTeRater achieves a sentence-level correlation with human rankings equivalent to BLEU.", "labels": [], "entities": [{"text": "MTeRater", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.6742115616798401}, {"text": "BLEU", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9969249367713928}]}, {"text": "Since MTeRater only assesses fluency, we build a meta-metric, MTeRater-Plus, that incorporates adequacy by combining MTeRater with other MT evaluation met-rics and heuristics.", "labels": [], "entities": [{"text": "MTeRater-Plus", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.7992992997169495}, {"text": "MT evaluation met-rics", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.8602450887362162}]}, {"text": "This meta-metric has a higher correlation with human rankings than either MTeRater or individual MT metrics alone.", "labels": [], "entities": [{"text": "MTeRater", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.8699767589569092}]}, {"text": "However, we also find that e-rater features may not have significant impact on correlation in every case.", "labels": [], "entities": [{"text": "correlation", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9652076363563538}]}], "introductionContent": [{"text": "The evaluation of machine translation (MT) systems has received significant interest over the last decade primarily because of the concurrent rising interest in statistical machine translation.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8337276518344879}, {"text": "statistical machine translation", "start_pos": 161, "end_pos": 192, "type": "TASK", "confidence": 0.6631655593713125}]}, {"text": "The majority of research on evaluating translation quality has focused on metrics that compare translation hypotheses to a set of human-authored reference translations.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8357141017913818}]}, {"text": "However, there has also been some work on methods that are not dependent on human-authored translations.", "labels": [], "entities": []}, {"text": "One subset of such methods is task-based in that the methods determine the quality of a translation in terms of how well it serves the need of an extrinsic task.", "labels": [], "entities": []}, {"text": "These tasks can either be downstream NLP tasks such as information extraction) and information retrieval () or human tasks such as answering questions on a reading comprehension test).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7828259468078613}, {"text": "information retrieval", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.8068607449531555}]}, {"text": "Besides extrinsic evaluation, there is another set of methods that attempt to \"learn\" what makes a good translation and then predict the quality of new translations without comparing to reference translations.", "labels": [], "entities": []}, {"text": "proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features.", "labels": [], "entities": []}, {"text": "attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST.", "labels": [], "entities": [{"text": "WER", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9031599164009094}, {"text": "PER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9506770372390747}, {"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9908848404884338}, {"text": "NIST", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.8564378023147583}]}, {"text": "They also claim that the confidence score for the classifier being used, if available, maybe taken as an estimate of translation quality.", "labels": [], "entities": []}, {"text": "Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides.", "labels": [], "entities": []}, {"text": "Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 148, "end_pos": 150, "type": "TASK", "confidence": 0.9906418919563293}]}, {"text": "They used features derived from several other reference-driven MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9032558798789978}]}, {"text": "In other work, they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9515700936317444}]}, {"text": "build a classifier to distinguish machine-generated translations from human ones using fluency-based features and show that by combining the scores of this classifier with LM perplexities, they obtain an MT metric that has good correlation with human judgments but not better than the baseline BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 294, "end_pos": 298, "type": "METRIC", "confidence": 0.9940804839134216}]}, {"text": "The fundamental questions that inspired our proposed metrics are as follows: \u2022 Can an operational English-proficiency measurement system, built with absolutely no forethought of using it for evaluation of translation quality, actually be used for this purpose?", "labels": [], "entities": []}, {"text": "\u2022 Obviously, such a system can only assess the fluency of a translation hypothesis and not the adequacy.", "labels": [], "entities": [{"text": "translation hypothesis", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8948492407798767}]}, {"text": "Can the features derived from this system then be combined with metrics such as BLEU, METEOR or TERp-measures of adequacy-to yield a metric that performs better?", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9988270401954651}, {"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9913832545280457}, {"text": "TERp-measures", "start_pos": 96, "end_pos": 109, "type": "METRIC", "confidence": 0.9882016777992249}]}, {"text": "The first metric we propose (MTeRater) is an SVM ranking model that uses features derived from the ETS e-rater R system to assess fluency of translation hypotheses.", "labels": [], "entities": [{"text": "ETS e-rater R system", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.796388640999794}]}, {"text": "Our second metric (MTeRaterPlus) is a meta-metric that combines MTeRater features with metrics such as BLEU, METEOR and TERp as well as features inspired by other MT metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9964938759803772}, {"text": "METEOR", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.8873106241226196}, {"text": "TERp", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9572903513908386}]}, {"text": "Although our work is intimately related to some of the work cited above in that it is a trained regression model predicting translation quality at the sentence level, there are two important differences: 1.", "labels": [], "entities": []}, {"text": "We do not use any human translations -reference or otherwise -for MTeRater, not even when training the metric.", "labels": [], "entities": [{"text": "MTeRater", "start_pos": 66, "end_pos": 74, "type": "TASK", "confidence": 0.6586546897888184}]}, {"text": "The classifier is trained using human judgments of translation quality provided as part of the shared evaluation task.", "labels": [], "entities": []}, {"text": "2. Most of the previous approaches use feature sets that are designed to capture both translation adequacy and fluency.", "labels": [], "entities": []}, {"text": "However, MTeRater uses only fluency-based features.", "labels": [], "entities": [{"text": "MTeRater", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.8932315111160278}]}, {"text": "The next section provides some background on the e-rater system.", "labels": [], "entities": []}, {"text": "Section 3 presents a discussion of the differences between MT errors and learner errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9909501671791077}]}, {"text": "Section 4 describes how we use e-rater to build our metrics.", "labels": [], "entities": []}, {"text": "Section 5 outlines our experiments and Section 5 discusses the results of these experiments.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments were run using data from three years of previous WMT shared tasks (WMT08, WMT09 and WMT10).", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.5061103502909342}, {"text": "WMT08", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9523873925209045}, {"text": "WMT09", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9121674299240112}, {"text": "WMT10", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9110042452812195}]}, {"text": "In these evaluations, annotators were asked to rank 3-5 translation hypotheses (with ties allowed), given a source sentence and a reference translation, although they were only required to be fluent in the target language.", "labels": [], "entities": []}, {"text": "Since e-rater was developed to rate English sentences only, we only evaluated tasks with English as the target language.", "labels": [], "entities": []}, {"text": "All years included source languages French, Spanish, German and Czech.", "labels": [], "entities": []}, {"text": "WMT08 and WMT09 also included Hungarian and multisource English.", "labels": [], "entities": [{"text": "WMT08", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9865339994430542}, {"text": "WMT09", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.96165931224823}]}, {"text": "The number of MT systems was different for each language pair and year, from as few as 2 systems (WMT08 Hungarian-English) to as many as 25 systems (WMT10 German-English).", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9840011596679688}, {"text": "WMT08 Hungarian-English", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.8325262367725372}, {"text": "WMT10 German-English", "start_pos": 149, "end_pos": 169, "type": "DATASET", "confidence": 0.8971468806266785}]}, {"text": "All years had a newswire testset, which was divided into stories.", "labels": [], "entities": [{"text": "newswire testset", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.8976707458496094}]}, {"text": "WMT08 had testsets in two additional genres, which were not split into documents.", "labels": [], "entities": [{"text": "WMT08", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.983066976070404}]}, {"text": "All translations were pre-processed and run through e-rater.", "labels": [], "entities": []}, {"text": "Each document was treated as an essay, although news articles are generally longer than essays.", "labels": [], "entities": []}, {"text": "Testsets that were not already divided into documents were split into pseudo-documents of 20 contiguous sentences or less.", "labels": [], "entities": []}, {"text": "Missing end of sentence markers were added so that e-rater would not merge neighboring sentences.", "labels": [], "entities": []}, {"text": "The first group of rows in shows the Kendall's tau correlation with human rankings of MTeRater and the best-performing version of the three standard MT metrics.", "labels": [], "entities": [{"text": "MTeRater", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8148269057273865}, {"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9207735657691956}]}, {"text": "Even though MTeRater is blind to the MT context and does not use the source or references at all, MTeRater's correlation with human judgments is the same as case-sensitive bleu (bleu-case).", "labels": [], "entities": [{"text": "MTeRater", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.8022668361663818}]}, {"text": "This indicates that a metric trained to assess English proficiency in non-native speakers is applicable to machine translated text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kendall's tau correlation with human rankings.  BMT includes bleu, meteor and TERp; Aux includes aux- iliary features. BMT+Aux+MTeRater is MTeRater-Plus.", "labels": [], "entities": [{"text": "BMT", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.8929935097694397}, {"text": "TERp", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9902873039245605}, {"text": "MTeRater-Plus", "start_pos": 149, "end_pos": 162, "type": "DATASET", "confidence": 0.8210825324058533}]}]}