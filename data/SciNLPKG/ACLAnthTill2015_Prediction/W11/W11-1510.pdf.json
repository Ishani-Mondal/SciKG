{"title": [{"text": "Crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption", "labels": [], "entities": [{"text": "Crowdsourcing syntactic relatedness judgements", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6647644490003586}, {"text": "opinion mining", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7601823508739471}]}], "abstractContent": [{"text": "We present an end-to-end pipeline including a user interface for the production of word-level annotations for an opinion-mining task in the information technology (IT) domain.", "labels": [], "entities": []}, {"text": "Our pre-annotation pipeline selects candidate sentences for annotation using results from a small amount of trained annotation to bias the random selection over a large corpus.", "labels": [], "entities": []}, {"text": "Our user interface reduces the need for the user to understand the \"meaning\" of opinion in our domain context, which is related to community reaction.", "labels": [], "entities": []}, {"text": "It acts as a preliminary buffer against low-quality annotators.", "labels": [], "entities": []}, {"text": "Finally, our post-annotation pipeline aggregates responses and applies a more aggressive quality filter.", "labels": [], "entities": []}, {"text": "We present positive results using two different evaluation philosophies and discuss how our design decisions enabled the collection of high-quality annotations under subjective and fine-grained conditions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Crowdsourcing permits us to use a bank of anonymous workers with unknown skill levels to perform complex tasks given a simple breakdown of these tasks with user interface design that hides the full task complexity.", "labels": [], "entities": []}, {"text": "Use of these techniques is growing in the areas of computational linguistics and information retrieval, particularly since these fields now rely on the collection of large datasets for use in machine learning.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7078427970409393}, {"text": "information retrieval", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.8064135015010834}]}, {"text": "Considering the variety of applications, a variety of datasets is needed, but trained, known workers are an expense in principle that must be furnished for each one.", "labels": [], "entities": []}, {"text": "Consequently, crowdsourcing offers away to collect this data cheaply and quickly).", "labels": [], "entities": []}, {"text": "We applied crowdsourcing to perform the finegrained annotation of a domain-specific corpus.", "labels": [], "entities": []}, {"text": "Our user interface design and our annotator quality control process allows these anonymous workers to perform a highly subjective task in a manner that correlates their collective understanding of the task to our own expert judgements about it.", "labels": [], "entities": []}, {"text": "The path to success provides some illustration of the pitfalls inherent in opinion annotation.", "labels": [], "entities": [{"text": "opinion annotation", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7307717204093933}]}, {"text": "Our task is: domain and application-specific sentiment classification at the sub-sentence level-at the word level.", "labels": [], "entities": [{"text": "application-specific sentiment classification", "start_pos": 24, "end_pos": 69, "type": "TASK", "confidence": 0.6743207573890686}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results by number of workers excluded from the task. The prior polarity baseline comes from a lexicon by", "labels": [], "entities": []}]}