{"title": [{"text": "A Note on Sequential Rule-Based POS Tagging", "labels": [], "entities": [{"text": "Sequential Rule-Based POS Tagging", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7063092663884163}]}], "abstractContent": [{"text": "Brill's part-of-speech tagger is defined through a cascade of leftmost rewrite rules.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6393217742443085}]}, {"text": "We revisit the compilation of such rules into a single sequential transducer given by Roche and Schabes (Comput. Ling.", "labels": [], "entities": [{"text": "Comput. Ling.", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.8727974444627762}]}, {"text": "1995) and provide a direct construction of the minimal sequential transducer for each individual rule.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech (POS) tagging consists in assigning the appropriate POS tag to a word in the context of its sentence.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6178563714027405}]}, {"text": "The program that performs this task, the POS tagger, can be learned from an annotated corpus in case of supervised learning, typically using hidden Markov model-based or rule-based techniques.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.6238481849431992}]}, {"text": "The most famous rule-based POS tagging technique is due to.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8614374101161957}]}, {"text": "He introduced a three-parts technique comprising: 1.", "labels": [], "entities": []}, {"text": "a lexical tagger, which associates a unique POS tag to each word from an annotated training corpus.", "labels": [], "entities": []}, {"text": "This lexical tagger simply associates to each known word its most probable tag according to the training corpus annotation, i.e. a unigram maximum likelihood estimation; 2.", "labels": [], "entities": []}, {"text": "an unknown word tagger, which attempts to tag unknown words based on suffix or capitalization features.", "labels": [], "entities": [{"text": "unknown word tagger", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.6843840976556143}]}, {"text": "It works like the contextual tagger, using the presence of a capital letter and bounded sized suffixes in its rules: for instance in English, a -able suffix usually denotes an adjective; 3.", "labels": [], "entities": []}, {"text": "a contextual tagger, on which we focus in this paper.", "labels": [], "entities": [{"text": "contextual tagger", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.685968205332756}]}, {"text": "It consists of a cascade of string rewrite rules, called contextual rules, which correct tag assignments based on some surrounding contexts.", "labels": [], "entities": []}, {"text": "In this note, we revisit the proof that contextual rules can be translated into sequential transducers proposed by: whereas Roche and Schabes give a separate proof of sequentiality and exercise it to show that their constructed non-sequential transducer can be determinized (at the expense of a worst-case exponential blow-up), we give a direct translation of a contextual rule into the minimal normalized sequential transducer, by adapting Simon (1994)'s string matching automaton to the transducer case.", "labels": [], "entities": []}, {"text": "Our resulting sequential transducers are of linear size (before their composition).", "labels": [], "entities": []}, {"text": "A similar construction can be found in, but no claim of minimality is made there.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}