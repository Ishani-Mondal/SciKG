{"title": [{"text": "Incorporating Lexicon Knowledge into SVM Learning to Improve Sentiment Classification", "labels": [], "entities": [{"text": "Improve Sentiment Classification", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.8411774039268494}]}], "abstractContent": [{"text": "Two typical approaches to sentiment analysis are lexicon lookup and machine learning.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9639686048030853}]}, {"text": "Even though recent studies have shown that machine learning approaches in general outperform the lexicon lookup approaches, completely ignoring the knowledge encoded in sentiment lexicons may not be optimal.", "labels": [], "entities": []}, {"text": "We present an alternative method that incorporates sentiment lexicons as prior knowledge with machine learning approaches such as SVM to improve the accuracy of sentiment analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9982288479804993}, {"text": "sentiment analysis", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.8474794626235962}]}, {"text": "This paper also describes a method to automatically generate domain specific sentiment lexicons for this learning purpose.", "labels": [], "entities": []}, {"text": "Our experiment results show that the domain specific lexicons we constructed lead to a significant accuracy improvement for our sentiment analysis task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.998588502407074}, {"text": "sentiment analysis task", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.9407381216684977}]}], "introductionContent": [{"text": "Two typical approaches to sentiment analysis are lexicon lookup and machine learning.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9639686048030853}]}, {"text": "A lexicon lookup approach normally starts with a lexicon of positive and negative words.", "labels": [], "entities": []}, {"text": "The overall sentiment of a text is determined by the sentiments of a group of words and expressions appearing in the text (.", "labels": [], "entities": []}, {"text": "However, a significant challenge to this approach is that the polarity of many words is domain and context dependent.", "labels": [], "entities": []}, {"text": "For example, long is positive in long battery life and negative in long shutter lag.", "labels": [], "entities": []}, {"text": "Such words are associated with sentiment in a particular domain, but are not subjective in nature.", "labels": [], "entities": []}, {"text": "Nevertheless, current sentiment lexicons do not capture such domain and context sensitivities of sentiment expressions.", "labels": [], "entities": []}, {"text": "They either exclude such expressions or tag them with an overall polarity tendency based on statistics gathered from certain corpus.", "labels": [], "entities": []}, {"text": "While excluding such expressions leads to poor coverage, simply tagging them with a polarity tendency leads to poor precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9975051283836365}]}, {"text": "Because of these limitations, machine learning approaches have been gaining increasing popularity in the area of sentiment analysis (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.9510667622089386}]}, {"text": "A machine learning approach such as Support Vector Machine (SVM) does not rely on a sentiment lexicon to determine the polarity of words and expressions, and can automatically learn some of the context dependencies illustrated in the training data.", "labels": [], "entities": []}, {"text": "Although recent studies have shown that machine learning approaches in general outperform the lexicon lookup approaches for the task of sentiment analysis), completely ignoring the advantages and knowledge provided by sentiment lexicons may not be optimal.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.9335597157478333}]}, {"text": "We present an alternative method that incorporates sentiment lexicons as prior knowledge with machine learning approaches such as SVM to improve the accuracy of sentiment analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9982288479804993}, {"text": "sentiment analysis", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.8474794328212738}]}, {"text": "This paper also describes a method to automatically generate domain specific sentiment lexicons for this learning purpose.", "labels": [], "entities": []}, {"text": "Our experiments show that compared to general purpose domain independent sentiment lexicons, the domain specific lexicons lead to more significant accuracy improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9984591007232666}]}, {"text": "The sentiment analysis task performed in this paper is a fine grained product aspect level sentiment classification task for camera reviews.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9610288441181183}, {"text": "product aspect level sentiment classification", "start_pos": 70, "end_pos": 115, "type": "TASK", "confidence": 0.6505764961242676}]}, {"text": "Namely, for each sentence in the camera reviews, we need to predict whether this sentence discusses any camera aspects, and if so, what is the associated sentiment.", "labels": [], "entities": []}], "datasetContent": [{"text": "The sentiment analysis task we performed is a combined 45-way sentiment classification task.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9465436041355133}, {"text": "sentiment classification task", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.8285858631134033}]}, {"text": "These 45 classes are derived from 22 aspects related to camera purchases such as picture quality, LCD screen, battery life and customer support and their associated polarity values positive and negative, as well as a class of no opinion about any of the 22 aspects.", "labels": [], "entities": []}, {"text": "An example of such a class is picture quality: positive.", "labels": [], "entities": []}, {"text": "The goal is to map each input sentence into one of the 45 classes.", "labels": [], "entities": []}, {"text": "As mentioned in the previous section, we performed a two step classification for our task.", "labels": [], "entities": []}, {"text": "Namely, our final combined classifier consists of two classifiers.", "labels": [], "entities": []}, {"text": "The first is an 'Aspect Classifier', which performs a 23-way camera aspect classification.", "labels": [], "entities": [{"text": "camera aspect classification", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.5705771843592325}]}, {"text": "The second is a 'Polarity Classifier', which performs a 3-way (positive, negative and none) classification.", "labels": [], "entities": []}, {"text": "The final predictions are aggregated from the predictions produced by these two classifiers.", "labels": [], "entities": []}, {"text": "The classification accuracy is defined as follows.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9442602396011353}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9185190796852112}]}, {"text": "(1) In our experiment we labeled 2718 sentences randomly chosen from the Multi-Domain Sentiment Dataset created by; therefore, the classes in this data set are not balanced, and the majority class has 13% of the sentences.", "labels": [], "entities": []}, {"text": "As mentioned in the Related Work section, our task is different from those of the early studies on product aspect level sentiment analysis.", "labels": [], "entities": [{"text": "product aspect level sentiment analysis", "start_pos": 99, "end_pos": 138, "type": "TASK", "confidence": 0.6903648734092712}]}, {"text": "Earlier works such as and only extract explicitly expressed product aspects, and they do not identify implicitly expressed product aspects.", "labels": [], "entities": []}, {"text": "In addition, they do not further categorize the extracted noun phrases.", "labels": [], "entities": []}, {"text": "By contrast, we need to extract both the explicitly and implicitly expressed product aspects and further categorize the semantically related expressions regarding product aspects.'s work did extract both explicitly and implicitly mentioned product aspects, and they also further categorized the product aspects.", "labels": [], "entities": []}, {"text": "However, in terms of opinion extraction, they only extracted opinion words associated with product aspects, and did not further identify the polarities of the opinion words.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7902581095695496}]}, {"text": "By contrast, we need to identify the polarities associated with the product aspects.", "labels": [], "entities": []}, {"text": "Therefore, we cannot compare our results directly with those presented in the earlier works.", "labels": [], "entities": []}, {"text": "Instead, we used the majority class (13%) as our baseline, and we compared our approach to incorporating lexicon knowledge with SVM learning mainly with a conventional SVM learning, because the latter is the state-of-the-art algorithm reported in the literature for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 266, "end_pos": 284, "type": "TASK", "confidence": 0.960055947303772}]}, {"text": "Our results show that both the conventional SVM learning and our approach significantly outperform the majority class baseline.", "labels": [], "entities": [{"text": "SVM", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9375616908073425}]}, {"text": "We selected the Nouns, Verbs, Adjectives and Adverbs as our unigram word features.", "labels": [], "entities": []}, {"text": "All of them are stemmed using the Porter Stemmer (.", "labels": [], "entities": [{"text": "Porter Stemmer", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9149229824542999}]}, {"text": "Negators are attached to the next selected feature word.", "labels": [], "entities": []}, {"text": "We also use a small set of stop words 3 to exclude copulas and words such as take.", "labels": [], "entities": []}, {"text": "The reason that we choose these words as stop words is because they are both frequent and ambiguous and thus tend to have a negative impact on the classifier.", "labels": [], "entities": []}, {"text": "The SVM algorithm we adopted is implemented by.", "labels": [], "entities": []}, {"text": "We use linear kernel type and use the default setting for all other parameters.", "labels": [], "entities": []}, {"text": "In experiment 1, we used the conventional SVM algorithm, in which no lexicon knowledge was incorporated; we refer to this experiment as SVM.", "labels": [], "entities": []}, {"text": "In experiment 2, we incorporated only the knowledge encoded in the domain independent MPQA opinion dictionary into SVM learning; we refer to this experiment as 'MPQA + SVM'.", "labels": [], "entities": []}, {"text": "In experiment 3, we incorporated only the knowledge encoded in the domain specific lexicons we constructed into SVM learning; we refer to this experiment as 'Domain- The stop words we use include copulas and the following words: take, takes, make, makes, just, still, even, too, much, enough, back, again, far, same Lexicons + SVM'.", "labels": [], "entities": []}, {"text": "In experiment 4, we incorporated both the knowledge encoded in the MPQA and the domain specific lexicons we constructed into SVM learning; we refer to this experiment as 'DomainLexicons + MPQA + SVM'.", "labels": [], "entities": []}, {"text": "All of our results are based on 10-fold cross-validation, and they are summarized in.", "labels": [], "entities": []}, {"text": "The results in show that incorporating both the domain independent MPQA lexicon and the domain specific lexicons that we built achieves the best overall performance.", "labels": [], "entities": []}, {"text": "Of these two types of lexicon, incorporating the domain specific lexicons is more effective, as they contributed the most to the improvement of the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9319915175437927}]}, {"text": "The improvement achieved by our approach is statistically significant with p <0.000001 according to paired t-test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall Performance Comparison", "labels": [], "entities": []}, {"text": " Table 2: Breakdown Performance Comparison", "labels": [], "entities": []}]}