{"title": [], "abstractContent": [{"text": "This paper documents recent work carried out for PeEn-SMT, our Statistical Machine Translation system for translation between the English-Persian language pair.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6583026250203451}]}, {"text": "We give details of our previous SMT system, and present our current development of significantly larger corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9942243695259094}]}, {"text": "We explain how recent tests using much larger corpora helped to evaluate problems in parallel corpus alignment, corpus content, and how matching the domains of PeEn-SMT's components affect translation output.", "labels": [], "entities": [{"text": "parallel corpus alignment", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6665036578973135}]}, {"text": "We then focus on combining corpora and approaches to improve test data, showing details of experimental setup, together with a number of experiment results and comparisons between them.", "labels": [], "entities": []}, {"text": "We show how one combination of corpora gave us a metric score outperform-ing Google Translate for the English-to-Persian translation.", "labels": [], "entities": []}, {"text": "Finally, we outline areas of our intended future work, and how we plan to improve the performance of our system to achieve higher metric scores, and ultimately to provide accurate, reliable language translation.", "labels": [], "entities": [{"text": "language translation", "start_pos": 190, "end_pos": 210, "type": "TASK", "confidence": 0.7245904952287674}]}], "introductionContent": [{"text": "Machine Translation is one of the earliest areas of research in Natural Language Processing.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8775223195552826}, {"text": "Natural Language Processing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6409348746140798}]}, {"text": "Research work in this field dates as far back as the 1950's.", "labels": [], "entities": []}, {"text": "Several different translation methods have been explored to date, the oldest and perhaps the simplest being rule-based translation, which is in reality transliteration, or translating each word in the source language with its equivalent counterpart in the target language.", "labels": [], "entities": [{"text": "rule-based translation", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7137641906738281}]}, {"text": "This method is very limited in the accuracy it can give.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999346911907196}]}, {"text": "A method known as Statistical Machine Translation (SMT) seems to be the preferred approach of many industrial and academic research laboratories, due to its recent success (.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.8810124893983206}]}, {"text": "Different evaluation metrics generally show SMT approaches to yield higher scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9915384650230408}]}, {"text": "The SMT system itself is a phrase-based translation approach, and operates using a parallel or bilingual corpus -a huge database of corresponding sentences in two languages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9924874305725098}, {"text": "phrase-based translation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7173823118209839}]}, {"text": "The system is programmed to employ statistics and probability to learn by example which translation of a word or phrase is most likely to be correct.", "labels": [], "entities": []}, {"text": "For more accurate translation results, it is generally necessary to have a large parallel corpus of aligned phrases and sentences from the source and target languages.", "labels": [], "entities": []}, {"text": "Our work is focussed on implementing a SMT for the Persian-English language pair.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.99285489320755}]}, {"text": "SMT has only been employed in several experimental translation attempts for this language pair, and is still largely undeveloped.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9708573818206787}]}, {"text": "This is due to several difficulties specific to this particular language pair.", "labels": [], "entities": []}, {"text": "Firstly, several characteristics of the Persian language cause issues with translation into English, and secondly, effective SMT systems generally rely on large amounts of parallel text to produce decent results, and there are no parallel corpora of appropriate size currently available for this language pair.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9957608580589294}]}, {"text": "These factors are prime reasons why there is a distinct shortage of research work aimed at SMT of this particular language pair.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9956302642822266}]}, {"text": "This paper firstly gives a brief background to the Persian language, focusing on its differences to English, and how this affects translation between the two languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 130, "end_pos": 141, "type": "TASK", "confidence": 0.9709665179252625}]}, {"text": "Next, we give details of our PeEn-SMT system, how we developed and manipulated the data, and aligned our parallel corpora using a hybrid sentence aligning method.", "labels": [], "entities": []}, {"text": "We give a brief overview of previous tests", "labels": [], "entities": []}], "datasetContent": [{"text": "The original tests performed using PeEn-SMT as shown in some of previous papers produced unsatisfactory results.", "labels": [], "entities": [{"text": "PeEn-SMT", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.7592971920967102}]}, {"text": "It was initially thought that this was due to the small corpora and training models used.", "labels": [], "entities": []}, {"text": "As detailed in these papers, a number of preliminary tests were carried out, and each time the language model was increased in size to a maximum of 7005 sentences.", "labels": [], "entities": []}, {"text": "The training model at its largest consisted of 2343 sentences.", "labels": [], "entities": []}, {"text": "The language model in these tests consisted of text collected from BBC news stories, and the training model consisted of a bilingual corpus of mostly UN news.", "labels": [], "entities": []}, {"text": "It was thought that the unsatisfactory test results achieved could be remedied by enlarging the language model and corpus, since the amounts of data in each model were far too small to achieve any decent success in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 215, "end_pos": 218, "type": "TASK", "confidence": 0.9948872923851013}]}, {"text": "In order to develop the translation model, an English-Persian parallel corpus was built as explained in the Data Development section.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9712329506874084}]}, {"text": "We divided the parallel corpus into different sized groups for each test system.", "labels": [], "entities": []}, {"text": "The details of the corpus size for each test are shown in. shows the size of each test's corpus after the text was tokenized, converted to lowercase, and stripped of blank lines and their correspondences in the corpora.", "labels": [], "entities": []}, {"text": "This data was obtained after applying the hybrid sentence alignment method.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7444420158863068}]}, {"text": "We divided the corpus to construct five different systems, beginning from 10,000 sentences in the smallest corpus, and increasing in steps of approximately 10,000 sentences each time up to the 5 th test system, with a corpus of almost 53,000 sentences.", "labels": [], "entities": []}, {"text": "In addition to the news stories corpus as shown earlier, we only had access to one freely available corpus, and this consisted of movie subtitles in Persian and English.", "labels": [], "entities": []}, {"text": "This was shown to be in a completely different domain to our main corpus, so for most cases we preferred to run tests separately when using these corpora.", "labels": [], "entities": []}, {"text": "Finally in NSPEC, we concatenated these two corpora, to ascertain the potential output with a combined corpus.", "labels": [], "entities": [{"text": "NSPEC", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8451172113418579}]}, {"text": "We tested the subtitle corpus separately because we wished to see how an out-of-domain cor-  One aspect of Machine Translation that poses a challenge is developing an effective automated metric for evaluating machine translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8487601280212402}, {"text": "machine translation", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.7019800096750259}]}, {"text": "This is because each output sentence has a number of acceptable translations.", "labels": [], "entities": []}, {"text": "Most popular metrics yield scores primarily based on matching phrases in the translation produced by the system to those in several reference translations.", "labels": [], "entities": []}, {"text": "The metric scores mostly differ in how they show reordering and synonyms.", "labels": [], "entities": []}, {"text": "In general, BLEU is the most popular metric used for both comparison of Translation systems and tuning of machine translation models); most systems are trained to optimize BLEU scoring.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9989508390426636}, {"text": "Translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.8227236270904541}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.6644962430000305}, {"text": "BLEU scoring", "start_pos": 172, "end_pos": 184, "type": "METRIC", "confidence": 0.9405718743801117}]}, {"text": "Many alternative metrics are also available however.", "labels": [], "entities": []}, {"text": "In this paper we explore how optimizing a selection of different evaluation metrics effect the resulting model.", "labels": [], "entities": []}, {"text": "The metrics we chose to work with were BLEU, IBM-BLEU, METEOR, NIST, and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9970922470092773}, {"text": "IBM-BLEU", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.5524013042449951}, {"text": "METEOR", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9599131345748901}, {"text": "NIST", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.597466766834259}, {"text": "TER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9914214015007019}]}, {"text": "While BLEU is a relatively simple metric, it has a number of shortcomings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9969800114631653}]}, {"text": "There have been several recent developments in evaluation metrics, such as TER (Translation Error Rate).", "labels": [], "entities": [{"text": "TER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9974334836006165}, {"text": "Translation Error Rate)", "start_pos": 80, "end_pos": 103, "type": "METRIC", "confidence": 0.7908062636852264}]}, {"text": "TER operates by measuring the amount of editing that a human would have to undertake to produce a translation so that it forms an exact match with a reference translation).METEOR) is a metric for evaluating translations with explicit ordering, and performs a more in-depth analysis of the translations under evaluation.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8375977873802185}, {"text": "METEOR", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9759646058082581}]}, {"text": "The scores they yield tend to achieve a better correlation with human judgments than those given by BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9948844313621521}]}, {"text": "Another metric used was IBM-BLEU) , which performs case-insensitive matching of n-grams up to n=4.", "labels": [], "entities": []}, {"text": "BLEU and NIST) both produce models that are more robust than that of other metrics, and because of this, we still consider them the optimum choice for training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9483432173728943}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9404996633529663}]}, {"text": "Our first experiment was carried outwith 10,000 sentences (System1) in the English-to-Persian translation direction.", "labels": [], "entities": []}, {"text": "For comparison we tested the SMT model on different language models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9898658394813538}]}, {"text": "As shown in, and 6, the best result was achieved when we trained the machine on the IR-NA language model.", "labels": [], "entities": [{"text": "IR-NA language model", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.7100504835446676}]}, {"text": "We gradually increased the size of the corpora to the next test set (System 2), which was almost 21,000 sentences, and we repeated the test for different language models.", "labels": [], "entities": []}, {"text": "Again the result showed that using IRNA resulted in the best translation, followed by BBC, then Hamshahri.", "labels": [], "entities": [{"text": "translation", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.8058795928955078}, {"text": "BBC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9755566716194153}, {"text": "Hamshahri", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.8567859530448914}]}, {"text": "We observed almost identical trends with each test set; up to the set with the largest corpus (53,000 sentences, System 5).", "labels": [], "entities": []}, {"text": "It was originally thought that the dramatic increase in the size of both models would yield a much higher metric score, since it gave the translation program more data to work with.", "labels": [], "entities": [{"text": "metric score", "start_pos": 106, "end_pos": 118, "type": "METRIC", "confidence": 0.9391568303108215}]}, {"text": "However, these new tests proved that this was not necessarily always true, and corpus size alone was not synonymous with improved translation.", "labels": [], "entities": []}, {"text": "For instance, in the case where the Hamshahri corpus was used for the language model, the output result was even worse than the original tests with afar smaller corpus like BBC.", "labels": [], "entities": [{"text": "Hamshahri corpus", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.958575963973999}, {"text": "BBC", "start_pos": 173, "end_pos": 176, "type": "DATASET", "confidence": 0.9829220771789551}]}, {"text": "The IRNA corpus, larger than the original BBC corpus (7005 sentences) but still smaller than Hamshahri, yielded the best result of the two.", "labels": [], "entities": [{"text": "IRNA corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.870775043964386}, {"text": "BBC corpus", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9810753464698792}, {"text": "Hamshahri", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.8307711482048035}]}, {"text": "To establish a reason for the apparently illogical test results, the characteristics of each corpus were examined, together with their combinations in each test.", "labels": [], "entities": []}, {"text": "After analysis, it was seen that there were a number of likely factors contributing to the poor results.", "labels": [], "entities": []}, {"text": "One such factor involved the nature of the data comprising each corpus, and how this affected the match between the language model and the training model.", "labels": [], "entities": []}, {"text": "For instance, in the case where we achieved an even lower score than the original tests, it was noted that the training model consisted of a bilingual corpus based mainly on movie subtitles, yet the Hamshahri corpus was a collection of news stories.", "labels": [], "entities": [{"text": "Hamshahri corpus", "start_pos": 199, "end_pos": 215, "type": "DATASET", "confidence": 0.8358167707920074}]}, {"text": "For the most part, movies consist of spoken, natural language in everyday situations, filled with idioms, colloquial expressions and terms, and often incorrect grammar and sentence structure.", "labels": [], "entities": []}, {"text": "These characteristics were heavily present in the training model.", "labels": [], "entities": []}, {"text": "News stories on the other hand not only ideally consist of wellstructured sentences, with correct grammar and little presence of colloquialism, but the very nature of this kind of literature is unique, and rarely found in natural language.", "labels": [], "entities": []}, {"text": "Another example showing this involved the subtitle corpus (TEP) that we had access to.", "labels": [], "entities": []}, {"text": "This corpus was significantly larger in size (612,000 sentences) when compared to the other corpora that we had available to us.", "labels": [], "entities": []}, {"text": "However, when we performed the same experiment against different language models, the result was quite unsatisfactory.", "labels": [], "entities": []}, {"text": "We believe that this was due to our test sets being in a different domain than that of the movie subtitles.", "labels": [], "entities": []}, {"text": "These results led us to conclude that using larger language and training models alone was not a reliable determining factor in satisfactory output.", "labels": [], "entities": []}, {"text": "For the sake of comparison, Google Translator was tested on the same test data and results are in-.", "labels": [], "entities": []}, {"text": "We compared our system to Google's SMT for this language pair, and compared to the evaluation metric score released by Google.", "labels": [], "entities": []}, {"text": "Our PeEn-SMT system outperforms the Google translator in the English-to-Persian translation direction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Bilingual Corpora Used to Train the  Translation Model", "labels": [], "entities": []}, {"text": " Table 2: Bilingual Corpora after Hybrid Alignment  Method", "labels": [], "entities": [{"text": "Bilingual Corpora after Hybrid Alignment", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.8517090201377868}]}, {"text": " Table 3: Monolingual Corpora Used to Train the  Language Model", "labels": [], "entities": []}, {"text": " Table 4: Automatic Evaluation Metrics of PeEn- SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.6511446237564087}]}, {"text": " Table 5: Automatic Evaluation Metrics of PeEn- SMT System", "labels": [], "entities": [{"text": "PeEn- SMT", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.5866812666257223}]}, {"text": " Table 6: Automatic Evaluation Metrics of PeEn- SMT System", "labels": [], "entities": [{"text": "PeEn- SMT", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.5867834885915121}]}]}