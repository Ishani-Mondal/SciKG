{"title": [{"text": "An Incremental Model for Coreference Resolution with Restrictive Antecedent Accessibility", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9843217730522156}]}], "abstractContent": [{"text": "We introduce an incremental model for coref-erence resolution that competed in the CoNLL 2011 shared task (open regular).", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.867297887802124}, {"text": "CoNLL 2011 shared task", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.8586301207542419}]}, {"text": "We decided to participate with our baseline model, since it worked well with two other datasets.", "labels": [], "entities": []}, {"text": "The benefits of an incremental over a mention-pair architecture are: a drastic reduction of the number of candidate pairs, a means to overcome the problem of underspecified items in pair-wise classification and the natural integration of global constraints such as transitivity.", "labels": [], "entities": [{"text": "pair-wise classification", "start_pos": 182, "end_pos": 206, "type": "TASK", "confidence": 0.6162219941616058}]}, {"text": "We do not apply machine learning, instead the system uses an empirically derived salience measure based on the dependency labels of the true mentions.", "labels": [], "entities": []}, {"text": "Our experiments seem to indicate that such a system already is on par with machine learning approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "With notable exceptions () supervised approaches to coreference resolution are often realized by pairwise classification of anaphor-antecedent candidates.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.978444904088974}]}, {"text": "A popular and often reimplemented approach is presented in ().", "labels": [], "entities": []}, {"text": "A clustering phase on top of the pairwise classifier no longer is needed and the number of candidate pairs is reduced, since from each coreference set (be it large or small) only one mention (the most representative one) needs to be compared to anew anaphor candidate.", "labels": [], "entities": []}, {"text": "We form a 'virtual prototype' that collects information from all the members of each coreference set in order to maximize 'representativeness'.", "labels": [], "entities": []}, {"text": "Constraints such as transitivity and morphological agreement can be assured by just a single comparison.", "labels": [], "entities": []}, {"text": "If an anaphor candidate is compatible with the virtual prototype, then it is by definition compatible with all members of the coreference set.", "labels": [], "entities": []}, {"text": "We designed our system to work purely with a simple, yet empirically derived salience measure.", "labels": [], "entities": []}, {"text": "It turned out that it outperformed (for German and English, using CEAF, B-cubed and Blanc) the systems from the 2010's SemEval shared task 1 on 'coreference resolution in multiple languages'.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.7810561656951904}, {"text": "coreference resolution", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.937689870595932}]}, {"text": "Only with the more and more questioned) MUC measure our system performed worse (at least for English).", "labels": [], "entities": [{"text": "MUC measure", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.778453916311264}]}, {"text": "Our system uses real preprocessing (i.e. a dependency parser) and extracts markables (nouns, named entities and pronouns) from the chunks and based on POS tags delivered by the preprocessing pipeline.", "labels": [], "entities": []}, {"text": "Since we are using a parser, we automatically take part in the open regular session.", "labels": [], "entities": []}, {"text": "Please note that the dependency labels are the only additional information being used by our system.", "labels": [], "entities": []}, {"text": "Let I be the chronologically ordered list of markables, C be the set of coreference sets (i.e. the coreference partition) and Ba buffer, where markables are stored, if they are not found to be anaphoric (but might be valid antecedents, still).", "labels": [], "entities": []}, {"text": "Furthermore mi is the current markable and \u2295 means concatenation of a list and a single item.", "labels": [], "entities": []}, {"text": "The algorithm proceeds as follows: a set of antecedent candidates is determined for each markable mi (steps 1 to 7) from the coreference sets and the buffer.", "labels": [], "entities": []}, {"text": "A valid candidate r j orb k must be compatible with mi . The definition of compatibility depends on the POS tags of the anaphor-antecedent pair (in order to be coreferent, e.g. two pronouns must agree in person, number and gender etc.).", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of our evaluation over the CoNLL 2011 shared task development set are given in (development set) and 3 (official results on the test set).", "labels": [], "entities": [{"text": "CoNLL 2011 shared task development set", "start_pos": 39, "end_pos": 77, "type": "DATASET", "confidence": 0.9009517431259155}]}, {"text": "The official overall score of our system in the open regular setting is 51.77.", "labels": [], "entities": []}, {"text": "There are several rea-  sons for that.", "labels": [], "entities": []}, {"text": "First and foremost, the scorer requires chunk extensions to match perfectly.", "labels": [], "entities": []}, {"text": "That is, even if the head of an antecedent is found, this does not count if the chunk extension of that noun phrase was not correctly identified.", "labels": [], "entities": []}, {"text": "Since chunks do not play a major role in depencendy parsing, our approximation might be faulty 4 . Another shortcomming are nominal anaphora that cannot be identified by string matching (e.g. Obama ... The president).", "labels": [], "entities": [{"text": "depencendy parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6891564577817917}]}, {"text": "Our simple salience-based approach does not cope at all with this type of anaphora.", "labels": [], "entities": []}], "tableCaptions": []}