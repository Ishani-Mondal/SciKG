{"title": [{"text": "Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models", "labels": [], "entities": [{"text": "Topic Adaptation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8638970255851746}, {"text": "Lecture Translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7703737914562225}]}], "abstractContent": [{"text": "This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training.", "labels": [], "entities": [{"text": "bilingual topic modeling", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.663027415672938}, {"text": "language model adaptation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6525421837965647}, {"text": "Probabilistic Latent Semantic Analysis (PLSA)", "start_pos": 192, "end_pos": 237, "type": "TASK", "confidence": 0.7024217162813459}]}, {"text": "During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language's vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM).", "labels": [], "entities": [{"text": "Minimum Discrimination Information (MDI) adaptation", "start_pos": 188, "end_pos": 239, "type": "TASK", "confidence": 0.6289361545017788}]}, {"text": "We apply our approach on the English-French IWSLT 2010 TED Talk exercise , and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task.", "labels": [], "entities": [{"text": "IWSLT 2010 TED Talk exercise", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.888097620010376}, {"text": "perplexity", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9859376549720764}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9876618385314941}, {"text": "NIST", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9496567845344543}, {"text": "translation task", "start_pos": 251, "end_pos": 267, "type": "TASK", "confidence": 0.8831089735031128}]}, {"text": "Our topic modeling approach is simpler to construct than its counterparts .", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8203842341899872}]}], "introductionContent": [{"text": "Adaptation is usually applied to reduce the performance drop of Statistical Machine Translation (SMT) systems when translating documents that deviate from training and tuning conditions.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9243419170379639}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.8398959636688232}]}, {"text": "In this paper, we focus primarily on language model (LM) adaptation.", "labels": [], "entities": [{"text": "language model (LM) adaptation", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6255335162083308}]}, {"text": "In SMT, LMs are used to promote fluent translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9907799363136292}]}, {"text": "As probabilistic models of sequences of words, language models guide the selection and ordering of phrases in translation.", "labels": [], "entities": []}, {"text": "With respect to * This work was carried out during an internship period at Fondazione Bruno Kessler.", "labels": [], "entities": []}, {"text": "LM training, LM adaptation for SMT tries to improve an existing LM by using smaller amounts of texts.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8861792683601379}, {"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9642048478126526}]}, {"text": "When adaptation data represents the translation task domain one generally refers to domain adaptation, while when they just represent the content of the single document to be translated one typically refers to topic adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.6992920935153961}, {"text": "topic adaptation", "start_pos": 210, "end_pos": 226, "type": "TASK", "confidence": 0.7128940522670746}]}, {"text": "We propose a cross-language topic adaptation method, enabling the adaptation of a LM based on the topic distribution of the source document during translation.", "labels": [], "entities": [{"text": "cross-language topic adaptation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6237442990144094}]}, {"text": "We train a latent semantic topic model on a collection of bilingual documents, in which each document contains both the source and target language.", "labels": [], "entities": []}, {"text": "During inference, a latent topic distribution of words across both the source and target languages is inferred from a source document to be translated.", "labels": [], "entities": []}, {"text": "After inference, we remove all source language words from the topic-word distributions and construct a unigram language model which is used to adapt our background LM via Minimum Discrimination Information (MDI) estimation;.", "labels": [], "entities": []}, {"text": "We organize the paper as follows: In Section 2, we discuss relevant previous work.", "labels": [], "entities": []}, {"text": "In Section 3, we review topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7803650498390198}]}, {"text": "In Section 4, we review MDI adaptation.", "labels": [], "entities": [{"text": "MDI adaptation", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8974213600158691}]}, {"text": "In Section 5, we describe our new bilingual topic modeling based adaptation technique.", "labels": [], "entities": [{"text": "bilingual topic modeling based adaptation", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.6759657979011535}]}, {"text": "In Section 6, we report adaptation experiments, followed by conclusions and future work in Section 7.", "labels": [], "entities": []}, {"text": "construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolin-gual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9823050498962402}]}, {"text": "In, domain-specific language models are obtained by including only the sentences that are similar to the ones in the target domain via a relative entropy based criterion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were done using the TED Talks collection, used in the IWSLT 2010 evaluation task 1 . In IWSLT 2010, the challenge was to translate talks from the TED website 2 from English to French.", "labels": [], "entities": [{"text": "TED Talks collection", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.8063503901163737}, {"text": "IWSLT 2010 evaluation task", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.8051794767379761}]}, {"text": "The talks include a variety of topics, including photography and pyschology and thus do not adhere to a single genre.", "labels": [], "entities": []}, {"text": "All talks were given in English and were manually transcribed and translated into French.", "labels": [], "entities": []}, {"text": "The TED training data consists of 329 parallel talk transcripts with approximately 84k sentences.", "labels": [], "entities": [{"text": "TED training data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7239450414975485}]}, {"text": "The TED test data consists of transcriptions created via 1-best ASR outputs from the KIT Quaero Evaluation System.", "labels": [], "entities": [{"text": "TED test data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7085389494895935}, {"text": "KIT Quaero Evaluation System", "start_pos": 85, "end_pos": 113, "type": "DATASET", "confidence": 0.9057413339614868}]}, {"text": "It consists of 758 sentences and 27,432 and 27,307 English and French words, respectively.", "labels": [], "entities": []}, {"text": "The TED talk data is segmented at the clause level, rather than at the level of sentences.", "labels": [], "entities": [{"text": "TED talk data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7897838155428568}]}, {"text": "Our SMT systems are built upon the Moses opensource SMT toolkit ( . The translation and lexicalized reordering models have been trained on parallel data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9847914576530457}]}, {"text": "One 5-gram background LM was constructed from the French side of the TED training data (740k words), smoothed with the improved Kneser-Ney technique) and computed with the IRSTLM toolkit.", "labels": [], "entities": [{"text": "TED training data", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8091787894566854}, {"text": "IRSTLM toolkit", "start_pos": 172, "end_pos": 186, "type": "DATASET", "confidence": 0.9188744723796844}]}, {"text": "The weights of the log-linear interpolation model were optimized via minimum error rate training (MERT) on the TED development set, using 200 best translations at each tuning iteration.", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 69, "end_pos": 103, "type": "METRIC", "confidence": 0.84813420687403}, {"text": "TED development set", "start_pos": 111, "end_pos": 130, "type": "DATASET", "confidence": 0.9366521040598551}]}, {"text": "This paper investigates the effects of language model adaptation via bilingual latent semantic modeling on the TED background LM against a baseline model that uses only the TED LM.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6391786436239878}]}], "tableCaptions": [{"text": " Table 1: Sample unigram probabilities of the adaptation  model for document #230, compared to the baseline un- igram probabilities. The French words selected are se- mantically related to the English words in the adapted  document. The PLSA adaptation infers higher unigram  probabilities for words with latent topics related to the  source document.", "labels": [], "entities": [{"text": "Sample unigram probabilities", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8773941993713379}]}, {"text": " Table 2: Perplexity, BLEU, and NIST scores for the base- line and adapted models. The perplexity scores are aver- aged across each document-specific LM adaptation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9990003705024719}]}]}