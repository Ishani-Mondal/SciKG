{"title": [{"text": "Towards technology-assisted co-construction with communication partners", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we examine the idea of technology-assisted co-construction, where the communication partner of an AAC user can make guesses about the intended messages , which are included in the user's word completion/prediction interface.", "labels": [], "entities": [{"text": "word completion/prediction interface", "start_pos": 202, "end_pos": 238, "type": "TASK", "confidence": 0.8112508594989777}]}, {"text": "We run some human trials to simulate this new interface concept, with subjects predicting words as the user's intended message is being generated in real time with specified typing speeds.", "labels": [], "entities": []}, {"text": "Results indicate that people can provide substantial keystroke savings by providing word completion or prediction, but that the savings are not as high as n-gram language models.", "labels": [], "entities": [{"text": "word completion or prediction", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.7738149166107178}]}, {"text": "Interestingly , the language model and human predictions are complementary in certain key ways-humans doing a better job in some circumstances on contextually salient nouns.", "labels": [], "entities": []}, {"text": "We discuss implications of the enhanced co-construction interface for real-time message generation in AAC direct selection devices.", "labels": [], "entities": [{"text": "message generation", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7087788432836533}, {"text": "AAC direct selection", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8242024183273315}]}], "introductionContent": [{"text": "Individuals who cannot use standard keyboards for text entry because of physical disabilities have a number of alternative text entry methods that permit typing.", "labels": [], "entities": [{"text": "text entry", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.7711601853370667}, {"text": "text entry", "start_pos": 123, "end_pos": 133, "type": "TASK", "confidence": 0.7169335186481476}]}, {"text": "Referred to as keyboard emulation within augmentative and alternative communication (AAC), there are many different access options for the user, ranging from direct selection of letters with any anatomical pointer (e.g., head, eyes) to use of a binary switch -triggered by button-press, eye-blink or even through event related potentials (ERP) such as the P300 detected in EEG signals.", "labels": [], "entities": [{"text": "augmentative and alternative communication (AAC)", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.7501147985458374}]}, {"text": "These options allow the individual to indirectly select a symbol based on some process for scanning through alternatives (.", "labels": [], "entities": []}, {"text": "Typing speed is a challenge, yet is critically important for usability, and as a result there is a significant line of research into the utility of statistical language models for improving typing speed ().", "labels": [], "entities": []}, {"text": "Methods of word, symbol, phrase and message prediction via statistical language models are widespread in both direct selection and scanning devices (.", "labels": [], "entities": [{"text": "word, symbol, phrase and message prediction", "start_pos": 11, "end_pos": 54, "type": "TASK", "confidence": 0.5488002710044384}]}, {"text": "To the extent that the predictions are accurate, the number of keystrokes required to type a message can be dramatically reduced, greatly speeding typing.", "labels": [], "entities": []}, {"text": "AAC devices for spontaneous and novel text generation are intended to empower the user of the system, to place them in control of their own communication, and reduce their reliance on others for message formulation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7381330132484436}, {"text": "message formulation", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.7313398271799088}]}, {"text": "As a result, all such devices (much like standard personal computers) are built fora single user, with a single keyboard and/or alternative input interface, which is driven by the user of the system.", "labels": [], "entities": []}, {"text": "The unilateral nature of these high technology solutions to AAC stands in contrast to common low technology solutions, which rely on collaboration between the individual formulating the message and their communication partner.", "labels": [], "entities": [{"text": "AAC", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9360470175743103}]}, {"text": "Many adults with acquired neurological conditions rely on communication partners for co-construction of messages (.", "labels": [], "entities": []}, {"text": "One key reason why low-tech co-construction maybe preferred to high-tech stand-alone AAC system solutions is the resulting speed of communication.", "labels": [], "entities": []}, {"text": "Whereas spoken language reaches more than one hundred words per minute and an average speed typist using standard touch typing will achieve approximately 35 words per minute, a user of an AAC device will typically input text in the 3-10 words per minute range.", "labels": [], "entities": []}, {"text": "With a communication partner guess-22 ing the intended message and requesting confirmation, the communication rate can speedup dramatically.", "labels": [], "entities": []}, {"text": "For face-to-face communication -a modality that is currently very poorly served by AAC devices -such a speedup is greatly preferred, despite any potential authorship questions.", "labels": [], "entities": []}, {"text": "Consider the following low-tech scenario.", "labels": [], "entities": []}, {"text": "Sandy is locked-in, with just a single eye-blink serving to provide binary yes/no feedback.", "labels": [], "entities": []}, {"text": "Sandy's communication partner, Kim, initiates communication by verbally stepping through an imagined row/column grid, first by number (to identify the row); then by letter.", "labels": [], "entities": []}, {"text": "In such away, Sandy can indicate the first desired symbol.", "labels": [], "entities": []}, {"text": "Communication can continue in this way until Kim has a good idea of the word that Sandy intends and proposes the word.", "labels": [], "entities": []}, {"text": "If Sandy says yes, the word has been completed, much as automatic word completion may occur within an AAC device.", "labels": [], "entities": []}, {"text": "But Kim doesn't necessarily stop with word completion; subsequent word prediction, phrase prediction, in fact whole utterance prediction can follow, driven by Kim's intuitions derived from knowledge of Sandy, true sensitivity to context, topic, social protocol, etc.", "labels": [], "entities": [{"text": "word completion", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7355050891637802}, {"text": "word prediction", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7349066883325577}, {"text": "phrase prediction", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7621637284755707}, {"text": "whole utterance prediction", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.6694864630699158}]}, {"text": "It is no wonder that such methods are often chosen over high-tech alternatives.", "labels": [], "entities": []}, {"text": "In this paper, we present some preliminary ideas and experiments on an approach to providing technology support to this sort of co-construction during typing.", "labels": [], "entities": []}, {"text": "The core idea is to provide an enhanced interface to the communication partner (Kim in the example above), which does not allow them to directly contribute to the message construction, but rather to indirectly contribute, by predicting what they believe the individual will type next.", "labels": [], "entities": []}, {"text": "Because most text generation AAC devices typically already rely upon symbol, word and phrase prediction from statistical language models to speed text input, the predictions of the conversation partner could be used to influence (or adapt) the language model.", "labels": [], "entities": [{"text": "text generation AAC", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.6821569204330444}]}, {"text": "Such adaptation could be as simple as assigning high probability to words or symbols explicitly predicted by the communication partner, or as complex as deriving the topic or context from the partner's predictions and using that context to improve the model.", "labels": [], "entities": []}, {"text": "Statistical language models in AAC devices can capture regularities in language, e.g., frequent word collocations or phrases and names commonly used by an individual.", "labels": [], "entities": []}, {"text": "People, however, have access to much more information than computational models, including rich knowledge of language, any relevant contextual factors that may skew prediction, familiarity with the AAC user, and extensive world knowledge -none of which can be easily included in the kinds of simple statistical models that constitute the current state of the art.", "labels": [], "entities": []}, {"text": "People are typically quite good at predicting what might come next in a sentence, particularly if it is part of a larger discourse or dialogue.", "labels": [], "entities": [{"text": "predicting what might come next in a sentence", "start_pos": 35, "end_pos": 80, "type": "TASK", "confidence": 0.862411305308342}]}, {"text": "Indeed, some of the earliest work looking at statistical models of language established the entropy of English by asking subjects to play a simple language guessing game.", "labels": [], "entities": []}, {"text": "The socalled \"Shannon game\" starts with the subject guessing the first letter of the text.", "labels": [], "entities": []}, {"text": "Once they have guessed correctly, it is uncovered, and the subject guesses the next letter, and soon.", "labels": [], "entities": []}, {"text": "A similar game could be played with words instead of letters.", "labels": [], "entities": []}, {"text": "The number of guesses required is a measure of entropy in the language.", "labels": [], "entities": []}, {"text": "People are understandably very good at this game, often correctly predicting symbols on the first try for very long stretches of text.", "labels": [], "entities": []}, {"text": "No purely computational model can hope to match the contextual sensitivity, partner familiarity, or world knowledge that a human being brings to such a task.", "labels": [], "entities": []}, {"text": "A co-construction scenario differs from a Shannon game in terms of the time constraints under which it operates.", "labels": [], "entities": []}, {"text": "The communication partner in such a scenario must offer completions and predictions to the user in away that actually speeds communication relative to independent text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 163, "end_pos": 178, "type": "TASK", "confidence": 0.735032707452774}]}, {"text": "Given an arbitrary amount of time, it is clear that people have greater information at their disposal for predicting subsequent content; what happens under time constraints is less clear.", "labels": [], "entities": [{"text": "predicting subsequent content", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.8610911170641581}]}, {"text": "Indeed, in this paper we demonstrate that the time constraints put human subjects at a strong disadvantage relative to language models in the scenarios we simulated.", "labels": [], "entities": []}, {"text": "While it is far from clear that this disadvantage will also apply in scenarios closer to the motivating example given above, it is certainly the case that providing useful input is a challenging task.", "labels": [], "entities": []}, {"text": "The principal benefit of technology-assisted coconstruction with communication partners is making use of the partner's knowledge of language and context, as well as their familiarity with the AAC user and the world, to yield better predictions of likely continuations than are currently made by the kinds 23 of relatively uninformed (albeit state of the art) computational language models.", "labels": [], "entities": []}, {"text": "A secondary benefit is that such an approach engages the conversation partner in a high utility collaboration during the AAC user's turn, rather than simply sitting and waiting for the reply to be produced.", "labels": [], "entities": []}, {"text": "Lack of engagement is a serious obstacle to successful conversation in AAC ().", "labels": [], "entities": [{"text": "AAC", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.7338348031044006}]}, {"text": "The slow speed of AAC input is itself a contributing factor to AAC user dissatisfaction with face-to-face conversation, one of the most critical modes of human social interaction, and the one least served by current technology.", "labels": [], "entities": [{"text": "AAC input", "start_pos": 18, "end_pos": 27, "type": "TASK", "confidence": 0.7942551970481873}, {"text": "AAC user dissatisfaction", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.8464952309926351}]}, {"text": "Because of the slow turnaround, the conversation partner tends to lose focus and interest in the conversation, leading to shorter and less satisfying exchanges than those enjoyed by those using spoken language.", "labels": [], "entities": []}, {"text": "A system which leverages communication partner predictions will more fully engage the conversation partner in the process, rather than forcing them to wait fora response with nothing to do.", "labels": [], "entities": []}, {"text": "Importantly, an enhanced interface such as that proposed here provides predictive input from the communication partner, but not direct compositional input.", "labels": [], "entities": []}, {"text": "The responsibility of selecting symbols and words during text entry remains with the AAC user, as the sole author of the text.", "labels": [], "entities": [{"text": "text entry", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.6421114951372147}]}, {"text": "In the preliminary experiments presented later in the paper, we simulate a direct selection typing system with word prediction, and measure the utility of human generated word completions and predictions relative to n-gram models.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 111, "end_pos": 126, "type": "TASK", "confidence": 0.7890443503856659}]}, {"text": "In such a scenario, n-gram predictions can be replaced or augmented by human predictions.", "labels": [], "entities": []}, {"text": "This illustrates how easily technology assisted coconstruction with communication partners could potentially be integrated into a user's interface.", "labels": [], "entities": []}, {"text": "Despite the lack of speedup achieved versus ngram models in the results reported below, the potential for capturing communication partner intuitions about AAC user intended utterances seems a compelling topic for future research.", "labels": [], "entities": [{"text": "capturing communication partner intuitions about AAC user intended utterances", "start_pos": 106, "end_pos": 183, "type": "TASK", "confidence": 0.7596279780069987}]}], "datasetContent": [{"text": "In this section, we present a preliminary experiment to evaluate the potential utility of our technologyassisted co-construction scenario.", "labels": [], "entities": []}, {"text": "The experiment is akin to a Shannon Game), but with a time limit for guesses imposed by the speed of typing.", "labels": [], "entities": []}, {"text": "For the current experiment we chose 5 seconds per keystroke as the simulated typing speed: target sentences appeared one character at a time, every five seconds.", "labels": [], "entities": []}, {"text": "The subjects' task was to provide a For this preliminary experiment, we used a simple program running in the terminal window of a Mac laptop.", "labels": [], "entities": []}, {"text": "shows a screenshot from this program in operation.", "labels": [], "entities": []}, {"text": "The target string is displayed at the top of the terminal window, one character at a time, with the carat symbol showing white space word boundaries.", "labels": [], "entities": []}, {"text": "Predicted word completions are made by typing with a standard qwerty keyboard; and when the enter key is pressed, the word that has been typed is aligned with the current incomplete word.", "labels": [], "entities": [{"text": "word completions", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7005654275417328}]}, {"text": "If it is consistent with the prefix of the word that has been typed, it remains as a candidate for completion.", "labels": [], "entities": []}, {"text": "When the current five second interval has passed, the set of accumulated predictions are filtered to just those which are consistent with the new letter that the user would have typed (e.g., 'i' in).", "labels": [], "entities": []}, {"text": "If the correct word completion for the target string is present, it is selected with the following keystroke.", "labels": [], "entities": []}, {"text": "Otherwise the following letter will be typed (with the typical 5-second delay) and the interface proceeds as before.", "labels": [], "entities": []}, {"text": "Three able-bodied, adult, literate subjects were recruited for this initial experiment, and all three completed trials with both Enron email and New York Times target strings.", "labels": [], "entities": [{"text": "New York Times target strings", "start_pos": 145, "end_pos": 174, "type": "DATASET", "confidence": 0.7737827658653259}]}, {"text": "The Enron data comes from the Enron email dataset (http://www-2.cs.cmu.edu/\u223cenron/) and the NY Times data from the English Gigaword corpus (LDC2007T07).", "labels": [], "entities": [{"text": "Enron data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9292614161968231}, {"text": "Enron email dataset", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9240592916806539}, {"text": "NY Times data", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.8877628842989603}, {"text": "English Gigaword corpus (LDC2007T07)", "start_pos": 115, "end_pos": 151, "type": "DATASET", "confidence": 0.8669008513291677}]}, {"text": "Both corpora were pre-processed to remove duplicate data (e.g., spam or multiple recipient emails), tabular data and other material that does not represent written sentences.", "labels": [], "entities": []}, {"text": "Details on this normalization can be found in The two corpora were split into training and testing sets, to allow for training of n-gram language models to compare word completion performance.", "labels": [], "entities": []}, {"text": "To ensure fair comparison between n-gram and human word completion performance, no sentences in the test sets were seen in the training data.", "labels": [], "entities": []}, {"text": "From each test corpus, we extracted sets of 10 contiguous sentences at periodic intervals, to use as test or practice sets.", "labels": [], "entities": []}, {"text": "Each subject used a 10 sentence practice set from the NY Times to become familiar with the task and interface; then performed the word completion task on one 10 sentence set from the NY Times and one 10 sentence set from the Enron corpus.", "labels": [], "entities": [{"text": "NY Times", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8581121861934662}, {"text": "word completion", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.7499537467956543}, {"text": "NY Times", "start_pos": 183, "end_pos": 191, "type": "DATASET", "confidence": 0.8371418118476868}, {"text": "Enron corpus", "start_pos": 225, "end_pos": 237, "type": "DATASET", "confidence": 0.9323368966579437}]}, {"text": "Statistics of the training and test sets are given in.", "labels": [], "entities": []}, {"text": "Language models were n-gram word-based models trained from the given corpora using Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We performed no pruning on the models.", "labels": [], "entities": []}, {"text": "We evaluate in terms of keystroke savings percentage.", "labels": [], "entities": [{"text": "keystroke savings percentage", "start_pos": 24, "end_pos": 52, "type": "METRIC", "confidence": 0.9437041680018107}]}, {"text": "Let k be the baseline number of keystrokes without word completion, which is the number of characters in the sample, i.e., 1 keystroke per character.", "labels": [], "entities": []}, {"text": "With a given word completion method, let c be the number of keystrokes required to enter the text, i.e., if the word completion method provides correct words for selection, those will reduce the number of keystrokes required . Then keystroke savings percentage is 100 * (k \u2212 c)/k, the percentage of original keystrokes that were saved with word completion.", "labels": [], "entities": [{"text": "word completion", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7151684165000916}]}, {"text": "shows the keystroke savings percentage on our two tasks for three n-gram language models (unigram, bigram and trigram) and our three subjects.", "labels": [], "entities": []}, {"text": "It is clear from this table that the n-gram language models are achieving much higher keystroke savings than our three human subjects.", "labels": [], "entities": []}, {"text": "Further, our three subjects performed quite similarly, not only in com- parison with each other, but across the two tasks.", "labels": [], "entities": []}, {"text": "On the face of it, the relatively poor performance of the human predictors might be surprising, given that the original Shannon game was intended to establish a lower bound on the entropy of English.", "labels": [], "entities": []}, {"text": "The assumption has always been that people have better language models than we can hope to learn automatically.", "labels": [], "entities": []}, {"text": "However, in contrast to the original Shannon game, our predictions are carried outwith a fairly tight time limit, i.e., predictions need to be made within a fairly short period in order to be made available to individuals for word completion.", "labels": [], "entities": [{"text": "word completion", "start_pos": 226, "end_pos": 241, "type": "TASK", "confidence": 0.7556684613227844}]}, {"text": "The time limit within the current scenario is one factor that seems to be putting the subjects at a disadvantage compared to automated n-gram models on this task.", "labels": [], "entities": []}, {"text": "There area couple of additional reasons why ngram models are performing better on these tasks.", "labels": [], "entities": []}, {"text": "First, they are specific domains with quite ample training data for the language models.", "labels": [], "entities": []}, {"text": "As the amount of training data decreases -which would certainly be the case for individual AAC users -the efficacy of the n-gram models decrease.", "labels": [], "entities": []}, {"text": "Second, there is a 1-character advantage of n-gram models relative to human predictions in this approach.", "labels": [], "entities": []}, {"text": "To see this point clearly, consider the position at the start of the string.", "labels": [], "entities": []}, {"text": "N-gram models can (for practical purposes) instantaneously provide predictions for that word.", "labels": [], "entities": []}, {"text": "But our subjects must begin typing the words that they are predicting for this position at the same time the individual is making their first keystroke.", "labels": [], "entities": []}, {"text": "Those predictions do not become operative until after that keystroke.", "labels": [], "entities": []}, {"text": "Hence the time overhead of prediction places a lag relative to what is possible for the n-gram model.", "labels": [], "entities": [{"text": "prediction", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.9653279185295105}]}, {"text": "We will return to this point in the discussion section at the end of the paper.", "labels": [], "entities": []}, {"text": "There are some scenarios, however, where the subjects did provide word completions prior to the trigram language model in both domains.", "labels": [], "entities": [{"text": "word completions", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.6973934769630432}]}, {"text": "Interestingly, a fairly large fraction of these words were faster than n-gram for more than one of the three 26: Words completed using subject suggestions with fewer keystrokes than trigram model.", "labels": [], "entities": []}, {"text": "Bold indicates more than one subject was faster for that word. subjects.", "labels": [], "entities": []}, {"text": "shows the list of these words for our trials.", "labels": [], "entities": []}, {"text": "These tended to be longer, open-class words with high topical importance.", "labels": [], "entities": []}, {"text": "In addition, they tended to be words with common word prefixes, which lead to higher confusability in the ngram model.", "labels": [], "entities": []}, {"text": "Of course, common prefixes also lead to higher confusability in our subjects, yet they appear to be able to leverage their superior context sensitivity to yield effective disambiguation earlier than the n-gram model in these cases.", "labels": [], "entities": []}, {"text": "Based on these results, we designed a second experiment, with a few key changes from this preliminary experiment, including an improved interface, the ability to predict as well as complete, and a domain that is closer to a proposed model for this coconstruction task.", "labels": [], "entities": []}, {"text": "Based on the preliminary experiment, we created anew protocol and ran seven able-bodied, adult, literate subjects.", "labels": [], "entities": []}, {"text": "We changed the interface and domain in ways that we believed would make a difference in the ability of subjects to compete with ngram models in keystroke savings.", "labels": [], "entities": []}, {"text": "What remained the same was the timing of the interface: characters for target strings were displayed every five seconds.", "labels": [], "entities": [{"text": "timing", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9883600473403931}]}, {"text": "Word completions were then evaluated for consistency with what had been typed, and if the correct word was present, the word was completed and revealed, and typing continued.", "labels": [], "entities": [{"text": "consistency", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.95563143491745}]}, {"text": "Data Our primary motivating case for technologyassisted co-construction comes from face-to-face dialog, yet the corpora from which target strings were extracted in the preliminary experiments were from large corpora of text produced under very different conditions.", "labels": [], "entities": []}, {"text": "One corpus that does represent a variedtopic, conversational dialog scenario is the Switchboard corpus, which contains transcripts of both sides of telephone conversations.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.8696238994598389}]}, {"text": "The idea in using this data was to provide some number of utterances of dialog context (from the 10 previous dialog turns), and then ask subjects to provide word completions for some number of subsequent utterances.", "labels": [], "entities": []}, {"text": "While the Switchboard corpus does represent the kind of conversational dialog we are interested in, it is a spoken language corpus, yet we are modeling written (typed) language.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.7463941276073456}]}, {"text": "The difference between written and spoken language does present something of an issue for our task.", "labels": [], "entities": []}, {"text": "To mitigate this mismatch somewhat, we made use of the Switchboard section of the Penn Treebank (, which contains syntactic annotations of the Switchboard transcripts, including explicit marking of disfluencies (\"EDITED\" non-terminals in the treebank), interjections or parentheticals such as \"I mean\" or \"you know\".", "labels": [], "entities": [{"text": "Switchboard section of the Penn Treebank", "start_pos": 55, "end_pos": 95, "type": "DATASET", "confidence": 0.7651279469331106}]}, {"text": "Using these syntactic annotations, we produced edited transcripts that omit much of the spoken language specific phenomena, thus providing a closer approximation to the kind of written dialogs we would like to simulate.", "labels": [], "entities": []}, {"text": "In addition, we decased the corpus and removed all characters except the following: the 26 letters of the English alphabet, the apostrophe, the space, and the dash.", "labels": [], "entities": []}, {"text": "Interface shows the graphical user interface that was created for these trials.", "labels": [], "entities": [{"text": "Interface", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8738359808921814}]}, {"text": "In the upper box, ten utterances from the context of the dialog are presented, with an indication of which speaker (A or B) took the turn.", "labels": [], "entities": []}, {"text": "Participants are asked to first read this context and then press enter to begin the session.", "labels": [], "entities": []}, {"text": "Below this box, the current utterance is displayed, along with which of the two participants is currently producing the utterance.", "labels": [], "entities": []}, {"text": "As in the previous experiment, the string is displayed one character at a time in this region.", "labels": [], "entities": []}, {"text": "Below this is a text box where word completions and predictions are entered.", "labels": [], "entities": [{"text": "word completions", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7100122720003128}]}, {"text": "Finally, at the bottom of the interface, shows two of the five rows of current word completions (left column) and next word predictions (right column).", "labels": [], "entities": []}, {"text": "Perhaps the largest departure from the preliminary experiment is the ability to not only complete the current word but also to provide predictions about the subsequent word.", "labels": [], "entities": []}, {"text": "The subject uses a space delimiter to indicate whether predictions are for the current word or for the subsequent word.", "labels": [], "entities": []}, {"text": "Words preceding a space are taken as current word completions; the first word after a space is taken as a 27 Figure 2: Experimental graphical user interface subsequent word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.7252369076013565}]}, {"text": "To just predict the subsequent word, one can lead with a space, which results in no current word completion and whatever comes after the space as next word prediction.", "labels": [], "entities": []}, {"text": "Once the current word is complete, any words on the subsequent word prediction list are immediately shifted to the word completion list.", "labels": [], "entities": []}, {"text": "We limited current and next word predictions to five.", "labels": [], "entities": []}, {"text": "We selected ten test dialogs, and subjects produced word completions and predictions for three utterances per dialog, fora total of thirty utterances.", "labels": [], "entities": []}, {"text": "We selected the test dialogs to conform to the following characteristics: 1.", "labels": [], "entities": []}, {"text": "Each group of three utterances was consecutive and spoken by the same person.", "labels": [], "entities": []}, {"text": "2. Each utterance contained more than 15 characters of text.", "labels": [], "entities": []}, {"text": "3. Each group of three utterances began turninitially; the first of the three utterances was always immediately after the other speaker in the corpus had spoken at least two consecutive utterances of 15 characters or more.", "labels": [], "entities": []}, {"text": "4. Each group of three utterances was far enough into its respective conversation that there was enough text to provide the ten lines of context required above.", "labels": [], "entities": []}, {"text": "Language models used to contrast with human performance on this task were trained separately for every conversation in the test set.", "labels": [], "entities": []}, {"text": "For each conversation, Kneser-Ney smoothed n-gram models were built using all other conversations in the normalized Switchboard corpus.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 116, "end_pos": 134, "type": "DATASET", "confidence": 0.8079378008842468}]}, {"text": "Thus no conversation is in its own training data.", "labels": [], "entities": []}, {"text": "shows statistics of training and test sets.", "labels": [], "entities": []}, {"text": "shows the results for n-gram models and our seven subjects on this test.", "labels": [], "entities": []}, {"text": "Despite the differences in the testing scenario from the preliminary experiment, we can see that the results are very similar to what was found in that experiment.", "labels": [], "entities": []}, {"text": "Also similar to the previous trial was the fact that a large percentage of tokens for which subjects provided faster word completion than the trigram model were faster for multiple subjects.", "labels": [], "entities": [{"text": "word completion", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.6789160817861557}]}, {"text": "shows the nine words that were completed faster by more than half of the subjects than the trigram model.", "labels": [], "entities": []}, {"text": "Thus, while there is some individual variation in task performance, subjects were fairly consistent in their ability to predict.: Words completed in more than half of the Switchboard trials using subject suggestions with fewer keystrokes than trigram model. from both experiments are negative, in terms of the ability of our human subjects to speedup communication via word prediction under time constraints beyond what is achievable with n-gram language models.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 369, "end_pos": 384, "type": "TASK", "confidence": 0.7270494401454926}]}, {"text": "These results are somewhat surprising given conventional wisdom about the superiority of human language models versus their simplified computational counterparts.", "labels": [], "entities": []}, {"text": "One key reason driving the divergence from conventional wisdom is the time constraint on production of predictions.", "labels": [], "entities": []}, {"text": "Another is the artificiality of the task and relative unfamiliarity of the subjects with the individuals communicating.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for each task of n-gram training corpus  size and test set size in terms of sentences, words and  characters (baseline keystrokes)", "labels": [], "entities": []}, {"text": " Table 2: Keystroke savings percentage for test set across  models and subjects", "labels": [], "entities": [{"text": "Keystroke savings", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9142470061779022}]}, {"text": " Table 4: Statistics for the Switchboard task of n-gram  training corpus size and test set size in terms of utter- ances, words and characters (baseline keystrokes)", "labels": [], "entities": []}, {"text": " Table 5: Keystroke savings percentage for Switchboard test set across models and subjects", "labels": [], "entities": [{"text": "Keystroke savings", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8917848765850067}]}]}