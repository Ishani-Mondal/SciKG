{"title": [{"text": "Who wrote What Where: Analyzing the content of human and automatic summaries", "labels": [], "entities": [{"text": "Analyzing the content of human and automatic summaries", "start_pos": 22, "end_pos": 76, "type": "TASK", "confidence": 0.6080604717135429}]}], "abstractContent": [{"text": "ive summarization has been a long-standing and long-term goal in automatic sum-marization, because systems that can generate abstracts demonstrate a deeper understanding of language and the meaning of documents than systems that merely extract sentences from those documents.", "labels": [], "entities": []}, {"text": "Genest (2009) showed that summaries from the top automatic sum-marizers are judged as comparable to manual extractive summaries, and both are judged to be far less responsive than manual abstracts, As the state of the art approaches the limits of extractive summarization, it becomes even more pressing to advance abstractive summa-rization.", "labels": [], "entities": []}, {"text": "However, abstractive summarization has been sidetracked by questions of what qualifies as important information, and how do we find it?", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.49722933769226074}]}, {"text": "The Guided Summarization task introduced at the Text Analysis Conference 2010 attempts to neutralize both of these problems by introducing topic categories and lists of aspects that a responsive summary should address.", "labels": [], "entities": [{"text": "Guided Summarization task", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7703983982404073}, {"text": "Text Analysis Conference 2010", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.8220883309841156}]}, {"text": "This design results in more similar human models, giving the automatic summa-rizers a more focused target to pursue, and also provides detailed diagnostics of summary content , which cancan help build better meaning-oriented summarization systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "What qualifies as important information and how do we find it?", "labels": [], "entities": []}, {"text": "These questions have been leading research in automatic summarization since its beginnings, and we are still nowhere near a definitive answer.", "labels": [], "entities": [{"text": "automatic summarization", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.6503323018550873}]}, {"text": "Worse, experiments with humans subjects suggest a definitive answer might not even exist.", "labels": [], "entities": []}, {"text": "With all their near-perfect language understanding and world knowledge, two human summarizers will still produce two different summaries of the same text, simply because they will disagree on what's important.", "labels": [], "entities": []}, {"text": "Fortunately, usually some of this information will overlap.", "labels": [], "entities": []}, {"text": "This is represented by the idea behind the Pyramid evaluation framework (, where different levels of the pyramid represent the proportion of concepts (\"Summary Content Units\", or SCUs) mentioned by 1 ton summarizers in summaries of the same text.", "labels": [], "entities": []}, {"text": "Usually, there are very few SCUs that are mentioned by all summarizers, a few more that are mentioned by some of them, and the greatest proportion are the SCUs that are mentioned by individual summarizers only.", "labels": [], "entities": []}, {"text": "This variance in what should be a \"gold standard\" makes research in automatic summarization methods particularly difficult.", "labels": [], "entities": [{"text": "summarization", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.8199049234390259}]}, {"text": "How can we reach a goal so vague and under-defined?", "labels": [], "entities": []}, {"text": "Using term frequency to determine important concepts in a text has proven to be very successful, largely because of its simplicity and universal applicability, but statistical methods can only provide the most basic level of performance.", "labels": [], "entities": []}, {"text": "On the other hand, there is no real motivation to use any deeper meaning-oriented text analysis if we are not even certain what information to look for in order to produce a responsive summary.", "labels": [], "entities": []}, {"text": "To address these concerns, the Summarization track at the 2010 Text Analysis Conference 1 (TAC) introduced anew summarization task -Guided Summarization -in which topics are divided into narrow categories and a list of required aspects is provided for each category.", "labels": [], "entities": [{"text": "Summarization track at the 2010 Text Analysis Conference 1 (TAC)", "start_pos": 31, "end_pos": 95, "type": "TASK", "confidence": 0.7773256649573644}]}, {"text": "This serves two purposes: first, it creates a more focused target for automatic summarizers, neutralizing human variance and pointing to concrete types of information the reader requires, and second, it provides a detailed diagnostic tool to analyze the content of automatic summaries, which can help build more meaningoriented systems.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.8600755333900452}]}, {"text": "This paper shows how these objectives were achieved in TAC 2010, looking at the similarity of human-crafted models, and then using the category and aspect information to look in depth at the differences between human and top automatic summarizers, discovering strengths and weaknesses of automatic systems and areas for improvement.", "labels": [], "entities": [{"text": "TAC 2010", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.7925009429454803}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Macro-average Pyramid and Responsiveness  scores for initial and update summaries for years 2008- 2010. Responsiveness scores for 2009 were scaled from  a ten-point to a five-point scale.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of SCUs with weights 1-4 in pyra- mids for initial and update summaries for years 2008- 2010.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9612507224082947}]}]}