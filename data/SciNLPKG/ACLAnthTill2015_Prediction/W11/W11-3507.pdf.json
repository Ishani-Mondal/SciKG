{"title": [{"text": "From pecher to p\u00eacher\u2026 or p\u00e9cher: Simplifying French Input by Accent Prediction", "labels": [], "entities": [{"text": "Simplifying French Input by Accent Prediction", "start_pos": 34, "end_pos": 79, "type": "TASK", "confidence": 0.7480928103129069}]}], "abstractContent": [{"text": "In this paper we describe our approach to building a French text input system, in which no explicit typing of accents is required.", "labels": [], "entities": []}, {"text": "This makes typing of French available to a wider range of keyboards and to occasional writers of the language.", "labels": [], "entities": [{"text": "typing of French", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8971716165542603}]}, {"text": "Our method is built on the noisy-channel model, and achieves 99.8% character-level accuracy on the test data consisting of formal and casual writing styles.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9854132533073425}]}, {"text": "This system is part of a larger project of making text input easier for multiple languages , including those that have traditionally been the target of input methods (such as Chinese and Japanese) as well as phonetically based non-Roman script languages (such as Greek, Russian, and Indic languages).", "labels": [], "entities": []}, {"text": "A demo of this system including these languages will be shown at the workshop.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research on input methods has so far been limited to those languages in which an input method editor (IME) is an absolute necessity, such as Chinese and Japanese, that are written with a very large number of characters.", "labels": [], "entities": []}, {"text": "However, with the recent availability of text prediction in typing web search queries and text on mobile devices, it has become obvious that text input methods area very useful tool for many languages beyond the traditional IME languages (e.g.,.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7293912172317505}]}, {"text": "Phonetic text input has also become widely popular in inputting non-Roman script languages in the last few years.", "labels": [], "entities": [{"text": "Phonetic text input", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7269652088483175}]}, {"text": "In this paper we deal with a problem of text input fora language that has never been a target of traditional IME: French.", "labels": [], "entities": []}, {"text": "French uses the Roman alphabet with a few additions: accented vowels (\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00eb\u00ef\u00fc), the consonant '\u00e7', and the ligatures 'oe' and 'ae' Inputting these characters requires a special arrangement such as installing an international keyboard, using ALT codes (which uses the ALT key and a three or four digit code), 1 cutting and pasting from an existing text or inserting a symbol from a table.", "labels": [], "entities": []}, {"text": "This makes French typing especially difficult for those who do not input French on a regular basis.", "labels": [], "entities": [{"text": "French typing", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.7075907588005066}]}, {"text": "Automatic prediction of accents should make typing faster for native speakers as well, as accented characters account for 3.64 % of French text (computed based on our training data, to be mentioned in Section 4.1).", "labels": [], "entities": [{"text": "prediction of accents", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8429538011550903}]}, {"text": "2 Therefore, our goal is to correctly predict accents in French text within an IME scenario: users simply type characters without accents, and the accented characters are restored automatically.", "labels": [], "entities": []}, {"text": "We implemented our French input system based on the noisy-channel approach, which is commonly used for transliteration.", "labels": [], "entities": []}, {"text": "Our channel model is trained using finite state transducers; our motivation for this choice will be discussed in Section 2.", "labels": [], "entities": []}, {"text": "For language models, we use both character-and word-based n-gram models, in order to handle both contextual disambiguation of the words in the lexicon as well as accent prediction in out-of-vocabulary (OOV) words.", "labels": [], "entities": [{"text": "accent prediction in out-of-vocabulary (OOV) words", "start_pos": 162, "end_pos": 212, "type": "TASK", "confidence": 0.7472738176584244}]}, {"text": "We use abeam search decoder to find the best candidate.", "labels": [], "entities": []}, {"text": "The models and the decoder are described in Section 3.", "labels": [], "entities": []}, {"text": "We present our experimental results on the task of French accent prediction in Section 4, and show that our best model achieves 99.8% character-level accuracy on two test sets of different styles.", "labels": [], "entities": [{"text": "French accent prediction", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.7173305451869965}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9712338447570801}]}], "datasetContent": [{"text": "In this section we describe the experiments we ran to evaluate the quality of French accent restoration, which serves as the basis for the French text input method.", "labels": [], "entities": [{"text": "French accent restoration", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.652969757715861}]}, {"text": "The input to the task is French text without any accent, simulating the scenario where a user types unaccented French.", "labels": [], "entities": []}, {"text": "The output is fully accented French text.", "labels": [], "entities": []}, {"text": "We then evaluate this output against correctly accented reference French text.", "labels": [], "entities": []}, {"text": "We measured our results using character error rate (CER), which is based on the longest common subsequence match in characters between the reference and the best system output.", "labels": [], "entities": [{"text": "character error rate (CER)", "start_pos": 30, "end_pos": 56, "type": "METRIC", "confidence": 0.8717223505179087}]}, {"text": "This is a standard metric used in evaluating IME systems (e.g.,.", "labels": [], "entities": [{"text": "IME", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9540669322013855}]}, {"text": "Let N REF be the number of characters in a reference sentence, N SYS be the character length of a system output, and N LCS be the length of the longest common subsequence between them.", "labels": [], "entities": [{"text": "REF", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9455801248550415}]}, {"text": "Then the character-level recall is defined as N LCS /N REF , and the precision as N LCS /N SYS . CER based on recall (CER-R) and on precision (CER-P) are then defined as 1 -recall and 1 -precision, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9183818101882935}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9993501305580139}, {"text": "CER", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9428040981292725}, {"text": "recall (CER-R)", "start_pos": 110, "end_pos": 124, "type": "METRIC", "confidence": 0.8501878529787064}, {"text": "precision (CER-P)", "start_pos": 132, "end_pos": 149, "type": "METRIC", "confidence": 0.8895632922649384}, {"text": "1 -recall", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.5601216554641724}, {"text": "precision", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.5797848105430603}]}, {"text": "In transliteration scenarios where the character lengths of the system output and the reference may differ (e.g., a target character corresponds to multiple source characters), CER-R and CER-P will be different.", "labels": [], "entities": [{"text": "CER-R", "start_pos": 177, "end_pos": 182, "type": "METRIC", "confidence": 0.7367265820503235}]}, {"text": "In the case of French, however, the reference and the system output are the same length inmost cases (the only exceptions are the sentences that include the ligatures 'oe' and 'ae').", "labels": [], "entities": []}, {"text": "Therefore, we report the results using only CER-R in the next section.", "labels": [], "entities": [{"text": "CER-R", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.5328130125999451}]}, {"text": "show the results of French accent restoration in CER-R in two test sets, WMT2009 and MTLog, for various beam sizes (b=3, 10 and 30).", "labels": [], "entities": [{"text": "French accent restoration", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6151271859804789}, {"text": "CER-R", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9602164030075073}, {"text": "WMT2009", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.8432862758636475}, {"text": "MTLog", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.6144189834594727}]}, {"text": "Each row of the table refers to the different models we built and tested, with different combinations of the channel model, character-based and word-based language models.", "labels": [], "entities": []}, {"text": "The first row (E1) is the baseline model which only uses the channel model.", "labels": [], "entities": []}, {"text": "The rows E2, E4 and E6 are the systems that use the channel model and a character language model of various orders.", "labels": [], "entities": []}, {"text": "The rows E3, E5 and E7 are the models that additionally use the word trigram model for rescoring the 50-best results of E2, E4 and E6, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (in CER-R, in %) of accent prediction on WMT2009 and MTLog", "labels": [], "entities": [{"text": "CER-R", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8557794690132141}, {"text": "accent prediction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.8187186121940613}, {"text": "WMT2009", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9406456351280212}, {"text": "MTLog", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.7030366659164429}]}]}