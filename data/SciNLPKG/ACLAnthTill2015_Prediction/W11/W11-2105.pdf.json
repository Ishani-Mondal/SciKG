{"title": [{"text": "AMBER: A Modified BLEU, Enhanced Ranking Metric", "labels": [], "entities": [{"text": "AMBER", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6311648488044739}, {"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9900189638137817}]}], "abstractContent": [{"text": "This paper proposes anew automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties , and some text processing variants.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.7982966899871826}, {"text": "AMBER", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9541906714439392}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9918338060379028}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.999271810054779}]}, {"text": "There is very little linguistic information in AMBER.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8510218858718872}]}, {"text": "We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance.", "labels": [], "entities": [{"text": "WMT shared evaluation task", "start_pos": 108, "end_pos": 134, "type": "DATASET", "confidence": 0.6650874614715576}]}], "introductionContent": [{"text": "Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems.", "labels": [], "entities": [{"text": "machine translation (MT) quality", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.8483231763044993}, {"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.8318353891372681}]}, {"text": "Several metrics have been proposed in recent years.", "labels": [], "entities": []}, {"text": "Metrics such as BLEU), NIST), WER, PER, and TER) do not use any linguistic information -they only apply surface matching.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9985294342041016}, {"text": "WER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9885581731796265}, {"text": "PER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9156956076622009}, {"text": "TER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9770252704620361}]}, {"text": "METEOR (), METEOR-NEXT, TER-Plus (,, and TESLA () exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7208038568496704}, {"text": "TESLA", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.8279052376747131}, {"text": "part-of-speech tagging", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.6682780832052231}]}, {"text": "More sophisticated metrics such as RTE ( and DCU-LFG () use higher level syntactic or semantic analysis to score translations.", "labels": [], "entities": []}, {"text": "Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9981157779693604}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.998449444770813}]}, {"text": "This is probably due to the following facts: 1.", "labels": [], "entities": []}, {"text": "BLEU is language independent (except for word segmentation decisions).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9154773950576782}, {"text": "word segmentation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7388694733381271}]}, {"text": "2. BLEU can be computed quickly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9986629486083984}]}, {"text": "This is important when choosing a metric to tune an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9705673456192017}]}, {"text": "3. BLEU seems to be the best tuning metric from a quality point of view -i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9989379048347473}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9894906878471375}]}, {"text": "When we developed our own metric, we decided to make it a modified version of BLEU whose rankings of translations would (ideally) correlate even more highly with human rankings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9912716746330261}]}, {"text": "Thus, our metric is called AMBER: \"A Modified Bleu, Enhanced Ranking\" metric.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.7115877270698547}]}, {"text": "Some of the AMBER variants use an information source with a mild linguistic flavour -morphological knowledge about suffixes, roots and prefixes -but otherwise, the metric is based entirely on surface comparisons.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.7844462990760803}]}], "datasetContent": [{"text": "We evaluated AMBER on WMT data, using WMT 2008 all-to-English submissions as the dev set.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.6707071661949158}, {"text": "WMT data", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.9252018332481384}, {"text": "WMT 2008 all-to-English submissions", "start_pos": 38, "end_pos": 73, "type": "DATASET", "confidence": 0.9434193670749664}]}, {"text": "Test sets include WMT 2009 all-to-English, WMT 2010 all-to-English and 2010 English-to-all submissions.", "labels": [], "entities": [{"text": "WMT", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.9168018698692322}, {"text": "WMT", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8848192691802979}]}, {"text": "We used Spearman's rank correlation coefficient to measure the correlation of AMBER with the human judgments of translation at the system level.", "labels": [], "entities": [{"text": "rank correlation coefficient", "start_pos": 19, "end_pos": 47, "type": "METRIC", "confidence": 0.7308246493339539}]}, {"text": "The human judgment score we used is based on the \"Rank\" only, i.e., how often the translations of the system were rated as better than the translations from other systems).", "labels": [], "entities": []}, {"text": "Thus, AMBER and the other metrics were evaluated on how well their rankings correlated with the human ones.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.4706478416919708}]}, {"text": "For the sentence level, we use consistency rate, i.e., how consistent the ranking of sentence pairs is with the human judgments.", "labels": [], "entities": [{"text": "consistency rate", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.9876247644424438}]}], "tableCaptions": [{"text": " Table 1 summarizes the dev and test set  statistics.", "labels": [], "entities": []}, {"text": " Table 2.  3. We took the average of runs over input text  types 1, 4, and 6 (i.e. normalized text,  split type 1 and split type 3).  4. In Chunk penalty (CKP),", "labels": [], "entities": []}, {"text": " Table 2: Weight of each penalty", "labels": [], "entities": [{"text": "Weight", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9979866743087769}]}, {"text": " Table 3: Results of AMBER vs BLEU and METEOR", "labels": [], "entities": [{"text": "AMBER", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.7593536376953125}, {"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9910098314285278}, {"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9033989906311035}]}, {"text": " Table 4: Varying AMBER preprocessing (best  linguistic = bold, best non-ling. = underline)", "labels": [], "entities": [{"text": "AMBER preprocessing", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.4088364243507385}]}, {"text": " Table 5: Effect of tf-idf on AMBER(1,4,6)", "labels": [], "entities": [{"text": "AMBER", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.5328407883644104}]}, {"text": " Table 6: Dropping penalties from AMBER(1,4,6) - biggest drops on test in bold", "labels": [], "entities": [{"text": "Dropping", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9213346242904663}, {"text": "AMBER", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9452598094940186}]}]}