{"title": [{"text": "Measuring the Impact of Sense Similarity on Word Sense Induction", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.6373990674813589}]}], "abstractContent": [{"text": "Word Sense Induction (WSI) is an unsuper-vised learning approach to discovering the different senses of a word from its contextual uses.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7626842459042867}]}, {"text": "A core challenge to WSI approaches is distinguishing between related and possibly similar senses of a word.", "labels": [], "entities": [{"text": "WSI", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9825594425201416}]}, {"text": "Current WSI evaluation techniques have yet to analyze the specific impact of similarity on accuracy.", "labels": [], "entities": [{"text": "WSI evaluation", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.9492804110050201}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9955183267593384}]}, {"text": "Therefore , we present anew WSI evaluation that quantifies the relationship between the relat-edness of a word's senses and the ability of a WSI algorithm to distinguish between them.", "labels": [], "entities": []}, {"text": "Furthermore, we perform an analysis on sense confusions in SemEval-2 WSI task according to sense similarity.", "labels": [], "entities": []}, {"text": "Both analyses fora representative selection of clustering-based WSI approaches reveals that performance is most sensitive to the clustering algorithm and not the lexical features used.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many words in a language have several distinct meanings.", "labels": [], "entities": []}, {"text": "For example, \"earth\" may refer to the planet Earth, dirt, or solid ground, depending on the context.", "labels": [], "entities": []}, {"text": "The goal of Word Sense Induction (WSI) is to automatically discover the different senses by examining how a word is used.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7906616826852163}]}, {"text": "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law.", "labels": [], "entities": []}, {"text": "Furthermore, these discovered senses can be used to automatically expand lexical resources such as WordNet or FrameNet ( . Discovering the multiple senses is frequently confounded by the relationships between a word's senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9746243953704834}]}, {"text": "While homonyms such as \"bass\" or \"bank\" have unrelated senses, many polysemous words have interrelated senses, with lexicographers often in disagreement for the number of fine-grained senses.", "labels": [], "entities": []}, {"text": "For example, the most frequent four senses for \"law\" according to WordNet, shown in, are similar in several aspects and could be ascribed interchangeably in some contexts.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9801490902900696}]}, {"text": "The difficulty of automatically distinguishing two senses is proportional to their similarity because of the increasing likelihood of the two senses sharing similar contexts.", "labels": [], "entities": []}, {"text": "While the issue distinguishing between related senses is a recognized issue for Word Sense Disambiguation (), which uses supervised training to learn sense distinctions, measuring the impact of sense relatedness on the harder problem of WSI remains unaddressed.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.6703916788101196}, {"text": "WSI", "start_pos": 237, "end_pos": 240, "type": "TASK", "confidence": 0.7435383796691895}]}, {"text": "The recent SemEval WSI tasks) have provided a standard framework for evaluating WSI systems, with a controlled training corpus designed to limit sense ambiguity in the example contexts.", "labels": [], "entities": [{"text": "SemEval WSI tasks", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7747498154640198}]}, {"text": "However, given the potential relatedness of a word's senses, we view it necessary to consider how WSI methods perform relative to the degree of contextual ambiguity.", "labels": [], "entities": [{"text": "WSI", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9483329653739929}]}, {"text": "Our goal is therefore to quantify the similarity at which a WSI approach is unable to distinguish between two senses, which reflects the sense granularity at which the approach operates.", "labels": [], "entities": [{"text": "WSI", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9211822748184204}]}, {"text": "We propose two new evaluations.", "labels": [], "entities": []}, {"text": "The first, described in Section 4, uses a similarity-based pseudoword discrimination task to measure the discrimination capability for related senses along a graded scale of similarity.", "labels": [], "entities": [{"text": "similarity-based pseudoword discrimination task", "start_pos": 42, "end_pos": 89, "type": "TASK", "confidence": 0.6751462519168854}]}, {"text": "As a second evaluation, in 1 the collection of rules imposed by authority 2 legal document setting forth rules governing a particular kind of activity 3 a rule or body of rules of conduct inherent inhuman nature and essential to or binding upon human society 4 a generalization that describes recurring facts or events in nature: Definitions for the top four senses of \"law\" according to WordNet Section 5 we perform an error analysis using the SemEval-2010 WSI task, examining sense confusion relative to the sense similarities.", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 445, "end_pos": 466, "type": "TASK", "confidence": 0.4604227642218272}]}, {"text": "For both evaluations, we examine twenty different WSI clusteringbased models through combining five feature types and four clustering algorithms.", "labels": [], "entities": [{"text": "WSI clusteringbased", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7972974479198456}]}, {"text": "These models were selected to be representative of a wide class of existing algorithms as away of influence future algorithmic directions based on the current model's performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We expect that if sense similarity is a factor in sense confusion, the probability of confusion will increase with sense similarity.", "labels": [], "entities": []}, {"text": "Therefore, we measure the probability of labeling an instance with the incorrect OntoNotes sense relative to the sense similarity with the gold standard sense.", "labels": [], "entities": []}, {"text": "In order to calculate the incorrect assignments, the induced senses must be mapped to OntoNotes senses.", "labels": [], "entities": []}, {"text": "Each induced sense, s i , is mapped to the OntoNotes sense that occurs most frequently among the instances in the test corpus that are assigned induced sense s i . We note that this labeling process is only an approximate solution to assigning gold standard labels to induced senses.", "labels": [], "entities": []}, {"text": "A more robust  labeling could take into account the distribution of gold standard senses labels in the corpus from which the senses are induced; however, such labels are not available in the Task 14 training corpus.", "labels": [], "entities": []}, {"text": "For each incorrect sense assignment, we measure the similarity of the confused sense to the correct sense.", "labels": [], "entities": [{"text": "similarity", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.966030478477478}]}, {"text": "To our knowledge, no work has been done on calculating sense similarity within the OntoNotes sense hierarchy.", "labels": [], "entities": [{"text": "OntoNotes sense hierarchy", "start_pos": 83, "end_pos": 108, "type": "DATASET", "confidence": 0.7900687654813131}]}, {"text": "Therefore, we approximate OntoNotes sense similarity by using sense similarity in the WordNet ontology, on which has many similarity measures have been defined.", "labels": [], "entities": [{"text": "WordNet ontology", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.9696957767009735}]}, {"text": "Following, we estimate the WordNet sense similarity using the method proposed by.", "labels": [], "entities": [{"text": "WordNet sense similarity", "start_pos": 27, "end_pos": 51, "type": "METRIC", "confidence": 0.672719399134318}]}, {"text": "Each OntoNotes sense s i is mapped to a set of WordNet 3.0 senses Si = {wn 1 , . .", "labels": [], "entities": []}, {"text": ", wn n } using We suspect that this is in part because a word's OntoNotes senses have been designed to minimize sense confusion.", "labels": [], "entities": []}, {"text": "the sense mapping provided by the CoNLL shared task.", "labels": [], "entities": [{"text": "CoNLL shared task", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.762245774269104}]}, {"text": "The sense similarity for two OntoNotes senses is computed using one of two methods: (1) or sim = argmax where JCN indicates the Jiang-Conrath similarity of two WordNet senses, calculated using WordNet::Similarity (  To quantify the impact, we compare each system's error distribution against a null model over the set of incorrect test instances missed by that system.", "labels": [], "entities": [{"text": "argmax", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.973063051700592}, {"text": "WordNet", "start_pos": 193, "end_pos": 200, "type": "DATASET", "confidence": 0.9625864624977112}]}, {"text": "In the null model, the incorrect sense for each instance is selected with uniform probability from the available senses.", "labels": [], "entities": []}, {"text": "This behavior produces a distribution with no similarity bias.", "labels": [], "entities": []}, {"text": "The cumulative error distribution for the null model is not uniform due to multiple sense pairings having the same similarity.", "labels": [], "entities": []}, {"text": "To quantify the difference between a system's error distribution and corresponding null model, we calculate the G-test as a measure of Goodness of Fit (GoF).", "labels": [], "entities": [{"text": "G-test", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9646996855735779}, {"text": "Goodness of Fit (GoF)", "start_pos": 135, "end_pos": 156, "type": "METRIC", "confidence": 0.9570834040641785}]}, {"text": "The resulting p-values reflect the probability of observing the system's error distribution if there was no bias from sense-similarity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example confounders for \"festival\" and  \"laws\" and their similarities", "labels": [], "entities": []}, {"text": " Table 3: Unsupervised and Supervised scores on the SemEval-2010 WSI Task for each feature and clustering  models, with reference scores for the top performing systems for each evaluation shown below.", "labels": [], "entities": [{"text": "SemEval-2010 WSI Task", "start_pos": 52, "end_pos": 73, "type": "DATASET", "confidence": 0.5713462034861246}]}]}