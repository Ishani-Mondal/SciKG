{"title": [], "abstractContent": [{"text": "Stemming from distributed representation theories , we investigate the interaction between distributed structure and distributional meaning.", "labels": [], "entities": []}, {"text": "We propose a pure distributed tree (DT) and distributional distributed tree (DDT).", "labels": [], "entities": []}, {"text": "DTs and DDTs are exploited for defining distributed tree kernels (DTKs) and distributional distributed tree kernels (DDTKs).", "labels": [], "entities": []}, {"text": "We compare DTKs and DDTKs in two tasks: approximating tree kernels TK (Collins and Duffy, 2002); performing textual entailment recognition (RTE).", "labels": [], "entities": [{"text": "approximating tree kernels TK", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.7734610140323639}, {"text": "textual entailment recognition (RTE)", "start_pos": 108, "end_pos": 144, "type": "TASK", "confidence": 0.7973863283793131}]}, {"text": "Results show that DTKs correlate with TKs and perform in RTE better than DDTKs.", "labels": [], "entities": []}, {"text": "Then, including distributional vectors in distributed structures is a very difficult task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Demonstrating that distributional semantics is a semantic model of natural language is areal research challenge in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6433105170726776}]}, {"text": "Frege's principle of compositionality (Frege, 1884), naturally taken into account in logic-based semantic models of natural language, is hardly effectively included in distributional semantics models.", "labels": [], "entities": []}, {"text": "These models should compositionally derive distributional vectors for sentences and phrases from the distributional vectors of the composing words.", "labels": [], "entities": []}, {"text": "Besides vector averaging), that can model distributional meaning of sentences, recent distributional compositional models focus on finding distributional vectors of word pairs.", "labels": [], "entities": [{"text": "vector averaging", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.710515171289444}]}, {"text": "Scaling up these 2-word sequence models to the sentence level is not trivial as syntactic structure of sentences plays a very important role.", "labels": [], "entities": []}, {"text": "Understanding the relation between the structure and the meaning is needed for building distributional compositional models for sentences.", "labels": [], "entities": []}, {"text": "Research in Distributed Representations (DR) ( proposed models and methods for encoding data structures in vectors, matrices, or high-order tensors.", "labels": [], "entities": [{"text": "Distributed Representations (DR)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.8586278676986694}]}, {"text": "Distributed Representations are oriented to preserve the structural information in the final representation.", "labels": [], "entities": [{"text": "Distributed Representations", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8780856132507324}]}, {"text": "For this purpose, DR models generally use random and possibly orthogonal vectors for words and structural elements.", "labels": [], "entities": []}, {"text": "As distributional semantics vectors are unlikely to be orthogonal, syntactic structure of sentences maybe easily lost in the final vector combination.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the interaction between distributed structure and distributional meaning by proposing a model to encode syntactic trees in distributed structures and by exploiting this model in kernel machines to determine the similarity between syntactic trees.", "labels": [], "entities": []}, {"text": "We propose a pure distributed tree (DT) and a distributional distributed tree (DDT).", "labels": [], "entities": []}, {"text": "In line with the distributed representation theory, DTs use random vectors for representing words whereas DDTs use distributional vectors for words.", "labels": [], "entities": []}, {"text": "Our interest is in understanding if the introduction of distributional semantic information in an inherently syntactic based model, such as distributed representations, leads to better performances in semantic aware tasks.", "labels": [], "entities": []}, {"text": "DTs and DDTs are exploited for defining distributed tree ker-nels (DTKs) and distributional distributed tree kernels (DDTKs).", "labels": [], "entities": []}, {"text": "We study the interaction between structure and meaning in two ways: 1) by comparing DTKs and DDTKs with the classical tree similarity functions, i.e., the tree kernels TK (); 2) by comparing the accuracy of DTKs and DDTKs in a semantic task such as recognizing textual entailment (RTE).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9938055276870728}, {"text": "recognizing textual entailment (RTE)", "start_pos": 249, "end_pos": 285, "type": "TASK", "confidence": 0.8433270553747813}]}, {"text": "Results show that DTKs correlate with TKs and perform in RTE better than DDTKs.", "labels": [], "entities": []}, {"text": "This indicates that including distributional vectors in distributed structures should be performed in a more complex fashion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we experiment with the distributed tree kernels (DTK) and the distributional distributed tree kernels (DDTK) in order to understand whether or not the syntactic structure and the distributional meaning can be easily encoded in the distributed trees.", "labels": [], "entities": []}, {"text": "We will experiment in two ways: (1) direct comparison of the distances produced by the original tree kernel (TK) () and the novel kernels DTK and DDTK; (2) task driven evaluation of DTK and DDTK using the RTE task.", "labels": [], "entities": []}, {"text": "The rest of the section is organized as follows.", "labels": [], "entities": []}, {"text": "We firstly introduce the experiment setup that is used for the two settings (Sec. 4.1).", "labels": [], "entities": []}, {"text": "Secondly, we report on the experimental results (Sec. 4.2).", "labels": [], "entities": []}, {"text": "We have the double aim of producing a direct comparison of how the distributed tree kernel (DTK) is approximating the original tree kernel (TK) and a task based comparison for assessing if the approximation is enough effective to similarly solve the task that is textual entailment recognition.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 263, "end_pos": 293, "type": "TASK", "confidence": 0.7537597219149271}]}, {"text": "For both experimental settings, we take the recognizing textual entailment sets ranging from the first challenge (RTE-1) to the fifth (RTE-5) ().", "labels": [], "entities": []}, {"text": "The distributional vectors used for DDTK have been obtained by an LSA reduction of the word-byword cooccurrence matrix generated on the UKWaC corpus, using a context window of size 3.", "labels": [], "entities": [{"text": "UKWaC corpus", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.9963849186897278}]}, {"text": "An appropriate size for the LSA reduction was deemed to be 250.", "labels": [], "entities": [{"text": "LSA reduction", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.8030162751674652}]}, {"text": "Thus, in the experiments we used 250 dimensions both for distributional and random vectors, to allow a correct comparison between DTK and DDTK models.", "labels": [], "entities": []}, {"text": "For the direct comparison, we used tree pairs derived from the RTE sets.", "labels": [], "entities": [{"text": "RTE sets", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.8665439486503601}]}, {"text": "Each pair is derived from a T-H pair where T and H are syntactically analyzed and each RTE set produces the corresponding set of tree pairs, e.g., the development set of RTE1 produces a set of 567 tree pairs.", "labels": [], "entities": []}, {"text": "To determine whether or not a distributed kernel, DTK or DDTK, is behaving similarly to the original TK kernel, given a set of tree pairs, we produce two ranked lists of tree pairs: the first is ranked according to the original TK applied to the tree pairs and the second according to the target distributed kernel.", "labels": [], "entities": []}, {"text": "We evaluate the correlation of the two ranked lists according to the spearman's correlation.", "labels": [], "entities": []}, {"text": "Higher correlation corresponds to a better approximation of TK.", "labels": [], "entities": [{"text": "correlation", "start_pos": 7, "end_pos": 18, "type": "METRIC", "confidence": 0.9901684522628784}, {"text": "TK", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.938255786895752}]}, {"text": "For the task driven comparison, we experimented with the datasets in the classical learning setting: the development set is used as training set and the final classifier is tested on the testing set.", "labels": [], "entities": []}, {"text": "We used a support vector machine) with an implementation of the original tree kernel).", "labels": [], "entities": []}, {"text": "The classifiers are evaluated according to the accuracy of the classification decision on the testing set, i.e., the ratio of the correct decisions overall the decisions to take.: Accuracies of the different methods on the textual entailment recognition task  In the first experiment of this set, we want to investigate which one between DTK and DDTK correlates better with original TK.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9892920255661011}, {"text": "Accuracies", "start_pos": 180, "end_pos": 190, "type": "METRIC", "confidence": 0.9882126450538635}, {"text": "textual entailment recognition task", "start_pos": 223, "end_pos": 258, "type": "TASK", "confidence": 0.716291643679142}]}, {"text": "reports the spearman's correlations of tree kernels with DTK and DDTK in a vector space with 250 dimensions.", "labels": [], "entities": []}, {"text": "These correlations are obtained averaging the correlations over the 9 RTE sets.", "labels": [], "entities": [{"text": "RTE sets", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8857629895210266}]}, {"text": "According to these results, DTK better correlates with TK with respect to DDTK.", "labels": [], "entities": [{"text": "DDTK", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9157251119613647}]}, {"text": "Distributional vectors used for words are not orthogonal as these are used to induce the similarity between words.", "labels": [], "entities": []}, {"text": "Yet, this important feature of these vectors determines a worse encoding of the syntactic structure.", "labels": [], "entities": []}, {"text": "In the task driven experiment, we wanted to investigate whether the difference in correlation has some effect on the performance of the different systems.", "labels": [], "entities": []}, {"text": "Accuracy results on the RTE task are reported in.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9832217693328857}, {"text": "RTE task", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.8871324956417084}]}, {"text": "The columns RTE1, RTE2, RTE3, and RTE5 represent the accuracies of the different kernels using the traditional split of training and testing.", "labels": [], "entities": [{"text": "RTE1", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.6741704344749451}, {"text": "RTE2", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.6458975672721863}, {"text": "RTE3", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.6224455833435059}, {"text": "RTE5", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6281644105911255}]}, {"text": "The column avg reports the average accuracy of the different methods in the 4 sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9976310729980469}]}, {"text": "Rows represent the different kernels used in this comparative experiment.", "labels": [], "entities": []}, {"text": "These kernels are used with the task specific kernel P K by changing the generic kernel K.", "labels": [], "entities": []}, {"text": "The first 3 rows represent the pure kernels while the last 3 rows represent the kernels boosted with the lexical similarity (Lex), a simple feature computed using WordNet-based metrics, as in).", "labels": [], "entities": [{"text": "lexical similarity (Lex)", "start_pos": 105, "end_pos": 129, "type": "METRIC", "confidence": 0.6562655866146088}]}, {"text": "Looking at the first 3 rows, we derive that there is not a significant difference between TK, DTK, and DDTK.", "labels": [], "entities": []}, {"text": "DTK and DDTK can then be used instead of the TK.", "labels": [], "entities": [{"text": "DTK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7923593521118164}, {"text": "DDTK", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8595109581947327}]}, {"text": "This is an important result, since the computation of DTK (or DDTK) is much faster than that of TK, due to TK's complexity being quadratic with respect to the size of the trees, and DTK requiring a simple dot product over vectors that can be obtained with linear complexity with respect to the tree size.", "labels": [], "entities": []}, {"text": "The second fact is that there is no difference between DTK and DDTK: more semantically informed word vectors have the same performance of random vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average Spearman's correlations of the tree ker- nel (TK) with the distributed tree kernel (DTK) and the  distributed distributional tree kernel (DDTK) in a vector  space with 250 dimensions", "labels": [], "entities": []}, {"text": " Table 2: Accuracies of the different methods on the tex- tual entailment recognition task", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9967228770256042}, {"text": "tex- tual entailment recognition", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.8117852449417114}]}]}