{"title": [{"text": "Collecting Semantic Data by Mechanical Turk for the Lexical Knowledge Resource of a Text-to-Picture Generating System", "labels": [], "entities": []}], "abstractContent": [{"text": "WordsEye is a system for automatically converting natural language text into 3D scenes representing the meaning of that text.", "labels": [], "entities": [{"text": "WordsEye", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9279782772064209}]}, {"text": "At the core of WordsEye is the Scenario-Based Lexical Knowledge Resource (SBLR), a unified knowledge base and representational system for expressing lexical and real-world knowledge needed to depict scenes from text.", "labels": [], "entities": []}, {"text": "To enrich a portion of the SBLR, we need to fill out some contextual information about its objects, including information about their typical parts, typical locations and typical objects located near them.", "labels": [], "entities": []}, {"text": "This paper explores our proposed methodology to achieve this goal.", "labels": [], "entities": []}, {"text": "First we try to collect some semantic information by using Amazon's Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (AMT)", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.8857320717402867}]}, {"text": "Then, we manually filter and classify the collected data and finally, we compare the manual results with the output of some automatic filtration techniques which use several WordNet similarity and corpus association measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "WordsEye (, ) is a system for automatically converting natural language text into 3D scenes representing the meaning of that text.", "labels": [], "entities": [{"text": "WordsEye", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8922301530838013}]}, {"text": "A version of WordsEye has been tested online (www.wordseye.com) with several thousand real-world users.", "labels": [], "entities": [{"text": "WordsEye", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9032185673713684}]}, {"text": "The system works by first parsing each input sentence into a dependency structure.", "labels": [], "entities": []}, {"text": "These dependency structures are then processed to resolve anaphora and other coreferences.", "labels": [], "entities": []}, {"text": "The lexical items and dependency links are then converted to semantic nodes and roles drawing on lexical valence patterns and other information in the ScenarioBased Lexical Knowledge Resource (SBLR) ( ).", "labels": [], "entities": []}, {"text": "The resulting semantic relations are then converted to a final set of graphical constraints representing the position, orientation, size, color, texture, and poses of objects in the scene.", "labels": [], "entities": []}, {"text": "Finally, the scene is composed from these constraints and rendered in OpenGL (http://www.opengl.org).", "labels": [], "entities": []}, {"text": "The SBLR is the core of the text-to-scene conversion mechanism.", "labels": [], "entities": [{"text": "text-to-scene conversion", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8203927874565125}]}, {"text": "It is a unified knowledge base and representational system for expressing lexical and real-world knowledge needed to depict scenes from text.", "labels": [], "entities": []}, {"text": "The SBLR will ultimately include information on the semantic categories of words; the semantic relations between predicates (verbs, nouns, adjectives, and prepositions) and their arguments; the types of arguments different predicates typically take; additional contextual knowledge about the visual scenes various events and activities occur in; and the relationship between this linguistic information and the 3D objects in our objects library.", "labels": [], "entities": []}, {"text": "To enrich a portion of the SBLR we need to fill out some contextual information about several hundred objects in WordsEye's database, including information about their typical parts, typical location and typical objects nearby them.", "labels": [], "entities": [{"text": "WordsEye's database", "start_pos": 113, "end_pos": 132, "type": "DATASET", "confidence": 0.9221404194831848}]}, {"text": "Such information can in principle be extracted from online corpora (e.g.), but such data is invariably noisy and requires hand editing.", "labels": [], "entities": []}, {"text": "Furthermore, precisely because much of the information is commonsense it is rarely explicitly stated in text.", "labels": [], "entities": []}, {"text": "Ontologies of commonsense information such as Cyc are effectively useless for extracting such information.", "labels": [], "entities": []}, {"text": "This paper explores our proposed methodology to achieve this goal.", "labels": [], "entities": []}, {"text": "First we try to collect some semantic information by Amazon's Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (AMT)", "start_pos": 53, "end_pos": 83, "type": "DATASET", "confidence": 0.9114246538707188}]}, {"text": "Then, we manually filter and classify the collected data and finally, we compare the manual results with the output of some automatic filtration techniques which use WN similarity and corpus association measures.", "labels": [], "entities": []}], "datasetContent": [{"text": "The collected responses of each AMT task were ranked separately by each of the above similarity and association measures.", "labels": [], "entities": [{"text": "AMT task", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.9131766855716705}]}, {"text": "We classify the ranked responses into \"keep\" (higher-scoring) and \"reject\" (lowerscoring) classes by defining a specific threshold for each list.", "labels": [], "entities": [{"text": "keep", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9658896923065186}]}, {"text": "Then we evaluated the accuracy of each filtration approach by computing their precision and recall on correct \"keep\" items (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9993789196014404}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9992600083351135}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9978582262992859}]}, {"text": "In this table the baseline score shows the accuracy of the responses of each AMT task before using automatic filtration techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9993823766708374}, {"text": "AMT task", "start_pos": 77, "end_pos": 85, "type": "TASK", "confidence": 0.9107231795787811}]}, {"text": "It should be added that collecting data by using AMT is rather cheap and fast, so we are more interested in higher precision (achieving highly accurate data) than higher recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9977741837501526}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9985190033912659}]}, {"text": "Lower recall means we lose some data, which is not too expensive to collect.", "labels": [], "entities": [{"text": "recall", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9996019005775452}]}, {"text": "As can be seen in table 2, within the object-location data set, we gained the best precision (0.7832) by using log-odds with relatively high recall (0.6690).", "labels": [], "entities": [{"text": "object-location data set", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.7353087365627289}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9990100860595703}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9989325404167175}]}, {"text": "Target-response pairs that were approved or rejected contrary to automatic predictions were due primarily to the specificity of the response location.", "labels": [], "entities": []}, {"text": "In the part-whole task, the best precision (0.9010) was achieved by using WN matrix similarities but again we lost a noticeable portion of data (recall= 0.2516).", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9991863369941711}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9989363551139832}]}, {"text": "Rejected target-response pairs from the higher-scoring part-whole set were often due to responses that named attributes, rather than parts, of the target item (e.g. croissant -flaky).", "labels": [], "entities": []}, {"text": "Many responses were too general (e.g, gong -material).", "labels": [], "entities": []}, {"text": "Many target-response pairs would have fallen under the next-to.r relation rather than any of the meronymic relations.", "labels": [], "entities": []}, {"text": "The majority of the approved target-response pairs from the lower-scoring part-whole set were due to obvious, \"common sense responses that would usually be inferred rather than explicitly stated, particularly body parts (e.g, bunny -brain).", "labels": [], "entities": []}, {"text": "The baseline accuracy of the nearby objects task is quite high (precision=0.8934, recall=1.0), and we gain the best precision by using WN average pairwise similarity (0.9855) by removing lower-scoring part of AMT responses (recall=0.3215).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9856491088867188}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9991325736045837}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9991814494132996}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9991538524627686}, {"text": "WN average pairwise similarity", "start_pos": 135, "end_pos": 165, "type": "METRIC", "confidence": 0.7012145444750786}, {"text": "AMT responses", "start_pos": 209, "end_pos": 222, "type": "TASK", "confidence": 0.7323857247829437}, {"text": "recall", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.9931715726852417}]}, {"text": "The high precision in all automatic techniques is due primarily to the fact that the open-ended nature of the task resulted in a large number of target-response pairs that, while not pertinent to the next-to.r relation, could be labeled by other relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9993520379066467}]}, {"text": "Again, the open-ended nature of the nearby objects task resulted in the lowest percentage of rejected high-scoring pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of AMT tasks, payments and the completion time", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.6203498840332031}, {"text": "AMT tasks", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.8523390889167786}]}, {"text": " Table 2: The accuracy of automatic filtering approaches", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995243549346924}, {"text": "automatic filtering", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.691341295838356}]}]}