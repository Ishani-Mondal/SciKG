{"title": [], "abstractContent": [{"text": "This paper deals with normalization of language data from Early New High Ger-man.", "labels": [], "entities": [{"text": "normalization of language data from Early New High Ger-man", "start_pos": 22, "end_pos": 80, "type": "DATASET", "confidence": 0.7338859604464637}]}, {"text": "We describe an unsupervised, rule-based approach which maps historical wordforms to modern wordforms.", "labels": [], "entities": []}, {"text": "Rules are specified in the form of context-aware rewrite rules that apply to sequences of characters.", "labels": [], "entities": []}, {"text": "They are derived from two aligned versions of the Luther bible and weighted according to their frequency.", "labels": [], "entities": []}, {"text": "The evaluation shows that our approach (83%-91% exact matches) clearly outper-forms the baseline (65%).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For evaluation, we generated normalized forms of all words in the evaluation corpus and compared them to their aligned forms in the modernized bible text.", "labels": [], "entities": []}, {"text": "Two methods of comparison were applied: (i) counting the number of identical wordforms; and (ii) calculating the average normalized Levenshtein distance (NLD).", "labels": [], "entities": [{"text": "average normalized Levenshtein distance (NLD)", "start_pos": 113, "end_pos": 158, "type": "METRIC", "confidence": 0.7710162273475102}]}, {"text": "Full evaluation results are shown in.", "labels": [], "entities": []}, {"text": "Results for the dictionary method are only reported in combination with the best-probability method, which clearly outperform the combination with the best-rule method.", "labels": [], "entities": []}, {"text": "Exact matches As our aim is to generate modernized wordforms from historical ones, the logi-cal first step of an evaluation is to check how many words from the ENHG text from 1545, when processed with our algorithms, exactly match their modernized NHG counterparts.", "labels": [], "entities": [{"text": "ENHG text from 1545", "start_pos": 160, "end_pos": 179, "type": "DATASET", "confidence": 0.9791816473007202}, {"text": "NHG", "start_pos": 248, "end_pos": 251, "type": "DATASET", "confidence": 0.9391785860061646}]}, {"text": "Before normalization, the ratio of identical tokens in the historical and the modernized text is about 65%, i.e., only a third of all wordforms even differ at all.", "labels": [], "entities": []}, {"text": "This is the baseline for our algorithm; any normalizing process that results in less than 65% exact matches has likely done more harm than good, and it would be better to leave all words unchanged.", "labels": [], "entities": []}, {"text": "shows that both ranking methods we employed, best-rule and best-probability, achieve match ratios above 83% (lines 'All', column 'Ratio'), which is a significant increase from the baseline; combining the best-probability method with the dictionary lookup even yields 91% exact matches.", "labels": [], "entities": []}, {"text": "Our normalization approach is not only successful in changing historical forms to modern ones, but also in correctly leaving most of the wordforms unchanged that do not need to be changed (97.46-99.57%; lines 'Identical', column 'Ratio').", "labels": [], "entities": []}, {"text": "Results from evaluating the dictionary method on the annotated sample set of non-identical word pairs are shown in.", "labels": [], "entities": []}, {"text": "Although the sample size is very small, the numbers suggest that our approach is mostly suitable for pairs of type 1 (besides identical word pairs); in particular, it can only 'repair' some inflection or affixes (types 2-3).", "labels": [], "entities": []}, {"text": "Incorrect mappings (type 6) are not produced.: NLD evaluation of word pairs that do not match their aligned word after normalization.", "labels": [], "entities": []}, {"text": "The first line of each method shows the NLD before rule application, the second line afterwards.", "labels": [], "entities": [{"text": "NLD before rule application", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.5395907983183861}]}, {"text": "Normalized Levenshtein distance However, simply counting correct guesses does not take into account near-misses, where the algorithm edited only apart of the word correctly, and is also not a very fine-grained way of evaluation.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 11, "end_pos": 31, "type": "METRIC", "confidence": 0.6185934692621231}]}, {"text": "Therefore, we chose to use normalized Levenshtein distance ( to assess whether our normalized variants are 'closer' to the correct modern version than the (non-normalized) source words.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 38, "end_pos": 58, "type": "METRIC", "confidence": 0.6235003173351288}]}, {"text": "The NLD of a word pair is defined as the Levenshtein distance divided by the length of the longest possible alignment of the two words.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.6813020408153534}]}, {"text": "8 To evaluate a set of word pairs, we calculate the average NLD of all word pairs in that set.", "labels": [], "entities": []}, {"text": "This measure is not ideal for our task, since short words are unduly penalized for being wrong, but it has the advantage of being relatively intuitive; e.g., a NLD of 0.5 indicates that roughly every second character in a word was altered.", "labels": [], "entities": [{"text": "NLD", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.9309642314910889}]}, {"text": "Comparing the old and the modernized bible text yields an average NLD of 0.1019; as two thirds of all words are already identical, this number is quite low.", "labels": [], "entities": [{"text": "NLD", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.993137001991272}]}, {"text": "The three normalization methods reduce that number to 0.0408-0.0251; this reduction stems, in parts, from the higher match ratio.", "labels": [], "entities": [{"text": "match ratio", "start_pos": 117, "end_pos": 128, "type": "METRIC", "confidence": 0.9745498597621918}]}, {"text": "To evaluate whether the normalization improved the wordforms even if they do not exactly match the aligned form, we re-evaluated average NLD on the sets of words that did not result in an exact match.", "labels": [], "entities": []}, {"text": "The results, given in, show only a slight improvement for the best-rule method, while the best-probability method has even increased the average NLD.", "labels": [], "entities": []}, {"text": "With the latter method, a high percentage of mismatches (10.14%) results from source words that did not need to be changed.", "labels": [], "entities": []}, {"text": "Combined with the dictionary lookup, the ratio of such cases is considerably reduced (to 3.12%).", "labels": [], "entities": []}, {"text": "Even the dictionary method is notable to completely avoid superfluous modifications.", "labels": [], "entities": []}, {"text": "The reason is that there are ambiguous words such as waren, which can either be mapped to wahren 'true' or left unchanged: waren 'were'.", "labels": [], "entities": []}, {"text": "This means that at the type level it cannot be decided which of the two mappings is correct.", "labels": [], "entities": []}, {"text": "Instead, we would need context information at the token level, which is beyond our word-based approach.", "labels": [], "entities": []}, {"text": "Method comparison Even though the results for both ranking methods are quite close when evaluated on all word pairs (83.31% versus 83.81%), the difference is statistically significant within a confidence level of 99.9%, i.e., the bestprobability method results in better normalizations on average.", "labels": [], "entities": []}, {"text": "This is especially reflected in the numbers for the non-identical word pairs, where the difference between the two methods is even greater.", "labels": [], "entities": []}, {"text": "On the other hand, the best-rule method performs better on 'unknowns', i.e. words which were not already part of the training set (see).", "labels": [], "entities": []}, {"text": "This seems to indicate that a combination of both methods could be favorable.", "labels": [], "entities": []}, {"text": "The overlap of the word lists generated by the two methods is 93.13%, showing that there is a noticeable percentage of words (around 3%) which is modernized correctly with one method but not the other.", "labels": [], "entities": [{"text": "overlap", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9909831285476685}]}, {"text": "One crucial difference is the normalization of second person verb forms ending in -tu, e.g. soltu 'should you', which should be modernized to sollst du.", "labels": [], "entities": []}, {"text": "These forms show a contraction of verb and pronoun and are quite common in the original ENHG bible text.", "labels": [], "entities": [{"text": "ENHG bible text", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9850231210390726}]}, {"text": "With the best-rule method, they do not get changed at all, as the epsilon identity rules are ranked higher than the ones that would perform the necessary insertions.", "labels": [], "entities": []}, {"text": "The best-probability method, on the other hand, outputs the correctly modernized form; rules that process the letter u appearing word-final after t have a very low probability, thus decreasing the total probability of the unmodified wordform.", "labels": [], "entities": []}, {"text": "Finally, combining the methods with a dictionary lookup results in remarkable improvements.", "labels": [], "entities": []}, {"text": "The number of exact matches increases from 83.31% to 88.66% (best-rule), and from 83.81% to 91.00% (best-probability).", "labels": [], "entities": []}, {"text": "As can be seen from, the dictionary lookup on the one hand helps to avoid superfluous normalization (with identical word pairs).", "labels": [], "entities": []}, {"text": "On the other hand, it also improves on rule application, by filtering out rules (or combinations of rules) that do not result insensible words.", "labels": [], "entities": [{"text": "rule application", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8312605917453766}]}, {"text": "In contrast to rules, which operate locally, the dictionary lookup has access to the entire wordform and thus serves as a complementary \"guide\" for suitable rule selection.", "labels": [], "entities": []}, {"text": "Since we use the complete modernized Luther bible as the source of our dictionary, all correctly normalized wordforms are guaranteed to be listed in the dictionary.", "labels": [], "entities": []}, {"text": "In this sense, the results represent an upper bound of this method.", "labels": [], "entities": []}, {"text": "However, the larger a dictionary is, the higher will be the chance of \"false friends\", i.e. wordforms that accidentally match a generated wordform.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Alignment types: types 1-5 are correct to  various degrees, type 6 is incorrect. For each type,  its frequency in the sample and evaluation results  for the more frequent types are given (see Sec. 6).", "labels": [], "entities": []}, {"text": " Table 3: Sample rankings and rules", "labels": [], "entities": [{"text": "Sample", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9059011340141296}]}, {"text": " Table 5: Evaluation of exact matches and normalized Levenshtein distance (NLD) compared to the  modernized bible text; separately for all tokens (All), tokens that are or are not identical in both old  and modernized version (Identical/Non-identical), and tokens that were not seen in the training corpus  (Unknowns). The best result for each class is indicated in bold, the second-best in bold italics.", "labels": [], "entities": [{"text": "Levenshtein distance (NLD)", "start_pos": 53, "end_pos": 79, "type": "METRIC", "confidence": 0.9407922506332398}]}, {"text": " Table 6: NLD evaluation of word pairs that do not  match their aligned word after normalization. The  first line of each method shows the NLD before  rule application, the second line afterwards.", "labels": [], "entities": []}]}