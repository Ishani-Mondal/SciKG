{"title": [{"text": "Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing", "labels": [], "entities": [{"text": "Understanding Differences", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7502562701702118}]}], "abstractContent": [{"text": "Identifying peer-review helpfulness is an important task for improving the quality of feedback received by students, as well as for helping students write better reviews.", "labels": [], "entities": [{"text": "Identifying peer-review helpfulness", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8001998066902161}]}, {"text": "As we tailor standard product review analysis techniques to our peer-review domain, we notice that peer-review helpfulness differs not only between students and experts but also between types of experts.", "labels": [], "entities": []}, {"text": "In this paper, we investigate how different types of perceived helpfulness might influence the utility of features for automatic prediction.", "labels": [], "entities": [{"text": "automatic prediction", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.655727207660675}]}, {"text": "Our feature selection results show that certain low-level linguistic features are more useful for predicting student perceived helpfulness, while high-level cognitive constructs are more effective in modeling experts' perceived helpfulness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Peer review of writing is a commonly recommended technique to include in good writing instruction.", "labels": [], "entities": [{"text": "Peer review of writing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8549165278673172}]}, {"text": "It not only provides more feedback compared to what students might get from their instructors, but also provides opportunities for students to practice writing helpful reviews.", "labels": [], "entities": []}, {"text": "While existing web-based peerreview systems facilitate peer review from the logistic aspect (e.g. collecting papers from authors, assigning reviewers, and sending reviews back), there still remains the problem that the quality of peer reviews varies, and potentially good feedback is not written in a helpful way.", "labels": [], "entities": []}, {"text": "To address this issue, we propose to add a peer-review helpfulness model to current peer-review systems, to automatically predict peer-review helpfulness based on features mined from textual reviews using Natural Language Processing (NLP) techniques.", "labels": [], "entities": []}, {"text": "Such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews.", "labels": [], "entities": []}, {"text": "In our prior work (Xiong and Litman, 2011), we examined whether techniques used for predicting the helpfulness of product reviews) could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced by the educational context of peer review.", "labels": [], "entities": []}, {"text": "While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.", "labels": [], "entities": []}, {"text": "In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2).", "labels": [], "entities": []}, {"text": "In the following examples, students judge helpfulness with discrete ratings from one to seven; experts judge it using a one to five scale.", "labels": [], "entities": []}, {"text": "Higher ratings on both scales correspond to the most helpful reviews.", "labels": [], "entities": []}], "datasetContent": [{"text": "We take a machine learning approach to model different types of perceived helpfulness (student helpfulness, writing-expert helpfulness, content-expert helpfulness, average-expert helpfulness) based on combinations of linguistic and non-linguistic features extracted from our peer-review corpus.", "labels": [], "entities": []}, {"text": "Then we compare the different helpfulness types in terms of the predictive power of features used in their corresponding models.", "labels": [], "entities": []}, {"text": "For comparison purpose, we consider the linguistic and non-linguistic features both separately and in combination, which generates three set of features: 1) linguistic features, 2) nonlinguistic features, and 3) all features.", "labels": [], "entities": []}, {"text": "For each set of features, we train four models, each corresponding to a different kind of helpfulness rating.", "labels": [], "entities": []}, {"text": "For each learning task (three by four), we use two standard feature selection algorithms to find the most useful features based on 10-fold cross validation.", "labels": [], "entities": []}, {"text": "First, we perform Linear Regression with Greedy Stepwise search (stepwise LR) to select the most useful features when testing in each of the ten folds, and count how many times each features is selected in the ten trials.", "labels": [], "entities": [{"text": "Greedy Stepwise search (stepwise LR)", "start_pos": 41, "end_pos": 77, "type": "METRIC", "confidence": 0.7206253622259412}]}, {"text": "Second, we use Relief Feature Evaluation with Ranker (Relief) () to rank all used features based on their average merits (the ability of the given feature to differentiate between two example pairs) often trials.", "labels": [], "entities": [{"text": "Ranker (Relief)", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.8467347919940948}]}, {"text": "Although both methods are supervised, the wrapper is \"more aggressive\" because its feature evaluation is based on the performance of the regression model and thus the resulting feature set is tailored to the learning algorithm.", "labels": [], "entities": []}, {"text": "In contrast, Relief does not optimize feature sets directly for classifier performance, thus it takes into account class information in a \"less aggressive\" manner than the Wrapper method.", "labels": [], "entities": []}, {"text": "We use both methods in our experiment to Relief evaluates the worth of an attribute by repeatedly sampling an instance and changing the value of the given attribute based on the nearest instance of the same and different class.", "labels": [], "entities": []}, {"text": "10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/).", "labels": [], "entities": [{"text": "Weka", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9654573798179626}]}, {"text": "While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.", "labels": [], "entities": []}, {"text": "To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.", "labels": [], "entities": []}, {"text": "Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.", "labels": [], "entities": []}, {"text": "(Details of how the average-expert model performs can be found in our prior work (Xiong and Litman, 2011).) presents the feature selection results of computational linguistic features used in modeling the four different types of peer-review helpfulness.", "labels": [], "entities": []}, {"text": "The first row lists the four sources of helpfulness ratings, and each column represents a corresponding model.", "labels": [], "entities": []}, {"text": "The second row presents the most useful features in each model selected by stepwise LR, where \"# of folds\" refers to the number of trials in which the given feature appears in the resulting feature set during the 10-fold cross validation.", "labels": [], "entities": []}, {"text": "Here we only report features that are selected by no less than five folds (half the time).", "labels": [], "entities": []}, {"text": "The third row presents feature ranks computed using Relief, where we only report the top six features due to the space limit.", "labels": [], "entities": [{"text": "Relief", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9495888352394104}]}, {"text": "Features are ordered in descending ranks, and the average merit and its standard deviation is reported for each one of the features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of features", "labels": [], "entities": []}, {"text": " Table 4: Feature selection based on linguistic features", "labels": [], "entities": []}, {"text": " Table 5: Feature selection based on non-linguistic features", "labels": [], "entities": []}, {"text": " Table 6: Feature selection based on all features", "labels": [], "entities": []}]}