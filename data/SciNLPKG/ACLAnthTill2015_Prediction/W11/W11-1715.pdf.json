{"title": [{"text": "Mining Subjective Knowledge from Customer Reviews: A Specific Case of Irony Detection", "labels": [], "entities": [{"text": "Irony Detection", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.6526828706264496}]}], "abstractContent": [{"text": "The research described in this work focuses on identifying key components for the task of irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.9312165677547455}]}, {"text": "By means of analyzing a set of customer reviews, which are considered as ironic both in social and mass media, we try to find hints about how to deal with this task from a computational point of view.", "labels": [], "entities": []}, {"text": "Our objective is to gather a set of discriminating elements to represent irony.", "labels": [], "entities": []}, {"text": "In particular, the kind of irony expressed in such reviews.", "labels": [], "entities": [{"text": "irony", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9656341671943665}]}, {"text": "To this end, we built a freely available data set with ironic reviews collected from Amazon.", "labels": [], "entities": []}, {"text": "Such reviews were posted on the basis of an online viral effect; i.e. contents whose effect triggers a chain reaction on people.", "labels": [], "entities": []}, {"text": "The findings were assessed employing three clas-sifiers.", "labels": [], "entities": []}, {"text": "The results show interesting hints regarding the patterns and, especially, regarding the implications for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.9723908305168152}]}], "introductionContent": [{"text": "Verbal communication is not a trivial process.", "labels": [], "entities": [{"text": "Verbal communication", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8984303176403046}]}, {"text": "It implies to share a common code as well as being able to infer information beyond the semantic meaning.", "labels": [], "entities": []}, {"text": "A lot of communicative acts imply information not grammatically expressed to be able to decode the whole sense: if the hearer is not capable to infer that information, the communicative process is incomplete.", "labels": [], "entities": []}, {"text": "Let us consider a joke.", "labels": [], "entities": []}, {"text": "The amusing effect sometimes relies on not given information.", "labels": [], "entities": [{"text": "amusing", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9611285328865051}]}, {"text": "If such information is not filled, the result is a bad, or better said, a misunderstood joke.", "labels": [], "entities": []}, {"text": "This information, which is not expressed with \"physical\" words, supposes a great challenge, even from a linguistic analysis, because it points to social and cognitive layers quite difficult to be computationally represented.", "labels": [], "entities": []}, {"text": "One of the communicative phenomena which better represents this problem is irony.", "labels": [], "entities": []}, {"text": "According to, irony is essentially a communicative act which expresses an opposite meaning of what was literally said.", "labels": [], "entities": []}, {"text": "Due to irony is common in texts that express subjective and deeply-felt opinions, its presence represents a significant obstacle to the accurate analysis of sentiment in such texts (cf.).", "labels": [], "entities": [{"text": "accurate analysis of sentiment", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.6340854316949844}]}, {"text": "In this research work we aim at gathering a set of discriminating elements to represent irony.", "labels": [], "entities": []}, {"text": "In particular, we focus on analyzing a set of customer reviews (posted on the basis of an online viral effect) in order to obtain a set of key components to face the task of irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.9118080139160156}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the theoretical problem of irony.", "labels": [], "entities": []}, {"text": "Section 3 presents the related work as well as the evaluation corpus.", "labels": [], "entities": []}, {"text": "Section 4 describes our model and the experiments that were performed.", "labels": [], "entities": []}, {"text": "Section 5 assesses the model and presents the discussion of the results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 draws some final remarks and addresses the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to the scarce work on automatic irony processing, and to the intrinsic features of irony, it is quite difficult and subjective to obtain a corpus with ironic data.", "labels": [], "entities": [{"text": "automatic irony processing", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6477484405040741}]}, {"text": "Therefore, we decided to rely on the wisdom of the crowd and use a collection of customer reviews from the Amazon website.", "labels": [], "entities": []}, {"text": "These reviews are considered as ironic by customers, as well as by many journalists, both in mass and social media.", "labels": [], "entities": []}, {"text": "According to such means, all these reviews deal with irony, sarcasm, humor, satire and parody (hence, they are consistent with our definition of irony).", "labels": [], "entities": []}, {"text": "All of them were posted by means of an online viral effect, which inmost cases, increased the popularity and sales of the reviewed products.", "labels": [], "entities": []}, {"text": "The Three Wolf Moon T-shirt is the clearest example.", "labels": [], "entities": [{"text": "Three Wolf Moon T-shirt", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.895924523472786}]}, {"text": "This item became one of the most popular products, both in Amazon as well as in social networks, due to the ironic reviews posted by people 1 . Our positive data are thus integrated with reviews of five different products published by Amazon.", "labels": [], "entities": []}, {"text": "All of them were posted through the online viral effect.", "labels": [], "entities": []}, {"text": "The list of products is: i) Three Wolf Moon T-shirt (product id: B002HJ377A); ii) Tuscan Whole Milk (product id: B00032G1S0); iii) Zubaz Pants (product id: B000WVXM0W); iv) Uranium Ore (product id: B000796XXM); and v) Platinum Radiant Cut 3-Stone (product id: B001G603AE).", "labels": [], "entities": [{"text": "Three Wolf Moon T-shirt", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.8632193505764008}]}, {"text": "A total of 3,163 reviews were retrieved.", "labels": [], "entities": []}, {"text": "Then, in order to automatically filter the ones more likely to be ironic without performing a manual annotation (which is planned to be carried out in the near feature), we removed the reviews whose customer rating, according to the Amazon rating criteria, was lesser than four stars.", "labels": [], "entities": []}, {"text": "The assumptions behind this decision rely on two facts: i) the viral purpose, and ii) the ironic effect.", "labels": [], "entities": []}, {"text": "The former caused that people to post reviews whose main purpose, and perhaps the only one, was to exalt superficial properties and non-existent consequences; thus the possibilities to find real reviews were minimal.", "labels": [], "entities": []}, {"text": "Considering this scenario, the lat-ter supposes that, if someone ironically wants to reflect properties and consequences such as the previous ones, s/he will not do it by rating the products with one or two stars, instead, s/he will rate them with the highest scores.", "labels": [], "entities": []}, {"text": "After applying this filter, we obtained an ironic set integrated with 2,861 documents.", "labels": [], "entities": []}, {"text": "On the other hand, two negative sets were automatically collected from two sites: Amazon.com (AMA) and Slashdot.com (SLA).", "labels": [], "entities": []}, {"text": "The products selected from AMA were: Bananagrams (toy), The Help by Kathryn Stockett (book), Flip UltraHD Camcorder (camera), I Dreamed A Dream (CD), Wii Fit Plus with Balance Board (Videogame console).", "labels": [], "entities": [{"text": "AMA", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7610931396484375}]}, {"text": "Finally, the data collected from SLA contain web comments categorized as funny in a community-driven process.", "labels": [], "entities": []}, {"text": "The whole evaluation corpus is integrated with 8,861 documents.", "labels": [], "entities": []}, {"text": "It is available at http://users.dsic.upv.es/grupos/nle.", "labels": [], "entities": []}, {"text": "In order to verify the effectiveness of our model, we evaluated it through a classification task.", "labels": [], "entities": []}, {"text": "Two underlying goals were analyzed: a) feature relevance; and b) the possibility of automatically finding ironic documents.", "labels": [], "entities": []}, {"text": "The classifiers were evaluated by comparing the positive set against each of the two negative subsets (AMA and SLA, respectively).", "labels": [], "entities": [{"text": "SLA", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.8879127502441406}]}, {"text": "All the documents were represented as frequency-weighted term vectors according to a representativeness ratio.", "labels": [], "entities": []}, {"text": "This ratio was estimated using Formula 1: where i is the i-th conceptual category (i = 1.", "labels": [], "entities": []}, {"text": "6); j is the j-th feature of i; f df i,j (feature dimension frequency) is the frequency of features j of category i; and |d| is the length of the k-th document d k . For categories funny, positive/negative, affective, and pleasantness, we determined an empirical threshold of representativeness \u2265 0.5.", "labels": [], "entities": []}, {"text": "A document was assigned the value = 1 (presence) if its \u03b4 exceeded the threshold, otherwise a value = 0 (absence) was assigned.", "labels": [], "entities": []}, {"text": "A different criterion was determined for the n-grams and POS-grams because we were not only interested in knowing whether or not the sequences appeared in the corpus, but also in obtaining a measure to represent the degree of similarity among the sets.", "labels": [], "entities": []}, {"text": "In order to define a similarity score, we used the Jaccard similarity coefficient.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 21, "end_pos": 37, "type": "METRIC", "confidence": 0.9804103374481201}, {"text": "Jaccard similarity coefficient", "start_pos": 51, "end_pos": 81, "type": "METRIC", "confidence": 0.6620840827624003}]}, {"text": "The classification accuracy was assessed employing three classifiers: Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB), support vector machines (SVM), and decision trees (DT).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9658020734786987}, {"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB)", "start_pos": 70, "end_pos": 93, "type": "METRIC", "confidence": 0.667195717493693}]}, {"text": "The sets were trained with 5,861 instances (2,861 positive and 3,000 negative ones).", "labels": [], "entities": []}, {"text": "10-fold cross validation method was used as test.", "labels": [], "entities": []}, {"text": "Global accuracy as well as detailed performance in terms of precision, recall, and F \u2212 measure, are given in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9933491349220276}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9996861219406128}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9996308088302612}, {"text": "F \u2212 measure", "start_pos": 83, "end_pos": 94, "type": "METRIC", "confidence": 0.9886018633842468}]}], "tableCaptions": []}