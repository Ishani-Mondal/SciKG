{"title": [{"text": "CMU Syntax-Based Machine Translation at WMT 2011", "labels": [], "entities": [{"text": "CMU Syntax-Based Machine Translation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5834036767482758}, {"text": "WMT 2011", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9128389954566956}]}], "abstractContent": [{"text": "We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2011 shared translation task.", "labels": [], "entities": [{"text": "WMT 2011 shared translation task", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.7184912919998169}]}, {"text": "We built a hybrid syntactic MT system for French-English using the Joshua decoder and an automatically acquired SCFG.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.7220509648323059}]}, {"text": "New work for this year includes training data selection and grammar filtering.", "labels": [], "entities": [{"text": "data selection", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7878484129905701}, {"text": "grammar filtering", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8873133361339569}]}, {"text": "Expanded training data selection significantly increased translation scores and lowered OOV rates, while results on grammar filtering were mixed.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9590045213699341}, {"text": "OOV rates", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.986111044883728}, {"text": "grammar filtering", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.8835359811782837}]}], "introductionContent": [{"text": "During the past year, the statistical transfer machine translation group at Carnegie Mellon University has continued its work on large-scale syntactic MT systems based on automatically acquired synchronous context-free grammars (SCFGs).", "labels": [], "entities": [{"text": "statistical transfer machine translation", "start_pos": 26, "end_pos": 66, "type": "TASK", "confidence": 0.8247936517000198}, {"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.7276643514633179}]}, {"text": "For the 2011 Workshop on Machine Translation, we built a hybrid MT system, including both syntactic and non-syntactic rules, and submitted it as a constrained entry to the French-English translation task.", "labels": [], "entities": [{"text": "2011 Workshop on Machine Translation", "start_pos": 8, "end_pos": 44, "type": "TASK", "confidence": 0.5648118734359742}, {"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9789392948150635}, {"text": "French-English translation task", "start_pos": 172, "end_pos": 203, "type": "TASK", "confidence": 0.7286746998627981}]}, {"text": "This is our fourth yearly submission to the WMT shared translation task.", "labels": [], "entities": [{"text": "WMT shared translation task", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8769904375076294}]}, {"text": "In design and construction, the system is similar to our submission from last year's workshop, with changes in the methods we employed for training data selection and SCFG filtering.", "labels": [], "entities": [{"text": "training data selection", "start_pos": 139, "end_pos": 162, "type": "TASK", "confidence": 0.6304713090260824}, {"text": "SCFG filtering", "start_pos": 167, "end_pos": 181, "type": "TASK", "confidence": 0.8858094215393066}]}, {"text": "Continuing WMT's general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French-English training data and an English language model of 1.8 billion words.", "labels": [], "entities": [{"text": "WMT", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.6994089484214783}]}, {"text": "Decoding was carried out in Joshua (, an open-source framework for parsing-based MT.", "labels": [], "entities": [{"text": "parsing-based MT", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.8644266724586487}]}, {"text": "We managed our experiments with LoonyBin , an open-source tool for defining, modifying, and running complex experimental pipelines.", "labels": [], "entities": []}, {"text": "We describe our system-building process in more detail in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we evaluate the system's performance on WMT development sets and examine the aftermath of training data selection and grammar filtering.", "labels": [], "entities": [{"text": "WMT development", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.8796824514865875}, {"text": "grammar filtering", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7868043482303619}]}, {"text": "Section 4 concludes with possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tuned each system variant on the newstest2008 data set, using the Z-MERT package for minimum error-rate training to the BLEU metric.", "labels": [], "entities": [{"text": "newstest2008 data set", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.9651777346928915}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9961199760437012}]}, {"text": "We ran development tests on the newstest2009 and newstest2010 data sets; reports the results obtained according to various automatic metrics.", "labels": [], "entities": [{"text": "newstest2010 data sets", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.91174844900767}]}, {"text": "The evaluation consists of case-insensitive scoring according to) tuned to HTER with the exact, stemming, and synonymy modules enabled, case-insensitive BLEU () as implemented by the NIST mteval-v13 script, and case-insensitive TER 0.7.25).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.961181640625}, {"text": "NIST mteval-v13 script", "start_pos": 183, "end_pos": 205, "type": "DATASET", "confidence": 0.8432942628860474}, {"text": "TER", "start_pos": 228, "end_pos": 231, "type": "METRIC", "confidence": 0.8812711834907532}]}, {"text": "gives comparative results for two major systems: one based on our WMT 2011 data selection as outlined in Section 2.1, and one based on the smaller WMT 2010 training data that we used last year (8.6 million sentence pairs).", "labels": [], "entities": [{"text": "WMT 2011 data selection", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.893514409661293}, {"text": "WMT 2010 training data", "start_pos": 147, "end_pos": 169, "type": "DATASET", "confidence": 0.885254830121994}]}, {"text": "Each system was run with the two grammar filtering variants described in Section 2.4: the 10,000 most frequently extracted hierarchical rules of any type (\"10k\"), and a combination of the 2000 most frequently extracted abstract rules and the 100,000 most frequently extracted partially lexicalized rules that matched the test set (\"2k+100k\").", "labels": [], "entities": []}, {"text": "Our primary submission to the WMT 2011 shared task was the fourth line of Table 2 (\"WMT 2011 2k+100k\"); we also made a constrastive submission with the system from the second line (\"WMT 2010 2k+100k\").", "labels": [], "entities": [{"text": "WMT 2011 shared task", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.6962348073720932}, {"text": "WMT 2011 2k", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.9129114349683126}, {"text": "WMT 2010 2k", "start_pos": 182, "end_pos": 193, "type": "DATASET", "confidence": 0.9381241202354431}]}, {"text": "Using part of the Giga-FrEn data -along with the additions to the Europarl, news commentary, and UN document courses released since last year   -is beneficial to translation quality, as there is a clear improvement in metric scores between the 2010 and 2011 systems.", "labels": [], "entities": [{"text": "Giga-FrEn data", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.9366651177406311}, {"text": "Europarl", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9919716119766235}, {"text": "translation", "start_pos": 162, "end_pos": 173, "type": "TASK", "confidence": 0.9638437628746033}]}, {"text": "Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap resampling method) with n = 1000 and p < 0.01.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9750263094902039}]}, {"text": "They are also larger than the 0.7-to 1.1-point gains reported by when the full Giga-FrEn was added.", "labels": [], "entities": []}, {"text": "The 2011 system also shows a significant reduction in the out-of-vocabulary (OOV) rate on both test sets: 38% and 47% fewer OOV types, and 44% and 45% fewer OOV tokens, when compared to the 2010 system.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) rate", "start_pos": 58, "end_pos": 86, "type": "METRIC", "confidence": 0.8530441641807556}]}, {"text": "Differences between grammar filtering techniques, on the other hand, are much less significant according to all three metrics.", "labels": [], "entities": [{"text": "grammar filtering", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7521706819534302}]}, {"text": "Under paired bootstrap resampling on the newstest2009 set, the grammar variants in both the 2010 and 2011 systems are statistically equivalent according to BLEU score.", "labels": [], "entities": [{"text": "newstest2009 set", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.949335902929306}, {"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9980771541595459}]}, {"text": "On newstest2010, the 2k+100k grammar improves over the 10k version (p < 0.01) in the 2010 system, but the situation is reversed in the 2011 system.", "labels": [], "entities": []}, {"text": "We investigated differences in grammar use with an analysis of rule applications in the two variants of the 2011 system, the results of which are summarized in.", "labels": [], "entities": []}, {"text": "Though the configuration with the 2k+100k grammar does apply syntactic rules 20% more frequently than its 10k counterpart, the 10k system uses overall 53% more unique rules.", "labels": [], "entities": []}, {"text": "One contributing factor to this situation could be that the fully abtract rule cutoff is set too low compared to the increase in partially lexicalized rules.", "labels": [], "entities": []}, {"text": "The effect of the 2k+100k filtering is to reduce the number of abstract rules from 4000 to 2000 while increasing the number of partially lexicalized rules from 6000 to 100,000.", "labels": [], "entities": []}, {"text": "However, we find that the 10k system makes heavy use of some short, meaningful abstract rules that were excluded from the 2k+100k system.", "labels": [], "entities": []}, {"text": "The 2k+100k grammar, by contrast, includes along tail of less frequently used partially lexicalized grammar rules.", "labels": [], "entities": []}, {"text": "In practice, there is a balance between the use of syntactic and non-syntactic grammar rules during decoding.", "labels": [], "entities": []}, {"text": "We highlight an example of how both types of rules work together in, which shows our primary system's translation of part of newstest2009 sentence 2271.", "labels": [], "entities": []}, {"text": "The French source text is given in italics and segmented into phrases.", "labels": [], "entities": [{"text": "French source text", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.873989482720693}]}, {"text": "The SCFG rules used in translation are shown above each phrase, where numerical superscripts on the nonterminal labels indicate those constituents' relative ordering in the original French sentence.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9733547568321228}]}, {"text": "(Monotonic glue rules are not shown.)", "labels": [], "entities": []}, {"text": "While nonsyntactic rules can be used for short-distance reordering and fixed phrases, such as t\u00e9l\u00e9phones mobiles \u2194 mobile phones, the model prefers syntactic translations for more complicated patterns, such as the head-children reversal in appareils musicaux portables \u2194 portable music devices.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Total number of training sentence pairs released,  by corpus, and the number used in building our system.", "labels": [], "entities": []}, {"text": " Table 2: Development test results for systems based on WMT 2010 data (without the Giga-FrEn corpus) and WMT  2011 data (with some Giga-FrEn). The fourth line is our primary shared-task submission.", "labels": [], "entities": [{"text": "WMT 2010 data", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9447178244590759}, {"text": "Giga-FrEn corpus", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.7844250798225403}, {"text": "WMT  2011 data", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.9434502522150675}]}, {"text": " Table 3: Summary of 2011 system syntactic rule applica- tions on both test sets.", "labels": [], "entities": []}, {"text": " Table 3. Though the configuration with the  2k+100k grammar does apply syntactic rules 20%  more frequently than its 10k counterpart, the 10k  system uses overall 53% more unique rules. One  contributing factor to this situation could be that the", "labels": [], "entities": []}]}