{"title": [{"text": "Using the Mutual k-Nearest Neighbor Graphs for Semi-supervised Classification of Natural Language Data", "labels": [], "entities": []}], "abstractContent": [{"text": "The first step in graph-based semi-supervised classification is to construct a graph from input data.", "labels": [], "entities": [{"text": "semi-supervised classification", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.706040620803833}]}, {"text": "While the k-nearest neighbor graphs have been the de facto standard method of graph construction, this paper advocates using the less well-known mutual k-nearest neighbor graphs for high-dimensional natural language data.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8279751837253571}]}, {"text": "To compare the performance of these two graph construction methods, we run semi-supervised classification methods on both graphs in word sense disambiguation and document classification tasks.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7859353423118591}, {"text": "word sense disambiguation", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.6428304016590118}, {"text": "document classification", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.6908275038003922}]}, {"text": "The experimental results show that the mutual k-nearest neighbor graphs, if combined with maximum spanning trees, consistently outperform the k-nearest neighbor graphs.", "labels": [], "entities": []}, {"text": "We attribute better performance of the mutual k-nearest neighbor graph to its being more resistive to making hub vertices.", "labels": [], "entities": []}, {"text": "The mutual k-nearest neighbor graphs also perform equally well or even better in comparison to the state-of-the-art b-matching graph construction, despite their lower computational complexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9299602508544922}]}, {"text": "In particular, graph-based techniques for semi-supervised classification () are recognized as a promising approach.", "labels": [], "entities": [{"text": "semi-supervised classification", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7190936505794525}]}, {"text": "Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation (), sentiment analysis), and statistical machine translation (, to name but a few.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 60, "end_pos": 69, "type": "TASK", "confidence": 0.8890498876571655}, {"text": "word sense disambiguation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7357653975486755}, {"text": "sentiment analysis", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.9521840810775757}, {"text": "statistical machine translation", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.717059870560964}]}, {"text": "However, the focus of these studies is how to assign accurate labels to vertices in a given graph.", "labels": [], "entities": []}, {"text": "By contrast, there has not been much work on how such a graph should be built, and graph construction remains \"more of an art than a science\" ().", "labels": [], "entities": [{"text": "graph construction", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.75922030210495}]}, {"text": "Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results.", "labels": [], "entities": [{"text": "semisupervised classification", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.660990297794342}]}, {"text": "Both for semi-supervised classification and for clustering, the k-nearest neighbor (k-NN) graph construction has been used almost exclusively in the literature.", "labels": [], "entities": [{"text": "semi-supervised classification", "start_pos": 9, "end_pos": 39, "type": "TASK", "confidence": 0.7010240852832794}, {"text": "clustering", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.9641996026039124}]}, {"text": "However, k-NN graphs often produce hubs, or vertices with extremely high degree (i.e., the number of edges incident to a vertex).", "labels": [], "entities": []}, {"text": "This tendency is obvious especially if the original data is high-dimensional-a characteristic typical of natural language data.", "labels": [], "entities": []}, {"text": "Ina later section, we demonstrate that such hub vertices indeed deteriorate the accuracy of semi-supervised classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9994707703590393}]}, {"text": "While not in the context of graph construction, Radovanovi\u00b4c made an insightful observation into the nature of hubs in high-dimensional space; in their context, a hub is a sample close to many other samples in the (high-dimensional) sample space.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7733926475048065}]}, {"text": "They state that such hubs inherently emerge in high-dimensional data as aside effect of the \"curse of dimensionality,\" and argue that this is a reason nearest neighbor classification does notwork well in high-dimensional space.", "labels": [], "entities": [{"text": "nearest neighbor classification", "start_pos": 151, "end_pos": 182, "type": "TASK", "confidence": 0.6878002683321635}]}, {"text": "Their observation is insightful for graph construction as well.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7472078800201416}]}, {"text": "Most of the graph-based semisupervised classification methods work by gradually propagating label information of a vertex towards neighboring vertices in a graph, but the neighborhood structure in the graph is basically determined by the proximity of data in the original highdimensional sample space.", "labels": [], "entities": []}, {"text": "Hence, it is very likely that a hub in the sample space also makes a hub in the k-NN graph, since k-NN graph construction greedily connects a pair of vertices if the sample corresponding to one vertex is among the k closest samples of the other sample in the original space.", "labels": [], "entities": []}, {"text": "It is therefore desirable to have an efficient graph construction method for high-dimensional data that can produce a graph with reduced hub effects.", "labels": [], "entities": []}, {"text": "To this end, we propose to use the mutual knearest neighbor graphs (mutual k-NN graphs), a less well-known variant of the standard k-NN graphs.", "labels": [], "entities": []}, {"text": "All vertices in a mutual k-NN graph have a degree upper-bounded by k, which is not usually the case with standard k-NN graphs.", "labels": [], "entities": []}, {"text": "This property helps not to produce vertices with extremely high degree (hub vertices) in the graph.", "labels": [], "entities": []}, {"text": "A mutual k-NN graph is easy to build, at a time complexity identical to that of the k-NN graph construction.", "labels": [], "entities": []}, {"text": "We first evaluated the quality of the graphs apart from specific classification algorithms using the \u03c6-edge ratio of graphs.", "labels": [], "entities": []}, {"text": "Our experimental results show that the mutual k-NN graphs have a smaller number of edges connecting vertices with different labels than the k-NN graphs, thus reducing the possibility of wrong label information to be propagated.", "labels": [], "entities": []}, {"text": "We also compare the classification accuracy of two standard semi-supervised classification algorithms on the mutual k-NN graphs and the k-NN graphs.", "labels": [], "entities": [{"text": "classification", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.9492717385292053}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9080581665039062}]}, {"text": "The results show that the mutual k-NN graphs consistently outperorm the k-NN graphs.", "labels": [], "entities": []}, {"text": "Moreover, the mutual k-NN graphs achieve equally well or better classification accuracy than the state-of-the-art graph construction method called b-matching, while taking much less time to construct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9125528931617737}]}], "datasetContent": [{"text": "We compare the k-NN, mutual k-NN, and bmatching graphs in word sense disambiguation and document classification tasks.", "labels": [], "entities": [{"text": "word sense disambiguation and document classification tasks", "start_pos": 58, "end_pos": 117, "type": "TASK", "confidence": 0.6628332819257464}]}, {"text": "All of these tasks are multi-class classification problems.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.645064502954483}]}, {"text": "We used two word sense disambiguation datasets in our experiment: \"interest\" and \"line.\"", "labels": [], "entities": []}, {"text": "The \"interest\" data is originally taken from the POS-tagged   portion of the Wall Street Journal Corpus.", "labels": [], "entities": [{"text": "POS-tagged   portion of the Wall Street Journal Corpus", "start_pos": 49, "end_pos": 103, "type": "DATASET", "confidence": 0.8810907378792763}]}, {"text": "Each instance of the polysemous word \"interest\" has been tagged with one of the six senses in Longman Dictionary of Contemporary English.", "labels": [], "entities": [{"text": "Longman Dictionary of Contemporary English", "start_pos": 94, "end_pos": 136, "type": "DATASET", "confidence": 0.9563090920448303}]}, {"text": "The details of the dataset are described in.", "labels": [], "entities": []}, {"text": "The \"line\" data is originally used in numerous comparative studies of word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7164729535579681}]}, {"text": "Each instance of the word \"line\" has been tagged with one of the six senses on the WordNet thesaurus.", "labels": [], "entities": [{"text": "WordNet thesaurus", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9413783848285675}]}, {"text": "Further details can be found in the.", "labels": [], "entities": []}, {"text": "Following, we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local collocation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.7061599493026733}]}, {"text": "Details of these context features can be found in.", "labels": [], "entities": []}, {"text": "The Reuters dataset is extracted from RCV1-v2/LYRL2004, a text categorization test collection ().", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.974077582359314}, {"text": "RCV1-v2/LYRL2004", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8378538489341736}]}, {"text": "In the same manner as, we produced the classification dataset by selecting approximately 4,000 documents from 4 general topics (corporate, economic, government and markets) at random.", "labels": [], "entities": []}, {"text": "The features described in are used with this dataset.", "labels": [], "entities": []}, {"text": "The 20 newsgroups dataset is a popular dataset frequently used for document classification and clustering.", "labels": [], "entities": [{"text": "20 newsgroups dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.6851807733376821}, {"text": "document classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.7263177782297134}]}, {"text": "The dataset consists of approximately 20,000 messages on newsgroups and is originally distributed by.", "labels": [], "entities": []}, {"text": "Each message is assigned one of the 20 possible labels indicating which newsgroup it has been posted to, and represented as binary bag-of-words features as described in. summarizes the characteristics of the datasets used in our experiments.", "labels": [], "entities": []}, {"text": "Our focus in this paper is a semi-supervised classification setting in which the dataset contains a small amount of labeled examples and a large amount of unlabeled examples.", "labels": [], "entities": []}, {"text": "To simulate such settings, we create 10 sets of labeled examples, with each set consisting of randomly selected l examples from the original dataset, where l is 10 percent of the total number of examples.", "labels": [], "entities": []}, {"text": "For each set, the remaining 90 percent constitute the unlabeled examples whose labels must be inferred.", "labels": [], "entities": []}, {"text": "After we build a graph from the data using one of the graph construction methods discussed earlier, a graph-based semi-supervised classification algorithm must be run on the resulting graph to infer labels to the unlabeled examples (vertices).", "labels": [], "entities": []}, {"text": "We use two most frequently used classification algorithms: Gaussian Random Fields (GRF) ( and the Local/Global Consistency algorithm (LGC) ().", "labels": [], "entities": []}, {"text": "Averaged classification accuracy is used as the evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8824909925460815}]}, {"text": "For all datasets, co- sine similarity is used as the similarity measure between examples.", "labels": [], "entities": [{"text": "co- sine similarity", "start_pos": 18, "end_pos": 37, "type": "METRIC", "confidence": 0.6057849824428558}]}, {"text": "In \"interest\" and \"line\" datasets, we compare the performance of the graph construction methods over the broad range of their parameters; i.e., bin bmatching graphs and kin (mutual) k-NN graphs.", "labels": [], "entities": []}, {"text": "In Reuters and the 20 newsgroups datasets, 2-fold cross validation is used to determine the hyperparameters (k and b) of the graph construction methods; i.e., we split the labeled data into two folds, and used one fold for training and the other for development, and then switch the folds in order to find the optimal hyperparameter among k, b \u2208 {2, . .", "labels": [], "entities": [{"text": "Reuters and the 20 newsgroups datasets", "start_pos": 3, "end_pos": 41, "type": "DATASET", "confidence": 0.7738009889920553}]}, {"text": "The smoothing parameter \u00b5 of LGC is fixed at \u00b5 = 0.9.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy of vertices around hubs  in a k-NN graph, before (\"original\") and after (\"hub- removed\") hubs are removed. The value d represents the  shortest distance (number of hops) from a vertex to its  nearest hub vertex in the graph.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9195504188537598}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9359942674636841}]}, {"text": " Table 2: Datasets used in experiments.", "labels": [], "entities": []}, {"text": " Table 3: Document classification accuracies for k-NN graphs, b-matching graphs, and mutual k-NN graphs. The col- umn for 'Dense' is the result for the graph with the original similarity matrix W as the adjacency matrix; i.e., without  using any graph construction (sparsification) methods. The column for 'MST' is the result the for the maximum span- ning tree. b-matching graph construction did not complete after one week on the 20 newsgroups data, and hence no  results are shown.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7123525589704514}]}]}