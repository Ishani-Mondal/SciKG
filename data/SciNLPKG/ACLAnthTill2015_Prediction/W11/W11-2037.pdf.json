{"title": [], "abstractContent": [{"text": "Error-return plots show the rate of error (misunderstanding) against the rate of non-return (non-understanding) for Natural Language Processing systems.", "labels": [], "entities": []}, {"text": "They area useful visual tool for judging system performance when other measures such as recall/precision and detection-error tradeoff are less informative , specifically when a system is judged on the correctness of its responses, but may elect to not return a response.", "labels": [], "entities": [{"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9985417127609253}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.7208094000816345}]}], "introductionContent": [{"text": "Many Natural Language Processing systems make a distinction between misunderstanding, where the system interprets an input incorrectly, and nonunderstanding, where the system is aware that it is notable to interpret an input).", "labels": [], "entities": []}, {"text": "This distinction is common in dialogue systems, where it pertains to Natural Language Understanding components which pass their output to a dialogue manager: a dialogue manager will act on the contents of misunderstood input, but if it knows that the input is not understood then it can engage in a variety of recovery techniques, such as asking for clarification, moving on, or changing the topic.", "labels": [], "entities": []}, {"text": "For this reason non-understanding is usually preferred to misunderstanding.", "labels": [], "entities": []}, {"text": "While common to dialogue systems, the concept of non-understanding is useful for other tasks as well, whenever a system can benefit from the knowledge that its best interpretation is likely to be incorrect (see below for an example in question answering).", "labels": [], "entities": [{"text": "question answering", "start_pos": 235, "end_pos": 253, "type": "TASK", "confidence": 0.7819656133651733}]}, {"text": "Detecting non-understanding is a tradeoff: a system that is prone to non-understanding will inevitably miss some inputs that it would have understood correctly under a forced interpretation.", "labels": [], "entities": []}, {"text": "This is similar but not identical to the familiar tradeoffs between recall and precision and between detection and error ().", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9976835250854492}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9301903247833252}]}, {"text": "Recall and precision are measures taken from information retrieval, where there are typically multiple documents relevant to a query, and ideal performance is defined as retrieving all and only the relevant documents: recall measures the \"all\" part while precision measures the \"only\" part, and tuning a system to increase one measure typically implies decreasing its counterpart.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9339860677719116}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9963625073432922}, {"text": "information retrieval", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7172206342220306}, {"text": "recall", "start_pos": 218, "end_pos": 224, "type": "METRIC", "confidence": 0.9953246116638184}, {"text": "precision", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9975243210792542}]}, {"text": "Detection and error apply to forced choice tasks: each input must be classified as either positive or negative, and decreasing false positives typically implies increasing false negatives and vice versa.", "labels": [], "entities": [{"text": "Detection", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.977841854095459}, {"text": "error", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9108226895332336}]}, {"text": "The tradeoff between misunderstanding and non-understanding is similar to recall-precision in that a response need not be given to each input, and is similar to detection-error in that when a response is given, we only care about its correctness and not about its exhaustiveness.", "labels": [], "entities": [{"text": "recall-precision", "start_pos": 74, "end_pos": 90, "type": "METRIC", "confidence": 0.994517982006073}]}, {"text": "There is presently no accepted measure for the tradeoff between misunderstanding and nonunderstanding.", "labels": [], "entities": []}, {"text": "A recent example illustrating the confusion, and need fora standard measure, comes from the QALD-1 Open Challenge (Question Answering over Linked Data).", "labels": [], "entities": [{"text": "Question Answering over Linked Data)", "start_pos": 115, "end_pos": 151, "type": "TASK", "confidence": 0.7963177363077799}]}, {"text": "The task is defined as giving a complete and correct answer to a natural language question, but systems are allowed to not return an answer.", "labels": [], "entities": []}, {"text": "The evaluation metric uses recall and precision, but they are defined in a nonstandard way.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.999279797077179}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9991239905357361}]}, {"text": "Precision is defined as the number of correctly answered questions divided by the total number of answered questions; given that each question receives at most one answer, this is equivalent to the standard definition of correct answers divided by the total number of answers provided by the system -it penalizes misunderstanding and gives credit to non-understanding.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.981186032295227}]}, {"text": "Recall is also defined in a non-standard way.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6366193890571594}]}], "datasetContent": [], "tableCaptions": []}