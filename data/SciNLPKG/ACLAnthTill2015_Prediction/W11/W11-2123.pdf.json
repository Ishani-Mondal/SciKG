{"title": [{"text": "KenLM: Faster and Smaller Language Model Queries", "labels": [], "entities": []}], "abstractContent": [{"text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs.", "labels": [], "entities": []}, {"text": "The PROBING data structure uses linear probing hash tables and is designed for speed.", "labels": [], "entities": [{"text": "PROBING data structure", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7799005607763926}]}, {"text": "Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57% of the memory.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.6035097241401672}]}, {"text": "The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption.", "labels": [], "entities": [{"text": "TRIE data structure", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8557507395744324}]}, {"text": "TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline.", "labels": [], "entities": []}, {"text": "Our code is open-source 1 , thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.", "labels": [], "entities": []}, {"text": "This paper describes the several performance techniques used and presents benchmarks against alternative implementations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8058958351612091}]}, {"text": "This paper presents methods to query N -gram language models, minimizing time and space costs.", "labels": [], "entities": []}, {"text": "Queries take the form p(w n |w n\u22121 1 ) where w n 1 is an n-gram.", "labels": [], "entities": []}, {"text": "Backoff-smoothed models estimate this probability based on the observed entry with longest matching  . where the probability p(w n |w n\u22121 f ) and backoff penalties b(w n\u22121 i ) are given by an already-estimated model.", "labels": [], "entities": []}, {"text": "The problem is to store these two values fora large and sparse set of n-grams in away that makes queries efficient.", "labels": [], "entities": []}, {"text": "Many packages perform language model queries.", "labels": [], "entities": []}, {"text": "Throughout this paper we compare with several packages: SRILM 1.5.12) is a popular toolkit based on tries used in several decoders.", "labels": [], "entities": [{"text": "SRILM 1.5.12", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.6410164833068848}]}, {"text": "IRSTLM 5.60.02) is a sorted trie implementation designed for lower memory consumption.", "labels": [], "entities": [{"text": "IRSTLM 5.60.02", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9467397630214691}]}, {"text": "MITLM 0.4 ( is mostly designed for accurate model estimation, but can also compute perplexity. RandLM 0.) stores large-scale models in less memory using randomized data structures.", "labels": [], "entities": [{"text": "MITLM 0.4", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9223224222660065}, {"text": "model estimation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7056706249713898}]}, {"text": "BerkeleyLM revision 152 ( implements tries based on hash tables and sorted arrays in Java with lossy quantization.", "labels": [], "entities": [{"text": "BerkeleyLM revision 152", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.9234533111254374}]}, {"text": "Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code.", "labels": [], "entities": []}, {"text": "TPT describe tries with better locality properties, but did not release code.", "labels": [], "entities": [{"text": "TPT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.902004063129425}]}, {"text": "These packages are further described in Section 3.", "labels": [], "entities": []}, {"text": "We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.", "labels": [], "entities": []}, {"text": "Performance improvements transfer to the Moses (), cdec, and Joshua () translation systems where our code has been integrated.", "labels": [], "entities": []}, {"text": "Our open-source (LGPL) implementation is also available for download as a standalone package with minimal (POSIX and g++) dependencies.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Single-threaded time and memory consumption  of Moses translating 3003 sentences. Where applicable,  models were loaded with lazy memory mapping (-L),  prefaulting (-P), and normal reading (-R); results differ  by at most than 0.6 minute.", "labels": [], "entities": []}, {"text": " Table 3: Multi-threaded time and memory consumption  of Moses translating 3003 sentences on eight cores. Our  code supports lazy memory mapping (-L) and prefault- ing (-P) with MAP POPULATE, the default. IRST is not  threadsafe. Time for Moses itself to load, including load- ing the language model and phrase table, is included.  Along with locking and background kernel operations  such as prefaulting, this explains why wall time is not  one-eighth that of the single-threaded case.", "labels": [], "entities": [{"text": "prefault- ing (-P)", "start_pos": 154, "end_pos": 172, "type": "METRIC", "confidence": 0.8718570371468862}]}, {"text": " Table 4: CPU time, memory usage, and uncased BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9724961519241333}]}]}