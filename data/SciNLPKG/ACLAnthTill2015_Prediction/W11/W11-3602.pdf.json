{"title": [{"text": "Using Explicit Semantic Analysis for Cross-Lingual Link Discovery", "labels": [], "entities": [{"text": "Cross-Lingual Link Discovery", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.8069634040196737}]}], "abstractContent": [{"text": "This paper explores how to automatically generate cross-language links between resources in large document collections.", "labels": [], "entities": []}, {"text": "The paper presents new methods for Cross-Lingual Link Discovery (CLLD) based on Explicit Semantic Analysis (ESA).", "labels": [], "entities": [{"text": "Cross-Lingual Link Discovery (CLLD)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.8198603590329488}]}, {"text": "The methods are applicable to any multilingual document collection.", "labels": [], "entities": []}, {"text": "In this report, we present their comparative study on the Wikipedia corpus and provide new insights into the evaluation of link discovery systems.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9634072184562683}, {"text": "link discovery", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.7209324687719345}]}, {"text": "In particular, we measure the agreement of human annotators in linking articles in different language versions of Wikipedia, and compare it to the results achieved by the presented methods.", "labels": [], "entities": [{"text": "agreement", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9719009399414062}]}], "introductionContent": [{"text": "Cross-referencing documents is an essential part of organising textual information.", "labels": [], "entities": []}, {"text": "However, keeping links in large, quickly growing, document collections up-to-date, is problematic due to the number of possible connections.", "labels": [], "entities": []}, {"text": "In multilingual document collections, interlinking semantically related information in a timely manner becomes even more challenging.", "labels": [], "entities": []}, {"text": "Suitable software tools that could facilitate the link discovery process by automatically analysing the multilingual content are currently lacking.", "labels": [], "entities": [{"text": "link discovery", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.8477402329444885}]}, {"text": "In this paper, we present new methods for Cross-Lingual Link Discovery (CLLD) applicable across different types of multilingual textual collections.", "labels": [], "entities": [{"text": "Cross-Lingual Link Discovery (CLLD)", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.8169457912445068}]}, {"text": "Our methods are based on Explicit Semantic Analysis (ESA) introduced by.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis (ESA)", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7102792213360468}]}, {"text": "ESA is a method that calculates semantic relatedness of two texts by mapping their term vectors to a high dimensional space (typically, but not necessarily, the space of Wikipedia concepts) and by calculating the similarity between these vectors (instead of comparing them directly).", "labels": [], "entities": []}, {"text": "The method has received much attention in the recent years and it has also been extended to a multilingual version called Cross-Lingual Explicit Semantic Analysis (CL-ESA).", "labels": [], "entities": [{"text": "Cross-Lingual Explicit Semantic Analysis (CL-ESA)", "start_pos": 122, "end_pos": 171, "type": "TASK", "confidence": 0.7035779654979706}]}, {"text": "To the best of our knowledge, this method has not yet been applied in the context of automatic link discovery systems.", "labels": [], "entities": [{"text": "automatic link discovery", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.5894401371479034}]}, {"text": "Since the CLLD field is relatively young, it is also important to establish a constructive means for evaluating these systems.", "labels": [], "entities": []}, {"text": "Our paper provides insight into this problem by investigating the agreement/reliability of man-made links and by presenting a possible approach for the definition of ground truth, i.e. gold standard.", "labels": [], "entities": [{"text": "definition of ground truth", "start_pos": 152, "end_pos": 178, "type": "TASK", "confidence": 0.7565588057041168}]}, {"text": "The paper brings the following contributions:", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the main obstacles in systematically improving link discovery systems is the difficulty to evaluate the results.", "labels": [], "entities": [{"text": "link discovery", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.786264568567276}]}, {"text": "The issue that makes reliable evaluation problematic is due to both technical and cognitive aspects.", "labels": [], "entities": []}, {"text": "The difficulty in obtaining the \"ground truth\" fora sufficiently large dataset is caused both by the lack of human resources to manually annotate a very large number of document combinations, and the inherent subjectivity of the task.", "labels": [], "entities": []}, {"text": "As a result, we find it essential to estimate the agreement between annotators and see to what extent the precision and recall characteristics can be measured with respect to interlinked document collections.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9991157650947571}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9775517582893372}]}, {"text": "We claim that the reasons for linking two pieces of information is made at the level of semantics, i.e. the annotator has to understand the concepts/ideas described in two papers to decide if they should be connected by a link.", "labels": [], "entities": []}, {"text": "We claim that this process should be language independent.", "labels": [], "entities": []}, {"text": "Thus, an article about London will be related to an article about the United Kingdom regardless of the language the articles are written in.", "labels": [], "entities": []}, {"text": "Therefore, let us define the link generation task in the following way: Given a document 2 in the source language, find documents in the target language that are suitable link targets for the source document, i.e. there is a semantic relationship between the source document and the linked target documents.", "labels": [], "entities": [{"text": "link generation", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7251719087362289}]}, {"text": "Based on the definition, the ground truth fora topic document dis the set of documents that can be considered (semantically) suitable link targets.", "labels": [], "entities": []}, {"text": "Though this set is typically unknown to us, we can in our experiment approximate it by taking the existing Wikipedia links as ground truth.", "labels": [], "entities": []}, {"text": "Because the Wikipedia link structure has been agreed by a large number of contributing authors, it is likely to have a relatively consistent link structure in comparison to content that would be linked just by a single person.", "labels": [], "entities": []}, {"text": "To establish the ground truth for the original source document, we can extract all links originating in the source document and pointing to other documents.", "labels": [], "entities": []}, {"text": "Since the process of linking information is performed at the semantic level, and is thus language independent, we can enrich our ground truth with link graphs from different language versions of Wikipedia.", "labels": [], "entities": []}, {"text": "This causes the ground truth to get larger which has two consequences: (1) It increases the reliability of the evaluation as many relevant links are often omitted () It is more difficult to achieve higher recall.", "labels": [], "entities": [{"text": "reliability", "start_pos": 92, "end_pos": 103, "type": "METRIC", "confidence": 0.9952501058578491}, {"text": "recall", "start_pos": 205, "end_pos": 211, "type": "METRIC", "confidence": 0.9994086027145386}]}, {"text": "The experiment was carried out for two language pairs: Spanish to English and Czech to English.", "labels": [], "entities": []}, {"text": "We will denote the source language L source and the target language L target . The input for the different CLLD methods are two document sets: \u2022 Let SOU RCE Lsource be the set of topic documents selected as pages that contain a Wikipedia link between different language versions.", "labels": [], "entities": [{"text": "SOU RCE Lsource", "start_pos": 149, "end_pos": 164, "type": "METRIC", "confidence": 0.7816763122876486}]}, {"text": "In our case, 100 pages were selected.", "labels": [], "entities": []}, {"text": "\u2022 Let T ARGET Ltarget be the collection of documents in the target language from which the link targets are selected.", "labels": [], "entities": [{"text": "T", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.9937254786491394}, {"text": "ARGET Ltarget", "start_pos": 8, "end_pos": 21, "type": "METRIC", "confidence": 0.6209464222192764}]}, {"text": "In our case, this collection contains all (3.8 million) Wikipedia pages in English.", "labels": [], "entities": []}, {"text": "The output of the method is a set (ranked list) LIST result = T ARGET Ltarget , score.", "labels": [], "entities": [{"text": "LIST result", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9736515879631042}, {"text": "T", "start_pos": 62, "end_pos": 63, "type": "METRIC", "confidence": 0.9949187636375427}, {"text": "ARGET", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8567786812782288}, {"text": "Ltarget", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.6329918503761292}]}, {"text": "A given generated item d, score \u2208 LIST result is evaluated as a hit if and only if d \u2208 GT .  To investigate the performance of the first part of CLLD -the cross-language step carried out by CL-ESA, we have analysed how well the system finds fora given topic document in the source language the duplicate document in the target language.", "labels": [], "entities": []}, {"text": "In this step, the system takes a document in the source language, and selects from the 3.8 million large document set in the target language the documents with the highest similarity.", "labels": [], "entities": []}, {"text": "We then check, if a duplicate document (d = \u03c1d source ) appears among the top k retrieved documents.", "labels": [], "entities": []}, {"text": "The experiment is repeated for all examples in SOU RCE Lsource and the results are then averaged.", "labels": [], "entities": [{"text": "SOU RCE Lsource", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.8586544195810953}]}, {"text": "The graph suggests that the method performs well, as the document often appears among the first few results.", "labels": [], "entities": []}, {"text": "In about 65% of cases, the document is found among the first 50 retrieved items.", "labels": [], "entities": []}, {"text": "We believe that if the set of candidates (controlled by the t parameter) contains this document, the CLLD method is likely to produce better results, this is especially true for the CLESA2Links method.", "labels": [], "entities": []}, {"text": "The overall results for all the methods are presented in.", "labels": [], "entities": []}, {"text": "We have experimentally sett = 10 for Spanish to English and t = 3 for Czech to English CLLD.", "labels": [], "entities": []}, {"text": "CL-ESA2Links performed in the experiments the best achieving 0.2 precision at 0.3 recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9850053787231445}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9936345815658569}]}, {"text": "CL-ESA2Similar performed the best out of the purely content-based methods.", "labels": [], "entities": []}, {"text": "Though the precision/recall might seem quite low, a number of things should betaken into account: \u2022 A significant number of potentially useful links is still missing in our ground truth, because people typically do not intend to link all relevant information.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9996102452278137}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.987999439239502}]}, {"text": "As a result, many potentially useful connections are not explicitly present in Wikipedia ().", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9616975784301758}]}, {"text": "The problem can be partly mitigated by combining the ground truth from more language versions.", "labels": [], "entities": []}, {"text": "Another approach is to measure the agreement instead of precision/recall characteristics (see Section 6.3).", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.999187171459198}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9322847723960876}]}, {"text": "\u2022 A significant number of links in Wikipedia are conceptual links.", "labels": [], "entities": []}, {"text": "These links do not express a particularly strong relationship at the article level.", "labels": [], "entities": []}, {"text": "This makes it very difficult for the pure-content based methods to find them, which results in low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9988101720809937}]}, {"text": "It seems that CLESA2Links is the only method that does not suffer from this issue.", "labels": [], "entities": []}, {"text": "\u2022 The experiment settings make it hard for the methods to achieve high precision/recall performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9987504482269287}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.935616672039032}]}, {"text": "The T ARGET Ltarget set contains 3.8 million articles, out of which, the methods are supposed to identify on average just a small subset of target documents.", "labels": [], "entities": [{"text": "T ARGET Ltarget set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.6625476405024529}]}, {"text": "More precisely, in Spanish to English CLLD, our ground truth contains on average 341 target documents with standard deviation 293, in Czech to English, it contains on average 382 target documents with standard deviation 292.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The agreement of Spanish and English  Wikipedia and Czech and English Wikipedia on  their link structures calculated and summed for all  pages in SOU RCE es . Y -indicates yes, N -no,  N/A -not available/no decision", "labels": [], "entities": [{"text": "SOU RCE es", "start_pos": 156, "end_pos": 166, "type": "DATASET", "confidence": 0.6914704938729604}]}]}