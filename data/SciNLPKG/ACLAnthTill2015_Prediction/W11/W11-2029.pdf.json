{"title": [{"text": "Learning to Balance Grounding Rationales for Dialogue Systems", "labels": [], "entities": [{"text": "Learning to Balance Grounding Rationales", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6907062530517578}]}], "abstractContent": [{"text": "This paper reports on an experiment that investigates clarification subdialogues in intentionally noisy speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7575088739395142}]}, {"text": "The architecture learns weights for mixtures of grounding strategies from examples provided by a human wizard embedded in the system.", "labels": [], "entities": []}, {"text": "Results indicate that the architecture learns to eliminate misunderstandings reliably despite high word error rate.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 99, "end_pos": 114, "type": "METRIC", "confidence": 0.7704489429791769}]}], "introductionContent": [{"text": "We seek to develop spoken dialogue systems (SDSs) that communicate effectively despite uncertain input.", "labels": [], "entities": []}, {"text": "Our thesis is that a task-oriented SDS can perform well despite a high degree of recognizer noise by relying on context.", "labels": [], "entities": []}, {"text": "The SDS described here uses FORRSooth, a semisynchronous architecture underdevelopment for task-oriented human-computer dialogue.", "labels": [], "entities": [{"text": "FORRSooth", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9268868565559387}]}, {"text": "Our immediate goals are to reduce nonunderstandings of user utterances (where the SDS produces no interpretation) and to eliminate misunderstandings (where the SDS misinterprets user utterances).", "labels": [], "entities": []}, {"text": "The experiment recounted here investigates subdialogues consisting of an initial user response to a system prompt, and any subsequent turns that might be needed to result in full understanding of the original response.", "labels": [], "entities": []}, {"text": "Our principal finding is that a FORRSooth-based SDS learns to build on partial understandings and to eliminate misunderstandings despite noisy ASR.", "labels": [], "entities": [{"text": "FORRSooth-based SDS", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.4547761380672455}]}, {"text": "A FORRSooth-based SDS is intended to interact effectively \"without the luxury of perfect components\"), such as high-performance ASR.", "labels": [], "entities": [{"text": "FORRSooth-based SDS", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.5290881097316742}, {"text": "ASR", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9249383211135864}]}, {"text": "FORRSooth relies on portfolios of strategies for utterance interpretation and grounding, and learns to balance them from its experience.", "labels": [], "entities": [{"text": "FORRSooth", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9000065326690674}, {"text": "utterance interpretation", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.9009668827056885}]}, {"text": "Its confidence in its interpretations is dynamically calibrated against its past experience.", "labels": [], "entities": []}, {"text": "At each user utterance, FORRSooth selects grounding actions modulated to build upon partial interpretations in subsequent exchanges with the user.", "labels": [], "entities": [{"text": "FORRSooth", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9419773817062378}]}, {"text": "The experiment presented here bootstraps the SDS with human expertise.", "labels": [], "entities": []}, {"text": "Ina Wizard of Oz (WOz) study, a person (the wizard) replaces selected SDS components.", "labels": [], "entities": []}, {"text": "Knowledge is then extracted from the wizard's behavior to improve the SDS.", "labels": [], "entities": [{"text": "SDS", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9426842927932739}]}, {"text": "FORRSooth uses the Relative Support Weight Learning (RSWL) algorithm) to learn weights that balance its individual strategies.", "labels": [], "entities": [{"text": "FORRSooth", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9401239156723022}, {"text": "Relative Support Weight Learning (RSWL)", "start_pos": 19, "end_pos": 58, "type": "TASK", "confidence": 0.5690266149384635}]}, {"text": "Training examples for grounding strategies are based upon examples produced by an ablated wizard who was restricted to the same information and actions as the system ().", "labels": [], "entities": []}, {"text": "Our domain is the Andrew Heiskell Braille and Talking Book Library.", "labels": [], "entities": [{"text": "Andrew Heiskell Braille and Talking Book Library", "start_pos": 18, "end_pos": 66, "type": "DATASET", "confidence": 0.7863596422331673}]}, {"text": "Heiskell's patrons order their books by telephone, during conversation with a librarian.", "labels": [], "entities": []}, {"text": "The next section of this paper presents related work.", "labels": [], "entities": []}, {"text": "Subsequent sections describe the weight learning, the SDS architecture, and an experiment that challenges the robustness of utterance interpretation and grounding with intentionally noisy ASR.", "labels": [], "entities": [{"text": "weight learning", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.8142907321453094}, {"text": "SDS", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9572263360023499}, {"text": "utterance interpretation", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.7742763459682465}]}, {"text": "We conclude with a discussion of the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "This experiment tests FX2's ability to learn IN-TERPRETATION and GROUNDING weights.", "labels": [], "entities": [{"text": "IN-TERPRETATION", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.9786179065704346}, {"text": "GROUNDING", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9969815611839294}]}, {"text": "In each dialogue, FX2 introduces itself, prompts the subject for her name or a book title, and then continues the dialogue until FX2 commits to a binding for the concept, or gives up.", "labels": [], "entities": []}, {"text": "Four undergraduate native English speakers (two female, two male) participated.", "labels": [], "entities": []}, {"text": "Speech input and output was through a microphone headset.", "labels": [], "entities": []}, {"text": "The PocketSphinx speech recognizer produced ASR output) with Wall-Street Journal dictation acoustic models adapted with ten hours of spontaneous speech.", "labels": [], "entities": [{"text": "PocketSphinx speech recognizer", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.7765186230341593}, {"text": "Wall-Street Journal dictation acoustic", "start_pos": 61, "end_pos": 99, "type": "DATASET", "confidence": 0.9185895621776581}]}, {"text": "We built distinct trigram statistical language models for each type of agreement using names and titles from the Heiskell database.", "labels": [], "entities": [{"text": "Heiskell database", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.9246940016746521}]}, {"text": "We collected three data sets, referenced here as baseline, wizard, and learning.", "labels": [], "entities": []}, {"text": "Each had two agreement graphs: UserName seeks a grounded value for the patron's full name, and BookTitle seeks a grounded value fora book title.", "labels": [], "entities": [{"text": "BookTitle", "start_pos": 95, "end_pos": 104, "type": "DATASET", "confidence": 0.9357376098632812}]}, {"text": "120 dialogues were collected for each dataset.", "labels": [], "entities": []}, {"text": "FX2 includes an optional wizard component.", "labels": [], "entities": [{"text": "FX2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9434323310852051}]}, {"text": "When active, the wizard component displays a GUI showing the current interpretation hypotheses for target concepts, along with their respective merit.", "labels": [], "entities": []}, {"text": "A screenshot for the wizard GUI appears in.", "labels": [], "entities": []}, {"text": "A wizard dialogue activates the wizard component and uses INTERPRETATION as usual, but embeds a person (the wizard) in GROUNDING.", "labels": [], "entities": [{"text": "INTERPRETATION", "start_pos": 58, "end_pos": 72, "type": "METRIC", "confidence": 0.9852710962295532}, {"text": "GROUNDING", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.5795281529426575}]}, {"text": "The wizard's purpose in this experiment is to provide training data for GROUNDING.", "labels": [], "entities": [{"text": "GROUNDING", "start_pos": 72, "end_pos": 81, "type": "TASK", "confidence": 0.6877792477607727}]}, {"text": "After each user turn, the wizard makes two decisions based on data from the GUI: whether to consider any target as grounded, and which in a set of possible grounding actions to use next.", "labels": [], "entities": []}, {"text": "The GUI displays what FX2 would choose for each decision; the wizard can either accept or override it.", "labels": [], "entities": []}, {"text": "Ordinarily, a FORR-based system begins with uniform Advisor weights and learns more appropriate values during its experience.", "labels": [], "entities": []}, {"text": "Because correct interpretation and grounding are difficult tasks, however, we chose hereto prime these weights and hypothesis merits using training examples collected during development.", "labels": [], "entities": []}, {"text": "Development data for INTERPRETATION included 200 patron names, 400 book titles, and 50 indicator.", "labels": [], "entities": [{"text": "INTERPRETATION", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.6969889402389526}]}, {"text": "The wizard GUI displays hypotheses fora title from a user utterance. concepts.", "labels": [], "entities": []}, {"text": "ASR output for each item, along with its correct value, became a training example.", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.623900830745697}]}, {"text": "Development data for GROUNDING came from 20 preliminary wizard dialogues.", "labels": [], "entities": [{"text": "GROUNDING", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.7076335549354553}]}, {"text": "The development data also served to prime hypothesis merit.", "labels": [], "entities": []}, {"text": "Each subject had 30 dialogues with the system for the baseline dataset.", "labels": [], "entities": []}, {"text": "For the wizard data set, FX2 used the same primed weights and merits as the baseline.", "labels": [], "entities": []}, {"text": "The wizard's grounding actions and the target graphs on which they were based were saved as training examples.", "labels": [], "entities": []}, {"text": "Weights for GROUNDING Advisors were learned from the development data training examples and the training examples saved from the wizard data set together before collecting the learned data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Performance across three data sets.  Table 2. Distribution of grounding actions.", "labels": [], "entities": [{"text": "Distribution of grounding", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.8225038051605225}]}]}