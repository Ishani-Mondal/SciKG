{"title": [{"text": "Testing the Robustness of Online Word Segmentation: Effects of Linguistic Diversity and Phonetic Variation", "labels": [], "entities": [{"text": "Online Word Segmentation", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.643029123544693}]}], "abstractContent": [{"text": "Models of the acquisition of word segmen-tation are typically evaluated using phonem-ically transcribed corpora.", "labels": [], "entities": []}, {"text": "Accordingly, they implicitly assume that children know how to undo phonetic variation when they learn to extract words from speech.", "labels": [], "entities": []}, {"text": "Moreover, whereas models of language acquisition should perform similarly across languages, evaluation is often limited to English samples.", "labels": [], "entities": []}, {"text": "Using child-directed corpora of English, French and Japanese, we evaluate the performance of state-of-the-art statistical models given inputs where phonetic variation has not been reduced.", "labels": [], "entities": []}, {"text": "To do so, we measure segmentation robustness across different levels of segmen-tal variation, simulating systematic allophonic variation or errors in phoneme recognition.", "labels": [], "entities": [{"text": "phoneme recognition", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7707394361495972}]}, {"text": "We show that these models do not resist an increase in such variations and do not generalize to typologically different languages.", "labels": [], "entities": []}, {"text": "From the perspective of early language acquisition, the results strengthen the hypothesis according to which phonological knowledge is acquired in large part before the construction of a lexicon.", "labels": [], "entities": [{"text": "early language acquisition", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6906108657519022}]}], "introductionContent": [{"text": "Speech contains very few explicit boundaries between linguistic units: silent pauses often mark utterance boundaries, but boundaries between smaller units (e.g. words) are absent most of the time.", "labels": [], "entities": []}, {"text": "Procedures by which infants could develop word segmentation strategies have been discussed at length, from both a psycholinguistic and a computational point of view.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7210056483745575}]}, {"text": "Many models relying on statistical information have been proposed, and some of them exhibit satisfactory performance: MBDP-1, NGS-u) and DP (Goldwater, can be considered state-of-the-art.", "labels": [], "entities": [{"text": "MBDP-1", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.5613585114479065}]}, {"text": "Though there is evidence that prosodic, phonotactic and coarticulation cues may count more than statistics, it is still a matter of interest to know how much can be learned without linguistic cues.", "labels": [], "entities": []}, {"text": "To use Venkataraman's words, we are interested in \"the performance of bare-bones statistical models.\"", "labels": [], "entities": []}, {"text": "The aforementioned computational simulations have two major downsides.", "labels": [], "entities": []}, {"text": "First, all models of language acquisition should generalize to typologically different languages; however, the word segmentation experiments mentioned above have never been carried out on phonemically transcribed, childdirected speech in languages other than English.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7399857342243195}, {"text": "word segmentation", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7063521593809128}]}, {"text": "Second, these experiments use phonemically transcribed corpora as the input and, as such, make the implicit simplifying assumption that, when children learn to segment speech into words, they have already learned phonological rules and know how to reduce the inherent variability in speech to a finite (and rather small) number of abstract categories: the phonemes.", "labels": [], "entities": []}, {"text": "Rytting, addressed this issue and replaced the usual phonemic input with probability vectors over a finite set of symbols.", "labels": [], "entities": []}, {"text": "Still, this set of symbols is limited to the phonemic inventory of the language: the reduction of phonetic variation is taken for granted.", "labels": [], "entities": []}, {"text": "In other words, previous simulations evaluated the performance of the models given idealized input but offered no guarantee as to the performance of the mod-els on realistic input.", "labels": [], "entities": []}, {"text": "We present a comparative survey that evaluates the extent to which state-of-the-art statistical models of word segmentation resist segmental variation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7311093062162399}]}, {"text": "To do so, we designed a parametric benchmark where more and more variation was gradually introduced into phonemic corpora of child-directed speech.", "labels": [], "entities": []}, {"text": "Phonetic variation was simulated applying contextdependent allophonic rules to phonemic corpora.", "labels": [], "entities": [{"text": "Phonetic variation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7729603052139282}]}, {"text": "Other corpora in which noise was created by random phoneme substitutions were used as controls.", "labels": [], "entities": []}, {"text": "Furthermore, to draw language-independent conclusions, we used corpora from three typologically different languages: English, French and Japanese.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Venkataraman's (2001) implementation of the now-standard evaluation protocol proposed by and then extended by.", "labels": [], "entities": []}, {"text": "Obviously, orthographic words are not the optimal target fora model of language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.6942828744649887}]}, {"text": "Yet, inline with previously reported experiments, we used the orthographic segmentation as the standard of correct segmentation.", "labels": [], "entities": []}, {"text": "Performance of the segmentation models 2 on phonemic corpora is presented in in terms of F sand F l -score (upper and lower panel, respectively).", "labels": [], "entities": [{"text": "F sand F l -score", "start_pos": 89, "end_pos": 106, "type": "METRIC", "confidence": 0.8348381320635477}]}, {"text": "We were able to replicate previous results on English by Brent and Venkataraman almost exactly; the small difference, less than one percent, was probably caused by the use of different implementations.", "labels": [], "entities": []}, {"text": "From a cross-linguistic point of view, the main observation is that these models do not seem to generalize to typologically different languages.", "labels": [], "entities": []}, {"text": "Whereas MBDP-1 and NGS-u's F s value is 69% for English, it is only 54% for French and 41% for Japanese.", "labels": [], "entities": [{"text": "MBDP-1", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.6793525218963623}, {"text": "F s value", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9648434718449911}]}, {"text": "Similar observations can be made for F l . Purely statistical strategies seem to be particularly ineffective on our Japanese sample: inserting word boundaries at random yields a better lexicon than using probabilistic models.", "labels": [], "entities": []}, {"text": "A crude way to determine whether a word segmentation model tends to break words apart (oversegmentation) or to cluster various words in a single chunk (under-segmentation) is to compare the average word length (AWL) in its output to the AWL in the standard segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7086665481328964}, {"text": "word length (AWL)", "start_pos": 198, "end_pos": 215, "type": "METRIC", "confidence": 0.7966837704181671}, {"text": "AWL", "start_pos": 237, "end_pos": 240, "type": "METRIC", "confidence": 0.9956356883049011}]}, {"text": "If the output's AWL is greater than the standard's, then the output is undersegmented, and vice versa.", "labels": [], "entities": [{"text": "AWL", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9990180730819702}]}, {"text": "Even if NGS-u produces shorter words than MBDP-1, both models exhibit, once again, similar within-language behaviors.", "labels": [], "entities": [{"text": "MBDP-1", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.6832830309867859}]}, {"text": "English was slightly under-segmented by MBDP-1 and over-segmented by NGS-u: ouputs' AWL are respectively 3.1 and 2.7, while the standard is 2.9.", "labels": [], "entities": [{"text": "MBDP-1", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9687702655792236}, {"text": "NGS-u", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9509139060974121}, {"text": "AWL", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9196268320083618}]}, {"text": "Our results are consistent with what observed for DP: error analysis shows that both MBDP-1 and NGS-u also break off frequent English morphological affixes, namely /IN/ (-ing) and /s,z/ (-s).", "labels": [], "entities": [{"text": "MBDP-1", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.7319408655166626}]}, {"text": "As for French, AWL values suggest the corpus was under-segmented: 3.1 for MBDP-1's output and 2.9 for NGS-u's, while the standard is 2.4.", "labels": [], "entities": [{"text": "AWL", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.8393758535385132}, {"text": "MBDP-1", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.785702109336853}]}, {"text": "On the contrary, Japanese was heavily oversegmented: many monophonemic words emerged and, whereas the standard AWL is 3.9, the ouputs' AWL is 2.7 for both models.", "labels": [], "entities": [{"text": "AWL", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9983225464820862}, {"text": "AWL", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.8794897198677063}]}, {"text": "Over-segmentation maybe correlated to the number of syllable types in the language: English and French phonotactics allow consonantal clusters, bringing the number of syllable types to a few thousands.", "labels": [], "entities": []}, {"text": "By contrast, Japanese has a much simpler syllabic structure and less syllable types which, as a consequence, are often repeated and may (incorrectly) be considered as words by statistical models.", "labels": [], "entities": []}, {"text": "The fact that the models do worse for French and Japanese is not especially surprising: both languages have many more affixal morphemes than English.", "labels": [], "entities": []}, {"text": "Consider French, where the lexical autonomy of clitics is questionable: whereas /s/ (s' or c') or /k/ (qu') are highly frequent words in our orthographic standard, many errors are due to the agglutination of these clitics to the following word.", "labels": [], "entities": []}, {"text": "These are counted as segmentation errors, but should they?", "labels": [], "entities": [{"text": "segmentation", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9604910612106323}]}, {"text": "Furthermore, none of the segmentation models we benchmarked exhibit similar performance across languages: invariably, they perform better on English.", "labels": [], "entities": []}, {"text": "There maybe a correlation between the performance of segmentation models and the percentage of word hapaxes, i.e. words which occur only once in the corpus: the English, French and Japanese corpora contain 31.7%, 37.1% and 60.7% of word hapaxes, respectively.", "labels": [], "entities": []}, {"text": "The more words tend to occur only once, the less MBDP-1, NGS-u and DP perform on segmentation.", "labels": [], "entities": [{"text": "MBDP-1", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.6239304542541504}, {"text": "DP", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9036580324172974}]}, {"text": "This is consistent with the usual assumption that infants use familiar words to find new ones.", "labels": [], "entities": []}, {"text": "It may also be the case that these models are not implicitly tuned to English, but that the contribution of statistical cues to word segmentation differs across languages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7100243717432022}]}, {"text": "In French, for example, stress invariably marks the end of a word (although the end of a word is not necessarily marked by stress).", "labels": [], "entities": []}, {"text": "By contrast, there are languages like English or Spanish where stress is less predictable: children cannot rely solely on this cue to extract words and may thus have to give more weight to statistics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Elementary corpus statistics, including number  of utterances (U), words (W) and phonemes (P).", "labels": [], "entities": []}]}