{"title": [], "abstractContent": [{"text": "We examine sentiment analysis on Twitter data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9473669826984406}]}, {"text": "The contributions of this paper are: (1) We introduce POS-specific prior polarity features.", "labels": [], "entities": []}, {"text": "(2) We explore the use of a tree kernel to obviate the need for tedious feature engineering.", "labels": [], "entities": []}, {"text": "The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art base-line.", "labels": [], "entities": []}], "introductionContent": [{"text": "Microblogging websites have evolved to become a source of varied kind of information.", "labels": [], "entities": []}, {"text": "This is due to nature of microblogs on which people post real time messages about their opinions on a variety of topics, discuss current issues, complain, and express positive sentiment for products they use in daily life.", "labels": [], "entities": []}, {"text": "In fact, companies manufacturing such products have started to poll these microblogs to get a sense of general sentiment for their product.", "labels": [], "entities": []}, {"text": "Many times these companies study user reactions and reply to users on microblogs.", "labels": [], "entities": []}, {"text": "One challenge is to build technology to detect and summarize an overall sentiment.", "labels": [], "entities": [{"text": "summarize an overall sentiment", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.849320650100708}]}, {"text": "In this paper, we look atone such popular microblog called Twitter and build models for classifying \"tweets\" into positive, negative and neutral sentiment.", "labels": [], "entities": []}, {"text": "We build models for two classification tasks: a binary task of classifying sentiment into positive and negative classes and a 3-way task of classifying sentiment into positive, negative and neutral classes.", "labels": [], "entities": []}, {"text": "We experiment with three types of models: unigram model, a feature based model and a tree kernel based model.", "labels": [], "entities": []}, {"text": "For the feature based model we use some of the features proposed in past literature and propose new features.", "labels": [], "entities": []}, {"text": "For the tree kernel based model we design anew tree representation for tweets.", "labels": [], "entities": []}, {"text": "We use a unigram model, previously shown to work well for sentiment analysis for Twitter data, as our baseline.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9183961153030396}]}, {"text": "Our experiments show that a unigram model is indeed a hard baseline achieving over 20% over the chance baseline for both classification tasks.", "labels": [], "entities": []}, {"text": "Our feature based model that uses only 100 features achieves similar accuracy as the unigram model that uses over 10,000 features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9991865754127502}]}, {"text": "Our tree kernel based model outperforms both these models by a significant margin.", "labels": [], "entities": []}, {"text": "We also experiment with a combination of models: combining unigrams with our features and combining our features with the tree kernel.", "labels": [], "entities": []}, {"text": "Both these combinations outperform the unigram baseline by over 4% for both classification tasks.", "labels": [], "entities": []}, {"text": "In this paper, we present extensive feature analysis of the 100 features we propose.", "labels": [], "entities": []}, {"text": "Our experiments show that features that have to do with Twitter-specific features (emoticons, hashtags etc.) add value to the classifier but only marginally.", "labels": [], "entities": []}, {"text": "Features that combine prior polarity of words with their parts-of-speech tags are most important for both the classification tasks.", "labels": [], "entities": []}, {"text": "Thus, we see that standard natural language processing tools are useful even in a genre which is quite different from the genre on which they were trained (newswire).", "labels": [], "entities": []}, {"text": "Furthermore, we also show that the tree kernel model performs roughly as well as the best feature based models, even though it does not require detailed feature engineering.", "labels": [], "entities": []}, {"text": "We use manually annotated Twitter data for our experiments.", "labels": [], "entities": []}, {"text": "One advantage of this data, over previously used data-sets, is that the tweets are collected in a streaming fashion and therefore represent a true sample of actual tweets in terms of language use and content.", "labels": [], "entities": []}, {"text": "Our new data set is available to other researchers.", "labels": [], "entities": []}, {"text": "In this paper we also introduce two resources which are available (contact the first author): 1) a hand annotated dictionary for emoticons that maps emoticons to their polarity and 2) an acronym dictionary collected from the web with English translations of over 5000 frequently used acronyms.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we discuss classification tasks like sentiment analysis on micro-blog data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9605308771133423}]}, {"text": "In section 3, we give details about the data.", "labels": [], "entities": []}, {"text": "In section 4 we discuss our pre-processing technique and additional resources.", "labels": [], "entities": []}, {"text": "In section 5 we present our prior polarity scoring scheme.", "labels": [], "entities": []}, {"text": "In section 6 we present the design of our tree kernel.", "labels": [], "entities": []}, {"text": "In section 7 we give details of our feature based approach.", "labels": [], "entities": []}, {"text": "In section 8 we present our experiments and discuss the results.", "labels": [], "entities": []}, {"text": "We conclude and give future directions of research in section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present experiments and results for two classification tasks: 1) Positive versus Negative and 2) Positive versus Negative versus Neutral.", "labels": [], "entities": []}, {"text": "For each of the classification tasks we present three models, as well as results for two combinations of these models: For the unigram plus Senti-features model, we present feature analysis to gain insight about what kinds of features are adding most value to the model.", "labels": [], "entities": []}, {"text": "We also present learning curves for each of the models and compare learning abilities of models when provided limited data.", "labels": [], "entities": []}, {"text": "Experimental-Set-up: For all our experiments we use Support Vector Machines (SVM) and report averaged 5-fold cross-validation test results.", "labels": [], "entities": []}, {"text": "We tune the C parameter for SVM using an embedded 5-fold cross-validation on the training data of each fold, i.e. for each fold, we first run 5-fold cross-validation only on the training data of that fold for different values of C.", "labels": [], "entities": [{"text": "SVM", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9426905512809753}]}, {"text": "We pick the setting that yields the best cross-validation error and use that C for determining test error for that fold.", "labels": [], "entities": []}, {"text": "As usual, the reported accuracies is the average over the five folds.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9881105422973633}]}], "tableCaptions": [{"text": " Table 2: Part of the dictionary of emoticons", "labels": [], "entities": []}, {"text": " Table 3: Statistics about the data used for our experi- ments.", "labels": [], "entities": []}, {"text": " Table 5: Average and standard deviation for test accuracy  for the 2-way classification task using different models:  Unigram (baseline), tree kernel, Senti-features, unigram  plus Senti-features, and tree kernel plus senti-features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9779762625694275}]}, {"text": " Table 6: Accuracy and F1-measure for 2-way classifica- tion task using Unigrams and Senti-features. All f i refer  to", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991496801376343}, {"text": "F1-measure", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.999534010887146}]}, {"text": " Table 8: Average and standard deviation for test accuracy  for the 3-way classification task using different models:  Unigram (baseline), tree kernel, Senti-features, unigram  plus Senti-features, and Senti-features plus tree kernels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9776667952537537}]}, {"text": " Table 9: Accuracy and F1-measure for 3-way classifica- tion task using unigrams and Senti-features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987608194351196}, {"text": "F1-measure", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9995176792144775}]}]}