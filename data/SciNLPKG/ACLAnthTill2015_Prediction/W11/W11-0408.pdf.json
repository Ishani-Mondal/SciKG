{"title": [{"text": "Reducing the Need for Double Annotation", "labels": [], "entities": [{"text": "Double Annotation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.602459266781807}]}], "abstractContent": [{"text": "The quality of annotated data is crucial for supervised learning.", "labels": [], "entities": []}, {"text": "To eliminate errors in single annotated data, a second round of annotation is often used.", "labels": [], "entities": []}, {"text": "However, is it absolutely necessary to double annotate every ex-ample?", "labels": [], "entities": []}, {"text": "We show that it is possible to reduce the amount of the second round of annotation by more than half without sacrificing the performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised learning has become the dominant paradigm in NLP in recent years thus making the creation of high-quality annotated corpora atop priority in the field.", "labels": [], "entities": []}, {"text": "A corpus where each instance is annotated by a single annotator unavoidably contains errors.", "labels": [], "entities": []}, {"text": "To improve the quality of the data, one may choose to annotate each instance twice and adjudicate the disagreements thus producing the gold standard.", "labels": [], "entities": []}, {"text": "For example, the OntoNotes () project opted for this approach.", "labels": [], "entities": []}, {"text": "However, is it absolutely necessary to double annotate every example?", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that it is possible to double annotate only a subset of the single annotated data and still achieve the same level of performance as with full double annotation.", "labels": [], "entities": []}, {"text": "We accomplish this task by using the single annotated data to guide the selection of the instances to be double annotated.", "labels": [], "entities": []}, {"text": "We propose several algorithms that accept single annotated data as input.", "labels": [], "entities": []}, {"text": "The algorithms select a subset of this data that they recommend for another round of annotation and adjudication.", "labels": [], "entities": []}, {"text": "The single annotated data our algorithms work with can potentially come from any source.", "labels": [], "entities": []}, {"text": "For example, it can be the single annotated output of active learning or the data that had been randomly sampled from some corpus and single annotated.", "labels": [], "entities": []}, {"text": "Our approach is applicable whenever a second round of annotation is being considered to improve the quality of the data.", "labels": [], "entities": []}, {"text": "Our approach is similar in spirit to active learning but more practical in a double annotation multitagger environment.", "labels": [], "entities": []}, {"text": "We evaluate this approach on OntoNotes word sense data.", "labels": [], "entities": [{"text": "OntoNotes word sense data", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.8052396774291992}]}, {"text": "Our best algorithm detects 75% of the errors, while the random sampling baseline only detects less than a half of that amount.", "labels": [], "entities": []}, {"text": "We also show that this algorithm can lead to a 54% reduction in the amount of annotation needed for the second round of annotation.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: we discuss the relevant work in section 2, we explain our approach in section 3, we evaluate our approach in section 4, we discuss the results and draw a conclusion in section 5, and finally, we talk about our plans for future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation we use the word sense data annotated by the OntoNotes project.", "labels": [], "entities": []}, {"text": "The OntoNotes data was chosen because it is fully double-blind annotated by human annotators and the disagreements are adjudicated by a third (more experienced) annotator.", "labels": [], "entities": [{"text": "OntoNotes data", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8942565321922302}]}, {"text": "This type of data allows us to: (1) Simulate single annotation by using the labels assigned by the first annotator, (2) Simulate the second round of annotation for selected examples by using the labels assigned by the second annotator, (3) Evaluate how well our algorithms capture the errors made by the first annotator, and (4) Measure the performance of the corrected data against the performance of the double annotated and adjudicated gold standard.", "labels": [], "entities": []}, {"text": "We randomly split the gold standard data into ten parts of equal size.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.6510698695977529}]}, {"text": "Nine parts are used as a pool of data from which a subset is selected for repeated labeling.", "labels": [], "entities": []}, {"text": "The rest is used as a test set.", "labels": [], "entities": []}, {"text": "Before passing the pool to the algorithm, we \"single annotate\" it (i.e. relabel with the labels assigned by the first annotator).", "labels": [], "entities": []}, {"text": "The test set always stays double annotated and adjudicated to make sure the performance is evaluated against the gold standard labels.", "labels": [], "entities": []}, {"text": "The cycle is repeated ten times and the results are averaged.", "labels": [], "entities": []}, {"text": "Since our goal is finding errors in single annotated data, a brief explanation of what we count as an error is appropriate.", "labels": [], "entities": []}, {"text": "In this evaluation, the errors are the disagreements between the first annotator and the gold standard.", "labels": [], "entities": []}, {"text": "The fact that our data is double annotated allows us to be reasonably sure that most of the errors made by the first annotator were caught (as disagreements with the second annotator) and resolved.", "labels": [], "entities": []}, {"text": "Even though other errors may still exist in the data (e.g. when the two annotators made the same mistake), we assume that there are very few of them and we ignore them for the purpose of this study.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of performance evaluation. Error detection performance is shown at the top part of the table. Model  performance is shown at the bottom.", "labels": [], "entities": [{"text": "Error detection", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7992076575756073}]}, {"text": " Table 4: Performance at various sizes of selected data.", "labels": [], "entities": []}]}