{"title": [{"text": "An Expectation Maximization Algorithm for Textual Unit Alignment", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7812586426734924}, {"text": "Textual Unit Alignment", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6904862920443217}]}], "abstractContent": [{"text": "The paper presents an Expectation Maximiza-tion (EM) algorithm for automatic generation of parallel and quasi-parallel data from any degree of comparable corpora ranging from parallel to weakly comparable.", "labels": [], "entities": []}, {"text": "Specifically, we address the problem of extracting related textual units (documents, paragraphs or sentences) relying on the hypothesis that, in a given corpus, certain pairs of translation equivalents are better indicators of a correct textual unit correspondence than other pairs of translation equivalents.", "labels": [], "entities": [{"text": "extracting related textual units (documents, paragraphs or sentences)", "start_pos": 40, "end_pos": 109, "type": "TASK", "confidence": 0.7574586868286133}]}, {"text": "We evaluate our method on mixed types of bilingual comparable corpora in six language pairs, obtaining state of the art accuracy figures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9993565678596497}]}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) is in a constant need of good quality training data both for translation models and for the language models.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8414738774299622}]}, {"text": "Regarding the latter, monolingual corpora is evidently easier to collect than parallel corpora and the truth of this statement is even more obvious when it comes to pairs of languages other than those both widely spoken and computationally well-treated around the world such as English, Spanish, French or German.", "labels": [], "entities": []}, {"text": "Comparable corpora came as a possible solution to the problem of scarcity of parallel corpora with the promise that it may serve as a seed for parallel data extraction.", "labels": [], "entities": [{"text": "parallel data extraction", "start_pos": 143, "end_pos": 167, "type": "TASK", "confidence": 0.6752623915672302}]}, {"text": "A general definition of comparability that we find operational is given by.", "labels": [], "entities": []}, {"text": "They say that a (bilingual) comparable corpus is a set of paired documents that, while not parallel in the strict sense, are related and convey overlapping information.", "labels": [], "entities": []}, {"text": "Current practices of automatically collecting domain-dependent bilingual comparable corpora from the Web usually begin with collecting a list oft terms as seed data in both the source and the target languages.", "labels": [], "entities": []}, {"text": "Each term (in each language) is then queried on the most popular search engine and the first N document hits are retained.", "labels": [], "entities": []}, {"text": "The final corpus will contain t \u00d7 N documents in each language and in subsequent usage the document boundaries are often disregarded.", "labels": [], "entities": []}, {"text": "At this point, it is important to stress out the importance of the pairing of documents in a comparable corpus.", "labels": [], "entities": []}, {"text": "Suppose that we want to wordalign a bilingual comparable corpus consisting of M documents per language, each with k words, using the IBM-1 word alignment algorithm.", "labels": [], "entities": [{"text": "IBM-1 word alignment", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.7596812645594279}]}, {"text": "This algorithm searches for each source word, the target words that have a maximum translation probability with the source word.", "labels": [], "entities": []}, {"text": "Aligning all the words in our corpus with no regard to document boundaries, would yield a time complexity of operations.", "labels": [], "entities": []}, {"text": "The alternative would be in finding a 1:p (with pa small positive integer, usually 1, 2 or 3) document assignment (a set of aligned document pairs) that would enforce the -no search outside the document boundary\u2016 condition when doing word alignment with the advantage of reducing the time complexity to operations.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 234, "end_pos": 248, "type": "TASK", "confidence": 0.7523855268955231}]}, {"text": "When M is large, the reduction may actually be vital to getting a result in a reasonable amount of time.", "labels": [], "entities": [{"text": "M", "start_pos": 5, "end_pos": 6, "type": "METRIC", "confidence": 0.8486955761909485}]}, {"text": "The downside of this simplification is the loss of information: two documents may not be correctly aligned thus depriving the wordalignment algorithm of the part of the search space that would have contained the right alignments.", "labels": [], "entities": []}, {"text": "Word alignment forms the basis of the phrase alignment procedure which, in turn, is the basis of any statistical translation model.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6796604990959167}, {"text": "phrase alignment", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7890822887420654}, {"text": "statistical translation", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.6220159232616425}]}, {"text": "A comparable corpus differs essentially from a parallel corpus by the fact that textual units do not follow a translation order that otherwise greatly reduces the word alignment search space in a parallel corpus.", "labels": [], "entities": [{"text": "word alignment search", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.7792952557404836}]}, {"text": "Given this limitation of a comparable corpus in general and the sizes of the comparable corpora that we will have to deal within particular, we have devised one variant of an Expectation Maximization (EM) algorithm that generates a 1:1 (p = 1) document assignment from a parallel and/or comparable corpus using only preexisting translation lexicons.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 175, "end_pos": 204, "type": "TASK", "confidence": 0.780444186925888}]}, {"text": "Its generality would permit it to perform the same task on other textual units such as paragraphs or sentences.", "labels": [], "entities": []}, {"text": "In what follows, we will briefly review the literature discussing document/paragraph alignment and then we will present the derivation of the EM algorithm that generates 1:1 document alignments.", "labels": [], "entities": [{"text": "document/paragraph alignment", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.5763518884778023}]}, {"text": "We will end the article with a thorough evaluation of the performances of this algorithm and the conclusions that arise from these evaluations.", "labels": [], "entities": []}], "datasetContent": [{"text": "The test data for document alignment was compiled from the corpora that was previously collected in the ACCURAT project and that is known to the project members as the \u2016Initial Comparable Corpora\u2016 or ICC for short.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6755504608154297}, {"text": "ACCURAT project", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.8411012291908264}]}, {"text": "It is important to know the fact that ICC contains all types of comparable corpora from parallel to weakly comparable documents but we classified document pairs in three classes: parallel (class name: p), strongly comparable (cs) and weakly comparable (cw).", "labels": [], "entities": []}, {"text": "We have considered the following pairs of languages: English-Romanian (en-ro), English-Latvian (en-lv), English-Lithuanian (en-lt), English-Estonian (enet), English-Slovene (en-sl) and English-Greek (en-el).", "labels": [], "entities": []}, {"text": "For each pair of languages, ICC also contains a Gold Standard list of document alignments that were compiled by hand for testing purposes.", "labels": [], "entities": []}, {"text": "We trained GIZA++ translation lexicons for every language pair using the DGT-TM 4 corpus.", "labels": [], "entities": [{"text": "GIZA++ translation lexicons", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.572479285299778}, {"text": "DGT-TM 4 corpus", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9355801145235697}]}, {"text": "The input texts were converted from their Unicode encoding to UTF-8 and were tokenized using a tokenizer web service described by.", "labels": [], "entities": []}, {"text": "Then, we applied a parallel version of GIZA++ () that gave us the translation dictionaries of content words only (nouns, verbs, adjective and adverbs) at wordform level.", "labels": [], "entities": []}, {"text": "For Romanian, Lithuanian, Latvian, Greek and English, we had lists of inflectional suffixes which we used to stem entries in respective dictionaries and processed documents.", "labels": [], "entities": []}, {"text": "Slovene remained the only language which involved wordform level processing.", "labels": [], "entities": [{"text": "wordform level processing", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6860237717628479}]}, {"text": "This parameter will introduce precision and recall with the -perfect\u2016 value for recall equal to ThrOut%.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9997000694274902}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9994059801101685}, {"text": "perfect\u2016 value", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.9362764755884806}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9955920577049255}, {"text": "ThrOut", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9973177313804626}]}, {"text": "Values for this parameter are from the set * +.", "labels": [], "entities": []}, {"text": "We ran EMACC (10 EM steps) on every possible combination of these parameters for the pairs of languages in question on both initial distributions D1 and D2.", "labels": [], "entities": []}, {"text": "For comparison, we also performed a baseline document alignment using the greedy algorithm of EMACC with the equation 8 supplying the document similarity measure.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.695971742272377}, {"text": "EMACC", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.959933876991272}]}, {"text": "The following 4 tables report a synthesis of the results we have obtained which, because of the lack of space, we cannot give in full.", "labels": [], "entities": []}, {"text": "We omit the results of EMACC with D1 initial distribution because the accuracy        In every table above, the P/R column gives the maximum precision and the associated recall EMACC was able to obtain for the corresponding pair of languages using the parameters (Prms.) from the next column.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.999279797077179}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9988393187522888}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9938377737998962}]}, {"text": "The P/R column gives the maximum recall with the associated precision that we obtained for that pair of languages.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9993162155151367}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9960173964500427}]}, {"text": "columns contain parameter settings for EMACC (see) and for the D2 baseline algorithm values for ThrGiza, ThrUpdate and ThrOut are given from the top (of the cell) to the bottom and in values of ThrGiza and ThrOut are also given from top to bottom (the ThrUpdate parameter is missing because the D2 baseline algorithm does not do re-estimation).", "labels": [], "entities": [{"text": "EMACC", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.886335015296936}, {"text": "ThrUpdate", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9530021548271179}, {"text": "ThrOut", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.909479022026062}]}, {"text": "The # column contains the size of the test set: the number of documents in each language that have to be paired.", "labels": [], "entities": []}, {"text": "The search space is # * # and the gold standard contains # pairs of human aligned document pairs.", "labels": [], "entities": []}, {"text": "To ease comparison between EMACC and the D2 baseline for each type of corpora (strongly and weakly comparable), we grayed maximal values between the two: either the precision in the P/R column or the recall in the P/R column.", "labels": [], "entities": [{"text": "EMACC", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8680645823478699}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9983354210853577}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9989368319511414}]}, {"text": "In the case of strongly comparable corpora (Tables 1 and 2), we see that the benefits of reestimating the probabilities of the translation equivalents (based on which we judge document alignments) begin to emerge with precisions for all pairs of languages (except en-sl) being better than those obtained with the D2 baseline.", "labels": [], "entities": [{"text": "precisions", "start_pos": 218, "end_pos": 228, "type": "METRIC", "confidence": 0.9943768382072449}]}, {"text": "But the real benefit of re-estimating the probabilities of translation equivalents along the EM procedure is visible from the comparison between.", "labels": [], "entities": []}, {"text": "Thus, in the case of weakly comparable corpora, in which EMACC with the D2 distribution is clearly better than the baseline (with the only exception of en-lt precision), due to the significant decrease in the lexical overlap, the EM procedure is able to produce important alignment clues in the form of re-estimated (bigger) probabilities of translation equivalents that, otherwise, would have been ignored.", "labels": [], "entities": [{"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.5261474251747131}]}, {"text": "It is important to mention the fact that the results we obtained varied a lot with values of the parameters ThrGiza and ThrUpdate.", "labels": [], "entities": [{"text": "ThrGiza", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9800631999969482}, {"text": "ThrUpdate", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9685482382774353}]}, {"text": "We observed, for the majority of studied language pairs, that lowering the value for ThrGiza and/or ThrUpdate (0.1, 0.01, 0.001\u2026), would negatively impact the performance of EMACC due to the fact of introducing noise in the initial computation of the D2 distribution and also on re-estimating (increasing) probabilities for irrelevant translation equivalents.", "labels": [], "entities": [{"text": "ThrGiza", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9050925970077515}, {"text": "ThrUpdate", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9565911293029785}]}, {"text": "At the other end, increasing the threshold for these parameters (0.8, 0.85, 0.9\u2026) would also result in performance decreasing due to the fact that too few translation equivalents (be they all correct) are not enough to pinpoint correct document alignments since there are great chances for them to actually appear in all document pairs.", "labels": [], "entities": [{"text": "pinpoint correct document alignments", "start_pos": 219, "end_pos": 255, "type": "TASK", "confidence": 0.573332205414772}]}, {"text": "So, we have experimentally found that there is a certain balance between the degree of correctness of translation equivalents and their ability to pinpoint correct document alignments.", "labels": [], "entities": [{"text": "pinpoint correct document alignments", "start_pos": 147, "end_pos": 183, "type": "TASK", "confidence": 0.6031026691198349}]}, {"text": "In other words, the paradox resides in the fact that if a certain pair of translation equivalents is not correct but the respective words appear only in documents which correctly align to one another, that pair is very important to the alignment process.", "labels": [], "entities": []}, {"text": "Conversely, if a pair of translation equivalents has a very high probability score (thus being correct) but appears in almost every possible pair of documents, that pair is not informative to the alignment process and must be excluded.", "labels": [], "entities": []}, {"text": "We see now that the EMACC aims at finding the set of translation equivalents that is maximally informative with respect to the set of document alignments.", "labels": [], "entities": [{"text": "EMACC", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8738160729408264}]}, {"text": "We have introduced the ThrOut parameter in order to have better precision.", "labels": [], "entities": [{"text": "ThrOut parameter", "start_pos": 23, "end_pos": 39, "type": "METRIC", "confidence": 0.9725188910961151}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9981495141983032}]}, {"text": "This parameter actually instructs EMACC to output only the top (according to the alignment score probability ( )) ThrOut% of the document alignments it has found.", "labels": [], "entities": [{"text": "alignment score probability", "start_pos": 81, "end_pos": 108, "type": "METRIC", "confidence": 0.8917148311932882}, {"text": "ThrOut", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.8372125625610352}]}, {"text": "This means that, if all are correct, the maximum recall can only be ThrOut%.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9996159076690674}, {"text": "ThrOut", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9983857870101929}]}, {"text": "But another important function of ThrOut is to restrict the translation equivalents re-estimation (equation 7b) for only the top ThrOut% alignments.", "labels": [], "entities": []}, {"text": "In other words, only the probabilities of translation equivalents that are to be found in top ThrOut% best alignments in the current EM step are re-estimated.", "labels": [], "entities": []}, {"text": "We introduced this restriction in order to confine translation equivalents probability re-estimation to correct document alignments found so far.", "labels": [], "entities": []}, {"text": "Regarding the running time of EMACC, we can report that on a cluster with a total of 32 CPU cores (4 nodes) with 6-8 GB of RAM per node, the total running time is between 12h and 48h per language pair (about 2000 documents per language) depending on the setting of the various parameters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: EMACC with D2 initial distribution on strong- ly comparable corpora", "labels": [], "entities": [{"text": "EMACC", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8348574638366699}]}, {"text": " Table 2: D2 baseline algorithm on strongly comparable  corpora", "labels": [], "entities": []}, {"text": " Table 3: EMACC with D2 initial distribution on weakly  comparable corpora", "labels": [], "entities": [{"text": "EMACC", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8091905117034912}]}, {"text": " Table 4: D2 baseline algorithm on weakly comparable  corpora", "labels": [], "entities": []}]}