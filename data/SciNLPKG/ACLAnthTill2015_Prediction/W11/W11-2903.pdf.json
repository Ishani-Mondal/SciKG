{"title": [{"text": "Tree Parsing with Synchronous Tree-Adjoining Grammars", "labels": [], "entities": [{"text": "Tree Parsing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6421035677194595}]}], "abstractContent": [{"text": "Restricting the input or the output of a grammar-induced translation to a given set of trees plays an important role in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.738966315984726}]}, {"text": "The problem for practical systems is to find a compact (and in particular, finite) representation of said restriction.", "labels": [], "entities": []}, {"text": "For the class of synchronous tree-adjoining grammars, partial solutions to this problem have been described, some being restricted to the unweighted case, some to the monolingual case.", "labels": [], "entities": []}, {"text": "We introduce a formulation of this class of grammars which is effectively closed under input and output restrictions to regular tree languages, i.e., the restricted translations can again be represented by grammars.", "labels": [], "entities": []}, {"text": "Moreover, we present an algorithm that constructs these grammars for input and output restriction, which is inspired by Earley's algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many recent systems for statistical machine translation (SMT) use some grammar at their core., e. g., uses synchronous context-free grammars (SCFG) that derive pairs of translationally equivalent sentences.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8208859314521154}]}, {"text": "use tree-to-string transducers (called xRLNS) that describe pairs of the form (phrasestructure tree, string).", "labels": [], "entities": []}, {"text": "Other systems, such as, use variants of synchronous tree-adjoining grammars (STAGs) () that derive pairs of dependency or phrase-structure trees.", "labels": [], "entities": []}, {"text": "Common variants of STAGs are synchronous tree-substitution grammars (STSGs) and synchronous tree-insertion grammars (STIGs).", "labels": [], "entities": [{"text": "STAGs", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.9624817371368408}]}, {"text": "For grammar-based systems, a variety of tasks can be described using the general concepts of input product and output product.", "labels": [], "entities": []}, {"text": "Roughly speaking, these products restrict the translation described by the grammar to a given tree or string language on the input or output side.", "labels": [], "entities": []}, {"text": "For practical purposes, the derivations of the restricted translation are represented in a compact way, e.g., using a weighted regular tree grammar (WRTG).", "labels": [], "entities": []}, {"text": "The process of obtaining this representation is called tree parsing or string parsing, depending on the type of restriction.", "labels": [], "entities": [{"text": "tree parsing or string parsing", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6548725843429566}]}, {"text": "We illustrate the importance of input and output product by considering its role in three essential tasks of SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9959636926651001}]}, {"text": "After the rules of the grammar have been obtained from a sample of translation pairs (rule extraction), the probabilities of the rules need to be determined.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.7563409507274628}]}, {"text": "To this end, two approaches have been employed.", "labels": [], "entities": []}, {"text": "Some systems such as those by and hypothesize a canonical derivation for each translation pair, and apply relative-frequency estimation to the resulting derivations to obtain rule probabilities.", "labels": [], "entities": []}, {"text": "While this procedure is computationally inexpensive, it only maximizes the likelihood of the training data under the assumption that the canonical derivations are the true ones.", "labels": [], "entities": []}, {"text": "Other systems such as those by,, and use a variant of the EM algorithm called Inside-Outside.", "labels": [], "entities": []}, {"text": "This algorithm requires that the set of derivations fora given translation pair be representable by a WRTG.", "labels": [], "entities": [{"text": "WRTG", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.8173390626907349}]}, {"text": "In most cases, this can be computed by restricting the grammar at hand to the given translation pair, that is, by applying input and output product.", "labels": [], "entities": []}, {"text": "Note that the pair can contain strings or trees or even some combination thereof.", "labels": [], "entities": []}, {"text": "In the systems mentioned at the beginning of this section, a probability distribution of the form p(e, d | f ) is chosen from a log-linear model, where e, d, and fare an English sentence, a derivation, and a foreign sentence, respectively.", "labels": [], "entities": []}, {"text": "Such a distribution combines information from different sources, called features, such as the grammar or a probability distribution over English sentences.", "labels": [], "entities": []}, {"text": "The features are represented by real-valued functions hi (e, d, f ).", "labels": [], "entities": []}, {"text": "For said combination, each feature gets a weight \u03bb i . The feature weights are usually estimated using minimum-error-rate training.", "labels": [], "entities": []}, {"text": "For this it is necessary to compute, fora given f , the set D f of n highest ranking derivations generating f on the foreign side.", "labels": [], "entities": []}, {"text": "Roughly speaking, this set can be computed by applying the input product with f , and then applying the n-best algorithm.", "labels": [], "entities": []}, {"text": "We note that, while f is usually a string, it can in some circumstances be a phrase-structure tree, as in ().", "labels": [], "entities": []}, {"text": "The actual translation, or decoding, problem amounts to finding, fora given f , Even for the simplest grammars, this problem is NP hard (Casacuberta and de la).", "labels": [], "entities": []}, {"text": "As a result, SMT systems use approximations such as crunching or variational decoding ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9944164752960205}]}, {"text": "Here we focus on the former, which amounts to restricting the sum in the equation to the set D f . Since this set is finite, the sum is then zero for almost all e, which makes the computation of\u00eaof\u02c6of\u00ea feasible.", "labels": [], "entities": []}, {"text": "As mentioned before, the input product can be used to compute D f . As we have seen in these tasks, tree parsing is employed in recent SMT systems.", "labels": [], "entities": [{"text": "tree parsing", "start_pos": 100, "end_pos": 112, "type": "TASK", "confidence": 0.691452756524086}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9944018721580505}]}, {"text": "lists five relevant contributions in this area.", "labels": [], "entities": []}, {"text": "These contributions can be classified according to a number of characteristics indicated by the column headings.", "labels": [], "entities": []}, {"text": "One of these characteristics is the abstraction level (AL), which we categorize as follows: 1.", "labels": [], "entities": [{"text": "abstraction level (AL)", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.7021554350852967}]}, {"text": "algorithm, 4. implementation.", "labels": [], "entities": []}, {"text": "The first three entries of Tab.", "labels": [], "entities": []}, {"text": "1 deal with contributions that are restricted to tree substitution.", "labels": [], "entities": [{"text": "tree substitution", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7675553560256958}]}, {"text": "show an algorithm for computing the best derivation of the input product of an xRLNS with a singletree.", "labels": [], "entities": []}, {"text": "present an algorithm for computing the derivation WRTG for the input and output product of a treeto-tree transducer (called xRLN) with a single pair of trees.", "labels": [], "entities": []}, {"text": "describes an algorithm for computing the set of derivations for the input and output product of an STSG with a single pair of trees.", "labels": [], "entities": []}, {"text": "We note that the grammar classes covered so far are strictly less powerful than STAGs.", "labels": [], "entities": []}, {"text": "This is due to the fact that STAGs additionally permit an operation called adjoining.", "labels": [], "entities": []}, {"text": "As and point out, the adjoining operation has a well-founded linguistic motivation, and permitting it improves translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.9592586755752563}]}, {"text": "There are two papers approaching the problem of tree parsing for STAGs, given in the fourth and fifth entries of the table.", "labels": [], "entities": [{"text": "tree parsing", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.669795423746109}, {"text": "STAGs", "start_pos": 65, "end_pos": 70, "type": "TASK", "confidence": 0.7368990182876587}]}, {"text": "These papers establish closure properties, that is, their constructions yield a grammar of the same type as the original grammar.", "labels": [], "entities": []}, {"text": "Since the resulting grammars are compact representations of the derivations of the input product or output product, respectively, these constructions constitute tree parsing.", "labels": [], "entities": [{"text": "tree parsing", "start_pos": 161, "end_pos": 173, "type": "TASK", "confidence": 0.7290643453598022}]}, {"text": "Nederhof shows that weighted linear index grammars (WLIGs) are closed under weighted intersection with tree languages generated by WRTGs.", "labels": [], "entities": [{"text": "WRTGs", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.8727588653564453}]}, {"text": "WLIGs derive phrase-structure trees, and they are equivalent to tree-adjoining grammars (TAGs).", "labels": [], "entities": []}, {"text": "His construction can be extended to some kind of synchronous WLIG without problems.", "labels": [], "entities": []}, {"text": "However, synchronization interacts with the height restriction present for WLIG rules in away that makes synchronous WLIGs less powerful than STAGs.", "labels": [], "entities": []}, {"text": "Maletti (2010a) uses an alternative representation of STAG, namely as extended tree transducers (XTT) with explicit substitution.", "labels": [], "entities": [{"text": "STAG", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.8202477097511292}]}, {"text": "In this framework, adjoining is encoded into the phrasestructure trees by introducing special symbols, to be evaluated in a separate step.", "labels": [], "entities": []}, {"text": "He indicates that his representation of STAG is closed under input and output product with regular tree languages by providing a corresponding construction.", "labels": [], "entities": [{"text": "STAG", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.8078616261482239}]}, {"text": "However, in his setting, both the translations and the languages are unweighted.", "labels": [], "entities": []}, {"text": "The advantage of closure properties of the above kind is that they allow cascades of input and output products to be constructed in a uniform way, as well as applying further operations on the grammars, such as projection.", "labels": [], "entities": []}, {"text": "Ultimately, SMT tasks maybe described in this framework, as witnessed by toolboxes that exist for WFSTs and XTTs ( In this paper, we propose a weighted formulation of STAGs which is closed under input and output product with WRTGs, and we present a corresponding tree-parsing algorithm.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.9277264773845673}, {"text": "WFSTs", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.7930567264556885}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "2, we introduce our formulation of STAGs, which is called weighted synchronous tree-adjoining grammar (WSTAG).", "labels": [], "entities": []}, {"text": "The major difference with respect to the classical STAGs is twofold: (i) we use states and (ii) we encode substitution and adjoining sites as variables in the tree.", "labels": [], "entities": []}, {"text": "The states make intersection with regular properties possible (without the need for relabeling as in and).", "labels": [], "entities": []}, {"text": "In addition, they permit implementing all features of conventional STAG/STIG, such as potential adjoining and left/right adjoining.", "labels": [], "entities": [{"text": "STAG/STIG", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.5727832118670145}]}, {"text": "The variables are used for synchronization of the input and output sides.", "labels": [], "entities": []}, {"text": "3, we show that WSTAGs are closed under input and output product with tree languages generated by WRTGs (cf. Theorem 1).", "labels": [], "entities": [{"text": "WSTAGs", "start_pos": 16, "end_pos": 22, "type": "TASK", "confidence": 0.8732274770736694}, {"text": "WRTGs", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.7477239966392517}]}, {"text": "We do this by means of a direct construction (cf. Sec. 4).", "labels": [], "entities": []}, {"text": "Our construction is based on the standard technique for composing two top-down tree transducers (cf. page 195 of).", "labels": [], "entities": []}, {"text": "This technique has been extended in Theorem 4.12 of to the composition of a macro tree transducer and a top-down tree transducer (also cf.); in fact, our direct construction is very similar to the latter one.", "labels": [], "entities": []}, {"text": "Section 5 contains Algorithm 1, which computes our construction (modulo reduction).", "labels": [], "entities": []}, {"text": "It is inspired by a variant of Earley's algorithm.", "labels": [], "entities": []}, {"text": "In this way we avoid computation of a certain portion of useless rules, and we ensure that the complexity is linear in the size of the input WSTAG.", "labels": [], "entities": [{"text": "WSTAG", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.792236864566803}]}, {"text": "The algorithm is presented in the framework of deductive parsing.", "labels": [], "entities": [{"text": "deductive parsing", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7520782649517059}]}, {"text": "In Sections 6 and 7, we discuss the correctness of our algorithm and its complexity, respectively.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}