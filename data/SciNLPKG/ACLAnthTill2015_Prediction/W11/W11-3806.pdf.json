{"title": [{"text": "Multiword Expressions in Statistical Dependency Parsing", "labels": [], "entities": [{"text": "Statistical Dependency Parsing", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.687912384668986}]}], "abstractContent": [{"text": "In this paper, we investigated the impact of extracting different types of multiword expressions (MWEs) in improving the accuracy of a data-driven dependency parser fora morphologically rich language (Turkish).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9980173110961914}]}, {"text": "We showed that in the training stage, the unification of MWEs of a certain type, namely compound verb and noun formations, has a negative effect on parsing accuracy by increasing the lexical sparsity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.944342851638794}]}, {"text": "Our results gave a statistically significant improvement by using a variant of the treebank excluding this MWE type in the training stage.", "labels": [], "entities": []}, {"text": "Our extrinsic evaluation of an ideal MWE recognizer (for only extracting MWEs of type named entities, duplications, numbers, dates and some predefined list of compound prepositions) showed that the pre-processing of the test data would improve the labeled parsing accuracy by 1.5%.", "labels": [], "entities": [{"text": "MWE recognizer", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.9633348882198334}, {"text": "labeled parsing", "start_pos": 248, "end_pos": 263, "type": "TASK", "confidence": 0.44471076130867004}, {"text": "accuracy", "start_pos": 264, "end_pos": 272, "type": "METRIC", "confidence": 0.9031703472137451}]}], "introductionContent": [{"text": "Multiword expressions are compound word formations formed by two or more words.", "labels": [], "entities": []}, {"text": "They generally represent a different meaning than the words which compose them.", "labels": [], "entities": []}, {"text": "The importance of detecting multiword expressions in different NLP problems is emphasized by many researchers and is still a topic which is being investigated for different NLP layers (.", "labels": [], "entities": []}, {"text": "It is impossible to neglect the importance for machine translation where the translation of a MWE would be totally different than the translation of its constituents.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7704484462738037}]}, {"text": "But the effect of different MWE types on parsing accuracy is still an open research topic and needs to be analyzed in detail.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9803053140640259}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.912882924079895}]}, {"text": "recently give their preliminary results on detecting named-entities and reports no improvement in parsing accuracy for English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9712634086608887}]}, {"text": "reports a 5% parsing accuracy increase for Swedish by using a depedency parser which uses a memory-based learner as its oracle.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9428634643554688}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.966017484664917}]}, {"text": "In this study, they only focus on multiword names and compound function words.", "labels": [], "entities": []}, {"text": "reports very small (falling short of their initial expectations) but statistically significant improvements fora PCFG parser (on English).", "labels": [], "entities": []}, {"text": "reports an improvement of sentence accuracy 7.5% in shallow parsing by concentrating on MWE of types compound nominals, proper names and adjective noun constructions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9652260541915894}, {"text": "shallow parsing", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.659613847732544}, {"text": "MWE", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.6165221333503723}]}, {"text": "The conflicting results and the difference in success ranges maybe caused by many factors; such as the parsing paradigm, the language in focus, the MWE types used in the experiments and the evaluation metrics.", "labels": [], "entities": [{"text": "parsing", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.9629310965538025}]}, {"text": "In this study, we are making a detailed investigation of extracting different MWE classes as a preprocessing step fora statistical dependency parser (MaltParser v1.5.1).", "labels": [], "entities": []}, {"text": "We conducted our experiments on Turkish Dependency Treebanks (.", "labels": [], "entities": [{"text": "Turkish Dependency Treebanks", "start_pos": 32, "end_pos": 60, "type": "DATASET", "confidence": 0.9279125134150187}]}, {"text": "We semi-automatically created different versions of the data by manually annotating and classifying MWEs.", "labels": [], "entities": []}, {"text": "We made an in depth analysis of using the new treebank versions both on training and testing stages.", "labels": [], "entities": []}, {"text": "We evaluated the parser's performance both on MWEs and the remaining parts of the sentences.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.7891883254051208}]}, {"text": "Our results showed that different MWE types have different impacts on the parsing accuracy.", "labels": [], "entities": [{"text": "MWE", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9432322978973389}, {"text": "parsing", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.981349766254425}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9419018626213074}]}, {"text": "For Turkish, we showed that the preprocessing of compound verb and noun formations causes a considerable decrease inaccuracy.", "labels": [], "entities": []}, {"text": "We also demonstrated that a MWE extractor which finds the MWEs from the remaining types would make a significant improvement for parsing.", "labels": [], "entities": [{"text": "MWE extractor", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.8523821234703064}, {"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9857408404350281}]}, {"text": "For now, it is not possible to generalize the results for other languages and parsing paradigms.", "labels": [], "entities": [{"text": "parsing paradigms", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.9031598269939423}]}, {"text": "But we believe, we present a systematic approach for evaluating the scenario.", "labels": [], "entities": []}, {"text": "in their article \"Dependency Parsing of Turkish\" points out to a decrease of nearly 4 percentage points when they test their parser on the raw data.", "labels": [], "entities": [{"text": "Dependency Parsing of Turkish", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.8331135362386703}]}, {"text": "Although they only look at this decrease from the point of the errors caused by partsof-speech (POS) tagging, the decrease could actually be due to two reasons: 1.", "labels": [], "entities": [{"text": "partsof-speech (POS) tagging", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.5699153423309327}]}, {"text": "The errors caused by the automatic POS tagging, 2.", "labels": [], "entities": [{"text": "errors", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9811716079711914}, {"text": "POS tagging", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.6863632649183273}]}, {"text": "The lack of MWE handling which exists in the gold standard.", "labels": [], "entities": [{"text": "MWE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9192238450050354}]}, {"text": "In this study, we will focus onto the second item and on the improvement that could be reached by handling MWEs.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 107, "end_pos": 111, "type": "TASK", "confidence": 0.8730344176292419}]}, {"text": "With this purpose, we are asking and answering the following questions during the remaining of the paper:", "labels": [], "entities": []}], "datasetContent": [{"text": "We have four sets of experiments.", "labels": [], "entities": []}, {"text": "Before introducing them, we will first explain our evaluation strategy.", "labels": [], "entities": []}, {"text": "We exactly follow the evaluation metrics used in: We use ten-fold crossvalidation in the experiments on the treebank (except the experiments on the validation set).", "labels": [], "entities": []}, {"text": "We report the results as mean scores of the ten-fold crossvalidation, with standard error.", "labels": [], "entities": []}, {"text": "The main evaluation metrics that we use are the unlabeled attachment score (AS U ) and labeled attachment score (AS L ), namely, the proportion of IGs that are attached to the correct head (with the correct label for ASL ).", "labels": [], "entities": [{"text": "unlabeled attachment score (AS U )", "start_pos": 48, "end_pos": 82, "type": "METRIC", "confidence": 0.8227918488638741}, {"text": "labeled attachment score (AS L )", "start_pos": 87, "end_pos": 119, "type": "METRIC", "confidence": 0.9222128902162824}]}, {"text": "A correct attachment is one in which the dependent IG (the last IG in the dependent word) is not only attached to the correct headword but also to the correct IG within the headword.", "labels": [], "entities": []}, {"text": "We also report the (unlabeled) word-to-word score (W W U ), which only measures whether a dependent word is connected to (some IG in) the correct headword.", "labels": [], "entities": [{"text": "word-to-word score (W W U )", "start_pos": 31, "end_pos": 58, "type": "METRIC", "confidence": 0.8937778132302421}]}, {"text": "We will refer to this metric (W W U ) especially while evaluating the dependencies between MWEs' constituents since we are automatically creating these dependencies (thus automatically selecting the IG to which the dependent will be connected).", "labels": [], "entities": []}, {"text": "Where relevant, we also test the statistical significance of the results.", "labels": [], "entities": []}, {"text": "In order to seethe impact of different approaches, we are making 3 different evaluations: 3.", "labels": [], "entities": []}, {"text": "the dependencies excluding the ones with \"MWE\" labels (the surrounding structure in the sentence): AS U , ASL , WW U scores provided  In this set of experiments we first repeated the results of Eryigit at al.", "labels": [], "entities": [{"text": "AS U", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.970397025346756}, {"text": "ASL", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.8712759017944336}]}, {"text": "The fifth line of the gives the results on the enlarged version of the treebank.", "labels": [], "entities": []}, {"text": "We see that although the results are higher than Vd, they are not as good as Vo.", "labels": [], "entities": [{"text": "Vd", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.8217504620552063}]}, {"text": "From here, we understand that by collapsing some words into single units, we disappeared some of the dependencies where the parser was already very successful at finding; the average scores were getting higher by the success coming from this type of dependencies.", "labels": [], "entities": []}, {"text": "As a final step in this set of experiments, we tested our parser on version Ve-o and saw that the results are worse than the results on the detached version (Vd): Collecting the new MWE components into single units gives worse results than doing no MWE processing at all.", "labels": [], "entities": []}, {"text": "But is this just an illusion 8 or is there really a bad effect on the discovery of the remaining dependencies also?", "labels": [], "entities": []}, {"text": "Actually the results provided here is not enough for answering this question.", "labels": [], "entities": []}, {"text": "In order to seethe exact picture we should examine the results more closely (Section 5.5).", "labels": [], "entities": []}, {"text": "In this set of experiments, we are looking at the results by using the new treebank versions in the training stage as well.", "labels": [], "entities": []}, {"text": "shows that the results on the detached test set (Vd) are better when trained by Vd (76.0\u00b10.2 AS U ) rather than the original treebank Vo (74.7\u00b10.2 AS U ).", "labels": [], "entities": []}, {"text": "This means that the parser is better at find-ing the dependencies if it has samples from the same genre.", "labels": [], "entities": []}, {"text": "But again by just looking the results this way, it is not possible to understand the situation entirely.", "labels": [], "entities": []}, {"text": "In order to be able to understand what is happening on the dependencies within MWEs and their surrounding structures, we are evaluating the results on 3 different ways described in Section 5.2.", "labels": [], "entities": []}, {"text": "gives the scores in all of thees three evaluation types; The first column block states each training and testing combination together with the number of combined MWEs and the total number of dependencies in the test sets.", "labels": [], "entities": []}, {"text": "The second column block gives the overall results of the parser.", "labels": [], "entities": []}, {"text": "The fourth column block lists the results obtained on the dependencies (with MWE label) occurring between the constituent words which appeared after the detachment of the MWEs annotated on the original treebank.", "labels": [], "entities": [{"text": "MWE label) occurring between the constituent words which appeared after the detachment of the MWEs annotated on the original treebank", "start_pos": 77, "end_pos": 210, "type": "Description", "confidence": 0.7605656995659783}]}, {"text": "The third column block gives the results obtained on the remaining dependencies (excluding the ones with MWE label).", "labels": [], "entities": []}, {"text": "Example: The number of MWE labeled dependencies is 0 for the Vo and 2427 for Vd meaning that the detachment of 2040 combined MWEs (two or more words) on Vo resulted to 2427 dependencies.", "labels": [], "entities": []}, {"text": "Thus, the average number of dependency per MWE is 1.19.", "labels": [], "entities": []}, {"text": "From the, we may now analyze the parser's performance in detail; We observe that the parser's performance (trained on the original treebank) drops significantly when it is tested on the detached version both on the overall results (76.1 \u2192 74.7 AS U ) and excluding MWEs (76.1 \u2192 75.9 AS U , 83.0 \u2192 82.8 WW U the difference is small but statistically significant (McNemar p<0.01)).", "labels": [], "entities": []}, {"text": "We see that the tests on Ve-o not only causes a decrease on the overall accuracy (74.7 \u2192 74.0 AS U ) but also a decrease on the dependencies excluding MWEs (75.9 \u2192 74.7 AS U ).", "labels": [], "entities": [{"text": "Ve-o", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9058392643928528}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9985517859458923}, {"text": "AS U )", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9564345081647238}]}, {"text": "Thus, we may say that the combination of MWEs listed in the dictionary has a harming effect on the determination of other syntactic structures as well.", "labels": [], "entities": []}, {"text": "On the other side, we observe that the combination of MWEs annotated in the original treebank has a positive effect on the determination of other syntactic structures (from Vd to Vo the AS U differs 75.9 \u2192 76.1 but from Vd to Ve-o the AS U differs 75.9 \u2192 74.7).", "labels": [], "entities": [{"text": "AS U", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9384084045886993}, {"text": "AS U", "start_pos": 235, "end_pos": 239, "type": "METRIC", "confidence": 0.9088681638240814}]}, {"text": "These results bring the question: \"What is the difference between the dictionary MWE list and the treebank MWE list?\"", "labels": [], "entities": []}, {"text": "In order to answer this question, we manually classified the MWEs in the Turkish treebank into six categories which are listed in.", "labels": [], "entities": [{"text": "Turkish treebank", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.8870184123516083}]}, {"text": "The second column in the table lists the number of MWEs in each categories, the third column lists the number of dependencies when we detach these MWEs and the fourth column gives the WW U scores on the dependencies from these specific MWE categories.", "labels": [], "entities": [{"text": "WW U scores", "start_pos": 184, "end_pos": 195, "type": "METRIC", "confidence": 0.7649509509404501}]}, {"text": "We only look at the WW U scores since the IG-based links are created automatically and WW U scores is considered to be more informative.", "labels": [], "entities": [{"text": "WW U scores", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.6844327251116434}]}, {"text": "The results are obtained with a parsing model trained with Version Vd.", "labels": [], "entities": []}, {"text": "(it is obvious that a model trained with Vo won't be able to find most of these dependencies because of the lack of samples.)", "labels": [], "entities": []}, {"text": "We see from the, the parser is very bad (5.6 WW U ) at determining the compound function words (which are very rare)) and duplications 9 (66.5 WW U ).", "labels": [], "entities": []}, {"text": "These dependencies could actually be easily discovered by a rule-based extractor.", "labels": [], "entities": []}, {"text": "The success on named entities (83.7) and number expressions (82.1) could be considered as good but one shouldn't forget that the training and testing data is from the same treebank and the sentences could actually not be considered as random.", "labels": [], "entities": []}, {"text": "Thus on a totally unseen data, these results would be lower.", "labels": [], "entities": []}, {"text": "But again we believe a rule-based extractor for numerical and date expressions could be developed with  To alleviate the problem observed in the labeled accuracy, instead of assigning the new \"MWE\" label to the detached MWEs, we developped a rule based dependency label chooser which assigns an appropriate label to these dependencies obeying the Turkish Treebank annotation approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.8229511976242065}, {"text": "Turkish Treebank annotation", "start_pos": 347, "end_pos": 374, "type": "DATASET", "confidence": 0.9639572699864706}]}, {"text": "We have 16 rules similar to the following one: if DEPENDENCY LABEL eq 'MWE'{ if HEAD's POSTAG eq 'Verb' && DEPENDENT's POSTAG eq 'Adverb' then change MWE\u2192 MODIFIER } We changed the MWE labels in S1 and Vd by using this rule based dependency label chooser.", "labels": [], "entities": []}, {"text": "gives the results at the end of this operation.", "labels": [], "entities": []}, {"text": "The results are as we expected.", "labels": [], "entities": []}, {"text": "In the training stage, if we use our new version of the treebank (S1*) (where we detached the MWEs of type compound noun and verb formations) instead of the original one, the results on raw data (Vd*) (2nd and 4th lines of) became significantly better (AS U and ASL difference is statistically signif-icant with McNemar (p < 0.01)).", "labels": [], "entities": []}, {"text": "We also validated this outcome by testing on the validation set.", "labels": [], "entities": []}, {"text": "gives the results on both the original version of the validation set (ITU-Vo) and the detached version (ITU-Vd*).", "labels": [], "entities": [{"text": "ITU-Vo", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.9175484776496887}, {"text": "ITU-Vd", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.9247711896896362}]}, {"text": "Although the small number of available MWEs (only 89), we observe an improvement by using the model trained with S1*.", "labels": [], "entities": []}, {"text": "For the first two lines the improvement is rather small but statistically significant on labeled accuracy with Mcnemar(p<0.05  Another outcome that could be observed from Table 7 (by comparing the first and third lines of results) is that a MWE extractor for only MWEs of types named entities, duplications, numbers, dates and some predefined list of compound prepositions would be enough for obtaining the results of the gold-standard treebank (there is no statistically significant results between these two lines).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9214925169944763}, {"text": "Mcnemar", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.6698126792907715}]}, {"text": "As a final comment, we may conclude that the preprocessing of the test data would improve the results by nearly 1.5 in IG-based evaluations (74.7\u219276.1, 66.1\u219267.6) and 1.1 (81.8\u219282.9) in word-based evaluation (2nd and 3rd lines of) if we also train with S1* instead of the original treebank.", "labels": [], "entities": []}, {"text": "One should remember that there are in total 1324 fewer dependencies) in S1 compared to Vd.", "labels": [], "entities": []}, {"text": "These dependencies are expected to be discovered by the MWE extractor.", "labels": [], "entities": [{"text": "MWE extractor", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.7229807674884796}]}], "tableCaptions": [{"text": " Table 1: Data sets' statistics: Version (Vo -Version Original; Vd -Version detached; Ve -Version enlarged; Ve-o - Version enlarged excluding the combined collocations of Vo )", "labels": [], "entities": []}, {"text": " Table 2: Performance of MWE Extractors: Ofl.(2004):the  MWE processor of Oflazer et al. (2004);Mod.x:Model x", "labels": [], "entities": [{"text": "Ofl.(2004)", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.9153670221567154}, {"text": "Oflazer et al. (2004)", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.9161263534000942}]}, {"text": " Table 3: The parser's performance trained on the original  treebank (Vo)", "labels": [], "entities": []}, {"text": " Table 4: Parser's performance by training and testing  with the different versions of the treebank", "labels": [], "entities": []}, {"text": " Table 6: Parser's success on special MWE types:  Named Entities (Named ent.), Numerical Expressions  (Num.exp.), Compound function words (Comp.func.),  Duplications (Dup), Compound verb and noun forma- tions (Comp.vn.)", "labels": [], "entities": []}, {"text": " Table 5: Overall Parsing Results (on and outside MWEs) with different treebank versions:  n/c:no change with the previous results on the left column block (overall results); n/a:not available", "labels": [], "entities": []}, {"text": " Table 7: Parsing results with MWE labels replaced by the  label chooser", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9607135653495789}]}, {"text": " Table 8: Results on Validation Set", "labels": [], "entities": [{"text": "Validation", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.96038419008255}]}]}