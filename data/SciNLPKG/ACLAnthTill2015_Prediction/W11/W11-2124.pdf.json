{"title": [{"text": "Wider Context by Using Bilingual Language Models in Machine Translation", "labels": [], "entities": [{"text": "Wider Context", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7720657885074615}, {"text": "Machine Translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7519740760326385}]}], "abstractContent": [{"text": "In past Evaluations for Machine Translation of European Languages, it could be shown that the translation performance of SMT systems can be increased by integrating a bilingual language model into a phrase-based SMT system.", "labels": [], "entities": [{"text": "Machine Translation of European Languages", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.8470953822135925}, {"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9917171597480774}, {"text": "SMT", "start_pos": 212, "end_pos": 215, "type": "TASK", "confidence": 0.8088947534561157}]}, {"text": "In the bilingual language model, target words with their aligned source words build the tokens of an n-gram based language model.", "labels": [], "entities": []}, {"text": "We analyzed the effect of bilingual language models and show where they could help to better model the translation process.", "labels": [], "entities": [{"text": "translation process", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.9063903093338013}]}, {"text": "We could show improvements of translation quality on German-to-English and Arabic-to-English.", "labels": [], "entities": []}, {"text": "In addition, for the Arabic-to-English task, training an extra bilingual language model on the POS tags instead of the surface word forms led to further improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many state-of-the art SMT systems, the phrasebased ( approach is used.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9926901459693909}]}, {"text": "In this approach, instead of building the translation by translating word byword, sequences of source and target words, so-called phrase pairs, are used as the basic translation unit.", "labels": [], "entities": []}, {"text": "A table of correspondences between source and target phrases forms the translation model in this approach.", "labels": [], "entities": []}, {"text": "Target language fluency is modeled by a language model storing monolingual n-gram occurrences.", "labels": [], "entities": []}, {"text": "A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses.", "labels": [], "entities": []}, {"text": "Then the decoder searches for the translation with the highest score.", "labels": [], "entities": []}, {"text": "A different approach to SMT is to use a stochastic finite state transducer based on bilingual ngrams ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9957596659660339}]}, {"text": "This approach was for example successfully applied by on the French-English translation task.", "labels": [], "entities": [{"text": "French-English translation task", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.6992483536402384}]}, {"text": "In this so-called n-gram approach the translation model is trained by using an n-gram language model of pairs of source and target words, called tuples.", "labels": [], "entities": []}, {"text": "While the phrase-based approach captures only bilingual context within the phrase pairs, in the n-gram approach the n-gram model trained on the tuples is used to capture bilingual context between the tuples.", "labels": [], "entities": []}, {"text": "As in the phrase-based approach, the translation model can also be combined with additional models like, for example, language models using log-linear combination.", "labels": [], "entities": []}, {"text": "Inspired by the n-gram-based approach, we introduce a bilingual language model that extends the translation model of the phrase-based SMT approach by providing bilingual word context.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.7805382609367371}]}, {"text": "In addition to the bilingual word context, this approach enables us also to integrate a bilingual context based on part of speech (POS) into the translation model.", "labels": [], "entities": []}, {"text": "When using phrase pairs it is complicated to use different kinds of bilingual contexts, since the context of the POS-based phrase pairs should be bigger than the word-based ones to make the most use of them.", "labels": [], "entities": []}, {"text": "But there is no straightforward way to integrate phrase pairs of different lengths into the translation model in the phrase-based approach, while it is quite easy to use n-gram models with different context lengths on the tuples.", "labels": [], "entities": []}, {"text": "We show how we can use bilingual POS-based language models to capture longer bilingual context in phrase-based translation systems.", "labels": [], "entities": []}, {"text": "This paper is structured in the following way: In the next section, we will present some related work.", "labels": [], "entities": []}, {"text": "Afterwards, in Section 3, a motivation for using the bilingual language model will be given.", "labels": [], "entities": []}, {"text": "In the following section the bilingual language model is described in detail.", "labels": [], "entities": []}, {"text": "In Section 5, the results and an analysis of the translation results is given, followed by a conclusion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: German-to-English results (Test 1)", "labels": [], "entities": []}, {"text": " Table 3: German-to-English results (Test 2)", "labels": [], "entities": []}, {"text": " Table 4: Different N-Gram Lengths (Test 1)", "labels": [], "entities": []}, {"text": " Table 5: Different N-Gram Lengths (Test 2)", "labels": [], "entities": []}, {"text": " Table 6: Results on Arabic to English: Translation of  News", "labels": [], "entities": [{"text": "Translation of  News", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8878954450289408}]}, {"text": " Table 7: Results on Arabic to English: Translation of  Web documents", "labels": [], "entities": [{"text": "Translation of  Web", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8599988420804342}]}, {"text": " Table 8: Bilingual Context in Arabic-to-English results  (News)", "labels": [], "entities": []}, {"text": " Table 9: Bilingual Context in Arabic-to-English results  (Web data)", "labels": [], "entities": []}, {"text": " Table 10. The performance of com- petitive systems could be improved in all three lan- guages by up to 0.4 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9990480542182922}]}, {"text": " Table 10: Preformance of Bilingual language model at  WMT2011", "labels": [], "entities": [{"text": "WMT2011", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.8942091464996338}]}]}