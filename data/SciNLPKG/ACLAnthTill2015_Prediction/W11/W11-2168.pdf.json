{"title": [{"text": "From n-gram-based to CRF-based Translation Models", "labels": [], "entities": [{"text": "CRF-based Translation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.5565325766801834}]}], "abstractContent": [{"text": "A major weakness of extant statistical machine translation (SMT) systems is their lack of a proper training procedure.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7933696111043295}]}, {"text": "Phrase extraction and scoring processes rely on a chain of crude heuristics, a situation judged problematic by many.", "labels": [], "entities": [{"text": "Phrase extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9792592227458954}]}, {"text": "In this paper, we recast the machine translation problem in the familiar terms of a sequence labeling task, thereby enabling the use of enriched feature sets and exact training and inference procedures.", "labels": [], "entities": [{"text": "machine translation problem", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.8427279591560364}, {"text": "sequence labeling task", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7513787349065145}]}, {"text": "The tractabil-ity of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library.", "labels": [], "entities": []}, {"text": "This approach is experimentally contrasted with several conventional phrase-based systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8427731394767761}]}, {"text": "Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. ()) have thus far failed to overcome the related combinatorial problems and/or to yield improved training heuristics ().", "labels": [], "entities": [{"text": "phrase-to-phrase alignments", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6886714845895767}]}, {"text": "Phrase extraction and scoring thus rely on a chain of heuristics see (, which evolve phrase alignments from \"symmetrized\" word-toword alignments obtained with IBM models) and the like (;.", "labels": [], "entities": [{"text": "Phrase extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9611584544181824}]}, {"text": "Phrase scoring is also mostly heuristic and relies on an optimized interpolation of several simple frequencybased scores.", "labels": [], "entities": [{"text": "Phrase scoring", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9647626578807831}]}, {"text": "Overall, the training procedure of translation models within conventional phrase-based (or hierarchical) systems is generally considered unsatisfactory and the design of better estimation procedures remains an active research area).", "labels": [], "entities": []}, {"text": "To overcome the NP-hard problems that derive from the need to consider all possible permutations of the source sentence, we make here a radical simplification and consider training the translation model given a fixed segmentation and reordering.", "labels": [], "entities": []}, {"text": "This idea is not new, and is one of the grounding principle of n-gram-based approaches () in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9935550093650818}]}, {"text": "The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task.", "labels": [], "entities": [{"text": "recast machine translation (MT)", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7380499045054117}, {"text": "sequence labeling task", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.686277041832606}]}, {"text": "This reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs ().", "labels": [], "entities": []}, {"text": "It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model.", "labels": [], "entities": []}, {"text": "In return, in addition to having a better trained model, we also expect (i) to make estimation less sensible to data sparsity issues and (ii) to improve the ability of our system to make the correct lexical choices based on the neighboring source words.", "labels": [], "entities": []}, {"text": "As explained in Section 2, this reformulation borrows much from the general architecture of n-gram MT systems and implies to solve several computational challenges.", "labels": [], "entities": [{"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.939892590045929}]}, {"text": "In our ap-proach, the tractability of the whole enterprise is achieved through an efficient reimplementation of CRFs using a public domain library for weighted finite-state transducers (WFSTs) (see details in Section 3).", "labels": [], "entities": []}, {"text": "This approach is experimentally contrasted with more conventional n-gram based and phrasebased approaches on a standard benchmark in Section 4, where we also evaluate the benefits of various feature sets and training regimes.", "labels": [], "entities": []}, {"text": "We finally relate our new system with alternative proposals for training discriminatively SMT systems in Section 5, before drawing some lessons and discussing possible extensions of this work.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.937214732170105}]}, {"text": "The main contribution of this work are thus (i) a detailed presentation of the CRF in translation including all necessary implementation details and (ii) an experimental study of various feature functions and of various ways to integrate target side LM information.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training time The first lesson learned is that training can be performed efficiently.", "labels": [], "entities": []}, {"text": "Our baseline system, which only contains trs and trg contains approximately 87 million features, out of which a little bit more than 600K are selected.", "labels": [], "entities": []}, {"text": "Adding up all supplementary features raises the number of parameters to about 130M features, out of which 1.5M are found useful.", "labels": [], "entities": []}, {"text": "All these systems require between 3 and 5 hours to train 9 . These numbers are obtained with a 1 penalty term \u2248 1, which offers a good balance between accuracy and sparsity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9993528723716736}]}, {"text": "Test conditions In order to better assess the strengths and weaknesses of our approach, we compare several test settings: the most favorable considers only one possible segmentation/reordering\u02dcfreordering\u02dcreordering\u02dcf for each f , obtained through forced alignment with the reference; we then consider the more challenging case where the reordering is fixed, but several segmentations are considered; then the regular decoding task, where both segmentation and reordering are unknown and where the entire space of all segmentations and reordering is searched.", "labels": [], "entities": []}, {"text": "For each condition, we also vary (i) the set of features used and (ii) the target language model used, if any.", "labels": [], "entities": []}, {"text": "Wherever applicable, we also report contrasts with n-gram-based systems subject to the same input and comparable resources, varying the order of the tuple language model, as well as with Moses.", "labels": [], "entities": []}, {"text": "Results are in  Extending the feature set As expected, the use of increasingly complex feature sets seems beneficial in all experimented conditions.", "labels": [], "entities": [{"text": "Extending", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.9409700632095337}]}, {"text": "It is noteworthy that throwing in reordering and contextual features is helping, even when decoding one single segmentation and reordering.", "labels": [], "entities": []}, {"text": "This is because these features do not help to select the best input reordering, but help choose the best target phrase.", "labels": [], "entities": []}, {"text": "Searching a larger space Going from the simpler to the more difficult conditions yields significant degradations in the model, as our best score drops down from 25.6 to 23.5 (with known reordering) then to 19.1 (regular decoding).", "labels": [], "entities": []}, {"text": "This is a clear indication that our current segmentation/reordering model is not delivering very useful scores.", "labels": [], "entities": [{"text": "segmentation/reordering", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.9039092063903809}]}, {"text": "A similar loss is incurred by the n-gram system, which loses 4 bleu points between the two conditions.", "labels": [], "entities": []}, {"text": "LM rescoring Our results to date with target side language models have proven inconclusive, which might explain why our best results remain between one and two BLEU points behind the n-gram based system using comparable information.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9986181259155273}]}, {"text": "Note also that preliminary experiments with incorporating a large bigram during training have also failed to date to provide us with improvements over the baseline.", "labels": [], "entities": []}, {"text": "Summary In sum, the results accumulated during this first round of experiments tend to show that our CRF model is still underperforming the more established baseline by approximately 1 to 1.5 BLEU point, when provided with comparable resources.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9983536005020142}]}, {"text": "Sources of improvements that have been clearly identified is the scoring of reordering and segmentations, and the use of a target language model in training and/or decoding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora used for the experiments", "labels": [], "entities": []}]}