{"title": [{"text": "Whitepaper of NEWS 2011 Shared Task on Machine Transliteration *", "labels": [], "entities": [{"text": "Whitepaper of NEWS 2011 Shared Task", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.88101593653361}]}], "abstractContent": [{"text": "Transliteration is defined as phonetic translation of names across languages.", "labels": [], "entities": [{"text": "phonetic translation of names across languages", "start_pos": 30, "end_pos": 76, "type": "TASK", "confidence": 0.8364028831322988}]}, {"text": "Transliteration of Named Entities (NEs) is necessary in many applications, such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition.", "labels": [], "entities": [{"text": "Transliteration of Named Entities (NEs)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7704070806503296}, {"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8142935037612915}, {"text": "corpus alignment", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.792730987071991}, {"text": "cross-language IR", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.6132356524467468}, {"text": "information extraction", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.851386308670044}, {"text": "automatic lexicon acquisition", "start_pos": 168, "end_pos": 197, "type": "TASK", "confidence": 0.6447758177916209}]}, {"text": "All such systems call for high-performance transliteration, which is the focus of shared task in the NEWS 2011 workshop.", "labels": [], "entities": [{"text": "NEWS 2011 workshop", "start_pos": 101, "end_pos": 119, "type": "DATASET", "confidence": 0.8632699847221375}]}, {"text": "The objective of the shared task is to promote machine transliteration research by providing a common benchmarking platform for the community to evaluate the state-of-the-art technologies.", "labels": [], "entities": [{"text": "machine transliteration research", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.8445916970570883}]}], "introductionContent": [], "datasetContent": [{"text": "We plan to measure the quality of the transliteration task using the following 4 metrics.", "labels": [], "entities": []}, {"text": "We accept up to 10 output candidates in a ranked list for each input entry.", "labels": [], "entities": []}, {"text": "Since a given source name may have multiple correct target transliterations, all these alternatives are treated equally in the evaluation.", "labels": [], "entities": []}, {"text": "That is, any of these alternatives are considered as a correct transliteration, and the first correct transliteration in the ranked list is accepted as a correct hit.", "labels": [], "entities": []}, {"text": "The following notation is further assumed: N : Total number of names (source words) in the test set n i : Number of reference transliterations for i-th name in the test set (n i \u2265 1) r i,j : j-th reference transliteration for i-th name in the test set c i,k : k-th candidate transliteration (system output) for i-th name in the test set (1 \u2264 k \u2264 10) K i : Number of candidate transliterations produced by a transliteration system 1.", "labels": [], "entities": []}, {"text": "Word Accuracy in Top-1 (ACC) Also known as Word Error Rate, it measures correctness of the first transliteration candidate in the candidate list produced by a transliteration system.", "labels": [], "entities": [{"text": "Word Accuracy in Top-1 (ACC)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.8011697445596967}, {"text": "Word Error Rate", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.7143941223621368}]}, {"text": "ACC = 1 means that all top candidates are correct transliterations i.e. they match one of the references, and ACC = 0 means that none of the top candidates are correct.", "labels": [], "entities": [{"text": "ACC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9303718209266663}, {"text": "ACC = 0", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9591849247614542}]}, {"text": "2. Fuzziness in Top-1 (Mean F-score) The mean F-score measures how different, on average, the top transliteration candidate is from its closest reference.", "labels": [], "entities": [{"text": "Fuzziness", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9588280320167542}, {"text": "Mean F-score)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.8145806590716044}, {"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9304599165916443}]}, {"text": "F-score for each source word is a function of Precision and Recall and equals 1 when the top candidate matches one of the references, and 0 when there are no common characters between the candidate and any of the references.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9893835186958313}, {"text": "Precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9984169006347656}, {"text": "Recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.993984043598175}]}, {"text": "Precision and Recall are calculated based on the length of the Longest Common Subsequence between a candidate and a reference: where ED is the edit distance and |x| is the length of x.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9911085367202759}, {"text": "Recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.995324969291687}, {"text": "ED", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9869146943092346}]}, {"text": "For example, the longest common subsequence between \"abcd\" and \"afcde\" is \"acd\" and its length is 3.", "labels": [], "entities": []}, {"text": "The best matching reference, that is, the reference for which the edit distance has the minimum, is taken for calculation.", "labels": [], "entities": []}, {"text": "If the best matching reference is given by then Recall, Precision and F-score for i-th word are calculated as \u2022 The length is computed in distinct Unicode characters.", "labels": [], "entities": [{"text": "Recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9984847903251648}, {"text": "Precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9993471503257751}, {"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9983808994293213}]}, {"text": "\u2022 No distinction is made on different character types of a language (e.g., vowel vs. consonants vs. combining diereses etc.)", "labels": [], "entities": []}, {"text": "3. Mean Reciprocal Rank (MRR) Measures traditional MRR for any right answer produced by the system, from among the candidates.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 3, "end_pos": 29, "type": "METRIC", "confidence": 0.96321702003479}, {"text": "MRR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.7460817694664001}]}, {"text": "1/M RR tells approximately the average rank of the correct transliteration.", "labels": [], "entities": [{"text": "RR", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7214033007621765}]}, {"text": "MRR closer to 1 implies that the correct answer is mostly produced close to the top of the n-best lists.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9521684646606445}]}, {"text": "4. MAP ref Measures tightly the precision in the n-best candidates for i-th source name, for which reference transliterations are available.", "labels": [], "entities": [{"text": "MAP", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.8660256862640381}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9992367029190063}]}, {"text": "If all of the references are produced, then the MAP is 1.", "labels": [], "entities": [{"text": "MAP", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9895591735839844}]}, {"text": "Let's denote the number of correct candidates for the i-th source word in k-best list as num(i, k).", "labels": [], "entities": []}, {"text": "MAP ref is then given by \u2022 File formats: All data will be made available in XML formats ().", "labels": [], "entities": []}, {"text": "\u2022 Data Encoding Formats: The data will be in Unicode UTF-8 encoding files without byte-order mark, and in the XML format specified.", "labels": [], "entities": []}], "tableCaptions": []}