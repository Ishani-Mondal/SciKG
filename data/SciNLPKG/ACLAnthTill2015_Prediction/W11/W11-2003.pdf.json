{"title": [{"text": "Which System Differences Matter? Using 1 // 2 Regularization to Compare Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate how to jointly explain the performance and behavioral differences of two spoken dialogue systems.", "labels": [], "entities": []}, {"text": "The Join Evaluation and Differences Identification (JEDI), finds differences between systems relevant to performance by formulating the problem as a multi-task feature selection question.", "labels": [], "entities": [{"text": "Join Evaluation and Differences Identification (JEDI)", "start_pos": 4, "end_pos": 57, "type": "TASK", "confidence": 0.7565429583191872}]}, {"text": "JEDI provides evidence on the usefulness of a recent method, 1 // p-regularized regression (Obozinski et al., 2007).", "labels": [], "entities": [{"text": "JEDI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9649872183799744}]}, {"text": "We evaluate against manually annotated success criteria from real users interacting with five different spoken user interfaces that give bus schedule information.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses the problem of how to determine which differences between two versions of a system affect their behavior.", "labels": [], "entities": []}, {"text": "Researchers in Spoken Dialogue Systems (SDSs) can be perplexed as to which of the differences between alternative systems affect performance metrics (.", "labels": [], "entities": []}, {"text": "For example, when testing on real users at different periods of time, the variance of the performance metrics might be higher than the difference between systems, causing (i) significantly different scores in identical systems deployed at different times, and (ii) the same score on different systems.", "labels": [], "entities": []}, {"text": "We approach the problem of finding which system differences matter by describing dialogues as feature vectors constructed from the logs of dialogs generated by the SDSs interacting with real users.", "labels": [], "entities": []}, {"text": "Hence, we aim to identify features that jointly characterize the system differences and the performance of the SDS being evaluated.", "labels": [], "entities": []}, {"text": "These features should be able to (i) predict a performance metric and (ii) distinguish between the two SDS being evaluated.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is a novel algorithm for detecting differences between two systems that can explain performance.", "labels": [], "entities": []}, {"text": "Additionally, we provide details on how to implement state-of-the-art multi-task learning for SDSs.", "labels": [], "entities": [{"text": "SDSs", "start_pos": 94, "end_pos": 98, "type": "TASK", "confidence": 0.9585146903991699}]}, {"text": "The rest of this manuscript is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews multi-task feature selection.", "labels": [], "entities": [{"text": "multi-task feature selection", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6772753596305847}]}, {"text": "Section 3 describes two algorithms to find which system differences matter.", "labels": [], "entities": []}, {"text": "Section 4 describes the specific SDS used to illustrate our algorithms.", "labels": [], "entities": []}, {"text": "Section 5 presents some experimental results.", "labels": [], "entities": []}, {"text": "Section 6 reviews related prior work.", "labels": [], "entities": []}, {"text": "Section 7 presents some concluding remarks and future work.", "labels": [], "entities": []}, {"text": "Appendix A provides implementation details of the multi-task learning approach we used.", "labels": [], "entities": []}], "datasetContent": [{"text": "We assess the performance of our algorithms by evaluating the classification accuracy using the features selected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9254510998725891}]}, {"text": "To facilitate assessment of SDS, we only consider models that select up to 15 features.", "labels": [], "entities": [{"text": "SDS", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9720582365989685}]}, {"text": "reports mean classification accuracy using five-fold cross-validation.", "labels": [], "entities": [{"text": "classification", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9523432850837708}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9750118255615234}]}, {"text": "Its first column describes how well the features selected perform on detecting system differences, and the second column describes how well they predict task success as a performance metric.", "labels": [], "entities": [{"text": "detecting system differences", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.8861898382504781}]}, {"text": "We compare JEDI and SERENA against the following approaches: \u2022 Majority classifier baseline.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.6927093267440796}, {"text": "SERENA", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8455055356025696}]}, {"text": "A classifier that always selects the majority class (datasets B and C are not balanced in the number of successful dialogues).", "labels": [], "entities": []}, {"text": "\u2022 Same Task Classifier We report the classification accuracy of the model trained and tested on the same task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8637132048606873}]}, {"text": "Features are selected using an 1 penalty, and the coefficients are estimated with 2 -regularized logistic regression.", "labels": [], "entities": []}, {"text": "For example, in the column of the left, SERENA uses the most predictive features of system differences to predict success, while the same task classifier uses them to predict system differences.", "labels": [], "entities": [{"text": "SERENA", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.5209094882011414}]}, {"text": "The same task classifier does not answer \"which system differences matter\", it is just an interesting benchmark.", "labels": [], "entities": []}, {"text": "We used a one-sample t-test to check for statistically significant differences against the classification accuracy of the majority classifier baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9342483282089233}]}, {"text": "We used a paired-sample t-test to check for significant differences in classification accuracy between classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.8936073780059814}]}, {"text": "Paired samples have the same \u03bb hyper-parameter, which was described in the risk- This hyper-parameter is related to the number of features selected -as \u03bb increases, the number of features selected decreases.", "labels": [], "entities": []}, {"text": "We use 5% as the significance level at which to reject the null hypothesis.", "labels": [], "entities": [{"text": "significance level", "start_pos": 17, "end_pos": 35, "type": "METRIC", "confidence": 0.9663142263889313}]}, {"text": "When checking for statistical differences, we tested on the range of \u03bbs computed 2 . First we investigate the performance of the simpler algorithm SERENA.", "labels": [], "entities": [{"text": "SERENA", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.8213775157928467}]}, {"text": "For Dataset A, SERENA does not yield significant differences over the majority classifier baseline.", "labels": [], "entities": [{"text": "SERENA", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.971875786781311}]}, {"text": "For Dataset B, SERENA is significantly better than the majority classifier in predicting system differences, but is significantly worse for predicting success.", "labels": [], "entities": [{"text": "SERENA", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9975987076759338}, {"text": "predicting", "start_pos": 140, "end_pos": 150, "type": "TASK", "confidence": 0.9561113119125366}]}, {"text": "This means that the order in which we choose the tasks in SERENA affects its performance.", "labels": [], "entities": [{"text": "SERENA", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.45171409845352173}]}, {"text": "SERENA performs significantly worse in the Control Set C. We conclude that SER-2 \u03bb = {100, 30, 25, 20, 19, 18, . .", "labels": [], "entities": [{"text": "SERENA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9198307394981384}, {"text": "SER-2 \u03bb", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9369142055511475}]}, {"text": ", 1, 0.5, 0.25, 0.1}  ENA is not very reliable in predicting which system differences matter.", "labels": [], "entities": [{"text": "ENA", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.44648900628089905}]}, {"text": "We now discuss how well JEDI is able to fill-in for the deficiencies of SERENA.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9388995170593262}, {"text": "SERENA", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.361152321100235}]}, {"text": "As an \"upper-bound\", we will compare it to a classifier trained and tested in the same task.", "labels": [], "entities": []}, {"text": "This classifier significantly dominates over the majority baseline, even for the the Control Set C, where there were no changes in the SDS.", "labels": [], "entities": []}, {"text": "This suggests that the classifier might be picking upon seasonal differences.", "labels": [], "entities": []}, {"text": "For Set A, JEDI performs significantly better than the majority classifier and than SERENA.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.7272741198539734}]}, {"text": "For Set B, there are no significant differences between the upper-bound classifier and JEDI when predicting for changes in the SDS.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.5896174907684326}]}, {"text": "Again, JEDI dominates over SERENA and the majority baseline.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8687149882316589}, {"text": "SERENA", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9300631880760193}]}, {"text": "For the Control Set C, JEDI is not statistically different from the majority baseline.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.7148088812828064}]}, {"text": "This is the expected behavior, since the difference in performance cannot be explained by the differences between the SDS.", "labels": [], "entities": []}, {"text": "We hypothesize that the classification accuracy of JEDI could be used as a distance function between SDS: The closer the accuracy of distinguishing SDS is to 50%, the more similar the SDSs are.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.674189031124115}, {"text": "JEDI", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.872063398361206}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.998414158821106}]}, {"text": "Conversely, when JEDI is able to classify system differences closer to 100%, it is because the SDSs are more different.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.7677192091941833}]}, {"text": "describe the features selected for Sets A and B respectively.", "labels": [], "entities": []}, {"text": "The numbers indicate in how many folds the feature was selected by JEDI and by classifiers trained to predict Success and SDS differences using five-fold cross validation.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9268664717674255}, {"text": "Success", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.988961398601532}]}, {"text": "The \u03bb used is selected to contain the closest to five features (ties are resolved randomly).", "labels": [], "entities": []}, {"text": "We only report features that appeared in at least three folds.", "labels": [], "entities": []}, {"text": "In Dataset A we see that time of day is selected to predict dialogue success.", "labels": [], "entities": []}, {"text": "Anecdotally, we have noticed that many users during weekend nights appear to be intoxicated when calling the system.", "labels": [], "entities": []}, {"text": "JEDI does not select \"is weekend night\" as a feature, because it has little predictive power to detect system differences.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9640181064605713}]}, {"text": "In Dataset A, JEDI selects a speech recognition feature (the token \"Forbes St\" was recognized), and an end-pointing feature.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7170785963535309}, {"text": "Forbes St", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.8998408019542694}]}, {"text": "Since in Dataset A, the difference between systems correspond to a different acoustic model, these features make sense intuitively.", "labels": [], "entities": []}, {"text": "In Dataset B, JEDI detected that the features most predictive with system differences and success are percentage of failed prompts and the length of the turn.", "labels": [], "entities": [{"text": "JEDI", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.3817432224750519}, {"text": "length", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9568715691566467}]}, {"text": "The models for both systems make sense after the fact.", "labels": [], "entities": []}, {"text": "However, neither model was known beforehand, nor did we know which of many features considered would turnout to be informative.", "labels": [], "entities": []}, {"text": "Anecdotally, the documentation of the history of changes of Let's Go! is maintained manually.", "labels": [], "entities": [{"text": "Let's Go!", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.6561241373419762}]}, {"text": "Sometimes, because of human error, this history is incomplete.", "labels": [], "entities": []}, {"text": "The ability of JEDI to identify system differences has been able to help completing the history of changes ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Features selected in Dataset A", "labels": [], "entities": []}, {"text": " Table 4: Features selected in Dataset B", "labels": [], "entities": []}]}