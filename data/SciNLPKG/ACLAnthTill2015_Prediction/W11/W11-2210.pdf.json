{"title": [{"text": "Unsupervised Mining of Lexical Variants from Noisy Text", "labels": [], "entities": [{"text": "Unsupervised Mining of Lexical Variants from Noisy Text", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.7932988852262497}]}], "abstractContent": [{"text": "The amount of data produced in user-generated content continues to grow at a staggering rate.", "labels": [], "entities": []}, {"text": "However, the text found in these media can deviate wildly from the standard rules of orthography, syntax and even semantics and present significant problems to downstream applications which make use of this noisy data.", "labels": [], "entities": []}, {"text": "In this paper we present a novel unsupervised method for extracting domain-specific lexical variants given a large volume of text.", "labels": [], "entities": []}, {"text": "We demonstrate the utility of this method by applying it to normalize text messages found in the online social media service, Twitter, into their most likely standard English versions.", "labels": [], "entities": []}, {"text": "Our method yields a 20% reduction in word error rate over an existing state-of-the-art approach.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.6147445738315582}]}], "introductionContent": [{"text": "The amount of data produced in user-generated content, e.g. in online social media, and from machinegenerated sources such as optical character recognition (OCR) and automatic speech recognition (ASR), surpasses that found in more traditional media by orders of magnitude and continues to grow at a staggering rate.", "labels": [], "entities": [{"text": "optical character recognition (OCR)", "start_pos": 126, "end_pos": 161, "type": "TASK", "confidence": 0.7976421316464742}, {"text": "automatic speech recognition (ASR)", "start_pos": 166, "end_pos": 200, "type": "TASK", "confidence": 0.7968382239341736}]}, {"text": "However, the text found in these media can deviate wildly from the standard rules of orthography, syntax and even semantics and present significant problems to downstream applications which make use of this 'noisy' data.", "labels": [], "entities": []}, {"text": "In social media this noise might result from the need for social identity, simple spelling errors due to high input cost associated with the device (e.g. typing on a mobile phone), space constraints imposed by the specific medium or even a user's location).", "labels": [], "entities": []}, {"text": "In machine-generated texts, noise might result from imperfect inputs, imperfect conversion algorithms, or various degrees of each.", "labels": [], "entities": []}, {"text": "Recently, several works have looked at the process of normalizing these 'noisy' types of text into more standard English, or in other words, to convert the various forms of idiosyncratic spelling and writing errors found in these media into what would normally be considered standard English orthography.", "labels": [], "entities": []}, {"text": "Many of these works rely on supervised methods which share the common burden of requiring training data in the form of noisy input and clean output pairs.", "labels": [], "entities": []}, {"text": "The problem with developing large amounts of annotated training data is that it is costly and requires annotators with sufficient expertise.", "labels": [], "entities": []}, {"text": "However, the volume of data that is available in these media makes this a suitable domain for applying semi-and even fully unsupervised methods.", "labels": [], "entities": []}, {"text": "One interesting observation is that these noisy out-of-vocabulary (OOV) words are typically formed through some semi-deterministic process which doesn't render them completely indiscernible at a lexical level from the original words they are meant to represent.", "labels": [], "entities": []}, {"text": "We therefore refer to these OOV tokens as lexical variants of the clean in-vocabulary (IV) tokens they are derived from.", "labels": [], "entities": []}, {"text": "For instance, in social media '2morrow' '2morow' and '2mrw' still share at least some lexical resemblance with 'tomorrow', due to the fact that it is mainly the 82: A plot of the OOV distribution found in Twitter.", "labels": [], "entities": []}, {"text": "Also indicated is the potential for using (OOV,mostlikely-IV) training pairs found on this curve for either exception dictionary entries (the most frequent pairs), or for learning lexical transformations (the long tail).", "labels": [], "entities": [{"text": "OOV,mostlikely-IV", "start_pos": 43, "end_pos": 60, "type": "METRIC", "confidence": 0.915236234664917}]}, {"text": "The threshold between the two (vertical bar) is domainspecific.", "labels": [], "entities": []}, {"text": "result of a phonetic transliteration procedure.", "labels": [], "entities": [{"text": "phonetic transliteration", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.7543913722038269}]}, {"text": "Also, 'computer' and 'conpu7er' share strong lexical overlap, and might be the result of noise in the OCR process.", "labels": [], "entities": []}, {"text": "As with many aspects of NLP, the distribution of these OOV tokens resemble a power law distribution (see for the OOV distribution in Twitter).", "labels": [], "entities": []}, {"text": "Thus, some words are commonly converted to some OOV representation (e.g. domain-specific abbreviations in social media, or words which are commonly incorrectly detected in OCR) and these account for most of the errors, with the rest making up the long tail.", "labels": [], "entities": []}, {"text": "If one could somehow automatically extract a list of all the domain-specific OOV tokens found in a collection of texts, along with the most likely clean word (or words) each represents, then this could play a key role in for instance normalizing individual messages.", "labels": [], "entities": [{"text": "normalizing individual messages", "start_pos": 234, "end_pos": 265, "type": "TASK", "confidence": 0.8365698258082072}]}, {"text": "Very frequent (noisy, clean) pairs at the head of the distribution could be used for extracting common domain-specific abbreviations, and wordpairs in the long tail maybe used as input to learning algorithms for automatically learning the types of transformations found in these media, as shown in.", "labels": [], "entities": []}, {"text": "For example, taking Twitter as our target domain, examples for learning common exception pairs may include 'gf '\u2192'girlfriend'.", "labels": [], "entities": []}, {"text": "For learning types of lexical transformations, one might learn from 'thinking'\u2192'thinkin' and 'walking'\u2192'walkin' that 'ng' could go to 'n' (known as 'g-clipping').", "labels": [], "entities": []}, {"text": "In this paper we present a novel unsupervised method for extracting an approximation to such a domain-specific list of (noisy, clean) pairs, given only a large volume of representative text.", "labels": [], "entities": []}, {"text": "We furthermore demonstrate the utility of this method by applying it to normalize text messages found in the online social media service, Twitter, into their most likely standard English versions.", "labels": [], "entities": []}, {"text": "The primary contributions of this paper are: \u2022 We present an unsupervised method that mines (noisy, clean) pairs and requires only large amounts of domain-specific noisy data \u2022 We demonstrate the utility of this method by incorporating it into a standard method for noisy text normalization, which results in a significant reduction in the word error rate compared to the original method.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 272, "end_pos": 290, "type": "TASK", "confidence": 0.75848388671875}, {"text": "word error rate", "start_pos": 340, "end_pos": 355, "type": "METRIC", "confidence": 0.6787750820318857}]}], "datasetContent": [{"text": "We make use of the Twitter dataset discussed in.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.8086328506469727}]}, {"text": "It consists of a random sampling of 549 English tweets, annotated by three independent annotators.", "labels": [], "entities": []}, {"text": "All OOV words were pre-identified and the annotators were requested to determine the standard form (gold standard) for each ill-formed word.", "labels": [], "entities": [{"text": "gold standard)", "start_pos": 100, "end_pos": 114, "type": "METRIC", "confidence": 0.7408031721909841}]}, {"text": "In this study, we are interested in measuring the quality of our mined training pairs by evaluating its utility on an external task: Using the training pairs to induce a (noisy\u2192clean) exception dictionary to augment the working of a standard noisy text normalization system.", "labels": [], "entities": []}, {"text": "Hence, our focus is entirely on the accuracy of the candidate selection procedure as defined in Section 3.1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9992541670799255}, {"text": "candidate selection", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7769602239131927}]}, {"text": "We compute this accuracy in terms of the word error rate (WER), defined as the number of token substitutions, insertions or deletions one has to make to turn the system output into the gold standard, normalized by the total number of tokens in the output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993164539337158}, {"text": "word error rate (WER)", "start_pos": 41, "end_pos": 62, "type": "METRIC", "confidence": 0.8984528680642446}]}, {"text": "In order to remove the possible bias introduced by our very basic OOV-detection 87", "labels": [], "entities": []}], "tableCaptions": []}