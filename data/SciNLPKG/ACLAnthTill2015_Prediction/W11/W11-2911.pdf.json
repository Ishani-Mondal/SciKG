{"title": [{"text": "Learning Structural Dependencies of Words in the Zipfian Tail", "labels": [], "entities": []}], "abstractContent": [{"text": "Using semi-supervised EM, we learn fine-grained but sparse lexical parameters of a generative parsing model (a PCFG) initially estimated over the Penn Treebank.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.9224267601966858}, {"text": "Penn Treebank", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.9966140389442444}]}, {"text": "Our lexical parameters employ supertags, which encode complex structural information at the pre-terminal level, and are particularly sparse in labeled data-our goal is to learn these for words that are unseen or rare in the labeled data.", "labels": [], "entities": []}, {"text": "In order to guide estimation from unlabeled data, we incorporate both structural and lexical priors from the labeled data.", "labels": [], "entities": []}, {"text": "We get a large error reduction in parsing ambiguous structures associated with unseen verbs, the most important case of learning lexico-structural dependencies.", "labels": [], "entities": [{"text": "parsing ambiguous structures associated with unseen verbs", "start_pos": 34, "end_pos": 91, "type": "TASK", "confidence": 0.8971887656620571}]}, {"text": "We also obtain a statistically significant improvement in labeled bracketing score of the treebank PCFG, the first successful improvement via semi-supervised EM of a generative structured model already trained overlarge labeled data.", "labels": [], "entities": [{"text": "labeled bracketing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.5655539184808731}]}], "introductionContent": [{"text": "Computational models of natural language trained on labeled data contain many parameters that are not estimated accurately, due to the data sparsity inherent in labeled data.", "labels": [], "entities": []}, {"text": "This is especially true of complex structured models like parsers, which contain a large number of parameters, and where labeled training data is expensive to create.These models employ various forms of parameter smoothing to deal with overfitting and with unknown or low-frequency words.", "labels": [], "entities": []}, {"text": "However, it is desirable, and in many cases necessary, to augment supervised models using readily available unlabeled data, such as raw news-wire or from the web.", "labels": [], "entities": []}, {"text": "Semi-supervised methods have therefore received a lot of attention in recent years.", "labels": [], "entities": []}, {"text": "In this paper, we present a method for semisupervised training of a large-scale structured model (a Penn Treebank PCFG) using the Expectation Maximization algorithm.", "labels": [], "entities": [{"text": "Penn Treebank PCFG", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.9887762864430746}, {"text": "Expectation Maximization", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.7711636126041412}]}, {"text": "We focus on learning only those parameters of the model that are particularly difficult or impossible to obtain from labeled data, namely parameters related to low-frequency and unseen words (the Zipfian tail).", "labels": [], "entities": []}, {"text": "Words are important determiners of structural information for parsers; for instance, verb subcategorization information improved the Collins' parser.", "labels": [], "entities": []}, {"text": "However, this data is very sparse in even the largest labeled dataset available today, i.e., the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9950371980667114}]}, {"text": "To illustrate the severity of the problem, consider the fact that close to 40% of verb types in the training sections of the Penn Treebank have occurred only once therein.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.9930998384952545}]}, {"text": "Thus, modelling the structural properties of these verbs that maybe useful for disambiguation in a parser (such as subcategorization properties) is simply not possible from labeled data, and one has to look to unlabeled data.", "labels": [], "entities": []}, {"text": "From the machine learning point of view, semi-supervised learning in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g.,,).", "labels": [], "entities": []}, {"text": "For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8639512956142426}, {"text": "grammar learning", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7959345877170563}]}, {"text": "There have been some instances of successful large-scale semi-supervised learning for structured models, where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.9942255318164825}]}, {"text": "These methods have typically depended on the complementarity of multiple views of the data (a discriminative reranking model over a generative model as in)), and/or complex or heuristic objective functions (as in;) or simply by incorporating surface counts from unlabeled data (as in a recent paper by).", "labels": [], "entities": []}, {"text": "A contribution of this paper is that we show that using EM in a semi-supervised manner with a simple objective function can improve a parser, contrary to common belief in the field.", "labels": [], "entities": []}, {"text": "The PCFG model used in this paper is trained on the Penn Treebank.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7782239317893982}, {"text": "Penn Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9950065612792969}]}, {"text": "It contains fine-grained structural information marked on pre-terminal categories, making them similar in spirit to supertags for strongly lexicalised formalisms like LTAG () and CCG.", "labels": [], "entities": []}, {"text": "A supertag encodes structure that is distributed over the tree and localises it onto a single parameter of the model.", "labels": [], "entities": []}, {"text": "Our learning problem is cast very simply as estimating the parameters p(w|\u03c4 ) (where w is a word and \u03c4 a supertag) from labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "The problem is, however, more complex than a sequence labeling task because these supertags are highly ambiguous and encode argument-adjunct distinctions as well as long-distance dependencies (illustrated later in examples).", "labels": [], "entities": [{"text": "sequence labeling task", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7549163500467936}]}, {"text": "Semi-supervised EM is known to often give models that are worse than the supervised model.", "labels": [], "entities": []}, {"text": "To address this, we incorporate probabilistic constraints on unsupervised estimation by using labeled data to derive prior knowledge at two levels: (a) structural constraints in the form of higher PCFG rules (b) preferences over the distributions p(w|\u03c4 ) themselves.", "labels": [], "entities": []}, {"text": "We obtain large improvements in assigning correct structures to unseen verbs, and also a statistically significant improvement in labeled bracketing over a smoothed supervised model.", "labels": [], "entities": [{"text": "labeled bracketing", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.6778087466955185}]}, {"text": "The rest of the paper is structured as follows: a description of the Treebank PCFG model and its smoothing is in \u00a72.", "labels": [], "entities": [{"text": "Treebank PCFG model", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.9086381793022156}]}, {"text": "\u00a73 describes the semisupervised method, the constraints derived from labeled data, and their theoretical interpretation.", "labels": [], "entities": []}, {"text": "\u00a74 contains experiments and \u00a75 evaluations.", "labels": [], "entities": []}, {"text": "A discussion of related literature is in \u00a76.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experiments using a treebank PCFG trained on approximately 36,000 sentences from sections 0-22 of the Wall Street Journal (WSJ) portion of the PTB, with about 5000 sentences heldout for testing and development.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the PTB", "start_pos": 112, "end_pos": 156, "type": "DATASET", "confidence": 0.9516190171241761}]}, {"text": "Semi-supervised training is carried out using 4, 8, 12 and 16 million words of unlabeled WSJ data, after limiting sentence length to <25 words.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8005083799362183}]}, {"text": "Inside-outside estimation is implemented in Bitpar).", "labels": [], "entities": []}, {"text": "The corpus scaling factor for labeled data is set to 8 (i.e., a = 1 and b = 8 in Eq.", "labels": [], "entities": []}, {"text": "3; this value makes our labeled data ( 1 million words) weigh about twice as much as our smallest unlabeled corpus of 4 million words.", "labels": [], "entities": []}, {"text": "We experimented with setting the scaling factor to 4, making the labeled corpus of 1 million words effectively equal in size to the unlabeled corpus of 4 million words; however, a value of 8 gives better results, and we report only these.", "labels": [], "entities": []}, {"text": "Learning lexico-syntactic information We evaluate the learning of lexico-syntactic dependencies by measuring the accuracy of supertag assignment in Viterbi (maximum-probability) parses of test sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9993294477462769}]}, {"text": "We report this number for verbs, since they are the most important lexical determiners of structure in a sentence, as well as the most ambiguous.", "labels": [], "entities": []}, {"text": "To evaluate unseen verbs, we created a separate testset of 1200 sentences with about 1250 token occurrences of unseen verbs (about 110 types), by holding out all sentences with occurrences of these verbs from the labeled data.", "labels": [], "entities": []}, {"text": "These verbs have a wide variety of ambiguous subcategorization frames and are thus representative of typical verbs in the lexicon of a language.", "labels": [], "entities": []}, {"text": "This evaluation is a parsing-based evaluation and gives us a focused way of measuring the learning of syntactic structures associated with unseen words (verbs in this case).", "labels": [], "entities": []}, {"text": "Note that each supertag is associated with a local or nonlocal structure, and hence counting supertag accuracy in effect measures the accuracy of getting this subtree-structure right.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.965268075466156}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9991796612739563}]}, {"text": "Since these supertags encode empty categories and functional tags, it is not possible to compare other standard state-of-the-art parsers on this metric, since they do not contain either in their output.", "labels": [], "entities": []}, {"text": "shows the error in identifying the correct supertag for these unseen verbs in Viterbi parses of test sentences, for unla- beled training data of sizes 4, 8, 12 and 16 million words.", "labels": [], "entities": []}, {"text": "The baseline model is the smoothed treebank PCFG t smooth ( \u00a72.1), with an error of 29.86%.", "labels": [], "entities": []}, {"text": "This model does not contain lexical information specific to these verbs (being unseen).", "labels": [], "entities": []}, {"text": "Thus, in about 70% of the cases the parser assigns a correct supertag without verb-specific information.", "labels": [], "entities": []}, {"text": "We create a second baseline by parsing the unlabeled corpus with the model t smooth and obtaining Viterbi parses -this parsed corpus is merged with the labeled data, keeping corpus scaling factors same as before, and a PCFG t parse extracted from it.", "labels": [], "entities": []}, {"text": "This model is thus a self-trained model -it improves the supertag error overt smooth to 27.8%, and does not change subsequently.", "labels": [], "entities": [{"text": "supertag error overt smooth", "start_pos": 57, "end_pos": 84, "type": "METRIC", "confidence": 0.7491960376501083}]}, {"text": "Semi-supervised EM training improves the error rate overt smooth in the first iteration, and t parse in the second.", "labels": [], "entities": [{"text": "error rate overt smooth", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.9253678321838379}]}, {"text": "This improvement is already significant (p < 0.01, using McNemar's test).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.8562725186347961}]}, {"text": "The error rate goes onto further improve in subsequent iterations.", "labels": [], "entities": [{"text": "error rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9841204881668091}]}, {"text": "The error rate also improves with increasing sizes of unlabeled data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9904770851135254}]}, {"text": "The best obtained error is 24.62% with 16M words (p < 0.0001), a substantial error reduction of 17.5% over the smoothed supervised model.", "labels": [], "entities": []}, {"text": "Since these verbs have not occurred in the labeled data, the improvements are solely the result of learning from unlabeled data.", "labels": [], "entities": []}, {"text": "We also evaluated seen but low-frequency verbs (frequency 1 to 5 in the training corpus).", "labels": [], "entities": []}, {"text": "We see a benefit for these as well, with an error reduction of 8.97% (from 23.51 for the baseline t smooth to 21.40 for 16M words of unlabeled data).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.9721207618713379}]}, {"text": "shows the learning curves for different sizes of unlabeled data.", "labels": [], "entities": []}, {"text": "The distance between the 12M and 16M curves suggests that further improvements maybe obtained by adding even more: Labeled bracketing f-scores on Sec.", "labels": [], "entities": []}, {"text": "23 of PTB (4M words, f < 5).", "labels": [], "entities": [{"text": "PTB", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.6813836097717285}]}, {"text": "*** p<0.001, ** p<0.01 unlabeled data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Supertag error for unseen verbs in test Viterbi  parses, for different sizes of unlabeled training data", "labels": [], "entities": [{"text": "Supertag error", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8728407621383667}]}, {"text": " Table 2: Labeled bracketing f-scores on Sec. 23 of PTB (4M words, f < 5). *** p<0.001, ** p<0.01", "labels": [], "entities": [{"text": "Sec. 23 of PTB", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.6907974123954773}]}, {"text": " Table 3: Overall verbal supertag error, 4M words unla- beled data. (It=iteration)", "labels": [], "entities": []}]}