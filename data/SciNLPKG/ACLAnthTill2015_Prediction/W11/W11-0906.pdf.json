{"title": [{"text": "Transition-based Semantic Role Labeling Using Predicate Argument Clustering", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.5838084717591604}]}], "abstractContent": [{"text": "This paper suggests two ways of improving semantic role labeling (SRL).", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.8165639539559683}]}, {"text": "First, we introduce a novel transition-based SRL algorithm that gives a quite different approach to SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9341365694999695}, {"text": "SRL", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9697057008743286}]}, {"text": "Our algorithm is inspired by shift-reduce parsing and brings the advantages of the transition-based approach to SRL.", "labels": [], "entities": [{"text": "shift-reduce parsing", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.5488706529140472}, {"text": "SRL", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.921809732913971}]}, {"text": "Second, we present a self-learning clustering technique that effectively improves labeling accuracy in the test domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9235490560531616}]}, {"text": "For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions.", "labels": [], "entities": []}, {"text": "All approaches are evaluated on the CoNLL'09 English data.", "labels": [], "entities": [{"text": "CoNLL'09 English data", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.9662607312202454}]}, {"text": "The new algorithm shows comparable results to another state-of-the-art system.", "labels": [], "entities": []}, {"text": "The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.", "labels": [], "entities": [{"text": "labeling", "start_pos": 34, "end_pos": 42, "type": "TASK", "confidence": 0.9527339339256287}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9475908875465393}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) has sparked much interest in.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8370598157246908}]}, {"text": "Lately, dependency-based SRL has shown advantages over constituent-based SRL.", "labels": [], "entities": [{"text": "dependency-based SRL", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.6410903036594391}, {"text": "SRL", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.806135892868042}]}, {"text": "Two main benefits can be found.", "labels": [], "entities": []}, {"text": "First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8517996668815613}, {"text": "constituent parsing", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6819817572832108}, {"text": "constituent parsing", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.6638765037059784}, {"text": "SRL", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.8709599375724792}]}, {"text": "Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs.", "labels": [], "entities": [{"text": "dependency structure", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.7910308241844177}]}, {"text": "Unlike constituent-based SRL that maps phrases to semantic roles, dependencybased SRL maps headwords to semantic roles because there is no phrasal node in dependency structure.", "labels": [], "entities": []}, {"text": "This may lead to a concern about getting the actual semantic chunks back, but have shown that it is possible to recover the original chunks from the headwords with minimal loss, using a certain type of dependency structure.", "labels": [], "entities": []}, {"text": "Traditionally, either constituent or dependencybased, semantic role labeling is done in two steps, argument identification and classification).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.6822749972343445}, {"text": "argument identification", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.7269115447998047}]}, {"text": "This is from a general belief that each step requires a different set of features (), and training these steps in a pipeline takes less time than training them as a joint-inference task.", "labels": [], "entities": []}, {"text": "However, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time).", "labels": [], "entities": []}, {"text": "Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8627719581127167}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.999051034450531}, {"text": "dependency parsing", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.7421020865440369}]}, {"text": "This motivates the development of anew semantic role labeling algorithm that treats these two steps as a joint inference task.", "labels": [], "entities": []}, {"text": "Our algorithm is inspired by shift-reduce parsing.", "labels": [], "entities": []}, {"text": "The algorithm uses several transitions to identify predicates and their arguments with semantic roles.", "labels": [], "entities": []}, {"text": "One big advantage of the transitionbased approach is that it can use previously identified arguments as features to predict the next argument.", "labels": [], "entities": []}, {"text": "We apply this technique to our approach and achieve comparable results to another state-of-theart system evaluated on the same data sets.: Transitions in our bidirectional top-down search algorithm.", "labels": [], "entities": []}, {"text": "For each row, the first line shows a transition and the second line shows preconditions of the transition.", "labels": [], "entities": []}, {"text": "For better generalization of the statistical models, we apply a self-learning clustering technique.", "labels": [], "entities": []}, {"text": "We first cluster predicates in test data using automatically generated predicate argument structures, then cluster predicates in training data by using the previously found clusters as seeds.", "labels": [], "entities": []}, {"text": "Our experiments show that this technique improves labeling accuracy for both in-domain and out-of-domain tasks.", "labels": [], "entities": [{"text": "labeling", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.9530655741691589}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9468767046928406}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Labeling accuracies evaluated on the WSJ (P:  precision, R: recall, F1: F1-score, all in %). 'AI' and  'AC' stand for argument identification and argument clas- sification, respectively.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8852143287658691}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.7861765027046204}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.7605860233306885}, {"text": "F1: F1-score", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.8108561038970947}, {"text": "AI", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9605467915534973}, {"text": "argument identification", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.7483618259429932}]}, {"text": " Table 5: Labeling accuracies evaluated on the Brown.", "labels": [], "entities": [{"text": "Labeling", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9343053102493286}, {"text": "Brown.", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.9413344264030457}]}]}