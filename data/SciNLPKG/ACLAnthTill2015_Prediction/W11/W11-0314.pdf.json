{"title": [{"text": "ULISSE: an Unsupervised Algorithm for Detecting Reliable Dependency Parses", "labels": [], "entities": [{"text": "Detecting Reliable Dependency Parses", "start_pos": 38, "end_pos": 74, "type": "TASK", "confidence": 0.8066932410001755}]}], "abstractContent": [{"text": "In this paper we present ULISSE, an unsu-pervised linguistically-driven algorithm to select reliable parses from the output of a dependency parser.", "labels": [], "entities": []}, {"text": "Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains.", "labels": [], "entities": []}, {"text": "In all cases, ULISSE appears to outperform the baseline algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "While the accuracy of state-of-the-art parsers is increasing more and more, this is still not enough for their output to be used in practical NLP-based applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993202686309814}]}, {"text": "In fact, when applied to real-world texts (e.g. the web or domain-specific corpora such as bio-medical literature, legal texts, etc.) their accuracy decreases significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9987377524375916}]}, {"text": "This is areal problem since it is broadly acknowledged that applications such as Information Extraction, Question Answering, Machine Translation, and soon can benefit significantly from exploiting the output of a syntactic parser.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8202122449874878}, {"text": "Question Answering", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8424267470836639}, {"text": "Machine Translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8479439318180084}]}, {"text": "To overcome this problem, over the last few years a growing interest has been shown in assessing the reliability of automatically produced parses: the selection of high quality parses represents nowadays a key and challenging issue.", "labels": [], "entities": []}, {"text": "The number of studies devoted to detecting reliable parses from the output of a syntactic parser is spreading.", "labels": [], "entities": []}, {"text": "They mainly differ with respect to the kind of selection algorithm they exploit.", "labels": [], "entities": []}, {"text": "Depending on whether training data, machine learning classifiers or external parsers are exploited, existing algorithms can be classified into i) supervised-based, ii) ensemble-based and iii) unsupervised-based methods.", "labels": [], "entities": []}, {"text": "The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types.", "labels": [], "entities": []}, {"text": "exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser.", "labels": [], "entities": []}, {"text": "relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas exploited an external constituency parser to extract text-based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 381, "end_pos": 389, "type": "METRIC", "confidence": 0.9697940349578857}]}, {"text": "The approaches proposed by and can be classified as ensemble-based methods.", "labels": [], "entities": []}, {"text": "Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data.", "labels": [], "entities": []}, {"text": "However, a widely acknowledged problem of both supervised-based and ensemble-based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9988651275634766}]}, {"text": "To our knowledge, are the first to address the task of high qual-ity parse selection by resorting to an unsupervisedbased method.", "labels": [], "entities": [{"text": "qual-ity parse selection", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.6605239907900492}]}, {"text": "The underlying idea is that syntactic structures that are frequently created by a parser are more likely to be correct than structures produced less frequently.", "labels": [], "entities": []}, {"text": "For this purpose, their PUPA (POSbased Unsupervised Parse Assessment Algorithm) uses statistics about POS tag sequences of parsed sentences produced by an unsupervised constituency parser.", "labels": [], "entities": []}, {"text": "In this paper, we address this unsupervised scenario with two main novelties: unlike Reichart and Rappoport (2009a), a) we address the reliable parses selection task using an unsupervised method in a supervised parsing scenario, and b) we operate on dependency-based representations.", "labels": [], "entities": [{"text": "parses selection task", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.8925024469693502}]}, {"text": "Similarly to Reichart and Rappoport (2009a) we exploit text internal statistics: but whereas they rely on features that are closely related to constituency representations, we use linguistic features which are dependencymotivated.", "labels": [], "entities": []}, {"text": "The proposed algorithm has been evaluated for selecting reliable parses from English and Italian corpora; to our knowledge, this is the first time that such a task has been applied to a less resourced language such as Italian.", "labels": [], "entities": []}, {"text": "The paper is organised as follows: in Section 2 we illustrate the ULISSE algorithm; sections 3 and 4 are devoted to the used parsers and baselines.", "labels": [], "entities": []}, {"text": "Section 5 describes the experiments and discusses achieved results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments were organised as follows: a target corpus was automatically POS tagged (Dell'Orletta, 2009) and dependency-parsed; the ULISSE and baseline algorithms of reliable parse selection were run on the POS-tagged and dependency-parsed target corpus in order to identify high quality parses; results achieved by the selection algorithms were evaluated with respect to a subset of the target corpus of about 5,000 word-tokens (henceforth referred to as \"test set\") for which gold-standard annotation was available.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 179, "end_pos": 194, "type": "TASK", "confidence": 0.7373473346233368}]}, {"text": "Different sets of experiments were devised to test the robustness of our algorithm.", "labels": [], "entities": []}, {"text": "They were performed with respect to i) the output of the parsers described in Section 3, ii) two different languages, iii) different domains.", "labels": [], "entities": []}, {"text": "For what concerns the languages, we chose Italian and English for two main reasons.", "labels": [], "entities": []}, {"text": "First of all, they pose different challenges to a parser since they are characterised by quite different syntactic features.", "labels": [], "entities": []}, {"text": "For instance, Italian, as opposed to English, is characterised by a relatively free word order (especially for what concerns subject and object relations with respect to the verb) and by the possible absence of an overt subject.", "labels": [], "entities": []}, {"text": "Secondly, as it is shown in Section 5.1, Italian is a less resourced language with respect to English.", "labels": [], "entities": []}, {"text": "This is a key issue, since as demonstrated by and, small and big treebanks pose different problems in the reliable parses selection.", "labels": [], "entities": [{"text": "parses selection", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.8949099481105804}]}, {"text": "Last but not least, we aimed at demonstrating that ULISSE can be successfully used not only with texts belonging to the same domain as the parser training corpus.", "labels": [], "entities": []}, {"text": "For this purpose, ULISSE was tested on a target corpus of Italian legislative texts, whose automatic linguistic analysis poses domain-specific challenges.", "labels": [], "entities": []}, {"text": "Out-of-domain experiments are being carried out also for English.", "labels": [], "entities": []}, {"text": "Performances of the ULISSE algorithm have been evaluated i) with respect to the accuracy of ranked parses and ii) in terms of Precision and Recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.998824417591095}, {"text": "Precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9964984655380249}, {"text": "Recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9441284537315369}]}, {"text": "First, for each experiment we evaluated how the ULISSE algorithm and the baselines classify the sentences in the test set with respect to the \"Labelled Attachment Score\" (LAS) obtained by the parsers, i.e. the percentage of tokens for which it has predicted the correct head and dependency relation.", "labels": [], "entities": [{"text": "Labelled Attachment Score\" (LAS)", "start_pos": 143, "end_pos": 175, "type": "METRIC", "confidence": 0.7954329337392535}]}, {"text": "In particular, we computed the LAS score of increasingly wider top lists of k tokens, where k ranges from 500 word tokens to the whole size of the test set (with a step size of 500 word tokens, i.e. k=500, k=1000, k=1500, etc.).", "labels": [], "entities": [{"text": "LAS score", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9421928524971008}]}, {"text": "As regards ii), we focused on the set of ranked sentences showing a LAS \u2265 \u03b1.", "labels": [], "entities": [{"text": "LAS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9917751550674438}]}, {"text": "Since imposing a 100% LAS was too restrictive, for each experiment we defined a different \u03b1 threshold taking into account the performance of each parser across the different languages and domains.", "labels": [], "entities": []}, {"text": "In particular, we took the top 25% and 50% of the list of ranked sentences and calculated Precision and Recall for each of them.", "labels": [], "entities": [{"text": "Precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9989186525344849}, {"text": "Recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9755442142486572}]}, {"text": "To this specific end, a parse tree showing a LAS \u2265 \u03b1 is considered as a trustworthy analysis.", "labels": [], "entities": [{"text": "LAS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9866529107093811}]}, {"text": "Precision has been computed as the ratio of the number of trustworthy analyses over the total number of sentences in each top list.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9327123761177063}]}, {"text": "Recall has been computed as the ratio of the number of trustworthy analyses which have been retrieved over the total number of trustworthy analyses in the whole test set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9875757694244385}]}, {"text": "In order to test how the ULISSE algorithm is able to select reliable parses by relying on parse features rather than on raw text features, we computed the accuracy score (LAS) of a subset of the top list of sentences parsed by both parsers and ranked by ULISSE: in particular, we focused on those sentences which were not shared by the MST and DeSR top lists.", "labels": [], "entities": [{"text": "accuracy score (LAS)", "start_pos": 155, "end_pos": 175, "type": "METRIC", "confidence": 0.982749605178833}, {"text": "MST and DeSR top lists", "start_pos": 336, "end_pos": 358, "type": "DATASET", "confidence": 0.7925164759159088}]}], "tableCaptions": [{"text": " Table 1. The accuracy has been com- puted in terms of LAS and of Unlabelled Attach- ment Score (UAS), i.e. the percentage of tokens with  a correctly identified syntactic head. It can be no- ticed that the performance of the two parsers is quite  similar for Italian (i.e. wrt ISST TS and Legal TS),  whereas there is a 2.3% difference between the MST  and DeSR accuracy as far as English is concerned.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996628761291504}, {"text": "LAS", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9806036353111267}, {"text": "Unlabelled Attach- ment Score (UAS)", "start_pos": 66, "end_pos": 101, "type": "METRIC", "confidence": 0.7697898261249065}, {"text": "DeSR accuracy", "start_pos": 358, "end_pos": 371, "type": "METRIC", "confidence": 0.595589816570282}]}, {"text": " Table 1: Overall accuracy of DeSR and MST parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9987356066703796}, {"text": "MST parsers", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.6094177216291428}]}, {"text": " Table 3: LAS of not-shared sentences in the DeSR and  MST top-lists.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9810861945152283}, {"text": "DeSR and  MST top-lists", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.772005170583725}]}, {"text": " Table 2: In all Tables: the number of sentences with a LAS \u2265 \u03b1 parsed by DeSr and MST parsers (first row); Precision  (Prec), Recall (Rec), the corresponding parser accuracy (LAS) of the top 25% and 50% of the list of sentences and  ranked by the ULISSE algorithm, Length of Sentence (LS) and dependency PUPA (dPUPA) and the corresponding  average length in tokens of ranked sentence (AvgSL).", "labels": [], "entities": [{"text": "Precision  (Prec)", "start_pos": 108, "end_pos": 125, "type": "METRIC", "confidence": 0.929988905787468}, {"text": "Recall (Rec)", "start_pos": 127, "end_pos": 139, "type": "METRIC", "confidence": 0.93553726375103}, {"text": "accuracy (LAS)", "start_pos": 166, "end_pos": 180, "type": "METRIC", "confidence": 0.962167426943779}, {"text": "Length of Sentence (LS)", "start_pos": 266, "end_pos": 289, "type": "METRIC", "confidence": 0.8665214478969574}]}]}