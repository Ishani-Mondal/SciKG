{"title": [{"text": "Automatic Category Label Coarsening for Syntax-Based Machine Translation", "labels": [], "entities": [{"text": "Syntax-Based Machine Translation", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.7523064613342285}]}], "abstractContent": [{"text": "We consider SCFG-based MT systems that get syntactic category labels from parsing both the source and target sides of parallel training data.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.680997908115387}]}, {"text": "The resulting joint nonterminals often lead to needlessly large label sets that are not optimized for an MT scenario.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9912596344947815}]}, {"text": "This paper presents a method of iteratively coarsening a label set fora particular language pair and training corpus.", "labels": [], "entities": []}, {"text": "We apply this label collapsing on Chinese-English and French-English grammars, obtaining test-set improvements of up to 2.8 BLEU, 5.2 TER, and 0.9 METEOR on Chinese-English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9994891881942749}, {"text": "TER", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9935894012451172}, {"text": "METEOR", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9983729124069214}]}, {"text": "An analysis of label collapsing's effect on the grammar and the decoding process is also given.", "labels": [], "entities": [{"text": "label collapsing", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7200977355241776}]}], "introductionContent": [{"text": "A common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (SCFG), where a source-language string and a target-language string are produced simultaneously by applying a series of re-write rules.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6467128793398539}]}, {"text": "Given a parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, SCFG models for MT can be learned via a variety of methods.", "labels": [], "entities": [{"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.990362286567688}]}, {"text": "Parsing maybe applied on the source side (), on the target side (), or on both sides of the parallel corpus (.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.6806702613830566}]}, {"text": "In any of these cases, using the raw label set from source-and/or target-side parsers can be undesirable.", "labels": [], "entities": []}, {"text": "Label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists.", "labels": [], "entities": []}, {"text": "These labels sets are not necessarily ideal for statistical parsing, let alone for bilingual syntax-based translation models.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7763663530349731}]}, {"text": "Further, the side(s) on which syntax is represented defines the nonterminal label space used by the resulting SCFG.", "labels": [], "entities": []}, {"text": "A pair of aligned adjectives, for example, maybe labeled ADJ if only source-side syntax is used, JJ if only target-side syntax is used, or ADJ::JJ if syntax from both sides is used in the grammar.", "labels": [], "entities": []}, {"text": "Beyond such differences, however, most existing SCFG-based MT systems do not further modify the nonterminal label set in use.", "labels": [], "entities": [{"text": "SCFG-based MT", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.6282108128070831}]}, {"text": "Those that do require either specialized decoders or complicated parameter tuning, or the label set maybe unsatisfactory from a computational point of view (Section 2).", "labels": [], "entities": []}, {"text": "We believe that representing both source-side and target-side syntax is important.", "labels": [], "entities": []}, {"text": "Even assuming two monolingually perfect label sets for the source and target languages, using label information from only one side ignores any meaningful constraints expressed in the labels of the other.", "labels": [], "entities": []}, {"text": "On the other hand, using the default node labels from both sides generates a joint nonterminal set of thousands of unique labels, not all of which maybe useful.", "labels": [], "entities": []}, {"text": "Our real preference is to use a joint nonterminal set adapted to our particular language pair or translation task.", "labels": [], "entities": []}, {"text": "In this paper, we present the first step towards a tailored label set: collapsing syntactic categories to remove the most redundant labels and shrink the overall source-target nonterminal set.", "labels": [], "entities": []}, {"text": "There are two problems with an overly large label set: First, it encourages labeling ambiguity among rules, a well-known practical problem in SCFGbased MT.", "labels": [], "entities": [{"text": "SCFGbased MT", "start_pos": 142, "end_pos": 154, "type": "TASK", "confidence": 0.4521263837814331}]}, {"text": "Most simply, the same right-hand side maybe observed in rule extraction with a variety of left-hand-side labels, each leading to a unique rule in the grammar.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7553001940250397}]}, {"text": "The grammar may further contain many rules with the same structure and reordering pattern that differ only with respect to the actual labels in use.", "labels": [], "entities": []}, {"text": "Together, these properties can cause an SCFG-based MT system to process a large number of alternative syntactic derivations that use different rules but produce identical output strings.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.6847071051597595}]}, {"text": "Limiting the possible number of variant labelings cuts down on ambiguous derivations.", "labels": [], "entities": []}, {"text": "Second, a large label set leads to rule sparsity.", "labels": [], "entities": []}, {"text": "A rule whose right-hand side can only apply on a very tightly specified set of labels is unlikely to be estimated reliably from a parallel corpus or to apply in all needed cases attest time.", "labels": [], "entities": []}, {"text": "However, a coarser version of its application constraints maybe more frequently observed in training data and more likely to apply on test data.", "labels": [], "entities": []}, {"text": "We therefore introduce a method for automatically clustering and collapsing category labels, on either one or both sides of SCFG rules, for any language pair and choice of statistical parsers (Section 3).", "labels": [], "entities": []}, {"text": "Turning to alignments between source and target parse nodes as an additional source of information, we calculate a distance metric between any two labels in one language based on the difference in alignment probabilities to labels in the other language.", "labels": [], "entities": []}, {"text": "We then apply a greedy label collapsing algorithm that repeatedly merges the two labels with the closest distance until some stopping criterion is reached.", "labels": [], "entities": [{"text": "label collapsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7430620193481445}]}, {"text": "The resulting coarsened labels are used in the SCFG rules of a syntactic machine translation system in place of the original labels.", "labels": [], "entities": [{"text": "syntactic machine translation", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.6752485036849976}]}, {"text": "In experiments on Chinese-English translation (Section 4), we find significantly improved performance of up to 2.8 BLEU points, 5.2 TER points, and 0.9 METEOR points by applying varying degrees of label collapsing to a baseline syntax-based MT system (Section 5).", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.6427803039550781}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9990897178649902}, {"text": "TER", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9978237152099609}, {"text": "METEOR", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9986215829849243}, {"text": "MT", "start_pos": 241, "end_pos": 243, "type": "TASK", "confidence": 0.9419243931770325}]}, {"text": "In our analysis of the results (Section 6), we find that the largest immediate effect of coarsening the label set is to reduce the number of fully abstract hierarchical SCFG rules present in the grammar.", "labels": [], "entities": []}, {"text": "These rules' increased permissiveness, in turn, directs the decoder's search into a largely disjoint realm from the search space explored by the baseline system.", "labels": [], "entities": []}, {"text": "A full summary and ideas for future work are given in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are conducted on Chinese-to-English translation using approximately 300,000 sentence pairs from the FBIS corpus.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.6633734852075577}, {"text": "FBIS corpus", "start_pos": 112, "end_pos": 123, "type": "DATASET", "confidence": 0.9575777351856232}]}, {"text": "To obtain parse trees over both sides of each parallel corpus, we used the English and Chinese grammars of the Berkeley parser (.", "labels": [], "entities": []}, {"text": "Given a parsed and word-aligned parallel sentence, we extract SCFG rules from it following the procedure of.", "labels": [], "entities": []}, {"text": "The method first identifies node alignments between the two parse trees according to support from the word alignments.", "labels": [], "entities": []}, {"text": "A node in the source parse tree will be aligned to anode in the target parse tree if all the words in the yield of the source node are either all aligned to words within the yield of the target node or have no alignments at all.", "labels": [], "entities": []}, {"text": "Then SCFG rules can be extracted from adjacent levels of aligned nodes, which specify points at which the tree pair can be decomposed into minimal SCFG rules.", "labels": [], "entities": []}, {"text": "In addition to producing a minimal rule, each decomposition point also produces a phrase pair rule with the node pair's yields as the right-hand side, as long as the length of the yield is less than a specified threshold.", "labels": [], "entities": []}, {"text": "Following grammar extraction, labels are optionally clustered and collapsed according to the algorithm in Section 3.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6879885494709015}]}, {"text": "The grammar is re-written with the modified nonterminals, then scored as usual according to our translation model features.", "labels": [], "entities": []}, {"text": "Feature weights themselves are learned via minimum error rate training as implemented in Z-MERT) with the BLEU metric ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.975993812084198}]}, {"text": "Decoding is carried outwith Joshua (, an open-source platform for SCFG-based MT.", "labels": [], "entities": [{"text": "SCFG-based MT", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.49210333824157715}]}, {"text": "Due to engineering limitations in decoding with a large grammar, we apply three additional errorcorrection and filtering steps to every system.", "labels": [], "entities": []}, {"text": "First, we observed that the syntactic parsers were most likely to make labeling errors for cardinal numbers in English and punctuation marks in all languages.", "labels": [], "entities": []}, {"text": "We thus post-process the parses of our training data to tag all English cardinal numbers as CD and to overwrite the labels of various punctuation marks with the correct labels as defined by each language's label set.", "labels": [], "entities": []}, {"text": "Second, after rule extraction, we compute the distribution of left-hand-side labels for each unique labeled right-hand side in the grammar, and we remove the labels in the least frequent 10% of the distribution.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7267186194658279}]}, {"text": "This puts a general-purpose limit on labeling ambiguity.", "labels": [], "entities": [{"text": "labeling ambiguity", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9000597596168518}]}, {"text": "Third, we filter and prune the final scored grammar to each individual development and test set before decoding: all matching phrase pairs are retained, along with the most frequent 10,000 hierarchical grammar rules.", "labels": [], "entities": []}, {"text": "In our first set of experiments, we sought to explore the effect of increasing degrees of label collapsing on a baseline system and to determine a reasonable stopping point.", "labels": [], "entities": []}, {"text": "Starting with the baseline grammar, we ran the label collapsing algorithm of Section 3 until all the constituent labels on each side had been collapsed into a single category.", "labels": [], "entities": [{"text": "label collapsing", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7063926011323929}]}, {"text": "We next examined the L 1 distances between the label pairs that had been merged in each iteration of the algorithm.", "labels": [], "entities": []}, {"text": "This data is shown in as a plot of L 1 distance versus iteration number.", "labels": [], "entities": [{"text": "L 1 distance", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.8398191332817078}]}, {"text": "The distances between the successive labels merged in the first 29 iterations of the algorithm are nearly monotonically increasing, followed by a much larger discontinuity at iteration 30.", "labels": [], "entities": []}, {"text": "Similar patterns emerge for iterations 30 to 45 and for iterations 46 to 60.", "labels": [], "entities": []}, {"text": "The next regions of the graph, from iterations 61 to 81 and from iterations 82 to 99, show an increasing prevalence of discontinuities.", "labels": [], "entities": []}, {"text": "Finally, from iterations 100 to 123, the successive L 1 distances entirely alternate between very high and very low values.", "labels": [], "entities": []}, {"text": "Discontinuities are merely the result of a label pair in one language suddenly scoring much lower on the distribution difference metric than previously, thanks to some change that has occurred in the label set of the other language.", "labels": [], "entities": []}, {"text": "Looking back to, for example, we could bring the distributions for JJ and JJS much closer together by merging A and ADV on the French side.", "labels": [], "entities": []}, {"text": "Although such sudden drops in distribution difference value are expected, they may provide an indication of when the label collapsing algorithm has progressed too far, since we have so reduced the label set that categories previously very different have become much less distinguishable.", "labels": [], "entities": [{"text": "label collapsing", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.710937574505806}]}, {"text": "On the other hand, further reduction of the label set may have a variety of pratical benefits.", "labels": [], "entities": []}, {"text": "We tested this trade-off empirically by building five Chinese-English MT systems, each exhibiting an increasing degree of label collapsing compared to the original label set, which serves as our baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9040875434875488}]}, {"text": "The degree of label collapsing in each of the five systems corresponds to one of the major discontinuity features highlighted in the right-hand side output.", "labels": [], "entities": []}, {"text": "With n = 1000 and p < 0.05, all five labelcollapsed systems were statistically significant improvements over the baseline, and all other collapsed systems were significant improvements over the 99-iteration system.", "labels": [], "entities": []}, {"text": "Thus, though the system that provides the highest score changes across metrics and test sets, the overall pattern of scores suggests that over-collapsing labels may start to weaken results.", "labels": [], "entities": []}, {"text": "A more moderate stopping point is thus preferable, but beyond that we suspect the best result is determined more by the test set, automatic metric choice, and MERT instability than systematic changes in the label set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9367914795875549}]}, {"text": "showed a strong practical benefit to running the label collapsing algorithm.", "labels": [], "entities": [{"text": "label collapsing algorithm", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.7999328374862671}]}, {"text": "In this section, we seek to further understand where this benefit comes from, tracing the effects of label collapsing via its modification of labels themselves, the differences in the resulting grammars, and collapsing's effect on decoding and output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of applying increasing degrees of label collapsing on our Chinese-English baseline system. Bold  figures indicate the best score in each column.", "labels": [], "entities": [{"text": "label collapsing", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7044120132923126}]}]}