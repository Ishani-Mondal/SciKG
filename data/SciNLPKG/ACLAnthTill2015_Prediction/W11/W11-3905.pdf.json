{"title": [{"text": "Robust Unsupervised and Semi-Supervised Methods in Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a simple method to achieve logical constraints on words for topic models based on a recently developed topic modeling framework with Dirichlet forest priors (LDA-DF).", "labels": [], "entities": []}, {"text": "Logical constraints mean logical expressions of pairwise constraints, Must-links and Cannot-Links, used in the literature of constrained clustering.", "labels": [], "entities": []}, {"text": "Our method cannot only cover the original constraints of the existing work, but also allow us easily to add new customized constraints.", "labels": [], "entities": []}, {"text": "We discuss the validity of our method by defining its asymptotic behaviors.", "labels": [], "entities": []}, {"text": "We verify the effectiveness of our method with comparative studies on a synthetic corpus and interactive topic analysis on areal corpus .", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models such as Latent Dirichlet Allocation or LDA ( are widely used to capture hidden topics in a corpus.", "labels": [], "entities": []}, {"text": "When we have domain knowledge of a target corpus, incorporating the knowledge into topic models would be useful in a practical sense.", "labels": [], "entities": []}, {"text": "Thus there have been many studies of semi-supervised extensions of topic models (, although topic models are often regarded as unsupervised learning.", "labels": [], "entities": []}, {"text": "Recently, ( ) developed a novel topic modeling framework, LDA with Dirichlet Forest priors (LDA-DF), which achieves two links Must-Link (ML) and Cannot-Link (CL) in the constrained clustering literature (.", "labels": [], "entities": []}, {"text": "For given words A and B, ML(A, B) and CL(A, B) are soft constraints that A and B must appear in the same topic, and that A and B cannot appear in the same topic, respectively.", "labels": [], "entities": []}, {"text": "Let us consider topic analysis of a corpus with movie reviews for illustrative purposes.", "labels": [], "entities": []}, {"text": "We know that two words 'jackie' (means Jackie Chan) and 'kung-fu' should appear in the same topic, while 'dicaprio' (means Leonardo DiCaprio) and 'kung-fu' should not appear in the same topic.", "labels": [], "entities": []}, {"text": "In this case, we can add constraints ML('jackie', 'kung-fu') and CL('dicaprio', 'kung-fu') to smoothly conduct analysis.", "labels": [], "entities": []}, {"text": "However, what if there is a word 'bruce' (means Bruce Lee) in the corpus, and we want to distinguish between 'jackie' and 'bruce'?", "labels": [], "entities": []}, {"text": "Our full knowledge among 'kung-fu', 'jackie', and 'bruce' should be (ML('kung-fu', 'jackie') \u2228 ML('kung-fu', 'bruce')) \u2227 CL('bruce', 'jackie'), although the original framework does not allow a disjunction (\u2228) of links.", "labels": [], "entities": []}, {"text": "In this paper, we address such logical expressions of links on LDA-DF framework.", "labels": [], "entities": [{"text": "LDA-DF framework", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.8742208480834961}]}, {"text": "Combination between a probabilistic model and logical knowledge expressions such as Markov Logic Network (MLN) is recently getting a lot of attention (, and our work can be regarded as on this research line.", "labels": [], "entities": []}, {"text": "At least, to our knowledge, our method is the first one that can directly incorporate logical knowledge into a prior for topic models without MLN.", "labels": [], "entities": []}, {"text": "This means the complexity of the inference in our method is essentially the same as in the original LDA-DF, despite that our method can broaden knowledge expressions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The average numbers of Dirichlet  trees compiled by minimum DNF (M-DNF) and  asymptotic minimum DNF (AM-DNF) in terms of  the number of random links. Each value is the av- erage of 100 trials.  # of links 1  2  4  8  16  M-DNF  1 2.08 3.43 6.18 10.35  AM-DNF 1 2.08 3.23 4.24 4.07", "labels": [], "entities": []}]}