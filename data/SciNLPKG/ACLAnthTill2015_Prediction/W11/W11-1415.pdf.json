{"title": [{"text": "Readability Annotation: Replacing the Expert by the Crowd", "labels": [], "entities": [{"text": "Readability Annotation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.807581752538681}]}], "abstractContent": [{"text": "This paper investigates two strategies for collecting readability assessments, an Expert Readers application intended to collect fine-grained readability assessments from language experts and a Sort by Readability application designed to be intuitive and open for everyone having internet access.", "labels": [], "entities": []}, {"text": "We show that the data sets resulting from both annotation strategies are very similar.", "labels": [], "entities": []}, {"text": "We conclude that crowdsourcing is a viable alternative to the opinions of language experts for readabil-ity prediction.", "labels": [], "entities": [{"text": "readabil-ity prediction", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.7849684953689575}]}], "introductionContent": [{"text": "The task of automatically determining the readability of texts has along and rich tradition.", "labels": [], "entities": []}, {"text": "This has not only resulted in a large number of readability formulas, but also to the more recent tendency of using insights from NLP for automatic readability prediction (.", "labels": [], "entities": [{"text": "readability prediction", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.6664170324802399}]}, {"text": "Potential applications include the selection of reading material for language learners, automatic essay scoring, the selection of online text material for automatic summarization, etc.", "labels": [], "entities": [{"text": "automatic essay scoring", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.5839671989281973}, {"text": "summarization", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.8252658843994141}]}, {"text": "One of the well-known bottlenecks in data-driven NLP research is the lack of sufficiently large data sets for which annotators provided labels with sufficient agreement.", "labels": [], "entities": []}, {"text": "Also readability research is faced with the crucial obstacle that very few corpora of generic texts exist of which reliable readability information is available.", "labels": [], "entities": []}, {"text": "When constructing such a corpus, the inherent subjectivity of the concept of readability cannot be ignored.", "labels": [], "entities": []}, {"text": "The ease with which a given reader can correctly identify the message conveyed in a text is, among other things, inextricably related to the reader's background knowledge of the subject at hand (.", "labels": [], "entities": [{"text": "correctly identify the message conveyed in a text", "start_pos": 39, "end_pos": 88, "type": "TASK", "confidence": 0.6548546031117439}]}, {"text": "The construction of a corpus, which can serve as a gold standard against which new scoring or ranking systems can be tested, thus requires a multifaceted approach taking into account both the properties of the text under evaluation and those of the readers.", "labels": [], "entities": []}, {"text": "In recent years, a tendency seems to have arisen to also explicitly address this subjective aspect of readability., for example, base their readability prediction method exclusively on the extent to which readers found a text to be \"well-written\" and take the assessments supplied by a number of experts as their gold standard, and test their readability prediction method as well as assessments by novices against these expert opinions.", "labels": [], "entities": [{"text": "readability prediction", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.6660570055246353}]}, {"text": "In this paper, we report on two methodologies to construct a corpus of readability assessments, which can serve as a gold standard against which new scoring or ranking systems can be tested.", "labels": [], "entities": []}, {"text": "Both methodologies were used for collecting readability assessments of Dutch and English texts.", "labels": [], "entities": [{"text": "collecting readability assessments of Dutch and English texts", "start_pos": 33, "end_pos": 94, "type": "TASK", "confidence": 0.7569119110703468}]}, {"text": "Since these data collection experiments for English only recently started, the focus in this paper will be on Dutch.", "labels": [], "entities": []}, {"text": "By collecting multiple assessments per text, the goal was to level out the reader's background knowledge and attitude.", "labels": [], "entities": []}, {"text": "We will both report on a data collection experiment designed for language experts and a simple crowdsourcing experiment.", "labels": [], "entities": []}, {"text": "We will introduce inter-annotator agreement and calculate K scores in different settings.", "labels": [], "entities": [{"text": "K scores", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9179185330867767}]}, {"text": "We will show that from the two readability assessment applications, two very similar data sets are obtained, with calculations of Pearson correlations of at least 87 %, and conclude that the simple crowdsourcing results area viable alternative to the assessments resulting from expert labelings.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 130, "end_pos": 150, "type": "METRIC", "confidence": 0.9717361032962799}]}, {"text": "In section 2, we describe the data from language experts and how those data can be converted to relative assessments.", "labels": [], "entities": []}, {"text": "Section 3 outlines a simpler crowsourcing application and its correspondences with the experts.", "labels": [], "entities": []}, {"text": "Finally, in section 4, we draw conclusions and give a short summary of future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Kappa statistics for all the different setups. The  second column shows the number of times a text pair  must have been labeled in order to be taken into account.", "labels": [], "entities": []}]}