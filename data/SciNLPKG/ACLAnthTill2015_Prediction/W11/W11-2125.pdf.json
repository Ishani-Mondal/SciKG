{"title": [{"text": "A Minimally Supervised Approach for Detecting and Ranking Document Translation Pairs", "labels": [], "entities": [{"text": "Detecting", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.97465580701828}, {"text": "Ranking Document Translation Pairs", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.8011263161897659}]}], "abstractContent": [{"text": "We describe an approach for generating a ranked list of candidate document translation pairs without the use of bilingual dictionary or machine translation system.", "labels": [], "entities": [{"text": "generating a ranked list of candidate document translation pairs", "start_pos": 28, "end_pos": 92, "type": "TASK", "confidence": 0.6750382648573982}]}, {"text": "We developed this approach as an initial, filtering step, for extracting parallel text from large, multilingual-but non-parallel-corpora.", "labels": [], "entities": []}, {"text": "We represent bilingual documents in a vector space whose basis vectors are the overlapping tokens found in both languages of the collection.", "labels": [], "entities": []}, {"text": "Using this representation , weighted by tf\u00b7idf, we compute cosine document similarity to create a ranked list of candidate document translation pairs.", "labels": [], "entities": []}, {"text": "Unlike cross-language information retrieval, where a ranked list in the target language is evaluated for each source query, we are interested in, and evaluate, the more difficult task of finding translated document pairs.", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 7, "end_pos": 43, "type": "TASK", "confidence": 0.7043768564860026}]}, {"text": "We first perform a feasibility study of our approach on parallel collections in multiple languages, representing multiple language families and scripts.", "labels": [], "entities": []}, {"text": "The approach is then applied to a large bilingual collection of around 800k books.", "labels": [], "entities": []}, {"text": "To avoid the computational cost of) (2 n O document pair comparisons, we employ locality sensitive hashing (LSH) approximation algorithm for cosine similarity, which reduces our time complexity to) log (n n O .", "labels": [], "entities": []}], "introductionContent": [{"text": "A dearth of parallel data has been, and still is, a major problem for developing highly reliable statistical machine translation systems in many languages and domains.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.6240528921286265}]}, {"text": "There have been many proposed approaches for alleviating this problem by utilizing techniques for creating and extracting parallel documents, sentences or phrases from comparable bilingual data available on the open web, such as Wikipedia articles), to name a few, or through digitized archives from various sources (), (Munteanu and).", "labels": [], "entities": []}, {"text": "In general, in the process of utilizing comparable corpora to obtain sentence-aligned bilingual text, the first step involves performing initial filtering where text entities from both language collections are compared to each other and based on comparison score they are matched and grouped as potential translation candidate pairs.", "labels": [], "entities": []}, {"text": "After this initial step, text entity pairs or tuples are further analyzed in order to extract parallel sentence pairs.", "labels": [], "entities": []}, {"text": "In this paper we only focus on this initial step.", "labels": [], "entities": []}, {"text": "We present a novel exploration of approaches that retrieve actual document translation pairs without the use of any bilingual resources such as lexicons or sentence aligned bitext.", "labels": [], "entities": []}, {"text": "Rather than solving separate retrieval or translation problems for each source language document, we retrieve translation pairs from the space of all possible bilingual document pairs.", "labels": [], "entities": []}, {"text": "Most machine translation (MT) and information retrieval (IR) systems rely on conditional probabilities; in contrast, we require comparable scores or probabilities overall document pairs.", "labels": [], "entities": [{"text": "machine translation (MT) and information retrieval (IR)", "start_pos": 5, "end_pos": 60, "type": "TASK", "confidence": 0.8115138113498688}]}, {"text": "To avoid directly computing the similarity of all pairs, we use a randomized approximation algorithm based on locality sensitive hashing (LSH).", "labels": [], "entities": []}, {"text": "For this joint approach, we represent each document in both languages using an n-dimensional feature vector template which consists of the set of intersecting words which are found across all documents in both language collections.", "labels": [], "entities": []}, {"text": "For each dimension i.e. word, in the feature vector template we calculate tf\u00b7idf score for the given document.", "labels": [], "entities": []}, {"text": "Unlike other approaches, where documents or their word representations are first translated from foreign language to English using bilingual dictionary (), (Munteanu and and) in our approach we don't utilize any existing MT type artifact.", "labels": [], "entities": [{"text": "MT type artifact", "start_pos": 221, "end_pos": 237, "type": "TASK", "confidence": 0.883370021979014}]}, {"text": "In other words, fora given language pair we don't use translation lexicon by training an existing statistical machine translation system using sentence aligned parallel bilingual data in the same language or existing translation lexicon.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6900438467661539}]}, {"text": "Earlier work done by uses only hapax words to represent and rank (based on the overlap number) translation documents pair in a parallel bilingual collection which is an easier task to evaluation due to the presence of a one-to-one matching among the bilingual documents.", "labels": [], "entities": []}, {"text": "Most recently, show an improvement over this method by using an IR system to first retrieve translation document candidates and then identify translation document pairs by training a classifier.", "labels": [], "entities": []}, {"text": "We start off by giving detailed explanation of the above mentioned data representation.", "labels": [], "entities": []}, {"text": "We then test the feasibility of our approach using aligned parallel document data from three different bilingual collections in several languages and writing systems.", "labels": [], "entities": []}, {"text": "Results from these tests are given in section 3.", "labels": [], "entities": []}, {"text": "The goal of developing our approach was to utilize it as an initial filtering step in developing parallel corpora from large, multilingual collections, such as the collection of more than 800K English and German books we describe in section 4.", "labels": [], "entities": []}, {"text": "Since we start with no information on the possible translation pairs in our large collection and in order to verify the potential of our method, we first show results on retrieving 17 known parallel book pairs embedded in a small randomly selected subset of 1K books (section 4.1).", "labels": [], "entities": []}, {"text": "Since performing cosine similarity across all document pairs is computationally expensive with time complexity of ) ( 2 n O we utilize the LSH based approximation algorithm for the cosine similarity measurement based on the work by Ravichandran et. al.", "labels": [], "entities": []}, {"text": "A brief overview of this approach is given in Section 5, which is followed by our implementation results explained and analyzed in section 6.", "labels": [], "entities": []}, {"text": "To conclude the paper, we give a brief outlook on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We start off by evaluating the above proposed approach of determining candidate document translation pairs using three different parallel collections: Europarl, created by, UN Arabic English Parallel Text (LDC2004E13) and the Arabic News Translation Part 1 (LDC2004T17).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 151, "end_pos": 159, "type": "DATASET", "confidence": 0.9645699262619019}, {"text": "UN Arabic English Parallel Text (LDC2004E13)", "start_pos": 173, "end_pos": 217, "type": "DATASET", "confidence": 0.8045032843947411}, {"text": "Arabic News Translation Part 1 (LDC2004T17)", "start_pos": 226, "end_pos": 269, "type": "DATASET", "confidence": 0.7415385320782661}]}, {"text": "The purposes of first testing our approach using the Europarl corpus were twofold: This collection contains parallel documents (sessions of the European Parliament) that are further aligned at the speech and sentence level, which allows us to test alignment accuracy at several levels of granularity.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.990249514579773}, {"text": "accuracy", "start_pos": 258, "end_pos": 266, "type": "METRIC", "confidence": 0.9599541425704956}]}, {"text": "Second, this collection contains parallel data from different groups of languages (Germanic, Romance, Slavic, Hellenic, etc.) and therefore is useful to observe the performance of our approach across different language families, which in turn are important to observe the difference in the cognate rates and the size of the overlapping words.", "labels": [], "entities": []}, {"text": "In addition to the Europarl corpus we use the two English-Arabic parallel collections to test our approach across various alphabets (Arabic in addition to the Latin, Greek and Cyrillic found in the Europarl collection).", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.9758783876895905}, {"text": "Europarl collection", "start_pos": 198, "end_pos": 217, "type": "DATASET", "confidence": 0.9524238109588623}]}], "tableCaptions": [{"text": " Table 1. Document length statistics over 6 Parallel  Collections.", "labels": [], "entities": []}, {"text": " Table 2. Number of overlapping words (vector  template length) in the six parallel collections.", "labels": [], "entities": []}, {"text": " Table 3. Bilingual book collection statistics.", "labels": [], "entities": [{"text": "Bilingual book collection", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6875985960165659}]}, {"text": " Table 5. Average precision on the large English- German book collection across various parameters  of the LSH based search algorithm.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9933072924613953}, {"text": "English- German book collection", "start_pos": 41, "end_pos": 72, "type": "DATASET", "confidence": 0.7229822874069214}]}]}