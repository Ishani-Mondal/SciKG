{"title": [{"text": "Learning the Optimal use of Dependency-parsing Information for Finding Translations with Comparable Corpora", "labels": [], "entities": [{"text": "Finding Translations", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7318292558193207}]}], "abstractContent": [{"text": "Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically.", "labels": [], "entities": []}, {"text": "The basic idea is based on the assumption that similar words have similar contexts across languages.", "labels": [], "entities": []}, {"text": "The context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. the predecessors and successors.", "labels": [], "entities": []}, {"text": "These different context positions are then combined into one context vector and compared across languages.", "labels": [], "entities": []}, {"text": "However, previous research makes the (implicit) assumption that these different context positions should be weighted as equally important.", "labels": [], "entities": []}, {"text": "Furthermore, only the same context positions are compared with each other, for example the successor position in Spanish is compared with the successor position in English.", "labels": [], "entities": []}, {"text": "However, this is not necessarily always appropriate for languages like Japanese and English.", "labels": [], "entities": []}, {"text": "To overcome these limitations, we suggest to perform a linear transformation of the context vectors , which is defined by a matrix.", "labels": [], "entities": []}, {"text": "We define the optimal transformation matrix by using a Bayesian probabilistic model, and show that it is feasible to find an approximate solution using Markov chain Monte Carlo methods.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that our proposed method constantly improves translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.9485587477684021}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.823942244052887}]}], "introductionContent": [{"text": "Using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular ().", "labels": [], "entities": []}, {"text": "The general idea is based on the assumption that similar words have similar contexts across languages.", "labels": [], "entities": []}, {"text": "The context of a word can be described by the sentence in which it occurs) or a surrounding word-window.", "labels": [], "entities": []}, {"text": "A few previous studies, like (), suggested to use the predecessor and successors from the dependency-parse tree, instead of a word window.", "labels": [], "entities": []}, {"text": "In (Andrade et al., 2011), we showed that including dependency-parse tree context positions together with a sentence bag-of-words context can improve word translation accuracy.", "labels": [], "entities": [{"text": "word translation", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.7827110588550568}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.6759949326515198}]}, {"text": "However previous works do not make an attempt to find an optimal combination of these different context positions.", "labels": [], "entities": []}, {"text": "Our study tries to find an optimal weighting and aggregation of these context positions by learning a linear transformation of the context vectors.", "labels": [], "entities": []}, {"text": "The motivation is that different context positions might be of different importance, e.g. the direct predecessors and successors from the dependency tree might be more important than the larger context from the whole sentence.", "labels": [], "entities": []}, {"text": "Another motivation is that dependency positions cannot be always compared across different languages, e.g. a word which tends to occur as a modifier in English, can tend to occur in Japanese in a different dependency position.", "labels": [], "entities": []}, {"text": "As a solution, we propose to learn the optimal combination of dependency and bag-of-words sentence information.", "labels": [], "entities": []}, {"text": "Our approach uses a linear transformation of the context vectors, before comparing them using the cosine similarity.", "labels": [], "entities": []}, {"text": "This can be considered as a generalization of the cosine similarity.", "labels": [], "entities": []}, {"text": "We define the optimal transformation matrix by the maximum-a-posterior (MAP) solution of a Bayesian probabilistic model.", "labels": [], "entities": []}, {"text": "The likelihood function fora translation matrix is defined by considering the expected achieved translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9670463800430298}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8892637491226196}]}, {"text": "As a prior, we use a Dirichlet distribution over the diagonal elements in the matrix and a uniform distribution over its non-diagonal elements.", "labels": [], "entities": []}, {"text": "We show that it is feasible to find an approximation of the optimal solution using Markov chain Monte Carlo (MCMC) methods.", "labels": [], "entities": []}, {"text": "In our experiments, we compare the proposed method, which uses this approximation, with the baseline method which uses the cosine similarity without any linear transformation.", "labels": [], "entities": []}, {"text": "Our experiments show that the translation accuracy is constantly improved by the proposed method.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9380707144737244}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9315338134765625}]}, {"text": "In the next section, we briefly summarize the most relevant previous work.", "labels": [], "entities": []}, {"text": "In Section 3, we then explain the baseline method which is based on previous research.", "labels": [], "entities": []}, {"text": "Section 4 explains in detail our proposed method, followed by Section 5 which provides an empirical comparison to the baseline, and analysis.", "labels": [], "entities": []}, {"text": "We summarize our findings in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments of the present study, we used a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT) 2 and another collection of complaints concerning automobiles compiled by the USA National Highway Traffic Safety Administration (NHTSA) . Both corpora are publicly available.", "labels": [], "entities": [{"text": "Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT)", "start_pos": 115, "end_pos": 186, "type": "TASK", "confidence": 0.5049926065481626}, {"text": "USA National Highway Traffic Safety Administration (NHTSA)", "start_pos": 265, "end_pos": 323, "type": "DATASET", "confidence": 0.7275969584782919}]}, {"text": "The corpora are non-parallel, but are comparable in terms of content.", "labels": [], "entities": []}, {"text": "The part of MLIT and NHTSA which we used for our experiments, contains 24090 and 47613 sentences, respectively.", "labels": [], "entities": [{"text": "MLIT", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.9105122685432434}, {"text": "NHTSA", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.9710913896560669}]}, {"text": "The Japanese MLIT corpus was morphologically analyzed and dependency parsed using Juman and KNP 4 . The English corpus NHTSA was POS-tagged and stemmed with Stepp Tagger ( and dependency parsed using the MST parser).", "labels": [], "entities": [{"text": "MLIT corpus", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.7100149542093277}, {"text": "English corpus NHTSA", "start_pos": 104, "end_pos": 124, "type": "DATASET", "confidence": 0.6057347357273102}]}, {"text": "Using the Japanese-English dictionary JMDic 5 , we found 1796 content words in Japanese which have a translation which is in the English corpus.", "labels": [], "entities": [{"text": "Japanese-English dictionary JMDic 5", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.6844252049922943}]}, {"text": "These content words and their translations correspond to our pivot words in Japanese and English, respectively.", "labels": [], "entities": []}, {"text": "6  For the evaluation we extract a gold-standard which contains Japanese and English noun pairs that actually occur in both corpora.", "labels": [], "entities": []}, {"text": "The gold-standard is created with the help of the JMDic dictionary, whereas we correct apparently inappropriate translations, and remove general nouns such as (possibility) and ambiguous words such as (rice, America).", "labels": [], "entities": [{"text": "JMDic dictionary", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9420193731784821}]}, {"text": "In this way, we obtain a final list of 443 domain-specific Japanese nouns.", "labels": [], "entities": []}, {"text": "Each Japanese noun in the gold-standard corresponds to one pair of the form <Japanese noun (query), English translations (answers)>.", "labels": [], "entities": []}, {"text": "We divide the gold-standard into two halves.", "labels": [], "entities": []}, {"text": "The first half is used for for learning the matrix A, the second part is used for the evaluation.", "labels": [], "entities": []}, {"text": "In general, we expect that the optimal transformation matrix A depends mainly on the languages (Japanese and English) and on the corpora (MLIT and NHTSA).", "labels": [], "entities": [{"text": "MLIT", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.6717584729194641}, {"text": "NHTSA", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.8483405709266663}]}, {"text": "However, in practice, the optimal matrix can also vary depending on the part of the gold-standard which is used for training.", "labels": [], "entities": []}, {"text": "These random variations are especially large, if the part of the gold-standard which is used for training or testing is small.", "labels": [], "entities": []}, {"text": "In order to take these random effects into account, we perform repeated subsampling of the gold-standard.", "labels": [], "entities": []}, {"text": "In detail, we randomly split the goldstandard into equally-sized training and test set.", "labels": [], "entities": []}, {"text": "This is repeated five times, leading to five training and five test sets.", "labels": [], "entities": []}, {"text": "The performance on each test set is shown in.", "labels": [], "entities": []}, {"text": "OPTIMIZED-ALL marks the result of our proposed method, where matrix A is optimized using the training set.", "labels": [], "entities": [{"text": "OPTIMIZED-ALL", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.862497091293335}]}, {"text": "The optimization of the diagonal elements d 1 , . .", "labels": [], "entities": []}, {"text": ", d 4 , and the non-diagonal value z is as described in Section 4.2.", "labels": [], "entities": []}, {"text": "Finally, the baseline method, as described in 3, corresponds to OPTIMIZED-ALL where d 1 , . .", "labels": [], "entities": [{"text": "OPTIMIZED-ALL", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9867520332336426}]}, {"text": ", d 4 are set to 1, and z is set to 0.", "labels": [], "entities": []}, {"text": "This baseline is denoted as NOR-MAL.", "labels": [], "entities": [{"text": "NOR-MAL", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.5128128528594971}]}, {"text": "We can see that the overall translation accuracy varies across the test sets.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9030826091766357}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.8193888664245605}]}, {"text": "However, we see that in all test sets our proposed method OPTIMIZED-ALL performs better than the baseline NORMAL.", "labels": [], "entities": [{"text": "OPTIMIZED-ALL", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9752116799354553}, {"text": "NORMAL", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.611217737197876}]}], "tableCaptions": [{"text": " Table 1: Shows the accuracy at different ranks for all test sets, and, in the last column, the average over all test sets.  The proposed method OPTIMIZED-ALL is compared to the baseline NORMAL. Furthermore, for analysis, the results  when optimizing only the diagonal are marked as OPTIMIZED-DIAG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995724558830261}, {"text": "OPTIMIZED-ALL", "start_pos": 145, "end_pos": 158, "type": "METRIC", "confidence": 0.837521493434906}, {"text": "NORMAL", "start_pos": 187, "end_pos": 193, "type": "DATASET", "confidence": 0.8028663992881775}]}, {"text": " Table 2: Shows the parameters which were learned using each training set. d 1 . . . d 4 are the weights of the context  positions, which sum up to 1. z marks the degree to which it is useful to compare context across different positions.", "labels": [], "entities": []}, {"text": " Table 3: The proposed method, but without the sentence  information in the context vector, is denoted OPT-DEP.  The baseline method, but without the sentence informa- tion in the context vector, is denoted NOR-DEP.", "labels": [], "entities": [{"text": "OPT-DEP", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.6504291296005249}, {"text": "NOR-DEP", "start_pos": 207, "end_pos": 214, "type": "DATASET", "confidence": 0.7887536287307739}]}]}