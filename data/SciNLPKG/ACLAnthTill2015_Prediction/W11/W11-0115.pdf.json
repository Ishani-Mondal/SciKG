{"title": [{"text": "Computing Semantic Compositionality in Distributional Semantics", "labels": [], "entities": [{"text": "Semantic Compositionality", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6910588443279266}]}], "abstractContent": [{"text": "This article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of Distributional Semantics and supervised Machine Learning.", "labels": [], "entities": [{"text": "semantic compositionality", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7211223542690277}]}, {"text": "In brief, distributional semantic spaces containing representations for complex constructions such as Adjective-Noun and Verb-Noun pairs, as well as for their constituent parts, are built.", "labels": [], "entities": []}, {"text": "These representations are then used as feature vectors in a supervised learning model using multivariate multiple regression.", "labels": [], "entities": []}, {"text": "In particular, the distributional semantic representations of the constituents are used to predict those of the complex structures.", "labels": [], "entities": []}, {"text": "This approach outperforms the rivals in a series of experiments with Adjective-Noun pairs extracted from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.9458190202713013}]}, {"text": "Ina second experimental setting based on Verb-Noun pairs, a comparatively much lower performance was obtained by all the models; however, the proposed approach gives the best results in combination with a Random Indexing semantic space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probably the most important missing ingredient from the current NLP state-of-the-art is the ability to compute the meaning of complex structures, i.e. semantically compositional structures.", "labels": [], "entities": []}, {"text": "In this paper, I propose a methodological approach and a series of experiments designed to teach computers the ability to compute the compositionality of (relatively simple) complex linguistic structures.", "labels": [], "entities": []}, {"text": "This work uses a combination of Distributional Semantics and Machine Learning techniques.", "labels": [], "entities": []}, {"text": "The starting data in the experiments reported below are multidimensional vectorial semantic representations extracted from electronic corpora.", "labels": [], "entities": []}, {"text": "This work extends the basic methodology presented in with new data collection techniques, improved evaluation metrics and new case studies.", "labels": [], "entities": []}, {"text": "Compositionality is probably one of the defining properties of human language and, perhaps, a nearly uncontroversial notion among linguists.", "labels": [], "entities": []}, {"text": "One of the best-known formulations of compositionality is: (1) The Principle of Compositionality: The meaning of a complex expression is a function of the meaning of its parts and of the syntactic rules by which they are combined.", "labels": [], "entities": []}, {"text": "318) The Principle of Compositionality is a standard notion in many different fields of research, notably in logic, in philosophy of language, in linguistics and in computer science; this intrinsic multi-disciplinarity makes tracing back its recent history somewhat difficult.", "labels": [], "entities": []}, {"text": "The recent years have witnessed an ever-increasing interest in techniques that enable computers to automatically extract semantic information from linguistic corpora.", "labels": [], "entities": []}, {"text": "In this paper I will refer to this new field in general as Distributional Semantics.", "labels": [], "entities": [{"text": "Distributional Semantics", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8690029978752136}]}, {"text": "Distributional Semantics, in short, extracts spatial representations of meaning from electronic corpora by using distributional (i.e. statistical) patterns of word usage.", "labels": [], "entities": [{"text": "Distributional Semantics", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6965791285037994}]}, {"text": "The main hypothesis in Distributional Semantics is the so-called distributional hypothesis of meaning, expressing the fact that \"words that occur in the same contexts tend to have similar meanings\" ().", "labels": [], "entities": []}, {"text": "The distributional hypothesis of meaning is ascribed to Zellig Harris, who proposed a general distributional methodology for linguistics.", "labels": [], "entities": []}, {"text": "Since representations in Distributional Semantics are spatial in nature (e.g. vectors representing points in a multidimensional space), differences in meaning are captured through differences in location: in the multidimensional space, two semantically (i.e. distributionally) similar words are closer than two words that are dissimilar.", "labels": [], "entities": []}, {"text": "See and for detailed overviews of the methodology and applications of Distributional Semantics.", "labels": [], "entities": [{"text": "Distributional Semantics", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8556524515151978}]}, {"text": "2 Compositionality in distributional semantics: state-of-the-art I stressed above that computers are still notable to deal with the compositionality of meaning.", "labels": [], "entities": []}, {"text": "However basically true, this statement should be qualified somewhat.", "labels": [], "entities": []}, {"text": "Previous work in the field has produced a small number of operations to approximate the composition of vectorial representations of word meaning.", "labels": [], "entities": []}, {"text": "In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by one of the following four basic operations: vector addition, vector pointwise-multiplication, tensor product or linear regression.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 156, "end_pos": 171, "type": "TASK", "confidence": 0.6964870691299438}]}, {"text": "In the literature on Information Retrieval, vector addition is the standard approach to model the composed meaning of a group of words (or a document) as the sum of their vectors (see, among many others, Widdows, 2004: ch. 5).", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7233906239271164}, {"text": "vector addition", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.739654541015625}]}, {"text": "More schematically: (2) Vector addition: Given two independent vectors v1 and v2, the compositional meaning of v3 consists of the sum of the corresponding components of the original vectors.", "labels": [], "entities": []}, {"text": "Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and pointwise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by.", "labels": [], "entities": []}, {"text": "While the additive model captures the compositionality of meaning by considering all available components, multiplicative models only operate on a subset of them, i.e. non-zero components.", "labels": [], "entities": []}, {"text": "They claim that when we pointwise-multiply the vectors representing two words, we obtain an output that captures their composition; actually, this operation is keeping in the output only the components which had corresponding non-zero values: whether this operation has any relation with semantics is still unclear.", "labels": [], "entities": []}, {"text": "However, in their experiments, Mitchell and Lapata prove that the pointwise-multiplicative model and the weighted combination of the additive and the multiplicative models perform equally well.", "labels": [], "entities": []}, {"text": "Of these, only the simple multiplicative model will be tested in the experiments I present in the following section.", "labels": [], "entities": []}, {"text": "Each corresponding pair of components of v1 and v2 is multiplied to obtain the corresponding component of v3.", "labels": [], "entities": []}, {"text": "Widdows proposes to apply a number of more complex vector operations imported from quantum mechanics to model composition in semantic spaces, in particular tensor product and the related operation of convolution product.", "labels": [], "entities": []}, {"text": "obtains results indicating that both the tensor product and the convolution product perform better than the simple additive model in two small experiments (relation extraction and phrasal composition).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.8201402723789215}]}, {"text": "Giesbrecht (2009) presents a more complex task, singling out non-compositional multiword expressions.", "labels": [], "entities": []}, {"text": "Her results clearly show that tensor product outperforms vector addition, multiplication and convolution.", "labels": [], "entities": []}, {"text": "where v3 is a matrix whose ij-th entry is equal to v1 i \u00d7 v2 j However, since the tensor product (also called outer product) of two vectors produces a result with higher dimensionality (a matrix), it cannot be directly compared against the other methods, which instead generate compositional representations in the same original space.", "labels": [], "entities": []}, {"text": "In the experiments reported in the following section, we will use the circular convolution composition method: in brief, circular convolution is a mathematical operation that effectively compresses the tensor product of two vectors onto the original space, thus allowing us to compare its outcome with that of the other methods here reviewed.", "labels": [], "entities": []}, {"text": "where v3 = n\u22121 It is interesting to note that a great deal of attention has recently been devoted to the tensor product as the basic operation for modelling compositionality, even at the sentential level (e.g., through a combination of mathematical operations and symbolic models of logic (inspired by.", "labels": [], "entities": []}, {"text": "Although extremely motivating and thought provoking, these proposals have not been tested on empirical grounds yet.", "labels": [], "entities": []}, {"text": "A common thread ties all the approaches briefly outlined above: all information that is present in the systems is conveyed by the vectors v1 and v2, e.g. the independent word representations, while completely disregarding v3 (the composed vector).", "labels": [], "entities": []}, {"text": "Furthermore, all of these approaches are based on the application of a single geometric operation on the independent vectors v1 and v2.", "labels": [], "entities": []}, {"text": "It seems highly unlikely that just one geometric operation could reliably represent all the semantic transformations introduced by all syntactic relations in every language.", "labels": [], "entities": []}, {"text": "Guevara and introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector v3.", "labels": [], "entities": []}, {"text": "For example, Guevara collects vector representations for nice and house, but also for the observed pair nice_house.", "labels": [], "entities": []}, {"text": "With these data, a model of Adjective-Noun (AN) compositionality is built by using a supervised machine learning approach: multivariate multiple linear regression analysis by partial least squares.", "labels": [], "entities": [{"text": "Adjective-Noun (AN) compositionality", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.6467721581459045}]}, {"text": "This method is able to learn the transformation function that best approximates v3 on the basis of both v1 and v2.", "labels": [], "entities": []}, {"text": "use a slightly different methodology: assuming that each adjective is a linear transformation function (i.e. the function to be learnt by the algorithm), they model AN compositionality by approximating v3 only on the basis of v2 (the noun) but running a different regression analysis for each adjective in their data.", "labels": [], "entities": [{"text": "AN compositionality", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.9705946445465088}]}, {"text": "The approach proposed by Guevara (2010) is really only an extension of the full additive model of, the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression.", "labels": [], "entities": []}, {"text": "In the following section, I present anew series of experiments that refine, extend and improve this approach to model the compositionality of adjacent AN and VN pairs by linear regression.", "labels": [], "entities": []}, {"text": "where A and B are weight matrices estimated by the supervised learning algorithm using multivariate multiple linear regression.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using a lemmatised and POS tagged version of the BNC, a list of adjacent AN pair candidates was extracted with simple regex-based queries targeting sequences composed of [Det/Art-A-N] (i.e. pairs expressing attributive modification of a nominal head like 'that little house').", "labels": [], "entities": [{"text": "BNC", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9499945044517517}]}, {"text": "In order to ensure the computational attainability of the successive steps, the candidate list was filtered by frequency (> 400) obtaining 1,367 different AN pairs.", "labels": [], "entities": []}, {"text": "A new version of the BNC was then prepared to represent the selected AN lemma pairs as a single token; for example, while in the original BNC the phrase [nice houses] consists in two separate POS-tagged lemmas, nice_AJ and house_NN, in the processed corpus it appears as a single entry nice_AJ_house_NN).", "labels": [], "entities": [{"text": "BNC", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8991820812225342}]}, {"text": "The corpus was also processed by stop-word removal (very high frequency items, mainly functional morphemes).", "labels": [], "entities": []}, {"text": "The re-tokenization process of the BNC enables us to extract independent context vectors for each AN pair in our list (v3) and their corresponding constituents (A and N, respectively v1 and v2), while ensuring that the extracted vectors do not contain overlapping information.", "labels": [], "entities": []}, {"text": "The same preprocessing steps were carried out to extract VN pair candidates.", "labels": [], "entities": [{"text": "VN pair candidates", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.8552600542704264}]}, {"text": "Sequences composed of [V-(Det/Art)-N] with an optional determiner were targeted and filtered by frequency (> 400), resulting in a first list of 545 VN pairs.", "labels": [], "entities": []}, {"text": "This list contained a large amount of noise due to lemmatisation and POS-tagging problems (e.g. housing association), and it also contained many very frequent lexicalized items (e.g. thank goodness).", "labels": [], "entities": []}, {"text": "The list was manually cleaned, resulting in 193 different VN pairs.", "labels": [], "entities": []}, {"text": "The evaluation of models of compositionality is still a very uncertain and problematic issue.", "labels": [], "entities": []}, {"text": "Previous work has relied mainly on \"external\" tasks such as rating sentence similarity or detection idioms.", "labels": [], "entities": [{"text": "rating sentence similarity", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.7387008468310038}]}, {"text": "These evaluation strategies are \"external\" in the sense that each compared model produces a set of predictions which are then used in order to reproduce human annotation of datasets that do not have a representation in the semantic space under consideration.", "labels": [], "entities": []}, {"text": "For example, use their models to approximate the human ratings in their sentence similarity dataset.", "labels": [], "entities": []}, {"text": "Giesbrecht (2009) also uses human annotated data (manually classified collocations, compositional and non-compositional) in her evaluation task.", "labels": [], "entities": []}, {"text": "However, any evaluation task requiring hand-annotated datasets will have a considerable cost in resource building.", "labels": [], "entities": []}, {"text": "At present time, there are no suitable datasets in the public domain.", "labels": [], "entities": []}, {"text": "I propose instead to take a radically different point of view, developing \"internal\" evaluation tasks that try to measure how well the proposed models approximate the distributional patterns of corpusextracted composed vectors.", "labels": [], "entities": []}, {"text": "That is to say, I want to compare the predicted output of every model (i.e. a predicted context vector for v3) with the real observation of v3 that was collected from the corpus.", "labels": [], "entities": []}, {"text": "The following subsections present a few experimental evaluation methods based on neighbour analysis and on the Euclidean measure of distance.", "labels": [], "entities": [{"text": "Euclidean measure of distance", "start_pos": 111, "end_pos": 140, "type": "METRIC", "confidence": 0.6225154101848602}]}, {"text": "The evaluation strategies here presented rests on the sensible assumption that if a model of AN compositionality is reliable, its predicted output for any AN pair, e.g. weird_banana, should be in principle usable as a substitute for the corresponding corpus-attested AN vector.", "labels": [], "entities": []}, {"text": "Moreover, if such a model performs acceptably, it could even be used predict the compositionality of unattested candidates like shadowy_banana: this kind of operations is the key to attaining human-like semantic performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Adjective-Noun pairs. Mantel tests of correlation (max. correlation = 1)", "labels": [], "entities": [{"text": "correlation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9832488298416138}, {"text": "correlation", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.592163622379303}]}, {"text": " Table 2: Verb-Noun pairs. Mantel tests of correlation (max. correlation = 1)", "labels": [], "entities": [{"text": "correlation", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9875695109367371}, {"text": "correlation", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.5674893260002136}]}, {"text": " Table 3: AN pairs. Observation-based neighbour analysis (max. score = 367)", "labels": [], "entities": [{"text": "Observation-based neighbour analysis", "start_pos": 20, "end_pos": 56, "type": "METRIC", "confidence": 0.8478642304738363}]}, {"text": " Table 4: AN pairs. Prediction-based neighbour analysis (max. score = 367)", "labels": [], "entities": [{"text": "Prediction-based neighbour analysis", "start_pos": 20, "end_pos": 55, "type": "METRIC", "confidence": 0.8007937868436178}]}, {"text": " Table 6: AN pairs. Gold-standard comparison of shared neighbours (max. score = 7340)", "labels": [], "entities": []}, {"text": " Table 7: VN pairs. Gold-standard comparison of shared neighbours (max. score = 1200)", "labels": [], "entities": []}]}