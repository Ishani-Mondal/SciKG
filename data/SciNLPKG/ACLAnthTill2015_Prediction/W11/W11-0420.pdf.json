{"title": [], "abstractContent": [{"text": "We describe an experiment on a temporal ordering task in this paper.", "labels": [], "entities": [{"text": "temporal ordering task", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7715081671873728}]}, {"text": "We show that by selecting event pairs based on discourse structure and by modifying the pre-existent temporal classification scheme to fit the data better , we significantly improve inter-annotator agreement, as well as broaden the coverage of the task.", "labels": [], "entities": []}, {"text": "We also present analysis of the current temporal classification scheme and propose ways to improve it in future work.", "labels": [], "entities": [{"text": "temporal classification", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7042434960603714}]}], "introductionContent": [{"text": "Event-based temporal inference is a fundamental natural language technology aimed at determining the temporal anchoring and relative temporal ordering between events in text.", "labels": [], "entities": [{"text": "Event-based temporal inference", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6044632395108541}, {"text": "temporal anchoring", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7133036553859711}]}, {"text": "It supports a wide range of natural language applications such as Information Extraction, Question Answering () and Text Summarization ().", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8338025808334351}, {"text": "Question Answering", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8116089999675751}, {"text": "Text Summarization", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8162631988525391}]}, {"text": "Creating consistently annotated domain-independent data sufficient to train automatic systems has been the bottleneck.", "labels": [], "entities": []}, {"text": "While low-level temporal annotation tasks such as identifying events and time expressions are relatively straightforward and can be done with high consistency, high-level tasks necessary to eventually arrange events in a document in a temporal order have proved to be much more challenging.", "labels": [], "entities": []}, {"text": "Among these high-level tasks, the task of annotating the temporal relation between main events stands out as probably the most challenging.", "labels": [], "entities": [{"text": "annotating the temporal relation between main events", "start_pos": 42, "end_pos": 94, "type": "TASK", "confidence": 0.6880200122083936}]}, {"text": "This task was the only task in the TempEval campaigns to deal with inter-sentential temporal relations, and also the only one to directly tackle event ordering.", "labels": [], "entities": [{"text": "event ordering", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.7094724476337433}]}, {"text": "The idea is that events covered in an article are scattered in different sentences, with some, presumably important ones, expressed as predicates in prominent positions of a sentence (i.e. the \"main event\" of the sentence).", "labels": [], "entities": []}, {"text": "By relating main events from different sentences of an article temporally, one could get something of a chain of important events from the article.", "labels": [], "entities": []}, {"text": "This task, in both previously reported attempts, one for English () and the other for Chinese, has the lowest inter-annotator agreement (at 65%) among all tasks focusing on annotating temporal relations.", "labels": [], "entities": [{"text": "agreement", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.6007608771324158}]}, {"text": "attribute the difficulty, shared by all tasks annotating temporal relations, mainly to two factors: rampant temporal vagueness in natural language and the fact that annotators are not allowed to skip hard-to-classify cases.", "labels": [], "entities": []}, {"text": "Xue and Zhou (2010) take a closer look at this task specifically.", "labels": [], "entities": []}, {"text": "They report that part of the difficulty comes from \"wrong\" main events (in the sense that they are not main events in the intended sense) being selected in the preparation step.", "labels": [], "entities": []}, {"text": "This step is a separate task upstream of the temporal relation task.", "labels": [], "entities": []}, {"text": "The \"wrong\" main events produced in this step become part of event pairs whose temporal relation it makes no sense to annotate, and often is hard-toclassify.", "labels": [], "entities": []}, {"text": "The reason \"wrong\" main events get selected is because the selection is based on syntactic criteria.", "labels": [], "entities": []}, {"text": "In fact, these syntactic criteria produce results so counter-intuitive that this seemingly simple preparation task only achieves 74% inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Another part of the difficulty comes from mechanical pairing of main events for temporal relation annotation.", "labels": [], "entities": [{"text": "temporal relation annotation", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.5880411167939504}]}, {"text": "Simply pairing up main events from adjacent sentences oversimplifies the structure within an article and is prone to produce hard-to-classify cases for temporal relation annotation.", "labels": [], "entities": [{"text": "temporal relation annotation", "start_pos": 152, "end_pos": 180, "type": "TASK", "confidence": 0.6210694213708242}]}, {"text": "Both causes point to the need fora deeper level of text analysis to inform temporal annotation.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.7036113888025284}]}, {"text": "For this, Xue and Zhou (2010) suggest introduction of discourse structure as annotated in the Penn Discourse Treebank (PDTB) into temporal relation annotation.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 94, "end_pos": 124, "type": "DATASET", "confidence": 0.9584856033325195}]}, {"text": "So the previous two reports, taken together, seem to suggest that the reason this task is especially challenging is because the difficulty associated with temporal vagueness in natural language, which is shared by all tasks dealing with temporal relation, is compounded by the problem of having to annotate farfetched pairs that should not be annotated, which is unique for the only task dealing with inter-sentential temporal relations.", "labels": [], "entities": []}, {"text": "These two problems are the foci of our experiment done on Chinese data.", "labels": [], "entities": [{"text": "Chinese data", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.7913661599159241}]}, {"text": "The paper is organized as follows: In Section 2, we describe the annotation scheme; in Section 3, we describe the annotation procedure; in Section 4 we report and discuss the experiment results.", "labels": [], "entities": []}, {"text": "And finally we conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The value \"groupie\" may also seem troublesome if we look at mis-classification as a percentage of its total occurrences, however, it may not be as bad as it seems.", "labels": [], "entities": []}, {"text": "As pointed out in Section 2.2.2, despite the fact that the linguistic phenomenon this value captures can, and does, co-occur with temporal relations represented by other values, we had to set it up as an opposing value to the rest due to technical restrictions.", "labels": [], "entities": []}, {"text": "If/when this value is setup as a stand-alone feature to capture the linguistic phenomenon fully, the percentage of mis-classification should drop significantly because the number of total occurrences will increase dramatically.", "labels": [], "entities": []}, {"text": "The overall distribution of values shown in is very skewed.", "labels": [], "entities": []}, {"text": "At one end of the distribution spectrum is the value \"overlap\", covering half of the data; at the other end are the values \"not-before\" and \"not-after\", covering less than 0.3% of the token combined.", "labels": [], "entities": [{"text": "overlap", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9383187294006348}]}, {"text": "It raises the question if such a classification scheme is well-designed to produce data useful for machine learning.", "labels": [], "entities": []}, {"text": "To shed light on what is behind the numbers and to uncover trends that numbers do not show, we also take a closer look at the annotation data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement over 10 weeks of  training.", "labels": [], "entities": []}, {"text": " Table 2: Confusion matrix on annotation from Weeks  7-10: a=after; b=before; o=overlap; na=not-after;  nb=not-before; g=groupie; i=irrelevant.", "labels": [], "entities": [{"text": "Confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.917176365852356}]}]}