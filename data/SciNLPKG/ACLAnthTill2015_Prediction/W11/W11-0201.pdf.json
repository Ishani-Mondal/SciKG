{"title": [{"text": "Not all links are equal: Exploiting Dependency Types for the Extraction of Protein-Protein Interactions from Text", "labels": [], "entities": [{"text": "Extraction of Protein-Protein Interactions from Text", "start_pos": 61, "end_pos": 113, "type": "TASK", "confidence": 0.7557750443617502}]}], "abstractContent": [{"text": "The extraction of protein-protein interactions (PPIs) reported in scientific publications is one of the most studied topics in Text Mining in the Life Sciences, as such algorithms can substantially decrease the effort for databases curators.", "labels": [], "entities": [{"text": "extraction of protein-protein interactions (PPIs)", "start_pos": 4, "end_pos": 53, "type": "TASK", "confidence": 0.8377263971737453}, {"text": "Text Mining", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.7766425013542175}]}, {"text": "The currently best methods for this task are based on analyzing the dependency tree (DT) representation of sentences.", "labels": [], "entities": []}, {"text": "Many approaches exploit only topological features and thus do not yet fully exploit the information contained in DTs.", "labels": [], "entities": []}, {"text": "We show that incorporating the grammatical information encoded in the types of the dependencies in DTs noticeably improves extraction performance by using a pattern matching approach.", "labels": [], "entities": []}, {"text": "We automatically infer a large set of linguistic patterns using only information about interacting proteins.", "labels": [], "entities": []}, {"text": "Patterns are then refined based on shallow linguistic features and the semantics of dependency types.", "labels": [], "entities": []}, {"text": "Together, these lead to a total improvement of 17.2 percent points in F 1 , as evaluated on five publicly available PPI corpora.", "labels": [], "entities": [{"text": "F 1", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8768233358860016}]}, {"text": "More than half of that improvement is gained by properly handling dependency types.", "labels": [], "entities": []}, {"text": "Our method provides a general framework for building task-specific relationship extraction methods that do not require annotated training data.", "labels": [], "entities": [{"text": "task-specific relationship extraction", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.6669670542081197}]}, {"text": "Furthermore, our observations offer methods to improve upon relation extraction approaches.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8851878941059113}]}], "introductionContent": [{"text": "Insights about protein-protein interactions (PPIs) are vital to understand the biological processes within organisms.", "labels": [], "entities": [{"text": "Insights about protein-protein interactions (PPIs)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6524504593440464}]}, {"text": "Accordingly, several databases, such as IntAct, DIP, or MINT, contain detailed information about PPIs.", "labels": [], "entities": [{"text": "MINT", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.7672855854034424}]}, {"text": "This information is often manually harvested from peer reviewed publications (.", "labels": [], "entities": []}, {"text": "However, it is assumed that a high amount of PPIs is still hidden in publications.", "labels": [], "entities": []}, {"text": "Therefore, the automated extraction of PPIs from text has attracted considerable attention from biology research.", "labels": [], "entities": [{"text": "automated extraction of PPIs from text", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.7800219058990479}]}, {"text": "A number of different techniques have been proposed to solve the problem of extracting PPIs from natural language text.", "labels": [], "entities": [{"text": "extracting PPIs from natural language text", "start_pos": 76, "end_pos": 118, "type": "TASK", "confidence": 0.8385409712791443}]}, {"text": "These can be roughly organized into one of three classes: co-occurrence, machine learning, and pattern matching (for a recent survey, see ().", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7832672297954559}]}, {"text": "The cooccurrence based approaches use only information on the co-existence of protein mentions in a given scope.", "labels": [], "entities": []}, {"text": "They are easy to implement and allow for efficient processing of huge amounts of texts, but they are also prone to generate many false positives because they cannot distinguish positive from negative pairs.", "labels": [], "entities": []}, {"text": "The second class is based on machine learning.", "labels": [], "entities": []}, {"text": "Here, a statistical model is learned from a set of positive and negative examples and then applied to unseen texts.", "labels": [], "entities": []}, {"text": "In general, machine learningbased methods to relation extraction perform very well for any task where sufficient, representative and high quality training data is available ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.9529805183410645}]}, {"text": "This need for training data is their major drawback, as annotated texts are, especially in the Life Sciences, rather costly to produce.", "labels": [], "entities": []}, {"text": "Furthermore, they are prone to over-fit to the training corpus, which renders evaluation results less inferable to real applications.", "labels": [], "entities": []}, {"text": "A third class of methods is based on pattern matching.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7857601046562195}]}, {"text": "Such methods work with patterns constructed from linguistically anno-tated text, which are matched against unseen text to detect relationships.", "labels": [], "entities": []}, {"text": "Patterns can either be inferred from examples ( or can be defined manually (.", "labels": [], "entities": []}, {"text": "Systems based on manually defined patterns typically use few patterns, leading to high precision but low recall ().", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9990930557250977}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9994599223136902}]}, {"text": "In contrast, systems that learn patterns automatically often produce more patterns and exhibit a better recall, at the cost of a decrease in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.999422550201416}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9983083009719849}]}, {"text": "To circumvent this penalty, several works have tried to improve patterns.", "labels": [], "entities": []}, {"text": "E.g., SPIES () filters patterns using the minimum description length (MDL) method which improves its F 1 by 6.72%.", "labels": [], "entities": [{"text": "minimum description length (MDL)", "start_pos": 42, "end_pos": 74, "type": "METRIC", "confidence": 0.735725000500679}, {"text": "F 1", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9922140538692474}]}, {"text": "Another classification of PPI extraction methods is based on the sentence representation that is applied.", "labels": [], "entities": [{"text": "PPI extraction", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9102099239826202}]}, {"text": "The simplest such representation is the bag of words (BoW) that occur in the sentence; more complex representations are constituent trees, capturing the syntactic structure of the sentence, and dependency trees (DTs), which represent the main grammatical entities and their relationships to each other.", "labels": [], "entities": []}, {"text": "PPI extraction methods use various sentence representation, e.g., are based only on BoW (), use only DTs (, or combine representations (.", "labels": [], "entities": [{"text": "PPI extraction", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9168471395969391}, {"text": "BoW", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8294947147369385}]}, {"text": "In the last years, dependency trees have become the most popular representation for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.9470773339271545}]}, {"text": "DTs characterize, via their dependency links, grammatical relationships among words.", "labels": [], "entities": []}, {"text": "They are particularly favored by kernel-based learning approaches, see e.g. () but also graph matching approaches using DTs have been proposed (.", "labels": [], "entities": [{"text": "graph matching", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.7183734476566315}]}, {"text": "However, these methods do not further utilize the grammatical information encoded in the dependency types (edge labels).", "labels": [], "entities": []}, {"text": "Recently proposed methods like ( modify the DTs by e.g. trimming irrelevant dependencies.", "labels": [], "entities": []}, {"text": "In contrast to these approaches, our method exploits the dependency types of DTs and performs basic transformations on DTs; we use Stanford dependencies, which are presumably the most often used DT representation in PPI extraction.", "labels": [], "entities": [{"text": "PPI extraction", "start_pos": 216, "end_pos": 230, "type": "TASK", "confidence": 0.8808343410491943}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We describe our novel method for extracting PPIs from text that is based on pattern matching in dependency graphs.", "labels": [], "entities": []}, {"text": "We evaluate our method against benchmark PPI corpora, and discuss results with a focus on dependency type information based methods.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Performance of pattern sets on the ensem- ble of all five corpora. # denotes the pattern set size.", "labels": [], "entities": []}, {"text": " Table 4: Results for collapsing interaction word  variants (G IW ).", "labels": [], "entities": []}, {"text": " Table 5: Dependency type aggregations used in gen- eralizer G UD . sopn combines the dependency ag- gregations for subj, obj, prep, and nn.", "labels": [], "entities": []}, {"text": " Table 6: Impact of collapsing the dependency types  appos and nn using generalizer G CD .", "labels": [], "entities": []}, {"text": " Table 7: Cross-learning results. Classifiers are trained on the ensemble of four corpora and tested on the  fifth one (except for the rule-based RelEx). Best results are typeset in bold.", "labels": [], "entities": [{"text": "RelEx", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.7468987703323364}]}]}