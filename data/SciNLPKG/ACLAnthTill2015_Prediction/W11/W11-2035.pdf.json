{"title": [{"text": "Improving Pronominal and Deictic Co-Reference Resolution with Multi-Modal Features", "labels": [], "entities": [{"text": "Deictic Co-Reference Resolution", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6099829077720642}]}], "abstractContent": [{"text": "Within our ongoing effort to develop a computational model to understand multi-modal human dialogue in the field of elderly care, this paper focuses on pronominal and deictic co-reference resolution.", "labels": [], "entities": [{"text": "pronominal and deictic co-reference resolution", "start_pos": 152, "end_pos": 198, "type": "TASK", "confidence": 0.5825201749801636}]}, {"text": "After describing our data collection effort, we discuss our annotation scheme.", "labels": [], "entities": []}, {"text": "We developed a co-reference model that employs both a simple notion of markable type, and multiple statistical models.", "labels": [], "entities": []}, {"text": "Our results show that knowing the type of the markable, and the presence of simultaneous pointing gestures improve co-reference resolution for personal and deictic pronouns.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.6507392972707748}]}], "introductionContent": [{"text": "Our ongoing research project, called RoboHelper, focuses on developing an interface for older people to effectively communicate with a robotic assistant that can help them perform Activities of Daily Living (ADLs), so that they can safely remain living in their home.", "labels": [], "entities": []}, {"text": "We are devising a multi-modal interface since people communicate with one another using a variety of verbal and non-verbal signals, including haptics, i.e., force exchange (as when one person hands a bowl to another person, and lets go only when s/he senses that the other is holding it).", "labels": [], "entities": []}, {"text": "We have collected amid size multi-modal human-human dialogue corpus, that we are currently processing and analyzing.", "labels": [], "entities": []}, {"text": "Meanwhile, we have started developing one core component of our multi-modal interface, a coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.8514214754104614}]}, {"text": "In this paper, we will present the component of the system that resolves pronouns, both personal (I, you, it, they), and deictic (this, that, these, those, here, there).", "labels": [], "entities": []}, {"text": "Hence, this paper presents our first steps toward a full co-reference resolution module, and ultimately, the multi-modal interface.", "labels": [], "entities": []}, {"text": "Co-reference resolution is likely the discourse and dialogue processing task that has received the most attention.", "labels": [], "entities": [{"text": "Co-reference resolution", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8228699564933777}, {"text": "dialogue processing", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7681500017642975}]}, {"text": "However, as notes, research on co-reference resolution has mostly been applied to written text; this task is more difficult in dialogue.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7246052920818329}]}, {"text": "First, utterances maybe informal, ungrammatical or disfluent; second, people spontaneously use hand gestures, body gestures and gaze.", "labels": [], "entities": []}, {"text": "Pointing gestures are the easiest gestures to identify, and vision researchers in our project are working on recognizing pointing and other hand gestures.", "labels": [], "entities": []}, {"text": "In this paper, we replicate the results from), that pointing gestures help improve co-reference, in a very different domain.", "labels": [], "entities": []}, {"text": "Other work has shown that gestures can help detect sentence boundaries or user intentions (.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the data collection and the ongoing annotation.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss our coreference resolution system, and we present experiments and results in Section 4.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9504801332950592}]}, {"text": "we undertook anew data collection effort.", "labels": [], "entities": [{"text": "data collection", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.6599759310483932}]}, {"text": "Our experiments were conducted in a fully functional studio apartment at Rush University in Chicago - shows two screen-shots from our recorded experiments.", "labels": [], "entities": []}, {"text": "We equipped the room with 7 web cameras to ensure multiple points of view.", "labels": [], "entities": []}, {"text": "Each of the two participants in the experiments wears a microphone, and a data glove on their dominant hand to collect haptics data.", "labels": [], "entities": []}, {"text": "The ADLs we focused on include ambulating, getting up from a bed or a chair, finding pots, opening cans and containers, putting pots on a stove, setting the table etc.", "labels": [], "entities": []}, {"text": "Two students in gerontological nursing play the role of the helper (HEL), both in pilot studies and with real subjects.", "labels": [], "entities": [{"text": "HEL", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.925227165222168}]}, {"text": "In 5 pilot dialogues, two faculty members played the role of the elderly person (ELD).", "labels": [], "entities": []}, {"text": "In the 15 real experiments, ELD resides in an assisted living facility and was transported to the apartment mentioned above.", "labels": [], "entities": []}, {"text": "All elderly subjects are highly functioning at a cognitive level and do not have any major physical impairment.", "labels": [], "entities": []}, {"text": "The size of our collected video data is shown in.", "labels": [], "entities": []}, {"text": "The number of subjects refers to the number of different ELD's and does not include the helpers; we do include our 5 pilot dialogues though, since those pilot interactions do not measurably differ from those with the real subjects.", "labels": [], "entities": []}, {"text": "Usually one experiment lasts about 50' (recording starts after informed consent and after the microphones and data gloves have been put on).", "labels": [], "entities": []}, {"text": "Further, we eliminated irrelevant content such as interruptions, e.g. by the person who accompanied the elderly subjects, and further explanations of the tasks.", "labels": [], "entities": []}, {"text": "This resulted in about 15 minutes of what we call effective data for each subject; the effective data comprises 4782 turns (see).", "labels": [], "entities": []}], "datasetContent": [{"text": "The classification models described above were implemented using the Weka package (.", "labels": [], "entities": [{"text": "Weka package", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9711749255657196}]}, {"text": "Specifically, for each model, we experimented with J48 (a decision tree implementation) and LibSVM (a Support Vector Machine implementation).", "labels": [], "entities": []}, {"text": "All the results reported below are calculated using 10 fold cross-validation.", "labels": [], "entities": []}, {"text": "We evaluated the performances of individual models separately, and of the system as a whole (  The results in are not surprising, since detecting the type of markables is a simple task.", "labels": [], "entities": []}, {"text": "Indeed the results of the baseline model are extremely 309   high as well.", "labels": [], "entities": []}, {"text": "We compute the baseline by assigning to the potential markable (i.e., each word) its most frequent class in the training set (recall that the four classes include None as well).", "labels": [], "entities": []}, {"text": "For the Co-reference model, we conducted 2 sets of experiments to ascertain the effect of including Gesture in the model.", "labels": [], "entities": [{"text": "Gesture", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9953581690788269}]}, {"text": "As shown in, both J48 and LibSVM obtain better results when we include gestures in the model.", "labels": [], "entities": [{"text": "J48", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.9068422317504883}, {"text": "LibSVM", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.8485479950904846}]}, {"text": "\u03c7 2 shows that differences in precision and recall 1 are significant at the p \u2264 0.01 level, though the absolute improvement is not high.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9993003606796265}, {"text": "recall 1", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9906021058559418}]}, {"text": "As concerns the evaluation of the whole system, we ran a 4-way experiment, where we examine the performance of the system on all pronouns, and on those pronouns left after eliminating first and second person pronouns, without and with Gesture information.", "labels": [], "entities": [{"text": "Gesture", "start_pos": 235, "end_pos": 242, "type": "METRIC", "confidence": 0.7947021126747131}]}, {"text": "We also ran two sets of baseline experiments.", "labels": [], "entities": []}, {"text": "In the baseline experiments, we link each pronoun we want to resolve, to the most recent utterancemarkable token and to a pointed-to markable token (if applicable).", "labels": [], "entities": []}, {"text": "Markables are filtered by the same compatibility rules mentioned above.", "labels": [], "entities": []}, {"text": "Regarding the metrics we used for evaluation, we used the same method as, which is also similar to MUC standard (Hirschman, 1 \u03c7 2 does not apply to the F-Measure. 1997).", "labels": [], "entities": [{"text": "MUC standard", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8701327443122864}, {"text": "F-Measure. 1997", "start_pos": 152, "end_pos": 167, "type": "DATASET", "confidence": 0.8366254270076752}]}, {"text": "As the golden set, we used the human annotated links from the pronouns to markables in the same context of four utterances used by the system.", "labels": [], "entities": []}, {"text": "Then, we compared the co-reference links found by the system against the golden set, and we finally calculated precision, recall and F-Measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9998200535774231}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9997212290763855}, {"text": "F-Measure", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.998589813709259}]}, {"text": "shows that the F-measure is higher when including gestures, no matter the type of pronouns.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9990377426147461}]}, {"text": "When we include gestures, there is no difference between \"All Pronouns\" and \"3rd Person + Deictic\".", "labels": [], "entities": []}, {"text": "In the \"3rd Person + Deictic\" experiments, we observed huge drops in recall, from 0.902 to 0.028 for J48, and from 0.695 to 0.009 for LibSVM algorithm.", "labels": [], "entities": [{"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9993745684623718}]}, {"text": "This confirms the point we made earlier, that 3rd person pronouns/deictic words) often do not have textual antecedents, since when accompanied by simultaneous pointing they introduce new entities in a dialogue.", "labels": [], "entities": []}, {"text": "Comparison to previous work is feasible only at a high level, because of the usage of different corpora and/or measurement metrics.", "labels": [], "entities": []}, {"text": "This said, our model with gestures outperforms, who did not use gesture information to resolve pronouns in spoken dialogue.", "labels": [], "entities": []}, {"text": "used the 20 Switchboard dialogues as their experiment dataset, and used the MUC metrics.", "labels": [], "entities": [{"text": "MUC metrics", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.8317670822143555}]}, {"text": "Our re-310 sults are similar to, but there are two main differences.", "labels": [], "entities": []}, {"text": "First, the corpus they used is smaller than what we used in this paper.", "labels": [], "entities": []}, {"text": "Their corpus was collected by themselves and consisted of 16 videos, each video was 2-3 minutes in length.", "labels": [], "entities": []}, {"text": "Second, they used a difference measurement metrics called CEAF ().", "labels": [], "entities": [{"text": "CEAF", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.5923058986663818}]}], "tableCaptions": [{"text": " Table 2: Annotated Corpus Size", "labels": [], "entities": [{"text": "Annotated Corpus Size", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5840304891268412}]}, {"text": " Table 3: Markable Model Performance", "labels": [], "entities": []}, {"text": " Table 4: Co-reference Model Performance", "labels": [], "entities": []}, {"text": " Table 5: Co-reference System Performance (Markable + Co-reference Models)", "labels": [], "entities": []}]}