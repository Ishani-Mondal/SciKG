{"title": [{"text": "Detecting Dependency Parse Errors with Minimal Resources", "labels": [], "entities": [{"text": "Detecting Dependency Parse Errors", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8611870259046555}]}], "abstractContent": [{"text": "To detect errors in automatically-obtained dependency parses, we take a grammar-based approach.", "labels": [], "entities": []}, {"text": "In particular, we develop methods that incorporate n-grams of different lengths and use information about possible parse revisions.", "labels": [], "entities": []}, {"text": "Using our methods allows annotators to focus on problematic parses, with the potential to find over half the parse errors by examining only 20% of the data, as we demonstrate.", "labels": [], "entities": []}, {"text": "A key result is that methods using a small gold grammar outperform methods using much larger grammars containing noise.", "labels": [], "entities": []}, {"text": "To perform annotation error detection on newly-parsed data, one only needs a small grammar.", "labels": [], "entities": [{"text": "annotation error detection", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6729141672452291}]}], "introductionContent": [], "datasetContent": [{"text": "The intended use of the scores is for annotators to start with the lowest-scoring positions (i.e., tokens) and work upwards.", "labels": [], "entities": []}, {"text": "The F-measures give some sense of this, but we need away to evaluate across different corpora.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9235363602638245}]}, {"text": "To account for this, we do two things: first, we report the values for the lowest threshold, so that one can get a sense of the precision for the highest-priority cases.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9982805252075195}]}, {"text": "Secondly, we use Talbanken for development, finding the best F 0.5 score and calculating the percentage of the test corpus that the threshold identifies.", "labels": [], "entities": [{"text": "Talbanken", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.8100290298461914}, {"text": "F 0.5 score", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.978847603003184}]}, {"text": "In the case of evaluating on the small test corpus, 424 5,656 = 7.5%, soother experiments with a similar large/small split would be evaluated with a threshold identifying as close to 7.5% of the evaluation corpus as possible.", "labels": [], "entities": []}, {"text": "We reset thresholds for experiments where the large corpus is the test corpus, as these behave differently, due to differing parser quality; in section 5.1, we will set this at 23%.", "labels": [], "entities": []}, {"text": "The bottom two lines of table 7, for instance, present results for the lowest threhold (0) and the one which identifies as close to 23% of the tokens as possible.", "labels": [], "entities": []}, {"text": "Also, as we emphasize precision, in future tables we report F 0.5 and ignore F 1 .", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9993826150894165}, {"text": "F 0.5", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.971993625164032}, {"text": "F 1", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9624734818935394}]}], "tableCaptions": [{"text": " Table 1: Talbanken error detection results for different  scores: parser = MaltParser trained on large data; gram- mar = large gold; evaluation data = small data (pos. =  positions below threshold (Thr.))", "labels": [], "entities": [{"text": "Talbanken error detection", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5874602893988291}, {"text": "Thr.))", "start_pos": 199, "end_pos": 205, "type": "METRIC", "confidence": 0.9604080319404602}]}, {"text": " Table 2: Revision checking: same set-up as in table 1,  using high-gram method for checking. (L=lowest,  A=all, F =flagged, U=unflagged, U x=unflagged up to  threshold x)", "labels": [], "entities": [{"text": "Revision checking", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7707459628582001}]}, {"text": " Table 3: Talbanken error detection results for differ- ent scores: parser = MaltParser trained on small data;  grammar = small gold; evaluation data = large data", "labels": [], "entities": [{"text": "Talbanken error detection", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5950830578804016}]}, {"text": " Table 4: Talbanken oracle error detection results: same  set-up as table 3, but grammar = large gold", "labels": [], "entities": [{"text": "Talbanken oracle error detection", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.5749183595180511}, {"text": "grammar", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.94793301820755}]}, {"text": " Table 5: Revision checking: same set-up as in table 3,  using high-gram method for checking", "labels": [], "entities": [{"text": "Revision checking", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9089058339595795}, {"text": "checking", "start_pos": 84, "end_pos": 92, "type": "TASK", "confidence": 0.7903010845184326}]}, {"text": " Table 7: WSJ: high-gram scores for lowest and 23%  thresholds: parser = default MST trained on differ- ent data (Par.); grammar = gold from section listed  (Gram.); evaluation data = sections 02-21", "labels": [], "entities": [{"text": "WSJ", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5102218389511108}]}, {"text": " Table 8: WSJ revision checking results, using the high- gram method, for the parser trained on section 00; same  set-up as in table 7", "labels": [], "entities": [{"text": "WSJ revision checking", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6304714878400167}]}, {"text": " Table 9: Talbanken error detection results for the high- gram method: parser = MaltParser trained on small  data; grammar = large noisy; evaluation data = small  data (No-Rev. = No revision checking)", "labels": [], "entities": [{"text": "Talbanken error detection", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.579086184501648}]}, {"text": " Table 10: WSJ: high-gram scores: parser = default  MST trained on different data (Par.); grammar = ex- tracted from appropriate parsed 02-21; evaluation data  = 02-21", "labels": [], "entities": [{"text": "WSJ", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.6721007823944092}]}]}