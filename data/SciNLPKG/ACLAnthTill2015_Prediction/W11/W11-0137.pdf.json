{"title": [], "abstractContent": [{"text": "Question classifiers are used within Question Answering to predict the expected answer type fora given question.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7583371698856354}]}, {"text": "This paper describes the first steps towards applying a similar methodology to identifying question classes in dialogue contexts, beginning with a study of questions drawn from the Enron email corpus.", "labels": [], "entities": [{"text": "Enron email corpus", "start_pos": 181, "end_pos": 199, "type": "DATASET", "confidence": 0.894932488600413}]}, {"text": "Human-annotated data is used as a gold standard for assessing the output from an existing, open-source question classifier (QA-SYS).", "labels": [], "entities": []}, {"text": "Problem areas are identified and potential solutions discussed.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The new dialogue taxonomy, with mappings to Li & Roth where applicable, and percentage  distribution in the Enron sample", "labels": [], "entities": []}, {"text": " Table 3: Examples of questions correctly classified", "labels": [], "entities": []}, {"text": " Table 4: Recall and precision by category", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9825316071510315}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9993625283241272}]}]}