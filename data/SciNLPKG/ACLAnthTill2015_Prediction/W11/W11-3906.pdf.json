{"title": [{"text": "Robust Unsupervised and Semi-Supervised Methods in Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Co-training, as a semi-supervised learning method , has been recently applied to semantic role labeling to reduce the need for costly annotated data using unannotated data.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7029427886009216}]}, {"text": "A main concern in co-training is how to split the problem into multiple views to derive learning features, so that they can effectively train each other.", "labels": [], "entities": []}, {"text": "We investigate various feature splits based on two SRL views, constituency and dependency, with different variations of the algorithm.", "labels": [], "entities": []}, {"text": "Balancing the feature split in terms of the performance of the underlying classifiers showed to be useful.", "labels": [], "entities": []}, {"text": "Also, co-training with a common training set performed better than when separate training sets are used for co-trained classifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (SRL) parses a natural language sentence into its event structure.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL) parses a natural language sentence", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8728588982061907}]}, {"text": "This information has been shown useful for several NLP tasks such as information extraction, question answering, summarization, and machine translation (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.8161636888980865}, {"text": "question answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.910131573677063}, {"text": "summarization", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.9867114424705505}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.8183867931365967}]}, {"text": "After its introduction by, a considerable body of NLP research has been devoted to SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9845631718635559}]}, {"text": "CoNLL) followed that seminal work by using similar input resources mainly built upon constituent-based syntax and achieved state-of-the-art results ().", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8840901255607605}]}, {"text": "Subsequent CoNLL shared tasks () put forth the use of another framework based on dependency syntax.", "labels": [], "entities": []}, {"text": "This framework also led to well-performed systems.", "labels": [], "entities": []}, {"text": "Almost all of the SRL research has been based on supervised machine learning methods exploiting manually annotated corpora like FrameNet () and).", "labels": [], "entities": [{"text": "SRL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9774442315101624}]}, {"text": "FrameNet annotates some example sentences for each semantic frame, which questions its representativeness of the language, necessary for statistical learning.", "labels": [], "entities": []}, {"text": "Propbank, on the other hand, annotates all the sentences from WSJ corpus and remedies that problem to some extent, but unlike FrameNet, its coverage is limited to the newswire text of WSJ.", "labels": [], "entities": [{"text": "Propbank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9598109722137451}, {"text": "WSJ corpus", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.975063145160675}, {"text": "newswire text of WSJ", "start_pos": 167, "end_pos": 187, "type": "DATASET", "confidence": 0.7052939012646675}]}, {"text": "This domain dependence affects the performance of the systems using PropBank on any different domain of text).", "labels": [], "entities": []}, {"text": "Considering the cost and difficulty of creating such resources with all of these shortcomings, it seems infeasible to build a comprehensive hand-crafted corpus of natural language for training robust SRL systems.", "labels": [], "entities": [{"text": "SRL", "start_pos": 200, "end_pos": 203, "type": "TASK", "confidence": 0.9826966524124146}]}, {"text": "Such issues in statistical learning have motivated researchers to devise semi-supervised learning methods.", "labels": [], "entities": []}, {"text": "These methods aim at utilizing a large amount of unannotated data along with small amount of annotated data.", "labels": [], "entities": []}, {"text": "The existence of raw natural text in huge amounts is a promising point of using such methods for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9938336610794067}]}, {"text": "Co-training is a semi-supervised algorithm in which two or more classifiers iteratively provide each other with the training examples by labeling unannotated data.", "labels": [], "entities": []}, {"text": "Each classifier is based on the learning features derived from conditionally independent and redundant views of the underlying problem.", "labels": [], "entities": []}, {"text": "These two assumptions are stated as the requirements of the algorithm in its original work by.", "labels": [], "entities": []}, {"text": "Constituency and dependency provide attractive views of SRL problem to be exploited in a co-training setup.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9920910596847534}]}, {"text": "The major motivation is the promising results of their use in SRL, which satisfies the first assumption.", "labels": [], "entities": [{"text": "SRL", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9840943217277527}]}, {"text": "There is a set of rules to convert constituency to dependency, which may question the second assumption.", "labels": [], "entities": []}, {"text": "However, these rules are oneway, and moreover, argues that this assumption can be loosened.", "labels": [], "entities": []}, {"text": "While several parameters are involved in cotraining of SRL systems, the most important one is the split of the feature views.", "labels": [], "entities": [{"text": "SRL", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9618282318115234}]}, {"text": "This work investigates the effects of feature split by comparing the co-training progress when using various splits.", "labels": [], "entities": [{"text": "feature split", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.6953168362379074}]}, {"text": "It also examines several variations of the algorithm.", "labels": [], "entities": []}, {"text": "The algorithm is applied to the SRL problem when only a small amount of labeled data is available.", "labels": [], "entities": [{"text": "SRL problem", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.932801216840744}]}], "datasetContent": [{"text": "This work uses co-training to address the SRL training problem when the amount of available annotated data is small.", "labels": [], "entities": [{"text": "SRL training", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9139338731765747}]}, {"text": "The data and evaluation settings used are similar to the CoNLL 2005 and 2008 shared tasks.", "labels": [], "entities": [{"text": "CoNLL 2005 and 2008 shared tasks", "start_pos": 57, "end_pos": 89, "type": "DATASET", "confidence": 0.9277777671813965}]}, {"text": "For evaluation, the same script used for 2005 shared task is used here and the measures are precision, recall, and their harmonic mean, F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9997305274009705}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9994235038757324}, {"text": "F1", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9881001710891724}]}, {"text": "However, the data is changed in some ways to fulfill the objectives of this research, which is explained in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the Base Classifiers with Various Syntactic Inputs and Feature Sets", "labels": [], "entities": []}, {"text": " Table 2: Co-training Performance with Confidence-based Selection", "labels": [], "entities": []}, {"text": " Table 3: Co-training Performance with Separate Training Sets", "labels": [], "entities": []}]}