{"title": [{"text": "A Practical Algorithm for Intersecting Weighted Context-free Grammars with Finite-State Automata", "labels": [], "entities": [{"text": "Intersecting Weighted Context-free Grammars", "start_pos": 26, "end_pos": 69, "type": "TASK", "confidence": 0.7375883162021637}]}], "abstractContent": [{"text": "It is well known that context-free parsing can be seen as the intersection of a context-free language with a regular language (or, equivalently, the intersection of a context-free grammar with a finite-state automaton).", "labels": [], "entities": [{"text": "context-free parsing", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.5947194248437881}]}, {"text": "The present article provides a practical efficient way to compute this intersection by converting the grammar into a special finite-state automaton (the GLR(0)-automaton) which is subsequently intersected with the given finite-state automaton.", "labels": [], "entities": []}, {"text": "As a byproduct, we present a gen-eralisation of Tomita's algorithm to recognize several inputs simultaneously.", "labels": [], "entities": []}], "introductionContent": [{"text": "At the least since the paper of which defined parsing as the intersection of a context-free language (given by a grammar) with a regular language (the \"input\", which can be seen as a simple finite-state automaton) the importance of the notion of intersection for parsing purposes became apparent.", "labels": [], "entities": [{"text": "parsing", "start_pos": 263, "end_pos": 270, "type": "TASK", "confidence": 0.9578197002410889}]}, {"text": "Intersection is first of all defined on the language level: Intersect the (possibly infinite) set of strings which the grammar generates with the (possibly infinite) set of strings the finite-state automaton accepts.", "labels": [], "entities": []}, {"text": "Since this may not be done effectively due to the infiniteness of the involved sets, the operation has to be lifted to the more compact level of the devices generating the sets.", "labels": [], "entities": []}, {"text": "In principle, there are at least two ways to intersect a context-free grammar G with a finite-state automaton A: 1.", "labels": [], "entities": []}, {"text": "Convert G into a suitable pushdown automaton (PDA) M and intersect A with M to get M . Then extract the result grammar G from M . 2. Intersect G and A directly.", "labels": [], "entities": []}, {"text": "With respect to 1., it is well known that the class of pushdown automata is closed under intersection with finite-state automata (henceforth FSA) (cf.).", "labels": [], "entities": []}, {"text": "The standard construction assumes a deterministic FSA (see next section) and operates both automata in parallel: Whenever the PDA makes a move on a certain input symbol a, this move is combined with a corresponding move of the FSA on a (see for details of the construction).", "labels": [], "entities": [{"text": "FSA", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7531187534332275}, {"text": "FSA", "start_pos": 227, "end_pos": 230, "type": "METRIC", "confidence": 0.6541692018508911}]}, {"text": "Method 1 works most efficiently in case of grammars in Greibach normal form (GNF).", "labels": [], "entities": []}, {"text": "Remember that a context-free grammar is in GNF if each of its rules conforms to the format A \u2192 aB 1 . .", "labels": [], "entities": []}, {"text": "Bk where a is an alphabet symbol, A is a nonterminal (phrase) symbol and B 1 . .", "labels": [], "entities": []}, {"text": "Bk is a possibly empty sequence of nonterminals.", "labels": [], "entities": []}, {"text": "Given a grammar in GNF, it is very easy to construct a pushdown automaton from it: when the PDA has A on its stack and next reads an a, it replaces A by B 1 . .", "labels": [], "entities": []}, {"text": "Bk . Every context-free grammar can be converted into a weakly equivalent grammar in GNF (cf.), but this changes the trees generated by the grammar and may lead to a substiantial increase in grammar size.", "labels": [], "entities": []}, {"text": "Method 2 was presented in and works in the following way: Consider a grammar rule X 0 \u2192 X 1 X 2 . .", "labels": [], "entities": []}, {"text": "X k . Then choose an arbitrary state sequence p 0 . .", "labels": [], "entities": []}, {"text": "pk from the state set Q of the FSA and create anew grammar rule Leaving the grammar fixed, the time complexity of this method is in O(|Q| r+1 ) where r is the length of the longest right-hand side of a grammar rule.", "labels": [], "entities": [{"text": "FSA", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8686196208000183}, {"text": "O", "start_pos": 132, "end_pos": 133, "type": "METRIC", "confidence": 0.9701195359230042}]}, {"text": "The drawback of the method is that it creates a great amount of useless nonterminals and rules.", "labels": [], "entities": []}, {"text": "Nederhof and Satta (2008) improved the algorithm by adding simultaneous top-down and bottom-up filtering mechanisms to prevent the creation of useless symbols which of course does not change its worstcase complexity.", "labels": [], "entities": []}, {"text": "This upper bound can be reduced to O(|Q| 3 ) by binarising the grammar rules (which naturally changes the generated trees).", "labels": [], "entities": [{"text": "O", "start_pos": 35, "end_pos": 36, "type": "METRIC", "confidence": 0.9959802627563477}]}, {"text": "However, it is not clear how the algorithm behaves in practice on grammars with several thousands of rules ubiquituous in natural language processing.", "labels": [], "entities": []}, {"text": "In the present paper, we propose another algorithm based on the creation of a GLR(0) automaton which is subsumed by method 1 above.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows: Section 2 briefly defines the relevant notions.", "labels": [], "entities": []}, {"text": "Section 3 defines GLR(0) automata and presents an algorithm how to intersect them with FSAs.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8619661331176758}]}, {"text": "In Section 4 we report some experiments conducted with a big grammar extracted from a treebank.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented the algorithm from the last section in the C++ programming language within the fsm2 framework (see).", "labels": [], "entities": []}, {"text": "The grammar used for the experiments has been extracted from the TiGer treebank (cf.).", "labels": [], "entities": [{"text": "TiGer treebank", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.933735728263855}]}, {"text": "It contains 14,379 rules 7 , and the sizes of the alphabet and the nonterminal sets are 51 and 25, resp.", "labels": [], "entities": []}, {"text": "The length of the longest right-hand side of a rule is 17.", "labels": [], "entities": [{"text": "length", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9788645505905151}]}, {"text": "For the FSA operand of the intersection algorithm, we created three minimal FSA accepting 1, 10 and 100 sentences (tag sequences) randomly taken from the TiGer corpus.", "labels": [], "entities": [{"text": "TiGer corpus", "start_pos": 154, "end_pos": 166, "type": "DATASET", "confidence": 0.9248034954071045}]}, {"text": "The sizes of the automata are summarised in. shows the results of the experiments which were carried out on a 2.8 GHz CPU.", "labels": [], "entities": []}, {"text": "The columns |Q| and |\u03b4| contain the sizes of the automata resulting from the operation mentioned before.", "labels": [], "entities": []}, {"text": "Since treebank grammars tend to avoid recursive rules and therefore assign flat structures to input strings, they create a lot of readings with spurious ambiguities.", "labels": [], "entities": []}, {"text": "reports that approximately 9% of the rules of the Penn treebank are never used in a maximum likelihood setting since these rules are subsumed by combinations of other rules with a higher combined probability.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9925697445869446}]}, {"text": "We expect even better intersection timings in the face of more linguistically realistic grammars.", "labels": [], "entities": [{"text": "intersection timings", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8006439805030823}]}], "tableCaptions": [{"text": " Table 1: Sizes of the input FSA.", "labels": [], "entities": [{"text": "FSA", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.40782222151756287}]}, {"text": " Table 2: Results of the experiments with the TiGer gram- mar.", "labels": [], "entities": [{"text": "TiGer gram- mar", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.6929069757461548}]}]}