{"title": [{"text": "Unsupervised Russian POS Tagging with Appropriate Context", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.7341738343238831}]}], "abstractContent": [{"text": "While adopting the contextualized hidden Markov model (CHMM) framework for unsupervised Russian POS tagging, we investigate the possibility of utilizing the left, right, and unambiguous context in the CHMM framework.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.6868072152137756}]}, {"text": "We propose a backoff smoothing method that incorporates all three types of context into the transition probability estimation during the expectation-maximization process.", "labels": [], "entities": []}, {"text": "The resulting model with this new method achieves overall and disambiguation accuracies comparable to a CHMM using the classic backoff smoothing method for HMM-based POS tagging from (Thede and Harper, 1999).", "labels": [], "entities": [{"text": "HMM-based POS tagging", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.6307397186756134}]}], "introductionContent": [{"text": "A careful review of the work on unsupervised POS tagging in the past two decades reveals that the hidden Markov model (HMM) has been the standard approach since the seminal work of and and that researchers sought to improve HMM-based unsupervised POS tagging from a variety of perspectives, including exploring dictionary usage, context utilization, sparsity control and modeling, and parameter and model updates tuned to linguistic features.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.7891729474067688}, {"text": "HMM-based unsupervised POS tagging", "start_pos": 224, "end_pos": 258, "type": "TASK", "confidence": 0.6562856510281563}]}, {"text": "For example, () and () utilized contextualized HMM (CHMM) to capture rich context.", "labels": [], "entities": []}, {"text": "To account for sparsity, and) utilized the Dirichlet hyperparameters of the Bayesian HMM.", "labels": [], "entities": []}, {"text": "() integrated the discriminative logistic regression model into the M-step of the standard generative model to allow rich linguistically-motivated features.", "labels": [], "entities": []}, {"text": "Unsupervised systems went beyond the mainstream HMM framework by employing methods such as prototype-driven clustering, Bayesian LDA (, integer programming (, and K-means clustering (.", "labels": [], "entities": []}, {"text": "Despite this large body of work, little effort has been devoted to unsupervised Russian POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.6251119077205658}]}, {"text": "Supervised Russian POS systems emerged in recent years.", "labels": [], "entities": []}, {"text": "For example, eleven supervised systems entered the POS track of the 2010 Russian Morphological Parsers Evaluation . Although the top two systems from the 2010 Evaluation achieved near perfect accuracy over the Russian National Corpus, little has been done on unsupervised Russian POS tagging.", "labels": [], "entities": [{"text": "POS track of the 2010 Russian Morphological Parsers Evaluation", "start_pos": 51, "end_pos": 113, "type": "DATASET", "confidence": 0.582050684425566}, {"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9979288578033447}, {"text": "Russian National Corpus", "start_pos": 210, "end_pos": 233, "type": "DATASET", "confidence": 0.7983612418174744}, {"text": "POS tagging", "start_pos": 280, "end_pos": 291, "type": "TASK", "confidence": 0.5721432864665985}]}, {"text": "In this paper, we present our solution to unsupervised Russian POS tagging by adopting the CHMM.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.7110199928283691}, {"text": "CHMM", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.9102324843406677}]}, {"text": "Our choice is based on the accuracy and efficiency of CHMM, an identical rationale to that behind (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994352459907532}, {"text": "CHMM", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.7121013402938843}]}, {"text": "We aim to achieve two goals.", "labels": [], "entities": []}, {"text": "First, we intend to resolve the potential issue of missing useful contextual features by the backoff smoothing scheme in) and () for transition probabilities.", "labels": [], "entities": []}, {"text": "Second, we explore the possibility of incorporating unambiguous context into transition probability estimation in an HMM framework.", "labels": [], "entities": [{"text": "transition probability estimation", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.6623507638772329}]}, {"text": "We propose a novel plan to achieve both goals in a unified approach.", "labels": [], "entities": []}, {"text": "In the following, we adopt the CHMM for unsupervised Russian POS tagging in section 2.", "labels": [], "entities": [{"text": "CHMM", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.538486659526825}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.6798640638589859}]}, {"text": "Section 3 highlights the potential issue of missing useful left context in the backoff scheme by).", "labels": [], "entities": []}, {"text": "Section 4 illustrates an updated backoff scheme to resolve this potential issue.", "labels": [], "entities": []}, {"text": "This scheme also unifies the left, right, and unambiguous context.", "labels": [], "entities": []}, {"text": "The experiments and discussion are presented in section 5.", "labels": [], "entities": []}, {"text": "We present conclusions in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We designed three experiments to test three combinations of the context, in addition to experimenting with a traditional second-order HMM.", "labels": [], "entities": []}, {"text": "The Appen corpus contains a development set and an: Experiments, overall and disambiguation accuracies over test data evaluation set.", "labels": [], "entities": [{"text": "Appen corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7786028683185577}]}, {"text": "We passed both sets through the Russian lemmatizer to obtain POS tags for the data and had the tags manually corrected by a Russian linguist.", "labels": [], "entities": []}, {"text": "Thus, we have created both development and evaluation data.", "labels": [], "entities": []}, {"text": "14% of words/tokens in both development and evaluation data have multiple POS tags.", "labels": [], "entities": []}, {"text": "summarizes our experimental settings and results over the evaluation data.", "labels": [], "entities": []}, {"text": "The second-order HMM was trained with the traditional transition probability p(t i |t i\u22122 t i\u22121 ) and emission probability p(w i |t i ).", "labels": [], "entities": [{"text": "emission probability p", "start_pos": 102, "end_pos": 124, "type": "METRIC", "confidence": 0.9509395559628805}]}, {"text": "It gained an overall accuracy of 94.88%, and was able to correctly disambiguate 63.42% of the ambiguous words/tokens.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.999438464641571}]}, {"text": "All three CHMM models were trained with the emission probability p(w i |t it i+1 ) initialized with 20% of the unlabeled training corpus.", "labels": [], "entities": [{"text": "emission probability p", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.9143532911936442}]}, {"text": "Model CHMM left context considered the left context bigram t i\u22121 ti when calculating the second term in equation (1).", "labels": [], "entities": []}, {"text": "Model CHMM right context considered the right context bi-gram ti t i+1 when calculating the same term.", "labels": [], "entities": []}, {"text": "Model CHMM unique \u2190 left/right unified both unambiguous context counts and estimated counts for left and right context from the EM process, using equation.", "labels": [], "entities": []}, {"text": "All CHMM models achieved accuracies 1% higher than the HMM, while the disambiguation accuracies from the former three are 7\u22129% higher than the latter.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9979196190834045}]}, {"text": "This shows that the CHMM models capture more useful context information for Russian POS tagging than the traditional HMM.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.7103812396526337}]}, {"text": "At the same time, the overall and disambiguation accuracies between CHMM right context and CHMM unique \u2190 left/right are comparable.", "labels": [], "entities": []}, {"text": "Error analyses indicate that a backoff scheme for emission probabilities is also needed to incorporate the left context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Experiments, overall and disambiguation  accuracies over test data", "labels": [], "entities": []}]}